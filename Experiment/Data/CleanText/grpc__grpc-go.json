{
    "googlebot": "Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. Thanks for your pull request.\nIt looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA) at https://cla.developers.google.com/.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check the information on your CLA or see this help article on setting the email on your git commits.\nOnce you've done that, please reply here to let us know.  If you signed the CLA as a corporation, please let us know the company's name.\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project, in which case you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request) and all commit authors, but as best as we can tell these commits were authored by someone else.  If that's the case,  please add them to this pull request and have them confirm that they're okay with these commits being contributed to Google.  If we're mistaken and you did author these commits, just reply here to confirm.\n need_author_consent \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this state. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. So there's good news and bad news.\n:thumbsup: The good news is that everyone that needs to sign a CLA (the pull request submitter and all commit authors) have done so.  Everything is all good there.\n:confused: The bad news is that it appears that one or more commits were authored by someone other than the pull request submitter.  We need to confirm that they're okay with their commits being contributed to this project.  Please have them confirm that here in the pull request.\nNote to project maintainer: This is a terminal state, meaning the cla/google commit status will not change from this State. It's up to you to confirm consent of the commit author(s) and merge this pull request when appropriate.\n need_author_consent \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. We found a Contributor License Agreement for you (the sender of this pull request), but were unable to find agreements for the commit author(s).  If you authored these, maybe you used a different email address in the git commits than was used to sign the CLA (login here to double check)?  If these were authored by someone else, then they will need to sign a CLA as well, and confirm that they're okay with these being contributed to Google.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n need_author_cla \n. Thanks for your pull request.  It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\n:memo: Please visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify.  Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address.  Check your existing CLA data and verify that your email is set on your git commits.\nIf your company signed a CLA, they designated a Point of Contact who decides which employees are authorized to participate. You may need to contact the Point of Contact for your company and ask to be added to the group of authorized contributors. If you don't know who your Point of Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have the pull request author add another comment and the bot will run again.\n\n need_sender_cla \n. CLAs look good, thanks!\n ok \n. ",
    "dsymonds": "Okay, I've changed the package doc too.\n. I don't have write access to this repo. Can you merge it please?\n. Calling this a \"defect\" is too strong. It's common Go style to snuggle uses of :=, but it's not wrong per se to not do that. There's a bunch of places that I think you've made the code worse; I'll comment on them.\n. I haven't studied your proposed changes to locking and connection closing, so I don't have an opinion either way on that. Your trivial change to clientconn.go seems fine and worth merging independently since it should be uncontroversial.\n. There's my final round of comments.\nAlso, it looks like the .pb.go file was not checked in. Please do that; we shouldn't force people to have to go install protoc and protoc-gen-go just to build this example.\n. You shouldn't need to do that explicitly unless you're going to be generating protos. Just using this package does not require that.\n. Yes, that's what I mean. The generated output should be checked in so you don't have to run protoc unless you're modifying it.\n. Let's not just sprinkle options for things that we can simply do the right thing. A dial failure can fail in permanent/temporary ways. There's no point in doing any retries if the address is completely malformed, since it'll never get a connection. But a grpc.DialTimeout would be okay.\nA context isn't a great choice for dialing. Contexts are meant to be request-scoped, and dialing should rarely be in the scope of a request. The OS primitives usually aren't cancellable either.\n. No, I suggest grpc.Dial should have a ...grpc.DialOption variadic argument, and have grpc.DialTimeout to return such an option.\n. Your original proposal said \"a configurable limit on retries\", which I interpreted to mean a limit on the number of retries, which I think is a bad option. A limit on the total time trying to dial would be fine though. Even better would be to (also) detect the types of errors, and for the known-permanent failures (e.g. malformed address) have grpc.Dial fail immediately.\n. The net package (http://golang.org/pkg/net/) has a bunch of specific error types (e.g. AddrError or InvalidAddrError that can tell you such details. Or you can do an interface check (with net.Error) to see if the error has a Temporary() bool method that will tell you if it's a permanent failure.\n. Are you sure that's not your terminal setup, @mattn? The protobuf package treats strings as opaque byte sequences. I don't think this has anything to do with fmt.Println or proto messages.\n. fmt.Println(person) is rendering the protobuf message in the standardised text format, which specifies using octal escapes for non-ASCII (see https://github.com/golang/protobuf/blob/master/proto/text.go#L476). This is working as intended.\n. The protobuf package matches the equivalent C++ code already.\n. Travis is reporting that something failed with this change.\n. I don't think there's much that can be done about it.\n. LGTM (after @iamqizhao's comment is addressed).\n. LGTM\n. +1 isn't a useful comment. Click the \"Subscribe\" button if you're just interested in updates.\n. In this sense, a \"server\" is something that implements a \"service\" or a \"server interface\". It's a little unfortunate that the grpc.Server type has the same name, but it's not a big deal once you get used to it.\n. I've thought about it, and I think the current state is fine. Let's close this and move onto more important issues.\n. I don't think we should be adding APIs that are fundamentally racy. And a network connection can fail at any time, so it's not just a startup problem.\nIf an RPC service needs a way to probe the liveness of a connection, it can easily add a Ping method or similar.\n. That's just as racy, and a lot of the time there's no way to tell that the connection is broken until you try to use it.\n. If this is to go ahead, there should really be a test. What about a test that defines some trivial non-standard codec (e.g. wraps the proto codec, but inserts three magic bytes before each message) that can ensure everything is plumbed correctly?\n. GitHub is a terrible place to have this kind of discussion unfortunately. I'm about to be travelling, so let me propose something to keep everything moving:\nTrim this change back to something simpler except for the mechanical plumbing: define the Codec interface in the grpc package, and change proto.Message to interface{} in all the relevant places. Leave out the Content-Type and String() changes for now. Don't change the transport package. That accounts for maybe 75% of this change, and there shouldn't be much contentious in it. We can get that submitted very quickly.\nMove the remainder (any changes to the transport package, any Content-Type selection/registration, etc.) to a separate pull request.\nDoes that sound okay?\n. Yeah, just hardcode to ProtoCodec for now. You can store it in whatever\nstruct makes sense, and just assign it to ProtoCodec{} when making that\nstruct.\n. Sure, we can talk in person then. Drop something on my calendar if you want\nto reserve a time; I'm getting booked out!\nWe can proceed with the basic plumbing change here in the meantime if you'd\nlike.\n. We generally don't take pull requests for the protobuf package. Send it to\nme internally instead (though synchronised with importing this change.)\n. It might actually be better to do the non-mechanical parts of this change\n(e.g. renaming the method) completely internally first, and then export in\none hit while I'm nearby?\n. Contexts are not for passing arbitrary bits of data from client to server. That's what the RPC message itself is for. There will probably be a mechanism for attaching bits of data to an RPC via a context (e.g. trace IDs), but it's going to be tightly controlled, and almost definitely not what you want to use.\n. LGTM.\ngolang/protobuf@655cdfa is the corresponding change.\n. LGTM\n. http://blog.golang.org/context\n. I'm not sure you understand what a context is. It carries deadlines, trace information and cancelation information. A server implementation (that is, a concrete implementation of the interfaces present in the generated code) cannot create any of that.\n. No, you misunderstand #130. I'm getting even more convinced you don't understand contexts, which are, admittedly, subtle.\n. The net/http package was created and locked down long before the context package existed. If it were being freshly designed today then a context arg could very well be added.\nNothing is imposing the use of contexts. I don't get what your problem is with the context argument being present. If you don't understand contexts and don't want to use them then just ignore that argument.\n. I'm not being condescending. You've claimed to understand contexts, but your comments imply otherwise. It's only your most recent comment that you've explained what your problem is.\nThe context is there because the server is a request handler. The context does what the context does: it carries deadlines and cancelation, as well as trace IDs and other request-scoped metadata.\nIf you have a long-running handler, yes, check ctx.Done() to determine if the request you are handling has timed out or been canceled. That's how to properly operate with contexts in general, whether in a gRPC handler or elsewhere that provides a context.\n. Yep, I'll try to make a pull request a bit later.\nYes, Andrew finished the Go implementation. It was all of ~150 lines; not hard at all.\n. FYI, @iamqizhao, I'd like to merge this or something similar early this week.\n. It's there. For non-streaming RPCs (e.g. https://github.com/grpc/grpc-go/blob/master/examples/route_guide/server/server.go#L75) it is the first arg. For streaming RPCs (e.g. https://github.com/grpc/grpc-go/blob/master/examples/route_guide/server/server.go#L86) it is available by calling the Context method on the stream value.\n. You should not expect that. You need to use the metadata mechanism.\n. Those are a bit more than headaches. They are major design issues. And there's only a limited set of context values that make sense to transit across machines transparently.\n. That number is -8404, overflowed into a uint32. It sounds like some maths went wrong somewhere along the line in the grpc-go code.\n. @iamqizhao, we also really need to figure out why the Travis CI build didn't find the fault in your original commit.\n. Not publicly, no. I can try to create a self-contained reproduction case though.\n. Okay. I'll wait to see your pull request and can test it easily enough.\n. Thanks. I haven't had time to create a reproduction from my code, so it's good that you've been able to reproduce it yourself. I can verify a fix that you come up with.\n. Yep, I can no longer reproduce the problem. Thanks!\n. I mean, instead of a WithNetwork function, have something like\ngo\nfunc WithDialer(f func(addr string) (net.Conn, error)) DialOption\nand then use that function in place of net.Dial (or as the dialer passed to tls.Dial).\n. If you're wanting to dial UDS, it's all of one line to do the dial (invoke net.DialUnix), so it's relatively little work for that use case to wrap that in a closure.\nThere's lots of other dialing requirements, like Google-internal things, or dialing on App Engine (e.g. you need to use the socket API there), or wanting to monitor/control/log outbound connection information, and so on.\n. That the code is already a bit weird in trying to handle timeouts is unfortunate, and should itself probably be fixed. I flagged that previously, but didn't have the time to pursue it.\nI wasn't suggesting getting rid of WithTimeout, only WithNetwork.\nA timeout is really the only option to the dialer that makes sense across almost every dialer, so it seems reasonable to make the signature be func WithDialer(f func(addr string, timeout time.Duration) (net.Conn, error)) DialOption.\n. My latest suggested WithDialer should be passed a timeout, which would be set from WithTimeout. So they work together fine. It's then up to the dialer to use that timeout appropriately.\ngrpc.Dial can use the tls package directly, and tell it to use a dialer, which would be a closure invoking whatever is passed in via WithDialer. The current code already does something vaguely along these lines; my proposal only slightly generalises it.\n. You don't have to use tls.DialWithDialer. It's a helper if you have a net.Dialer. You can use tls.Client and tls.Handshake over an existing net.Conn. That's only a few lines of code.\n. Sorry, yes, I didn't read your previous email. Yes, your plan seems fine.\nYou can see the App Engine dialing reference doc at\nhttps://cloud.google.com/appengine/docs/go/sockets/reference. A dialer to\nuse that would need to curry in the appengine.Context arg, but otherwise it\nis trivial to do if you can support the WithDialer option.\n. Contexts are request-scoped (see http://blog.golang.org/context). What would it mean for a server-wide context? If something is server-wide, it doesn't belong in a context.\n. Either you misread, or the blog post needs clarifying. That \"API boundaries\" reference does not suddenly let contexts escape from being request scoped. @Sajmani \n. internal.BackgroundContext is not a good counter example. It is an edge case that is somewhat unfortunate and largely dictated by how App Engine works. It is not a model for how to otherwise write good Go APIs. If I were to redo everything in the entire Go App Engine ecosystem then that function may not even exist.\n. It's deliberate. There's no need for an extra argument. A context is request-scoped, but so is the generated stream type. It doesn't make sense to talk about modifying the context for returning stream responses. gRPC isn't a general purpose proto/http2 interface, it's for RPCs specifically. There's a metadata package that is for attaching extra information.\n. looking...\n. I didn't look too closely at all the details, but it seems like a definite step in the right direction.\n(I think the colon handling may need to be changed in the future, but that can wait for later.)\n. One minor simplification that you can take or leave.\nLGTM\n. LGTM. Go ahead and submit. The golang/protobuf change is in: https://github.com/golang/protobuf/commit/e228b1a\n. By \"per-channel\" you mean per-ClientConn? That's fine with me.\nNote that http://tools.ietf.org/html/rfc7231#section-5.5.3 specifies that User-Agent should be ordered in descending order of identification specificity, so the application part should come first, and the grpc-go/1.2.3 tag should be added after that.\n. Your #255 put the User-Agent strings in the wrong order. Please fix.\n. Can you reopen this bug to track this? And poke the team to change their\nminds quickly?\n. FYI, I have something much more comprehensive coming soon (hopefully later this week). It might be worth waiting for that instead.\n. Finally got it done. I plan to tie grpc-go into https://godoc.org/golang.org/x/net/trace.\n. See https://github.com/grpc/grpc-go/pull/210.\n. This is a pretty significant change in the API. Was there discussion somewhere?\nI wonder if this will break existing code that is expecting Dial to actually establish a connection. As it is, I don't see any comment change on Dial that makes this clear.\n. It's a bit unexpected if your program is already structured to set things up first, and then serve requests by making outgoing RPCs after all that. This adds a performance hit to that first request when you otherwise had expected it to be taken care of during setup. Passing grpc.WithBlock will take care of that now, but it also means that every client library that builds on top of gRPC needs to have a mechanism to pass that through too.\nThe second aspect that makes this feel weird to me is error handling: Generally, if dialing outright fails, it is a permanent failure (e.g. malformed address), and your program can't do anything else at that point; however, an RPC failing is often not a permanent problem, and a program can keep going. Pushing the actual dial back to the first RPC means that programs need to put more thought into handling RPC errors than they would otherwise have to, which means most programs will behave poorly.\nI'm not too fussed about being consistent with C and Java on every last detail. This might be a situation where it makes sense to be consistent, but just saying \"C and Java also do this\" isn't a trump card, because we intentionally diverge from them in many places where we judge it to be better.\n@Sajmani @bcmills in case you guys want to enlighten me here.\n. This also makes grpc.Dial inconsistent with net.Dial in this regard. I must be missing the compelling reason for this change, since I'm only seeing downsides.\n. I have a different experience inside Google to Bryan and Sameer, but I'd still like to hear a concrete reason why a non-blocking dial is a better default. Maybe there's a good reason and I've simply forgotten it.\nSaving a small amount of memory doesn't seem like a big pay-off, and most uses of grpc.Dial are dialing a TCP connection so load balancing is irrelevant for them (since the multiplexing happens in the single HTTP2 connection, not over multiple TCP connections).\n. Not just yet. I'm running benchmarks now that I've got those fixed.\n. Here's a benchmark comparison between enableTracing = false and enableTracing = true:\nbenchmark                     old ns/op     new ns/op     delta\nBenchmarkClientStreamc1       78052         77161         -1.14%\nBenchmarkClientStreamc8       52947         53064         +0.22%\nBenchmarkClientStreamc64      51948         51674         -0.53%\nBenchmarkClientStreamc512     57291         56697         -1.04%\nBenchmarkClientUnaryc1        143743        150490        +4.69%\nBenchmarkClientUnaryc8        113032        118492        +4.83%\nBenchmarkClientUnaryc64       109480        114804        +4.86%\nBenchmarkClientUnaryc512      101987        107993        +5.89%\nI suspect the ~zero impact on streaming is because everything isn't hooked up, but it shows that the impact on this change is ~4-5%. That's relatively small for such a big feature, and is an even smaller impact on a program that isn't simply benchmarking the gRPC code.\nI am happy to keep the unexported bool here to make it easy to benchmark this in the future, but I'd prefer to not export it right now. The API for this package is big enough, and it'd be nice to not grow the API unless there's clear impact that requires it. It's easy to come back later and export it if someone is actually having problems due to tracing.\nPTAL.\n. Here's a comparison with 1 KB payloads. Note that I had to run with -benchtime=10s to get something even reasonably stable, though note that there's still a lot of noise. We're looking at small numbers here (~120-170 microseconds per op), and that's tiny compared to actual network speeds. You can even see that this makes the 1-concurrent version appear to get faster with tracing turned on. This is exercising the Go runtime as much as gRPC itself.\nbenchmark                     old ns/op     new ns/op     delta\nBenchmarkClientUnaryc1        175516        170908        -2.63%\nBenchmarkClientUnaryc8        125572        132750        +5.72%\nBenchmarkClientUnaryc64       135132        143311        +6.05%\nBenchmarkClientUnaryc512      158194        167593        +5.94%\nI can try benchmarking across machines later today. I suspect the delta will appear very close to zero under even vaguely realistic situations.\n. I've exported EnableTracing, defaulting to true. I think that's the right compromise. Almost all users of grpc-go will find ~7\u00b5s to be an utterly trivial amount of time for a big benefit, and the rare bit of code that both only does loopback networking and also almost no other work can set it to false easily enough.\n. If it's eventually going to default to true, we should do that now, so it'll show up in benchmarks and profiles so we stay aware of it and don't let it get out of control.\nYes, this is ready for review.\n. The golang.org/x/net/trace package docs say to look at /debug/requests on a running program. Note that only client-side tracing has been implemented so far.\n. The golang.org/x/net/trace package registers a HTTP handler. It'll look something like this: http://go-talks.appspot.com/github.com/dsymonds/talks/2015-jun-golang-syd/grpc.slide#8\nNo server is implicitly run. You need to do that yourself. I'm sure we'll document it better when its functionality is fleshed out more.\n. It's /debug/requests, but yeah, that's the concept. There's nothing explicit that you need to do to opt-in. Just listen on a port and use a browser, or use trace.Render to write the HTML somewhere else (e.g. in your own handler). Is there a particular step along the way that you don't follow? I thought we had this reasonably documented (http://godoc.org/golang.org/x/net/trace), but less experienced eyes will get stuck at unpredictable places.\n. It's very finely tuned around in-memory-only traces, and there's plans for it to start sampling them when even that isn't keeping up, so I expect it'll never be pushing the information somewhere else. But there's #240 for general downstream consumption of RPC information.\nIt's more plausible to be able to write an RPC handler for extracting this data. We have that inside Google, though it's not really used much. I don't want to rush that, though, since there's a major dependency issue to figure out first (e.g. grpc-go depends on trace, so trace can't use grpc-go), and we would also want to consider whether something like that should work cross-language.\nAs for http.DefaultServeMux, that doesn't generally get named explicitly. The trace package docs say \"It exports HTTP interfaces on /debug/requests and /debug/events.\", which implies those handlers are present if you use any of the default net/http serving things (e.g. http.ListenAndServe). The trace package itself only calls http.HandleFunc and doesn't mention http.DefaultServeMux directly. Do you think that needs more elaboration? How does it compare with, for instance, http://golang.org/pkg/expvar/ (another of my packages)? I guess http://golang.org/pkg/net/http/pprof/ is more explicit?\n. Do you have an alternate solution here? Perhaps running the benchmark pieces on different machines should require passing a flag?\n. PTAL\n. Woo. I managed to get this to identify a data race:\n```\nWARNING: DATA RACE\nWrite by goroutine 80:\n  google.golang.org/grpc/transport.(recvBufferReader).Read()\n      /Users/dsymonds/go/src/google.golang.org/grpc/transport/transport.go:149 +0x89f\n  google.golang.org/grpc/transport.(Stream).Read()\n      /Users/dsymonds/go/src/google.golang.org/grpc/transport/transport.go:281 +0x94\n  io.ReadAtLeast()\n      /Users/dsymonds/src/go/src/io/io.go:298 +0x11b\n  io.ReadFull()\n      /Users/dsymonds/src/go/src/io/io.go:316 +0x79\n  encoding/binary.Read()\n      /Users/dsymonds/src/go/src/encoding/binary/binary.go:216 +0x1758\n  google.golang.org/grpc.(parser).recvMsg()\n      /Users/dsymonds/go/src/google.golang.org/grpc/rpc_util.go:143 +0x136\n  google.golang.org/grpc.recv()\n      /Users/dsymonds/go/src/google.golang.org/grpc/rpc_util.go:184 +0x55\n  google.golang.org/grpc.(clientStream).RecvMsg()\n      /Users/dsymonds/go/src/google.golang.org/grpc/stream.go:163 +0xb8\n  google.golang.org/grpc/benchmark/grpc_testing.(*testServiceStreamingCallClient).Recv()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/grpc_testing/test.pb.go:401 +0xb0\n  google.golang.org/grpc/benchmark.DoStreamingRoundTrip()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark.go:134 +0x213\n  google.golang.org/grpc/benchmark.streamCaller()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark_test.go:109 +0x65\n  google.golang.org/grpc/benchmark.runStream.func1()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark_test.go:86 +0xce\nPrevious read by goroutine 79:\n  google.golang.org/grpc/transport.(recvBufferReader).Read()\n      /Users/dsymonds/go/src/google.golang.org/grpc/transport/transport.go:136 +0x149\n  google.golang.org/grpc/transport.(Stream).Read()\n      /Users/dsymonds/go/src/google.golang.org/grpc/transport/transport.go:281 +0x94\n  io.ReadAtLeast()\n      /Users/dsymonds/src/go/src/io/io.go:298 +0x11b\n  io.ReadFull()\n      /Users/dsymonds/src/go/src/io/io.go:316 +0x79\n  encoding/binary.Read()\n      /Users/dsymonds/src/go/src/encoding/binary/binary.go:216 +0x1758\n  google.golang.org/grpc.(parser).recvMsg()\n      /Users/dsymonds/go/src/google.golang.org/grpc/rpc_util.go:143 +0x136\n  google.golang.org/grpc.recv()\n      /Users/dsymonds/go/src/google.golang.org/grpc/rpc_util.go:184 +0x55\n  google.golang.org/grpc.(clientStream).RecvMsg()\n      /Users/dsymonds/go/src/google.golang.org/grpc/stream.go:163 +0xb8\n  google.golang.org/grpc/benchmark/grpc_testing.(*testServiceStreamingCallClient).Recv()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/grpc_testing/test.pb.go:401 +0xb0\n  google.golang.org/grpc/benchmark.DoStreamingRoundTrip()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark.go:134 +0x213\n  google.golang.org/grpc/benchmark.streamCaller()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark_test.go:109 +0x65\n  google.golang.org/grpc/benchmark.runStream.func1()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark_test.go:86 +0xce\nGoroutine 80 (running) created at:\n  google.golang.org/grpc/benchmark.runStream()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark_test.go:93 +0x579\n  google.golang.org/grpc/benchmark.BenchmarkClientStreamc8()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark_test.go:117 +0x3e\n  testing.(B).runN()\n      /Users/dsymonds/src/go/src/testing/benchmark.go:124 +0xf2\n  testing.(B).launch()\n      /Users/dsymonds/src/go/src/testing/benchmark.go:216 +0x1ba\nGoroutine 79 (running) created at:\n  google.golang.org/grpc/benchmark.runStream()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark_test.go:93 +0x579\n  google.golang.org/grpc/benchmark.BenchmarkClientStreamc8()\n      /Users/dsymonds/go/src/google.golang.org/grpc/benchmark/benchmark_test.go:117 +0x3e\n  testing.(B).runN()\n      /Users/dsymonds/src/go/src/testing/benchmark.go:124 +0xf2\n  testing.(B).launch()\n      /Users/dsymonds/src/go/src/testing/benchmark.go:216 +0x1ba\n==================\n``\n. It looks likebenchmark_test.gois callingRecv` concurrently on a single stream. I thought streams were not safe to be used in that way.\n. Yep, making a new stream (over the same client connection) for each goroutine seems to fix the data race.\nI'll send a pull request.\n. Never mind. I'll roll this into a bigger fix in this file.\n. Client connections should work as long as the destination address is permitted by the Socket API (the googleapis.com hostnames unfortunately haven't been whitelisted yet).\nServers won't work for a while on classic App Engine. They work already on Managed VMs though.\n. I don't think we should be reshuffling the credentials package right now just to trim down dependencies. The credentials package in general needs a broad rethink, and this just makes it harder to do that.\n@adg @okdave\n. http://www.grpc.io/docs/guides/auth.html, which is the official gRPC docs for authentication, holds OAuth 2.0 as a supported auth mechanism. It is a standard piece. Unless there's a compelling scenario in which this dependency is particularly harmful (beyond just unnecessary), we should not do this. It's making it harder for a standard, supported use case.\n. Please explain how the oauth dependency is actively harmful for that use case, rather than just being a (fairly lightweight) unnecessary dependency.\n. I don't care about a potential Cloud Bigtable break. It's trivial to solve that.\nI still think it's a fairly academic point and some unnecessary fuss for a very common case, but no-one else seems to agree, so I'll withdraw my objections.\nLet me know when there's a package for bigtable to import and I'll take care of that.\n. It'd be better if it were staged so that a new package happens first, then we can update anyone who cares, and only then do we drop the default import.\n. That doesn't seem wise to always disable tracing when using that benchmark package. We should make sure that most benchmarking keeps using it because it's the default. Individual benchmarks can disable it if they want, but a benchmark should try to be representative, and this pull request makes all the benchmarks less representative.\n. What's the use case for this? The sanity check is really quite important: it stops a panic upon receipt of an RPC at some arbitrary time later.\n. Subverting a safety mechanism for short term velocity is almost always the wrong tradeoff.\n. It is indeed the responsibility of the caller to ensure they have a well-formed server value, but the point of this safety check is to help them get that right up front rather than them making a mistake and having a ticking time bomb in their server. The IDL turns the up front dynamic check into a static check as an additional layer.\n. LGTM\n. LGTM with a thought\n. Sounds like you have a problem with your network.\n. All done. The EnableTracing check now only happens once per clientStream.\nPTAL\n. PTAL\n. This is okay by me after you address those comments.\n. Looks fine to me.\nOver to @iamqizhao.\n. The trace package is not the place for hooks. It is the reporting mechanism.\nI think #131 hashed out the stance on hooks/interceptors in grpc-go. We are very nervous about permitting hooks because a badly behaved hook has so much ability to drag things down. I would advocate you start with your option (a) for now.\n(If anyone's at GopherCon, I'm happy to have a sit down and chat about the options tomorrow.)\n. I'm a little tied to the Google room, but maybe we can catch up later in the afternoon?\n. Email me at dsymonds at golang.org.\n. I'm not really qualified to review this. @iamqizhao is the right person.\n. There's generally no reason to have async APIs in Go. It's trivial enough to start a new goroutine when you have need for that.\n. You're not making any sense. What problem are you observing in your Go program?\n. I don't see the benefit of you testing the grpc dispatch code, which is really all you're getting if you aren't doing a full server. The generated code has interfaces for the client and server side; I recommend you write tests against those, or write full end-to-end tests that use the complete network stack (i.e. a full grpc server listening on localhost). That's plenty quick enough, and it is actually representative of the environment your code will be running under.\n. It is still unclear what you're trying to achieve. What's wrong with using grpc.Server, listening on localhost, and having your client code dial that? It's plenty quick enough (O(150\u00b5s) roundtrip) and exercises everything fully.\nIf you're worried about ports, listen on \"localhost:0\" and let your OS pick a free port.\n. The commit description should also mention that it is prefixing all the traces with \"sent: \" or \"received: \".\n. LGTM\n. He's a Google employee. He shouldn't be signing an individual CLA for this.\n. Summon @willnorris for googlebot support.\n. You should not have. You don't own the copyright to your work done at Google; Google does. That's what the Google CLA covers.\n. I'd s/ind/i/, but LGTM either way.\n. LGTM after addressing that one conditional.\n. Why does this belong here rather than in some etcd repo?\n. I'm still not sure I understand why something etcd-specific is needed in the grpc repo. Shouldn't grpc instead provide a general API for name resolution such that any scheme can plug into it?\n. That approach seems fine; I'm just suggesting that the etcd implementation of that doesn't need to live here but should live in the etcd repo. Otherwise, are we going to collect a dozen different implementations of this here?\n. Looks like you've picked up a Vim swap file. (I think you'll want to rewrite the commits to get rid of that, not just stack another commit to delete it, since it can hold sensitive information.)\n. Simplest is to probably add a new commit, then squash the whole series. http://gitready.com/advanced/2009/02/10/squashing-commits-with-rebase.html is a simple approach.\n. what about the \"Recv\" family?\nI think you should split out the rename from the addition of trace.EventLog. The first is a lot less controversial.\n. A pull request applies to the whole series of commits that you stack on each other (unlike Gerrit, which will make a new change for each commit). You'll need to make separate branches for independent changes.\n. current state LGTM.\n. Why would one want to cancel all the RPCs on the server side?\nThis seems somewhat at odds with a Context's purpose, which is meant to be request scoped. A whole server scoped thing is broader than that.\n@mwitkow-io: What's your actual use case?\n. Well, we shouldn't design general APIs if they're only going to be used for testing.\n. For that, a client cancelling their context should definitely signal (where possible) to the server, and terminate the server-side context tree. But that should happen automatically and shouldn't require anything like this issue's feature.\n. Remember that a context and its values are meant to be request-scoped. Your RpcAuther sounds like it is server-wide, and you want to hang it off context as a convenience. That's not what context is for. Pass it around to your server objects as required, probably during initialisation. The same thing applies to the name of the server, which is also not request-scoped.\nInserting magic values into contexts to avoid a bit of boilerplate is not okay.\n. It's not, and this is a good extra reason not to have the etcd stuff in this repo at all, though at least you won't get all those packages linked into your builds.\n@iamqizhao: I think this is a good reason to ditch the naming/etcd directory.\n. It would have been nice to roll out the grpc.WithInsecure() bit first (and maybe only log if it was absent), and give people time to update their code before enforcing this.\n. I'm not keen on this at all. It makes the code significantly harder to read. I understand why one would want this, but I don't think it's worth the cost.\n. I can't say I'm a fan of the trend this is extending. Logging has been made over-complicated. Arguably grpc-go shouldn't even need to use glog. There's too many places where grpc-go logs, where it should instead report via some better channel.\n. I'm not really trying to hold up this particular pull request, just expressing my dissatisfaction. Maybe channeling some of Rob Pike too. Too many things just log random information instead of thinking harder about why one would care about a particular situation arising, and thus all this great infrastructure around logging gets built to cope with that.\n. Sorry that your network situation sucks.\nHowever, you can still fetch everything okay if you side-step go get:\ngit clone https://github.com/grpc/grpc-go $GOPATH/src/google.golang.org/grpc\ngit clone https://go.googlesource.com/net $GOPATH/src/golang.org/x/net\n. A Go client should automatically reconnect. Getting back an EOF (because the TCP connection died) should cause an immediate reconnect, I believe.\n. @Sajmani and @bcmills might be interested in reviewing this.\n. That's not true. Using an existing dependency is an easy case, but we aren't forbidding new dependencies completely; if they are justified then they're okay.\n. Ah, I see what you're getting at. Yes, that's probably okay to link to for this purpose.\n(FYI, this applies not just to new contributors; @iamqizhao would have to justify new dependencies too.)\n. It wouldn't work for methodHandler to return the request, because it doesn't return until the RPC is over, so we wouldn't be able to see the requests for active RPCs.\nThis is the only place that codec appears in the generated code. There's a certain symmetry to pulling it out.\n. Okay, I'm just about convinced that what you've got here is approximately as good as we're going to get, even though I still think it's imperfect. Let me grab some food, and I'll give it one more thought; consider this a LGTM with a 1h timeout.\n. LGTM\nIt now occurs to me that we could repurpose streamHandler/Stream for unary RPCs. We already treat unidirectional streams (in either direction) as simply a special case of a stream, so we could do the same with unary RPCs. But we can consider that later.\n. I think this is breaking through the abstraction layer between the grpc and transport packages. Tracing doesn't warrant that, IMO.\n. Nice gain for relatively little code impact.\n@iamqizhao: the go/ links on the types above this function really need public versions!\n. LGTM\nThis is much better.\n. Interesting. That message originates from rpc_util.go:209, but should only arise if (*parser).recvMsg returned something odd.\nI bet I know: it's probably an overflow. gRPC messages have a fixed size header, with a 32-bit length. I reckon your code is producing a message that breaks over that limit, and is being encoded incorrectly as a result, leading to a confused decoder.\n. Nope. That's not it. Your message is ending up ~18MB big, well short of 32 bits, and the encoder code wouldn't break on it that way either.\n. A quick binary search shows it breaks somewhere between length=16513903 (0xFBFB6F) and length=16988903 (0x1033AE7), so I'm back to the overflow suspect, but at 24-bits, which is weird.\n. Oh, I found the bug. I think it was caused by overuse of constants, which hid the problem.\n. gRPC isn't designed for arbitrary streams, only streams of messages. Your DataChunk is exactly what's supported.\n. On the other hand, quietly catching a panic might leave the server in an inconsistent or incorrect state, so while crashing sucks at least you'll find out and can go fix the panic.\nWe've come to regret the net/http panic catching after running Go in production for a while. It sounded like a good idea at the time, but it has caused its own issues in practice.\n. If it's mission critical, you'll want some sort of frontend (e.g. nginx) that can quietly retry the request if a backend crashes. There's no way to completely rule out crashes in bad code, and catching panics in only the RPC server handler goroutines isn't going to stop them all.\n. The gRPC spec seems to imply that clients may limit the size of an RPC that it'll accept back (in the Trailers production). Maybe this is a bug in the behaviour of handling that limit, rather than a bug in accepting a large error at all.\n(In other words, perhaps such large errors are simply not permitted, but the Go client is not passing that on properly.)\n. This is #334.\n. Dupe of #147.\n. Don't bother opening a new issue. In general, if you upgrade protobuf then you need to regenerate protos. I have a plan to make the failure mode if you don't more obvious, but it'll still end up the same: you'll need to regenerate protos if you update the protobuf package.\n. The only failure mode I can think of is if the .pb.go file containing the oneof wasn't regenerated, and that'd be due to commit 0879490.\n. Like I said earlier, I have a plan for how to improve the situation. It'll be something like the generated code verifying with the proto package that they are compatible (e.g. a versioning scheme).\n. If you mean the generated protocol buffer messages, you need to use github.com/golang/protobuf/jsonpb to marshal them to JSON. There's several features that won't work with the standard encoding/json package.\n. Yes, that's the specified format for JSON and protocol buffers (https://developers.google.com/protocol-buffers/docs/proto3#json). If you have an existing JSON format to match, protocol buffers may not be the best choice.\n. protoc-gen-go doesn't know what import path to use for test.proto. You need to tell it (via M parameters), or run protoc from a parent directory that is suitably relevant so it'll guess the right thing.\n. You can do it in a somewhat-hacky manner by using grpc.WithDialer to return an arbitrary net.Conn.\n. grpc.NewServer().Serve takes a net.Listener. You can write a trivial implementation of that that'll yield a net.Conn.\n. gRPC supports bidirectional streams, but isn't really for bidirectional connections. One side is meant to dial the other side. It sounds like the things I pointed you at satisfy your requirements, though. If you want extra things on top of that, you could have an RPC method that is merely a byte transport, and do a connection over that, but there'd be a lot of overhead.\n. I don't think this is right. If NewStream fails, it could be for some reason that you'd want to see in a trace. For instance, one way NewStream can fail is if the context has already expired. If someone tried to start an RPC that way, we should show the trace as failing (and for that reason), not silently avoid tracing it.\n. LGTM\n. The gRPC protocol is specified to require a length prefix, so it needs to know the serialised size before sending anything.\nBut if your protocol is merely sending chunks of []byte around, you could specify a streaming protocol, and send the []byte in smaller chunks. It's still not quite as efficient, but it avoids a complete copy.\n. There's tons of naming systems out there, and they don't all belong in the grpc-go repo. The API is such that they don't need to. What warrants this package being here, rather than in the Kubernetes repo?\n. Which places don't have contexts as their first arg?\nIt's fine for contexts to be stored in structs when they are inherently tied to the lifetime of the struct. In the case of a Stream, the context is for that stream (and created by grpc-go), so it's fine for it to be stored in that struct.\n. That one isn't so obvious. The first arg is really behaving like a curried receiver. At least it isn't an exported API, though, so I am not too bothered by it. Did you have any others?\n. Dialing a localhost address is the recommended way. It's very efficient, and exercises all parts of the connection so that things are less likely to mysteriously break when you start operating over multiple machines.\n. Seems like rpcError should be exported with an exported Err field (like os.PathError), or there should be some kind of API to \"unpack\" the returned error to get at the underlying thing. @iamqizhao is right that there should be a consistent error type, but I can definitely see the utility of finding out the exact (unwrapped) error.\n. I can't say I'm a fan of cramming stuff into a context just to signal a log level. That's very different to what context values are intended for.\n. You said \"The context could contain the requested log level, ...\". Did I misunderstand that?\n. Ah, right. I see. Almost like a request-scoped v/vmodule setting. That might be okay, though I think the majority of grpc-go's logging is currently not request-specific.\n. I think proto3 specifies that encoders should use packed=true by default, but that's moot since compliant proto parsers are meant to work with both non-packed and packed.\n. The link is correct (or could even be to grpc.io). It's a link for more information about gRPC in general, not about grpc-go specifically.\n. There's something going wrong on your end. The likely problem is that the trace package is being imported in two different ways. Or do you have it vendored? Or other code registering on /debug/requests?\n. LGTM\n. I'm not sure that would help, without making a truly massive interface for ClientConn. That type has a good number of internals that the grpc package uses (via, say, grpc.Invoke).\nWhat is the bigger task you are trying to achieve? There's probably a better approach. Can you describe your goal in a bit more detail?\n. Consistency within a language trumps consistency across languages, since the former is what most people will spend all their time with. The Go standard library long ago standardised on the single L form.\nUnlike the \"Referer\" case, which has always been a misspelling, \"canceled\" has been a long accepted spelling, even though I, as a non-USian, was raised on the \"cancelled\" spelling. http://grammarist.com/spelling/cancel/\n. LGTM\n. 1.4.3 seems too far in the past. This repo depends on x/net (and maybe some other Go subrepository), and those only officially support Go 1.5 onwards.\n. That probably needs fixing too, then.  ;-)\n@iamqizhao \n. Let's wait until grpc/grpc#4672 is resolved. It's not at all clear that percent-encoding is a foregone conclusion.\n. What broke this recently? If it's always been broken, I don't see the haste, but if something unrelated broke this then can we fix/rollback just that?\n. I don't think we need to state the \"last two major versions\" thing. That doesn't need to be a policy here, and this code won't necessarily break just because Go 1.7 comes out. We should just bump the supported version here when this package wants to use something from a newer version, or a dependency forces us to.\n. What you say is true about the method sets, but does that really matter for an unexported interface, with only unexported types implementing that interface?\n. I think it's appropriate for these comments to be as they are. 99% of users should not be calling these functions directly. And it is true that they are designed to be called by generated code.\n. I could see the argument that cert errors are not transient, though. There's no point in retrying because it's expected to fail consistently.\n. I don't understand your use case. You want to write your own implementation of *grpc.Server? You don't have to use the RegisterFooServer function in the generated .pb.go files; that only provides a type-safe wrapper.\n. Sorry, I still don't quite understand what you're trying to do. Are you wanting to make RPCs to another server? Or iterate over the servers that are registered on a grpc.Server?\nDo you have a snippet of code that shows how you'd use what you originally asked for?\n. Registering a service implementation with a server is the only way that a server is going to know what RPC services to serve. And that's most of what the registration is: sticking info into a map so the server can dispatch incoming RPCs. If I understand, you're proposing pulling that map out into its own type so it can be reused. But it's trivial; other places can have their own map easily enough.\nWhy can't you register a service implementation with multiple servers?\n. Sorry, I don't think it's worth it to extract that trivial amount of code. It will only complicate the gRPC API for a niche use case that can trivially implement the same thing itself.\n. The golang/protobuf change has been pushed: https://github.com/golang/protobuf/commit/331aba2\n. Your grpc/grpc-go and golang/protobuf packages need to be both in sync. Most likely one of them is out of date.\n. It's because your test infrastructure is using Go 1.4, but grpc-go requires Go 1.5 or later.\nLet me see if I can send you a quick fix.\n. The old unary API didn't work for the interceptor design requirements so there needed to be a breaking change. You need to have grpc/grpc-go and golang/protobuf both up to date.\n. Are you sure? The Travis build is showing everything building fine, and I just checked that go build -v google.golang.org/grpc/... works for me when everything is at HEAD.\n. I don't know why that CLA check is firing. I should be covered by the Google corporate CLA.. You can say that binding to :0 is a Go problem not a grpc problem (though I don't know how you'd then listen on all interfaces), but from the perspective of a user of grpc-go that is a distinction without a difference. The fact is that people get it wrong, and the point of having common libraries is to make it harder to get things like that wrong.\nAn in-process transport seems like a fine direction (though then you miss out on testing with the OS's network code, so your test becomes less representative), but note that my proposed code here actually makes it easier to switch over to that and people don't have to specifically use some in-process transport in their tests but rather grpc-go will handle it for them.\nI don't think there's much to maintain here; it's very similar to the Google-internal rpctest package that I wrote, and it had only about 10 notable changes in 6 years. But if you really don't want this package, that's fine, it's your call; I'll leave it in my own project and then nobody else can benefit from it.. I should be covered by the Google corporate CLA for this. I've reached out to the OSPO to find out what they need to do from their end to fix the automation here; did you want to wait for that, or verify me OOB?. I did that already, but signed in with my GitHub account. I guess that was the wrong move.\nI've done it again but with my google.com Google account. It looks like the new thing is \"Social network logins\" (wat). However, since I previously signed in via my GitHub account, I can't add that to this new account (\"This identity is registered to another user.\"). I signed back in with my GitHub account, but now can't add my google.com Google account for the same reason (\"This identity is registered to another user.\"). Their interface doesn't seem to let me delete either account, or even delete the identity for the account so I can use it on the other. I seem to be stuck.. I thought I might be able to add a second Google account (e.g. my personal gmail) to one, then delete the primary identity, but it seems the linuxfoundation interface only works with one of each kind of identity. I don't have Facebook or OpenStack accounts, and the only other options are GitHub and Google accounts, and I've used both of those. D'oh.. @jtattermusch: But I don't want to author my commits with a different email address. It should work with alternate addresses, which it does, except that I can no longer get to a state where I have multiple addresses registered with a linuxfoundation CLA account.. Anyway, the OSPO got back to me and said I could sign the individual CLA, making this moot.\n@thelinuxfoundation: Check the CLA again please.. @dfawley: CLA check is passing now.. The solution inside Google is that the stubby codegen creates an UnimplementedFooService concrete type that implements the interface with methods that return an unimplemented error. Servers that wish to avoid breaking when the Foo service is extended can embed that type in their own implementation, and new methods will thus transparently work (by returning that unimplemented error). That's effectively what the OP suggests, and it seems to work well enough in practice.. The redirector is already set up and running now.\n. Please stick to using YYYY-MM-DD date formats. This date does not make sense in many places.\n. The original code is fine here.\n. this was fine\n. this was fine\n. this was fine (and preferred)\n. this was fine\n. Line 123's err refers to the named return value err (there's no other err between the lines 98 and 123). The way named return values work is that they effectively get assigned to by explicit return statements before pending deferred code is run, so line 128 here is defining a new err, but then it explicitly returns an error value on line 130, which the compiler assigns to the err named return value before running the deferred functions.\nIt would absolutely have been buggy if naked return statements had been used (which is why we strongly discourage those), but it's fine if you always return explicit values.\nhttp://golang.org/ref/spec#Return_statements\nhttp://golang.org/ref/spec#Defer_statements\n. A bunch of the changes below are still fine, and make the code a bit more idiomatic, so I suggest reverting the changes that I've flagged, and keeping the rest.\n. This should have a doc comment saying what this binary does.\n. I'd combine these two into a single server_addr flag.\n. why not just \"tls\"?\n. both \"do\" and \"Get\" are noise words at the start of this name. Given that it doesn't even return anything, consider calling this PrintFeature instead?\n. dd\n. ditto\n. This seems needlessly indirect.\ngo\nfor {\n  msg, err := stream.Recv()\n  if err == io.EOF {\n    break\n  }\n  if err != nil {\n    log.Fatalf(...)\n  }\n  log.Println(msg)\n}\n. I think this function should make its own RNG. It's bad form for some function to start seeding the global RNG for its own use.\n. go\nvar points []*pb.Point\nfor i := 0; i < pointCount; i++ {\n  points = append(points, randomPoint())\n}\n. The &pb.RouteNote shouldn't be needed on all these lines.\n. call this waitc to make its purpose a bit more obvious.\n. this function may need to take an RNG argument.\n. Maybe this file should be in a proto/ subdir. It seems odd to have the proto as the top-level code.\n. just \"tls\"?\n. This is an invalid way to compare protos. You need to use proto.Equal.\n. This should probably be []pb.Feature. Protos should generally always be T, not T.\n. This is a dicey API. Either the RPC should return some message with an error qualifier, or this should return an error.\n. Pull out a helper function instead.\n. proto.Equal\n. protos do not make good map keys. Can you make this, say, keyed by a string representation instead?\n. you shouldn't need \\n at the end of log calls; logging is inherently line-oriented.\n. what's with THE_YELLING?\n. unwrap\n. \"lng\" is the more common abbreviation for longitude.\n. This looks like it is computing according to some specific function. Please add a comment saying what that it and linking to some reference material.\n. go\nlis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", *port))\n. I meant changing the flag name too. Passing -tls on the command-line is just as descriptive as -use_tls, except it is less typing.\nEither way, the var name here should really match the flag name, so it should be tls/\"tls\" or useTLS/\"use_tls\".\n. same with these other flags: match the var name (modulo converting to camel case) to the flag name.\n. no need for return after log.Fatalf\n. unnecessary parens\n. drop this blank line\n. drop the , 0.\n. The latest stable release of Go in travis is 1.4.x (see the build results for this pull request).\n. It might be better to use d for a time.Duration and leave t for time.Time.\n. what's this going to be if there isn't a timeout?\n. unwrap\n. no need for the time.Duration conversion.\n. mention what the received error actually was.\n. Yes, being explicit with handling the cc.dopts.timeout > 0 case seems wiser. I'd do start := time.Now() at the start of the function, and then do something like\ngo\nif d := cc.dopts.timeout; d > 0 && time.Since(start) > d {\n  // oops, gone past timeout\n}\nwhen it's needed. Then the use of cc.dopts.timeout is crystal clear and you don't have to trace its implicit effects through other variables.\n. s/which/that/\n. shouldn't this then break out of the loop?\n. combine this into the previous line.\n. would it be better to use context.DeadlineExceeded instead?\n. which -> that\n. unwrap\n. I wouldn't change dopts.Timeout. It makes the logic too confusing. start.Add(dopts.Timeout) [when the timeout is set] should always be the cutoff.\n. This doesn't seem like a good API. Perhaps move it to its own pull request to not distract from this one?\n. \"for dial a client connection\" -> \"for dialing a server\"\n. this should take *DialOptions; the struct is of non-trivial size\n. no need to copy the entire set of options. Only the timeout is relevant here. You can also collapse these two conditionals a bit.\ngo\ntimeout := cc.dopts.Timeout\nif timeout > 0 {\n  timeout -= time.Since(start)\n}\nif timeout < 0 {\n  ...\n}\n. oh, except this now wants a whole dialopts. hrm.\n. I don't understand why this is here. If the operation timed out then the next time through this loop (or a few lines below) should notice that equally well.\n. More particularly, a net.Error being a timeout does not guarantee that the full cc.dopts.Timeout has elapsed. It might be a sub-second failure due to name resolution, for instance.\n. By making these unexported, this interface can not be implemented by other packages. I don't think that's what you intended.\n. No need to put these on *protoMessage. These methods can happily be on protoMessage (just drop the ampersand inside the constructor func).\n. might be nicer to keep the flags together (before the import path spec).\n. I'm not sure this warrants its own package. It also leads to a clumsy name (codec.Codec). Can it not go in the grpc package?\n. just export this and throw away the NewProtoCodec func, which is useless.\n. Where is the name used? would protobuf be better here?\n. I'd leave this auto-selection out for now. I'm not sure there's a real demand for it.\n. oh, this is where it is used. hmm.\nIs this use of Content-Type documented somewhere?\n. yeah, I don't think this is so compelling, and auto-switching probably doesn't belong here anyway. Let's expect users of non-proto encodings to specify their codec explicitly at a higher level.\n. I just had a quick look at the transport package. What is there that needs to know about codec? As far as I can see, it only deals with []byte.\n. yes, I said \"export this\". That's what that means.  ;-)\n. The design for this feature was such that the client explicitly specifies to use a non-standard codec. It'd be nice just from the symmetry perspective to have the server be similarly explicit. I don't think it's a particularly necessary goal that a server can automatically switch between wire formats, but maybe you're thinking about that?\nIf we're going for some arbitrary string-matching scheme here, the codec package (or wherever that code lives) will probably need a registration mechanism. We can't bake in knowledge about every possible codec.\nThe use of the codec's String() result in Content-Type needs explicit documentation in the Codec interface definition.\n. You could grab the codec's name (this is the only bit that is needed in transport, right?) and stuff that in the transport.Stream struct, no?\n. make the map up here. It shouldn't hurt to always allocate the map.\n. you should be able to drop these three lines entirely.\n. you can name the arg c. There's enough mentions of Codec on this line.\n. shouldn't need the &\n. That's not how Go interfaces work. Here, protoCodec is an empty struct. It doesn't need to be a pointer, because the methods for the Codec interface are on protoCodec, not *protoCodec. If a user has a complex codec, they can implement those methods on *myCodec and pass &myCodec{...} to the relevant functions in this package.\n. This feels too specific. It doesn't help this support any other address types either. Why don't we simply accept a dialer option that takes complete responsibility for turning the address into a net.Conn?\n. // WithDialer returns a DialOption that specifies a function to use for dialing network addresses.\n. This signature is weird. We're passing in a function and its two arguments. Maybe instead of Dial this should be Handshake(net.Conn) error instead, and then the caller can do the dialing?\n. It's fine to have a goroutine even if it's not strictly necessary here. It'll make the code simpler to write\ngo\ngo func() {\n  errChannel <- conn.Handshake()\n}()\nif err := <-errChannel; err != nil {\n  rawConn.Close()\n  return nil, err\n}\nreturn conn, nil\n. in general it is not valid to call t.Fatal from a goroutine other than the one that the test started in. This should be t.Errorf so the wg.Done runs anyway.\n. good practice to do defer wg.Done() at the start of a goroutine that you're monitoring.\n. We could, but I suspect the performance impact of this code specifically will be lost in the noise. It's pretty efficient (as opposed to, say, binary logging).\nHow about we add a switch if we can get profiles that finger this as a problem?\n. Moved these bits to trace.go, but not sure what there is to unit test that isn't trivial.\n. What complexity are you thinking of? This change has 90% of the complexity it'll have for the client side, and the server side will be comparable.\nI'll take a look at benchmark_test.go.\n. The golang.org/x/net/trace package buckets and displays things by family. We could have one trace family per method, but experience shows that that is too fine grained and just ends with bucket explosion and a messier /debug/requests page.\n. Hmm. It looks like the benchmark stuff is a bit broken for me. I'll send you a pull request to fix one small thing, but that only uncovered a crash in it, which I'll file separately.\n. /debug/requests only responds to requests from localhost by default, so it's somewhat secure in that regard. Programs with different requirements (whether more or less stringent) can control that through the trace package.\nThis will pin payloads (out of necessity) for a certain length of time, but there's a bounded number that will be stored.\n. It's called lazySprintf because it's an arbitrary Sprintf call wrapper that's called lazily. It isn't stringing any RPC (it's not RPC specific at all). I could go with lazyStringer if you would be satisfied with that.\n. fmtStringer? It uses fmt to implement the Stringer interface. That seems fine.\n. it doesn't seem like you need the pretest target either.\n. This all looks like generated protobuf code. Why isn't this in a .pb.go file?\nIf it's to avoid a circular dependency, the only thing here should be the service desc. But I don't understand why so much of this is exported either. This all needs more commentary.\n. this is incorrect. This is making a copy of the traceInfo struct, but the trace has a pointer to the original, so any changes to firstLine after this will not be reflected, for instance. It'll also pin both bits of memory.\nEither clientStream needs to be allocated higher up (probably the simplest option), or it needs to have a *traceInfo field instead.\n. why is this buried all the way in here? This seems like it'll be missed in several situations (e.g. the error handling above here), and the scope of this method is not the same as the scope of the RPC.\n. yeah, this is a sure sign that the handling of Finish is wrong in here. There should be one guaranteed path to get to the completion of the trace, but you've scattered this all over the place.\n. under what situations can t.NewStream fail? Perhaps this should be higher up so you'll get a trace entry in certain failure situations?\n. so it is certain that this method won't be called again after this?\n. this needs to be after the two calls below. cs.traceInfo.tr cannot be used after Finish is called.\n. I wonder if some of the fields of cs should be set to nil here (independent of EnableTracing) to make a clearer panic if RecvMsg is called again after it returns an error. e.g. setting cs.p = nil and cs.codec = nil would probably help, since an erroneous use of a trace after Finish can be hard to diagnose.\n. I don't think the particular use of payloads for streaming should matter. Unary RPCs can be just as big. Controls over the size are a fine thing to have, but that's independent of this change.\n(There's no need for control on the number of payloads; the trace package will only hold on to 10 events per trace. The only constraint may be on a per-payload basis, which will be a little harder to do since we permit arbitrary codecs. I have ideas for how to handle that though.)\n. It holds 10 events, and drops from the middle, so it'll be something like 7-8 payloads (excluding the first line, the last line, and one that says how many events were dropped). Yes, it's definitely bounded in that regard.\nNothing bounds the payload here. It'll just stringify it. We need to improve how that's handled, but it's not fatal since an active server will only retain a fixed number of traces in total, so it'll still be bounded in some sense.\n. Yes, this would break because of that. I couldn't figure out how this code should clearly delimit when a clientStream is \"done\", since both SendMsg and RecvMsg appear to indicate the termination of the stream when things fail. \nDoes clientStream need a mutex to mediate this? What's supposed to happen if concurrent SendMsg/RecvMsg calls are running?\n. I thought this approach was better since it limited the number of places touching the global EnableTracing var.\nKeep in mind that this is a per-stream lock, not a global lock, so it only is contended when the stream itself is busy. That's not often the case (since streams usually require a specific ordering), so the mutex overhead will normally be unnoticed. I can run the benchmarks if you'd like me to verify this.\n. An uncontended mutex lock/unlock is a compare-and-swap (for the lock) and an atomic add (for the unlock), which together is around 16ns on my (getting old) workstation. It's definitely not \"hundreds of nanoseconds\". It's only a little slower than reading a global var (EnableTracing), which may, in fact, be slower if there's lots of threads (a mutex per clientStream is thus effectively sharding that).\nI suggest we keep this how this change has it. It keeps the code a bit simpler, it's an internal only thing, and we can switch it out when higher speed transports arrive and 16ns is enough to show up on a benchmark.\n. Okay, I care about avoiding hitting the global bool each time more than saving a few bytes, so I'll add a bool to clientStream. Stand by...\n. revert this. Please try to minimise gratuitous changes.\n. Done.\n. fix year\n. use correct caps; only the first \"oauth\" is lowercase (since that's what the package name is); use \"gRPC\" and \"OAuth\" later on.\n. this seems like a good a time as any to drop this. it's trivial to replace this wherever it is used.\n. TokenSource needs to be exported. There are places that need it (and that don't use google.ComputeTokenSource). It is NewComputeEngine that is the trivial wrapper.\n. @iamqizhao: You're missing my point. There is a need for credentials.TokenSource where it will not be used with google.ComputeTokenSource, so we can't unexport it. We can keep NewComputeEngine around if you really want; I'm just saying we can't unexport TokenSource.\n. Yes, other code bases. That's why I exported TokenSource in the first place (in f2936c4).\n. please use \"recv: %v\" instead. it'll line up better with \"sent: %v\".\n. better yet, put a little bool in the payload struct and render the prefix lazily too.\n. This is the wrong way around. The higher level User-Agent string should be coming first. I pointed you at the RFC for that.\n. there's no need for these to be separate vars. they are tiny. inline it in the LazyLog calls.\n. duplicate nil check\n. // whether this is an outgoing payload\n. This is now incorrect. The application level can be setting multiple agent strings. Instead of splitting on space and taking the first element, this should look for the last space (strings.LastIndex), and take everything before that.\n. Check traceInfo.tr != nil instead. Keep only one place here consulting the EnableTracing global.\n. something below should be marking the trace as an error if an error occurs.\n. does this really need an extra arg? is it possible to reconstitute the string from MethodDesc, or some other way?\n. format this a bit more like the corresponding clientStream fields.\n. That's fine, as long as the return statements below are not naked (as this commit also changes). The defer above will still work.\n. &&\n. is this going to catch io.EOF?\n. drop this comment. It has typos, and the code below is self-evident.\n. merge this with the next defer\n. remove this\n. if ss.tracing is true, this should always be true.\nOr is there a way that SendMsg can get called after the stream is finished?\n. turn the previous line into an else if with this.\n. if err != nil && err != io.EOF {\n. \"ClientConn\"\n. why not for all user-agent values?\n. this change seems like it belongs elsewhere.\n. Uh, hopefully we're not going to immediately start requiring Go 1.5 for gRPC. It's polite to wait for a little while after a release before assuming everyone has upgraded.\n. Oh, yeah, that phrasing is fine.\n. This seems weird, and deserving of more comments. The doc comment on this interface says this is common across all supported credentials, but as far as I know this \"audience\" concept is OAuth-specific.\n. You should configure your editor to gofmt-on-save!\n. This isn't a great name in the context. grpc.Desc doesn't say much (whereas at least the above grpc.Code is referring to the more specific \"code\" concept).\nPerhaps ErrorDesc would be clearer?\n. This isn't sensitive info. This can be ss.traceInfo.tr.LazyPrintf(\"OK\"). I wonder if some of your other uses of stringer are unnecessary too.\n. Please put this in trace.go, next to fmtStringer. Or consider using that type directly instead.\n. this grouping is confusing. at a glance it looks like this is only set when tracing, but that's not the case. I'd put this field higher up in this struct.\n. LazyPrintf does exactly what LazyLog+fmtStringer does, except it doesn't have a control for sensitivity. There is indeed a struct overhead, though I suspect it would be negligible. Not too fussed.\n. This seems an odd place to introduce this, no? Shouldn't whoever is creating that context be canceling it? Anything that gets to the transport.Stream will be getting a context differing from this one.\n. This seems like it should be addressed in its own change.\n. ditto\n. s/Golang/Go/\n. I don't understand why this needs to change. The knowledge of mapping []byte to some value is exactly the purpose of codec.\n. This feels like we have a layering issue to sort out. This doesn't seem like the right way to solve that.\n. I think one part of the issue is that we want to unmarshal the request, and log that before going anywhere near the method handler. But this code doesn't know how to unmarshal the request because it doesn't know how to construct the destination type for passing to codec.Unmarshal.\nThe streaming equivalent (stream.go:RecvMsg) works by taking the message to be populated. The generated stream handler calls RecvMsg, though that works because a stream starts and then you deal with the incoming message, whereas a unary RPC couples those two steps. It works for streams because there's a value specific to the stream (the Service_MethodServer interface embeds a grpc.ServerStream that the handler can use).\nMaybe there's no way around it. It still feels wrong to have a decodeFunc, which explicitly here does more than a \"decode\". It's a hack to get the message traced between unmarshaling and running the server's unary RPC handler. I don't have a better solution right now, but it might be good to think about this a bit before we make a change.\n. This now has to hope that the stream is being traced, splitting the decision making into different packages. I'm not sure this was a good move.\n. This is now pushing RPC-specific knowledge into the transport package that was otherwise fairly agnostic on the matter.\n. Yeah, this just doesn't have the same meaning in this context now.\n. I don't think you need the extra uint8 cast in there.\n. It's usually a red flag for sync primitives to be passed around in APIs. I think it's a mistake for sync.WaitGroup args to be here and in operateHeaders. In general, it is a better idea for the caller to pass in a closure over the wait group, rather than squeeze it in here explicitly, particularly since there's no clear rules stated here about what's meant to be done with it.\n. If the concern is overflow on 32-bit machines, use int64 here.\n. I guess we're already covered on 32-bit machines, since you can't have a too-big []byte there. So the only thing to worry about is 64-bit machines. We should probably have a test for too-long messages.  ;-)\nI don't think we need to care about supporting Go 1.3. That's ancient at this point. Go 1.4 is the only baseline, and it's what the grpc-go README says too.\n. I'm not sure a Stream or ClientStream is intended to be safe for concurrent use (except for maybe getting its Context).\n. I'd say \"This requires Go 1.5 or later.\"; it's a little clearer than the \"+\".\n. The benefit of an isFoo method in an interface like this returning a bool is to permit wrapper types to implement this method correctly.\n. I don't know. I didn't write this code originally. I'm just saying that this could be a reason why the bool is here.\nYou didn't mention this change in the commit message. Making unnecessary or unrelated changes at the same time as a real change makes it harder to review. Please try to avoid that.\n. It's fine to pass around testing.T in limited circumstances. That's why t.Helper exists, after all. It solves the problems that passing around a testing.T produces.\nIt may be easy to start and stop a server for people like us who know what's going on, but lots of people aren't so comfortable. I see a lot of people getting it wrong in various ways (e.g. binding to :0 instead of a loopback address, or starting the server before registering service handlers). It's not a lot of code, but it cuts down the error rate a lot.. ",
    "iamqizhao": "LGTM. Feel free to merge it. \nI am going to port it into google3/third_party early this week. Hopefully, you can make the codegen of streaming rpc happen also. Then we can start to migrate the current users and remove net/grpc/go. Thank you very much for the help! \n. done. Thanks.\nOn Sun, Feb 8, 2015 at 5:55 PM, David Symonds notifications@github.com\nwrote:\n\nI don't have write access to this repo. Can you merge it please?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/29#issuecomment-73447137.\n. converted to an internal cl.\n\nOn Tue, Feb 17, 2015 at 11:11 AM, Brad Fitzpatrick <notifications@github.com\n\nwrote:\nI don't have access to see the commit, so I can't review.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/40#issuecomment-74730796.\n. We are making examples and docs.\n\nFor now,\ntest/end2end_test.go\ninterop/client/client.go\ninterop/server/server.go\nare your friends if you want some examples.\nOn Fri, Feb 20, 2015 at 12:03 AM, Michael Stapelberg \nnotifications@github.com wrote:\n\nAh, after having a closer look it seems like you can just not specify that\nfield, since it\u2019s variadic :).\nIn any case, an example would make that clearer.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/51#issuecomment-75202506.\n. more example can be found at grpc/grpc-go/examples and grpc/grpc-common/go now. \n. the change in clientconn.go has been merged in the pull request 54. Thanks.\n. It would be good to have a unit test in transport_test.go to verify your change.\n. LGTM. Thanks for your contribution!\n. @dsymonds \n\nHi David,\nCould you please help review this pull request also to make sure it is Go-styleish? Thanks!\n. Per @tbetbetbe 's request ([grpc-common] Add route guide documentation and sample in each language (#51)), this example should stay in grpc/common.\n. @dsymonds \nWe need to submit this before alpha release (02-26-2015 9:00AM PST). Let's know if you have any further comments and we can make post mortem fix for them. Thanks.\n. On Thu, Feb 26, 2015 at 3:23 PM, David Symonds notifications@github.com\nwrote:\n\nThere's my final round of comments.\nAlso, it looks like the .pb.go file was not checked in. Please do that; we\nshouldn't force people to have to go install protoc and protoc-gen-go just\nto build this example.\nWe kinda like to make this as a real-life example to get readers hands\ndirty and be prepared to use gRPC lib.  We deleted the generated code of\nthis example for all the languages (except  a small number of exceptions --\ntheir codegen are faulty).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/58#issuecomment-76297593.\n. Yup, refork and create a new PR please.\n\nOn Sun, Mar 1, 2015 at 5:16 PM, Daniel Wang notifications@github.com\nwrote:\n\nGitHub is not picking up my latest commits because my fork stopped working\nas the upstream went public and I recreated it. Should I start a new PR\ninstead?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/58#issuecomment-76645689.\n. To be clear, \n\ngithub.com/golang/protobuf/proto is needed because we have imported \"github.com/golang/protobuf/proto\" for encoding and decoding. But it has been installed automatically when you run \"go get google.golang.org/grpc\".\ngithub.com/golang/protobuf/proto-gen-go is not required to use grpc package. It is needed when you want to generate code from proto file. \n. fixed in PR #71 \n. No, it is called in https://github.com/grpc/grpc-go/blob/master/transport/http2_client.go#L113 when some TransportAuthenticator is set in authOpts.\n. Close now. Please reopen if you still think there is a problem here. Thanks.\n. On Thu, Feb 26, 2015 at 2:22 PM, Tv notifications@github.com wrote:\n\nOkay here's the better diagnosis: the client lib assumes addresses always\nhave a port. It calls net.SplitHostPort and fails unless the address was\nRight now the client lib does not assume addresses always have a port. It\nREQUIRES!\nhost:port style. There's several calls, removing the error path from this\nmakes the greeter example work over unix domain sockets, with a custom Dial:\nhttps://github.com/grpc/grpc-go/blob/master/call.go#L118\nUnix domain socket is not supported now. And I plan to add it in the next a\ncouple of weeks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/72#issuecomment-76287444.\n. make #73 to track UDS separately.\n. This is not sufficient because it does not work in the case when there is no TransportAuthenticator (i.e., plain TCP). Plus, how does the client lib differentiate a malformed host:port address (e.g. a user passed an IP addr and forgot setting the port) and real unix socket domain name if we allow users pass an arbitrary string. My idea is to make things explicit. For example, if users want to use UDS, they need to specify the protocol explicitly when creating the transport. This approach is also scaling when we want to support more transport protocols.\n. Agree, those kind of transports (e.g., QUIC, RDMA, etc.) need the native support from the underlying library (e.g., Go net package).\n. closed via #169 \n. Yup, I think adding some additional handlers for non-grpc content is the way to go just like what we have done for google internal rpc framework. I will raise it to design discussion and get back to you once we have a decision.\n. I think this is good to have. But I do not see a compelling demand to have\nit now given a lot of other stuffs I am working on. So it is on my radar\nbut a low priority item. I will be more than happy to take a PR from you\nguys.\n\nOn Wed, Apr 22, 2015 at 10:53 AM, Xiang Li notifications@github.com wrote:\n\n@iamqizhao https://github.com/iamqizhao Have you had the discussion\nyet? Any decision? Anything I can help to make this happen?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-95282883.\n. On Wed, Apr 22, 2015 at 11:16 AM, Xiang Li notifications@github.com wrote:\n@iamqizhao https://github.com/iamqizhao I can work on it.\nMotivation:\n1. currently etcd and a few other my side projects have a http1\n   endpoint as it API.\n2. I want to support GRPC in the next few releases without adding a\n   new port or break compatibility.\nSo ideally, grpc can forward http1 requests to my old handlers. But can\nyou give me some direction to do that? I can start to explore and hopefully\nhave a pr for it.\ngrpc-go uses bradfitz/http2 package to write & read http2 frames. I suspect\nthe framer.ReadFrame will error out if it receives a http1 request (need\nlook into the code or check with brad). If it is the case, it is not\ntrivial to implement this feature without some change to http2 package...\n\nYou can probably check https://github.com/gengo/grpc-gateway to see if\nthere is alternative to your problem.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-95289439.\n. On Wed, Apr 22, 2015 at 4:04 PM, Xiang Li notifications@github.com wrote:\n@iamqizhao https://github.com/iamqizhao\nIt seems @bradfitz https://github.com/bradfitz's http2 server supports\nboth http1 and http2 at the same time. Here is the related code\nhttps://github.com/bradfitz/http2/blob/91f80303028022bc2034c277126b405a2257d990/server.go#L185-L192\nYep, the idea is to push the divergence stage (http1 or http2) before there\nis any real traffic. It uses the usage NPN/ALPN to decide whether\nhttp.Server creates a http2 ServerConn and dispatches the traffic there.\nhttp1 traffic can never enter http2 package. If we put grpc into this\npicture, there are grpc, http2 and http1 handlers. Then the usage of\nNPN/ALPN is not enough  to make decision which server conn we should setup\n(apparently no idea between http2.ServerConn and grpc.ServerTransport; and\nactually I am not convinced using NPN/ALPN is sufficient for http1 and\ngrpc). To summarize, to design a working solution, we have to find\nsomething which can be used to see the upcoming traffic on the connection\nis http1/http2/grpc. NPN/ALPN seems not the choice to me.\nAre we able to do the similar thing?\nCan\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-95361388.\n. okay, my proposal is sketched as follows:\n\nwe can implement a handshaker. The client connects to a http.Server and\nthen sends a http1.1 GET with some magic string such as \"gRpC-sWiTcH\". The\ncorresponding handler will pass that fd to a grpc server (and response to\nthe client via handshaker too). After that, all the traffic on that fd is\ntaken care of by the grpc server. Otherwise, the normal http handler will\nbe used.\nOn Wed, Apr 22, 2015 at 4:33 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nOn Wed, Apr 22, 2015 at 4:04 PM, Xiang Li notifications@github.com\nwrote:\n\n@iamqizhao https://github.com/iamqizhao\nIt seems @bradfitz https://github.com/bradfitz's http2 server supports\nboth http1 and http2 at the same time. Here is the related code\nhttps://github.com/bradfitz/http2/blob/91f80303028022bc2034c277126b405a2257d990/server.go#L185-L192\nYep, the idea is to push the divergence stage (http1 or http2) before\nthere is any real traffic. It uses the usage NPN/ALPN to decide whether\nhttp.Server creates a http2 ServerConn and dispatches the traffic there.\nhttp1 traffic can never enter http2 package. If we put grpc into this\npicture, there are grpc, http2 and http1 handlers. Then the usage of\nNPN/ALPN is not enough  to make decision which server conn we should setup\n(apparently no idea between http2.ServerConn and grpc.ServerTransport; and\nactually I am not convinced using NPN/ALPN is sufficient for http1 and\ngrpc). To summarize, to design a working solution, we have to find\nsomething which can be used to see the upcoming traffic on the connection\nis http1/http2/grpc. NPN/ALPN seems not the choice to me.\nAre we able to do the similar thing?\nCan\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-95361388.\n. On Wed, Apr 22, 2015 at 5:50 PM, Brian Akins notifications@github.com\nwrote:\n\nWhy not use HTTP Upgrade?\nTo me, it means a more complex solution -- http upgrade can help us\ndifferentiate http1 and http2. Then for http2, after http2.Framer.ReadFrame\nreturns a frame, we can use the content-type header to differentiate grpc\nand normal http2. This constructs a 2-layer solution -- on connection\nlevel, we use http upgrade and on per-stream level, we use content-type.\nThis sounds too complex to me -- keep in mind that we also need to\nimplement a hook for grpc server to handle normal http2 traffic in this\nsolution.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-95377569.\n. On Mon, May 18, 2015 at 10:08 AM, Xiang Li notifications@github.com wrote:\n@iamqizhao https://github.com/iamqizhao @philips\nhttps://github.com/philips\nI tend to agree this\nhttps://groups.google.com/forum/#!topic/grpc-io/JnjCYGPMUms. (grpc-java\nplan)\nCan we do something similar in grpc-go?\nAs I mentioned before, this will involve some redesign of http2 package. In\ngeneral, I suspect whether it is worth doing a lot of redesign and\nrewriting to accommodate http1 traffic and I strongly lean to serve http2\nand http1 traffic on different ports.\nThanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-103134571.\n. I think with PR#196 and my previous proposal in this thread it is already\npossible to serve both http1.1 and grpc traffic on a single port. You need\nto write your own credentials.TransportAuthenticator to do some handshake\nbetween client and server. And in ServerHandshake, you can pass rawConn to\nyour own http1.1 server so that all the traffic on rawConn will be\ndelivered to http1.1 server.\n\nOn Mon, May 18, 2015 at 12:44 PM, Michael Stapelberg \nnotifications@github.com wrote:\n\n+1 to what @philips https://github.com/philips said. I\u2019m exactly in the\nsame boat with http://robustirc.net/, where I\u2019d be happy to switch to\ngRPC, but not if it involves a separate port.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-103188267.\n. On Thu, May 28, 2015 at 3:54 PM, Xiang Li notifications@github.com wrote:\n@iamqizhao https://github.com/iamqizhao\nI have tried the way you suggested, which implements a HTTP 1.1 based hand\nshaking.\nhttps://github.com/xiang90/grpchttp/blob/master/server/server.go#L46-L62\nBut this also requires EVERY client want to talk to the gRPC sever\nimplementing the handshake mechanism. It basically means my gRPC server\nbecomes a special one.\nI do not understand why you care other servers.\nOne of the important point of using gRPC is for its generalization. Users\ncan get easily get working gRPC clients in different language very easily.\nI looked at the code, and agreed that if we want to get better support we\nmight need to redesign the gRPC server (http2 server?) a little bit. But I\nthink it worth the effort since it will largely improve the adoption of\ngRPC for existing applications.\nI personally do not like this approach because it adds some overhead (path\ndispatch) to every single rpc regardless whether you use http1 or not.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106621711.\n. On Thu, May 28, 2015 at 4:35 PM, Xiang Li notifications@github.com wrote:\nI do not understand why you care other servers.\nI care about other clients. Every client that wants to talk to my server\nneeds to implement the handshaking, since my server is a special one. I do\nnot want make my server a special gRPC server.\nI assume your clients use your client library and your client library\nshould implement the handshaker. So this is transparent to your clients.\nNo?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106629066.\n. On Thu, May 28, 2015 at 4:36 PM, Xiang Li notifications@github.com wrote:\nI personally do not like this approach because it adds some overhead (path\ndispatch) to every single rpc regardless whether you use http1 or not.\nPath dispatching only happens for dialing, correctly? If we can make this\nlow cost, do you want to have a try?\nI think I get lost. What is your proposal to change http2 package? How can\nyou make it happen only for dialing?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106629478.\n. On Thu, May 28, 2015 at 4:51 PM, Xiang Li notifications@github.com wrote:\nI assume your clients use your client library and your client library\nshould implement the handshaker. So this is transparent to your clients.\nNo?\nNot necessarily. For example, in etcd we will maintain the go client (go\ngrpc client and the upper level client on top of it).\nThe community will maintain the clients in other languages. They need to\nimplement the handshaker for etcd special gRPC server. And I am not sure it\nis easy to do in all available gRPC client bindings. It would be a overhead.\nGot it. But again, it is not an grpc-go specific question and this is not\nthe right place to discuss and have solution. The topic should go to\ngrpc-io. In Go world, the handshaker is a mechanism to achieve that (if\nboth your client and server are written in gRPC-Go) if you need it now.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106631071.\n. On Thu, May 28, 2015 at 4:59 PM, Xiang Li notifications@github.com wrote:\nI think I get lost. What is your proposal to change http2 package? How can\nyou make it happen only for dialing?\nI briefly looked through the spec (I may misunderstand the spec). The path\nis / Service-Name / {method name}. We can assume gRPC-go owns all the\nregistered server-name path. When dialing, we can pass a service name\npath and we will be able to distinguish it from other general http request.\nHTTP handler can then pass the conn to gRPC-go. This means the gRPC-go\nsever will not control the whole life-cycle of a connection.\nConnection and service are two completely orthogonal concepts. A connection\ncan deliver N different services and a service can be delivered on N\ndifferent connections. When clients dial, there is 0 knowledge about what\nservices will run on this connection.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106632285.\n. On Thu, May 28, 2015 at 5:02 PM, Xiang Li notifications@github.com wrote:\nGot it. But again, it is not an grpc-go specific question and this is not\nthe right place to discuss and have solution. The topic should go to\ngrpc-io. In Go world, the handshaker is a mechanism to achieve that (if\nboth your client and server are written in gRPC-Go) if you need it now.\nI totally agree with you. That is why I do not think your suggestion is a\nvery generic solution.\nActually it is a quite generic solution. And we use it internally for some\nother kind of switching.\nI did ask the same question in grpc-io group. Java is using path\ndispatching. And they suggest me to explore the same thing in grpc-go. I\nthink it might be doable and reasonable.(Again I am not super familiar with\ngRPC or gRPC-go, if you think it is reasonable I would love to explore more)\nI am not aware any grpc server can serve http1 traffic for now. There would\nbe some miscommunication somewhere. At very least, your understanding about\npath dispatching seems not correct according to your previous reply.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106633346.\n. On Thu, May 28, 2015 at 5:06 PM, Xiang Li notifications@github.com wrote:\nConnection and service are two completely orthogonal concepts.\nI understand that.\nWhen clients dial, there is 0 knowledge about what\nservices will run on this connection.\nWhy do we care? We just want to distinguish it from non-grpc connections.\nWe can simply pass an arbitrary registered service name path when creating\nthe initial gRPC-go http2 connection.\nI got your points. But it is actually a kinda handshaking mechanism which\nis pretty similar to my former proposal (unidirectional handshake where\nserver does not send response). Your client needs to send this name to the\nserver and you run into the same issue. All of your client impl need to\nknow what is to send first after a conn is setup. Also if your server is\nexpecting this name, it is a \"special\" one according to your previous def.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106635082.\n. On Thu, May 28, 2015 at 5:13 PM, Xiang Li notifications@github.com wrote:\nAt very least, your understanding about\npath dispatching seems not correct according to your previous reply.\nWhich part? Can you give me some explanation?\nI think there are two ways:\n1. use the http server do path dispatching for every gRPC call\nagain, it adds per-rpc overhead.\n1. use the http server do path dispatching for the first one and use\n   this\n   https://github.com/grpc/grpc-go/blob/master/transport/transport.go#L172\n   to dispatch further calls inside grpc-go as it is today.\nagain, does this require special treatment for each impl of your client --\n\"special\".\n\nIn other words, if all grpc versions implements handshaker, do u still\nthink your server is \"special\"?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106638593.\n. On Thu, May 28, 2015 at 5:22 PM, Xiang Li notifications@github.com wrote:\nYour client needs to send this name to the\nserver and you run into the same issue. All of your client impl need to\nknow what is to send first after a conn is setup. Also if your server is\nexpecting this name, it is a \"special\" one according to your previous def.\nThere is a difference. If the connection is set up without the path, we\ncan delay transferring the ownership of the connection until the first\nstream is created. And I think we can safely assume if the first stream has\ngRPC prefix, all following streams within the connection will have.\nIt is a horrible idea to mess up the boundary of connection setup and rpc\nperforming -- it will add non-trivial and unneccessary complexity to http2\nserver. It should be a unidirectional handshaking before any rpcs/http\nrequests issued on this connection -- so now you can see the similarity to\nmy proposal.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-106639538.\n. Thanks for the contribution. I have not looked into it but it seems you sniffs each and every header frame.  This could have potential performance impact (especially for high qps scenarios). Please evaluate it.\n. got it. gRPC sends/receives http2 settings frame before any headers frame. Does this introduce problems? It seems the server should know whether it is gRPC or http before the 1st headers frame.\n. okay, so cmux buffers the frames until it sees HEADERS. Actually, this is pretty much what  @xiang90 proposed to me during a visit. The design basically looks good to me.\n. I have not gone through cmux impl to see if it resolves all the problems. I\nactually plan to have a built-in solution inside gRPC library instead of\nrelying in cmux. But I have not got time to iron all the wrinkles. I plan\nto take some time to fix this after I finish naming and load balance work.\n\nOn Wed, Sep 16, 2015 at 5:53 PM, Brandon Philips notifications@github.com\nwrote:\n\nWhere are we on this? Is the consensus that we put up a PR that documents\nusing cmux?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/75#issuecomment-140935295.\n. We will escalate the priority of this issue. I am going to talk to the dev of other languages to have a common solution (probably after thanksgiving) which may be the one proposed in this issue.\n. yup, it is on my radar. Plan to add a dialOption to enforce a limit.\n. or have a context.Context for transport so that you can set deadline/timeout or issue cancellation.\n. So you proposed adding another API grpc.DialTimeout? sgtm.\n\nRegarding connection-scoped context, it was originally from bcmills on a previous code review. I actually think it is probably a good idea (but maybe overkill). For OS primitive, it is not special for connection-scoped context. It is the issue for request-scoped context too (e.g., a socket write stuck in kernel when the corresponding context is canceled/timeout).\n. On Thu, Feb 26, 2015 at 5:20 PM, David Symonds notifications@github.com\nwrote:\n\nNo, I suggest grpc.Dial should have a ...grpc.DialOption variadic\nargument, and have grpc.DialTimeout to return such an option.\nThis is exactly what the current impl is (\nhttps://github.com/grpc/grpc-go/blob/master/clientconn.go#L83) and my\noriginal proposal then ...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/76#issuecomment-76313226.\n. On Thu, Feb 26, 2015 at 5:53 PM, David Symonds notifications@github.com\nwrote:\nYour original proposal said \"a configurable limit on retries\", which I\ninterpreted to mean a limit on the number of retries, which I think is a\nbad option. A limit on the total time trying to dial would be fine though.\nI see. Sorry for confusion. Actually I was debating which one (timeout vs #\nretries) is better at that time.\nEven better would be to (also) detect the types of errors, and for the\nknown-permanent failures (e.g. malformed address) have grpc.Dial fail\nimmediately.\nYup. Is it possible to get some hint from the error returned by net.Dial\n(e.g., DNS resolving failure)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/76#issuecomment-76316619.\n. On Thu, Feb 26, 2015 at 6:39 PM, David Symonds notifications@github.com\nwrote:\nThe net package (http://golang.org/pkg/net/) has a bunch of specific\nerror types (e.g. AddrError or InvalidAddrError that can tell you such\ndetails. Or you can do an interface check (with net.Error) to see if the\nerror has a Temporary() bool method that will tell you if it's a\npermanent failure.\nNice, this is the thing I missed. Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/76#issuecomment-76320829.\n. fixed in #93 \n. If you change \n\nfmt.Println(person)\nto\nfmt.Println(person.Name)\nit works fine. This means fmt.Println does not work well with proto message. gRPC is just fine. I am going to to talk to goprotobuf team to get help.\n. Hi David,\nSimply on my desktop, I got\nperson := &pb.Person{\nName: \"\u307b\u3052\",\nAge:  int32(1),\n}\nfmt.Println(person)\noutput:\nname:\"\\343\\201\\273\\343\\201\\222\" age:1\nfmt.Println(person.Name)\noutput:\n\u307b\u3052\nOn Fri, Feb 27, 2015 at 2:30 PM, David Symonds notifications@github.com\nwrote:\n\nAre you sure that's not your terminal setup, @mattn\nhttps://github.com/mattn? The protobuf package treats strings as opaque\nbyte sequences. I don't think this has anything to do with fmt.Println or\nproto messages.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/84#issuecomment-76484922.\n. On Fri, Feb 27, 2015 at 9:40 AM, Ruben Vermeersch notifications@github.com\nwrote:\nHow does grpc handle concurrency? Can I use a client from different\nthreads in parallel?\nOn client, if you want to perform multiple rpc in parallel, you should\nspawn multiple goroutines to do that since the rpc is synchronous/blocking.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/85.\n. On Fri, Feb 27, 2015 at 11:27 AM, Jacques Fuentes notifications@github.com\nwrote:\nsince the rpc is synchronous/blocking\nThat's what I got from looking at the route_guide example: the server's\nend points access a slice without locks. Is this considered quasi similar\nto an actor model? Is there documentation about this fact (I might have\nmissed it)?\nTo be clear, I was talking about client side. On the server side, each rpc\nstill has its own goroutine but the concurrency is model is different --\nthe goroutine is spawned by grpc internals instead of applications.\n\nWere you talking about \"savedFeatures\" slice? There is no lock for that\nbecause it is read-only once it is initialized (in grpc.NewServer).\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/85#issuecomment-76455866.\n. Hi Matt,\n\nThanks for making this happen. Can you move the benchmark to a separate test file such as microbenchmark_test.go?\n. I would also suggest moving it to test/ dir. One positive side effect is that you can reuse the proto (and its generated code) there.\n. Hi Matt,\nI just realized you only benchmarked encode function instead of the\nend-to-end thing (sorry, I only glanced the file names and did not look\ninto your code last night.). If that is the case and you think it has some\nvalue to do, your original choice is better here. Actually, I did not plan\nto do this kind of microbenchmark. But it seems it is good to have if you\nalready started working on it. Thanks.\nOn Mon, Mar 2, 2015 at 10:08 AM, Matt T. Proud notifications@github.com\nwrote:\n\nI have gone ahead and renamed it. As for moving it, I am curious for your\nsuggestions:\nWe are benchmarking an unexported, internal function encode at the\nmoment, which belongs to the top-level package. There will be more that we\nbenchmark, but this is just the beginning.\nThe only way we can benchmark it while moving it to another package\nappears to be \u2026\n1. Exporting the function (highly undesirable due to public API\n   cluttering).\n2. Moving it to an internal package http://golang.org/s/go14internal,\n   provided that meets the Golang versioning compatibility requirements for\n   grpc and is actually implemented in upsteam Go (a quick search at\n   stdlib in 1.4.1 answers: no).\nWhat are your thoughts?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/87#issuecomment-76765389.\n. On Mon, Mar 2, 2015 at 12:03 PM, Matt T. Proud notifications@github.com\nwrote:\nYeah, there is value in instrumenting these smaller units to get a better\napproximation of the whole. I started withencode, because of your TODO in\nthe function itself. More broadly, I have a few ideas about end-to-end\nbenchmarking. I just wanted to start with the simpler cases and expand from\nthere.\nsgtm. then either rename microbenchmark_test.go to\nunit_microbenchmark_test.go or put it back into rpc_util_test.go as you\noriginally did. Thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/87#issuecomment-76802166.\n. Probably rename the new proto to codec_perf and put it under test/codec_perf correspondingly. We could enhance it to benchmark various codec related unit benchmark. The current name is a bit misleading in test/ dir.\n. LGTM. Thanks for contributing.\n. On Sun, Mar 1, 2015 at 10:25 PM, Wes notifications@github.com wrote:\nBleh, I have no idea why that is. The tests all pass on my local machine,\nand seem to pass on travis even though I haven't changed anything\nsubstantial. Perhaps it's flaky?\nI do not think it is flaky (tested with a few thousand runs). I have not\nlooked into your change but it seems your second commit fixed the issue.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/88#issuecomment-76664175.\n. Ping this thread when you make the new commit to address all comments. Thanks for contributing!\n. This PR does not fully resolve dial timeout issue if net.Dial stuck. I have a TODO at https://github.com/grpc/grpc-go/blob/9e1539115ed0457553c8e84f9b6e53222ecedcef/transport/http2_client.go#L105. The timeout value should be plumbed in and use net.DialTimeout instead.\n. On Tue, Mar 3, 2015 at 1:32 AM, Wes notifications@github.com wrote:\nOkay, fair point. I've updated the code to pipe timeouts down into\nnewHTTP2Client and extended the TransportAuthenticator interface to include\na DialTimeout function.\nThanks, will take a look.\nI also did some rejiggering of ConnectionError to include the error it's\nwrapping, since this seems to be its overwhelming use. I think it's\nidiomatic Go--os.PathError takes a similar approach--and it seemed cleaner\nthan shoehorning Temporary() and Timeout() into CE.\nI've also reintroduced the !Temporary() check, since we can get our hands\non the net.Error now. Tomorrow I'll take a look at properly testing that\nfunctionality.\nhmm, actually I have different opinion on this reconnecting behavior.\nAccording to what I read, it seems except some time-out error, all the\nother errors in net package has Temporary() to return false. Thus if we use\n!Temporary() check, we almost always disable reconnecting, which is quite\ndebatable -- e.g., if user starts the server later than the client, the\nclient needs to restart. Actually, in google internal version, we reconnect\nregardless what error we got unless the user explicitly disables reconnect\nfeature. I tend to follow the same strategy here. Can you remove this part\nfrom this PR? I will take care of that.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/88#issuecomment-76913524.\n. Any reason you deleted your previous timeout change like \"if d := cc.dopts.timeout; d > 0 && time.Since(start) > d\"? It is still required because net.Dial is not necessarily hanging. Both are needed to handle different scenarios. It seems the PR becomes a bit messy and Pull request in github really makes miscommunication happen more often. Do you mind I take this over and you can help review? Thanks for the help.\n. On Tue, Mar 3, 2015 at 3:53 PM, Wes notifications@github.com wrote:\nIt didn't seem necessary anymore, since piping the timeout through to\nnet.Dial was sufficient for the case I had in mind (server not listening).\nWere you thinking of bounding the total amount of time Dial could be called\nduring an attempt to reset the transport?\nYes, I think bounding the total grpc.Dial time is the right thing to do.\nIf you think it'd be easier to take over, that's okay with me. I can also\nsquash the commits if that would make things look prettier.\nYeah, let me take it over because it involves some messy plumbing work I\ncan do faster. Thanks for the effort!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/88#issuecomment-77065715.\n. Please take a look at https://github.com/grpc/grpc-go/pull/93. Thanks.\n\nOn Tue, Mar 3, 2015 at 4:34 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nOn Tue, Mar 3, 2015 at 3:53 PM, Wes notifications@github.com wrote:\n\nIt didn't seem necessary anymore, since piping the timeout through to\nnet.Dial was sufficient for the case I had in mind (server not listening).\nWere you thinking of bounding the total amount of time Dial could be called\nduring an attempt to reset the transport?\nYes, I think bounding the total grpc.Dial time is the right thing to do.\nIf you think it'd be easier to take over, that's okay with me. I can also\nsquash the commits if that would make things look prettier.\nYeah, let me take it over because it involves some messy plumbing work I\ncan do faster. Thanks for the effort!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/88#issuecomment-77065715.\n. Thanks for benchmarking this. Per my reply to #108, this is because we have not done any batching IO so far, which is the focus of performance optimization in the next a couple of months. \n\n\nIt would be highly appreciated if you can have a pull request to wrap up what you have done to kick off the benchmark work on github.\n. okay, I am going to try to push out a basic benchmark framework this week and then all the contributors can experiment various performance optimization ideas on the same ground.\n. I made some improvement on client (not checked in yet) and got the significant improvement already:\nhttp client:\n2015/03/11 15:39:40 1: 5736.642904 op/sec @ p99=0.336000ms\n2015/03/11 15:39:50 2: 11015.068698 op/sec @ p99=0.321000ms\n2015/03/11 15:40:00 3: 17261.079718 op/sec @ p99=0.424000ms\n2015/03/11 15:40:10 4: 20806.615173 op/sec @ p99=0.674000ms\n2015/03/11 15:40:20 5: 917.775790 op/sec @ p99=7.762000ms\nimproved grpc client:\n2015/03/11 15:28:20 1: 3474.533592 op/sec @ p99=0.453000ms\n2015/03/11 15:28:30 2: 8111.900305 op/sec @ p99=0.657000ms\n2015/03/11 15:28:40 3: 14023.490286 op/sec @ p99=0.663000ms\n2015/03/11 15:28:50 4: 21179.714876 op/sec @ p99=0.617000ms\n2015/03/11 15:29:00 5: 27564.232005 op/sec @ p99=0.612000ms\n2015/03/11 15:29:10 6: 33875.344691 op/sec @ p99=0.603000ms\n2015/03/11 15:29:20 7: 39543.547520 op/sec @ p99=0.616000ms\n2015/03/11 15:29:30 8: 43567.770711 op/sec @ p99=0.668000ms\n2015/03/11 15:29:40 9: 46393.655755 op/sec @ p99=0.711000ms\n2015/03/11 15:29:50 10: 47250.902113 op/sec @ p99=0.796000ms\n2015/03/11 15:30:00 11: 47733.268011 op/sec @ p99=0.961000ms\n2015/03/11 15:30:10 12: 47531.488503 op/sec @ p99=1.130000ms\n2015/03/11 15:30:20 13: 50293.363756 op/sec @ p99=0.938000ms\n2015/03/11 15:30:30 14: 50614.134504 op/sec @ p99=1.006000ms\n2015/03/11 15:30:40 15: 50902.922158 op/sec @ p99=1.061000ms\n2015/03/11 15:30:50 16: 51342.060561 op/sec @ p99=1.103000ms\n2015/03/11 15:31:00 17: 51172.659114 op/sec @ p99=1.200000ms\n2015/03/11 15:31:10 18: 51340.048872 op/sec @ p99=1.230000ms\n2015/03/11 15:31:20 19: 51575.161160 op/sec @ p99=1.277000ms\n2015/03/11 15:31:30 20: 51478.429739 op/sec @ p99=1.376000ms\n2015/03/11 15:31:40 21: 49980.940379 op/sec @ p99=1.738000ms\n2015/03/11 15:31:50 22: 51071.717198 op/sec @ p99=1.681000ms\n2015/03/11 15:32:00 23: 52140.282287 op/sec @ p99=1.553000ms\n2015/03/11 15:32:10 24: 52124.439668 op/sec @ p99=1.593000ms\n2015/03/11 15:32:20 25: 52222.698417 op/sec @ p99=1.646000ms\n2015/03/11 15:32:30 26: 52365.708092 op/sec @ p99=1.795000ms\n2015/03/11 15:32:40 27: 52629.151019 op/sec @ p99=1.771000ms\n2015/03/11 15:32:50 28: 52815.300753 op/sec @ p99=1.857000ms\n2015/03/11 15:33:00 29: 53304.403154 op/sec @ p99=1.839000ms\n2015/03/11 15:33:10 30: 53081.078234 op/sec @ p99=1.986000ms\n2015/03/11 15:33:20 31: 53571.344040 op/sec @ p99=1.945000ms\n2015/03/11 15:33:30 32: 53348.888919 op/sec @ p99=2.008000ms\n2015/03/11 15:33:40 33: 53429.124904 op/sec @ p99=2.147000ms\n2015/03/11 15:33:50 34: 53687.364050 op/sec @ p99=2.196000ms\n2015/03/11 15:34:00 35: 54029.536526 op/sec @ p99=2.184000ms\n2015/03/11 15:34:10 36: 53837.964570 op/sec @ p99=2.264000ms\n2015/03/11 15:34:20 37: 53558.114539 op/sec @ p99=2.358000ms\n2015/03/11 15:34:31 38: 54276.048533 op/sec @ p99=2.395000ms\n2015/03/11 15:34:41 39: 54714.463152 op/sec @ p99=2.424000ms\n2015/03/11 15:34:51 40: 54343.651778 op/sec @ p99=2.501000ms\n2015/03/11 15:35:01 41: 54334.642340 op/sec @ p99=2.557000ms\n2015/03/11 15:35:11 42: 52279.954783 op/sec @ p99=3.224000ms\n2015/03/11 15:35:21 43: 54623.572538 op/sec @ p99=2.759000ms\n2015/03/11 15:35:31 44: 55033.028392 op/sec @ p99=2.729000ms\n2015/03/11 15:35:41 45: 55097.426098 op/sec @ p99=2.797000ms\n2015/03/11 15:35:51 46: 54736.133552 op/sec @ p99=2.973000ms\n2015/03/11 15:36:01 47: 54809.898913 op/sec @ p99=2.926000ms\n2015/03/11 15:36:11 48: 55465.691515 op/sec @ p99=3.009000ms\n2015/03/11 15:36:21 49: 55142.363729 op/sec @ p99=3.053000ms\n2015/03/11 15:36:31 50: 54523.644725 op/sec @ p99=3.341000ms\n2015/03/11 15:36:41 51: 51818.090431 op/sec @ p99=3.874000ms\n2015/03/11 15:36:51 52: 55958.880606 op/sec @ p99=3.230000ms\n2015/03/11 15:37:02 53: 55770.858429 op/sec @ p99=3.363000ms\n2015/03/11 15:37:12 54: 55643.909388 op/sec @ p99=3.520000ms\n2015/03/11 15:37:22 55: 53500.566081 op/sec @ p99=3.894000ms\n2015/03/11 15:37:32 56: 55789.631428 op/sec @ p99=3.722000ms\n2015/03/11 15:37:42 57: 56027.047875 op/sec @ p99=3.556000ms\n2015/03/11 15:37:52 58: 56038.120343 op/sec @ p99=3.638000ms\n2015/03/11 15:38:02 59: 56292.927614 op/sec @ p99=3.793000ms\n2015/03/11 15:38:12 60: 55018.049146 op/sec @ p99=4.073000ms\n2015/03/11 15:38:22 61: 55992.929617 op/sec @ p99=4.024000ms\n2015/03/11 15:38:32 62: 55822.072635 op/sec @ p99=4.004000ms\n2015/03/11 15:38:42 63: 50557.821204 op/sec @ p99=5.170000ms\nI am going to \ni) investigate the p99 latency increase when concurrency is 1, 2, 3;\nii) improve server side IO.\n. fixed via #97 \n. This is good catch. I planned to fix this before releasing but forgot to do\nthat. will send out a PR to fix this.\nOn Tue, Mar 3, 2015 at 2:08 PM, Tv notifications@github.com wrote:\n\n$ cd $GOPATH/src/github.com/grpc/grpc-common/go\n$ git diffdiff --git i/go/greeter_server/main.go w/go/greeter_server/main.goindex c7fa06a..c7682cb 100644--- i/go/greeter_server/main.go+++ w/go/greeter_server/main.go@@ -40,6 +40,7 @@ import (        pb \"github.com/grpc/grpc-common/go/helloworld\"        \"golang.org/x/net/context\"        \"google.golang.org/grpc\"+       \"google.golang.org/grpc/codes\" )\n const (@@ -51,7 +52,7 @@ type server struct{}\n // SayHello implements helloworld.GreeterServer func (s _server) SayHello(ctx context.Context, in *pb.HelloRequest) (_pb.HelloReply, error) {-       return &pb.HelloReply{Message: \"Hello \" + in.Name}, nil+       return nil, grpc.Errorf(codes.DataLoss, \"too grumpy to greet\") }\n func main() {\n$ go run greeter_server/main.go &[1] 12775\n$ go run greeter_client/main.go 2015/03/03 13:53:30 could not greet: rpc error: code = 2 desc = \"rpc error: code = 15 desc = \\\"too grumpy to greet\\\"\"exit status 1\nI expected to see code=15, now I see code=2 wrapping a code=15.\nMy reading of the source (\nhttps://github.com/grpc/grpc-go/blob/master/server.go#L246 ,\nhttps://github.com/grpc/grpc-go/blob/master/rpc_util.go#L227 ) says\nconvertCode should have if err, ok := err.(rpcError); ok { return\nerr.code }.\nThat still leaves the textual double-wrapping; it seems that should not be\ndone on the server-side for rpcError, just send rpcError.desc if the\nerror is a grpcError.\nAlso, could there be a little less textual wrapping? And could those codes\nget a go stringer on them? Here's what might be ideal:\ncould not greet: rpc error: DataLoss: too grumpy to greet\nand only start doing quoting if the message is \"hostile\", as in contains\n\\n etc. That would require defining a safe set of runes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/92.\n. okay,  this is actually a misuse. We do not expect users to set error code in their service handler implementation. They should simply return nil, fmt.Errorf(\"too grumpy to greet\"). We should have had an example for this.\n\nBut I probably should handle this case as well to obey whatever error code the server application sets if it does. Let me think ...\n. On Wed, Mar 4, 2015 at 6:41 PM, Tv notifications@github.com wrote:\n\nIf what you're saying is going to hold, then it sounds like grpc.Errorf\nshould not be exported.\nHowever, consider the implications: if Codes are only supposed to be\ncreated by convertCode/toRPCErr, that means the only way to return a\ncodes.AlreadyExists from something that is not an OS-level file system\noperation or such is to use platform-dependent erorrs like syscall.EEXIST\n(attempt to make os.IsExist(err)==true). At that point, it seems like the\nCode mechanism is unusable, and people would be better off with enum error\ntypes in their response messages. And in that case, why are there so many\nCodes?\nAnd yes, more clarity into intents of the mechanism is most welcome.\nYup, I will have a pull request to fix this shortly.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/92#issuecomment-77295991.\n. All comments are replied/addressed. Thanks.\n. Seems there is still a loose end to tighten. Will ping this back once it is done.\n. all addressed. Thanks.\n. All done. Thanks.\n. All done. Thanks.\n. This blocks my other changes. Submit it now. Any post-submission comments are welcome.\n. closed via #146 \n. fixed in #100 \n. LGTM, waiting for Travis result before merging ...\n\nOn Wed, Mar 4, 2015 at 7:47 PM, David Symonds notifications@github.com\nwrote:\n\nLGTM\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/100#issuecomment-77301580.\n. On Wed, Mar 4, 2015 at 7:57 PM, Andrew Gerrand notifications@github.com\nwrote:\nTravis reports this failure:\n../../../google.golang.org/grpc/credentials/credentials.go:206: undefined: oauth2.Context\nBut this CL specifically removes oauth2.Context from that line in that\nfile.\nLooking at the build log, it looks like the script that fetches and tests\nthe code does not apply the pull request patch between running go get and go\ntest.\nThis sounds very serious bug ...\n\nShould I go ahead merging your PR now and leave this issue later?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/100#issuecomment-77302335.\n. because I restarted it ...\n\nOn Wed, Mar 4, 2015 at 8:10 PM, Andrew Gerrand notifications@github.com\nwrote:\n\nNow I can't find the build failure log from travis?! It just disappeared.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/100#issuecomment-77303385.\n. now it tries to fetch the pull request which was merged and fails ...\n\nOn Wed, Mar 4, 2015 at 8:12 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nbecause I restarted it ...\nOn Wed, Mar 4, 2015 at 8:10 PM, Andrew Gerrand notifications@github.com\nwrote:\n\nNow I can't find the build failure log from travis?! It just disappeared.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/100#issuecomment-77303385.\n. compression will be off by default.\n\n\nOn Wed, Aug 26, 2015 at 8:54 AM, Paulo Pires notifications@github.com\nwrote:\n\nWill it be enabled by default? I vote for not doing it by default.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/101#issuecomment-135077780.\n. Thanks. I think bradfitz/http2 has handled this but I need to verify and see whether grpc handles it appropriately.\n. On Fri, Mar 6, 2015 at 11:55 PM, prazzt notifications@github.com wrote:\nI'm trying to figure out how to authenticate requests in simple password\nbased/session token case.\nShould I :\n- create my own proto, embedding token in each message ? or\n- do it in grpc: implementing my own credential.Credentials ? how to\n  check server side ?\nI would suggest using the metadata to transmit the passwd/token. Please\ntake a look at\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L353 for\nhow a client sets metadata for an RPC and\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L92 for\nhow a server gets the metadata.\n-\nWould appreciate if there's some basic examples.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/106.\n. credentials.tlsCreds is one example. You can look at\ni) the simple HOWTO https://github.com/grpc/grpc-go/blob/master/grpc-auth-support.md;\nii) examples/route_guide for an example (both client and server).\n. On Sat, Mar 7, 2015 at 12:02 AM, prazzt notifications@github.com wrote:\nMy first impression was \"TLS client certificate authentication\", i.e.\ndistinguish each clients by certificate that they sent. But from cursory\nlook, turns out it's actually certificate pinning.. making sure client\ntalks with pinned server CA.\nAm I right, or does grpc actually supports client certificate\nauthentication ?\nCurrently, we do not support client certificate authentication\n(i.e., NoClientCert is used). But it is not hard to add if it is needed.\nThe name basically means creating a TLS grpc credential for client from a\ncert (ca).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/107.\n. how about NewClientTLSFromCA?\n. I do not think we do extra work besides the normal TLS handshake.\n\nThis is more like a browser->web service type of usage -- clients do not have their own certs but root CA.\n. Nah, it is not TLS always. We will support SSH too. And we are working on some Google internal transport security protocol too. Therefore, you need to have TLS in the names. In addition, I prefer \"XXXFromFile\" to \"XXXFile\". Plus, it is not necessary a local file (e.g., it could be at NFS.).\n. yup, this is a known issue and one of a few major things we would like to improve in the ongoing performance benchmark and optimization. I had some discussion with @bradfitz  a couple of months back on this already -- we could either i) use bufio as you mentioned or ii) add a flush API into http2 framer and instrument buffering inside http2 package if i) still introduces some unnecessary data copies. \nIt is welcome to contribute on this. But please do it incrementally because I expect it would be somewhat convoluted and error-prone. \n. Thanks. Per my reply #89, I am going to try to have a basic benchmark framework ready this week so that you can use it to show the improvement from your pull request. :)\n. I made some improvement on client (not checked in yet) and got the significant improvement already:\nhttp client:\n2015/03/11 15:39:40 1: 5736.642904 op/sec @ p99=0.336000ms\n2015/03/11 15:39:50 2: 11015.068698 op/sec @ p99=0.321000ms\n2015/03/11 15:40:00 3: 17261.079718 op/sec @ p99=0.424000ms\n2015/03/11 15:40:10 4: 20806.615173 op/sec @ p99=0.674000ms\n2015/03/11 15:40:20 5: 917.775790 op/sec @ p99=7.762000ms\nimproved grpc client:\n2015/03/11 15:28:20 1: 3474.533592 op/sec @ p99=0.453000ms\n2015/03/11 15:28:30 2: 8111.900305 op/sec @ p99=0.657000ms\n2015/03/11 15:28:40 3: 14023.490286 op/sec @ p99=0.663000ms\n2015/03/11 15:28:50 4: 21179.714876 op/sec @ p99=0.617000ms\n2015/03/11 15:29:00 5: 27564.232005 op/sec @ p99=0.612000ms\n2015/03/11 15:29:10 6: 33875.344691 op/sec @ p99=0.603000ms\n2015/03/11 15:29:20 7: 39543.547520 op/sec @ p99=0.616000ms\n2015/03/11 15:29:30 8: 43567.770711 op/sec @ p99=0.668000ms\n2015/03/11 15:29:40 9: 46393.655755 op/sec @ p99=0.711000ms\n2015/03/11 15:29:50 10: 47250.902113 op/sec @ p99=0.796000ms\n2015/03/11 15:30:00 11: 47733.268011 op/sec @ p99=0.961000ms\n2015/03/11 15:30:10 12: 47531.488503 op/sec @ p99=1.130000ms\n2015/03/11 15:30:20 13: 50293.363756 op/sec @ p99=0.938000ms\n2015/03/11 15:30:30 14: 50614.134504 op/sec @ p99=1.006000ms\n2015/03/11 15:30:40 15: 50902.922158 op/sec @ p99=1.061000ms\n2015/03/11 15:30:50 16: 51342.060561 op/sec @ p99=1.103000ms\n2015/03/11 15:31:00 17: 51172.659114 op/sec @ p99=1.200000ms\n2015/03/11 15:31:10 18: 51340.048872 op/sec @ p99=1.230000ms\n2015/03/11 15:31:20 19: 51575.161160 op/sec @ p99=1.277000ms\n2015/03/11 15:31:30 20: 51478.429739 op/sec @ p99=1.376000ms\n2015/03/11 15:31:40 21: 49980.940379 op/sec @ p99=1.738000ms\n2015/03/11 15:31:50 22: 51071.717198 op/sec @ p99=1.681000ms\n2015/03/11 15:32:00 23: 52140.282287 op/sec @ p99=1.553000ms\n2015/03/11 15:32:10 24: 52124.439668 op/sec @ p99=1.593000ms\n2015/03/11 15:32:20 25: 52222.698417 op/sec @ p99=1.646000ms\n2015/03/11 15:32:30 26: 52365.708092 op/sec @ p99=1.795000ms\n2015/03/11 15:32:40 27: 52629.151019 op/sec @ p99=1.771000ms\n2015/03/11 15:32:50 28: 52815.300753 op/sec @ p99=1.857000ms\n2015/03/11 15:33:00 29: 53304.403154 op/sec @ p99=1.839000ms\n2015/03/11 15:33:10 30: 53081.078234 op/sec @ p99=1.986000ms\n2015/03/11 15:33:20 31: 53571.344040 op/sec @ p99=1.945000ms\n2015/03/11 15:33:30 32: 53348.888919 op/sec @ p99=2.008000ms\n2015/03/11 15:33:40 33: 53429.124904 op/sec @ p99=2.147000ms\n2015/03/11 15:33:50 34: 53687.364050 op/sec @ p99=2.196000ms\n2015/03/11 15:34:00 35: 54029.536526 op/sec @ p99=2.184000ms\n2015/03/11 15:34:10 36: 53837.964570 op/sec @ p99=2.264000ms\n2015/03/11 15:34:20 37: 53558.114539 op/sec @ p99=2.358000ms\n2015/03/11 15:34:31 38: 54276.048533 op/sec @ p99=2.395000ms\n2015/03/11 15:34:41 39: 54714.463152 op/sec @ p99=2.424000ms\n2015/03/11 15:34:51 40: 54343.651778 op/sec @ p99=2.501000ms\n2015/03/11 15:35:01 41: 54334.642340 op/sec @ p99=2.557000ms\n2015/03/11 15:35:11 42: 52279.954783 op/sec @ p99=3.224000ms\n2015/03/11 15:35:21 43: 54623.572538 op/sec @ p99=2.759000ms\n2015/03/11 15:35:31 44: 55033.028392 op/sec @ p99=2.729000ms\n2015/03/11 15:35:41 45: 55097.426098 op/sec @ p99=2.797000ms\n2015/03/11 15:35:51 46: 54736.133552 op/sec @ p99=2.973000ms\n2015/03/11 15:36:01 47: 54809.898913 op/sec @ p99=2.926000ms\n2015/03/11 15:36:11 48: 55465.691515 op/sec @ p99=3.009000ms\n2015/03/11 15:36:21 49: 55142.363729 op/sec @ p99=3.053000ms\n2015/03/11 15:36:31 50: 54523.644725 op/sec @ p99=3.341000ms\n2015/03/11 15:36:41 51: 51818.090431 op/sec @ p99=3.874000ms\n2015/03/11 15:36:51 52: 55958.880606 op/sec @ p99=3.230000ms\n2015/03/11 15:37:02 53: 55770.858429 op/sec @ p99=3.363000ms\n2015/03/11 15:37:12 54: 55643.909388 op/sec @ p99=3.520000ms\n2015/03/11 15:37:22 55: 53500.566081 op/sec @ p99=3.894000ms\n2015/03/11 15:37:32 56: 55789.631428 op/sec @ p99=3.722000ms\n2015/03/11 15:37:42 57: 56027.047875 op/sec @ p99=3.556000ms\n2015/03/11 15:37:52 58: 56038.120343 op/sec @ p99=3.638000ms\n2015/03/11 15:38:02 59: 56292.927614 op/sec @ p99=3.793000ms\n2015/03/11 15:38:12 60: 55018.049146 op/sec @ p99=4.073000ms\n2015/03/11 15:38:22 61: 55992.929617 op/sec @ p99=4.024000ms\n2015/03/11 15:38:32 62: 55822.072635 op/sec @ p99=4.004000ms\n2015/03/11 15:38:42 63: 50557.821204 op/sec @ p99=5.170000ms\nI am going to \ni) investigate the p99 latency increase when concurrency is 1, 2, 3;\nii) improve server side IO.\n. BTW, GOMAXPROCS = 8, running on my desktop (Intel 6-Core Xeon CPU, 32GB Quad-Channel).\n. and Go 1.4.1, ubuntu (with google customized kernel).\n. Made another tiny change to client. now we have the peak throughput 63301 QPS and it also beats http when concurrency is 1. 2 and 3 are left for further investigation (probably due to benchmark warm-up issue).\n2015/03/11 16:18:18 1: 8287.741866 op/sec @ p99=0.259000ms\n2015/03/11 16:18:28 2: 8740.703689 op/sec @ p99=0.460000ms\n2015/03/11 16:18:38 3: 13895.774170 op/sec @ p99=0.664000ms\n2015/03/11 16:18:49 4: 20081.196002 op/sec @ p99=0.653000ms\n2015/03/11 16:18:59 5: 26879.486075 op/sec @ p99=0.626000ms\n2015/03/11 16:19:09 6: 32999.525010 op/sec @ p99=0.627000ms\n2015/03/11 16:19:19 7: 39121.779852 op/sec @ p99=0.621000ms\n2015/03/11 16:19:29 8: 45110.061786 op/sec @ p99=0.627000ms\n2015/03/11 16:19:39 9: 49552.546522 op/sec @ p99=0.657000ms\n2015/03/11 16:19:49 10: 51632.747219 op/sec @ p99=0.711000ms\n2015/03/11 16:19:59 11: 52957.872081 op/sec @ p99=0.780000ms\n2015/03/11 16:20:09 12: 54336.444169 op/sec @ p99=0.829000ms\n2015/03/11 16:20:19 13: 55156.122747 op/sec @ p99=0.864000ms\n2015/03/11 16:20:29 14: 55970.795801 op/sec @ p99=0.913000ms\n2015/03/11 16:20:39 15: 56392.698372 op/sec @ p99=0.958000ms\n2015/03/11 16:20:49 16: 56639.934694 op/sec @ p99=1.006000ms\n2015/03/11 16:20:59 17: 57212.360536 op/sec @ p99=1.052000ms\n2015/03/11 16:21:09 18: 56732.254707 op/sec @ p99=1.102000ms\n2015/03/11 16:21:19 19: 57302.967797 op/sec @ p99=1.149000ms\n2015/03/11 16:21:29 20: 57268.604606 op/sec @ p99=1.189000ms\n2015/03/11 16:21:39 21: 56108.716471 op/sec @ p99=1.436000ms\n2015/03/11 16:21:49 22: 57034.746842 op/sec @ p99=1.389000ms\n2015/03/11 16:21:59 23: 57868.233547 op/sec @ p99=1.332000ms\n2015/03/11 16:22:09 24: 56552.212083 op/sec @ p99=1.563000ms\n2015/03/11 16:22:19 25: 57846.229820 op/sec @ p99=1.507000ms\n2015/03/11 16:22:29 26: 56325.985601 op/sec @ p99=1.684000ms\n2015/03/11 16:22:39 27: 57354.014531 op/sec @ p99=1.692000ms\n2015/03/11 16:22:49 28: 58479.597553 op/sec @ p99=1.570000ms\n2015/03/11 16:22:59 29: 57459.811004 op/sec @ p99=1.774000ms\n2015/03/11 16:23:09 30: 57723.544929 op/sec @ p99=1.831000ms\n2015/03/11 16:23:19 31: 58054.148338 op/sec @ p99=1.802000ms\n2015/03/11 16:23:29 32: 58245.871014 op/sec @ p99=1.904000ms\n2015/03/11 16:23:39 33: 57989.680830 op/sec @ p99=1.999000ms\n2015/03/11 16:23:49 34: 58629.714685 op/sec @ p99=1.986000ms\n2015/03/11 16:23:59 35: 58676.437061 op/sec @ p99=2.052000ms\n2015/03/11 16:24:09 36: 58030.467134 op/sec @ p99=2.235000ms\n2015/03/11 16:24:19 37: 58978.829010 op/sec @ p99=2.166000ms\n2015/03/11 16:24:29 38: 58860.499125 op/sec @ p99=2.233000ms\n2015/03/11 16:24:39 39: 59112.461256 op/sec @ p99=2.242000ms\n2015/03/11 16:24:49 40: 58102.794015 op/sec @ p99=2.554000ms\n2015/03/11 16:24:59 41: 58533.033816 op/sec @ p99=2.582000ms\n2015/03/11 16:25:10 42: 57432.107985 op/sec @ p99=2.877000ms\n2015/03/11 16:25:20 43: 57441.592555 op/sec @ p99=2.862000ms\n2015/03/11 16:25:30 44: 59182.285817 op/sec @ p99=2.679000ms\n2015/03/11 16:25:40 45: 60218.583441 op/sec @ p99=2.514000ms\n2015/03/11 16:25:50 46: 59339.214638 op/sec @ p99=2.635000ms\n2015/03/11 16:26:00 47: 59887.236005 op/sec @ p99=2.724000ms\n2015/03/11 16:26:10 48: 60498.420696 op/sec @ p99=2.759000ms\n2015/03/11 16:26:20 49: 59776.344567 op/sec @ p99=2.876000ms\n2015/03/11 16:26:30 50: 58319.649181 op/sec @ p99=3.157000ms\n2015/03/11 16:26:40 51: 59763.456368 op/sec @ p99=3.000000ms\n2015/03/11 16:26:50 52: 59949.755663 op/sec @ p99=2.936000ms\n2015/03/11 16:27:00 53: 60523.324912 op/sec @ p99=3.022000ms\n2015/03/11 16:27:10 54: 61237.016441 op/sec @ p99=2.900000ms\n2015/03/11 16:27:20 55: 60443.541833 op/sec @ p99=3.110000ms\n2015/03/11 16:27:31 56: 61355.069920 op/sec @ p99=3.002000ms\n2015/03/11 16:27:41 57: 61220.384786 op/sec @ p99=3.256000ms\n2015/03/11 16:27:51 58: 61390.398581 op/sec @ p99=3.121000ms\n2015/03/11 16:28:01 59: 61282.281079 op/sec @ p99=3.324000ms\n2015/03/11 16:28:11 60: 61879.932352 op/sec @ p99=3.189000ms\n2015/03/11 16:28:21 61: 62044.597061 op/sec @ p99=3.139000ms\n2015/03/11 16:28:31 62: 61517.119104 op/sec @ p99=3.518000ms\n2015/03/11 16:28:41 63: 62252.693242 op/sec @ p99=3.356000ms\n2015/03/11 16:28:51 64: 62464.251294 op/sec @ p99=3.363000ms\n2015/03/11 16:29:01 65: 61825.208182 op/sec @ p99=3.604000ms\n2015/03/11 16:29:11 66: 58922.860966 op/sec @ p99=4.134000ms\n2015/03/11 16:29:22 67: 61939.896746 op/sec @ p99=3.701000ms\n2015/03/11 16:29:32 68: 61960.670654 op/sec @ p99=3.775000ms\n2015/03/11 16:29:42 69: 61829.558540 op/sec @ p99=3.928000ms\n2015/03/11 16:29:52 70: 62892.344351 op/sec @ p99=3.711000ms\n2015/03/11 16:30:02 71: 61878.997245 op/sec @ p99=3.989000ms\n2015/03/11 16:30:12 72: 62742.805674 op/sec @ p99=3.801000ms\n2015/03/11 16:30:22 73: 63161.662421 op/sec @ p99=3.925000ms\n2015/03/11 16:30:32 74: 62578.722089 op/sec @ p99=4.087000ms\n2015/03/11 16:30:42 75: 62649.500442 op/sec @ p99=4.234000ms\n2015/03/11 16:30:53 76: 62845.936641 op/sec @ p99=3.981000ms\n2015/03/11 16:31:03 77: 62615.585104 op/sec @ p99=4.138000ms\n2015/03/11 16:31:13 78: 62959.456672 op/sec @ p99=4.205000ms\n2015/03/11 16:31:23 79: 63169.017659 op/sec @ p99=4.314000ms\n2015/03/11 16:31:33 80: 63301.385304 op/sec @ p99=4.133000ms\n2015/03/11 16:31:43 81: 63278.517512 op/sec @ p99=4.333000ms\n2015/03/11 16:31:53 82: 62771.565187 op/sec @ p99=4.552000ms\n2015/03/11 16:32:03 83: 62730.621970 op/sec @ p99=4.504000ms\n2015/03/11 16:32:13 84: 61752.486115 op/sec @ p99=4.856000ms\n2015/03/11 16:32:24 85: 61467.460666 op/sec @ p99=4.877000ms\n2015/03/11 16:32:34 86: 61068.068302 op/sec @ p99=5.045000ms\n. When I started to quantify the improvement of my changes, I ran your load test on my desktop. I cannot reproduce your results when GOMAXPROCS is 8 (or something > 1). When it is 8 (for both client and server), the existing grpc without any performance tuning can reach 50K QPS (> 70K with my changes) and the peak QPS for client http client is 20K QPS. I understand you used 2 EC2 but I used my desktop. But this still should not happen. I suspect you did not get GOMAXPROCS set to 8 in your load test because I also ran the load test with GOMAXPROCS = 1 and the results seemed match what you saw.\n. I enabled io batching in #123. Close this one and will track and tackle the remaining perf related issues in #89.\n. We did not construct a new exported type for RPC error because we were\ndiscussing whether we should open source the google internal status type\nwhich has been used by all rpc frameworks and a lot of others inside google\nand rpcErr was treated as a short-to-medium term solution. But that\ndiscussion went silent for a while... I will see how I can rpcErr itself\nbetter probably sometime next week and try to revive the previous\ndiscussion. Thanks for all the valuable suggestions here!\nOn Sat, Mar 28, 2015 at 11:59 AM, yinhm notifications@github.com wrote:\n\n+1\nI want to return something like \"404 page not found\" to the client, It\nwould be easier if export RPCError.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/110#issuecomment-87284170.\n. How about adding a Desc(err error) string to return the original error\ndescription?\n\nOn Tue, Sep 22, 2015 at 2:50 PM, Ed Rooth notifications@github.com wrote:\n\nThis is something I am very much in need of. Otherwise we'll have to\nresort to the gross last resort of parsing the error string.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/110#issuecomment-142432416.\n. I would suggest adding a read-only accessor (e.g., Conn()) to\ncredentials.TransportAuthenticator instead.\n\nOn Tue, Mar 10, 2015 at 1:24 PM, Tv notifications@github.com wrote:\n\nDon't know how well this aligns with your plans, but adding a Conn()\nnet.Conn method to ServerTransport, with a trivial implementation in\nhttp2Server, is enough to do this, and also provides RemoteAddr etc.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-78140158.\n. Probably\n\ncredentials.TransportAuthenticator.ConnectionState() returns a struct similar to http://golang.org/pkg/crypto/tls/#ConnectionState.\n. On Tue, Mar 10, 2015 at 4:20 PM, Tv notifications@github.com wrote:\n\nI don't see credentials.TransportAuthenticator interacting with the\nserver side at all. At most it's used to wrap a net.Listener (with no\nplace to stash extra data). The plan for its future is unclear. Is there\nsomething planned that is just not implemented yet?\nhttps://github.com/grpc/grpc-go/blob/master/interop/server/server.go#L201\nI agree that exposing net.Conn feels dirty, but the alternative is a lot\nof case-by-case things (TLS ConnectionState! RemoteAddr! Client public key\nfor a non-TLS protocol!). Needing to support every underlying transport\nexplicitly sounds wrong.\nI said something similar to tls.ConnectionState instead of  a copy of it.\nPlus, most of members are pretty common for various transport level\nsecurity protocols.\nPerhaps the really modular way is something like func (s *Server)\nServeConn(ctx context.Context) error, that lets the caller pass whatever\nis needed in the context. This would do what one iteration of the Serve for\n{ Accept() } loop does now.\nIf pb.RegisterXxxServer was done differently, I'd be happy creating the\nstruct that's the receiver of all the RPC methods separately for every\nconnection, instead of sharing the same value for all connections, and\nstoring any state I want in there -- but it seems like Context is more\nwhere the world is heading. In my own explorations of RPC in\nhttp://godoc.org/github.com/tv42/birpc , imitating net/rpc, I separated\nRegistry from Codec to make that nicer -- though I ended up implementing an\nAPI I don't personally like anymore, filling method arguments based on type\nwith reflect.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-78169930.\n. Hi Tv,\n\nI think there was a miscommunication in the beginning. To clarify, your\nintention is to let users be able to extract the security info (e.g.,\nclient auth cert) for each incoming RPC on the server? If it is the case,\nthis is a generic question for gRPC. Do you have a real use case for this\nrequirement? Inside google, we do need to export some security info per RPC\nto users but this part has not been fleshed out.\nOn Tue, Mar 10, 2015 at 4:43 PM, Tv notifications@github.com wrote:\n\nI still don't see how you want the RPC method to actually access the\nConnectionState. RPC methods don't get passed the TransportAuthenticator.\nTransportAuthenticator is also not a per-connection value, so I don't see\nhow adding methods to that is helpful -- except if they also take either\nnet.Conn or Context as an argument.\nIf you're still talking about exposing net.Conn, then I personally don't\neven need anything more from TransportAuthenticator, I can hardcode my\napp to type assert to *tls.Conn etc.\nIf you're talking about passing Context to a TransportAuthenticator\nmethod to look up some sort of client credentials, then where does the\nTransportAuthenticator implementation get to do context.WithValue? Also,\nat that point, it might as well be a standalone function -- I'm not at all\nconvinced the client credentials are all the same type, so the calling code\nalready needs to know what it's asking for, and then can call\ntherightpkg.AuthFromContext(ctx).\nI'm really keen to know if you have a plan for all this, e.g. based on\nsome Google internal service or Java code, that's just slowly getting\nwritten in Go..\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-78172948.\n. Thanks. Haven't got time reading through. will get back to you shortly.\n\nOn Thu, Mar 19, 2015 at 6:16 PM, Tv notifications@github.com wrote:\n\nMaybe that error in 5. above should be just ok bool, as I don't see where\nthe error message could go, at that stage. Or maybe make ctx==nil\nspecial? Your call. My prototype didn't actually include that error there,\nbut I can see how closing the error could be desirable. What would happen\nif NewServerConn just said conn.Close()?\nNewServerCall can report the error to the client, so there error makes a\nlot more sense.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-83836296.\n. Given the recent change, users can write a custom TransportAuthenticator to\nextract auth info on server side via\nhttps://github.com/grpc/grpc-go/blob/master/credentials/credentials.go#L154.\nThe mapping from rpc to transport is still pending though.\n\nOn Thu, May 14, 2015 at 7:52 AM, Peter Sanford notifications@github.com\nwrote:\n\nAny update on this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-102060883.\n. This API is under design for the beta. I'll finalize the design ASAP once I\nam back from vacation (7/17).\n\nOn Tue, Jun 23, 2015 at 4:21 PM, Tv notifications@github.com wrote:\n\nFWIW the numbered branches are historical, the one without the number is\nlatest: https://github.com/bazil/grpc-go/commits/auth\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-114671076.\n. working on it. Instead of giving the entire certificate, I would like to know which fields in tls.ConnectionState are needed typically? The plan is to return the required fields to server handler as part of metadata. Using entire certificate introduces a lot of problems because we do not know the exact type of the certificate.\n. After talking to grpc c ppl, I am going to provide a minimal set of info which is consistent with what grpc c does right now in TLS transport authenticator. If it does not fit your requirements, it is trivial to provide your own custom authenticator impl to grpc.\n. @psanford  I will tune the authenticator API so that it can pass info about the cert in the coming PR.\n. You can add a print at\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L122 to\ncheck whether there is an entry keyed with \"transport_security_type\" which\nis from ServerHandshake. If you checked the client side, yes the current\nClientHandshake does not return any auth info.\n\nOn Tue, Aug 25, 2015 at 4:41 PM, Peter Sanford notifications@github.com\nwrote:\n\nIt seems like the information is still not available to the rpc endpoints.\nFrom my brief investigation, it looks like the code in\ntransport.newHPACKDecoder() that copies the metadata from d.mdata to\nd.state.mdata never gets called.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-134771478.\n. #304 changes the way on how to access client auth info. Now it is not part of normal metadata. Check https://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L116 as an example on how to access auth info in service handler.\n. It should be exported (I made a mistake when I switched among multiple\npotential implementations in my pr). Thanks for your fix.\n\nOn Sun, Sep 6, 2015 at 9:49 AM, Tamir Duberstein notifications@github.com\nwrote:\n\nThat seems wrong. What is the point of TLSInfo.state at all? It's not\nused internally and not exported.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-138099288.\n. Rename Formatter to Marshaler. I do not have good idea regarding RecvMessage and SendMessage though. But I thin their names are probably just fine -- send/recv a message via the input marshaler. Let me know if you have better idea.\n. close. will have a new PR for this per the discussion with dsymonds, sameer and bcmills.\n. I do not like Handler since it is already used in grpc meaning different thing. I like \"Service\"  and am also okay with \"Server\". I leave the decision to @dsymonds who is the owner of grpc codegen and protobuf in Go.\n. This is not put into the grpc package because both grpc and transport (a private package taking care of all transport level mechanisms) need to use it and calling chain from transport to grpc is not allowed. \n\nCredentials in credentials package is probably not ideal but this is not rare in golang packages such as Context in the context package. I will do quite a lot changes to accommodate the google internal security protocol in near future and will see if I can find a better name.\n. On Wed, Apr 15, 2015 at 4:42 PM, Dave notifications@github.com wrote:\n\nAt the moment, I'm looking at unifying credentials for all Go APIs, in\nparticular those under google.golang.org/cloud. I'm trying to design the\ninterfaces in such a way that it's useful for both those APIs as well as\ngrpc. When that's ready and stable, it should be able to replace the\ncredential package here altogether.\nsounds good but I am not sure if it is achievable. Keep in mind we have\ninternal security protocol that needs to be dealt with too, which could be\nvery different from those on cloud.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/115#issuecomment-93599069.\n. close this for now. We can revisit this later.\n. Hi Burcu,\n\nI am planning to rename package name from \"credentials\" to \"security\" so that it can cover more security related stuffs. Does this sound better?\n. done.\n. On Mon, Mar 16, 2015 at 6:38 PM, jellevandenhooff notifications@github.com\nwrote:\n\nI am running \"go version go1.4.1 darwin/amd64\". I accidentally pointed a\ngrpc client at an address that didn't speak grpc. Afterwards, grpc printed\na lot of messages in the log that were not helpful at best and distracting\nat worst. I would prefer grpc to a) not generate as many errors, perhaps\nwith some back-off mechanism, and b) not print as many errors.\nSpecifically, my terminal filled with hundreds of lines of the form\n2015/03/16 21:08:01 transport: http2Client.notifyError got notified that\nthe client transport was broken unexpected EOF.\n2015/03/16 21:08:01 transport: http2Client.notifyError got notified that\nthe client transport was broken unexpected EOF.\n2015/03/16 21:08:01 transport: http2Client.notifyError got notified that\nthe client transport was broken unexpected EOF.\n2015/03/16 21:08:01 transport: http2Client.notifyError got notified that\nthe client transport was broken unexpected EOF.\n2015/03/16 21:08:01 transport: http2Client.notifyError got notified that\nthe client transport was broken unexpected EOF.\nI tried sticking in a \"c.failFast = true\" in grpc.Invoke, but that did not\nhelp.\nI had got another user request which complained that there was no log\nmessages when the transport sees some error triggering reconnect. So I\nadded this log which seems spam you now. :) Let me see if there is a way I\ncan win in both worlds.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/120.\n. Exponential back-off is already there (https://github.com/grpc/grpc-go/blob/master/clientconn.go#L164). You actually hit a different case: your connect actually succeeded because that port is listening but when you sent the first rpc it got rejected by the peer because the peer does not speak grpc. Therefore, you already hit the initial reconnect interval.\n. If you replace it with glogger (https://github.com/grpc/grpc-go/blob/master/grpclog/glogger/glogger.go), all the logs will go to some files instead of stderr unless you configure it explicitly.\n. I can add a getter for the state of ClientConn. But be aware of that this kind of getter is always racy -- you could get stale state (the underlying could change the state at any time.) when you call that getter. That's the reason I did not add that in the first place. Actually, you can feel free to send requests when the connection is doing retries unless you inject too many pending requests leading to memory issues.\n. regarding callback approach,\ni) As dsymonds mentioned, it is still racy unless the calls holds a lock (so that others cannot change the state) when it runs, which is clearly not a sound solution;\nii) I always try hard to avoid all these callback based stuffs in grpc-go lib and favor the simple synchronous model which has been used in it.\n. That degradation could be measurement noise. Now it is disappearing from my runs. I am going to setup a dedicated env to revisit it.\n. Thanks for asking. I plan to start it sometime next week. But I could be\npreempted by other things. :)\n\nOn Wed, Mar 18, 2015 at 8:06 AM, Walter Schulze notifications@github.com\nwrote:\n\nMay I ask what the plan is over here?\nhttps://github.com/grpc/grpc-go/blob/9c4157fb5a0472c1dbfa016439a0146cd8f0221e/rpc_util.go#L142\n// TODO(zhaoq): optimize to reduce memory alloc and copying.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124.\n. The good news is that I am making a change so that it will be very easy to\nsupport various codecs for grpc-go because essentially grpc is independent\nof IDL and codec. Once it is done, it will be very easy to use your own\ncodec (gogoprotobuf) to try out grpc.\n\nThe plan for protobuf is to minimize memory alloc/dealloc and copying in\ngrpc. For example, proto.Buffer (\nhttps://github.com/golang/protobuf/blob/1e73516e50e04b41a6760c856eee86b00a384526/proto/lib.go#L250)\nwould be among the first things I will try on.\nOn Thu, Mar 19, 2015 at 12:53 AM, Walter Schulze notifications@github.com\nwrote:\n\nWhat do you plan to do?\nWith github.com/gogo/protobuf I generate a Size, Marshal and Marshalto\nmethod.\nThe Marshal method calls Size and then allocates a byte slice of that size.\nThen it pass that to MarshalTo which fills in the byte slice.\nThis way there is only one alloc per message.\nGiven a big enough pre allocated buffer it would be easy to reuse the\nbuffer with Marshalto.\nAnd Marshalto could even be changed to a MarshalZeroTo which does the same\njob as Marshalto, but with less copying.\nMy experience is that golang/protobuf does not really want to generate\ncode, but wants to do almost everything with reflect and unsafe pointer.\nThis is definitely slower, but I understand that it is less code to\nmaintain.\nSo what I want to know is what you plan to do here and if you\ngolang/protobuf is still not generating code (things might have changed),\nhow can I still get the maximum speed out by leveraging gogoprotobuf's\ngenerated code?\nIf you just call Marshal then atleast I just do one alloc.\nBut there is so much speed to gain here, since writing the buffer does a\ncopy anyway thus it is really easy to reuse a big buffer.\nFor example\nhttps://github.com/gogo/protobuf/blob/master/io/varint.go\nI could make this even faster by not having any copying instructions in\nMarshalTo\ngrpc looks great and I would really love to use it, but speed is very\nimportant.\nI would like to be prepared for any changes I can make to gogoprotobuf to\nstill gain the maximum amount of speed.\nI really don't want to make another fork.\nSo maybe there is someway we could work together?\nI have grpc code generation merged into gogoprotobuf on the proto3 branch\nif you are at all interested.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-83400326.\n. On Thu, Mar 19, 2015 at 1:31 AM, Walter Schulze notifications@github.com\nwrote:\nThat is great news about being IDL and codec independent.\nI could also use that in totally different project I am working on :)\nCould you tell me when you are ready so I can test your change?\nIt should be within next week.\nThank you very much.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-83416600.\n. Hi Walter,\n\nI just checked in #146 to allow custom codec working with grpc-go. For now, we allow a custom codec by doing the following:\ni) implement your custom codec satisfying https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L53;\nii) on client side,  conn, err := grpc.Dial(serverAddr, grpc.WithCodec(yourCodec))\niii) on server side, server := grpc.NewServer(grpc.CustomCodec(yourCodec))\nFeel free to ping us if there is any problem.\n. I am not clear how you will proceed. Just be aware of that the grpc server\ndoes unmarshaling inside the generated code (e.g.,\nhttps://github.com/grpc/grpc-go/blob/master/test/grpc_testing/test.pb.go#L544)\nfor unary rpc. You probably need to treat it properly depending on your\napproach.\nOn Thu, Apr 2, 2015 at 1:21 AM, Walter Schulze notifications@github.com\nwrote:\n\nThis looks great :) I can't wait to start playing this.\nThank you very much.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-88820770.\n. Do you still have problems running the test and benchmark here? Do you want me to try out?\n. On Sat, May 9, 2015 at 12:25 AM, Walter Schulze notifications@github.com\nwrote:\nI was wondering if you have had any time to check those benchmarks out?\nnah, planned to do this week but got sick ...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-100438344.\n. Thanks for contributing. Notice that this behavior is not only for grpc-go.\nIf we really want to make this change, it should happen for other grpc\nimplementations too. The typical way we are doing this is that i) write a\ndetailed spec and circulate to reach consensus; ii) implement in all the\nlanguages and have test and iii) make an interop test to make sure it works\nacross languages. If you are interested, you can write a spec first. My\nfeeling about your proposal is that it seems make the reconnect behavior\nover-complicated (2 nested exponential back-off). Actually I think what the\nright treatment is is still debatable. Connecting to a peer which does not\nspeak grpc is a serious and fatal error  so that spam might not be that bad\nbecause you should deal with it immediately and the only way you can deal\nwith that is to stop your client. I might be able to use glog to make it\nless spammy.\n\nIn addition, FYI, we are working on load balancing channel which could\nchange some semantics here also in the near future.\nOn Thu, Mar 19, 2015 at 8:49 PM, jellevandenhooff notifications@github.com\nwrote:\n\nThis fixes my problems from #120\nhttps://github.com/grpc/grpc-go/issues/120.\nI considered using sleeps and times instead of time.After, as I thought\nthat\nmight be more efficient. However, this version using time.After looks far\nnicer\nand is easier to understand.\nLet me know what you think and/or how this PR could be improved!\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/grpc/grpc-go/pull/125\nCommit Summary\n- Add back-off logic to transportMonitor's reconnect.\nFile Changes\n- M clientconn.go\n  https://github.com/grpc/grpc-go/pull/125/files#diff-0 (21)\nPatch Links:\n- https://github.com/grpc/grpc-go/pull/125.patch\n- https://github.com/grpc/grpc-go/pull/125.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/125.\n. On Fri, Mar 20, 2015 at 10:55 PM, Walter Schulze notifications@github.com\nwrote:\nLooks great. Am I right in my interpretation that you can now create your\nown buffer optimisations inside a codec since the codec object is going to\nbe reused for every encode?\nYup! Please share your results (golang/protobuf vs. gogo/protobuf) with us\nonce it is done. It would be very interesting to see the difference.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-84261505.\n. Adding a test is the plan to go. But I hope to sort everything else out before I start to do that (otherwise I have to change a lot of code back and forth.).\n. On Sun, Mar 22, 2015 at 6:52 PM, David Symonds notifications@github.com\nwrote:\nGitHub is a terrible place to have this kind of discussion unfortunately.\nI'm about to be travelling, so let me propose something to keep everything\nmoving:\nTrim this change back to something simpler except for the mechanical\nplumbing: define the Codec interface in the grpc package, and change\nproto.Message to interface{} in all the relevant places. Leave out the\nContent-Type and String() changes for now. Don't change the transport\npackage. That accounts for maybe 75% of this change, and there shouldn't be\nmuch contentious in it. We can get that submitted very quickly.\nMove the remainder (any changes to the transport package, any Content-Type\nselection/registration, etc.) to a separate pull request.\nDoes that sound okay?\nHow does the server know the codec to be used? hardcoded to ProtoCodec for\nnow?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-84747112.\n. On Sun, Mar 22, 2015 at 7:02 PM, David Symonds notifications@github.com\nwrote:\nYeah, just hardcode to ProtoCodec for now. You can store it in whatever\nstruct makes sense, and just assign it to ProtoCodec{} when making that\nstruct.\n\nokay. Perhaps we can talk in person when you arrive MTV? :)\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-84748594.\n. Thanks!\n\nOn Sun, Mar 22, 2015 at 7:13 PM, David Symonds notifications@github.com\nwrote:\n\nSure, we can talk in person then. Drop something on my calendar if you want\nto reserve a time; I'm getting booked out!\nWe can proceed with the basic plumbing change here in the meantime if you'd\nlike.\n\nI probably won't have time for it today (Sunday evening now). So probably\nleaving it after the meeting  would be better.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-84753174.\n. On Sun, Mar 22, 2015 at 7:24 PM, Qi Zhao toqizhao@gmail.com wrote:\nThanks!\nOn Sun, Mar 22, 2015 at 7:13 PM, David Symonds notifications@github.com\nwrote:\n\nSure, we can talk in person then. Drop something on my calendar if you\nwant\nto reserve a time; I'm getting booked out!\nWe can proceed with the basic plumbing change here in the meantime if\nyou'd\nlike.\n\nI probably won't have time for it today (Sunday evening now). So probably\nleaving it after the meeting  would be better.\nI can probably make the pull request to golang/protobuf ready for review\nwhen you are on plane if you are okay with that part?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-84753174.\n. On Sun, Mar 22, 2015 at 7:37 PM, David Symonds notifications@github.com\nwrote:\n\nWe generally don't take pull requests for the protobuf package. Send it to\nme internally instead (though synchronised with importing this change.)\n\nI see. Then we will need your help for a really quick push from internal to\ngithub to minimize the potential breakage.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-84761279.\n. On Sun, Mar 22, 2015 at 7:45 PM, David Symonds notifications@github.com\nwrote:\nIt might actually be better to do the non-mechanical parts of this change\n(e.g. renaming the method) completely internally first, and then export in\none hit while I'm nearby?\n\nsgtm. I am going to prepare it when you are traveling.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-84761801.\n. Hi Walter,\n\ndsymonds and I reached consensus on the design. But for now only half of\nthe change is in -- I still hard coded protobuf as the codec for\neverything. Please hold a little while for the other half, which should be\ndone in the next a couple of days. Sorry about the delay.\nOn Sun, Mar 29, 2015 at 11:56 PM, Walter Schulze notifications@github.com\nwrote:\n\nLooks good :)\nNow I just need to be able to inject my own codec somewhere, or am I\nmissing something?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-87571958.\n. I am thinking reconstructing tlsCreds as\n\ntype tlsCreds struct {\n  config tls.Config\n}\nThen\nfunc NewClientTLS(c tls.Config) TransportAuthenticator {\n  return &tlsCreds{\n     config: c,\n  }\nWe may also provide other convenience functions to create a tlsCreds without constructing a tls.Config as input param.\n. make this possible with #167 \n. Thanks for reporting. I actually had a commit to fix this already which is\nnot checked in due to the lack of tests. Will add the tests and submit it\ntomorrow.\nOn Sun, Mar 22, 2015 at 2:13 PM, Max Hawkins notifications@github.com\nwrote:\n\nI'm making a client-side RPC call to a grpc-go server that has crashed. My\ncontext has a timeout (context.WithTimeout) so after a few seconds the RPC\nfails as expected with the following message:\n2015/03/22 14:01:04 [error] rpc error: code = 13 desc = \"stream error: code = 4 desc = \\\"context deadline exceeded\\\"\"\nHowever, the grpc.Code for this error is codes.Internal rather than\ncodes.DeadlineExceeded.\nI don't know the codebase well enough to add a failing test case, however\nI believe the issue comes from this line, which returns all errors with the\ncode codes.Internal:\nhttps://github.com/grpc/grpc-go/blob/fbd3f79d38f568d15970853ffe81112028962e1b/call.go#L150\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/129.\n. should be fixed in #132.  Please reopen it if the problem sustains.\n. Please use metadata API to transmit your key-value pair. The example is\nfor unary rpc:\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L355\nfor streaming rpc:\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L504\n\nCancellation is propagated to server only when it is needed.\nOn Sun, Mar 22, 2015 at 4:43 PM, David Symonds notifications@github.com\nwrote:\n\nContexts are not for passing arbitrary bits of data from client to server.\nThat's what the RPC message itself is for. There will probably be a\nmechanism for attaching bits of data to an RPC via a context (e.g. trace\nIDs), but it's going to be tightly controlled, and almost definitely not\nwhat you want to use.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/130#issuecomment-84733244.\n. On Sat, Apr 11, 2015 at 9:15 AM, Harlow Ward notifications@github.com\nwrote:\nThere will probably be a mechanism for attaching bits of data to an RPC\nvia a context (e.g. trace IDs), but it's going to be tightly controlled,\nand almost definitely not what you want to use.\n@dsymonds https://github.com/dsymonds any idea how far down the line\nthis will be? It would be really nice to have Trace IDs as part of the\ncontext. It feels a bit clunky having to wrap all the Protos with\nArgs/Req/Reply messages just to pass the Tracer around.\ngrpc-go does not allow users attach metadata to the context directly (grpc\nwill ignore that). Users are required to use metadata API to attach the\ninfo (e.g., trace ID) to the context.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/130#issuecomment-91872428.\n. Can you take a look at the examples in my very first reply on this issue on\nhow to set and receive metadata and see whether it answers your question?\n\nCopied/pasted for your convenience:\nPlease use metadata API to transmit your key-value pair. The example is\n*for unary rpc: *\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L355\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L355\nfor streaming rpc:\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L504\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L504\nOn Mon, Apr 27, 2015 at 10:12 AM, dramdass notifications@github.com wrote:\n\nIs there a way to set the metadata from the generated protobuf client code?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/130#issuecomment-96743878.\n. Thanks, Harlow!\n\nOn Fri, May 1, 2015 at 5:20 PM, Harlow Ward notifications@github.com\nwrote:\n\nHere is a commit for anyone interested in using the metadata. Works\nnicely for things such as traceID:\nharlow/go-micro-services@221a67b\nhttps://github.com/harlow/go-micro-services/commit/221a67bca4309aa12b69cd57ca6c1e9522c09cfc\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/130#issuecomment-98266303.\n. We do have related plan. But can you elaborate \"audit\" and \"global rate limiting or synchronization\" first so that I can make sure we are on the same page?\n. Why not do them in the application layer?\n\nOn Mon, Mar 23, 2015 at 12:09 PM, Xiang Li notifications@github.com wrote:\n\n@iamqizhao https://github.com/iamqizhao\naudit: before each actual client rpc invocation, the control will hand\nover to application pre-defined interceptor first. The interceptor can\nrecord the related metrics of the invocation.\nWhy don't you call it before you make the rpc call? Note that I do not mean\ninside the generated code.\nsynchronization: before each actual server rpc handling, the control will\nhand over to application pre-defined interceptor first. The interceptor can\ndecide when to actually call the handler to process the request.\nDitto, Why don't you call it in the server handler impl?\nAll of them can be done by manually adding hooks into the generated stub\nstatically. But there is no good way to do it dynamically.\nmaybe something like:\nclient.Intercept(method string, interceptF func(args ...interface{}))\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85152377.\n. On Mon, Mar 23, 2015 at 1:53 PM, Xiang Li notifications@github.com wrote:\nWhy don't you call it before you make the rpc call?\nIf I have 10 places to do one rpc call, I need to write the code 10 times.\nTo solve that, I need to wrapper round the actual client, which leads to my\nfirst point in the issue:\nnot really. You can wrap it into a struct so that you only write the code\nonce. Again, wrapping is not in the generated code. The same approach\napplies to the server side too.\nAt the client side, we can actually wrapper the client interface with a thin layer. But we still cannot make this happen dynamically at runtime.\nFor example, if I have a instruments framework, ideally all the grpc calls\ncan be examined by the framework transparently. The application can be\nunaware of the auditing and the framework can change the way to do the\nauditing without modifying the application code.\nActually our prior experience indicates that pushing all these complexities\ninto a generic library is a bad idea. We do not have plan to to support\ngeneric interceptor (except java) in grpc.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85191833.\n. On Mon, Mar 23, 2015 at 2:00 PM, Xiang Li notifications@github.com wrote:\n@iamqizhao https://github.com/iamqizhao I just found there is something\nrelated in java implementation:\nhttps://github.com/grpc/grpc-java/blob/master/core/src/main/java/io/grpc/ClientInterceptor.java\nYup, this is java specific thing. we do not have plan to support\ninterceptor for other languages.\nPersonally, I think adding a thin wrapper around the generated interface\nmight be good enough for the common cases. But adding the ability to\nintercept the rpc invocation and handling might still be useful, at least\nin the case I described.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85194478.\n. On Mon, Mar 23, 2015 at 2:13 PM, Xiang Li notifications@github.com wrote:\nnot really. You can wrap it into a struct so that you only write the code\nonce. Again, wrapping is not in the generated code. The same approach\napplies to the server side too.\nI understand that. But I cannot create the struct dynamically as I\nmentioned.\nEven if golang supports make a struct at runtime, this will add a great\namount of\ncomplexity comparing to add a \"hook place\" in the generated code.\nI do not get it. what is the problem with\n\nfunc callFoo(ctx context.Context, client yourpb.XXXClient, arg\nproto.Message, f func(...interface{}), hargs ...interface{})\n(proto.Message, error) {\n  if f != nil {\n    f(hargs...)\n  }\n  return client.Foo(ctx, arg)\n}\n\nWe do not have plan to to support generic interceptor (except java) in\ngrpc.\nCan you talk a little bit about your plan? Thanks!\nThere won't be generic interceptor support unless there is a huge demand\nfor that. It can be done in application layer.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85200083.\n. On Mon, Mar 23, 2015 at 4:58 PM, Hongchao Deng notifications@github.com\nwrote:\nHi @iamqizhao https://github.com/iamqizhao .\nThanks for the replying. I've been following up in this issue.\nLet me clarify our problem. We let user to define their grpc service and\nmessages. It's beneficial because it makes serialization and network\ncommunication of messages more efficient (via zero copy, if supported by\ngrpc).\nWhile user define grpc service and messages, our framework needs a layer\nbetween (user) grpc service methods and actual serialization/networking:\nfunc serviceCalled(methodName string, ctx context.Context, input proto.Message) (context.Context, proto.Message)\nfunc messageReturned(methodName string, ctx context.Context, output proto.Message, err error) (proto.Message, error)\nThis would be super awesome if grpc can support it. I believe there are\nmany more use cases.\nI think there is probably a design flaw here (or I misunderstood\nsomething). There are two typical ways to host a service using grpc/other\nrpc framework:\ni) expose grpc to users directly. In that case, the service provider only\nprovide service on server side and zero control on client side;\nii) wrap grpc as an internal communication mechanism and write your own\nclient library exposing to your users.\nYour use case seems not fall into either of them -- you expose grpc to\nusers directly but require some amount of control on client side in\nmeanwhile (so that\nyou want grpc itself to provide this control layer and corresponding hook\nfor you but I think it is not right thing to do).\n\nXiaoyun should know this. Or probably we still have some miscommunications\nhere. I can talk to Xiaoyun in person if you guys think it is helpful.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85252837.\n. On Tue, Mar 24, 2015 at 6:02 AM, xiaoyunwu notifications@github.com wrote:\n@iamqizhao https://github.com/iamqizhao, sorry I am late to this\nthread, on airplane.\nOne of the reason we liked grpc is that it provides typed method directly\nto application, so that application can be written in an domain natural way\nwithout worry about serialization, and dispatching (if one have to use http\ndirectly).\nScanning through the threads, I think there is a miscommunication (or at\nleast the wording suggested so). I agree that your callFoo is a good idiom,\nbut only for application. Not for framework. The project that we worked\n(taskgraph) is actually at framework, by that I mean we expect many\ndifferent machine learning applications can be build on top of this. In\nfact, we might have different framework implementations, each provide\ndifferent semantics (BSP, the current implementation is just an example).\nYour framework is still an APPLICATION from the point of view of grpc. I am\nnot clear what your project does. But per my previous email, I think the\nright approach for your project is to provide your client library (probably\nand server library) to your users instead of exposing grpc directly. Recall\nthat how bigtable, GFS, chubby etc. do using google internal rpc library.\nWhen defined on host:port level, grpc is really a low level interface.\nApplication might want to have the same typed method call at higher level.\nFor example, assume that we have multiple replica of the same service, and\nwe want to do automatic retries between them, or send request to all of\nthem, and return when the first response come back. While it is easy to\nwrap each rpc service in each application, but that will requires many code\nduplication, which is bad. Also, some of these things might be hard to get\nit right, without leaving hooks so that it makes it easy to write framework\nlevel code will only make go programmer life harder.\ngrpc is NOT defined on host:port level. load balancing channel  and name\nservice is under design right now, which achieves most of what you\ndescribed above.\nI did not really following your argument here, can you elaborate a bit\nmore.\n\"Actually our prior experience indicates that pushing all these\ncomplexities\ninto a generic library is a bad idea. We do not have plan to to support\ngeneric interceptor (except java) in grpc.\"\ngRPC is a generic library -- it does not bind to a particular application.\nI think this also clears your following confusion.\nIf I go with my understanding of what it is, I think we are not ask\ngrpc-go to provide a generic library, mere the ability to write such\ngeneric library. I do not think possibility of writing a bad generic\nlibrary is a good reason to do not support the attempt to write one.\nGRPC is one of the things that I miss when I left google (among many other\nthings), while I understand that applications might represented most of\ncurrent grpc users, I think adding support to allow write framework level\ncode will only make it more usable.\nGRPC is not the thing you miss because it was born after you left Google.\nIt must be something else. :P\nSo the question we have really is how do we write framework level code\nin grpc-go?\nMy suggestion would be following how bigtable/gfs architects their\nframework.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85485847.\n. Doesn't grpc do the serialization work for applications already? Or you\ntried to use this as an example to demonstrate why an interceptor is\nneeded? You seem to move the discussion back to the track. But\nunfortunately it makes me completely lost. What is your real question?\n\nOn Tue, Mar 24, 2015 at 2:18 PM, Hongchao Deng notifications@github.com\nwrote:\n\nJust like netty SimpleChannelInboundHandler\nhttp://netty.io/4.0/api/io/netty/channel/SimpleChannelInboundHandler.html\n.\nIt hands over the serialization work to communication layer. In this way,\nit's more efficient. As long as user defines Decode() method (in grpc case,\nas long as it's proto message), netty/grpc would handle it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85694847.\n. On Tue, Mar 24, 2015 at 2:56 PM, Hongchao Deng notifications@github.com\nwrote:\nDoesn't grpc do the serialization work for applications already?\nThat's right. But there are two layers in \"application\". We use grpc to\ntransfer messages over network. Message would look like this:\nmessage Entry {\n    required uint64     Type  = 1;\n    required uint64     Term  = 2;\n    required uint64     Index = 3;\n    optional bytes      UserData  = 4;\nThe fourth field is actually carrying the user data. User has defined a\nproto message, and then serialize in order to put it in fourth field. This\ncould be huge cost. Think that we are carrying 2GB user data. Serializing\nit TWICE would be huge cost. Not to mention that it could improved more in\nzero copy in network communication.\nokay, I see the problem here. This cannot be addressed now because the\nopen-sourced protobuf does not have this kind of zero-copy support (the\ninternal version does have). But I am wondering how this can be resolved\nwith a interceptor?\nWhat if we put the entire grpc layer to user? No, we still need the other\nmetadata and some control. One example would be:\nhttps://github.com/coreos/etcd/blob/master/raft/raftpb/raft.proto#L20\nhttps://github.com/coreos/etcd/blob/master/raft/multinode.go#L387\nIdeally, it's about huge data transfer and improving efficiency in\nserialization and network communication.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85709700.\n. On Tue, Mar 24, 2015 at 5:16 PM, Qi Zhao toqizhao@gmail.com wrote:\nOn Tue, Mar 24, 2015 at 2:56 PM, Hongchao Deng notifications@github.com\nwrote:\n\nDoesn't grpc do the serialization work for applications already?\nThat's right. But there are two layers in \"application\". We use grpc to\ntransfer messages over network. Message would look like this:\nmessage Entry {\n    required uint64     Type  = 1;\n    required uint64     Term  = 2;\n    required uint64     Index = 3;\n    optional bytes      UserData  = 4;\nThe fourth field is actually carrying the user data. User has defined a\nproto message, and then serialize in order to put it in fourth field. This\ncould be huge cost. Think that we are carrying 2GB user data. Serializing\nit TWICE would be huge cost. Not to mention that it could improved more in\nzero copy in network communication.\nokay, I see the problem here. This cannot be addressed now because the\nopen-sourced protobuf does not have this kind of zero-copy support (the\ninternal version does have). But I am wondering how this can be resolved\nwith a interceptor?\n\nBTW, I do not see this is the problem for grpc only. How can you avoid it\nin your previous http based solution? If you pass all these uint64 fields\nas http headers, grpc can do the same thing already.\nWhat if we put the entire grpc layer to user? No, we still need the other\n\nmetadata and some control. One example would be:\nhttps://github.com/coreos/etcd/blob/master/raft/raftpb/raft.proto#L20\nhttps://github.com/coreos/etcd/blob/master/raft/multinode.go#L387\nIdeally, it's about huge data transfer and improving efficiency in\nserialization and network communication.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85709700.\n. On Mon, Mar 23, 2015 at 5:07 PM, Max Hawkins notifications@github.com\nwrote:\n\nSetting the grpc.WithTimeout option in grpc.Dial causes all RPCs to fail\nwith the error \"grpc: the client connection is closing\" after a server\nrestart.\nSteps to reproduce:\n1.\nCreate a client with a connection timeout:\nconn, err := grpc.Dial(address, grpc.WithTimeout(timeout))\n2. Make an RPC call. It succeeds.\n1. Shut down the server\n2. Make an RPC call with a deadline. As expected, it cannot connect\n   and fails: 'rpc error: code = 4 desc = \"context deadline exceeded\"'\n3. Start up the server\n4. Make another RPC call. I expect this to work. Instead, it and each\n   subsequent RPC fails with 'rpc error: code = 2 desc = \"grpc: the\n   client connection is closing\"'\nThis is the expected behavior. Use Dial timeout only when you needed. If\nyou use it,  make sure it is long enough to accommodate the use case that\nyou do not want to fail.\n1.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/133.\n. Could you please sign CLA?\n. LGTM, can you add a test which will also be an example how to use it for other people?\n. This should not happen if the interval between step 2 and 4 is long enough.\nCan you provide the timing of the above steps (especially, step 2, 3, 4, 5).\n\nI am on vacation this week and will try to look into it sometime next week\nif you can provide enough info to facilitate my debugging.\nOn Mon, Apr 6, 2015 at 7:22 PM, prazzt notifications@github.com wrote:\n\nA simple ping pong rpc with increasing counter. Client send ping every n\nseconds interval, specifying timeout on context.\nWhat happened:\n1. Client-server ping pong smoothly.\n2. Turn off network connection.\n3. Client failed \"context deadline exceeded\". Client's main exited.\n4. Turn on network connection.\n5. After sometime, server received ghost ping message with updated counter.\nExpected:\nServer shouldn't receive message, since client process already exit.\nSeems like low level retransmission problem. I'm not familiar with HTTP2\nprotocol, but if we can control it, I expect client to not retransmit once\nthe context is canceled.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/150.\n. I had difficulty to reproduce it. Are you sure the client's main existed before the network connection is on?\n. On Thu, Apr 16, 2015 at 6:42 AM, prazzt notifications@github.com wrote:\nIf it helps, netstat says FIN_WAIT1 on machine1 while connection's off.\nokay, it seems you did not unplug the network cable. Instead you disabled\nnetwork connection causing machine1 sent FIN. In this case, the kernel of\nmachine1 will keep retrying with exponential backoff tcp_orphan_retries\n(default 8) times. If you enable network during retrying, the server could\nreceive 4th message. If you use netstat to monitor the state of tcp socket\non machine1 to make sure it disappears, the server should not receive 4th\nmsg any more.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/150#issuecomment-93736976.\n. grpc exposes context for unary rpc and wraps context in stream object for\nstreaming rpc given a lot of consideration during the design phase.\n\nOn Mon, Apr 6, 2015 at 8:56 PM, prazzt notifications@github.com wrote:\n\nIt wasn't clear to me what's the use of context in the generated server\ninterface. In route guide example:\nrpc GetFeature(Point) returns (Feature) {}\nrpc ListFeatures(Rectangle) returns (stream Feature) {}\nWhy is GetFeature\nhttps://github.com/grpc/grpc-go/blob/master/examples/route_guide/proto/route_guide.pb.go#L313\ngets context while ListFeatures\nhttps://github.com/grpc/grpc-go/blob/master/examples/route_guide/proto/route_guide.pb.go#L325\ndont ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/151.\n. There is no such thing (service, method impl) on client side. gRPC is a\nclient-server rpc framework instead of a peer-to-peer messaging library.\nYou can start both a grpc client and a grpc server on each peer to\naccommodate your requirements.\n\nOn Mon, Apr 6, 2015 at 9:04 PM, prazzt notifications@github.com wrote:\n\nDoes grpc support bidirection (e.g. server calls client's method anytime)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/152.\n. sgtm. I am on vacation this week. Feel free to make a pull request to me if\nyou want to contribute. Thanks. BTW, does Andrew finish the application\ndefault credentials already?\n\nOn Tue, Apr 7, 2015 at 12:36 AM, David Symonds notifications@github.com\nwrote:\n\nThe google.golang.org/grpc/credentials package should have a function\nthat can be given an oauth2.TokenSource and returns a\ncredentials.Credentials. This might just involve renaming and exporting\nits existing computeEngine type.\nThis will make it easier to use \"Application Default Credentials\" ([1],\n[2]) with gRPC.\n[1]\nhttps://developers.google.com/accounts/docs/application-default-credentials\n[2] http://godoc.org/golang.org/x/oauth2/google#DefaultTokenSource\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/153.\n. I think this is due to the lack of a health checking mechanism to detect the state of transport when no TCP signal (e.g., RST) is generated. The health checking is on our radar but the detailed design has not been fleshed out.  For now, you can set a timeout for the entire stream as a workaround. BTW, you can always cancel the rpc via the context binding with the rpc.\n\nFor yinhm's question, I have no clue from your simplistic description. Let me know when you have a reproduction.\n. The thread is unresponsive for long time. close it for now. Feel free to reopen if it sustains.\n. will do tomorrow morning. Thanks.\nOn Sun, Apr 12, 2015 at 1:32 AM, David Symonds notifications@github.com\nwrote:\n\nFYI, @iamqizhao https://github.com/iamqizhao, I'd like to merge this or\nsomething similar early this week.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/155#issuecomment-92010625.\n. done. :)\n. fixed by #166 \n. is there a way I can reproduce this?\n. This could be fixed by #160 already. Can you sync the repo and retry? Let me know if it still sustains. In meanwhile, I will add a new end2end test case to mimic your use case next week.\n. This should be fixed by #162. I reproduced the error you got with the existing test cases.\n. ah, bad timing to work after a long travel ... Thanks.\n. can you share your private reproduction?\n\nOn Sun, Apr 12, 2015 at 6:44 PM, David Symonds notifications@github.com\nwrote:\n\nIf I have a single gRPC connection and flood it such that a bunch of RPCs\nend up timing out, the connection gets into an unrecoverable state. Any\nfuture RPCs that should otherwise work end up failing with errors like\nrpc error: code = 13 desc = \"transport: HPACK header decode error:\ndecoding error: invalid indexed representation index 82\"\n(and the index increments over time).\nI traced it to the http2 hpack decoder, which has d.parseHeaderFieldRepr\nreturning an error. Somewhere after that, though, I'd expect the connection\nto recognise that it is stuck and automatically reset. But it doesn't.\nI have a private reproduction that appears to be able to reproduce this\n100% of the time. I haven't tried reproducing it with a smaller test case\nyet.\nI don't know who is at fault, grpc-go or http2. But it shouldn't be\npossible to send a bunch of RPCs on a gRPC connection such that the\nconnection ends up poisoned.\n/cc @bradfitz https://github.com/bradfitz in case he has thoughts.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/164.\n. grpc should treat it as ConnectionError so that reconnect is triggered (now\nit is a StreamError). I am generating a pull request now. But this error is\nnot expected and we should figure out the root cause. It seems the hpack\nstate is messed up either due to a miss use in grpc or a bug in hpack impl.\n\nOn Sun, Apr 12, 2015 at 7:20 PM, Qi Zhao toqizhao@gmail.com wrote:\n\ncan you share your private reproduction?\nOn Sun, Apr 12, 2015 at 6:44 PM, David Symonds notifications@github.com\nwrote:\n\nIf I have a single gRPC connection and flood it such that a bunch of RPCs\nend up timing out, the connection gets into an unrecoverable state. Any\nfuture RPCs that should otherwise work end up failing with errors like\nrpc error: code = 13 desc = \"transport: HPACK header decode error:\ndecoding error: invalid indexed representation index 82\"\n(and the index increments over time).\nI traced it to the http2 hpack decoder, which has d.parseHeaderFieldRepr\nreturning an error. Somewhere after that, though, I'd expect the connection\nto recognise that it is stuck and automatically reset. But it doesn't.\nI have a private reproduction that appears to be able to reproduce this\n100% of the time. I haven't tried reproducing it with a smaller test case\nyet.\nI don't know who is at fault, grpc-go or http2. But it shouldn't be\npossible to send a bunch of RPCs on a gRPC connection such that the\nconnection ends up poisoned.\n/cc @bradfitz https://github.com/bradfitz in case he has thoughts.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/164.\n. Hi David,\n\n\nnot in good condition to work today. The pull request will be made later.\nSorry about that. It would be great to have a self-contained reproduction\nfrom you so that I can investigate what is going on here. Thanks.\nOn Sun, Apr 12, 2015 at 7:50 PM, David Symonds notifications@github.com\nwrote:\n\nOkay. I'll wait to see your pull request and can test it easily enough.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/164#issuecomment-92177205.\n. okay, it seems I can reproduce it by running TestRPCTimeout test cases a few thousand times. will look into it.\n. should be fixed via #165. will have another pull request to make it connection though.\n. s/connection/connection error/\n. On Fri, Apr 17, 2015 at 4:02 PM, David Symonds notifications@github.com\nwrote:\nI mean, instead of a WithNetwork function, have something like\nfunc WithDialer(f func(addr string) (net.Conn, error)) DialOption\nand then use that function in place of net.Dial (or as the dialer passed\nto tls.Dial).\nThis is definitely more generic. My concern is that it pushes a bit too\nmuch work to users -- e.g., users have to write dialing code for uds.\n\nWhen you said \"It doesn't help this support any other address types\neither\", what example did you have in mind? I assumed the addr string\nmatches the named network when users call WithNetwork.\n\u2014\n\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/169#issuecomment-94092999.\n. The problem/difficulty to take this approach is that it has to merge with\ndial timeout option.\n\nhttps://github.com/grpc/grpc-go/blob/master/transport/http2_client.go#L118\nand\nhttps://github.com/grpc/grpc-go/blob/master/transport/http2_client.go#L113\nIf we merge them, again it seems it complicates the task if a user only\nwants to set a dial timeout -- he has to implement a fully functional\ndialing function...\nOn Fri, Apr 17, 2015 at 4:23 PM, David Symonds notifications@github.com\nwrote:\n\nIf you're wanting to dial UDS, it's all of one line to do the dial (invoke\nnet.DialUnix), so it's relatively little work for that use case to wrap\nthat in a closure.\nThere's lots of other dialing requirements, like Google-internal things,\nor dialing on App Engine (e.g. you need to use the socket API there), or\nwanting to monitor/control/log outbound connection information, and so on.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/169#issuecomment-94096172.\n. On Fri, Apr 17, 2015 at 11:29 PM, David Symonds notifications@github.com\nwrote:\nThat the code is already a bit weird in trying to handle timeouts is\nunfortunate, and should itself probably be fixed. I flagged that\npreviously, but didn't have the time to pursue it.\nI wasn't suggesting getting rid of WithTimeout, only WithNetwork.\nA timeout is really the only option to the dialer that makes sense across\nalmost every dialer, so it seems reasonable to make the signature be func\nWithDialer(f func(addr string, timeout time.Duration) (net.Conn, error))\nDialOption.\nThis still has some issues. For plain tcp, WithTimeout and the proposed\nWithDialer  cannot coexist (the timeout set by WithTimeout will be\ndiscarded). There are more problems for tls connection. If users use\nWithDialer to provide a closure to generate net.Conn (plain tcp) from addr\nstring, tlsCreds needs to use tls.Client to create a TLS client side\nconnection from the plain tcp connection and call tls.Conn.Handshake to\nfinish the tls handshaking. This is a bit troublesome comparing to the\ncurrent code. In additional, the timeout specified by WithDialer only takes\ncare of the duration for establish a plain tcp connection and does not\ncover the tls handshaking latency.\n\nI had some discussion with java and c grpc teams. They plan to do / is\ndoing the approach which puts network type into the addr string (e.g.,\nunix:///mysock.unix and tcp://host:port). Probably we can do the same thing\nhere and remove WithNetwork. The special treatment of dialing on AppEngine\n& monitor/log/control can be achieved in some other way.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/169#issuecomment-94134720.\n. On Mon, Apr 20, 2015 at 9:43 PM, David Symonds notifications@github.com\nwrote:\nMy latest suggested WithDialer should be passed a timeout, which would be\nset from WithTimeout. So they work together fine. It's then up to the\ndialer to use that timeout appropriately.\ngrpc.Dial can use the tls package directly, and tell it to use a dialer,\nwhich would be a closure invoking whatever is passed in via WithDialer. The\ncurrent code already does something vaguely along these lines; my proposal\nonly slightly generalises it.\ntls.DialWithDialer takes a *net.Dialer to support timeout. net.Dialer is a\nstruct. How can we use the user specified closure here? The only way I see\nis to user tls.Client and tls.Conn's Handshake(). What do I miss here?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/169#issuecomment-94636165.\n. On Mon, Apr 20, 2015 at 10:22 PM, David Symonds notifications@github.com\nwrote:\nYou don't have to use tls.DialWithDialer. It's a helper if you have a\nnet.Dialer. You can use tls.Client and tls.Handshake over an existing\nnet.Conn. That's only a few lines of code.\nThat is exactly what I said in the previous emails...  Then as I mentioned\nbefore, it is somewhat trouble some and has some code duplication (from\ntls.DialWithDialer) to support timeout for Handshake() -- you need to do\nsome subtraction to get the timeout value for Handshake and mimic what\ntls.DialWithDialer does for Handshake timeout ... It seems we basically\nduplicate the code of DialWithDialer here (into 2 pieces: dialer and\nhandshake) ... In general, this is fine with me and I can take on this\ntomorrow. It would be nice if you can point me to an example for app engine\ndialing so that I can be convinced this is a real demand and make sure  the\nnew one will be well fit for it. :) Thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/169#issuecomment-94641625.\n. I have not seen a strong reason to have a server-scoped context so far. Close this now.\n. all done.\n. Thanks for reviewing. Travis is down now (test worker is not spawned). will submit once it gets green.\n. Thanks for the PR. But I do not think this is the right behavior to handle the GOAWAY. According to http://http2.github.io/http2-spec/#GOAWAY, server should stop creating new stream on this transport but still keep processing the exsting active streams.\n. looks mostly good. My only concern is lack of test coverage.\n. I gave it a second think and think the current logic is not quite right. In my understanding, the handling of reception of GOAWAY is i) stopping to try to create new stream on the peer; and ii) keep processing the existing streams. On server side, it never tries to initiate streams on the client so that i) does not apply here. And it actually does ii) anyways. So I think the handling of GOAWAY on server side is simply no-op. The client side is different and more complex.\n\nIf this sound reasonable to you, we can either close this pr or change it to client side.\n. LGTM\n. On Mon, May 4, 2015 at 11:55 AM, Gian Biondi notifications@github.com\nwrote:\n\nI agree, the server shouldn't really do anything when it receives a GOAWAY\n(just don't throw an error). I've made the handler a no-op. If you accept\nit, this will close and then I'd like to implement it in the client then.\nAlthough under what condition would the client ever practically send the\nGOAWAY frame?\nThis is impl detail. AFAIK, c++ client send GOAWAY before it closes the\nconnection.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/180#issuecomment-98812597.\n. This is due to a bug on C side and will be fixed by the pr 1426. I created an issue in grpc/grpc and am closing this one.\n. I still do not understand why you require an interceptor ... It was deliberate to abandon interceptor in our design.\n. pls tell me how interceptors can help you address this issue?\n\nWe typically provide send(b []byte) to users instead of send(proto.Message)\nwhich provides more flexibility and the framework should not care what\nusers will to transmit.\nIf you are thinking about the encoding efficiency (e.g., memory alloc and\nstring copying), I think it is the work of protobuf should take care of.\nAnd I guess it is doable but it does not do because it will introduce\ntremendous complexity regarding memory model.\nOn Mon, May 11, 2015 at 1:55 PM, Hongchao Deng notifications@github.com\nwrote:\n\nNP. Let me try to explain better.\nIn TCP socket, we send bytes:\nsend(b []byte)\nNow we have protobuf and grpc, we want to send messages instead.\nsend(msg proto.Message)\nThe send(msg) API is provided by the framework, and message should be\ndefined by user.\nI wonder how can we do it efficiently without letting user define msg/rpc\non their own?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/187#issuecomment-101044182.\n. I guess calling Invoke(...) directly can fit your needs?\n. looking..\n\nOn Fri, May 8, 2015 at 7:45 AM, Russ Amos notifications@github.com wrote:\n\nI have a stream defined that will send messages perpetually to the client,\nuntil the client is no longer interested. Since there is no \"stream.Close\"\non the client side (only stream.CloseSend), I assume using\ncontext.WithCancel on the client side is the correct way to do this.\nHowever, it seems like cancelation is not being propagated to the server:\nThe server continues calling Send (with no errors) long after the client\nstops caring, and eventually the Send blocks entirely and the server's\ngoroutine hangs. I think the goroutines are unblocked and exit if the\nconnection is closed, but I didn't test that thoroughly.\nUnary RPCs seem to honor context cancelation.\nIs there some other way for the client to indicate it is no longer\ninterested in the stream?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/188.\n. ok, the right pattern to cancel a stream in your use case is as follows:\nfor {\n  reply, err := stream.Recv()\n  if err != nil {\n    ...\n  }\n  if A {\n    // not caring from now\n    ctx.Cancel()\n-  // YOU NEED TO KEEP READING UNTIL YOU GET CANCELED ERROR.*\n  for {\n    // stream.Recv() will make sure to cancel server side context if\n  necessary.\n    if _, err := stream.Recv(); err != nil {\n      break\n    }\n  }\n  }\n  }\nThis design is intended because other alternatives either increases a fair\namount of overhead (a lot of more goroutine spawned by grpc) or introduces\nmuch long latency to detect a cancellation. I should add this into the\ndocument anyways.\n\nLet me know if your issue gets addressed by this. Thank you for reporting.\nOn Fri, May 8, 2015 at 10:07 AM, Qi Zhao toqizhao@gmail.com wrote:\n\nlooking..\nOn Fri, May 8, 2015 at 7:45 AM, Russ Amos notifications@github.com\nwrote:\n\nI have a stream defined that will send messages perpetually to the\nclient, until the client is no longer interested. Since there is no\n\"stream.Close\" on the client side (only stream.CloseSend), I assume using\ncontext.WithCancel on the client side is the correct way to do this.\nHowever, it seems like cancelation is not being propagated to the server:\nThe server continues calling Send (with no errors) long after the client\nstops caring, and eventually the Send blocks entirely and the server's\ngoroutine hangs. I think the goroutines are unblocked and exit if the\nconnection is closed, but I didn't test that thoroughly.\nUnary RPCs seem to honor context cancelation.\nIs there some other way for the client to indicate it is no longer\ninterested in the stream?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/188.\n. fixed by #189 \n. In general, I have the following concerns about this:\ni) I think logger should be a global thing. It is not a proper DialOption or ServerOption.\nii) glog is google standard logging lib and it is hard if not impossible to make change to it. So following this approach, we probably need to wrap it by ourselves which introduces too many indirections.\niii) If we allow a custom logger which could be very complex and error-prone, then an logger bug could i) show as a mixed symptom (e.g., deadlock) to the user and hard to pinpoint it is due to logger; ii) performance degradation.\n. On Fri, May 8, 2015 at 1:47 PM, Peter Edge notifications@github.com wrote:\n\nI'll respond to these separately:\ni) It's not a DialOption or ServerOption really, we could make it some\nother type of option. On global - I agree, at the application level. The\nproblem is that grpc-go is a library, not an application, and so there\nneeds to be the ability to set logging separately. Go's http package, for\nexample, does not lock in any type of logging.\nOne compromise that could work, although is not optimal at least in my\nopinion, is we could have a global method on grpc such as grpc.SetLogger,\nthen at least a logger could be chosen. But again, my concerns about\napplication vs. library tell me at least that this should be a non-global\n(I really dislike globals :) ).\nI actually meant grpc lib scope when I said \"global\". Per-connection logger\nshould not exist. I think ServerOption might be okay but DialOption is not\nthe way to go. So if we do not allow different loggers between client and\nserver (or different servers), we can use grpc.SetLogger. Otherwise, the\nserver can use an ServerOption  and the client side can use\ngrpc.SetClientLogger.\nii) I'm not sure I follow the indirections - if we're worried about the\nfunction stack getting one level deeper, there's a ton of other places in\nthe code that this needs to be optimized. ie I don't think this is a\nconcern. I just added a commit that actually uses glog now as the default.\n\"a tons of other places in the code need to be optimized\" does not mean we\nshould make it worse. This is minor anyways. If this is an important\nfeature, let's add it.\niii). This one is definitely different. Most people in the go community\ndon't even seem to use golang's standard logger - docker, for example, uses\nlogrus. Other log packages allow more control, especially in terms of\noutput. For my application, I actually add semantics to all my logs so I\ncan idiomatically read them back, and if I were to use grpc-go, where my\nonly option is to send string logs to stderr, I would have to actually\nthrow the logs away. grpc-go has many applications in the future, including\nin the docker package, and this limitation might really hurt adoption.\nI agree.\nIf there is an issue with another logger, this is on the user - if I\nchoose to use a logger implementation that is not standard and there is a\nbug, that's my problem, not grpc's.\nThat's the user's problem but most of time the ball could be thrown to me\nwithout mentioning  a custom logger is used because they might think it is\nirrelevant ...  It is unfortunate but I can keep this into my check list to\nminimize the chance I run into this. :)\nOverall, I think the closest equivalent library would be net/http, and it\ndoes not lock in a logging implementation. Not having the option to use a\ndifferent logger will definitely affect adoption of grpc-go IMO, and hurts\nat least my ability to use it :)\nSure, let's do it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/190#issuecomment-100358174.\n. For simplicity, I would suggest going for SetLogger for now.\n\nOn Fri, May 8, 2015 at 2:22 PM, Peter Edge notifications@github.com wrote:\n\nAlright, so, should I do the SetLogger/SetClientLogger? Happy to refactor\nthis to do that. Thanks for all the quick replies, super appreciate it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/190#issuecomment-100372285.\n. On Fri, May 8, 2015 at 4:25 PM, Peter Edge notifications@github.com wrote:\nOf note, it would be nice if I could put this right in the grpc package,\nbut this would lead to cyclic dependencies (transport <-> grpc, for one) -\nmaybe a bigger overall structural issue we could address?\nIt is okay to put it into a separate package.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/192#issuecomment-100393095.\n. btw, having a separate transport package is intended for some proxy type\nusage which does not care the upper layer semantics (in grpc package).\n\nOn Fri, May 8, 2015 at 5:08 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nOn Fri, May 8, 2015 at 4:25 PM, Peter Edge notifications@github.com\nwrote:\n\nOf note, it would be nice if I could put this right in the grpc package,\nbut this would lead to cyclic dependencies (transport <-> grpc, for one) -\nmaybe a bigger overall structural issue we could address?\nIt is okay to put it into a separate package.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/192#issuecomment-100393095.\n. This should not happen. I quickly had a glance at the code:\n\n\nThe loop at https://github.com/grpc/grpc-go/blob/master/server.go#L193 will\nexit only either https://github.com/grpc/grpc-go/blob/master/server.go#L196\nor https://github.com/grpc/grpc-go/blob/master/server.go#L210. But I cannot\nthink your example triggered this.\nIt would be good to show me a reproducible example to debug this.\nOn Sun, May 10, 2015 at 10:17 PM, zoutaiqi notifications@github.com wrote:\n\n@iamqizhao https://github.com/iamqizhao,\nClient send request message to server per second, and server return the\nresponses back, two side are stream mode. to reproduce this issue, I\ncomment out the recv response message from server on client side, keep it\nseveral seconds, then killed client, server quit silently almost at the\nsame time, no core dump file found on server side. I did this just want to\nknow how server handle this scenario that client crashed(no deadline or\ntimeout set in context on client side, and CloseSend() will not be called\ndue to client crashed)? will the goroutine quit or just block on server\nside? thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/193.\n. On Mon, May 11, 2015 at 10:56 PM, zoutaiqi notifications@github.com wrote:\nSorry, it should be caused by my shell script, it works well if the server\nstarted by manual, thank you for your quick reply. and I have another\nquestion:\nif client crashed before streaming rpc finished, will the server detect it\nand clean the related resource(goroutine)? I noticed that the rpc goroutine\nblocked at stream.Recv on server side after client crashed, did that\nexpect? thanks!\nIt depends. If the client kernel gets a chance to send TCP RST to the\nserver, the connection on the server side will  be torn down also and you\nshould not see any streams blocking on stream.Recv. If the client kernel\ndoes not get chance (e.g., network cable unplugging, power outage, etc.),\nthe server has no idea what happened on the client side until some tcp user\ntimeout gets triggered and you could see the blocking on stream.Recv in\nthis case for some minutes. The latter case should be addressed by a health\nchecking mechanism which is on my radar but pending for implementation.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/193#issuecomment-101141534.\n. These logs are not useful. Can you add some logging to the exit points of\nthis loop:\nhttps://github.com/grpc/grpc-go/blob/master/server.go#L193\nand rerun your stuffs? Then if it happens again, we know where grpc exits.\nIf it happens without any of these logs, it means your program terminates\nthe process instead of grpc.\n\nOn Fri, May 15, 2015 at 1:15 AM, zoutaiqi notifications@github.com wrote:\n\nHI, @iamqizhao https://github.com/iamqizhao , I got the same problem\nafter I fixed the script issue, and I got a very little output when they\nquit, I don't know if there is any useful information for you debug.\nenv:\nI have 7 node as cluster in the test bed, every node just exchange three\nsimple message each other as streaming Client and Server, they ran normally\nabout 3 days, then one node quit with unknown reason(maybe the same issue),\nand the other nodes were trying to reconnect it, then I restart it and try\nto rejoin the cluster, then almost all of nodes quit here without core dump:\n.....\ngoroutine 621 [select, 9 minutes]:\ngoogle.golang.org/grpc/transport.(*http2Server).controller(0xc20827e380)\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:594\n+0x551\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:132\n+0x88a\ngoroutine 784 [IO wait]:\nnet.(\n_pollDesc).Wait(0xc2082a2450, 0x72, 0x0, 0x0)\n/usr/local/go/src/net/fd_poll_runtime.go:84 +0x47 net.(_pollDesc).WaitRead(0xc2082a2450,\n0x0, 0x0)\n/usr/local/go/src/net/fd_poll_runtime.go:89 +0x43\nnet.(\n_netFD).Read(0xc2082a23f0, 0xc2083a1cd4, 0x9, 0x9, 0x0, 0x7fd8d2f03bf0,\n0xc208364100) /usr/local/go/src/net/fd_unix.go:242 +0x40f net.(_conn).Read(0xc2082a6020,\n0xc2083a1cd4, 0x9, 0x9, 0x0, 0x0, 0x0)\n/usr/local/go/src/net/net.go:121 +0xdc\nio.ReadAtLeast(0x7fd8d2f06730, 0xc2082a6020, 0xc2083a1cd4, 0x9, 0x9, 0x9,\n0x0, 0x0, 0x0)\n/usr/local/go/src/io/io.go:298 +0xf1\nio.ReadFull(0x7fd8d2f06730, 0xc2082a6020, 0xc2083a1cd4, 0x9, 0x9,\n0x1000100000147, 0x0, 0x0)\n/usr/local/go/src/io/io.go:316 +0x6d\ngithub.com/bradfitz/http2.readFrameHeader(0xc2083a1cd4, 0x9, 0x9,\n0x7fd8d2f06730, 0xc2082a6020, 0x0, 0x0, 0x0, 0x0)\n/home/elc/go/src/github.com/bradfitz/http2/frame.go:228 +0xa2\ngithub.com/bradfitz/http2.(\n_Framer).ReadFrame(0xc2083a1cb0, 0x0, 0x0, 0x0, 0x0)\n/home/elc/go/src/github.com/bradfitz/http2/frame.go:373\nhttp://github.com/bradfitz/http2/frame.go:373 +0xf2\ngoogle.golang.org/grpc/transport.(\nhttp://google.golang.org/grpc/transport.(_framer).readFrame(0xc2082aa210,\n0x0, 0x0, 0x0, 0x0)\n/home/elc/go/src/google.golang.org/grpc/transport/http_util.go:436 +0x50\ngoogle.golang.org/grpc/transport.(\n_http2Server).HandleStreams(0xc20827e100, 0xc2082b9380)\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:244\nhttp://google.golang.org/grpc/transport/http2_server.go:244 +0x691\ngoogle.golang.org/grpc.func\u00c2\u00b7014()\nhttp://google.golang.org/grpc.func%C3%82%C2%B7014()\n/home/elc/go/src/google.golang.org/grpc/server.go:208\nhttp://google.golang.org/grpc/server.go:208 +0xc5 created by\ngoogle.golang.org/grpc.( http://google.golang.org/grpc.(_Server).Serve\n/home/elc/go/src/google.golang.org/grpc/server.go:212 +0x5f5\ngoroutine 783045 [select, 9 minutes]:\ngoogle.golang.org/grpc/transport.(*http2Server).controller(0xc20827e100)\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:594\n+0x551\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:132\n+0x88a\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/193#issuecomment-102312737.\n. I need more info to move forward. Can u elaborate what you were doing, what\nyou expect to see and what u observed?\n\nOn Mon, Sep 14, 2015 at 12:06 PM, Derek Perkins notifications@github.com\nwrote:\n\n@iamqizhao https://github.com/iamqizhao @zoutaiqi\nhttps://github.com/zoutaiqi I'm getting the same \"transport:\nhttp2Client.notifyError got notified that the client transport was broken\nEOF.\", while not triggering an error. I'm running on App Engine Managed VMs\nusing BigQuery and Bigtable.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/193#issuecomment-140178688.\n. You can think it is a tcp connection. Close it when you do not need it. You\ndo not need to close it in your each functions if there are multiple\nfunctions using this connection.\n\nOn Tue, May 12, 2015 at 1:26 AM, mitiger notifications@github.com wrote:\n\nhello,\nin my grpc client , if i need to Dial(\"server address\"), and then Close\nthe ClientConn when in the end of my func or not?\nthen , every times, i open then connection and the close the connection in\nmy each functions ???\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/194.\n. no idea how you got impression metadata and auth are tied to ClientConn,\nwhich is wrong. Metadata is definitely per rpc. Auth depends -- for some\ntypes of auth such as SSL/SSH/TLS, they have to be per connection. Some\nothers such as oauth2 are per-rpc.\n\nOn Tue, May 12, 2015 at 8:32 AM, Quinn Slack notifications@github.com\nwrote:\n\nBecause metadata and auth are tied to a ClientConn in the current Go API\n(because they're passed to Dial), you can't reuse/pool underlying TCP\nconnections for requests with differing metadata and auth credentials.\nConsider the case where you want to attach user credentials and a request\ntracing ID (Dapper-style) to each gRPC call. Even though there might be a\nlarge number of already open ClientConns, you need to create a new one (and\naccompanying underlying TCP connection) for each unique pair of user\ncredentials and tracing ID. Even if you quickly close the TCP connections,\nthey still stick around in TIME_WAIT state and can quickly exhaust the\ntotal number of ports (1000 public API requests per second, each public API\nrequest subsequently makes 5 other API calls with unique Dapper span IDs --\nyou will exhaust your port space in less than 11 seconds, even assuming all\n65,535 ports are available).\nThe net/http package's solution to this issue is to separate the Client\n(which handles connection reuse) from the Transport (which can be used to\ninject user auth and tracing IDs into, e.g., HTTP headers).\nAm I missing something, or is this indeed a limitation of the current API?\nI can think of pooling workarounds but none that work if you inject\nhigh-cardinality parameters (such as a Dapper span ID).\nOtherwise, gRPC is awesome. :) Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/195.\n. How to handle user-agent header is still under discussion. We have not\ndecided among i) grpc reserves it. do not allow users overwrite; ii) grpc\ndoes not reserve it. it is simply a metadata and ii) grpc reserves it but\nallows users overwrite. Will keep you posted.\n\nOn Thu, May 14, 2015 at 7:00 PM, David Symonds notifications@github.com\nwrote:\n\nIt'd be good to have some way to control the User-Agent used by the gRPC\npackage, whether on a per-ClientConn or per-RPC basis (I don't care which).\nThe use case is for client libraries that wrap gRPC to be able to indicate\nwho they are.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/198.\n. After some discussion, we will only support per-channel basis for now. It looks like\ngrpc-go/1.2.3 application-user-agent\n\nI will have a PR shortly to implement this.\n. On Wed, Jul 22, 2015 at 2:46 PM, David Symonds notifications@github.com\nwrote:\n\nBy \"per-channel\" you mean per-ClientConn? That's fine with me.\nyup\nNote that http://tools.ietf.org/html/rfc7231#section-5.5.3 specifies that\nUser-Agent should be ordered in descending order of identification\nspecificity, so the application part should come first, and the\ngrpc-go/1.2.3 tag should be added after that.\nI will bring this up to the team. Thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/198#issuecomment-123876128.\n. closed via #255 \n. I will keep this order until the team reaches the agreement on this.\nCurrent all the other implementations use the current order.\n\nOn Fri, Jul 24, 2015 at 3:30 PM, David Symonds notifications@github.com\nwrote:\n\nYour #255 https://github.com/grpc/grpc-go/pull/255 put the User-Agent\nstrings in the wrong order. Please fix.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/198#issuecomment-124748598.\n. otherwise, the corresponding interop test will fail.\n\nOn Fri, Jul 24, 2015 at 3:39 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nI will keep this order until the team reaches the agreement on this.\nCurrent all the other implementations use the current order.\nOn Fri, Jul 24, 2015 at 3:30 PM, David Symonds notifications@github.com\nwrote:\n\nYour #255 https://github.com/grpc/grpc-go/pull/255 put the User-Agent\nstrings in the wrong order. Please fix.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/198#issuecomment-124748598.\n. closed via #257 \n. Hi David,\n\n\nCool, I actually thought about it too. My only concern (also the reason I\nthought about it but did not work on it) is that it could be very different\nfrom the tracing stuff quite a few ppl here are working on (currently for\nC++ only but has plan to apply to other languages) so that we probably have\nto rewrite it eventually.\nAnyways, If you already got it done, I think it is very helpful to have\nright now. Thanks for the contribution.\nOn Thu, Jun 4, 2015 at 6:30 PM, David Symonds notifications@github.com\nwrote:\n\nFinally got it done. I plan to tie grpc-go into\nhttps://godoc.org/golang.org/x/net/trace.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/199#issuecomment-109118547.\n. fixed via #203 \n. @bcmills\n\nBryan,\nPTAL. Thanks!\n. seeing a tricky counting issue when closing a stream. Fixing ... will ping this back once it is done.\n. okay, all done. PTAL. Thanks.\n. This blocks my another change now. I submit this now. And will work with Bryan to improve/simplify the flow control logic shortly.\n. 1st run done. Please ping back when you address all the comments. Thanks!\n. 2nd run done. mostly good. \n. I actually thought it is mostly transparent to the users (hardly think of\nany case which requires Dial to actually connect the server). Added some\ncomments to Dial to make this clear. This behavior will be consistent\nacross C, Java and Go. Sameer Bryan and I had a discussion on this long\ntime ago and we agreed that wait skipping should be preferred default\nbehavior.\nOn Mon, Jun 1, 2015 at 4:23 PM, David Symonds notifications@github.com\nwrote:\n\nThis is a pretty significant change in the API. Was there discussion\nsomewhere?\nI wonder if this will break existing code that is expecting Dial to\nactually establish a connection. As it is, I don't see any comment change\non Dial that makes this clear.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/208#issuecomment-107745113.\n. On Mon, Jun 1, 2015 at 4:46 PM, David Symonds notifications@github.com\nwrote:\nIt's a bit unexpected if your program is already structured to set things\nup first, and then serve requests by making outgoing RPCs after all that.\nThis adds a performance hit to that first request when you otherwise had\nexpected it to be taken care of during setup. Passing grpc.WithBlock will\ntake care of that now, but it also means that every client library that\nbuilds on top of gRPC needs to have a mechanism to pass that through too.\nThe second aspect that makes this feel weird to me is error handling:\nGenerally, if dialing outright fails, it is a permanent failure (e.g.\nmalformed address), and your program can't do anything else at that point;\nhowever, an RPC failing is often not a permanent problem, and a program can\nkeep going. Pushing the actual dial back to the first RPC means that\nprograms need to put more thought into handling RPC errors than they would\notherwise have to, which means most programs will behave poorly.\nI'm not too fussed about being consistent with C and Java on every last\ndetail. This might be a situation where it makes sense to be consistent,\nbut just saying \"C and Java also do this\" isn't a trump card, because we\nintentionally diverge from them in many places where we judge it to be\nbetter.\nI did not mean I am doing this because \"C and Java also do this\". I was\njust telling the situation. It is is a reason but not \"a trump card\". The\ndefault would be the one most users choose to use. According to my\nexperience on google internal rpc framework in C++ and what I was told from\nBryan and Sameer regarding Go internal, it seems skip wait is the way most\nppl preferred. It saves some resources (1 fd, some mem ~8KB) and is\nessential for load balance channel when it consists of a large number of\nsubchannels.\n@Sajmani https://github.com/Sajmani @bcmills\nhttps://github.com/bcmills in case you guys want to enlighten me here.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/208#issuecomment-107748256.\n. Moved this discussion to internal temporarily ...\n\nOn Mon, Jun 1, 2015 at 5:16 PM, David Symonds notifications@github.com\nwrote:\n\nI have a different experience inside Google to Bryan and Sameer, but I'd\nstill like to hear a concrete reason why a non-blocking dial is a better\ndefault. Maybe there's a good reason and I've simply forgotten it.\nSaving a small amount of memory doesn't seem like a big pay-off, and most\nuses of grpc.Dial are dialing a TCP connection so load balancing is\nirrelevant for them (since the multiplexing happens in the single HTTP2\nconnection, not over multiple TCP connections).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/208#issuecomment-107751484.\n. Will have a new one according to the decision via the internal discussion.\n. Is it possible to add a end2end test for this?\n. ready for another pass?\n. I think 5% is already significant so that we need to take some action on it.\n\nWhat payload size did you use in the benchmark? You should enlarge it to\nbenchmark difference. Now it is 1 byte. You can try something like 1kB or\n8kB.\nOn Mon, Jun 8, 2015 at 11:36 AM, David Symonds notifications@github.com\nwrote:\n\nHere's a benchmark comparison between enableTracing = false and enableTracing\n= true:\nbenchmark                     old ns/op     new ns/op     delta\nBenchmarkClientStreamc1       78052         77161         -1.14%\nBenchmarkClientStreamc8       52947         53064         +0.22%\nBenchmarkClientStreamc64      51948         51674         -0.53%\nBenchmarkClientStreamc512     57291         56697         -1.04%\nBenchmarkClientUnaryc1        143743        150490        +4.69%\nBenchmarkClientUnaryc8        113032        118492        +4.83%\nBenchmarkClientUnaryc64       109480        114804        +4.86%\nBenchmarkClientUnaryc512      101987        107993        +5.89%\nI suspect the ~zero impact on streaming is because everything isn't hooked\nup, but it shows that the impact on this change is ~4-5%. That's relatively\nsmall for such a big feature, and is an even smaller impact on a program\nthat isn't simply benchmarking the gRPC code.\nI am happy to keep the unexported bool here to make it easy to benchmark\nthis in the future, but I'd prefer to not export it right now. The API for\nthis package is big enough, and it'd be nice to not grow the API unless\nthere's clear impact that requires it. It's easy to come back later and\nexport it if someone is actually having problems due to tracing.\nPTAL.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/210#issuecomment-110102081.\n. Hi David,\n\nThanks for doing this. For rpc frame benchmark (for both google internal\nand grpc), we target micro-second optimization (for C++, sometimes we do\nsub-micro optimization). One reason is that some usage is on the same host\nvia some IPC or even in process. For those case, network time is close to 0\nand the efficiency of grpc runtime is critical. So persistent ~5% increase\nis a big deal and we should take some action. Please have a switch with\ndefault off. @yangzhouhan is the summer intern here working on grpc-go\nperformance. She can take a close look after your submission.\nOn Mon, Jun 8, 2015 at 12:03 PM, David Symonds notifications@github.com\nwrote:\n\nHere's a comparison with 1 KB payloads. Note that I had to run with\n-benchtime=10s to get something even reasonably stable, though note that\nthere's still a lot of noise. We're looking at small numbers here (~120-170\nmicroseconds per op), and that's tiny compared to actual network speeds.\nYou can even see that this makes the 1-concurrent version appear to get\nfaster with tracing turned on. This is exercising the Go runtime as\nmuch as gRPC itself.\nbenchmark                     old ns/op     new ns/op     delta\nBenchmarkClientUnaryc1        175516        170908        -2.63%\nBenchmarkClientUnaryc8        125572        132750        +5.72%\nBenchmarkClientUnaryc64       135132        143311        +6.05%\nBenchmarkClientUnaryc512      158194        167593        +5.94%\nI can try benchmarking across machines later today. I suspect the delta\nwill appear very close to zero under even vaguely realistic situations.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/210#issuecomment-110108713.\n. On Mon, Jun 8, 2015 at 6:14 PM, David Symonds notifications@github.com\nwrote:\nI've exported EnableTracing, defaulting to true. I think that's the right\ncompromise. Almost all users of grpc-go will find ~7\u00b5s to be an utterly\ntrivial amount of time for a big benefit, and the rare bit of code that\nboth only does loopback networking and also almost no other work can set it\nto false easily enough.\nokay ... I do think it should be true eventually but probably not at this\ntime. Anyways, sgtm.\n\nready for review?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/210#issuecomment-110188116.\n. You can attach a http server to your binary (e.g.,\nhttps://github.com/grpc/grpc-go/blob/master/benchmark/client/main.go#L146).\nAnd then while your binary is running,  you can point your brower to\nhttp://localhost:XXXX/debug/request to get the tracing info (replace XXXX\nwith the profiling port you get).\n\nOn Tue, Sep 15, 2015 at 6:50 PM, Peter Edge notifications@github.com\nwrote:\n\n@dsymonds https://github.com/dsymonds is there any other documentation\nyou can point me to? I'm not sure how to use this still\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/210#issuecomment-140596993.\n. We also need to be able to run on multiple machines (https://github.com/grpc/grpc-go/blob/master/benchmark/server/main.go#L31).\n. sure, my proposal is to change the signature of benchmark.StartServer to\n\nfunc StartServer(addr string) func()\nso benchmark_test.go can pass \"locahost:0\" as \"addr\" and\na standalone benchmark server will pass a server_addr flag whose default is\n\":0\".\nOn Sun, Jun 7, 2015 at 9:09 PM, David Symonds notifications@github.com\nwrote:\n\nDo you have an alternate solution here? Perhaps running the benchmark\npieces on different machines should require passing a flag?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/212#issuecomment-109855388.\n. @yangzhouhan\n. @yangzhouhan \n\nYou probably also need to fix this in your open-loop benchmark in preparation.\n. Thanks for the fix.\n. @dsymonds \nDavid mentioned to me that dialer should work for appengine. Any particular issue for server (I do not have any experience with appengine)?\n. glog is the default logger in the library... \n. I think it is a good and convenient choice for most of users ...\nOn Mon, Jun 8, 2015 at 11:31 AM, Xiang Li notifications@github.com wrote:\n\n@iamqizhao https://github.com/iamqizhao Why do we use glog as the\ndefault logger? For performance reason? As #217\nhttps://github.com/grpc/grpc-go/issues/217 said, it registers some\nunwanted flags.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/218#issuecomment-110100538.\n. On Mon, Jun 8, 2015 at 11:43 AM, Tamir Duberstein notifications@github.com\nwrote:\nIt is a problem for some users and is impossible to work around.\nWhy? You can set whatever logger you want.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/218#issuecomment-110103765.\n. On Mon, Jun 8, 2015 at 11:47 AM, Qi Zhao toqizhao@gmail.com wrote:\nOn Mon, Jun 8, 2015 at 11:43 AM, Tamir Duberstein \nnotifications@github.com wrote:\n\nIt is a problem for some users and is impossible to work around.\nWhy? You can set whatever logger you want.\n\noh, you meant no way to avoid flag pollution ... hmm...\n\u2014\n\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/218#issuecomment-110103765.\n. I got the points from you guys. Let me think about it.\n\n\nOn Mon, Jun 8, 2015 at 11:49 AM, Xiang Li notifications@github.com wrote:\n\n@iamqizhao https://github.com/iamqizhao Yes.\nTake a look at\nhttps://github.com/golang/glog/blob/master/glog.go#L398-L411\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/218#issuecomment-110105149.\n. @adg @okdave\n\nHi Andrew and David,\nCould you guys have a proposal on how to improve credentials package? Thanks. I hope to get it done before beta release. \n. LGTM, but it might break some existing users. Let me check.\n. Hi Tamir,\nSorry about the delay. Since this PR will probably break google cloud\nbigtable in GO (\nhttps://github.com/GoogleCloudPlatform/gcloud-golang/tree/master/bigtable)\nwhich is in Beta. Please talk to @dsymonds to reach agreement before we\nproceed here.\nOn Thu, Jun 11, 2015 at 11:43 AM, Tamir Duberstein <notifications@github.com\n\nwrote:\nEr, I meant to ping here.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/220#issuecomment-111236249.\n. Sure, I will make enable/disable as separate benchmark suites shortly.\n\nOn Wed, Jun 10, 2015 at 5:32 PM, David Symonds notifications@github.com\nwrote:\n\nThat doesn't seem wise to always disable tracing when using that\nbenchmark package. We should make sure that most benchmarking keeps using\nit because it's the default. Individual benchmarks can disable it if they\nwant, but a benchmark should try to be representative, and this pull\nrequest makes all the benchmarks less representative.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/222#issuecomment-110954878.\n. @yangzhouhan \n. This is no-op if users use protobuf as IDL because it already guarantees the success of this sanity check. This change is for people who wants to call Server.RegisterService directly without a proto file. One use case is that we are making health check mechanism which is treated as an internal service hosted by a gRPC server. It is a rpc service but does not use codegen plugin to generate code. I found that by removing this check we can significantly reduce the complexity of the code (because the generated code introduces a lot of unnecessary indirection in this case). This is probably just a short term solution (to speed up the dev of my intern). I will figure out safer way to do this when all the pieces are on the spot.\n. I would claim that it is the responsibility of the caller of\nRegisterService to pass a working server instance. RegisterService function\nitself does not have obligation to check for the caller.\ni) if the user uses protobuf as IDL, this is guaranteed by protobuf codegen;\nii) if the user uses some other IDL, that IDL should guarantee that too;\niii) if the user wants to call RegisterService directly without any IDL,\nthe user himself should make sure.\n\nOtherwise, the program could be panic. The difference between the existing\ncode and this PR is just when to fatal/panic.\nOn Fri, Jun 12, 2015 at 1:38 PM, David Symonds notifications@github.com\nwrote:\n\nSubverting a safety mechanism for short term velocity is almost always the\nwrong tradeoff.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/224#issuecomment-111607625.\n. okay, we agree that this is optional and I agreed that this check brings\nsome goodness as I wrote in the PR desc. I think I probably have a way to\nkeep this check. Actually what I want to do (not in this PR) is that if the\nuser calls RegisterService and passes a server with method A, B only (in\nproto file , A, B, C are defined) and grpc client sends a request to C, the\nclient will get a \"not implemented\" error instead of the failure on this\nsanity check in the first place.\n\nOn Fri, Jun 12, 2015 at 2:24 PM, David Symonds notifications@github.com\nwrote:\n\nIt is indeed the responsibility of the caller to ensure they have a\nwell-formed server value, but the point of this safety check is to help\nthem get that right up front rather than them making a mistake and having a\nticking time bomb in their server. The IDL turns the up front dynamic check\ninto a static check as an additional layer.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/224#issuecomment-111622199.\n. Currently on server side you can use Metadata API FromContext(...) (\nhttps://github.com/grpc/grpc-go/blob/master/metadata/metadata.go#L141 )to\nextract oauth token. In the future, we probably will have dedicated API to\ndo that.\n\nOn Mon, Jun 15, 2015 at 1:01 PM, Donald King notifications@github.com\nwrote:\n\n\"google.golang.org/grpc/credentials\".Credentials seems to be oriented\naround attaching credentials to an outgoing client connection. What I want\nis to write a server that e.g. validates the client's JWT token against my\nlist of authorized clients.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/227.\n. Hi Zhouhan,\n\nLet's revise the design a bit.\ni) make a \"health\" subdir and place health.proto and its generated code health.pb.go there (rename the package to \"health\" instead. I will change the designdoc correspondingly). Making a health.go in this dir to implement HealthCheck.Check server handler as like other servers do.\nii) Move the current code in health.go to the end of rpc_util.go. Then only thing left is to make Check and Enable (rename them to HealthCheck and EnableHealthCheck respectively) protobuf agnostic:\n- change Check(...) to func HealthCheck(ctx context.Context, req interface{}) error to let the caller create request object;\n- let EnableHealthCheck(...) call the generated code so that itself does not need to create HealthCheckRequest/Response messages.\nLet's discuss tomorrow morning if I missed something or did not make it clear enough in the above.\n. The signature of EnableHealthCheck needs to be changed to \nfunc EnableHealthCheck(sd *ServiceDesc, ss interface{})\n. This is still problematic because of circular dependency. We need to remove it somehow. Let's talk tomorrow.\n. The quick fix would be moving the entire ii) to health package. \n. 1st pass done\n. LGTM\n. why is this needed? We should minimize the exported methods.\n. On Fri, Jun 19, 2015 at 10:55 AM, Ross Light notifications@github.com\nwrote:\n\nIf you're trying to write functions that operate over any stream, then you\ncan't write to the stream without having the ClientTransport. You're\nforced to pass around the triple of (server transport, client transport,\nstream). This would allow clients of Stream to only need the stream.\nYup, I thought about it too when I had to add ServerTransport for streaming\nrpc. But It is a non-trivial refactoring of the current code. To me, it is\nunpleasant to have what you have here as a separate PR. It will only\nconfuse the lib developers without the refactoring work I just mentioned\nstanding up.  I would suggest merging them as a single PR.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/231#issuecomment-113589446.\n. sorry for long delay. I am back from vacation today.\n. As I said, your intention sounds good to me. Please reach out dsymonds before proceeding (I did not see you guys reached agreement on another thread).\n. rpc is identified with a unique id. retry will issue a brand-new rpc with a different id. Let me know if I missed something.\n. Now I am back from vacation. I will update the issue once I finalize the design of LB.\n. I am working on it actively. It is a big surgery to the existing code and\nneeds a couple of rounds of design discussion. So please hold.\n\nJust FYI, our decision is that we won't make grpc.ClientConn an interface,\nwhich is too coarse. We will make it happen in the underlying\nimplementation but we do allow ppl implement their own load balancer.\nOn Sat, Sep 5, 2015 at 7:36 AM, Michal Witkowski notifications@github.com\nwrote:\n\nAny news here? We could lend a hand implementing DNS SRV :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/239#issuecomment-137960632.\n. 1. Conn is an abstraction connecting to a single destination (i.e., at most 1 underlying transport at any time); a ClientConn may be consistent of 1 or more Conn;\n2. This is not the default behavior and you need to make your own picker impl by deferring NewConn call;\n3. Without the concrete requirements I cannot give the perfect answer. But I think this also can be achieved using a custom picker impl.\n\nNote that Picker is still in experimental stage and may be subject to some revisions.\n. https://github.com/grpc/grpc/blob/master/doc/load-balancing.md is the direction for all gRPC implementations. The reason this work is kinda stuck now is because we observed some security concerns in that design and some major revision on the original design. Therefore, I stopped working on that part and will wait until the new design is out. We are working on that and we hope to get the new design done by the end of this month. Sorry about the delay.\n. Sorry for late response on this. Let's try to have an agreement on the solution.\nI am kind of lost in the discussion. Concretely, what is the problem of the current Picker and ClientConn?\n. I treat \"Dial\" as a function call to dial a communication channel (i.e., ClientConn) for users so that users can put rpcs onto it. I think it is not necessary for Dial to create a real TCP connection or something similar, which is implementation details and should be transparent to users.\nIt is not clear to me what \"loss of control over Conn objects\" means. Actually what @stevvooe  described in his first paragrah is the current model. ClientConn uses a picker to provide Conn instances.\n. > The term Dial, in every other Go project, means to create some sort of connection. When this is not the case, other terms, such as New or Open (see database/sql), are used. It is confusing to it to do anything else, and I've seen several developers stumble on this distinction. Furthermore, most of the \"dial options\" actually affect stream behavior and have nothing to do with dialing.\nIn grpc-go, I would say \"Dial\" creates ClientConn to perform rpcs. I want to emphasize that the connection could be an abstraction instead of a real tcp connection. And I think Dial still fits well here.\nSome stream behavior options here is to provide a connection-scoped config so that users do not need to configure every rpc on that connection.\n\nIt seems like the model that is followed here is that of database/sql or net/http.Transport. Both of these pool connections behind the scenes but can be composed to provide more sophisticated behavior. For example, one can replace the entire transport implementation for an HTTP client to get interesting behavior. Yes, there are problems with this model, but it should be used to inform the API of this package.\n\nWe are talking about different abstractions here. For gRPC-Go, we have\ni) ClientConn: contains all the control plane logic (e.g., create and pick Conn) to manage multiple Conn;\nii) Conn: contains all the control plane logic (e.g., reconnect) to manage a single Transport;\niii) Transport: data plane for real data flowing. And yes, you can replace the entire transport impl at the level. One of gRPC's goals is to support various transports (e.g., HTTP2, QUIC, etc.)\nPicker (part of ClientConn) is working on top of Conn layer (instead of Transport layer) so that we still have per-transport control logic.\n. > @iamqizhao Respectfully, I think you're completely missing the point here. There is a well-established convention that Dial does a certain thing. The behavior here has caught out every experienced Go programmer I've worked with on GRPC related code.\nok, I would buy this. I probably can tune the code so that once Dial is returned at least 1 network connection is started (or setup if grpc.WithBlock() is set.).\n\nWe aren't at all. I implore you to go back and thoroughly read my commentary. There are a few tweaks here that can expose the right abstractions. Many of these problems can be alleviated by leveraging the well-thought patterns from the standard library and extending them where needed.\n\nThank you very much for your thoughts and contributions here. It is very helpful on finalize the design. Keep in mind that we even have not finalized the design of load balancing story for gRPC yet (new issues keep jumping out.). So this is really a big moving piece right now.\nYour proposal seems missing some requirements. For example, how can a user add his custom load balancing scheme in your proposal (e.g., he wants to do weighted round-robin on the list of addresses returned by the name resolver. )? In my understanding, he needs to create his own invoker implementation (in his own package) but lacks the building blocks because there are no ClientConn and Conn or something similar.\nMore importantly, let me clarify some hard requirements here before we proceed any design proposals: i) we must NOT break any existing user-facing API except the experimental ones; ii) ClientConn or its counterpart cannot be an interface because we do not want to mislead users to let them feel they can make their own custom impl. The current Picker design was designed under these hard restrictions. To be honest, I am not satisfied with it too and do plan to improve/fix it in short team.\nIt seems github issue is not a good place to discuss the design issues like this. I will try to draft my ideas into a google doc and have the discussion there early next week.\n. I do not like the idea to fuse rpc invoking and connection management together and do not think it is necessary. In my understanding, the core issue bothering you is that the ClientConn, Conn and Picker do not have desirable and clear abstraction and interface. I am going to try to have a proposal to address/improve it. Since the very similar thing works very well inside Google, I do believe this is addressable without ruining the existing user-facing API. \n. > I apologize for not making a very clear point, as my proposal is to decouple them. By declaring an interface, invocation and connection become separate components.\nI meant you put both of them into a single \"Client\" struct. \n\nGoogle has a lot of internal infrastructure, such as machined-local load balancing, that can help to make this particular abstraction work very well. In the outside world, the environments are not nearly as homogeneous. The interfaces and abstractions must be much more flexible to work in the myriad environments in which GRPC may now find itself in. For example, in Google may find it acceptable to deploy a separate load balancing process that can manipulate IP tables to route RPC requests, but this may be impossible in another environment.\n\nGoogle has remote load balancing too. In our load balancing design, we do not introduce any new models beyond what we have seen inside Google. We are happy to know if there are outliers it does not cover. So far we have not found any.\n. @stevvooe  Can you send your email address to me (zhaoq@google.com) so that I can share some doc with u?\n. doing final polishing and it will be out this week.\n. We use https://godoc.org/golang.org/x/net/trace to gather these stats. We\nalready have the minimal instrumentation for both unary and streaming RPCs\non client side. Can you take a look whether it meets your requirement. If\nnot, we can probably wrap it to allow custom impl.\nOn Wed, Jul 8, 2015 at 11:46 AM, Michal Witkowski notifications@github.com\nwrote:\n\nWe're currently experimenting with GRPC and wondering how we'll monitor\nthe client code/server code dispatch using Prometheus metrics\nhttp://prometheus.io/docs/concepts/data_model/ (should look familiar ;)\nI've been looking for a place in the grpc-go to be able to hook up\ngathering of ServiceName, MethodName, bytes, latency data, and found none.\nReading upon the thread in #131\nhttps://github.com/grpc/grpc-go/issues/131 about RPC interceptors, it\nis suggested to add the instrumentation in our Application Code (a.k.a. the\ncode implementing the auto-generated Proto interfaces). I see the point\nabout not cluttering grpc-go implementation and being implementation\nagnostic.\nHowever, adding instrumentation into Application Code means that either we\nneed to:\na) add a lot of repeatable code inside Application Code to handle\ninstrumentation\nb) use the callFoo pattern described in #131\nhttps://github.com/grpc/grpc-go/issues/131 proposed [only applicable to\nClient]\nc) add a thin implementation of each Proto-generated interface that wraps\nthe \"real\" Proto-generated method calls with metrics [only applicable to\nClient]\nThere are downsides to each solution though:\na) leads to a lot of clutter and errors related to copy pasting, and some\nof these will be omitted or badly done\nb) means that we loose the best (IMHO) feature of Proto-generated\ninterfaces: the \"natural\" syntax that allows for easy mocking in unit tests\n(through injection of the Proto-generated Interface), and is only\napplicable on the Client-side\nc) is very tedious because each time we re-generate the Proto (add a\nmethod or a service) we need to go and manually copy paste some boiler\nplate. This would be a huge drag on our coding workflow, since we really\nwant to rely on Proto-generate code as much as possible. And also is only\napplicable on the Client-side.\nI think that cleanest solution would be a pluggable set of callbacks on\npre-call/post-call on client/server that would grant access to ServiceName,\nMethodName and RpcContext (provided the latter stats about bytes\ntransferred/start time of the call). This would allow people to plug an\ninstrumentation mechanism of their choice (statsd, grafana, Prometheus),\nand shouldn't have any impact on performance that interceptors described in\n131 https://github.com/grpc/grpc-go/issues/131 could have had (the\ndouble serialization/deserialization).\nHaving seen how amazingly useful RPC instrumentation was inside Google,\nI'm sure you've been thinking about solving this in gRPC, and I'm curious\nto know what you're planning :)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/240.\n. @Sajmani @matttproud As long as bufSize is configurable, I do not see a problem here. The user can configure a large bufSize if they need lossless data and the machine can afford it. \n. On Wed, Aug 19, 2015 at 3:30 AM, Michal Witkowski notifications@github.com\nwrote:\n@iamqizhao https://github.com/iamqizhao even if bufSize is\nconfigurable, as far as I understand the proposal here, it would control\nthe publication for both Tracing and Monitoring. As such, if I wanted full\nfidelity for Monitoring (which should be lightweight - incrementing a\ncounter), I would have lossless Tracing (which can be expensive - writes to\ndisks or something). The tracing could be fairly costly. Or alternatively I\ncould disable Tracing altogether to retain Monitoring.\nMy understanding is that tracing and monitoring can be completely separated\nwith different bufSize configured. Sameer just proposed the API. I do not\nsee why tracing and monitoring have to be bound together.\nI tend to agree with @matttproud https://github.com/matttproud\nregarding lossless monitoring. As a SRE or a systems operator, I want to\nhave full confidence that the counter I'm observing has been incremented on\nevery RPC. If it was lossy, I probably want the monitoring to export a\ncounter that said that a certain number of values was dropped... but that\nbreeds a chicken and egg problem.\n+1 for lossless monitoring. But keep in mind that there is a physical limit\n(memory size on that machine) anyways. You can set bufSize for monitoring\nto infinite but it will OOM if the monitoring data exceeds the limit.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/240#issuecomment-132532971.\n. @mwitkow-io \n\nSorry for the delay. I have got caught by some tight deadlines on some other stuffs these 2 weeks. I will have a close look early next week.\n. Allow HTTP-level server-side authentication (e.g., oauth2) and some sort of Before/After hooks.\n. @zellyn \nAre they unary rpcs or streaming rpcs? If they are streaming rpcs, do you need to intercept every operation (message read and write) for an RPC on the server side?\n. @stevvooe, I am pretty sure we can address all of your concerns about LB. But to be clear your Invoker interface is no-go because it breaks a lot of user-facing API. Can you hold for a while for our new proposal?\nFor the issue here, the server side interceptor is under the internal code review and the client side interceptor has not been designed yet. https://docs.google.com/document/d/1weUMpVfXO2isThsbHU8_AWTjUetHdoFe6ziW0n5ukVg is one proposal from zellyn.\n. We need to sort out some pre-GA issues including some API breaking changes now. And then the client-side interceptor will be my priority.  Because the GA date is still a bit murky, I cannot provide a ETA for client interceptor now. But I do hope to make it done (at least the design) this month.\n. The design is on the way. Sorry about the delay. I will keep u posted.\n. http2 is just one type of transport we have/will have. Probably It is not proper to use the thing in http world to \"canonicalize\" gRPC. gRPC probably should define its own canonicalization rules. Anyways, this is not a gRPC-Go-specific issue. Please have your proposal in grpc/grpc repo.\n. see https://github.com/grpc/grpc/issues/2587. I will follow once it is done.\n. Let's talk in person tomorrow. I would like to understand how the current impl breaks your newly added tests first.\n. I somehow lost the access to github because I am in China. Will be back\nnext week.\nOn Sat, Jul 11, 2015 at 10:13 AM, David Symonds notifications@github.com\nwrote:\n\nI'm not really qualified to review this. @iamqizhao\nhttps://github.com/iamqizhao is the right person.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/244#issuecomment-120645988.\n. This PR is incorrect. The control frames have to be sent by the controller.  If you can wait, I will make one for you next Monday.\n. 1. should be released by the end of this quarter.\n2. I can send a fix soon. But it is not a realistic scenario acctually.\n. Can you sign CLA again?\n. On Fri, Jul 24, 2015 at 4:03 PM, David Symonds notifications@github.com\nwrote:\nHe's a Google employee. He shouldn't be signing an individual CLA for this.\nhmm, I am not sure what the right policy is here. But all of us in the team\ndid.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/253#issuecomment-124759351.\n. This means googlebot has some issues.\n\nthe pr looks good. will merge once travis turns green. Thanks for the contribution.\n. I would have a test for this. But it is not trivial to add a broken deployment server to the test framework. I will try to see how it can be achieved sometime.\n. closed via #284 \n. done. Thanks. PTAL.\n. done. Thanks.\n. In grpc wire protocol, there is no empty data frame since it always has\ncompression flag and a length field (refer to \"Delimited-Message\" part in\nthe protocol doc). But yes, we need to deal with misbehaved peers. For this\npurpose, the right operation to deal with empty data frame and data frame\nwith EOS flag (your previous pr) should be tearing down the connection. We\nshould not keep talking to the peer when we already know it is broken. I\nwill make a pr to fix this (including the case of data frame with EOS).\nYour contribution is welcome too.\n. I was not saying there is a connection between frame boundaries and\ndelimited message boundaries. I was saying with the per-message header\n(compression flag + length), there is no way for a valid impl to send an\nempty data frame. If you do not have any data to send, why bother sending a\ndata frame? Show me a case where an empty data frame will be sent if you\ninsist.\nThe only reason for this handling is for mis-behaved peer in my mind. But I\nam  a bit panic to keep adding the code for corner case handling without\ntest cases showing the corner case could happen.\n. If we want do deal with empty dataframe, we should deal with it earlier. See #273 \n. CloseStream will be called to close headerChan.\nIt will almost be sure to get another failure if we retry that rpc with REFUSED_STREAM immediately. Some backoff algorithm is needed. I lean to leave this to users.\n. It would be good if you have a test case to confirm your hypothesis if you think there is still some issue.\n. Thanks for reporting. This seems an issue. Will have a fix soon.\n. LGTM. \nI will merge after getting @dsymonds LGTM and green travis.\n. seems break end2end_test. reverting ... investigate tomorrow.\n. etcd does have a client to achieve this. We only need a couple of very thin functions here. @yangzhouhan Please revise.\n. Without adding an additional general API, we will still use grpc.Dial to do connection but the param could be a custom name, and grpc.Dial should take an DialOption to define how to do name resolution for that custom name. And we will provide a DialOption impl for etcd-based name service. What Zhouhan is doing is to provide some convenience functions to implement this etcd-based DialOption. I think it is equavelant to what you think of. Any better idea is welcome.\n. I would like to provide an example using a popular open-sourced kv store solution as a showcase on how to do custom name resolution. zookeeper and etcd are only well-accepted solutions in open-source world. zookeeper does not have go client and does not support http-based operations. So I would say the etcd-based example will be the only one in grpc-go repo in the foreseeable future (grpc-C++ will have both.).  We will have another Google-internal one though.\n. Hi Philips,\nA half-baked doc is at https://github.com/a11r/grpc/blob/doc2/doc/naming.md. And I will have a complementary doc for go specific.\n. @Sajmani Thanks for the contribution. :)\n. my last comment: I prefer letting the connection level tracing is also guarded by EnableTracing var (https://github.com/grpc/grpc-go/blob/fe3b16a0f35bdda0a7f400dc4fdb8d9342463d0a/trace.go#L49) which is currently used for rpc level tracing. What do you think?\n. Yep, it is not performance critical but I do prefer that to make the story of tracing consistent. Can you do that in this PR?\n. Sorry about the delay. was busy on wrapping up my intern's work last week.\n. My biggest concern is as follows (copy and paste from the previous comment you probably missed):\n\"I actually tend to put the operations on cc.events (Printf, errorf, finish ) along with the change of cc.state. cc.state records the state change of ClientConn (and here we just need to push it to tracing) and has been protected by a mutex.\"\nIt sounds a natural choice.\n. regardless mutex, my point is that the trace log should (almost) be aligned with cc.state change, e.g.,\ncc.state = Ready --> log connection established;\ncc.state = TransientFailure --> log connection retry;\ncc.state = Shutdown --> finish trace;\nIt seems you are already doing this if you address my last comment about \"connection established\". So we are good to go once you fix that one. Thanks for the contribution.\n. i) The following comment did not get answered:\n\"is there any problem if cc.event.Errorf/Printf get called after cc.Event.Finish is called?\"\nii) It seems you need to sync your git client and resolve conflicts.\n. Sure, let's get this merged on next Monday (if not during this weekend). I\nthink the revision will take you 30 min at most. :-)\nOn Sat, Sep 19, 2015 at 4:08 AM, Sameer Ajmani notifications@github.com\nwrote:\n\nQi Zhao, I believe I've addressed all your comments. I'd like to present\nthis feature when I speak on gRPC at GothamGo Oct 2. Can we get this merged\nsoon? Thanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/287#issuecomment-141650675.\n. And there are some minor conflicts with the latest version. Please sync.\n. The idea sounds fine to me. Thanks for contribution.\n. You can check https://cloud.google.com/pubsub/libraries. But they probably do not use grpc though but it won't be hard to translate into grpc.\n. It won't be in beta. But I can do something after beta. To do that, can you elaborate your requirements a bit? \n. See my comments inline. I still do not see a compelling reason to have\nserver side per-connection hook.\n\nOn Tue, Aug 18, 2015 at 6:53 PM, zj8487 notifications@github.com wrote:\n\nShortage of abilitys for grpc\n- Checking the connection on client and server end\n- Marking the connection on client and server end\nChecking the connection on client and server end\nWhy need the ability?\nBoth client and server should know the status of the connection to the\nother side, so that the each end can do some work tor it. for example,\nreconnect, destroy the resource.\nmaybe you will say we can monitor the service status by zookeeper, etcd.\nyes, you are right. but it will not work on mobile client.\nWe have channel state API on client side now.  See\nhttps://github.com/grpc/grpc/blob/master/doc/connectivity-semantics-and-api.md\n.\nWhat is the status of the connection?\n-\nheartbeat timeout\nThis is the health check service we have implemented. See\nhttps://github.com/grpc/grpc/blob/master/doc/health-checking.md\n-\ngrpc should supply the ability to config the heartbeat. the options is\n   as follow:\n- enable the heartbeat detect ---if no, no heartbeat package will send\n    to detect the connection.\n  - the interval for the heartbeat package----send the heartbeat\n    package every interval\n  - heartbeat timeout time----if no package received in the time, heartbeat\n    timeout hook will be triggerd\n  - handshake between client and server. the client should detect the\n    connection like server. but should use the same config as above used in\n    server side. so handshake is needed to sync the config.\n## if the handshake is possible to sync the custom config is better\nconnection lost for reason, ETIMEDOUT, EHOSTDOWN, EHOSTUNREACH, etc.\n  if the event of connection tiggerd, the hook will trggered\nMarking the connection on client and server end\nWhy need the ability?\nIn general, we can not give every connection a hook. so we should know the\nowner of the connection in the hook.\nfor exapmle, if a client logon by a login rpc function, and then we\nrecord the userId or user object on the connection. and then, we will know\nwho is lost in the hook function.\nI do not see per-connection hook is required here. Why does you care\nconnection on server side?\n@iamqizhao https://github.com/iamqizhao\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/297#issuecomment-132414662.\n. This sounds a good idea. I can see this provides a way to cancel all the rpcs on server side, which is probably needed in some scenarios. Let me see if there is any hidden side effect. If there is no, I can probably make it happen very quickly.\n. Testing is the only use case I imagine for now ...\n. Sure thing. Just brainstorming, a client could talk to N equivalent servers to finish a transaction consisting of a number of rpcs ASAP. It will start the transaction with N server in parallel. Once it finishes the transaction with a particular server (the fastest one), it can notify the other servers via a side channel so that the other servers can cancel all the pending rpcs which are still under processing to save power. \n\nLet's see if @mwitkow-io has any real use cases.\n. You are right, but i) this approach might not be bandwidth efficient because the client will send a cancellation for each particular rpc ending up with a lot of http2 frames to server. With this feature, the client can send much less; ii) As mentioned in Jeff Dean's paper \"The tail at scale\"(http://research.google.com/pubs/pub40801.html), the fastest server could try to notify the other servers to cancel the transactions directly. Then this feature could be used to achieve that when a slow server receives the notification from the fastest server.\n. The general design looks good to me.\n. yep, this PR can be accepted. But I need to check all the points you inserted the code and have not got time to do that. Sorry about the delay.\nBTW, I know it is painful but can you sync your code to the latest?\n. On Mon, Oct 5, 2015 at 12:46 AM, Michal Witkowski notifications@github.com\nwrote:\n\n@iamqizhao https://github.com/iamqizhao Great to hear that. I have\nrebase over the latest master.\nFYI, I will consider merging this change after the WIP naming and load\nbalancing change is done. Sorry about that because this probably means you\nneed to do a couple of extra rebasing in the next a couple of weeks.\nI'm wondering how to add unit tests that will make the monitoring still\nwork correctly across major refactors such as the ones I'm currently\nrebasing on. Maybe retrofit some integration tests with monitoring counters?\nI would suggest creating a dummy monitoring impl in the end2end test and\nverify it works properly.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/299#issuecomment-145452077.\n. The ETA is by the end of this week or early next week. Sorry about the\ndelay. The internal review took much longer than I expected.\n\nOn Wed, Apr 13, 2016 at 3:33 PM, Roland Bracewell Shoemaker \nnotifications@github.com wrote:\n\n@iamqizhao https://github.com/iamqizhao has this been depreciated by\nthe server side interceptor currently being reviewed internally + proposals\nfor client side interceptors?\nThe Boulder team at Let's Encrypt is currently working on moving away from\nour current RPC implementation in favor of gRPC but the lack of exposed\nmetrics hooks is slowing us down somewhat (one of the reasons for moving to\ngRPC was to get rid of a bunch of non-CA code we had to maintain, including\nvarious hacks to collect client and server side metrics which we'd rather\nnot re-implement).\nWe'd prefer to use something native to grpc-go rather than writing\nanother set of wrappers/using additional code generators and are wondering\nif there is a prospect of this being merged or if the other proposals have\nsuperseded it (and in either case if you have a ETA, even if it is likely\nto shift).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/299#issuecomment-209676221\n. @pires To be clear, grpc build does not depend on etcd -- you can buid grpc-go without cloning etcd client. The same treatment was done for credentials/oauth2. I am not clear how this bothers you (probably you need to customize your fetching tools to omit naming/etcd?). If this is an issue for most of users, I can chop off the etcd dependency completely by writing my own etcd client using http package but this means I need to reinvent some wheels which are already done in etcd client lib.\n. @pires thanks for the update. I will close this issue.\n\n@dsymonds I am not sure how much trouble and confusion this will bring. If it made nontrivial trouble to our users, we have two approaches here i) rewrite it to eliminate etcd dependency completely; or ii) move it out of grpc-go repo (in a brand new repo or my personal repo). Let's discuss this after beta. Thanks.\n. 1. This belongs to application logic. In your service impl handler, you can simply take the context and pass it the downstream rpc calls. In this way, the upstream rpc is chained with dwonstream rpcs.\n2. It works for both. I am not sure what you missed. Can you take a look at https://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L767 as an example for metadata on streaming rpc?\n. 1. Because streaming rpc does not support CallOption yet.\n2. For unary rpc:\nctx := metadata.NewContext(context.Background(), testMetadata)\n   if _, err := tc.EmptyCall(ctx, &testpb.Empty{});\nfor streaming rpc:\ntc := testpb.NewTestServiceClient(cc)\n    ctx := metadata.NewContext(context.Background(), testMetadata)\nstream, err := tc.FullDuplexCall(ctx)\n. Thanks!\n. Done in #305 \n. This is a mandatory request we just got from security team and has to be\ndone before beta (tomorrow or early next week) ...  We are not given a\nreasonable time window to achieve graceful deployment unfortunately.\nOn Thu, Aug 27, 2015 at 6:11 PM, David Symonds notifications@github.com\nwrote:\n\nIt would have been nice to roll out the grpc.WithInsecure() bit first\n(and maybe only log if it was absent), and give people time to update their\ncode before enforcing this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/307#issuecomment-135596283.\n. @michael-berlin \n\nI have extended the error message in #313 to tell the caller how to fix.\nPlease expect a few other hiccups in the next a few days to indicate the end of alpha. :-)\n. Like the existing route_guide example, this greeter example should stay in a separate dir (e.g., examples/greeter or examples/helloworld).\n. Thanks!\n. Sorry for the breakage. We have to do some API breaking changes in Alpha to\nminimize the future risk which is harder to address in beta and GA.\nOn Fri, Aug 28, 2015 at 10:35 AM, Benoit Sigoure notifications@github.com\nwrote:\n\nThis causes runtime problems in existing code. The code still compiles\nfine, but existing tests for instance now fail with foo_test.go:88:\nFailed to connect to [::]:58898: grpc: no transport security set (use\ngrpc.WithInsecure() explicitly or set credentials).\nNot cool :(\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/310#issuecomment-135840741.\n. understood. Unfortunately I did not find a good way to generate a\ncompilation failure for this change unless I rename grpc.Dial which is not\nacceptable.\n\nOn Fri, Aug 28, 2015 at 10:44 AM, Benoit Sigoure notifications@github.com\nwrote:\n\nIf the API changes break compilation, that's fine, it's easier to catch.\nRuntime breaks are trickier ;o\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/310#issuecomment-135842399.\n. We have not constructed peer info yet (we will). For now applications need to use metadata to achieve this by themselves.\n. close this. Leave #334 ope for this issue.\n. Check out https://github.com/grpc/grpc/blob/master/doc/connectivity-semantics-and-api.md and find out the corresponding API in grpc.ClientConn in order to find out the state of a ClientConn.\n. Yup this is client side only. On server, we do not expose any underlying connection to the users. This is by design. We have not found a compelling reason to do it on server side because server side situation is much simpler: you either can send a response successfully meaning the connection is fine or fail to send because of a connection error. \n. I am almost sure that the rate limiting of server connection should be done by registering a rate limiting handler into the server without exposing connection state outside. Keep in mind that this type of API is always racy. When you get the state, the real state could be changed already.\n. Server will have a graceful shutdown API to shutdown a server gracefully. There won't be per-connection graceful shutdown. \n\nIf a feature requires the instant and accurate connection state to take action, it has to use callback type approach. It is possible to have a common hook for all these features.\n. I do not have plan. To be clear, like what we did for client side connection state, this should not be Go specific. If you really need it, please file a issue at grpc/grpc and we will coordinate on it across languages.\n. You should only feed the error you got from rpc call into grpc.Code instead of arbitrary ones. For now the only case you see codes.Unknown is server application error. Do I miss something here?\n. 1. If you see rpc error: code = 2 desc = \"grpc: the client connection is closing\" which is grpc.ErrClientConnClosing, it means your application calls grpc.ClientConn.Close() explicitly when that rpc is in flight. It seems this is an issue of your application. gRPC internal will do reconnect if it sees a connection issue. Applications does not need to do that in most cases. I still do not see a case this could introduce problem unless you feed an arbitrary error to grpc.Code() which should not happen.\n2. If you want to bind codes.Unknown with server side error, this seems wrong because grpc internals might also have a case where we need to return codes.Unknown too. I agree that rpcErr  is not very satisfying. I am thinking of a way to improve it. But before that I do not hope to go too far (e.g., add more exported API) if possible.\n. I will take a look at the unknown case. But regardless, your client should never get a connection error unless you set the connect time out (and then you may get ErrClientConnTimeout error as the connection error) because grpc internal always tries to address connection error by itself. You may get DeadlineExceeded error for sure if you set a deadline for your rpc but the connection cannot be established in time.\n. I have the same feeling about the readability and usability. In addition, the PR adds an additional mutex to logging, which is unpleasant.\n. Yes, I was wrong (that mutex is NOT \"Addtional\" one). :-) I somehow thought you use stdlib logger in your implementation, which is not true. \nActually your idea is good but the API is not sound. Probably we can have some better idea such as doing a bit reformating so that it would be easy to convert it to the desirable format in post-processing.\n. LGTM, thanks for the fix.\n. cc.state is used to guard Close is invoked only once. nil condition is actually not necessary here.\n. If you are using a TLS-secured transport, you can get the auth info from context on server side by using https://github.com/grpc/grpc-go/blob/master/credentials/credentials.go#L98.\nIn general, we will have a way to access peer info and connection info in near future.\n. fixed by #485 \n. We call this binary logging. It is on our road map but I do not have an ETA\nfor now though. I will ping this thread back once there is a decision.\nThanks for bringing this up.\nOn Fri, Sep 11, 2015 at 10:12 PM, Brandon Philips notifications@github.com\nwrote:\n\nHow can I dump the requests and responses from grpc-go? The net/http\npackage has some helpers, and I essentially want the equivalent for gRPC:\n- http://golang.org/pkg/net/http/httputil/#DumpRequest\n- http://golang.org/pkg/net/http/httputil/#DumpResponse\nWe would like to add a --dump-file flag to etcd to support analysis\ncoreos/etcd#3502 https://github.com/coreos/etcd/issues/3502\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/335.\n. We are figuring out. The Google internal one will be done by the end of the\nnext quarter. Have not got confirmed for the external version.\n\nOn Wed, Sep 16, 2015 at 5:46 PM, Brandon Philips notifications@github.com\nwrote:\n\n@iamqizhao https://github.com/iamqizhao Where is the roadmap? Is this\nsomething we can help with?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/335#issuecomment-140934706.\n. I will let you know if I need help from your side (probably right after I\nfinish the internal version). Thanks for the help!\n\nOn Wed, Sep 16, 2015 at 10:19 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nWe are figuring out. The Google internal one will be done by the end of\nthe next quarter. Have not got confirmed for the external version.\nOn Wed, Sep 16, 2015 at 5:46 PM, Brandon Philips <notifications@github.com\n\nwrote:\n@iamqizhao https://github.com/iamqizhao Where is the roadmap? Is this\nsomething we can help with?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/335#issuecomment-140934706.\n. On Wed, Jan 6, 2016 at 12:29 AM, Brandon Philips notifications@github.com\nwrote:\n\nLooking back at this issue this is another example of something that would\nbe solved by building on top of net/http2 directly instead of patching.\nTo be clear, I think the feature you requested here is on grpc message\nlevel (i.e., request message and response message) instead of http2 frames\n(a grpc message could be chunked into multiple http2 data frames). Do I\nmisunderstand? I am actually not sure whether httputil works for http2.\n\nIn addition, the approach of building on top of net/http2 may bring much\nmore problems/troubles than the benefit at least at the current stage\n(e.g., functional compatibility, performance, etc. plus massive effort on\nredesign and development).\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/335#issuecomment-169265834.\n. @xiang90 It will. But I have not got time to get my hands on that. Yes, you can do that if it fits your requirements.\n. Thanks for reporting. Looking ...\n\nOn Tue, Sep 15, 2015 at 6:42 AM, Florian Reiterer notifications@github.com\nwrote:\n\nGo version: go1.5 darwin/amd64\nWhat did you do?\n- Send an RPC call to the server\n- Before the server can answer, shut down the server (e.g. have a\n  time.Sleep() on the server to reproduce)\n- Restart the server\nWhat did you expect to see?\n- Client should resend the request when the connection is back up\nWhat did you see instead?\n- Client waits forever (call.go#L158\n  https://github.com/grpc/grpc-go/blob/master/call.go#L158), even\n  though the connection is back up\n- Happens because transportSeq is reset on each reconnect here:\n  clientconn.go#L307\n  https://github.com/grpc/grpc-go/blob/master/clientconn.go#L307.\n  Removing this line resolves the issue.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/339.\n. Sent #340 to fix. Let me know if the problem sustains.\n. Thanks for contributing. I got some concerns on this change:\ni) Installation of gRPC-Go must NOT have inherent dependency on a non-standard or non-google-backed package (bradfitz/http2 is the only exception and will be eliminated in the future).\nii) your way to import glog will introduce a lot of trouble when importing grpc-go into google internal code base.\n. Moving this into grpc repo is the way to go if you are not against it.\n. I always think the library developers are sorta biased when they add the logging to their lib. The logs they added probably only benefit themselves and not even readable to users. Therefore, I am very willing to listen to the feedback from our users on logging and make the corresponding changes. The current logging information were from our code reviewers and users besides myself. I agree that they are not in a very decent form at the current stage due to the long dev period without a refinement and lack of level logging support in the standard log package. We can definitely do a refinement and make it better in near future.\n. Thanks for the fix.\n. replaced by #364 \n. Adding additional options to protobuf has broad impact and very hard to proceed so that I do not want to pursue it. \n\nIn your example, what is the problem if you put the interceptor code into RepoCreate handler?\n. - @dsymonds the owner of go protobuf\n. Okay, it seems the demands on interceptor grows. I will look into the design options shortly and get back to you guys soon.\n. We have a brand new design of the interceptor. I will publish the design and start to implement it shortly.\n. The server interceptor has been done. Close this one.\n. @codefx9 We will have some HOWTO docs soon. For now, you can use the end2end tests I added as some toy examples.\n. If my memory works, I remember I could not access github at all without VPN\nwhen I was in China ... Unfortunately, this is a problem you have to figure\nout by yourself. :)\nOn Fri, Sep 18, 2015 at 5:54 PM, David Symonds notifications@github.com\nwrote:\n\nSorry that your network situation sucks.\nHowever, you can still fetch everything okay if you side-step go get:\ngit clone https://github.com/grpc/grpc-go $GOPATH/src/google.golang.org/grpc\ngit clone https://go.googlesource.com/net $GOPATH/src/golang.org/x/net\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/350#issuecomment-141605020.\n. grpc has a backoff strategy for reconnecting (\nhttps://github.com/grpc/grpc/blob/master/doc/connection-backoff.md). The go\nimpl is at https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L288.\n\nOn Tue, Sep 22, 2015 at 3:08 PM, Yutong Zhao notifications@github.com\nwrote:\n\nShould it cause an immediate reconnect? I assume the first time it tries\nto reconnect it will fail, (since it probably happens in between the server\nrestart). I'm trying to figure out where the reconnect/retry code is.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/351#issuecomment-142436087.\n. see https://github.com/grpc/grpc-go/blob/master/clientconn.go#L384.\n. I guess you set a timeout for the connection via https://github.com/grpc/grpc-go/blob/master/clientconn.go#L122. Did u?\n. merged. I will try to refactor the code to address context cancellation issue by Wed.\n. Cool, good to know. Thanks.\n. This blocks a series of upcoming changes. I checked it in for now. All the API is experimental for now. Feel free to comment at any time.\n. Yes, please take a look at\nhttps://github.com/grpc/grpc-go/blob/master/Makefile.\n\nOn Sat, Sep 26, 2015 at 8:30 AM, mattn notifications@github.com wrote:\n\ngoveralls run go test. is this same on grpc-go?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/363#issuecomment-143463951.\n. I still got this:\n\n0.05s$ goveralls google.golang.org/grpc/...\ncan't load package: package github.com/grpc/grpc-go: code in directory /home/travis/gopath/src/github.com/grpc/grpc-go expects import \"google.golang.org/grpc\"\nerror: exit status 1\n17:23:15 gocov.go:134: Error loading gocov results: exit status 1\nThe command \"goveralls google.golang.org/grpc/...\" exited with 1.\nDone. Your build exited with 1.\n. Updated the PR. Still got that error:\n0.28s$ make coverage\ngo get -d -v -t google.golang.org/grpc/...\ngoveralls google.golang.org/grpc/...\ncan't load package: package github.com/grpc/grpc-go: code in directory /home/travis/gopath/src/github.com/grpc/grpc-go expects import \"google.golang.org/grpc\"\nerror: exit status 1\n00:47:43 gocov.go:134: Error loading gocov results: exit status 1\nmake: *** [coverage] Error 1\nThe command \"make coverage\" exited with 2.\n. It does not add any useful info.\nThe command \"make test testrace\" exited with 0.\n0.15s$ make coverage\ngo get -d -v -t google.golang.org/grpc/...\ngoveralls -v google.golang.org/grpc/...\ncan't load package: package github.com/grpc/grpc-go: code in directory /home/travis/gopath/src/github.com/grpc/grpc-go expects import \"google.golang.org/grpc\"\nerror: exit status 1\n01:58:00 gocov.go:134: Error loading gocov results: exit status 1\nmake: *** [coverage] Error 1\nThe command \"make coverage\" exited with 2.\nLet me know if you are interested in taking this over. :)\n. @philips \nI think we have made it clear that it is not strict constraint. The short list could be dynamic and we could accept new ones via discussion (which is required).\n. Definitely. Both first-party and third-party contributors are required to justify if they bring new dependencies to gRPC-Go. I can have a PR to put this list in if it is helpful.\n. addressed in #378 \n. not sure. I guess the later install should overwrite the former one. Running\ngo install ./... at google.golang.org/grpc seems working fine. You probably should decompose the installation to find the root cause.\n. Feel free to rename one of them and send me a pull request if this still bothers u.\n. The main difficulty is that the existing code does unmarshalling in the generated code for unary rpc so that the server in grpc package does not have access to the received request proto (it only has the slice representation of the request).\n. Yep, this is the best solution I am thinking of right now. Sure, let's think a bit more before moving forward. Thanks.\n. I do not think anything \"wrong\" here. It is reasonable to have decodeFunc to do slightly more than \"decode\" like RecvMsg in streaming case does tracing besides receiving a message. The thing is that unary case lacks one level abstraction (aka grpc.Stream) compared to streaming case and we have to compensate it for tracing purpose (unfortunately). And decodeFunc is a way to introduce this missing abstraction. \nAn simple alternative is to let methodHandler returns both request and response. But it seems worse than this one.  The other alternative may involve non-benign refactoring and probably not worth doing that.\n. reverted that change. \n. @mwitkow-io Can you redo the change? Your previous pr did not compile.\n. I reverted the PR already. I or the original contributor will redo the commit.\n. I was not happy with it too and have a half-baked PR to make it better (will try to get it done by next Monday).\n. On Mon, Oct 5, 2015 at 2:24 PM, David Symonds notifications@github.com\nwrote:\n\nNice gain for relatively little code impact.\n@iamqizhao https://github.com/iamqizhao: the go/ links on the types\nabove this function really need public versions!\nsure, will take care of it shortly.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/385#issuecomment-145672647.\n. I saw one outlier\nBenchmarkClientUnaryc1-8 223208 257968 +15.57%\n\ncan you rerun the benchmark a couple of times (or longer time) to verify?\n. Thanks for taking care of it.\nOn Tue, Oct 20, 2015 at 7:12 PM, Jan Tattermusch notifications@github.com\nwrote:\n\nClosed #386 https://github.com/grpc/grpc-go/issues/386 via #406\nhttps://github.com/grpc/grpc-go/pull/406.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/386#event-441031240.\n. It seems my previous refactoring introduced this bug. It is actually fixed in my another bigger pr. But it is worth a separate fix. will send fix soon.\n. By design, we would like to offer constrained operations on metadata to\nprevent misuse.\n\nOn Fri, Oct 9, 2015 at 5:57 AM, Michal Witkowski notifications@github.com\nwrote:\n\nthe metadata.MD object is the only way to access headers for the RPC, for\nexample the authenticate: Bearer token on teh server side.\nHowever, it is incredibly hard to work with. It's hard to:\n- extract a single value under a given header (with binary decode)\nIt is not hard to have your own convenient function to do this. It is also\nwelcome to contribute to the metadata package.\n- dd a new value if you want to add it to Metadata you received from a\n  call into another call encodeKeyValue is private\nI agree we probably should add \"Add\" public API which is missing. Again,\ncontribution is welcome.\n- for some odd reason it supports multiple values per key (is this\n  even allowed by HTTP2?)\nYep, comparable to http is the only reason we introduce this.\n- if you construct new metadata.MD usingmetadata.New and then attach\n  it using metadata.NewContext, it seems like it will \"hide\" a\n  previously existing metadata value instead of adding to it\nWhich is better semantics is debatable. I think it is fine as long as we\nmake it clear in the comment.\nAre you guys planning on making it more user friendly? Would you accept\nPRs to make it so?\nAny proper contribution is welcome.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/395.\n. I treated this as a general query instead of some specific issues. I close this one. Feel free to contribute and open issues for particular issues.\n. Please sync.\n. I am not sure what you want exactly (can you elaborate?). But since picker\nis a dial option passed by users, it seems it does not make sense to have a\ngetter for it.\n\nOn Sun, Oct 11, 2015 at 11:52 AM, Michal Witkowski <notifications@github.com\n\nwrote:\nWhile working on a Reverse Proxy for gRPC (\nhttps://github.com/mwitkow-io/grpc-proxy) I stumbled with the problem of\ngetting a new ClientTransport and a new ClientStream in raw form.\nThat's because the Reverse Proxy skips framing and only operates on\nmetadata of the call.\nOne of the ways to do it was to expose the Picker as an interface of\nClientConn, which this CL is.\nAlternatively, a NewTransport method could be added to ClientConn that\nhides the Picker completely.\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/grpc/grpc-go/pull/397\nCommit Summary\n- cleanup references to Picker through dopts\nFile Changes\n- M call.go https://github.com/grpc/grpc-go/pull/397/files#diff-0\n  (2)\n- M clientconn.go\n  https://github.com/grpc/grpc-go/pull/397/files#diff-1 (11)\n- M stream.go https://github.com/grpc/grpc-go/pull/397/files#diff-2\n  (2)\nPatch Links:\n- https://github.com/grpc/grpc-go/pull/397.patch\n- https://github.com/grpc/grpc-go/pull/397.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/397.\n. Exposing transport. CientTransport to Picker is not the way to go. Pick\ncould implement various load balancing policies such as round robin in\nwhich case the ClientTransport you get from Picker() is meaningless. I\nexpect the proxy dev should use transport package directly and implement\nthe custom upper logic (may or may not be as same as what ClientConn does).\n\nOn Sun, Oct 11, 2015 at 12:22 PM, Michal Witkowski <notifications@github.com\n\nwrote:\nFor reference:\nhttps://github.com/mwitkow-io/grpc-proxy/blob/master/proxy.go#L187\nI can't rely on getting the Picker from the user, as the public interface\nfor \"dialing\" is ClientConn. Thus, I made ClientConn the interface I\nexpect the users of the gRPC proxy to use to specify the destinations of\ntheir RPCs.\nBasically the problem I'm facing is there is no way of getting a\ntransport.ClientTransport out of the ClientConn, which is needed for raw\nshoveling of data frames.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/397#issuecomment-147237455.\n. This won't be merged. Exposing Picker to users is not right way to proceed because calling Picker.Pick(...)  will change the internal state of Picker. It works in your case because your use case skips the grpc.ClientConn layer which calls Picker.Pick(...). Your current code messes up the layering of gRPC. I have not got time to figure out a desirable solution for you. But the first glance lets me think you should make a transport Picker by yourself, which is expected for proxy-type use cases.\n. I said you skip grpc .ClientConn because your code mostly on ClientTransport directly. In general, transport is the private package of gRPC-Go and should be used by users directly. Some special applications might need to operate on transport directly such as grpc proxy. For these applications, the developers should make their own upper layer control logic (counterpart of ClientConn). In principle, grpc.ClientConn is the entity to manage the underlying transport. You should either take it all or none. Trying to break it into pieces and taking advantage of part of them will make gRPC extremely hard to grow and maintain.\n\nYour proposal is problematic because:\ni) violate the above principle -- e.g., what if the ClientConn decides to tear down the ClientTransport returned by NewTransport at some point?\nii) it is not working for some load balancing policies. For example, for round robin, what ClientTransport should be returned for NewTransport given that the ClientConn may have hundreds of ClientTransport under it hood?\nIn terms of code duplication, I do not think it is a problem because this is dedicated for a special use case and is not unknown how much is duped when the code becomes stable. It is perfectly fine you can make it in your own repo and share it with other users with similar use cases. I ever thought to have a contrib directory in grpc-go repo for outside contributions and decided to give up due to the potential code quality and maintenance issues.\n. s/should be used/should NOT be used/\n. We can keep discussion here. Let me close this PR now to avoid confusing other people.\n. It is odd. will take a close look tomorrow.\nOn Thu, Oct 15, 2015 at 12:17 AM, David Symonds notifications@github.com\nwrote:\n\nNope. That's not it. Your message is ending up ~18MB big, well short of 32\nbits, and the encoder code wouldn't break on it that way either.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/399#issuecomment-148302646.\n. Thanks for fixing.\n. I am changing it to uint in #402. The original code cannot be compiled with Go 1.3 on my mac (int overflow error), which is not required but not pleasant.\n. @jtattermusch\n\nlet me know when I should merge. Thanks.\n. rpc is traditionally a client-server model. gRPC obeys this tradition by requiring client initiating the session. However, gRPC is more powerful than the traditional RPC libraries because it natively supports bi-directional streaming rpc where client is still responsible to initiate the communication but after that both sides can feel free to send the messages to the peer. I guess with some slight tuning of your service definition, your application might be able to use bi-directional streaming to meet your requirements.\nAnd setting up two servers is the last resort.\n. I do have plan to enforce this in metadata package.\n. not really. That goroutine works on the ctx from\nhttps://github.com/grpc/grpc-go/blob/master/transport/http2_client.go#L222\nwhich is not context.Background(). Let me know if I missed anything.\nOn Wed, Oct 28, 2015 at 10:51 AM, Steve Reed notifications@github.com\nwrote:\n\nThis is using go 1.5.1 on linux/amd-64 fwiw.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/416#issuecomment-151929639.\n. This is because the proto used by gRPC-Go has a different package name.\nFixing ....\n\nOn Thu, Oct 29, 2015 at 3:58 AM, Wari Wahab notifications@github.com\nwrote:\n\nWhen using Java to connect to the grpc-go route_guide server, I got the\nfollowing error:\n[wari@nucky examples]$ ./build/install/grpc-examples/bin/route-guide-client\nOct 29, 2015 6:35:04 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: *** GetFeature: lat=409,146,138 lon=-746,188,906\nOct 29, 2015 6:35:05 PM io.grpc.internal.TransportSet$1 run\nINFO: Created transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980\nOct 29, 2015 6:35:05 PM io.grpc.internal.TransportSet$TransportListener transportReady\nINFO: Transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980 is ready\nOct 29, 2015 6:35:05 PM io.grpc.examples.routeguide.RouteGuideClient getFeature\nWARNING: RPC failed\nio.grpc.StatusRuntimeException: UNIMPLEMENTED: unknown service routeguide.RouteGuide\n        at io.grpc.Status.asRuntimeException(Status.java:430)\n        at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:156)\n        at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:106)\n        at io.grpc.examples.routeguide.RouteGuideGrpc$RouteGuideBlockingStub.getFeature(RouteGuideGrpc.java:169)\n        at io.grpc.examples.routeguide.RouteGuideClient.getFeature(RouteGuideClient.java:80)\n        at io.grpc.examples.routeguide.RouteGuideClient.main(RouteGuideClient.java:232)\nOct 29, 2015 6:35:05 PM io.grpc.internal.TransportSet$TransportListener transportShutdown\nINFO: Transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980 is being shutdown\nOct 29, 2015 6:35:05 PM io.grpc.internal.TransportSet$TransportListener transportTerminated\nINFO: Transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980 is terminated\nException in thread \"main\" io.grpc.StatusRuntimeException: UNIMPLEMENTED: unknown service routeguide.RouteGuide\n        at io.grpc.Status.asRuntimeException(Status.java:430)\n        at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:156)\n        at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:106)\n        at io.grpc.examples.routeguide.RouteGuideGrpc$RouteGuideBlockingStub.getFeature(RouteGuideGrpc.java:169)\n        at io.grpc.examples.routeguide.RouteGuideClient.getFeature(RouteGuideClient.java:80)\n        at io.grpc.examples.routeguide.RouteGuideClient.main(RouteGuideClient.java:232)\nBut the same program worked when connected to the CPP route_guide server:\n[wari@nucky examples]$ ./build/install/grpc-examples/bin/route-guide-client\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: * GetFeature: lat=409,146,138 lon=-746,188,906\nOct 29, 2015 6:36:42 PM io.grpc.internal.TransportSet$1 run\nINFO: Created transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980\nOct 29, 2015 6:36:42 PM io.grpc.internal.TransportSet$TransportListener transportReady\nINFO: Transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980 is ready\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: Found feature called \"BerkshireValleyManagementAreaTrail,Jefferson,NJ,USA\" at 40.915, -74.619\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: * GetFeature: lat=0 lon=0\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: Found no feature at 0, 0\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: *** ListFeatures: lowLat=400,000,000 lowLon=-750,000,000 hiLat=420,000,000 hiLon=-730,000,000\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: Result: name: \"PatriotsPath,Mendham,NJ07945,USA\"\nlocation {\n  latitude: 407838351\n  longitude: -746143763\n}\n.......\nOf course, java server + cpp client works as well. Can't say the same for\ngo and java.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/417#issuecomment-152146690.\n. Can u elaborate \"the client left/disconnected\"?\n\nOn Thu, Oct 29, 2015 at 2:17 PM, Pieter Joost van de Sande \nnotifications@github.com wrote:\n\nI'm receiving a stream request on the server to stream updates to the\nclient. Here is the simplified code:\nfunc (this Controller) Read(request api.ReadRequest, stream api.ReadServer) error {\n    delay := backoff.Exp(time.Millisecond, time.Second)\nfor {\n    select{\n        case <-stream.Context.Done():\n            return stream.Context.Err()\n        case update := <-this.updates:\n            reply := &api.ReadReply{update}\n            if err := stream.Send(reply); err != nil {\n                return err\n            }\n    }\n}\nI expect the context to signal done when the client left/disconnected,\nbut for some reason this never happens. Is this by design? Is there any\nsimple way to get a signal when the client disconnected?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/419.\n. I probably should add it. I intentionally did not do that previously to\navoid complexity. But it seems it is needed for \"idle\" (i.e., no pending\nI/O) streaming rpcs in some cases. Making a PR...\n\nOn Thu, Oct 29, 2015 at 2:46 PM, Pieter Joost van de Sande \nnotifications@github.com wrote:\n\n@iamqizhao https://github.com/iamqizhao: the client process died.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/419#issuecomment-152337542.\n. Did u do this in China? If yes, you probably need a VPN to install it.\n\nOn Thu, Oct 29, 2015 at 7:03 PM, wengyanqing notifications@github.com\nwrote:\n\nI install grpc with command \"go get -u -v google.golang.org/grpc\", but it\nalways failed. The error message is below\nFetching https://google.golang.org/grpc?go-get=1\nhttps fetch failed.\nFetching http://google.golang.org/grpc?go-get=1\nimport \"google.golang.org/grpc\": http/https fetch: Get\nhttp://google.golang.org/grpc?go-get=1: EOF\npackage google.golang.org/grpc: unrecognized import path \"\ngoogle.golang.org/grpc\"\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/421.\n. The problem is that google.golang.org/grpc does not work in China. The workaround is that you can git clone but make sure you put it into the right directory so that it can build.\n. This seems a misuse because once grpc.Dial returns picker is never nil (\nhttps://github.com/grpc/grpc-go/blob/master/clientconn.go#L157). Can you\nshow me the corresponding code?\n\nOn Fri, Oct 30, 2015 at 7:08 AM, KaiBackman notifications@github.com\nwrote:\n\nWe are seeing the following panic consistently in production:\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal 0xb code=0x1 addr=0x0 pc=0x5149dc]\ngoroutine 269 [running]:google.golang.org/grpc.(*ClientConn).Close(0x0, 0x0, 0x0)\nsrc/google.golang.org/grpc/clientconn.go:225 +0x6c\nIt occurs a few times every hour on a system that that has a medium load\nof RPCs. There is no reliable short repro for the issue. Recovering from\nthe panic seems to put the gRPC system into an incorrect state and results\nin further panics a few hours later.\nI'm happy to test candidate patches or to do more diagnostics if required.\ngo version: 1.4.2\nos: linux/mac\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/422.\n. closes due to no update.\n. sure, you can.\n. if you use https://github.com/grpc/grpc-go/blob/master/grpclog/glogger/glogger.go#L45, all the logs go to the log files. If it still does not fit, you can even make your own logger.\n. it should be because the timeout u set is too short. Please enlarge it and\nretry. In addition, WithTime should not be used in most of cases. Use it\nonly when you really need it and have a good sense what value it should be\nset.\n\nOn Tue, Nov 3, 2015 at 3:39 PM, anzw notifications@github.com wrote:\n\nWhen \"TLS clientAuth: tls.RequireAndVerifyClientCert\" and\n\"grpc.WithTimeout\" are used together, the \"grpc: timed out trying to\nconnect\" error occurs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/426.\n. 1) This deserves a simple design proposal before we proceed. If we really want to do that, we need a solution for both unary and streaming rpcs instead of unary only in this PR;\n2) somehow this broken coverage test (https://travis-ci.org/grpc/grpc-go/builds/89182097).\n. Yes, we should. And we should involve @dsymonds in the discussion since it could change the generated code.\n. This has been supported by the server interceptor. close.\n. As David mentioned, a mission critical system should have\ni) retrying mechanism on client side;\nii) cluster management to restart one or more new backends if the old ones die;\niii) Name discovery to tell clients the updated set of backends.\n\nThis is much better than putting the system in a undefined state.\nThe thing we need to guarantee is that gRPC must not crash due to the errors of peers.\n. If I were here, I would tell you to update the certs for interop tests only. There is no reason to update the others. :)\n. Thanks for pointing out. I think this is a corner case worth an explicit spec in the designdoc and behave consistently across grpc impl in various languages. will initiate some discussion in the team once I am back from the travel next week.\n. Simply returning from the service handler impl closes the stream.\n. You can wait on another goroutine and cancel the stream if the timer fires.\n. To be clear, the scope of context is rpc (either unary or streaming) instead of a particular message.\n. hope this (https://github.com/grpc/grpc/blob/master/doc/health-checking.md) helps.\n. Yeah, we are about to change it to V1 now.  Let me talk to the team -- some coordination across languages is needed to minimize the chance of breakage. Should be done in a couple of days.\n. We do not have plan to make it. Some users are making their own.\n. Can you show me a repo?\nOn Mon, Nov 23, 2015 at 6:09 AM, Wari Wahab notifications@github.com\nwrote:\n\nI'm not sure if you guys consider it a bug, but the net/trace will still\nconsider this as an active connection that is not closed. The client does\nrespond with a proper UNIMPLEMENTED error, but the trace still stays on,\nfor example:\ngrpc.Recv.Utilities     [17 active]\nFamily: grpc.Recv.SinkStackUtilities\n[Normal/Summary] [Normal/Expanded]\nActive Requests When    Elapsed (s)\n2015/11/23 21:59:09.436954  215.948461      /sink.Utilities/Ping\n2015/11/23 21:58:42.341161  243.044283      /sink.Utilities/Ping\n2015/11/23 19:30:55.640609  9109.744858     /sink.Utilities/Ping\n2015/11/23 19:25:50.147901  9415.237598     /sink.Utilities/Ping\n2015/11/23 19:20:27.764727  9737.620792     /sink.Utilities/Ping\n2015/11/23 19:13:05.683845  10179.701688    /sink.Utilities/Ping\n2015/11/23 19:11:05.815413  10299.570132    /sink.Utilities/Ping\n2015/11/23 19:10:12.849414  10352.536149    /sink.Utilities/Ping\n2015/11/23 19:10:08.873467  10356.512108    /sink.Utilities/Ping\n... etc\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/456.\n. I think you do not install https://github.com/golang/protobuf properly. \n. it runs fine on my desktop by setting  runtime.GOMAXPPROCS(runtime.NumCPU()) on both client and server. I guess probably you let your client talk to a wrong port on server (maybe you use the profiling port)?\n. I do not quite get it. Can you elaborate what you want and why you think the current code cannot achieve?\n. On server side, exiting from the service handler is the way to go. You can run Recv in a separate goroutine spawn in the service handler. Once the service handler is returned, Recv is interrupted.\n\nBeyond this, we will flesh out a service interceptor mechanism so that you will have more different ways to achieve this.\n. grpc-go allows all ascii as header names. So there is no issue here.\n. Kelsey,\nSure. Thanks a lot for the help!. I will schedule a meeting with you and let's talk. \n. Yes, please open an issue to github.com/golang/protobuf and cc @dsymonds. Thanks. \n. The plan is to have a Peer struct to encapsulate all the peer information including peer address. I am happy if you want to take this. Otherwise, I can pick it up in Jan.\n. Thanks. For now, you can simply have a Peer struct and put peer addr as the only member. I will add more later.\n. https://github.com/grpc/grpc-go/pull/479 is under review.\n. On Wed, Jan 6, 2016 at 9:32 PM, bits01 notifications@github.com wrote:\n\nAh good point, just realized it's an interface.\nAny other solutions for the problem I'm trying to solve? Both sides need\nto act as both server and client at the same time. Ideally I'd like to use\na single connection (initiated from the side that can dial out) and do\nbi-directional gRPC on it but I'm not sure it's possible even with streams.\nTwo connections (both started from the side that can dial out) is the next\nbest thing if I can get it to work with the above hack-ish way.\nNote that RPC basically falls into client-server communication model\ninstead of peer-peer model. I think most of your requirements can be\nsatisfied using a bi-directional streaming rpc by tuning your wire protocol\na bit.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/484#issuecomment-169557041.\n. Your server behind the firewall dials the client and initiates a\nbi-directional streaming rpc in which both sides can send arbitrary numbers\nof messages to their peer.\n\nFor an example of bi-directional streaming, please refer to routechat at\nhttps://github.com/grpc/grpc-go/blob/master/examples/route_guide/routeguide/route_guide.proto#L62\n.\nOn Thu, Jan 7, 2016 at 11:20 AM, bits01 notifications@github.com wrote:\n\nAny good examples you could point to, please?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/484#issuecomment-169779217.\n. On Thu, Jan 7, 2016 at 11:53 AM, bits01 notifications@github.com wrote:\nI see. But then there's only one message type for the stream so you'd have\nto encode multiple message types into a single type, you lose the RPC-ness,\ndispatching to multiple functions that can take various inputs and return\nvarious values.\nEven though I stated one streaming rpc in the previous email, you can feel\nfree to extend it to multiple ones (e.g., 1 stream <-> (request type, reply\ntype)).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/484#issuecomment-169788013.\n. grpc-io@googlegroups.com is the email address you want.\n\nI think you need to refactor your service model somehow to fit into the client-server model of rpc. For example, your client (behind the firewall) can setup the connection and initiate a streaming rpc. Then your server can response with x in your example and eventually your client sends retdata1 to the server after processing it (i.e., doX). \n. sgtm. Done.\n. There is no API to send PING frame. If you control the server side code, you can host a health check service on your server (https://github.com/grpc/grpc-go/blob/master/health/health.go) and then your clients can issue health checking rpcs periodically to keep the liveness of the connection.\n. Can you give me an example of your use case of wrapper to demonstrate the\ncurrent facility is not sufficient?\nOn Tue, Jan 19, 2016 at 11:29 PM, Nodir Turakulov notifications@github.com\nwrote:\n\nWe often wrap errors to add extra behavior, for example to indicate that\nan error is transient. However, we cannot decorate grpc.rpcError because\ngrpc.Code and grpc.ErrorDesc do not work with wrappers.\nProposal:\n- introduce an interface, public or internal\ntype rpcErr interface {\n  Code() Code\n  Desc() string\n}\n- Make grpc.rpcError implement it.\n- in grpc.Code and grpc.ErrorDesc, check if an err implements the\n  interface, rather than if it is grpc.rpcErr\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/494.\n. I think instead of calling grpc.Code you should call\n\nerrors.Code(err)\nwhere the error code is specified when the error is constructed in\nerrors.WrapTransient(...).\nOn Wed, Jan 20, 2016 at 12:11 PM, Nodir Turakulov notifications@github.com\nwrote:\n\nerrors.WrapTransient\nhttps://github.com/luci/luci-go/blob/master/common/errors/transient.go#L36\nis the wrapper function we use to mark errors as transient.\nerr := grpc.Errorf(codes.Internal, \"Internal server error\")\nerr = errors.WrapTransient(err)actualCode := grpc.Code(err)\nhere actualCode is codes.Unknown instead of codes.Internal. We have other\nwrappers as well.\nSee also Dave Cheney's tutorial on error inspection\nhttp://dave.cheney.net/2014/12/24/inspecting-errors.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/494#issuecomment-173345992.\n. You can probably have another constructor which accepts two params:\n\nerr := errors.WrapTransient(err.Code(), err.ErrorDesc())\n. It has been supported (https://github.com/grpc/grpc-go/blob/master/credentials/oauth/oauth.go).\n. yup, some hook need to be added so that the token can be verified. We have not fleshed out the design. Do you have proposal here? :)\n. Merging this to #240 which I am actively working on.\n. The behavior under this scenario is undefined/unspecified in gRPC protocol.\nAnd it is debatable which order is preferred. Making your test rely on this\ntricky behavior is unpleasant.\nOn Thu, Jan 21, 2016 at 8:33 PM, J. Liu notifications@github.com wrote:\n\nAssuming in the arguments opts:\nThe first one is is generated with\ngrpc.WithTransportCredentials(credentials.NewClientTLSFromCert(nil, \"\"))\nThe second one is generated with\ncreds, err := credentials.NewClientTLSFromFile(*certificate, \"\")\ngrpc.WithTransportCredentials(creds)\"\nOn a machine without root certificate, this first option will lead to\nconnection error (x509: failed to load system roots and no roots provided).\nIf I only have the second option, there is no error. But if I have both\noptions in the order above, the connection error occurs again.\nWe hope the latter one can override the former one, as the opts are\ngenerally constructed with append(), where default options come first, then\nthe customized options will follow.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/497.\n. I just ran your code 1,000 times on my ubuntu desktop and have not seen any error.  Did you see any logs on client when it ran into bad case?\n. When it hangs in \"bad\" case, what does /debug/requests show on both client and server?\n. Can you also show me debug/pprof/block and debug/pprof/trace in the bad case?\n. I think I see what the problem is. Let me see how this can be fixed.\n. On Thu, Jan 28, 2016 at 10:51 AM, Don Petersen notifications@github.com\nwrote:\nYour changes in #505 https://github.com/grpc/grpc-go/pull/505 fixes my\ndemo app and my real application, they connect consistently whether I send\nfirst or not. Thanks so much for looking at this so quickly! I really\nappreciate it, I don't think I could have found the fix.\nThanks for reporting and tons of effort trying to reproduce!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/498#issuecomment-176335013.\n. \"Delimited-Message\" is one entire Message which may be split into multiple data frames. So the length refers to the the size of the entire Message.\n\nIn general transmission unit of gRPC is still message (instead of byte streaming) so that as David said it requires a length prefix.\n. This looks like a generic interceptor. I am working on a proposal to resolve the issue of interceptor. I will probably cover this case also.\n. close this issue. please comment on #240 if you have anything.\n. By design, gRPC library itself only includes protobuf codec as the example and default choice.  Users can feel free to implement their own codec (e.g., json) in their own package with grpc.Codec interface. Let me know if there is any issue to implement your own JSON codec in your own package for gRPC.\n. This is actually the right thing to do. It is not realistic to ask gRPC support tons of codecs natively.\n. There is nothing special for setting binary value. You only need to make sure the key has a \"-bin\" suffix.\n. Sorry, can you resolve the conflicts with #509?\n. LGTM, will merge after travis is done.\n. Hi Kelsey,\nAssume you will move this to k8s repo. I am closing this one.\n. Hi Kelsey,\nThanks for contribution. But shouldn't this stay in k8s repo instead of grpc?\n. ugh, did not see David already answered. Thanks, David!\n. Some errors on travis:\ngo test -v -race -cpu 1,4 google.golang.org/grpc/...\n=== RUN TestRetry-4\n2016/02/01 06:41:51 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:48304: connection reset by peer.\n2016/02/01 06:41:55 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2016/02/01 06:42:07 transport: http2Client.notifyError got notified that the client transport was broken EOF.\nFAIL    google.golang.org/grpc/test 134.489s\n?       google.golang.org/grpc/test/codec_perf  [no test files]\n?       google.golang.org/grpc/test/grpc_testing    [no test files]\n. My experience with travis is that its VM/container is much slower than our desktop so that it sometimes can expose some races which are hard to show on our desktops.\n. Can you put the greeter change into a separate pull request?\nAnd can you add a http server listening on the same port and a http client in the greeter client? I think after that the greeter is a much better example to show how to use the feature introduced by the new ServeHTTP.\nThank you.\n. @tamird \nThanks for early adopting. We will check this in ASAP. It may take a bit more time because it is a big change. Notice that this change directs your traffic onto a new transport which has not been exercised much in grpc world (yes, http2 package is heavily tested and it passes all end2end tests in grpc). So take your own risk here if you want to use it in prod.\n. Thanks for the fixing. I remembered I tried to do a thorough pass on this issue. But apparently I have not completed it.\n. It seems there is something wrong when fetching indirect dependency.\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 200)\nFetching https://google.golang.org/cloud/compute/metadata?go-get=1\nignoring https fetch with status code 500\nFetching http://google.golang.org/cloud/compute/metadata?go-get=1\nParsing meta tags from http://google.golang.org/cloud/compute/metadata?go-get=1 (status code 500)\nimport \"google.golang.org/cloud/compute/metadata\": parse http://google.golang.org/cloud/compute/metadata?go-get=1: no go-import meta tags\npackage github.com/golang/protobuf/proto\n    imports golang.org/x/net/context\n    imports golang.org/x/net/http2\n    imports golang.org/x/net/http2/hpack\n    imports golang.org/x/net/trace\n    imports golang.org/x/oauth2\n    imports golang.org/x/oauth2/google\n    imports golang.org/x/oauth2/jwt\n    imports google.golang.org/cloud/compute/metadata: unrecognized import path \"google.golang.org/cloud/compute/metadata\"\ngithub.com/golang/glog (download)\nmake: *** [testdeps] Error 1\n. I think the only place is\nfunc _TestService_UnaryCall_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error) (interface{}, error) {\n  ...\n}\nAnd I think this is fine because ctx in this function is not different from a common param.\n. @zellyn \nI am talking to @dsymonds about the interceptor change (yes, handler :). I will keep you posted on that part.\n. one- and two-character names are not rare in the standard package impl. But feel free to send me renaming PR if you feel you have better idea about naming. Closing this issue now.\n. In-process server (or local client) is not supported (we have not had plan for that yet). As David said, dialing a unix domain socket (or a localhost address) is the way to go assuming you do not have high demanded performance requirements.\n. This will be tricky because batching might enlarge latency. We have some minimal deliberate batching support in the code already. But how to enhance it to maximize throughput without hurting latency is complex and need more time. Performance would be one of our main focus next quarter.\n. This would be tricky on the interaction with Flush decision on the transport layer.\n. @bradfitz , as I mentioned, the current grpc transport does this automatically too (may be different from what http2 does). The problem is that whether Flush knob should be exposed to users.\n. We are working on better document but providing \"realistic\" but not too serious examples is not a easy job. :) Probably you can check some projects which have been or are adopting grpc-go to get some better idea (e.g. cloud bigtable, etcd). grpc-io@googlegroups.com is probably the best place to ask this kind of questions.\n1. ClientConn can be long lived and many rpcs can be multiplexed on that. It is still case-by-case though. For example, some applications (e.g., some android apps) may want to tear down the ClientConn when it knows it won't send out any rpcs for a while. I would say the application is the best decision maker here.\n2. In general, WithDialer is used to connect the server with some special custom code (e.g., unix domain socket, Google app engine). It is not designed for load balancing. WithPicker is designed for load balancing and naming. I saw some users use WithDialer to implement some simple load balance (mainly because Picker was not available at that moment).  I am not gonna stop them. But I would suggest using Picker.\n. Yes, it is the test problem. These two tests keep the streams open when they exit. I am going to fix them soon. Thanks for reporting.\n. BTW, goroutine leaking checker would be very helpful. Previously, I used debug/pprof/block to check manually. Thanks.\n. My PR is also ready. Running tests now to make sure it is okay. I think making it a separate PR would be better. Feel free to send your fix out if you want. Otherwise, I can send mine too. :)\n. sgtm\nOn Fri, Feb 5, 2016 at 1:24 PM, Brad Fitzpatrick notifications@github.com\nwrote:\n\nI'll send mine, since it has the leak checker in the same CL. I'll send it\nfirst, before my bigger changes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/528#issuecomment-180559919.\n. I am looking.\n. #535 will fix this. Waiting for travis about 2 hrs already. It is overwhelmed by a lot of pull requests from grpc C++...\n. @bradfitz \n\nHappy birthday and have fun! :)\n. This is typically because you perform an rpc after closing ClientConn. I could help if you provide more info. Your code probably has a race.\n. Yes, it is expected. What I am missing here is that why it bothers you.\n. I would say this is incorrect. I do not know the purpose you want to do this. But transport is a private package which should only be used by grpc package (yes and some proxy which does not need to know the upper level grpc semantics).\n. I would guess your definition of \"uninteresting\" is probably problematic. What is your intention? What errors do you want to filter out?\n. This sounds like what you want to do instead of why you want to do. Why do you want to ignore these two types of errors for logging?\n. This seems incorrect. You want to know the reason if your rpc is failed.\n. why? This is the final result of an rpc and should be an rpcError.\n. The principle an rpc (regardless unary or streaming) will have a final status. Unless it is okay status (nil or io.EOF), the final status is of type rpcError. \n. I have no idea what func you are talking about. Can you elaborate?\n. what prevents you from doing:\nfunc (_server) Do(context.Context, *test.Empty) (_test.Empty, error) {\n   return new(test.Empty), nil\n }\n?\n. I would think crash is right thing to do because this is a fatal error due to the local operations. \n. This is not grpc issue. please file a bug to golang.\n. Better ways to sync is definitely preferred and always my favorite when writing a test. All the Sleep remaining in the tests are the ones I have not figured out better ways to synchronize after some thinking.\n. There is no clear action items on this. I close this now. But feel free to send me your optimization PRs. Thanks.\n. In your use case, you only need hold a health checking service on your server (https://github.com/grpc/grpc-go/blob/master/health/health.go). And then your client can periodically health check the server to keep the connection on.\nThis does not mean Ping API is not needed. We will add it later.\n. There are 3 different layers relevant to this issue:\ni) transport layer (e.g., TCP): You can use grpc.WithDialer to pass a Dialer with KeepAlives on;\nii) rpc layer (e.g., PING frame if we use http2):\n    a) user-initiated PING: we will have API to support this;\n    b) grpc-initated PING: under debating.\niii) application layer: health check service as I mentioned\nIn principle, we lean to letting users initiate this health signal by themselves instead of grpc lib because it could generate non-trivial traffic which could be charged. \nOverall, I do not see any problem here.\n. I think we should. Need talk to the team.\n. Please keep interop dir and files intact? Otherwise we need redo quite a bit work on our interop framework (jenkins, docker, some google internal frameworks).\n. This sounds problematic. These should be isolated.  If the thing is like this, there is no way for a dev of a very popular library to name anything in his code because there is always a chance it will conflict with some of its users...\n. no, it is not accurate. It  would be \nStop forcefully stops the gRPC server. It immediately closes all open connections and open listeners, and abort all active RPCs, and then returns.\n147 has been opened for graceful shutdown.\n. It cancels all active RPCs on server side. It does not notify client (no RST_STREAM sent). The client side pending rpcs will get notified by a connection error.\n. sure, a PR will be out soon.\n. Thanks. Where can I find these limits for race detector?\nBTW,  #514 is almost there. \n. @bradfitz \nI got an impression that Brad has a grand plan to improve logging story of grpc. Brad, can you throw out the concrete plan before taking action on it?\n. Close since this is not a grpc issue. \n. We are working on that. But be aware of that is still in the experimental stage and may run into performance regression and stability issues.\n. @bradfitz \nI have not done anything on that yet. But I saw something like https://github.com/cockroachdb/rpc-bench which is not the exact case here but may imply the potential performance regression.\nI think it would be good to give a headsup.\n. I meant the when you compare grpc and protoHTTP2.\n. for example:\nname              time/op\nGRPC_1K-4           94.7\u00b5s \u00b1 4%\nProtoHTTP2_1K-4      167\u00b5s \u00b110%\nname              speed\nGRPC_1K-4         21.6MB/s \u00b1 4%\nProtoHTTP2_1K-4   12.3MB/s \u00b110%\n. It is relevant because ServeHTTP shares the same transport code as in http2 package.\n. On Tue, Mar 1, 2016 at 1:09 PM, Tamir Duberstein notifications@github.com\nwrote:\n\nDrawing any conclusions about the performance of ServeHTTP from that is\nnot sound. Yes, perhaps the transport is shared, but the remaining 80% of\nthe code is not, since it uses our (cockroachDB's) home-grown\nprotobuf-based RPC codec.\nDon't get me wrong. I have not drawn any conclusions yet. I just mentioned\nsome possibilities (if you read my message carefully, I mentioned this is\nNOT exact apple-to-apple comparison). If you think the difference is from\nyour custom codec (grpc uses the standard protobuf and protoHTTP2 uses your\nown one?), you probably should mention it when you published the results.\nOtherwise, it is really misleading when we read the results.\nPlease do not use this as a reference point.\nsure, no problem at all.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/549#issuecomment-190906170.\n. It seems at least I am getting confused here. :-) For the published results, what codec are used for\ni) GRPC\nii) ProtoHTTP2\nrespectively. Are they same?\n. Thanks, tamird. \n\n@zellyn, this is the reason why it is confusing -- GRPC and ProtoHTTP2 used two different codecs in benchmarks...\n. @tamird \nI did not check your code. But did you have multiple concurrent writers for a single streaming rpc? If yes, this is not supported for now. The current code may or may not work but we would claim it is not supported yet.\n. This does not help. Please show us your client and server code you were running.\n. This is still not clear to me whether this is a leak. Can you do the following instead:\ni) use fixed size  proto msg (e.g., 100k) instead of something random (i.e., zipf in your code) on client side;\nii) have a memory usage snapshot when the first rpc arrives on the server and have another when the last rpc arrvies. And show me the difference.\n. @seiflotfy is this still an issue?\n. close due to no response.\n. Non-TLS case needs some cmux type of connection dispatching solution we designed but not implemented. \nBut in general, if your system allows insecure connections, I do not see the reason you need to stick to a single port shared by http2 and grpc. You can have a grpc server and http server listening on 2 different ports in this case.\n. Please provide your use case to justify why your system cannot use 2 ports in the insecure case if you think we should pursue this further. It is not that hard to implement cmux in grpc but we want to focus on the real demands given the limited resource from our side. I think insecure port sharing is a really rare case and not needed by >99% grpc users right now.\n. To be clear, I was not saying the use case is illegitimate (I was actually saying we even have a designdoc for that). :)\nI am trying to clean up and organize (milestones and labels, etc.) all the issues in the repo for GA. I'll see what the best way is to deal with this kind of issues. Reopen this for now.\n. I fix the commit message if you think it is not great (or I can leave this to you). But I want to figure out the right fix.\n. Oh, if you wanted to reproduce the original failure, I am not sure how flaky it is (it just got failed when I merged your pull request and travis ran another pass on it). But it is clear that Body.Read returns a http2.StreamError there which introduced the problem.\nI am actually on vacation today so that the code review of #556 will happen tomorrow hopefully.\n. @bradfitz \n556 is in. Do you want me to revert this PR and leave this issue to you? Notice this makes the end2end test flaky and we should check in the fix asap. Thanks.\n. To be clear, a stream being closed with EOF is nothing to do with EOF in the log you posted. The log you posted is because the server is leaving.\nIn grpc, we always assume the client should always reconnect and the server should never actively shutdown a connection unless the entire server is down. The client should still reconnect when it gets EOF because it probably means the server is migrated to another machine.\nYes, there is some chance your checker has to wait longer to let grpc lib finish all the pending goroutine because I used some time.Sleep for some exponential back-off reconnecting logic. I plan to improve it actually.\n. What is the issue? you misunderstood the \"EOF\" and I clarified the intended behavior. \n. not yet. But it is definitely NOT about \"(*Conn).transportMonitor shutdown behaviour is inefficient\" as in the title of this issue.  It could be a new issue with something like \"remove sleep behavior in reconnection\".\n. Move the remaining improvement to #579\n. BTW, in your test, you can close your grpc.ClientConn before tearing down the server to avoid the problem.\n. I will change it to grpc.io.\nOn Fri, Feb 19, 2016 at 11:46 AM, David Symonds notifications@github.com\nwrote:\n\nThe link is correct (or could even be to grpc.io). It's a link for more\ninformation about gRPC in general, not about grpc-go specifically.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/561#issuecomment-186374231.\n. yup, a known issue for github hosting pages.\n. Failed on travis (it seems there are still unhandled errors):\n\n=== RUN TestClientRequestBodyError_Cancel_StreamingInput\n--- FAIL: TestClientRequestBodyError_Cancel_StreamingInput (0.17s)\n    end2end_test.go:458: Running test in tcp-clear environment...\n    end2end_test.go:458: Running test in tcp-tls environment...\n    end2end_test.go:458: Running test in unix-clear environment...\n    end2end_test.go:458: Running test in unix-tls environment...\n    end2end_test.go:458: Running test in handler-tls environment...\n    end2end_test.go:1783: got error transport.StreamError{Code:0x1, Desc:\"stream error: stream ID 1; CANCEL\"}; want transport.StreamError{Code:0x1, Desc:\"context canceled\"}\n. With grpc.WithPerRPCCredentials, it assumes the cred has a way to generate a token for each rpc automatically via GetRequestMetadata(). If this is not the case, you can use metadata API to pass your per-rpc tokens.\n. check out the metadata API (https://github.com/grpc/grpc-go/blob/master/metadata/metadata.go).\n. No, metadata works for both client and server sides. Check out some end2end tests (https://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L848) as examples.\n. @bradfitz Can you take a look? This seems introduced by the http2.MetaHeadersFrame change. I tried to append a \"\\n\" at the end of the error desc at https://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L97 and ran TestFailedEmptyUnary test case. It hangs there. Before the change of  http2.MetaHeadersFrame, it seems work fine.\n. Thanks, Brad. Yeah, I have planned to add the error/status encoding/processing in. But in my experiments (as I mentioned in my first reply), the client hangs forever instead of returning any error. Can you take a look?\n. This basically means the server code does not follow the grpc wire protocol which requires the server sends a trailer before an rpc is completed (your proxy might swallow the trailer from the grpc server and send an EOS data frame instead.). If it is the case, ServeHTTP should fail too (if it is not, it indicates an issue which needs to be fixed).\n. It is not clear to me what your setup is. I guess it is grpc-client <==>nghttp2 proxy<==> server.  Can you check what the proxy receives from grpc server when using grpc.Server and grpc.ServeHTTP respectively?\n. We need to figure out the difference. But from what you printed, it seems you only printed some intermediate results for ServeHTTP case -- e.g., what are the content of Grpc-Status and Grpc-Message?\n. We do need to provide a default codec using golang/protobuf for most of users who do not use custom codec and IDL. Any issue here? I do not see any circular deps issue here. \n. Personally I do not think it is an issue and there won't be any actions on this.\n. cc @xiang90\n. fixed by #680 \n. how to push a patch set to gerrit?\n. please rerun go get -u google.golang.org/grpc to get the latest updates in http2 package.\n. What go version do u use?\nOn Jul 31, 2016 8:34 AM, \"Sergey F.\" notifications@github.com wrote:\n\nsame issue, go get -u google.golang.org/grpc doesn't help\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/581#issuecomment-236436554, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJpyLDY86ihs33glq9pPcusNmHKKRNAsks5qbMB2gaJpZM4HmGio\n.\n. This uses grpc.Serve or grpc.ServeHTTP?\n. assigned to @bradfitz. I need to be on something else today.\n. Please move this proposal to grpc/grpc repo. This needs to be consistent across languages.\n. grpc/grpc#5377 seems only target max backoff delay. If we add a dedicated API for it, it may be problematic because later ppl might want to tune other params of the backoff mechanism and then we have add additional API which is not necessary. The API to be added should be generic enough to allow tuneup of all the relevant params of the mechanism.\n\n@stevvooe Feel free to send me the PR and in the meanwhile I will poke around other languages to see if they are willing to take the same change. Thanks.\n. @tamird Sorry. We are approaching GA very closely and so far this is treated as an experimental effort and we do not expect the real usage for this feature at the current stage. This is in Brad's plate and I am going to talk to him after GA to see how to make progress on this.\n. I agree. I simply had no idea what is going on with this PR. I remember you had some comments in this PR but could not find them any more.\n. The PR's description is out-dated. need either update.\n. what about #587? Should it be closed or merge?\n. Yes, we had a lot of discussions with Go team and decided not to make ClientConn an interface (which is  kinda common thing to do in other languages to support load balance channel).\nI think your requirements can be addressed once we have a good client interceptor solution. You can probably summarize your requirements and put it into a comment in the designdoc of the interceptor.\n. rpcError is typically the error status of a particular rpc. But net.Error usually indicates the connection level error. They are on different levels of concepts. You do need to use the error code to detect timeout of an RPC. I am thinking making transport.ConnectionError implements net.Error though.\n. Some side story: we used \"cancelled\" originally and then were advised by Go team to change it to \"canceled\" to have in-language consistency. :)\n. Thanks for reporting. the fix will be out soon.\n. The problem u ran into is that you used illegal header name in http/2 -- all header names in http/2 should be lower case. which is enforced by the recent change in golang http2 framer. The best fix here is probably returning a meaningful error.\n. I had minor concern that this internal conversion might introduce a little confusion on the server side.  But probably it is okay to do that (at least consistent with general metadata processing)...\n. Thanks. All done.\n. @menghanl\n. @adg Andrew, can you help review this?\n. any updates?\n. I think you need to submit the form. Anyways, it seems the bot has some issue. Myself was forced to resign it recently.\nIs the PR ready for review. If yes, can you remove \"WIP\" in the title?\n. Should be fixed in a sync.\nhttps://github.com/GoogleCloudPlatform/gcloud-golang/issues/231\n. This seems reveal a data race in serverHandlerTransport. \n@bradfitz\nCan you take a look? \n. Pls remove 1.4 then. I will update the README correspondingly.\n. duplication of #576 \nNewline is an illegal char in HTTP/2 so you can PROTOCOL Error which is expected behavior. But I do think HTTP/2 framer should give more detailed info about the error instead of simply returning PROTOCOL Error.\n. @bradfitz nice I am going to see how to instrument it into grpc. Thanks.\n. @bradfitz \nWhy don't we detect the illegal characters on the writing path to eliminate it in the early stage? Only enforcing the check on the reading path introduces some issue to gRPC -- e.g., if a client sends an illegal char in metadata, it is detected when the server's framer reads it then the server sends a rst_stream frame to the client. Because rst_stream only has an error code, the client loses the real root cause.\n. this solution cannot interop with other languages.\n. This does not interop with other languages which have no idea what the grpc-message-bin mean.\n. you probably should file issues to grpc/grpc and grpc/grpc-java and have your proposal there. Once we reach agreement, we can check in the fix at same time to minimize potential breakage.\n. I think https://github.com/grpc/grpc/issues/4672 is the decision.\n. I signed it!\n. @bradfitz can you check why https://github.com/golang/net/blob/4876518f9e71663000c348837735820161a42df7/http2/http2.go#L348 does not list all ascii? colon and slash should be there.\n. I see. BTW, this is a google example on checking illegal chars on reading path does not give good error info.  Can you add it? We can definitely add the checking in gRPC but it seems it should go into http2 frame code. If you are short of time, we can it to gRPC as a short term solution.\n. Sorry for confusion. I meant the checking should be added to the writing path. ErrorDetail is only for reading path.\n. @bradfitz in that case, it seems it makes sense grpc http2 transport enforces the rule by itself instead of relying on http2 framer.\n@ThomasHabets I actually agree with you regarding the abstraction. I am going to talk to the team to see how we can improve it.\n. done.\n. I think you only need use grpc.WithCodec(...) to pass in your custom codec. Why do you want to change SendMsg. Do I miss anything?\n. I see. Yes, feel free to use that. that's the way to go. That API is pretty stable now. \n. I saw it on travis when running in race mode with cpu 4. But even with that, I could not reproduce on my desktop with 1k run. In general, when is the STREAM_CLOSED err generated?\n. So maybe running in race mode with multiple cores on a slow machine might help ...\n. @bradfitz \nThis happened again at https://travis-ci.org/grpc/grpc-go/jobs/120696193 today.\n=== RUN   TestHealthCheckOff\n--- FAIL: TestHealthCheckOff (0.13s)\n    end2end_test.go:463: Running test in tcp-clear environment...\n    end2end_test.go:463: Running test in tcp-tls environment...\n    end2end_test.go:463: Running test in unix-clear environment...\n    end2end_test.go:463: Running test in unix-tls environment...\n    end2end_test.go:463: Running test in handler-tls environment...\n    end2end_test.go:694: Health/Check(, ) = , rpc error: code = 13 desc = , want , rpc error: code = 12 desc = unknown service grpc.health.v1.Health\n. This happened again at https://travis-ci.org/grpc/grpc-go/jobs/121308435. I am going to disable the test in the handler env. Hope you can have a fix soon.\n. This happened again to a newly added test (if my memory works, I also ever ran into this when running the tests several thousand times inside google):\n--- FAIL: TestStreamServerInterceptor (0.26s)\n    end2end_test.go:465: Running test in tcp-clear environment...\n    end2end_test.go:465: Running test in tcp-tls environment...\n    end2end_test.go:465: Running test in unix-clear environment...\n    end2end_test.go:465: Running test in unix-tls environment...\n    end2end_test.go:465: Running test in handler-tls environment...\n    end2end_test.go:1832: &{0xc82051d8c0}.StreamingInputCall() = , rpc error: code = 13 desc = , want _, error code 7\nhttps://travis-ci.org/grpc/grpc-go/jobs/129817040\nI am going to disable this case also until you get time to fix it.\n. The comments do not say \"they MUST be called by the generated code.\". So I think the more appropriate change would add a sentence like \"Users can call them directly when it is really needed in their use cases.\". \n. Connection error is treated as transient so that grpc internal keeps retrying until you set a timeout.  I expect you can see something from log with mismatched certificates.\n. s/until/unless\n. I can imagine that in some cases the name resolver or load balancer may direct you to another server which can accepts the cert. I am still debating with myself whether I should introduce some more tweaks into the connection errors.\n. Let me talk to our security guru to decide whether security handshake error should be treated as a fatal error first.\n. After the discussion, we will treat this as a fatal error. A pull request will be out to address this soon.\n. @menghanl is working on a PR to improve this. It should be out soon.\n. @stevvooe The thing becomes a bit tricky when there may be multiple underlying network connections. @menghanl will revise the PR (or in another separate PR) so that if WithBlock is not set:\ni) If the rpc is fail-fast, it fails immediately with the fatal certificate error;\nii) if the rpc is not fail-fast, the rpcs keeps waiting since the next address(i.e., a new server) from the name resolver/balancer might save the world.\n. @tamird I think typically this kind of error is on the server side and there is some chance a newly started server fixing the problem. Technically, a non-fail-fast rpc never fails with a single connection error regardless it is fatal or not. \n. @tamird In most of use case I have seen the client simply uses the machine CA (credentials.NewClientTLSFromCert(nil, \"\")) which has slim chance to get wrong compared to the server side.\n. @stevvooe Understood. To be clear, I said \"most of use cases I have seen\" instead of all the cases. \nFor non-fail-fast rpc and multiple underlying network connections, this is a complex mess even for Google internal rpc systems for years. The workaround we always suggested to our prod users is setting a deadline to the rpcs and checking the error log. This is not a good solution and we should improve. My thinking is that when a non-fail-fast rpc fails, we can put the real reason into its description so the the caller can take action. In addition, we can even add some grpc.IsErrorRetryable(...) to detect if the returned rpcError can be retried (e.g., in this case, it returns false). This deserves a good design.\nAs a short term goal, we can fix it when Balancer is not in the picture -- there is only a single destination address for both failfast and non-failfast rpcs. This will fix @tamird 's concern at least.\n. Stress testing is to explore the bugs in the code. It is not performance benchmark. We are working on performance benchmark too. And yes we will publish the benchmark results on our environment.\n. LGTM\n. I do not understand what you want to have. Can you elaborate?\n. Thanks for reporting. I had a fix and will polish it and send it out tomorrow.\n. not sure how you logged. I did observer t.fc.pendingData drops (the max is about 16k). But yes, I do observe the problem stays.\n. I think I found the problem. the fix will be out tomorrow.\n. all done. PTAL.\n. The issue is that both writes block until the peer completes read. The current code does not work for this kind of synchronous connection. One easy fix is to introduce the ordering for read and write (e.g., server always reads before write) when the connection is setup. But it slows the connection setup process. I am going to think how to resolve this in a decent way.\n. ok, I got a fix. will send out tomorrow.\n. Sorry for the breakage. This breaks only if you cache and use the generated code directly. Otherwise, it should be transparent to users. Anyways, I am going to pay more attention on this and will announce any potential breaking changes on a github issue. You do not need to subscribe anywhere else.\n. @riannucci It is because gRPC and protobuf are not a bundle. Users are allowed to use other things as their IDLs. But all the IDLs need to share this same API -- this interface{} could be a proto.Message or a slice or something else.\n. For the streaming rpc interceptor (which I am working on) the codegen is not changed. For unary rpc, we have to do that because protobuf decoding happens in the generated code. The interceptor impl needs to call handler(ctx, in) by itself to complete the rpc as usual if they want. We will have a doc to describe the design and show some examples.\n. We do not plan to support dynamic server so far.  For now, once s.m is created, it is read-only so that no mutex lock is needed to dispatch an inbound RPC. \nBTW, your PR does not add the lock on that code path and hence introduces race condition.\n. I am closing this PR now. We can revisit when it raises enough interest.\n. BTW, in this way you introduced a major contention point for all the RPCs on this server. I expect it will slow down your RPCs a lot when you have a number of in-flight RPCs.\n. RLock is lighter than Lock but it is not free. I just tried to remind you the potential performance impact you might run into.\n. https://golang.org/src/sync/rwmutex.go\n. chaining is left to contrib/ or the applications themselves. :)\n. The \"error\" param is the recommended way to handle your application errors. The \"error\" in the proto is a workaround if the error is a complex object (e.g., it is a proto msg).  Another workaround is to use trailer metadata (https://github.com/grpc/grpc-go/blob/master/Documentation/grpc-metadata.md) to transmit the error.\nCheck the tutorial of the grpc in other lanaguages (\"Tutorial section\" in http://grpc.io) to see how they handle the errors.\n. I am baking net.Pipe() into the end2end test as another network connection type (along with tcp and uds). I would put it into a separate PR due to the interest of time.\n. As @chancez pointed out, you can attach it to the context.\n. Please roll your own. It is important to minimize the public API for gRPC-Go at the current stage (we already have massive public API). Otherwise I can see we would easily run into API stability and maintenance problems when more and more new feature requests pop out in the future. I also think providing multiple ways to users for a single purpose sometimes hurts the user experience.\n. I think contrib will be out soon. It is outside the library repo -- we will have a new org and multiple repos for contrib.\n. @lukaszx0 I have no idea. We are not in the loop of this effort.. You can use Context() on grpc.ServerStream object directly.\n. a bit more elaboration: you can call Context() to get the context object and attach your data onto that. And then have a new grpc.ServerStream impl whose Context() returns your modified context object. You should use this new ServerStream to call the StreamHandler.\n. I think I did not. I described the way to get the ctx and feed it back to the handler with the modifications without adding any API.\n. I agree it is a bit harder than the unary case. I am pretty strict on adding any new public API to the lib though. It seems you can define and impl this function in your own code base?\n. It is not hard to add a simple wrapper to make this work simple. We will have a contrib repo to host the contributions from our users and I think you can make one for ppl having the same requirements there.\n. I have not followed the discussion on nghttp2. But why does nghttpx attach an empty data frame when it sees a header-only response? This is the root cause it failed the grpc client.\n. Does your version contain https://github.com/grpc/grpc-go/pull/649?\n. finished the initial pass.\n. @jtattermusch FYI this has been merged.\n. As ppl already pointed out, you need to sync both grpc and protobuf to make it work. It is a nice feature we recently added to guarantee the compability of these two libraries. \n. It would be a separate repo instead of living in other grpc repo.\n. @zinuga I think per-language (Go, Java, C) contrib repo would be better as Peter suggested.\n. https://github.com/grpc/grpc-contrib\n. I will have upcoming change to Picker API so that the connecting is redirected different backends in each retry. If none of the backends is up, the client blocks. In my understanding this should satisfy your use case. \nIn general, grpc lib wants to take over the task of reconnecting instead of pushing it to the applications.\n. I can make this as the behavior when dialing time-out is set to 0. But it is hard to give u a firm ETA because of some other high-priority tasks I am working on.\n. Is the client the grpc-go client? Any way I can repro?\n. Are there any rpc cancellation/timeout/failure along with this symptom?\n. Please reopen if the issue remains.\n. I am writing the test but currently have difficulty to reproduce the issue. I will keep working on that. checked this in now because I will take the next a couple of days off (not sure I can finish it today before I leave). Can you guys help verify whether it fixes the issue?\n. Thanks for reporting. The fix will be out soon.\n. We are investigating this issue internally. It seems once an rpc timeout happens, the google frontend server stops working (the grpc client can still send requests to the google frontend servers but nothing is received till the rpc is timeout).\n. @stxmendez This should be an issue in cloud bigtable instead of grpc since as I stated, I think grpc client behaves as expected but google frontend server stops responding.\nhttp2_server is irrelevant. The google frontend server and bigtable server are not written in grpc-go.\n. This has been fixed from both google GFE and grpc-go.\n. This is pretty much the right fix (it misses some condition though). I am making my own fix and the proper testing (almost done).  I am closing this one unless you want to take it over and have the proper test. :)\n. You do not need to make a new connection for every RPC. A connection can be multiplexed by multiple clients or RPCs.\nWe have health check service (https://github.com/grpc/grpc/blob/master/doc/health-checking.md) so that a client can detect the health of the services. We are also designing the connection level story (L7 counterpart of TCP keepalive).\n. grpc.io email list would be the better place for these questions. close this now.\n. Sounds good to me.\nI am not sure how comes the test failure. Can you verify if it is from your change?\n. @mwitkow Sorry about the delay. We are busy on a bunch of Pre-GA stuffs. We have to put this as a post-GA item.\n. This is not the plan yet. We probably will come up with a more sound story for rpc status but it is not decided yet. It would be good know why the current form cannot satisfy your requirements. \n. Isn't the following working?\nreply, err := foo.bar(ctx, &req)\nif err != nil {\n  if grpc.Code(err) ==  codes.Unavailable {\n    ...\n  }\n}\n...\n. Your service handler impl can return an rpcError using grpc.Errorf(...). Do I still miss something?\n. I am not strongly against this. We can probably make it happen along with the server reflection support @menghanl is working on. It may provide richer information -- e.g., beside all method names it may also provide the type info of the input&output params.\n. @zellyn We just had some discussion and plan to extend the API a bit to accommodate your requirements.  The revision will be sent out this afternoon if we do not find the issues in this plan.\n. @zellyn Moved the name resolution back to Balancer to have more flexibility. Also added Notify() API to let Balancer initiate the connections. I think now it can implement your policy easily.\n@tecbot You can count the streams/rpcs per connection in your own Balancer impl via Get and put calls.\n. @tecbot the max stream limit is part of flow controller instead of load balancer. I do not think flow controller should be part of load balancer. They are separate components which are in charge of traffic throttling and balancing respectively. Even if we notify the max streams limits to Balancer somehow, there is still no guarantee that rpc can go through because it may be throttled by the flow control window of this stream or the corresponding connection. In other words, there is no guarantee an rpc can be finished immediately on the server returned by the Balancer. In addition, this kind of notification is always racy because the server could change the setting anytime during the execution.\nPlus, we do not expect to implement over-complicated client load balancing policy. That should be implemented via an external load balancing server (https://github.com/grpc/grpc/blob/master/doc/load-balancing.md) which can collect the traffic stats from clients and servers. If you decide to make a custom one, you might be able to put max streams limit as part of reporting stats from server.\n. all done. PTAL.\n. @zellyn The name resolver is part of load balancer. I originally wanted to move naming out of balancer. But it seems making it part of balancer brings a lot of flexibility and enable the impl of some more complex balancing policy. Otherwise, we need to add quite a few API to Balancer to communicate the name resolution changes, which is pretty difficult and error prone in my experiments.\n. @zellyn Yes, I am actually considering to move naming back to grpc package ...\n. We haven't had the concrete plan to support versioning yet. You need to vendor by yourself and pay attention to the issue/pr with \"[API revision]\" in the beginning of the title from gRPC-Go. Sorry about the inconvenience.\nBTW, these state API is inherent racy (the state may change between it is returned from the function and you perform any real operations correspondingly) and should be avoided anyways. Plus it is extremely difficulty and actually undefined when there are multiple network connections under a single grpc.ClientConn.\n. @mikeatlas You should dial with grpc.WithBlock() to make sure once Dial returns the network connection is ready.\n. @derekchiang a ClientConn is closed if and only if you call ClientConn.Close().\n. I would say if you rely on the State API, there is a very high chance you are doing something wrong/inappropriate.\n. @derekchiang I have no idea about your use case. But in principle, the network connectivity is NOT the thing the application should care about. gRPC tries to deal with it for you. For example, if there is only 1 backend, gRPC internals would keep reconnecting if it is disconnected. If there are other backends available, gRPC internals would try to send your rpcs to other connected backends according to the load balancing policy you use. \nThe applications can sense the network reachability via setting a deadline for an rpc. There are few cases the applications do need some insight about the reachability. We provide health check service so that you can probe the server periodically. We are also working on some connection layer mechanism (similar to TPC keep-alive) too.\n. Plus, let me emphasize it again. The state API is racy -- it does not guarantee your rpcs will get the same state when you send them.\n. @karthequian I would recommend the former to reduce the resource consumption (fd, memory, etc.) unless your application has very demanding throughput requirement. \nIf you are talking about TPC TIME_WAIT state, it is expected.\n. closed via #690 \n. If we choose to disable it, the users may use timeout of an rpc on that ClientConn to achieve the similar effect.\n. We are working with another team in Google on replacing the existing tracing. Stay tuned.\n. The fail-fast semantics has not reached the full agreement across languages (still under discussion). I will roll it out once it is finalized.\n. closed via #738 \n. gRPC-Go API is pretty Go idiomatic so that it only has blocing/sync API. This is the whole point of Go's concurrency pattern to make things simpler to code and maintain. It may or may not have performance impact depending on your use cases. We do not have plan to add async API. If you are really demanding on performance and like async RPCs, you can probably try gRPC-C/C++.\n. sounds good. Feel free to contribute.\n. closed via #755 \n. @danruehle sorry about the delay. Can you rebase? Let's get this in soon.\n. Since you implement your own ServerStream you can attach whatever context you want.\n. when you implement https://github.com/grpc/grpc-go/blob/master/interceptor.go#L74, you get a \"ss\". You can use \"ss\" to implement your own ServerStream. In this step, you can decide what context ServerStream.Context() returns. Then when you call the grpc.StreamHandler (e.g., https://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L1804), you can pass in your own ServerStream instead of the original \"ss\".\n. This is not an grpc issue. My speculation is that CA is not loaded properly (e.g., due to the wrong path, etc.). I am not an expert here. Please use google to find the solutions (e.g., https://github.com/docker/docker/issues/8849). \n. There is no way gRPC-Go can support all kinds of compression algorithms. We only have plan to support a small number in standard Golang library (e.g., gzip, deflate). Snappy support can move to the contrib repo we will add shortly.\n. One reason I do not want to put this into gRPC-Go repo because it introduces a new dependency on another github repo, which is the thing I have tried hard to minimize. I think it should go to contrib repo instead.\n. I have not got time to have a close look. But a quick peek found you did not call s.Stop() to terminate the grpc server, which is needed to have a clean shutdown of a server. Can you try that?\n. The stack trace you posted is for a server transport instead of a particular rpc. If there are many of them, it means your clients create a lot of grpc.ClientConn to the server without closing them. I do not see any thing abnormal here. \n. errConnTimeout is never returned by an rpc.\naddressing offline...\n. I think this is incorrect. OnRead is called iff the application consumes the received data or there is any stream on this transport being closed. OnData is the one to be invoked when receiving a dataframe.\nWhen the capacity of an transport is completely assigned to the pending steams, no new streams should go through. This is the expected behavior.\n. 1.7 has not been released. Let's worry about it later.\n. Because this will be buried in a lot of other things after 2 months and some ppl will create a new one (most ppl won't check if there is a duplicate already) when 1.7 is released and we need to dig out this \"old\" one and mark the new one \"duplicated\"...\nI do not mind reopening this if you want anyways...\n. Feel free to contribute.\n. @jboeuf Even though we are making API breaking change, we still want to minimize the impact. :) I would propose to\ni) change WithTransportCredentials(TransportAuthenticator) to WithTransportCredentials(TransportCredentials);\nii) keep WithPerRPCCredentials(PerRPCCredentials) intact.\nIn this way, we do not break users unless they implemented their own Credentials/TransportAuthenticator previously and these names sound sane to me. What do you think?\nWithGoogleDefaultCredentials is good. It is an incremental change to this PR. Let's do it in a separate PR.\n. LGTM\n@xiang90 This spawned goroutine will also exit at the first error. Do I miss anything?\n. @xiang90 For simplicity, I let it exit at the first error now. I agree it is debatable what is the best thing to do here. Probably it should be exit if all the connections. \n. It seems context cancellation is racy with context printing .... It just showed up today on travis .... need further investigation.\nWARNING: DATA RACE\nRead by goroutine 1688:\n  reflect.Value.Uint()\n      /tmp/workdir/go/src/reflect/value.go:1701 +0x7e\n  fmt.(_pp).printReflectValue()\n      /tmp/workdir/go/src/fmt/print.go:867 +0x4cf\n  fmt.(_pp).printValue()\n      /tmp/workdir/go/src/fmt/print.go:848 +0x551\n  fmt.(_pp).printReflectValue()\n      /tmp/workdir/go/src/fmt/print.go:932 +0x4aa5\n  fmt.(_pp).printValue()\n      /tmp/workdir/go/src/fmt/print.go:848 +0x551\n  fmt.(_pp).printReflectValue()\n      /tmp/workdir/go/src/fmt/print.go:1009 +0x4352\n  fmt.(_pp).printArg()\n      /tmp/workdir/go/src/fmt/print.go:810 +0x652\n  fmt.(_pp).doPrintf()\n      /tmp/workdir/go/src/fmt/print.go:1219 +0x2a52\n  fmt.Sprintf()\n      /tmp/workdir/go/src/fmt/print.go:203 +0x7c\n  golang.org/x/net/context.(_valueCtx).String()\n      /home/travis/gopath/src/golang.org/x/net/context/pre_go17.go:292 +0x1ff\n  fmt.(_pp).handleMethods()\n      /tmp/workdir/go/src/fmt/print.go:730 +0x75a\n  fmt.(_pp).printArg()\n      /tmp/workdir/go/src/fmt/print.go:806 +0x5b6\n  fmt.(_pp).doPrintf()\n      /tmp/workdir/go/src/fmt/print.go:1219 +0x2a52\n  fmt.Sprintf()\n      /tmp/workdir/go/src/fmt/print.go:203 +0x7c\n  golang.org/x/net/context.(_valueCtx).String()\n      /home/travis/gopath/src/golang.org/x/net/context/pre_go17.go:292 +0x1ff\n  fmt.(_pp).handleMethods()\n      /tmp/workdir/go/src/fmt/print.go:730 +0x75a\n  fmt.(_pp).printArg()\n      /tmp/workdir/go/src/fmt/print.go:806 +0x5b6\n  fmt.(_pp).doPrintf()\n      /tmp/workdir/go/src/fmt/print.go:1219 +0x2a52\n  fmt.Sprintf()\n      /tmp/workdir/go/src/fmt/print.go:203 +0x7c\n  golang.org/x/net/context.(_valueCtx).String()\n      /home/travis/gopath/src/golang.org/x/net/context/pre_go17.go:292 +0x1ff\n  fmt.(_pp).handleMethods()\n      /tmp/workdir/go/src/fmt/print.go:730 +0x75a\n  fmt.(_pp).printArg()\n      /tmp/workdir/go/src/fmt/print.go:806 +0x5b6\n  fmt.(_pp).doPrintf()\n      /tmp/workdir/go/src/fmt/print.go:1219 +0x2a52\n  fmt.Sprintf()\n      /tmp/workdir/go/src/fmt/print.go:203 +0x7c\n  fmt.Errorf()\n      /tmp/workdir/go/src/fmt/print.go:212 +0x78\n  google.golang.org/grpc/test_test.(_testServer).UnaryCall()\n      /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:137 +0x47f\n  google.golang.org/grpc/test/grpc_testing._TestService_UnaryCall_Handler()\n      /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:595 +0x17d\n  google.golang.org/grpc.(_Server).processUnaryRPC()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:530 +0x12c5\n  google.golang.org/grpc.(_Server).handleStream()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:687 +0x1418\n  google.golang.org/grpc.(_Server).serveStreams.func1.1()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:352 +0xad\nPrevious write by goroutine 1473:\n  google.golang.org/grpc/transport.(_http2Server).closeStream()\n      /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:737 +0x26d\n  google.golang.org/grpc/transport.(_http2Server).handleRSTStream()\n      /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:378 +0x8f\n  google.golang.org/grpc/transport.(_http2Server).HandleStreams()\n      /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:272 +0xd59\n  google.golang.org/grpc.(_Server).serveStreams()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:354 +0x1dc\n  google.golang.org/grpc.(_Server).serveNewHTTP2Transport()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:341 +0x54d\n  google.golang.org/grpc.(_Server).handleRawConn()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:318 +0x5b1\nGoroutine 1688 (running) created at:\n  google.golang.org/grpc.(_Server).serveStreams.func1()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:353 +0xa7\n  google.golang.org/grpc/transport.(_http2Server).operateHeaders()\n      /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:209 +0x1957\n  google.golang.org/grpc/transport.(_http2Server).HandleStreams()\n      /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:268 +0xf3f\n  google.golang.org/grpc.(_Server).serveStreams()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:354 +0x1dc\n  google.golang.org/grpc.(_Server).serveNewHTTP2Transport()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:341 +0x54d\n  google.golang.org/grpc.(_Server).handleRawConn()\n      /home/travis/gopath/src/google.golang.org/grpc/server.go:318 +0x5b1\nGoroutine 1473 (running) created at:\n  google.golang.org/grpc.(_Server).Serve()\n. @tamird The reason this change is not appropriate for gRPC-Go repo is that we have tried hard to minimize the dependency of gRPC core in order to reduce the chance this repo becomes broken (or prohibits further submissions) due to the failure of other repo. Your PR introduces an additional dependency (i.e., the generated code of protobuf) for testing only, which will force grpc-go strictly sync with every single change of proto codegen (even not grpc plugin) regardless the change is breaking or not. This binding sounds too strict to me. I hope to hold some space to accommodate protobuf changes. I agree that this PR is a good alerting signal but I do not want to put it as a strict line.\n. The travis failure should be fixed by #784 which will be merged shortly.\n. We still have a couple of races which need to be fixed. GoAway is evil ...\n. LGTM\nPlease coordinate with proto team for the merging.\n. LGTM\n. There were. But I removed them recently because they are undefined when a grpc.ClientConn may contain multiple underlying network connections (https://github.com/grpc/grpc-go/issues/691). They are also undefined in this case for other languages too. I do not want to leave broken/undefined API when we are approaching GA. So far I do not have plan to revive them. FYI, these API are always racy -- the info you get from them may or may not be out of date already when you start to use them. The only valid use case I have seen is that when users want to hold the 1st rpc until the network connection is established. But this has been achieved by grpc.WithBlock() in gRPC-Go.\n. quoted from http2 spec:\n\"The server connection preface consists of a potentially empty SETTINGS frame (Section 6.5) that \nMUST be the first frame the server sends in the HTTP/2 connection.\"\n. I wanted to quote the following instead :)\n\"The client connection preface starts with a sequence of 24 octets, which in hex notation is:\n0x505249202a20485454502f322e300d0a0d0a534d0d0a0d0a\nThat is, the connection preface starts with the string PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n). This sequence MUST be followed by a SETTINGS frame (Section 6.5), which MAY be empty. \"\n. According to the error you saw, it seems the client sends WindowUpdateFrame before the Preface. My speculation is that they still send Preface every time but due to some race, the preface may arrive after WindowUpdateFrame.\n. I think you can implement your custom grpc.PerRPCCredentials which attaches the token to the first call and empty token to the others.\n. note that grpc.PerRPCCredentials is installed via a DialOption, which means it is per-connection thing and all the invocations of GetRequestMetadata are on the same connection.\n. Let's first clarify some terms to make our communication better.\ni) an RPC may have 1 request and 1 response (unary) or N requests and M responses (streaming);\nii) Per your question, you want oauth token attached on the first RPC on the connection and no token on the following RPCs on the connection? Correct?\n. Metadata API was designed without a plan to support interceptor at that time. Actually, I think SetTrailer works (i.e., it can be called multiple times before eventually returning from the interceptor; I need to verify though). For Header metadata, probably we can add another SetHeader API which stores metadata without sending (all metadata will be sent out when SendHeader is called.). \n. I am not sure I understand what exactly your question is. Do you mean \"conn.Recv()\" sometimes falls over with \"os.Exit(1)\"? If yes, this is expected, streaming RPC is NOT tolerant to the network errors.\n. I am still confused. What do you mean \"stop receiving messages\"?\n. Do you mean \"conn.Recv()\" is blocking even though you know the server sends something back to the client? This should not happen. Can you send me a reproduction?\n. perhaps it is the reason. But even in that case, conn.Recv() will eventually error out when TCP user time out trigger (but maybe up to tens of minutes).  One workaround would be hosting the health checking service on your server (https://github.com/grpc/grpc-go/blob/master/health/health.go) so that your client can periodically probe the server and cancel the pending streaming rpc if the server becomes unreachable. \nIf this is the case, do u mind closing this issue and creating a counterpart issue like grpc/grpc-java#1648 to make your requirement crystal clear?\n. We are planning to do extensive performance optimization from next quarter, which may be client/server specific. Thus this kind of merging won't happen in near term (TBD in long term). Thanks for your effort. I know this would take a lot of time. It would be great if you talk to us beforehand to have a consensus before you dive into this big refactoring of the code next time.\n. I think you only need to let leak checker skip this goroutine as we did in https://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L1910.\n. closed by #780 \n. Can you rebase? If the travis build is still failed, please fix it or let me know.\n. @heyitsanthony @xiang90 As @menghanl mentioned, we want to detect the fatal error and notify the callers ASAP. For examples, if you give grpc 10 addresses and 9 of them have fatal errors and 0 chance to revive later, our reasoning is that you want to know this immediately when dialing instead of throwing tons of rpcs onto this single address which is alive.\n. Sorry about delay. Will take a close look after finishing some urgent stuffs. I marked this as part of GA milestone.\n. I gave it a second thought and here is my proposal and the reasons:\nThe requirement for this should be:\ni) if a user does not specify custom dialer, he should not need to do anything special;\nii) if a user wants to specify his custom dialer (via WithDialer), he needs to to take care the difference between 1.6 and pre1.6. It is just like grpc-go as a golang net package user needs to deal with this divergence. The same thing applies to the users of grpc-go. We cannot hide all these divergences inside grpc.\nMy proposal would be:\ni) Still put the functions you added into transport package, but do not export them;\nii) grpc-go/test needs to have go1.6 and pre-go1.6 files to host dialer functions used by the end2end tests (for uds dialer) so that the tests will pass with go1.6 and 1.5. This mimics a real user who want to set his custom dialer.\nThis satisfies the above 2 requirements without exporting any new API. What do you think?\n. Don't underestimate the pain when you roll out an API and cannot roll it back at google's scale. I have been very cautious on this before I had the experience. The thing could become even worse for grpc because grpc is used in both Google and outside. If you really want to provide a simple sugar for other ppl who have same demand, you can make a contribution to https://github.com/grpc/grpc-contrib.\n. Sure, will go to that shortly. doing some final touchup for the race fix.\n. LGTM\n. @tamird sgtm\n. It is still unclear to me how you run into the problem. grpc.Dial does not block unless you specify grpc.WithBlock DialOption.\nCan you show me how you dial two addresses in this code?\n. LGTM when the comment is fixed.\n. > net.Dial does not do cancellation correctly (#751).\nYour proposal is not sufficient. There are some use cases which requires some special dialing ways (e..g, dialing from a GAE) that cannot be achieved by net.Dialer. We need a better solution. But technically, not all the dialing operations can be cancelled.\n\ngrpc.Dial takes only a \"target\" string, compared to net.Dial which takes \"network\" and \"address\" strings. This makes it necessary to supply a \"dialer\" which is hardcoded to \"unix\" in some tests.\n\nAs a rpc lib, users usually typically only has a service DNS name and do not know what network they should use. This makes it different from net.Dial case. This also leaves the possibility grpc internals can choose the optimal network type for the users in the future (e.g., for intra-cluster traffic, use RDMA, for inter-cluster traffic, use TCP, etc.). Maybe we can add a grpc.DialNet API for you if you really want to specify network.\n\ngrpc.WithTimeout is used to control grpc.Dial, but context.Context is used to control timeouts on individual RPCs.\n\ncontext is request scoped and I am not convinced it applies to connection. In addition, I think it introduces the confusion to the users.\n. > OK, let's discuss that in #751. In particular, what special dialing requirements are you thinking of?\nSure, replied there already. Some of them cannot be canceled.\n\nSeems like putting the cart before the horse. Take a look at some of the tests which already have to jump through hoops to get unix sockets working.\n\nMy stand is that golang net package is like the socket lib in C/C++ which provides the functionality for users to develop various communication apps/tools. RPC lib is one of them and builds on top of golang net. Since it is an application of golang net instead of a counterpart, it is reasonable to hide some config details (e.g., network type) inside just like we do not expose all socket type and options to our users when developing using socket. In my opinion, it is not necessary to let users specify the network type since most of them do not care. A small number of advanced users can use WithDialer option to achieve that purpose when they want. In addition, even if we add \"network\" to grpc.Dial, it is not equivalent to net.Dial because grpc does not support UDP and IP but instead it can support \"in-process\" and net.Pipe which are not supported by net.Dial. Thus we need to define our own list of \"network\". And again all of these sound overwhelming to common users.\n\nIs it? Go 1.7 introduced net.DialContext, so it seems that upstream disagrees with you. Perhaps @bradfitz could provide some guidance.\n\nGood to know. Interestingly if you look at https://github.com/grpc/grpc-go/blob/a0ff1e78a98cd140c2d18a0210ab0a2a183a2b4c/transport/http2_client.go#L70, you can see I had the similar concept but it was rejected by them. But now it seems they have changed their opinon. I think we could add another API (something like grpc.DialContext(...)) or DialOption while keeping the existing API intact if this is really helpful (for cancellable dial).\n. Yes, we will expose the hooks to configure that but we have not setup an ETA for that now. Will ping this back once we sort out the plan.\n. @AmandaCameron Yes, exactly. We can probably add something like \"server reset the rpc\" into the error description.\n. They are two different things -- one is to track the version of the generated code of grpc; the other is for protobuf itself.\n. Yes, some users already reported this. The fix should be out soon.\n. @mwitkow .Send() returns io.EOF to indicate the entire rpc is done. You still need to call CloseAndRecv() to get the final status. There will be a end2end test to show the usage pattern.\n. still buggy. please hold for an update.\n. LGTM\n. I do not see anything on our performance dashboard (https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5760820306771968). Do you have any etcd side changes along with this?\n. That benchmark we use is at https://github.com/grpc/grpc-go/tree/master/benchmark/worker. menghanl@, can you provide the instruction to run them locally?\n. Do you use custom name resolver or load balancer?\n. The only thing suspicious to me is the change of the round-robin load balancer -- to make it fit for fail-fast we added a bit memory alloc and copy. I do not expect it introduce any visible performance impact though (as shown in our perf dashboard). It might be possible your benchmark scenario runs into some cases to enlarge the impact of theses mem alloc and copy significantly. Let me see if I can improve the impl of round-robin balancer.\n. sounds good. Thank you.\n. We probably should. We also found some performance regression from 1.6 to 1.7 in some internal benchmarks.\n. LGTM otherwise.\n. LGTM otherwise\n. LGTM, will merge once travis is green.\n. @tamird Thanks for this. This is nice to have and I would like to merge this before GA. Can you fix the script error on travis? Thanks.\n. will have a new one to replace this.\n. @tamird Yes, it should be. I will do an entire pass on this file in a separate PR. Most of tests exist before codes became stringers.\n. Thanks for reporting. Looking ....\n. Yeah, it is expected. :)\n. Yes, will do it this afternoon.\n. Yes, we will do. Sorry about that. We made a mistake at that time and cannot roll it back.\n. has been fixed in #801 \n. ugh, LGTM :)\n. @puellanivis Yup, that is the right thing to do. \n. I haven't seen this before. Anyway it can be reproduced?\n. Channel sometimes can be more flexible in my opinion. But yes, I am open to this as long as we can make the thing better. Thanks for your contributions.\n. Do you want me to talk to go-team about this? According to your comments the tls package seems have fixed this already. So we are all fine once this PR is in?\n. I think it is because GFE closes the connection every 4 minute. On grpc-go side, we do need to improve logging and reduce logging spamming.\n. dup of #120. closed.\n. LGTM\n. It sounds good but my concern is that this adds some overhead to every single rpc and slow them down.\n. I think so. it was merged. I close this one. Please reopen if it remains.\n. anyway we can reproduce?\n. I remember I saw something similar one or two times in the past 2 weeks in our testing suite. It seems only happen on traivs and I cannot reproduce locally. I am looking. Is it possible you sync to the latest code so that we can i) make sure it remains in the latest code and ii) debugging effort can be synchronized? Thank you. Sorry about all these troubles. We have made quite a few complex changes (e.g., goaway) recently for GA and they seem introduce some issues ...\n. BTW, I have run our test suite thousands of times locally and about ten times on travis but have not seen anything yet.\n. Thanks, it works! I will take a look tomorrow or Monday. Have a good weekend.\n. Yeah, I found the root cause and have a fix now. will give it another careful check and send out on Monday. Thanks again for your input.\n. Squashing is done. Thanks for your review.\n. @xiang90 Sorry about the delay. Just back from the vacation. I see two questions in this thread:\ni) A connection error happens between Balancer.Get returns and addrConn.Wait starts. @menghanl just sent #830 to fix the issue. Note that right dial timeout only applies to the initial dialing and therefore should not be applied here;\nii) You want to retry an RPC when there is a connection error in the dial phase or before the request finishes sending, and not to retry when the request is already sent. This lives in the position between fail-fast and non-failfast we defined. I would like to emphasize that even though grpc write returns successfully there is no guarantee your request msg can reach the server (it still could fail on any part of the path client host <--> network <--> server host). Receiving the response is the only way to justify whether your request is received by the server. Therefore, I feel the retry scenario you described is somewhat problematic (I could be wrong because I am not clear about your use case :). I guess your decision is from the fact that your server is stateful (e.g., a successful request could acquire a file lock already.). According to my experience working with Chubby in Google, you need a special rpc to roll back any potential operations which have been done on the server side when one of your rpcs does not complete successfully.\n. I think even if grpc makes this change for you you still need to deal with the same problem if write returns successfully in step 3 but there is an kernel/network error so that both you and grpc have no idea whether the server has received the request. In my understanding, your proposal just shrink the retry corner a bit but you still need to deal with all the problems without this change.\n. @xiang90 In this case, the entire ClientConn is closing (and Balancer is closing too). I do not think put is required in this case. Any particular reason you need this?\n. LGTM\n. merged. I expected some end2end tests but we can leave them later.\n. I have no clue what is going on here according to the error msg. Isn't it a ssa issue/bug instead?\n. @heyitsanthony We will see whether we can export some errors so that you can rely on them to perform retry. But it won't happen in this pull request.\n. google.golang.org is not accessible from China. Please refer to https://github.com/grpc/grpc-go/issues/350 for a workaround using git clone.\n. Thanks for the reporting. This is not the right fix though. Let us take this over. will send the fix shortly.\n. replaced by #842\n. The root cause is that getTransport does not detect ClientConn has been closed in this case. This PR fixed the symptom for a cancelled rpc instead of the root case. The bug remains if this rpc is not cancelled/timeout with this PR.\n. The current code leaves the watcher until the Balancer is closed. In this way, we do not enforce the impl of Watcher.Close to be re-entrant since Close is called only once. We can call Close() when Next returns an error or even Next impl can call Close by itself if we require Close is re-entrant. The current way is chosen for simplicity and safety. Do you have any concrete scenario to show us which option you think is better?\n. This is ok to test. If you do not have time, @MakMukhi probably can help take this over because he is working on adding some tests for DialContext related code.\n. Probably you can add a testing function into \"internal\" package (https://github.com/grpc/grpc-go/blob/master/internal/internal.go) to set minConnectTimeout to a smaller value.\n. replaced by #859 \n. This typically means your server has been stopped (e.g., https://github.com/grpc/grpc-go/blob/38383692bafc79f1517438b04e4a00ea14fd4e9a/server_test.go#L63). \nIn the example, we do not stop the server for simplicity. In production, server stop code typically stays in the signal handler.\n. we will rework on grpclog so that this kind of error/warning won't be thrown out unless it is under debugging mode.\n. @toontong In this case, the server sends the response back and finds there is no pending rpc. Now the server closes all its connections, which is the reason this log is generated -- that goroutine is doing a blocking read from the socket which was just closed by the server. GracefulStop is graceful.\n. You seemed work on a very old version of grpc-go. We removed this error when adding Balancer since the Balancer can be used to get addresses even when the target is an empty string. menghanl@ will add some check back to the case (balancer is nil and target is empty string) to fix this.\n. In the current code, if balancer is specified, all the name resolution work is taken over by the balancer. For example, we can make a custom balancer which always returns 127.0.0.1 to the grpc internals regardless what the target is for testing purpose. And yes, we can also add the non-empty constraint to Balancer but we do not now.\n. LGTM\n. duplicate of #711\n. Yes, it appeared before interceptor and has not evolved yet.\n. cc/ @MakMukhi \n. replaced by #857 \n. The previous one is problematic. For example, if a user pass a context with 1 min timeout, the entire ClientConn becomes not usable after 1 min with the previous impl. \nFollowing the similar concept in net.DialContext, the user-passed context should be valid only during this dialing operation instead of the entire lifecyle of ClientConn.\n. @tamird As the revised comments says, the user should call ClientConn.Close to do clean-up once this function is returned. Keep in mind ClientConn.Close cancels the context of ClientConn in your recent contribution. Therefore, your original leak won't happen as long as you call ClientConn.Close at the end (which cancels all the child contexts in addrConn and transport). You can verify in your test suite.\nWe would like to put the user-passed context as a dial context as its name suggests. We do not want to treat it as the binding context of the ClienConn -- this introduced more complexity to the code base and pretty error-prone.\n. LGTM. Let me see what I can do in travis.\n. LGTM\n. yeah, it is part of performance optimization we will focus on shortly.\n. @neilgarb Can you add a log at https://github.com/grpc/grpc-go/blob/4c8d05f3b1260c2231af4493ffbd918ed51b69ca/transport/http2_client.go#L165 and let us know what error (if there is) you get there? Thanks.\n. @tamird we also discussed the blacklist approach about a week ago but have not come up with a decent solution because it requires the credential impl to do the proper pre-processing for error, which could be tricky. We keep thinking ...\n. Can you elaborate what you observed? What error did you get?\n. Can you send us a working repro? \n. Thanks for reporting. It seems we missed some type conversions for streaming RPC. The fix will be out soon.\n. replaced by #874 \n. we are working on improving grpc logging (this should only appear in debugging mode).\n. Currently we do not share the grpc port with httpserver so that you need to setup a httpserver on another port to make tracing work. You can use https://github.com/grpc/grpc-go/blob/master/benchmark/client/main.go#L152 as an example. Yes, we should have a howto doc but since the tracing part is under rewriting we hold off for now.\n. @zellyn can you verify #887 fix the issue?\n. BTW, the metadata part of grpc.Address is intended be immutable once being initiated. Is it necessary for you to change it in your codebase?\n. Close this one. will add the comment to clarify that the metadata of grpc.Address is immutable after initialization.\n. If my memory works, there was one there but I removed it when working on go-away feature since it changed the expected status code with some chance (being changed to Canceled from some expected values). We are investigating if there is a better solution. \n. The current grpc protocol requires a trailer to indicate the completion of an rpc. There are some discussions regarding this in progress but currently it is required.\n. Currently it is not supported and according to the info I got back in May they did not have plan to support it yet.\n. I think this is a race the balancer has to deal with because these operations (i.e., ClientConn.Close, lbWatcher founds a new address to connect) are racy. I think your balancer can simply do no-op in this case if balancer.Close() has been called.\n. \\cc @xiang90 @zellyn I need to tune the Balancer API a bit to accommodate this change for security reasons.\n. I think you need a transport layer interceptor. Your proposal does not work either because it is still too late (per-stream goroutine has been spawned). The good news is that we will add this feature (probably call it \"transport filter\" or something) for sure because we have the similar requirement internally. But there is high chance we won't have time to make it happen this quarter. The ETA would be Q1 2017.\n. Having native support of streaming rpc retrying is challenging. You need to do it in your code now. We are designing the retry policy and corresponding mechanism now. But the ETA would probably be Q1 next year.\n. To bypass all of these overhead, we need an in-process-transport impl (in addition to http2Client and http2Server transport) in the transport package and the corresponding custom codec, listener, etc.. We did think about this previously but this is not a trivial change and we do not have enough hands covering this now unfortunately. \nAs an alternative, I think if you can add a wrapper on the top of the grpc generated code to switch on in-process and normal case you should probably achieve this without any changes in grpc library (I have not thought through all the detail and could be wrong though).\n. v1.0.1-GA is a tag instead of a branch. We guarantee the non-experimental API stability in all the commits after v1.0.1-GA tag (except in very rare cases we have to change. In those cases, we will have some deprecation period for the old APIs). Therefore, you should probably always use the master\nWe will tag all the experimental API revisions too so that if you use some experimental APIs it is convenient to upgrade..\n. LGTM, the CI failure was due to some latest go vet change. It has been fixed. Can you rebase?\n. Sorry about the inconvenience. It will be released shortly.\n. This should be in go's protobuf repo instead of grpc. You can file an issue there.\n. It runs fine on my desktop. Did you run it with docker? If yes, I think there were some issues on the docker config. You can probably test with some simple non-grpc programs (e.g., simply use net.Conn or tls.Conn) to check.\n. This is intentional. We only provide a hook so that various complex interceptor patterns (e.g., chaining, nested, stack, concurrent, etc.) can be built on top of it without running into the argument of the execution ordering of the multiple interceptors while keeping grpc itself simple.\n. We are not going to put it as part of grpc-go. If you want to contribute, you can make one at https://github.com/grpc-ecosystem.\n. @bradfitz Thanks for taking care of this. We will only comment out the failed tests in this PR. They can be revived once your fix is in.\n. I should have deleted WithCancel at line 2507 (will do in another PR). It is not a random removal. The original test does not check the expected behavior strictly. I changed its behavior by letting the first rpc live in the entire test and verify the following rpcs cannot get through due to the max streams limit.\n. we do not support concurrent writers for a single stream for now.\n. https://github.com/grpc/grpc-go/blob/master/peer/peer.go#L62 gives the peer address.\n. I think this has been fixed in #947. We will tag the repo next Monday.\n. No, recvResponse does not close the stream since https://github.com/grpc/grpc-go/blob/master/call.go#L75 rewrites the returned \"err\".\n. The deferred \"err\" gets \"nil\" in this case.\n. https://play.golang.org/p/Gjsh24B4AU\n. can you test it?\n. Some errors with gofmt and goimports on travis. Please fix.\n. Can you rebase to address all the conflicts?\n. And you need to fix transport_test.go.\n. I think this is intended.\n. As @Thomasdezeeuw pointed out, this should not be in grpc package. I actually do not get your point on flatbuffer compiler (haven't got time to look into that) and why putting it into grpc package can address the issue.\n. grpc-go itself won't take this unnecessary dependency. You can put that into https://github.com/grpc/grpc-contrib so other people can use that.\n. Please fix the gofmt and goimports errors shown in travis.\nFor unimplemented_method issue, I think a better way to do that is that calling grpc.Invoke(...) (e.g., https://github.com/grpc/grpc-go/commit/f02984b7c6c90fa58a05c16a4986a9c441f5474a#diff-ac5bc39c5f66d83a645234afc74213deR208).\n. @tmc This fixes #901 and internally we are using it for rate control or other ACLs.\n. grpc.SupportPackageIsVersion is used to track the version of the protobuf generated code and is different from the grpc release version. Menghanl made an faq (https://github.com/grpc/grpc-go/pull/970/files) to address this type of questions.\n. Please rebase/fix the golint errors.\n. This behavior is intentional. As the function name advises, \"DialContext\" should start the dialing procedure to a target. If it fails to do that (as in your case), it should return an error. \nI think blocking is the right behavior. If you do not want it to block your other work you can put Dial into a separate goroutine -- you can start all the processes with a blocking goroutine doing dialing. I am not clear what the architecture design is in your project. But if this is a hard requirement for your design, I would question the design ...\n. Is it possible to have a mutex to guard your MD accesses? grpc MD is not concurrency-safe.\n. I feel the checked-in behavior is not friendly for most users. We plan to revert the changes in #974 and introduce a new dial option to allow users to change the default behavior.\n. I think it is because of some misuse of vendor tool so that the importing path is not set correctly. You can send the mail to grpc.io email group for help with more detail. Close it here since this is not a grpc issue.\n. Please follow the instruction at http://www.grpc.io/docs/quickstart/go.html to install grpc-go.\n. To terminate a stream, you can cancel the context on the client side and return from the service handler on the server side.\nTo be clear, CloseSend does not terminate a stream -- it just claims it won't send any data to the peer.\n. I would suggest sync-ing up to the latest code in this situation.\n. I speculate your previous version was very old (seems it was before the roll-out of fail-fast feature). Can you try to set your rpcs to non-failfast (via grpc.FailFast(false) call option)?. It is not clear to me how you got the \"the connection is unavailable\" error. Can you send me some code snippets or tests which can reproduce your observation?. This is not rocket science. We simply add a variable grpc.SupportPackageIsVersionX (X is the integer and the current value is 4) to make sure your grpc lib and the generated code work on the same version. We think this is a nice feature so that our users won't blindly mess up different version altogether which may have undefined errors and hard to tackle.  In general, if you encounter this problem, check this file (https://github.com/golang/protobuf/blob/master/protoc-gen-go/grpc/grpc.go#L51) and this file (https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L481) for both your project and its dependencies (if they also use gRPC) and see whether they are matched. If they are not, update the repo to make them consistent (you probably need to do additional steps if this happens to your dependencies. I do not want to recommend a way here because this highly depends on your best practice.). Once they are consistent, regenerate all the code. The problem should go away.\n@troylelandshields Yes it is a huge pain if your project has a deep dependency tree. But this pain is not specific to grpc. Basically it applies to all the libraries which are used in multiple places in the dependency tree. I think the library should ship with its vendored grpc library, which makes the life much easier.\n. I think we should use Go default as the default and ppl can change it when they run the benchmark. It is not justified that 1 is the value to achieve the ultimate limit of latency. Even if it is right now, when the environment change (e.g., machine is upgraded or golang gets a new version) it might not be true any more. We do not have an automatic procedure to adjust its value. . Yes, you can do that once it is established.. If you observe from 1ms, both grpc and http2 are increasing proportionally. The only issue with grpc is that it is started with much large value (i.e., 107ms vs 12ms). We need to figure out what is the expected value there (this counters my previous observation with http2.0...).\n@apolcyn, assign this to you and please help. Thank you.. as I said, your code cached the string used by http2. For example,\n12.423725ms 1ms HTTP/2.0\n3.02918ms   1ms HTTP/2.0\n2.775928ms  1ms HTTP/2.0\n4.161895ms  1ms HTTP/2.0\n2.951534ms  1ms HTTP/2.0\nthe 1st line is much larger than the rest. I believe this means the rest\ntakes advantage of caching. When I said they are both increased\nproportionally, I meant the first line of the result from http2.\nOn Mon, Jan 9, 2017 at 6:51 PM, Andrew Gerrand notifications@github.com\nwrote:\n\n@iamqizhao https://github.com/iamqizhao note the relationship between\nthe simulated latency and the request time.\nFor HTTP/2, with 0 latency a request takes ~1ms. With 1ms of latency (both\nup and down) you get requests around 3ms (after the first request, when the\nconnection is established). With 2ms of latency, requests are ~4ms. For 4ms\nlatency, requests take ~9ms. See the pattern? So the actual time spent\nmaking HTTP/2 requests, minus the introduced latency, is constant (around\n1ms).\nContrast this to the GRPC behavior, where the introduced latency\nmultiplies the overall request time.\nWe need to figure out what is the expected value there (this counters my\nprevious observation with http2.0...).\nStarting with zero latency, the GRPC requests take 4-6ms. What we should\nsee for GRPC requests is a similar pattern, where 1ms of latency yields 8ms\nrequests (6ms + 1ms + 1ms), 2ms latency = 10ms requests, and so on.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1043#issuecomment-271472457, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJpyLMwAwSMdNAK_VHa1KDXNyjBMIwl0ks5rQvI7gaJpZM4Le51s\n.\n. Err, I see -- I did not look into how you set up the http transport.. yes, it seems this is due to flow control issue -- if I enlarge the flow control window above 1M, this issue goes away.. @rsc Thanks for investigation. Nice findings. In gRPC-Go, there is no HTTP/2 flow control (FYI, Brad and I have not finished the migration to the standard HTTP2 package yet and gRPC-Go still uses its own native HTTP2 implementation). And yes gRPC does not apply compression by default.\n\nThe root case that the latency of gRPC-Go is not equal to network RTT + constant processing time is  because of its flow control mechanism. Currently, it takes a simplistic form -- it has a fixed 16KB window update as Russ discovered. It does not tune according to network RTT and incoming message sizes so that it favors low latency networks and sucks in long-haul networks (as shown in this issue). The current mechanism is the initial version and was consistent across languages. Recently, gRPC-C added some more intelligence to its flow control and more is under design. I summarize the effort we will put in this area as follows:\ni) window size tuning upon message size. This should fix this issue;\nii) window size tuning according to network RTT (under design);\niii) public surface to allow user to tune flow control parameters and enforce a server-scoped flow control limit (under design).. > Can user client code change the fixed flow control window size?\nCurrently they cannot. This is the 3rd effort I mentioned in the previous reply.\n\nCan user client code enable compression?\n\nYes, with grpc.WithCompressor dial option.. Yes, you get notified when you interact with stream. But be aware that all the underlying information of this stream is cleaned up once the server disconnects (instead of waiting for user interaction) and only the minimal info is left in the stream object used to notify the caller.. You can always have a pending recv call to either receive incoming messages or get notified if there is an error.. I meant you can have a stream.Recv() call pending so that you can get notified when there is an error.. You need a Recv call to receive the final status of the stream anyways.. Yep, you can spawn a goroutine to call Recv on that line.. And you probably should use bi-directional streaming instead of client streaming. With client streaming, you probably need to deal with concurrent readers in your use case.. So this should be merged after the redirector is setup. Is it done?\n. done.\nOn Thu, Feb 19, 2015 at 7:58 PM, David Symonds notifications@github.com\nwrote:\n\nIn codegen.sh\nhttps://github.com/grpc/grpc-go/pull/50#discussion_r25047949:\n\n@@ -0,0 +1,17 @@\n+#!/bin/bash\n+\n+# This script serves as an example to demonstrate how to generate the gRPC-Go\n+# interface and the related messages from .proto file.\n+#\n+# It assumes the installation of i) Google proto buffer compiler at\n+# https://github.com/google/protobuf (after v2.6.1) and ii) the Go codegen\n+# plugin at https://github.com/golang/protobuf (after 2/19/2015). If you have\n\nPlease stick to using YYYY-MM-DD date formats. This date does not make\nsense in many places.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/50/files#r25047949.\n. I think this is racy.\n. This is a client side only function because the finish logic on client and server is different. I tried to make transport.Stream client/server-agnostic as much as possible, which is the reason I did not do this before.\n. okay, no race by checking the src code. But this is an overkill and you added two more vars to the transport. It can be easily achieved by checking the value of t.state:\n\nt.mu.Lock()\nif t.state == closing {\n  t.mu.Unlock()\n  return nil  // or returns some error.\n}\nt.state = closing\nt.mu.Unlock()\n...\n. On Mon, Feb 23, 2015 at 12:01 AM, Matt T. Proud notifications@github.com\nwrote:\n\nIn transport/transport.go\nhttps://github.com/grpc/grpc-go/pull/52#discussion_r25148055:\n\n@@ -278,6 +278,24 @@ func (s *Stream) Read(p []byte) (n int, err error) {\n    return\n }\n+func (s *Stream) finish() {\n\nAhh! Good to know. Does this new version look better?\nThe new version is okay.\nPerhaps we could massage the behavior into transport.Stream a little more\nelegantly through the type system and use those throughout the respective\nclients and servers:\ntype (\n    clientStream Stream\n    serverStream Stream\n)\nfunc (s clientStream) Close() {}\nfunc (s serverStream) Close() {}\nLet me know what you think!\nLet's not pursue this way for now. I plan to do some performance profiling\nfirst. If we find the perf can be improved by having dedicated\ntransport.ClientStream and transport.ServerStream (so that we can do\nclient/server specific optimization), then we will have better idea how to\nproceed.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/52/files#r25148055.\n. The Once idea might be slightly faster in some cases when o.done has been set because it uses atomic op instead of mutex. But I still prefer the above mutex solution.\n. On Mon, Feb 23, 2015 at 12:32 AM, Matt T. Proud notifications@github.com\nwrote:\nIn transport/http2_client_transport.go\nhttps://github.com/grpc/grpc-go/pull/52#discussion_r25149057:\n\n}\n-       s.mu.Unlock()\n-       s.write(recvMsg{err: ErrConnClosing})\n-   }\n-   return\n-   })\n-   return t.closeErr\n\nSure, entirely. I was just proposing that we could make the API a bit less\nbrittle and more consistent in case of multiple closures: consistent return\nvalues.\nI am not sure consistent returning value is right thing to do. An\nalternative is that the 1st call returns nil, the following calls return\nsomething like \"transport has been closed\" or something similar. Anyways,\nthe good thing is that the current impl does not use the return value from\nClose().\nLet me take a few days offline to polish the rest of my thoughts offline,\nand I'll re-upload a draft into there. I'll go ahead and archive this until\nthen.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/52/files#r25149057.\n. well, I should have said whatever returned from net.Conn.Close() for the first call.\n\nSure, thanks a lot for the help!\n. do not do this. this enlarges the scope of mutex, which could introduce deadlock potentially.\n. ditto\n. \"already\" might not be accurate because it could be in progress.\n. To be clear, there is no deadlock for now. But please always try to avoid holding stream lock and transport lock at same time (0 case in the current code). Otherwise, it is easy to introduce deadlock if we change some stream/transport code later.\n. I am a little worrying here. we already have transport.ErrConnClosing, grpc.ErrClosingConn which need some renaming work. This one seems make the situation worse. I would suggest leaving it unnamed -- just \"return errors.New(\"....\")\" for now because no other functions need to use this.\n. \"grpc/transport: Close() was already called\"\n. I do not think this comment is helpful. It only introduces questions.\n. ditto here. I would suggest removing this comment.\n. ditto\n. I think this way. Avoiding nested locking is a common good-thing-to-do in concurrent programming. It is not a \"MUST NOT\". ppl can still do that if it is necessary. I do not think it is worth adding it explicitly -- it is hard wording. If we cannot word appropriated, it will only lead to questions.\n. ditto\n. why do you want to cache the error on the first faulty transport?\n. srv ==> server for consistency\n. ok, I see. You used that to test duplicated close. seems not very pleasant. Let me think whether there is a better way to do that.\n. I would like to suggest leaving server.Close() as it and specialize the closing code TestClientServerClose, i.e., call ServerTransport.Close() directly  without calling server.Close() at all.\n. these 3 lines for ClientTransport close repeat many times. Can you put it into a function?\n. Let's inline this so that we do not need firstErr trick.\ninline the following into TestClientServerClose()\ns.lis.Close()\ns.mu.Lock()\ndefer s.mu.Unlock()\nfor c := range s.conns {\n   if err := c.Close(); err != nil {\n    t.Fatalf(...)\n   }\n   // Duplicated close\n  if err := c.Close(); err == nil {\n   t.Fatalf(...)\n  }\n}\n. TestServerDuplicatedClose(t *testing.T) ?\n. I have merged https://github.com/grpc/grpc-go/pull/61 to change grpc/transport back to transport since \"transport\" is the package name (not \"grpc/transport\"). Please fix this in this PR too. Thanks.\n. It is okay. The failure will tell you which test case is failed. This is not a valid reason to duplicate the same code everywhere.\nBTW, I was not saying \"inline this\", what I said is on the opposite direction -- put them into a function.\n. \n. \n. \n. \n. ClientTransport ==> all ServerTransports\n. what do you mean \"shortcuts further analyses\"?\n. for all the empty messages, I actually typed \"\".\nOn Wed, Feb 25, 2015 at 12:36 PM, Matt T. Proud notifications@github.com\nwrote:\n\nIn transport/transport_test.go\nhttps://github.com/grpc/grpc-go/pull/52#discussion_r25376369:\n\n+\n+func TestClientServerDuplicatedClose(t *testing.T) {\n-   server, ct := setUp(t, true, 0, math.MaxUint32, false)\n-   if err := ct.Close(); err != nil {\n-       t.Fatalf(\"ct.Close() = %v, want nil\", err)\n-   }\n-   if err := ct.Close(); err == nil {\n-       // Duplicated closes should gracefully issue an error.\n-       t.Fatalf(\"ct.Close() = nil, want non-nil\")\n-   }\n-   if err := closeServerWithErr(server); err != nil {\n-       t.Fatalf(\"closeServerWithErr(server) = %v, want nil\", err)\n-   }\n-   if err := closeServerWithErr(server); err == nil {\n-       // Duplicated closes should gracefully issue an error.\n-       t.Fatalf(\"closeServerWithErr(server) = nil, want non-nil\")\n\nSame empty message.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/52/files#r25376369.\n. demonstrate\n. gRPC-Go\n. I renamed this to server_host_override yesterday to make it consistent with other languages. Maybe you should rename also.\n. make(chan struct{}) and then use close(waitc) to signal.\n. close(waitc)\n. rename these strange vars?\n. because interop client also has a flag (--server_host_override) to achieve the same purpose, I intend to use the same flag name here to reduce the burden of audience.\n. I was talking about var name instead of math. Per discussion offline, keep them as it.\n. compile --> build\n\nthe server --> the server and client\n. we actually require 1.4 due to http ALPN support.\n. ErrClientConnTimeout\n. t.Fatalf(\"grpc.Dail(, ) = %v, %v, want , %v\", conn, err, grpc.ErrClientConnTimeout) to be consistent with other tests in this file.\n. You need to start a server here. Otherwise, this test could be flaky in case server sends TCP RST before timeout.\n. This does not work -- \"ok\" is always false because the real type of err is transport.ConnectionError instead of net.Error. You need to add Temporary() and Timeout() to transport.ConnectionError to make this work. This needs to be properly tested.\n. transport.ConnectionError is defined at https://github.com/grpc/grpc-go/blob/master/transport/transport.go#L398.\n. On Sun, Mar 1, 2015 at 11:48 PM, Wes notifications@github.com wrote:\n\nIn clientconn.go\nhttps://github.com/grpc/grpc-go/pull/88#discussion_r25582140:\n\n@@ -137,7 +151,16 @@ func (cc *ClientConn) resetTransport(closeTransport bool) error {\n        if err != nil {\n            // TODO(zhaoq): Record the error with glog.V.\n            closeTransport = false\n-           time.Sleep(backoff(retries))\n-           if neterr, ok := err.(net.Error); ok {\n\nHappy to blow this away and handle it in a separate PR. Sound good to you?\nSGTM\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/88/files#r25582140.\n. I guess this test is probably fine because the error I just described should be classified as Temporary so that grpc will keep reconnecting.\n. log ==> t\n. log ==> t\n. On Mon, Mar 2, 2015 at 1:53 AM, Wes notifications@github.com wrote:\nIn test/end2end_test.go\nhttps://github.com/grpc/grpc-go/pull/88#discussion_r25587275:\n\n@@ -193,6 +193,16 @@ func (s *testServer) HalfDuplexCall(stream testpb.TestService_HalfDuplexCallServ\nconst tlsDir = \"testdata/\"\n+func TestDialTimeout(t testing.T) {\n-   conn, err := grpc.Dial(\"localhost:0\", grpc.WithTimeout(5time.Microsecond))\n\nHrm, I'm not sure I need to start up a server here, since the easiest way\nto test this timeout is to attempt connecting to an unopened socket.\nSo, how to go about getting a socket we're sure is unopened--in the flaky\ncase, perhaps some other test leaves one open on accident? I've implemented\na simplistic open/close/test-with-recently-closed-address logic, but I'm\nnot really happy with it. If you have a better idea, I'm all ears :) but\nit's 2AM here and I'll have to take another look in the morning.\nThanks for the effort. My idea is to connect to a non-existing dns name:\nconn, err := grpc.Dial(\"Non-Existing.Server:80\",\ngrpc.WithTimeout(time.Millisecond))\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/88/files#r25587275.\n. Instead of 1, 10, 100, 1000, I would like to do 1B, 1KB, 8KB, 64KB, 512KB, 1MB. This should not have any impact on your optimization but will be more useful later.\n. Remove \"simple\". We may put a complex proto later.\n. I prefer 2-base (e.g. 1024) instead of 10-base.\n. Can you elaborate why this is better than the original?\n. now it seems better if we simply pass cc.dopts toNewClientTransport. You do not need to rename the function.\n. you do not need to add a new user-facing API. You can add some logic into Dial instead.\n. hmm, ignore my previous new API comment for now. I need to think a bit.\n. shouldn't you have something like \"if timeout > 0\" here?\n. You do not need to add a new function.\n. Hi, Wes,\n\nCan you hold on a bit and let me finish an entire pass (because I could\nrevert my previous comments leading to unnecessary work on your side --\npull request review on github is not pleasant ...)? Thanks!\nOn Tue, Mar 3, 2015 at 2:15 PM, Wes notifications@github.com wrote:\n\nIn clientconn.go\nhttps://github.com/grpc/grpc-go/pull/88#discussion_r25732724:\n\n@@ -133,10 +146,13 @@ func (cc *ClientConn) resetTransport(closeTransport bool) error {\n        if closeTransport {\n            t.Close()\n        }\n-       newTransport, err := transport.NewClientTransport(cc.dopts.protocol, cc.target, cc.dopts.authOptions)\n-       newTransport, err := transport.NewClientTransportTimeout(cc.dopts.protocol, cc.target, cc.dopts.authOptions, cc.dopts.timeout)\n\nI was basically mirroring the Dial/DialTimeout approach taken in the net\npackage. If you feel it just adds to bloat, though, I'm happy to just\nupdate NewClientTransportto take a timeout.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/88/files#r25732724.\n. The only difference I can see is that i) there is a [8]byte alloc in the original code; [4]byte in your code (there is no additional alloc for slice bs because it just uses b's memory); ii) the original code has two type switches, your code does not have. Your code is the final solution but is okay to check in.\n. done\n. done\n. dopts is just used to pass the latest timeout value to transport.NewClientTransport when for loop iterates (each iteration needs to have different timeout value for NewClientTransport when time goes forward). The cutoff is start.Add(cc.dopts.Timeout) always.\n. This is essential part of dial timeout support. It is used to pass timeout when TLS does dialing and handshake. This just mimics the API in the standard tls package (http://golang.org/pkg/crypto/tls/#DialWithDialer).\n. Done\n. done.\n. We do now have \"context\" concept here. So using it here is probably confusing.\n. done.\n. done.\n. yes, we already have that guarantee. Done.\n. There was a bug here which is fixed. Your proposal is not working because  transport.NewClientTransport could take some time to return so that at this point the left time to finish dialing is cc.dopts.Timeout - time.Since(start).\n. Actually we guarantee the total order between cc.dopts.Timeout and dopts.Timeout: cc.dopts.Timeout >= dopts.Timeout. So probably it is not needed?\n. done.\n. On Wed, Mar 4, 2015 at 2:30 PM, Wes notifications@github.com wrote:\nIn clientconn.go\nhttps://github.com/grpc/grpc-go/pull/93#discussion_r25821293:\n\nif err != nil {\n-           // TODO(zhaoq): Record the error with glog.V.\n-           if netErr, ok := err.(net.Error); ok && netErr.Timeout() {\n-               cc.Close()\n-               return ErrClientConnTimeout\n-           }\n-           sleepTime := backoff(retries)\n-           // Fail early before falling into sleep.\n-           if dopts.Timeout > 0 && cc.dopts.Timeout < sleepTime + time.Since(start) {\n\nAh, I meant instead of. So\nif cc.dopts.Timeout > 0 && cc.dopts.Timeout < sleepTime +\ntime.Since(start) {\ninstead of\nif dopts.Timeout > 0 && cc.dopts.Timeout < sleepTime + time.Since(start) {\nBut you're right that we're constrained cc.dopts.Timeout >= dopts.Timeout.\nI just thought it made more sense to use the latter var, since we're\nexplicitly checking that it was set.\nmake sense. Done.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/93/files#r25821293.\n. \"else\" is not needed.\n. make sense. Removed. \n. yep. Avoid bloating of input arguments.\n. I am not convinced Retriever is a better name to go. It is not required to have a meaningful GetRequestMetadata for a credentials -- e.g., tlsCreds simply return nil, nil for GetRequestMetadata. Then is it a Retriever (it does not retrieve anything)?\n. On Tue, Mar 17, 2015 at 1:10 AM, Geert-Johan Riemer \nnotifications@github.com wrote:\nIn credentials/credentials.go\nhttps://github.com/grpc/grpc-go/pull/115#discussion_r26555073:\n\n// tokens if required. This should be called by the transport layer on\n// each request, and the data should be populated in headers or other\n// context. When supported by the underlying implementation, ctx can\n// be used for timeout and cancellation.\n// TODO(zhaoq): Define the set of the qualified keys instead of leaving\n// it as an arbitrary string.\n-   GetRequestMetadata(ctx context.Context) (map[string]string, error)\n-   Retrieve(ctx context.Context) (map[string]string, error)\n\nnil is a value too..\nIt can return anything. But the point is that the caller uses a Retriever\nbut does not care whatever is retrieved from it.\nIn the same way GetRequestMetadata is a getter that does not get anything?\nyes, but the struct is NOT called something like\n\"RequestMetadataRequester\". There is a close binding between \"Retriever\"\nand \"Retrieve()\".\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/115/files#r26555073.\n. On Tue, Mar 17, 2015 at 8:53 AM, Burcu Dogan notifications@github.com\nwrote:\nIn credentials/credentials.go\nhttps://github.com/grpc/grpc-go/pull/115#discussion_r26585640:\n\n// tokens if required. This should be called by the transport layer on\n// each request, and the data should be populated in headers or other\n// context. When supported by the underlying implementation, ctx can\n// be used for timeout and cancellation.\n// TODO(zhaoq): Define the set of the qualified keys instead of leaving\n// it as an arbitrary string.\n-   GetRequestMetadata(ctx context.Context) (map[string]string, error)\n-   Retrieve(ctx context.Context) (map[string]string, error)\n\nI am not convinced Retriever is a better name to go\nIt retrieves credentials, it explains the very basic single action the\nimplementers of this interface does. If you're not convinced Retriever is a\nbetter name, please suggest one. Because Credentials is no way a good name\nfor an interface in Go in this context.\nIt does for oauth2 type of protocols but does not for tls/ssh type of\nprotocols. To be honest, I am okay with \"Credentials\" but dislike\nGetRequestMetadata (I prefer something like AuthToken(). Do you think it\ncan co-exist with Credentials interface name?). But I used them because\nthis interface name and member function name are used across various\nlanguages and I do not want the naming for Go off other languages too much\nfor a similar interface/struct. Also I think \"an interface name should\nexplain the action that is done by the implementers of the interface\"\nshould apply to other languages too.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/115/files#r26585640.\n. I actually made a version which put this in the grpc package. Because we have a transport package and the constraint that the transport package should not call anything in the grpc package, putting it in the grpc package leads to less-neat code:\nthe current code makes transport.stream.codec the only place to record codec to be used . The old approach needs to scatter that in different places on upper layer (grpc package) and could lead to unnecessary complexity e.g., transport needs to pass the receive content-type string to the grpc package to do the processing to extract codec info.\n. In that, protoCodec needs to be ProtoCodec. Either way is fine with me though.\n. this is used to for content-type encoding -- search \"content-type\" in https://github.com/grpc/grpc-common/blob/master/PROTOCOL-HTTP2.md.\n. search \"content-type\" in https://github.com/grpc/grpc-common/blob/master/PROTOCOL-HTTP2.md. I think it is intended.\n. search \"content-type\" in https://github.com/grpc/grpc-common/blob/master/PROTOCOL-HTTP2.md.\n. I actually think content-type is intended for this purpose. Also there is no good place to specify this at higher level (you are against the proposal of putting it in MethodDesc and something similar) on server side.\n. I did not mean the transport package needs to know codec. I meant where to record the codec to be used. In the current code, if you want to record the codec in the grpc package, you need to deal with 4 cases differently -- (unary|streaming rpc) x (client|server). If the codec is recorded in the transport (actually transport.Stream struct which is common for all the above 4 cases), it is simplified. But I do not want users to interact with a private package (the transport package) directly. So I added a new package to do that.\n. On Sun, Mar 22, 2015 at 5:53 PM, David Symonds notifications@github.com\nwrote:\nIn transport/http_util.go\nhttps://github.com/grpc/grpc-go/pull/126#discussion_r26909794:\n\n@@ -159,6 +161,8 @@ func newHPACKDecoder() *hpackDecoder {\n            }\n        case \":path\":\n            d.state.method = f.Value\n-       case \"content-type\":\n\nThe design for this feature was such that the client explicitly specifies\nto use a non-standard codec. It'd be nice just from the symmetry\nperspective to have the server be similarly explicit. I don't think it's a\nparticularly necessary goal that a server can automatically switch between\nwire formats, but maybe you're thinking about that?\nThat is one thing I thought about it. But I also agree that it is not\nparticularly necessary. My real question is that if we want to adopt\nsymmetry where and how to specify it on server side. If you have a good\nsolution to my question, I am okay to switch to it.\nIf we're going for some arbitrary string-matching scheme here, the codec\npackage (or wherever that code lives) will probably need a registration\nmechanism. We can't bake in knowledge about every possible codec.\nThought about it too. But notice that this is on the fast path so that I do\nnot want to introduce a table lookup for it.\nThe use of the codec's String() result in Content-Type needs explicit\ndocumentation in the Codec interface definition.\nacked.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126/files#r26909794.\n. not sure whether you got chance reading though my reply before sending this. The real concern is where to cache the codec to simplify the logic.\n\nTo your question, yes but it is not the reason I took the current approach.\n. Let's decide whether we take the string match approach or something else first. And then come back to this because a major reason for me to do this is that I do not want to pass content-type string out of the transport package just for extracting the codec info from it in the grpc package.\n. done.\n. done.\n. Fine with me. My concern was the case in which a user has a complicated big custom codec so that a pointer is needed. Thus probably always passing pointer might be clearer...\n. On Wed, Apr 1, 2015 at 2:22 PM, David Symonds notifications@github.com\nwrote:\n\nIn server.go\nhttps://github.com/grpc/grpc-go/pull/146#discussion_r27613201:\n\n@@ -106,6 +113,10 @@ func NewServer(opt ...ServerOption) *Server {\n    for _, o := range opt {\n        o(&opts)\n    }\n-   if opts.codec == nil {\n-       // Set the default codec.\n-       opts.codec = &protoCodec{}\n\nThat's not how Go interfaces work. Here, protoCodec is an empty struct.\nIt doesn't need to be a pointer, because the methods for the Codec\ninterface are on protoCodec, not protoCodec. If a user has a complex\ncodec, they can implement those methods on myCodec and pass &myCodec{...}\nto the relevant functions in this package.\nThis is definitely understood. I think my concern was just about which way\ncan simplify the user's thinking. But perhaps it is not necessary...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/146/files#r27613201.\n. all done.\n\nOn Wed, Apr 1, 2015 at 2:28 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nOn Wed, Apr 1, 2015 at 2:22 PM, David Symonds notifications@github.com\nwrote:\n\nIn server.go\nhttps://github.com/grpc/grpc-go/pull/146#discussion_r27613201:\n\n@@ -106,6 +113,10 @@ func NewServer(opt ...ServerOption) *Server {\n   for _, o := range opt {\n       o(&opts)\n   }\n-  if opts.codec == nil {\n-      // Set the default codec.\n-      opts.codec = &protoCodec{}\n\nThat's not how Go interfaces work. Here, protoCodec is an empty struct.\nIt doesn't need to be a pointer, because the methods for the Codec\ninterface are on protoCodec, not protoCodec. If a user has a complex\ncodec, they can implement those methods on myCodec and pass\n&myCodec{...} to the relevant functions in this package.\nThis is definitely understood. I think my concern was just about which way\ncan simplify the user's thinking. But perhaps it is not necessary...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/146/files#r27613201.\n. On Fri, Apr 17, 2015 at 3:44 PM, David Symonds notifications@github.com\nwrote:\n\nIn clientconn.go\nhttps://github.com/grpc/grpc-go/pull/169#discussion_r28635771:\n\n@@ -95,6 +96,14 @@ func WithTimeout(d time.Duration) DialOption {\n    }\n }\n+// WithNetwork returns a DialOption that specifies the network on which\n+// the connection will be established.\n+func WithNetwork(network string) DialOption {\n\nThis feels too specific. It doesn't help this support any other address\ntypes either. Why don't we simply accept a dialer option that takes\ncomplete responsibility for turning the address into a net.Conn?\nI do not get it. Can you elaborate? What is \"a dialer option\"? Are you\nproposing to have our own struct to take complete responsibility for\nturning the addr to a net.Conn?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/169/files#r28635771.\n. Thanks. I prefer saving that goroutine for common cases and keeping the code as same as the corresponding part in tls.DialWithDialer.\n. we still need to process the headers frame to keep HPACK state consistent with the peer.\n. I do not think we need to iterate activeStreams because server does not initiate any streams in grpc protocol.\n. this needs t.mu.Lock()\n. goaway is true when the server receives GOAWAY frame.\n. not needed. the default is false already.\n. mutex.\n. On Thu, Apr 30, 2015 at 9:45 AM, Gian Biondi notifications@github.com\nwrote:\nIn transport/http2_server.go\nhttps://github.com/grpc/grpc-go/pull/180#discussion_r29446798:\n\n@@ -143,6 +146,10 @@ func (t http2Server) operateHeaders(hDec hpackDecoder, s *Stream, frame header\n            hDec.state = decodeState{}\n        }\n    }()\n-   if goaway {\n-       //Stop creating streams on this transport\n-       return nil\n-   }\n\nThis part I'm a little unclear on still. Can you point me to some\ndocumentation to explain this more?\nThis is basically the requirement of HPACK instead of gRPC. You can search\nHPACK IETF draft to see whether you can find something useful. HPACK uses\nhuffman coding to compress http headers. Thus to be able to encode&decode a\nheaders frame, the states of huffman coding on sender and receiver have to\nbe consistent. When a receiver receives a headers frame, it means the\nsender's huffman coding state has covered all the sent headers. In order to\nkeep consistency, the receiver has to process this headers frame and update\nits own huffman coding state regardless.\n\nActually, I would strongly recommend writing a test case in\ntransport_test.go to make sure your pull request works as expected. But I\nam not sure whether this is a mission impossible for you because\ntransport_test.go is not well constructed and hard to understand.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/180/files#r29446798.\n. we can probably add a logging here to help potential debugging.\n. just something like\n\nlog.Printf(\"transport: http2Server has received GOAWAY and started to reject new streams.\")\n. Thanks. Let me know if you have problem to understand the code. Or if it is too hard, you can leave it to me. :)\n. probably rename to rpclog to avoid conflicting with the standard one.\n. Thanks, will fix shortly.\nOn Wed, May 27, 2015 at 5:21 PM, David Symonds notifications@github.com\nwrote:\n\nIn test/end2end_test.go\nhttps://github.com/grpc/grpc-go/pull/202#discussion_r31193958:\n\n}\n-   var wg sync.WaitGroup\n-   wg.Add(1)\n-   go func() {\n-       // The 2nd stream should block until its deadline exceeds.\n-       ctx, _ := context.WithTimeout(context.Background(), time.Second)\n-       if , err := tc.StreamingInputCall(ctx); grpc.Code(err) != codes.DeadlineExceeded {\n-           t.Fatalf(\"%v.StreamingInputCall(%v) = , %v, want error code %d\", tc, ctx, err, codes.DeadlineExceeded)\n-       }\n-       wg.Done()\n\ngood practice to do defer wg.Done() at the start of a goroutine that\nyou're monitoring.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/202/files#r31193958.\n. On Wed, May 27, 2015 at 5:40 PM, Bryan C. Mills notifications@github.com\nwrote:\nIn transport/http2_client.go\nhttps://github.com/grpc/grpc-go/pull/202#discussion_r31194751:\n\nt.mu.Lock()\nif t.state != reachable {\n    t.mu.Unlock()\n    return nil, ErrConnClosing\n}\n-   if uint32(len(t.activeStreams)) >= t.maxStreams {\n-       t.mu.Unlock()\n-       t.writableChan <- 0\n-       return nil, StreamErrorf(codes.Unavailable, \"transport: failed to create new stream because the limit has been reached.\")\n-   if t.streamsQuota != nil {\n-       q, err := wait(ctx, t.shutdownChan, t.streamsQuota.acquire())\n\nThis call to wait occurs with t.mu still locked - is that safe?\nfixing ...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/202/files#r31194751.\n. I do not find issue here. For example, the new quota is 10 (v) and the old quota is 100 (t.maxStream). And line 255 wants to return 99 back to stream quota (100 -1). If the reset occurs before the add, we get t.streamsQuota == -90 after the reset and 9 quota on the t.streamsQuota.acquire() and t.streamsQuota == 9, which are all expected. Note that v-t.maxStreams is not the configured amount. Instead v is -- v - t.maxStreams + x < v because 0 < x <= t.maxStreams -1.\n. yes, it is atomic and does not matter. But I agree that it is unpleasant to make this inconsistent. I did not do that because it will introduce another a few booleans to this code.  Let me do it now.\n. On Thu, May 28, 2015 at 4:28 PM, Bryan C. Mills notifications@github.com\nwrote:\nIn transport/http2_client.go\nhttps://github.com/grpc/grpc-go/pull/205#discussion_r31287893:\n\n\nt.maxStreams = v\n// TODO(zhaoq): This is a hack to avoid significant refactoring of the\n// code to deal with the unrealistic int32 overflow. Probably will try\n// to find a better way to handle this later.\nif v > math.MaxInt32 {\nv = math.MaxInt32\n}\nt.mu.Lock()\nreset := t.streamsQuota != nil\nms := t.maxStreams\nt.maxStreams = int(v)\nt.mu.Unlock()\nif !reset {\nt.streamsQuota = newQuotaPool(int(v))\n} else {\nt.streamsQuota.reset(int(v) - ms)\n\n\nThis still doesn't work, as far as I can tell. The reset is atomic, but it\ndoesn't take into account the number of streams in-flight.\nDid you read the example I provided before:\n\nFor example, the new quota is 10 (v) and the old quota is 100\n(t.maxStream). And line 255 wants to return 99 back to stream quota (100\n-1). If the reset occurs before the add, we get t.streamsQuota == -90 after\nthe reset and 9 quota on the t.streamsQuota.acquire() and t.streamsQuota ==\n9, which are all expected. Note that v-t.maxStreams is not the configured\namount. Instead v is -- v - t.maxStreams + x < v because 0 < x <=\nt.maxStreams -1.\nIf you still think this approach does not count the number of streams\nin-flight, could you provide an example too? Note that this does remember\nthe old value which is t.maxStreams (the new value is v).\n\nIf you want to swap out t.streamsQuota, you need to ensure that there are\nno streams in-flight. Otherwise, you'll need to implement this in terms of\nacquire and/or add.\nOne alternative to consider: if you store the original quota size, then\nyou can compute the delta between the new and old settings. If new > old,\nyou can just add it; if new < old, you can acquire the difference and just\nnot add it back.\nBut that makes the handleSettings function take an arbitrarily long time\nwhile it waits for the existing streams to settle...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/205/files#r31287893.\n. Hi Bryan,\n\nYour example is impossible. I would blame the code review tool of github.\nNote that\n\n\nif !reset {\nt.streamsQuota = newQuotaPool(int(v))\n} else {\nt.streamsQuota.reset(int(v) - ms)\n\n\nis in handleSettings function instead of newStream. And handSetting is\nalways running sequentially (i.e., processing the incoming frames\none-by-one).\nOn Fri, May 29, 2015 at 2:34 PM, Bryan C. Mills notifications@github.com\nwrote:\n\nIn transport/http2_client.go\nhttps://github.com/grpc/grpc-go/pull/205#discussion_r31366928:\n\n\nt.maxStreams = v\n// TODO(zhaoq): This is a hack to avoid significant refactoring of the\n// code to deal with the unrealistic int32 overflow. Probably will try\n// to find a better way to handle this later.\nif v > math.MaxInt32 {\nv = math.MaxInt32\n}\nt.mu.Lock()\nreset := t.streamsQuota != nil\nms := t.maxStreams\nt.maxStreams = int(v)\nt.mu.Unlock()\nif !reset {\nt.streamsQuota = newQuotaPool(int(v))\n} else {\nt.streamsQuota.reset(int(v) - ms)\n\n\nA minimal example: what if you have two newStream calls that both update\nmaxStreams, from X to Y and then to Z?\nThe first call does:\nLock\nms := X\nt.maxStreams = Y\nUnlock\nThe second does:\nLock\nms := Y\nt.maxStreams = Z\nUnlock\nreset(Z - Y)\nAnd then back to the first:\nreset(Y - X)\nAnd that's not even taking into account the t.streamsQuota.add calls that\nmay have occurred in the interim, which AFAICT introduce problems of their\nown.\n(In general, when you're mixing atomic-increment and atomic-reset the\nresets need to be compare-and-swap, not blind writes.)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/205/files#r31366928.\n. On Fri, May 29, 2015 at 2:34 PM, Bryan C. Mills notifications@github.com\nwrote:\nIn transport/http2_client.go\nhttps://github.com/grpc/grpc-go/pull/205#discussion_r31366928:\n\n\nt.maxStreams = v\n// TODO(zhaoq): This is a hack to avoid significant refactoring of the\n// code to deal with the unrealistic int32 overflow. Probably will try\n// to find a better way to handle this later.\nif v > math.MaxInt32 {\nv = math.MaxInt32\n}\nt.mu.Lock()\nreset := t.streamsQuota != nil\nms := t.maxStreams\nt.maxStreams = int(v)\nt.mu.Unlock()\nif !reset {\nt.streamsQuota = newQuotaPool(int(v))\n} else {\nt.streamsQuota.reset(int(v) - ms)\n\n\nA minimal example: what if you have two newStream calls that both update\nmaxStreams, from X to Y and then to Z?\nThe first call does:\nLock\nms := X\nt.maxStreams = Y\nUnlock\nThe second does:\nLock\nms := Y\nt.maxStreams = Z\nUnlock\nreset(Z - Y)\nAnd then back to the first:\nreset(Y - X)\nAnd that's not even taking into account the t.streamsQuota.add calls that\nmay have occurred in the interim, which AFAICT introduce problems of their\nown.\n(In general, when you're mixing atomic-increment and atomic-reset the\nresets need to be compare-and-swap, not blind writes.)\nWe do not have concurrent resets so that compare-and-swap is not needed.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/205/files#r31366928.\n. delete it if it is not needed.\n. ditto\n. add a space between \"//\" and \"DoStreamingCall\"\n. This means you create a new stream every time, which is incorrect. You probably can use sync.Once to init the stream here.\n. if _, err := stream.Recv(); err != nil {\n  ...\n}\n. change the log to something like:\n\ngrpclog.Fatalf(\"%v.StreamingCall() = , %v\", tc, err)\n. ditto\n. ditto\n. run --> runUnary?\n. revoke my previous comment regarding renaming \"run\" since it seems runStream is not needed.\n. This seems simply duplication of run(...). Why do we need this?\n. we should have an additional flag to choose unary|stream|both.\n. ?\n. Can this be merged with closeLoop (caller can be treated as a a param)?\n. need to move this also\n. this comment is incorrect. It only does 1 round-trip for a single streaming rpc.\n. probably DoStreamingRoundTrip?\n. can you swap the order of \"stream\" and \"tc\"? tc should be first one.\n. ditto. swap \"stream\" and \"client\"\n. have a more detailed description of this flag (see https://github.com/grpc/grpc-go/blob/master/interop/client/client.go#L63 for a similar example).\n. add space to both sides of \"=\"\n. ditto\n. space\n. methodFamily sounds better.\n. we should have a switch to turn tracing on/off. It could impact performance.\n. I suggest to put the above trace related structs into a separate file (trace.go) and have some unit test for it (trace_test.go).\n. I strongly suggest this because the tracing code could become more complex later.\nBTW, benchmark/benchmark_test.go is ready for regression test. Feel free to play with it.\n. Why does it throw method away? What is the incentive to do this conversion?\n. BTW, the benchmark uses 1-byte payload. You probably should try larger sized payload.\n. lack a function to turn this on?\n. On Sat, Jun 6, 2015 at 8:42 AM, David Symonds notifications@github.com\nwrote:\n\nIn call.go https://github.com/grpc/grpc-go/pull/210#discussion_r31868998\n:\n\n+\n-   c.traceInfo.tr = trace.New(\"Sent.\"+familyForMethod(method), method)\n-   defer c.traceInfo.tr.Finish()\n-   c.traceInfo.firstLine.client = true\n-   if deadline, ok := ctx.Deadline(); ok {\n-       c.traceInfo.firstLine.deadline = deadline.Sub(time.Now())\n-   }\n-   c.traceInfo.tr.LazyLog(&c.traceInfo.firstLine, false)\n-   // TODO(dsymonds): Arrange for c.traceInfo.firstLine.remoteAddr to be set.\n-   defer func() {\n-       if err != nil {\n-           c.traceInfo.tr.LazyLog(&lazySprintf{\"%v\", []interface{}{err}}, true)\n-           c.traceInfo.tr.SetError()\n-       }\n-   }()\n  +\n\nWhat complexity are you thinking of? This change has 90% of the complexity\nit'll have for the client side, and the server side will be comparable.\nThis pull request probably won't add too much overhead (my only slight\nconcern is the string operations if method name is long). But if we enhance\nthis tracing later, it probably will run into some contention (e.g. update\nsome shared stats) and more expensive operations. So I prefer having a\nswitch at the 1st place.\nI'll take a look at benchmark_test.go.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/210/files#r31868998.\n. I still think we should have a setter for it now and the default would be false (we can probably turn it on by default later. But at this alpha release stage, I think it would be better as false.).\n. I would prefer a variable initialization instead of init().\n\nvar logger = log.New(os.Stderr, \"\", log.LstdFlags)\n. It is called by LazyLog but itself is not \"lazy\". Maybe rename to \"rpcStringer\"?\n. On Mon, Jun 8, 2015 at 9:51 PM, David Symonds notifications@github.com\nwrote:\n\nIn trace.go\nhttps://github.com/grpc/grpc-go/pull/210#discussion_r31982580:\n\n\nio.WriteString(&line, \"none\")\n}\nreturn line.String()\n  +}\n  +\n  +// payload represents an RPC request or response payload.\n  +type payload struct {\nm interface{} // e.g. a proto.Message\n// TODO(dsymonds): add stringifying info to codec, and limit how much we hold here?\n  +}\n  +\n  +func (p payload) String() string {\nreturn fmt.Sprint(p.m)\n  +}\n  +\n  +type lazySprintf struct {\n\n\nIt's called lazySprintf because it's an arbitrary Sprintf call wrapper\nthat's called lazily. It isn't stringing any RPC (it's not RPC specific at\nall). I could go with lazyStringer if you would be satisfied with that.\nI know. I agree rpcStringer is not a good name. But shouldn't we name a\nstruct according to what task/operations it completes? Why should we name a\nstruct according to the caller's behavior? how about traceStringer?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/210/files#r31982580.\n. On Tue, Jun 9, 2015 at 6:56 AM, David Symonds notifications@github.com\nwrote:\nIn trace.go\nhttps://github.com/grpc/grpc-go/pull/210#discussion_r32014738:\n\n\nio.WriteString(&line, \"none\")\n}\nreturn line.String()\n  +}\n  +\n  +// payload represents an RPC request or response payload.\n  +type payload struct {\nm interface{} // e.g. a proto.Message\n// TODO(dsymonds): add stringifying info to codec, and limit how much we hold here?\n  +}\n  +\n  +func (p payload) String() string {\nreturn fmt.Sprint(p.m)\n  +}\n  +\n  +type lazySprintf struct {\n\n\nfmtStringer? It uses fmt to implement the Stringer interface. That seems\nfine.\nsgtm.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/210/files#r32014738.\n. On Fri, Jun 12, 2015 at 6:08 PM, David Symonds notifications@github.com\nwrote:\nIn Makefile\nhttps://github.com/grpc/grpc-go/pull/225#discussion_r32366036:\n\n@@ -39,13 +38,7 @@ proto:\n        protoc -I $$(dirname $$file) --go_out=plugins=grpc:$$(dirname $$file) $$file; \\\n    done\n-lint: testdeps\n-   go get -v github.com/golang/lint/golint\n-   for file in $$(git ls-files '*.go' | grep -v '.pb.go$$' | grep -v '_string.go$$'); do \\\n-       golint $$file; \\\n-   done\n-pretest: lint\n+pretest:\n\nit doesn't seem like you need the pretest target either.\nIt is in a pending commit. I am trying to figure out why travis is broken\non this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/225/files#r32366036.\n. On Fri, Jun 12, 2015 at 6:10 PM, Qi Zhao toqizhao@gmail.com wrote:\nOn Fri, Jun 12, 2015 at 6:08 PM, David Symonds notifications@github.com\nwrote:\n\nIn Makefile\nhttps://github.com/grpc/grpc-go/pull/225#discussion_r32366036:\n\n@@ -39,13 +38,7 @@ proto:\n       protoc -I $$(dirname $$file) --go_out=plugins=grpc:$$(dirname $$file) $$file; \\\n   done\n-lint: testdeps\n-  go get -v github.com/golang/lint/golint\n-  for file in $$(git ls-files '*.go' | grep -v '.pb.go$$' | grep -v '_string.go$$'); do \\\n-      golint $$file; \\\n-  done\n-pretest: lint\n+pretest:\n\nit doesn't seem like you need the pretest target either.\nIt is in a pending commit. I am trying to figure out why travis is broken\non this.\n\nhttps://travis-ci.org/grpc/grpc-go/builds/66626132\n\u2014\n\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/225/files#r32366036.\n. ok, I got it.\n\n\nOn Fri, Jun 12, 2015 at 6:11 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nOn Fri, Jun 12, 2015 at 6:10 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nOn Fri, Jun 12, 2015 at 6:08 PM, David Symonds notifications@github.com\nwrote:\n\nIn Makefile\nhttps://github.com/grpc/grpc-go/pull/225#discussion_r32366036:\n\n@@ -39,13 +38,7 @@ proto:\n      protoc -I $$(dirname $$file) --go_out=plugins=grpc:$$(dirname $$file) $$file; \\\n  done\n-lint: testdeps\n- go get -v github.com/golang/lint/golint\n- for file in $$(git ls-files '*.go' | grep -v '.pb.go$$' | grep -v '_string.go$$'); do \\\n-     golint $$file; \\\n- done\n-pretest: lint\n+pretest:\n\nit doesn't seem like you need the pretest target either.\nIt is in a pending commit. I am trying to figure out why travis is broken\non this.\n\nhttps://travis-ci.org/grpc/grpc-go/builds/66626132\n\u2014\n\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/225/files#r32366036.\n. put this into the following brackets.\n. Comments. Please add comments to all exported vars/funcs.\n. Are these 3 functions all required?\n. you can remove this line.\n. do not use the function name starting from \"_\"? rename it to healthCheckHandler maybe?\n. and actually this is not needed to be exported.\n. we should remove this dependency.\n. It seems you do not care the value of \"err\". You can do\n\n\n\n_, err := hc.Check(ctx, req)\nreturn err\n. comments?\n. // Package health provides some utility functions to health-check a server. The implementation // is based on protobuf. Users need to write their own implementations if other IDLs are used.\n. remove newline\n. \"Succeed\" --> \"Success\"?\n. \"Succeed\" --> \"Success\"?\n. \"Succeed\" --> \"Success\"?\n. t.Fatalf(\" ... want error code %d\", err, codes.DeadlineExceeded)\n. t.Fatalf(\" ... want error code %d\", err, codes.Unimplemented)\n. On Wed, Jun 17, 2015 at 7:58 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/229#discussion_r32695701:\n\ns, err := t.NewStream(ctx, callHdr)\nif err != nil {\n    return nil, toRPCErr(err)\n}\nreturn &clientStream{\n-       t:     t,\n-       s:     s,\n-       p:     &parser{s: s},\n-       desc:  desc,\n-       codec: cc.dopts.codec,\n-       t:         t,\n-       s:         s,\n-       p:         &parser{s: s},\n-       desc:      desc,\n-       codec:     cc.dopts.codec,\n-       traceInfo: trInfo,\n\nthis is incorrect. This is making a copy of the traceInfo struct, but the\ntrace has a pointer to the original, so any changes to firstLine after this\nwill not be reflected, for instance. It'll also pin both bits of memory.\nI agree this needs to be changed. But the only reason is that it introduces\nunnecessary copying. This is the exit point of the entire function so that\nI do not understand what you meant by \"any changes to firstLine after this\nwill not be reflected\". Do I miss anything here?\nEither clientStream needs to be allocated higher up (probably the simplest\noption), or it needs to have a *traceInfo field instead.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/229/files#r32695701.\n. On Thu, Jun 18, 2015 at 11:16 AM, Qi Zhao toqizhao@gmail.com wrote:\nOn Wed, Jun 17, 2015 at 7:58 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/229#discussion_r32695701:\n\ns, err := t.NewStream(ctx, callHdr)\n   if err != nil {\n       return nil, toRPCErr(err)\n   }\n   return &clientStream{\n-      t:     t,\n-      s:     s,\n-      p:     &parser{s: s},\n-      desc:  desc,\n-      codec: cc.dopts.codec,\n-      t:         t,\n-      s:         s,\n-      p:         &parser{s: s},\n-      desc:      desc,\n-      codec:     cc.dopts.codec,\n-      traceInfo: trInfo,\n\nthis is incorrect. This is making a copy of the traceInfo struct, but the\ntrace has a pointer to the original, so any changes to firstLine after this\nwill not be reflected, for instance. It'll also pin both bits of memory.\nI agree this needs to be changed. But the only reason is that it\nintroduces unnecessary copying. This is the exit point of the entire\nfunction so that I do not understand what you meant by \"any changes to\nfirstLine after this will not be reflected\". Do I miss anything here?\n\nOh, okay, I see. I just noticed that firstLine is an object instead of a\npointer in traceInfo.\nEither clientStream needs to be allocated higher up (probably the simplest\n\noption), or it needs to have a *traceInfo field instead.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/229/files#r32695701.\n. This should have been discovered by the manual testing. We were lazy and reused the benchmark binary to test this. But unfortunately the benchmark only performs streaming rpcs whose lifetime is as same as the binary itself so that this bug was hidden. Please revise it and make sure it works properly.\n. On Thu, Jun 18, 2015 at 5:35 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/229#discussion_r32793004:\n\n@@ -161,6 +173,16 @@ func (cs *clientStream) SendMsg(m interface{}) (err error) {\nfunc (cs *clientStream) RecvMsg(m interface{}) (err error) {\n    err = recv(cs.p, cs.codec, m)\n-   defer func() {\n-       // err != nil indicates the termination of the stream.\n-       if EnableTracing && err != nil {\n-           if err != io.EOF {\n-               cs.traceInfo.tr.LazyLog(&fmtStringer{\"%v\", []interface{}{err}}, true)\n-               cs.traceInfo.tr.SetError()\n-           }\n-           cs.traceInfo.tr.Finish()\n-       }\n\nI wonder if some of the fields of cs should be set to nil here\n(independent of EnableTracing) to make a clearer panic if RecvMsg is\ncalled again after it returns an error. e.g. setting cs.p = nil and cs.codec\n= nil would probably help, since an erroneous use of a trace after Finish\ncan be hard to diagnose.\nI would prefer letting clientStream keep an err status. Once it becomes\nnon-nil, all the follow-up recv/send operations returns with that error\nimmediately. But let' do it in a separate PR.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/229/files#r32793004.\n. On Thu, Jun 18, 2015 at 6:02 PM, Qi Zhao toqizhao@gmail.com wrote:\nOn Thu, Jun 18, 2015 at 5:35 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/229#discussion_r32793004:\n\n@@ -161,6 +173,16 @@ func (cs *clientStream) SendMsg(m interface{}) (err error) {\nfunc (cs *clientStream) RecvMsg(m interface{}) (err error) {\n   err = recv(cs.p, cs.codec, m)\n-  defer func() {\n-      // err != nil indicates the termination of the stream.\n-      if EnableTracing && err != nil {\n-          if err != io.EOF {\n-              cs.traceInfo.tr.LazyLog(&fmtStringer{\"%v\", []interface{}{err}}, true)\n-              cs.traceInfo.tr.SetError()\n-          }\n-          cs.traceInfo.tr.Finish()\n-      }\n\nI wonder if some of the fields of cs should be set to nil here\n(independent of EnableTracing) to make a clearer panic if RecvMsg is\ncalled again after it returns an error. e.g. setting cs.p = nil and cs.codec\n= nil would probably help, since an erroneous use of a trace after\nFinish can be hard to diagnose.\nI would prefer letting clientStream keep an err status. Once it becomes\nnon-nil, all the follow-up recv/send operations returns with that error\nimmediately. But let' do it in a separate PR.\n\nhmm, but this introduces a lock to protect err access which is bad... let\nme think more about this.\n\u2014\n\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/229/files#r32793004.\n. I am not convinced that we should log all outbound payload (and inbound payload) for streaming rpc. There could be huge.\n. On Thu, Jun 25, 2015 at 5:23 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/235#discussion_r33319598:\n\n@@ -155,6 +155,15 @@ func (cs *clientStream) Trailer() metadata.MD {\n }\nfunc (cs *clientStream) SendMsg(m interface{}) (err error) {\n-   if EnableTracing {\n-       cs.traceInfo.tr.LazyLog(payload{m}, true)\n\nI don't think the particular use of payloads for streaming should matter.\nUnary RPCs can be just as big. Controls over the size are a fine thing to\nhave, but that's independent of this change.\n(There's no need for control on the number of payloads; the trace package\nwill only hold on to 10 events per trace. The only constraint may be on a\nper-payload basis, which will be a little harder to do since we permit\narbitrary codecs. I have ideas for how to handle that though.)\nSo a trace hold up to 10 payload for an RPC? The latest will overwrite the\noldest? If that is the case, it sgtm. Otherwise it could be much worse than\nunary rpc (infinite * 1 payload vs. 1 payload).\n\nBTW, does trace truncate the payload if it is large?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/235/files#r33319598.\n. The user could do write and read in 2 different goroutines.  i) Can cs.traceInfo.tr be accessed by multiple concurrent goroutines? ii) cs.traceInfo.tr = nil seems guarantee one of them will get a panic ... \n. On Thu, Jun 25, 2015 at 5:51 PM, David Symonds notifications@github.com\nwrote:\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/235#discussion_r33320833:\n\n@@ -226,6 +234,19 @@ func (cs *clientStream) CloseSend() (err error) {\n    return\n }\n+func (cs *clientStream) finish(err error) {\n\nYes, this would break because of that. I couldn't figure out how this code\nshould clearly delimit when a clientStream is \"done\", since both SendMsg\nand RecvMsg appear to indicate the termination of the stream when things\nfail.\nDoes clientStream need a mutex to mediate this? What's supposed to happen\nif concurrent SendMsg/RecvMsg calls are running?\n\ni) add clientStream.mu to protect payload tracing which could happens in\nSendMsg and RecvMsg at same time;\nii)  a streaming rpc is always terminated via a stream.Recv() or\nstream.CloseAndRecv(0 (both invoke RecvMsg) to get the final status even\nthough stream.Send(...) could get an IO error first. So trace termination\nshould be only in RecvMsg.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/235/files#r33320833.\n. This is the thing I always try to avoid -- it adds locking/unlocking overhead even when tracing is disabled. Let's shoot for a way where the fast path is not impacted when tracing is disabled. I am okay to check EnableTracing a couple of times -- it is better than acquiring/releasing locks regardless tracing is enabled or not.\n. I agree this makes the code look better. But I still think it should NOT\nimpact the performance of the fast path. It is understood it is per-stream\nlock and there is much contention on it. But it still adds at least a few\nhundreds of nanoseconds (according to the experience with C++ code, it is\nprobably longer in Go) even though there is no contention. It is likely\ninvisible in the current benchmark (unless some deliberate benchmark is\ndesigned) but it could show up when we introduce some other high-speed\ntransport (e.g., shared memory, RDMA) in the future. I hope we try our best\nto hold \"the fast path should not be impacted\" principle across the entire\ndev process.\n\nOn Tue, Jun 30, 2015 at 6:36 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/235#discussion_r33641754:\n\n@@ -155,6 +160,11 @@ func (cs *clientStream) Trailer() metadata.MD {\n }\nfunc (cs *clientStream) SendMsg(m interface{}) (err error) {\n-   cs.mu.Lock()\n\nI thought this approach was better since it limited the number of places\ntouching the global EnableTracing var.\nKeep in mind that this is a per-stream lock, not a global lock, so it only\nis contended when the stream itself is busy. That's not often the case\n(since streams usually require a specific ordering), so the mutex overhead\nwill normally be unnoticed. I can run the benchmarks if you'd like me to\nverify this.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/235/files#r33641754.\n. I do not have machine to do test and it is good to know that Go mutex impl\nspecializes the uncontended case so that it is fast. For the \"lots of\nthreads\" case, I am not sure (it would be more subtle and depends on how\nsmart the compiler is and the CPU caching). But regardless, we can have a\nclientStream.enableTracing boolean to gain all the sharding benefit without\nhaving an irrelevant lock when tracing is off.\n\nAs you mentioned the resulting code is only \"a bit simpler\", I am not\nconvinced why we want to leave a hole now for future benchmark and\noptimization...\nOn Wed, Jul 1, 2015 at 6:36 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/235#discussion_r33741310:\n\n@@ -155,6 +160,11 @@ func (cs *clientStream) Trailer() metadata.MD {\n }\nfunc (cs *clientStream) SendMsg(m interface{}) (err error) {\n-   cs.mu.Lock()\n\nAn uncontended mutex lock/unlock is a compare-and-swap (for the lock) and\nan atomic add (for the unlock), which together is around 16ns on my\n(getting old) workstation. It's definitely not \"hundreds of nanoseconds\".\nIt's only a little slower than reading a global var (EnableTracing), which\nmay, in fact, be slower if there's lots of threads (a mutex per\nclientStream is thus effectively sharding that).\nI suggest we keep this how this change has it. It keeps the code a bit\nsimpler, it's an internal only thing, and we can switch it out when higher\nspeed transports arrive and 16ns is enough to show up on a benchmark.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/235/files#r33741310.\n. Move this comment under \"mu\" declaration. It is for traceInfo.\n. fix the indent\n. remove redundant spaces\n. service_name ==> serviceName\n. use req := &healthpb.HealthCheckRequest{ ... }\n. add a comment for this map?\n. I think StatusMap does not need to be exported. right?\n. comment startomg with \"SetServingStatus ...\"\n. probably we should add \"host\" in the param list:\n\nfunc (s *HealthServer) SetServingStatus(host, service string, status int32) \n. this convenience func seems not add much value. Let's remove it and let users make a call by themselves.\n. run gofmt\n. revise: delete the following newline and run gofmt\n. I actually like NewComputeEngine (and make TokenSource unexported) better because it is consistent with other credentials (service account and default) and simpler.\nPlus, this change should not be in this PR.\n. @dsymonds I prefer leaving NewComputeEngine and making TokenSource unexported. NewComputeEngine is simpler and consistent with service account and application default.\n. On Wed, Jul 22, 2015 at 6:30 PM, David Symonds notifications@github.com\nwrote:\n\nIn credentials/oauth/oauth.go\nhttps://github.com/grpc/grpc-go/pull/237#discussion_r35283098:\n\n+\n+// GetRequestMetadata gets the request metadata as a map from a TokenSource.\n+func (ts TokenSource) GetRequestMetadata(ctx context.Context) (map[string]string, error) {\n-   token, err := ts.Token()\n-   if err != nil {\n-       return nil, err\n-   }\n-   return map[string]string{\n-       \"authorization\": token.TokenType + \" \" + token.AccessToken,\n-   }, nil\n  +}\n  +\n  +// NewComputeEngine constructs the credentials that fetches access tokens from\n  +// Google Compute Engine (GCE)'s metadata server. It is only valid to use this\n  +// if your program is running on a GCE instance.\n  +// TODO(dsymonds): Deprecate and remove this.\n\nTokenSource needs to be exported. There are places that need it (and that\ndon't use google.ComputeTokenSource). It is NewComputeEngine that is the\ntrivial wrapper.\nWhy does it need to if we keep NewComputeEngine?\ncredentials.NewComputeEngine()\nis clearly better than credentials.TokenSource{\ngoogle.ComputeTokenSource(\"\")}) from the perspective of users.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/237/files#r35283098.\n. On Wed, Jul 22, 2015 at 6:41 PM, David Symonds notifications@github.com\nwrote:\nIn credentials/oauth/oauth.go\nhttps://github.com/grpc/grpc-go/pull/237#discussion_r35283573:\n\n+\n+// GetRequestMetadata gets the request metadata as a map from a TokenSource.\n+func (ts TokenSource) GetRequestMetadata(ctx context.Context) (map[string]string, error) {\n-   token, err := ts.Token()\n-   if err != nil {\n-       return nil, err\n-   }\n-   return map[string]string{\n-       \"authorization\": token.TokenType + \" \" + token.AccessToken,\n-   }, nil\n  +}\n  +\n  +// NewComputeEngine constructs the credentials that fetches access tokens from\n  +// Google Compute Engine (GCE)'s metadata server. It is only valid to use this\n  +// if your program is running on a GCE instance.\n  +// TODO(dsymonds): Deprecate and remove this.\n\n@iamqizhao https://github.com/iamqizhao: You're missing my point. There\nis a need for credentials.TokenSource where it will not be used with\ngoogle.ComputeTokenSource, so we can't unexport it. We can keep\nNewComputeEngine around if you really want; I'm just saying we can't\nunexport TokenSource.\n\nWhat is the problem with\nhttps://github.com/iamqizhao/grpc-go/commit/3104ff998c521fb873ff5e0b8eb12c2bf280f0f8\n?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/237/files#r35283573.\n. or u meant some other places outside grpc repo?\n\nOn Wed, Jul 22, 2015 at 6:52 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nOn Wed, Jul 22, 2015 at 6:41 PM, David Symonds notifications@github.com\nwrote:\n\nIn credentials/oauth/oauth.go\nhttps://github.com/grpc/grpc-go/pull/237#discussion_r35283573:\n\n+\n+// GetRequestMetadata gets the request metadata as a map from a TokenSource.\n+func (ts TokenSource) GetRequestMetadata(ctx context.Context) (map[string]string, error) {\n-  token, err := ts.Token()\n-  if err != nil {\n-      return nil, err\n-  }\n-  return map[string]string{\n-      \"authorization\": token.TokenType + \" \" + token.AccessToken,\n-  }, nil\n  +}\n  +\n  +// NewComputeEngine constructs the credentials that fetches access tokens from\n  +// Google Compute Engine (GCE)'s metadata server. It is only valid to use this\n  +// if your program is running on a GCE instance.\n  +// TODO(dsymonds): Deprecate and remove this.\n\n@iamqizhao https://github.com/iamqizhao: You're missing my point.\nThere is a need for credentials.TokenSource where it will not be used\nwith google.ComputeTokenSource, so we can't unexport it. We can keep\nNewComputeEngine around if you really want; I'm just saying we can't\nunexport TokenSource.\n\nWhat is the problem with\nhttps://github.com/iamqizhao/grpc-go/commit/3104ff998c521fb873ff5e0b8eb12c2bf280f0f8\n?\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/237/files#r35283573.\n. On Wed, Jul 22, 2015 at 7:02 PM, David Symonds notifications@github.com\nwrote:\n\nIn credentials/oauth/oauth.go\nhttps://github.com/grpc/grpc-go/pull/237#discussion_r35284447:\n\n+\n+// GetRequestMetadata gets the request metadata as a map from a TokenSource.\n+func (ts TokenSource) GetRequestMetadata(ctx context.Context) (map[string]string, error) {\n-   token, err := ts.Token()\n-   if err != nil {\n-       return nil, err\n-   }\n-   return map[string]string{\n-       \"authorization\": token.TokenType + \" \" + token.AccessToken,\n-   }, nil\n  +}\n  +\n  +// NewComputeEngine constructs the credentials that fetches access tokens from\n  +// Google Compute Engine (GCE)'s metadata server. It is only valid to use this\n  +// if your program is running on a GCE instance.\n  +// TODO(dsymonds): Deprecate and remove this.\n\nYes, other code bases. That's why I exported TokenSource in the first\nplace (in f2936c4\nhttps://github.com/grpc/grpc-go/commit/f2936c474cefaf94a6f97a034c18cb8cdaac17d0\n).\nokay, I see.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/237/files#r35284447.\n. S -> s\n. statusMap stores the serving statuses of the services this HealthServer monitors.\n. mu needs to be a member var of s.\n. create out iff ok is true.\n. This should be like:\n\nif status, ok := s.statusMap[service]; ok {\n  return &healthpb.HealthCheckResponse{...}, nil\n}\nreturn nil, grpc.Errorf(codes.NotFound, ....)\n. Move this adjacent to the relevant tests.\n. You can merge the following tests into a single one as we discussed offline.\n. mu above statusMap\n. TestHealthCheck() ==> HealthCheck/Check(, )\n. ,_ two params. want needs 2 also.\n. return hc.Check(ctx, req)\n. s.statusCode and s.statusDesc need to be set properly to help user find out the server is broken (probably use codes.Internal for statusCode). The side effect if you set s.statusCode and statusDesc is that your refactoring of readClosed is ruined because I do not want grab and release s.mu twice here. I would suggest reverting your commit for readClosed and writeClosed for now and leave it as another PR when we figure out a good way to refactor.\n. The spec does not state this explicitly, meaning the impl can make its own decision. I prefer treating this as an error because the peer is a mis-behaved server which breaks the grpc wire protocol actually.\n. stream.Method() should contain that info. You can parse that string to get service name.\n. ditto\n. if err != nil && err != io.EOF ?\n. this actually creates a new \"err\".\n. fuse this with the following return.\n. move this upwards.\n. do not return this error which is not an op error.\n. have a single return for this and the above branch.\n. @dsymonds \nFYI, my comment was before we decided to add a return value (error) for this function. github is bad on showing this ...\n. done. Thanks.\n. add comments for all exported type.\n. item -> *namePair\n. this check should be above the backlog op in line 33. Probably something like:\nif b.stopping {\n  return\n}\nb.backlog = append(...)\n...\n. if b.stopping || len(b.backlog) == 0 {\n  return\n}\n...\n. add some comments.\n. you need to set cancel in this function too.\n. can Watch and Get share the same \"c\"? If yes, we can use \"c\" to replace the \"cfg\" in etcdNR struct.\n. not needed.\n. not needed\n. comment the return value for ending.\n. you do not need to have \"key\" and \"val\" in return.\n. NameResolver does name resolution and watches for the resolution changes.\n. Get gets a snapshot of the current name resolution results for target.\n. Watch watches for the name resolution changes on target. It blocks until Stop() is invoked. The watch results are obtained via GetUpdate().\n. GetUpdate returns a name resolution change when watch is triggered. It blocks until it observes a change. The caller needs to call it again to get the next change.\n. Stop shuts down the NameResolver.\n. the order should be\nfmt\nsync\netcdcl ...\ngolang.org/x/net/contex\n. namePair --> kv\n. comment\n. getNode builds the resulting key-value map starting from node recursively.\n. not needed\n. remove string\n. This needs to be restructured like:\nnaming/naming.go (type Resolver interface)\nnaming/etcd/etcd.go (etcd impl)\nYou can follow what credentials + oauth does as an example.\n. nr.ctx and nr.cancel have been set in NewETCDNR already.\n. log.Errorf(\"etcdNR.Get(_) failed %v\", err)\nreturn nil\n. // recvBuffer is an unbounded channel of kv to record all the pending changes from etcd server.\n. // stop terminates the recvBuffer. After it is called, the recvBuffer is not usable any more.\n. period at the end\n. probably returning (name.Resolver, error) would be better.\n. %v.Recv() = _, %v, want error code %d\n. This should be applied to the sync connecting case at line 152 too.\n. suggest using \"defer\" for this at line 416 because the exit path at line 418 also needs to call Finish.\n. you do not need this. Use in.Service directly.\n. add a comment\n. can you have a separate NewJWTAccessFromKey for users who get key from somewhere directly?\n. return NewJWTAccessFromKey(jsonKey, audience) directly?\n. service_account --> jwt_token_creds?\n. do not use pointer to avoid malicious behavior from the caller.\n. ditto\n. remove?\n. s/config/err\n. grpclog.Fatalf\n. just let it fatal.\n. we can exit when error happens. Only return the 1st one.\n. err, it is okay to take pointer here if it is convenient for impl. Then just change the following \"token\" to \"token\".\n. add a comment.\n. token ==> getoken()\n. set cc.events to nil to avoid further invoking?\n. I think events should be protected by \"mu\" to avoid race?\n. What if cc.events.Printf is called after cc.event.Finish() has been called? Is it no-op or there is some issue? If it is an issue, we need to set cc.events to nil here but then we still need a mutex to protect cc.events.\nI actually tend to put the operations on cc.events (Printf, errorf, finish ) along with the change of cc.state. cc.state records the state change of ClientConn (and here we just need to push it to tracing) and has been protected by a mutex.\n. rename to eventPrintf and eventErrorf respectively?\n. Go 1.4 or above?\n. ok, this is not accurate now because 1.5 has been released. I can fix it later.\n. @dsymonds I would change it to \"Go 1.4 and above\" instead of \"requiring 1.5\".\n. yeah, I should. I ran gofmt -w */.go but did not notice this file is 2-level deeper and did not get touched.\n. I got notified this morning by the auth team to make this change and they said the design decision is firm. This is what java does https://github.com/google/google-auth-library-java/blob/master/credentials/java/com/google/auth/Credentials.java#L45. I borrowed some text and naming from there. I am still waiting for their update on designdoc so that I can make this better.\n. move to line#363?\n. is there any problem if cc.event.Errorf/Printf get called after Finish is called?\n. I missed this in my previous PR. I actually tend to make state exported directly.\n. currently all server side error leads to a log fatal. I would suggest keeping that convention in this PR at least. we may or may not convert them to returning an error to client later.\n. The getter does not prohibit mutation. It is the same thing compared to making it exported.\n. My point is that there are other places which also log fatal on server side. If you want to do the change, you need to change all of them in one shot, which needs some refactoring (e.g., there is one in newPayload). I think it is probably not worth doing that for now.\n. On Tue, Sep 8, 2015 at 10:50 AM, Tamir Duberstein notifications@github.com\nwrote:\n\nIn credentials/credentials.go\nhttps://github.com/grpc/grpc-go/pull/328#discussion_r38957015:\n\n@@ -126,6 +126,10 @@ func (t TLSInfo) AuthType() string {\n    return \"tls\"\n }\n+func (t TLSInfo) ConnState() tls.ConnectionState {\n\nI'm pretty sure it does prohibit mutation; it returns a copy.\nMaking it exported also makes a copy when users work on it (notice\ntls.ConnectionState is a object not a pointer). Plus, making a copy does\nNOT prohibit mutation because tls.ConnectionState contains a number of\npointer fields.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/328/files#r38957015.\n. On Tue, Sep 8, 2015 at 10:55 AM, Qi Zhao toqizhao@gmail.com wrote:\nOn Tue, Sep 8, 2015 at 10:50 AM, Tamir Duberstein \nnotifications@github.com wrote:\n\nIn credentials/credentials.go\nhttps://github.com/grpc/grpc-go/pull/328#discussion_r38957015:\n\n@@ -126,6 +126,10 @@ func (t TLSInfo) AuthType() string {\n   return \"tls\"\n }\n+func (t TLSInfo) ConnState() tls.ConnectionState {\n\nI'm pretty sure it does prohibit mutation; it returns a copy.\nMaking it exported also makes a copy when users work on it (notice\ntls.ConnectionState is a object not a pointer). Plus, making a copy does\nNOT prohibit mutation because tls.ConnectionState contains a number of\npointer fields.\n\n\nActually adding a getter might make the 2nd copy (depends on how the\ncompiler optimizes it) which is not needed.\n\n\u2014\n\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/328/files#r38957015.\n. On Tue, Sep 8, 2015 at 11:00 AM, Tamir Duberstein notifications@github.com\nwrote:\n\nIn credentials/credentials.go\nhttps://github.com/grpc/grpc-go/pull/328#discussion_r38958528:\n\n@@ -126,6 +126,10 @@ func (t TLSInfo) AuthType() string {\n    return \"tls\"\n }\n+func (t TLSInfo) ConnState() tls.ConnectionState {\n\nMaking it exported also makes a copy when users work on it (notice\ntls.ConnectionState is a object not a pointer).\nDoesn't matter that it's not a pointer. If it's exported I can overwrite\nit entirely: tlsInfo.State = myOtherTlsState.\nI would say overwriting does not matter because you only overwrite your own\ncopy which is not the ConnectionState of the real tls connection (because\ntls.ConnectionState returns a copy). I think that is the whole point why\ntls.ConnectionState returns an object instead of a pointer. Keep in mind\nthat you are the service provider. I do not see any malicious operations\ncould happen here.\nPlus, making a copy does\nNOT prohibit mutation because tls.ConnectionState contains a number of\npointer fields.\nYes, you could mess with some of the slices in tls.ConnectionState\nhttp://golang.org/pkg/crypto/tls/#ConnectionState but it's still better\nthan exporting this.\nSlice elements are pointers.\nActually adding a getter might make the 2nd copy (depends on how the\ncompiler optimizes it) which is not needed.\nThis doesn't matter - those copies are on the stack and the Go compiler\ndoes RVO.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/328/files#r38958528.\n. This seems on an out-of-date version. See https://github.com/grpc/grpc-go/blob/master/server.go#L204. You probably need to sync your client.\n. why mixed errors.New and fmt.Errorf?\n. ditto\n. ditto\n. Please make them consistent. I think the fundamental reason to use errors.New instead of fmt.Errorf is when your code and/or tests need to reuse this error in multiple places.\n\nWhether it can Interpolate arguments is just a artifact in my mind. You can always use fmt.Sprintf  as input param of errors.New.\n. Yeah, github review UI is horrible ... :)\nDon't we need a mutex to guard cc.events and s.events (because you reset its value to nil now)? I think it can align with cc.state to share the same lock then.\n. if cc.dopts.clientMonitor != nil {\n   ...\n}\n. given the interface name, this can be simply \"New\".\n. NewClientMonitor --> New?\n. why do we need this? Cannot we just use some thing like \"if monitor != nil\" to avoid this? This introduces unnecessary memory allocation and copying and impact the perf of fast path (i.e., monitoring is disabled).\n. \"external\" is still murky for users. For example, grpc package depends on \"golang.org/x/net/context\". We need to better define \"external\".\n. my suggestion: \nThe google.golang.org/grpc Go package must not depend on any non-standard or non-official external Go libraries. \n. I meant https://github.com/golang. \n. sounds good. Had the same feeling actually. :) changing ...\nOn Tue, Sep 22, 2015 at 4:49 PM, David Symonds notifications@github.com\nwrote:\n\nIn rpc_util.go\nhttps://github.com/grpc/grpc-go/pull/353#discussion_r40158957:\n\n@@ -217,6 +217,18 @@ func Code(err error) codes.Code {\n    return codes.Unknown\n }\n+// Desc returns the error description of err if it was produced by the rpc system.\n+// Otherwise, it returns err.Error() or empty string when err is nil.\n+func Desc(err error) string {\n\nThis isn't a great name in the context. grpc.Desc doesn't say much\n(whereas at least the above grpc.Code is referring to the more specific\n\"code\" concept).\nPerhaps ErrorDesc would be clearer?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/353/files#r40158957.\n. put it under cc.mu.Lock() (line#369).\n. guard by cc.mu?\n. @Sajmani SGTM given the timeline for your demo. You can leave this to me. I'll figure out a way to address it.\n. I have slight concern that this introduces unnecessary latency on the fast path when tracing is disabled. We need a way to avoid it. But it is okay to put it into a separate pr.\n. It seems if at https://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L181, you do\n\nif EnableTracing {\n  ctx = trace.NewContext(ctx, traceInfo.tr)  // create traceInfo there?\n}\nthen tune line#183 and 185 a bit. All the concerns about cancellation are addressed. \n. destination service name such as bigtable.googleapis.com.\n. rewrite the API according to some offline discussion in the team. This API may not be sufficient to support all connection state requirements (e.g., one with many sub-connections). \n. changed to unicastPicker which sounds better and more accurate to me.\n. unicastPicker\n. removed.\n. @dsymonds  done.\n. fixed in #383\n. define \"5\" and \"1\" as constant vars instead of numeric literal.\n. const (\n  ...\n)\n. Yep, This is a dangling point I am still attacking. I actually spent about half an hour on looking for a good way to eliminate it and did not find a decent. Closure will be my last resort.\n. Yep, resetTransport keeps connecting unless it sees a permanent error (e.g., the caller actively closes the ClientConn). Will add a comment soon.\n. You are right. revised Picker API in #393.\n. This is a loose end. This API is defined in https://github.com/grpc/grpc/blob/master/doc/connectivity-semantics-and-api.md. But I am thinking to get rid of it since it is not very useful and extremely hard to implement properly for a complex picker (e.g., load balancing picker). \n. yup. done in #393 \n. len(...) returns int so that there is no overflow issue actually when length is int type on both 32 and 64 bit machines. Simply there is a compiling error when go 1.3 is used for the line:\nif length > math.MaxUint32 \nI guess Go 1.3 tries to convert math.MaxUint32 to int but the later Go version does not.\n. This is intentionally. Please keep uint (note that len(...) returns int). uint32 makes the overflow checking in line#177 void.\n. starts the comment with \"PeerFromContext\". So probably\n// PeerFromContext retrieves the peer information from ctx.\n. \"transport\" is an internal package which is supposedly used by grpc internal packages only (e.g., grpc package) instead of the library users. Is it possible to put \"Peer\" into grpc package?\n. understood. Let me think about it. Let service handler call transport.PeerFromContext is unpleasant...\n. okay, it seems it requires making changes to several code pieces to make this happen. Do you mind closing this PR and leaving this to me? I am trying to get it done by this Wed.\n. serveConnection --> serveStreams?\n. why do we need to skip verify when using -use_http mode?\n. Add some comments to this struct?\n. I do not understand the comment. What is \"Addr\"?\n. \"channel of net.Conn or error\"?\n. Since hl.authenticateConn(rawConn) is running in a separate goroutine, this could run into out-of-order issue:\nat iteration N: a valid rawConn is generated and h1.authenticateConn is scheduled;\nat iteration N+1: an error is returned by hl.Listener.Accept\nBut line#410 and line#432 could be executed in any order. Assume line#410 in the iteration N+1 gets executed first, then the \"conn\" in line#432 (iteration N) could be leaked -- the error got read and line#432 is executed (conn is written into hl.acceptc). But nobody will read from hl.acceptc again (http accept loop exits due to the error).\n. I think handlerListener is complex enough and worth doing some unit tests.\n. I would prefer the same treatment like s.conns. It does not matter how long it takes Stop to return as long as it does not hold the lock.\nWe do need a graceful shutdown (issue#147). But Stop() is not a method for testing only. Some service providers may want to stop a grpc server immediately but they do not want to crash/exit the entire process (there are other modules working for other purposes).\n. Comment is required.\n. why throw away unix + tls?\n. I think network and security are two orthogonal dimensions.\n. This should probably be placed before \"if *only != \"\".\n. in the description, mention this is only valid in non-windows platform?\n. t0 -> t?\n. checking error code is not right way to differentiate client and server side errors. The common practice is that the server application need pack a bit more info into the error description (e.g., serialized error protobuf).\n. I think the goal of this example is to demonstrate how to set and read error details. I am wondering if involving metadata handling makes it over complicated.\nBTW, you need mutex to guard the access of s.count. It is racy if you have multiple concurrent clients.\n. I see. Then I need think through how this can be achieved in a nice way.\n. done.\n. @bradfitz \nI observed some strange behavior here from http2 package. In some end2end tests (e.g., TestPingPong_4), this branch is entered when err is non-nil with \"stream error: stream ID 0; NO_ERROR\" and its type is not http2.StreamError. And the expected status in the test is OK (i.e., io.EOF). Here I forced it return a success (i.e., io.EOF). But this sounds strange to me and I am not sure it is the right thing to do. Can you take a look?\n. The TestRPCTimeout-4 failure (https://travis-ci.org/grpc/grpc-go/builds/108934236) is before this change. The problem is that recvMsg only expects transport.ConnectionError and transport.StreamError from the transport layer.\nWith this change, it is not clear to me what kind of errors can be returned by Body.Read so that I am not sure how to handle them correctly.\n. BTW, the current change makes the test passed (so that you cannot reproduce) but again I am not sure my error handling code is correct (e.g., could Body.Read returns a http2.ConnectionError besides http2.StreamError).\n. why is this changed to the pointer receiver?\n. done.\n. \"can be read like a normal map.\"\n. so that users can attach multiple values using a single key.\nAnd probably you can make an example for this case.\n. make this clear that it is for keys not values.\n. s/client/server\n. change to \"###\"?\n. change to \"##\"?\n. perhaps just leave \"md := metadata.FromContext(ctx)\".\n. remove \"RPC\"\n. remove \"RPC\".\n. move \"receiving metadata\" before \"Sending metadata\" since it is more common on the server side.\n. this is an empty struct so that the value receiver does have advantage here.\n. I do not see the point to make this change. What is the benefit here?\n. remove \"most of the time\" and add \"also\" to \"Users can also call ....\".\n. ditto\n. I am still not convinced we should do this. According to golang faq,\n\"For types such as basic types, slices, and small structs, a value receiver is very cheap so unless the semantics of the method requires a pointer, a value receiver is efficient and clear.\"\nActually I am thinking we should change the caller's code instead. Anyways, this is not the fast path and I do not care much.\n. why? the next iteration will do anyways (line 319).\n. buf[:n]?\n. ugh, I misread.\n. It is relevant because we are constructing a samll struct here and try to decide it should be value receiver or pointer receiver now. How a struct is defined should NOT depend on its caller's behavior.\n. I feel non-sense to make this change. Can you revert this part?\n. unexported\n. Currently s/Bakcoff/backoff/ to match the real method name.\n. s/Factor/factor\n. The name is confusing. Maybe use \"backoff\"?\n. yup\n. This is the only remaining issue. The others look good. I will merge once you fix this link.\n. what does \"Ptr\" mean?\n. remove all \"Ptr\"s\n. const (\n  emptyUnary testCaseType = iota\n  largeUnary\n  clientStreaming\n  ...\n)\n. All these are awkward. I think you can define \ntype testCase struct {\n  name string\n  weight int\n}\nThen build a slice of testCase from testCaseString. You do not need to have map for fast accessing. The slice size is very small\n. then this is not needed.\n. s/temp/testCase/g?\n. this should not happen. use panic.\n. rewrite this. what is gauge? what does it do?\n. map[string]gauge?\n. add the license to all the files.\n. createGauge\n. you can panic here directly to simplify the function since you log.Fatalf on the caller side anyways.\n. remove.\n. The current impl works but is somewhat weird. I propose the following:\nWithMaxBackoffDelay(d time.Duration) DialOption {\n  b := DefaultBackoffConfig\n  if d > 0 {\n    b.MaxDealy = d\n  }\n  return withbackoff(&b)\n}\nfunc WithBackoffConfig(b *BackoffConfig) DialOption {\n  return WithMaxDelay(b.MaxDelay)\n}\n. What do u mean by \"partial\"? This proposal is equivalent to what you did in this PR.\n. panic directly.\n. panic directly\n. panic directly\n. period at the end.\n. remove the comment.\n. if err := ....; err != nil {\n  ...\n}\n. I see WithMaxBackoffDelay could be helpful and used by most of users who want to tune this behavior. I do see it adds an additional exported function. I am fine with having WithBackoffConfig only. But I feel the following code is probably better:\nfunc WithBackoffConfig(b *BackoffConfig) DialOption {\nd := DefaultBackoffConfig\nif b.MaxDelay > 0 {\nd.MaxDealy = b.MaxDelay\n}\nreturn withbackoff(&d)\n}\nAnd please describe this behavior somehow in the comments.\n. constructs\n. do we need this?\n. s/mainLoop/run\n. startTime\n. This could be stuck forever if c.stop is read in line 251. Make done with buffer 1.\n. remove?\n. s/buf/name/g\n. period at the end.\n. period at the end.\n. I think DefaultBackoffConfig should be a constant instead of var (in backoff.go). It should not be mutable. Also making it a pointer sounds weird to me.\n. var (\n...\n)\n. v := gaugeResponse.GetLongValue()\noverallQPS += v\nif !totalOnly {\n  grpclog.Printf(...)\n}\n. change ok to !ok to reduce one indent.\n. *metricServerAddress == \"\"\n. change \"<\" to \"!=\". \n. Histogram accumulates values in the form of a histogram with exponentially increased bucket sizes.\n. v := float64(value)\n. such as RPC latency.\n. delta := 1.0\nh.buckets[0].lowBound = ...\nfor i := 1; i < opts.NumBuckets; i++ {\n ...\n}\n. add period at the end.\n. remove \"which ....\".\n. remove this line.\n. not need. 0 is the default value.\n. s/deltaValue/delta/g\n. can u put the math formula in the comment so that ppl can understand it easily?\n. it would be good to let the caller to specify all options.\n. returning the exact address the server is listening on would be more robust.\n. sounds a bad name. What do you mean by \"generic\"?\n. again returning the full addr would be better.\n. Using reqSize and respSize as the input params to a StartXXXServer function is weird. It would be better:\nfunc StartGenericServer(addr string, srv *genericTestServer, opts ...grpc.ServerOption) (string, func())\n. return tc.UnaryCall(...) directly?\n. return err directly.\n. return err directly.\n. io.EOF should be the valid non-nil error here?\n. return err directly?\n. ditto\n. 2016\n. 2016\n. 2016\n. 2016\n. 2016\n. comment\n. s/setup/config/\n. s/startBenchmarkClientWithSetup/newBenchmarkClient/\n. why is it a rpcErr? This is not an rpc...\n. Can we simply panic if there is anything wrong instead of returning an error?\n. replace the above with:\nif config.CoreLimit > 1 {\n  runtime.GoMAXPROCS(config.CoreLimit)\n}\n. var (\n  payloadReqSize, payloadRespSize int\n  payloadType string\n)\n. remove the relevant code if it is NOT supported yet.\n. \"Generic\" is misleading. please rename.\n. remove. and add a comment to add more dist above the line:\nswitch lp := setup.LoadParams.Load.(type).\n. remove?\n. default? this is the port for the benchmark server instead of the default.\n. this seems wrong.\n. b, ok := v.([]byte)\nif !ok {\n  return nil, fmt.Errorf(...)\n}\nreturn b, nil\n. s/Abs/abs\n. s/Abs/abs\n. s/doCloseLoopUnaryBenchmkar/doCloseLoopUnary since \"benchmarkClient\" already contains \"benchmark\".\n. ditto for streaming case.\n. move \"done\" out of the loop.\n. you need to read on bc.stop too so that if bc.stop is readable this goroutine will terminate.\n. ditto\n. move the warmup into the iteration in the following. The only difference is that you do not insert warmup results into the histogram.\n. ditto for the warmup.\n. ditto\n. comment.\n. ditto\n. ditto\n. port := int(config.Port)\nif port == 0 {\n  port = serverPort\n}\n. closeFunc\n. do not capitalize the 1st letter of the error msg.\n. ditto\n. bytebuffer\n. comment\n. check bs above this line. Then you do not need newbs any more.\n. ditto\n. add a description for this server.\n. comment all the exported fields.\n. \"on the given ServerInfo\" --> \"info\"\n. according to info\n. return fmt.Errorf(\"/BenchmarkService/UnaryCall(_, _) = _, %v, want _, <nil>\", err)\n. return fmt.Errorf(\"/BenchmarkService/StreamingCall.Send(_) = %v, want <nil>\", err)\n. ditto\n. ditto\n. ditto\n. add a comment to clarify this function.\n. The purpose is to benchmark the gRPC performance without protobuf serialization/deserialization overhead.\n. Remove this.\n. ServerInfo contains the information to create a gRPC benchmark server.\n. remove this sentence. It is impl detail.\n. panic?\n. panic so that we do not \"error\" as part of return value?\n. \"should be\" ==> \"is\".\n. using a custom codec for byte buffer\n. ditto\n. switch rpcType := config.RpcType {\n...\n}\n. you can move line 157 - 173 above the switch on line 145 so that you do not need to switch twice.\n. Some reconstruction hints:\ni)  place all the config printf into a function (e.g., printConfig);\nii) wrap the processing of all env related config into a func;\niii) wrap the connection setup and teardown into 2 funcs;\niv) wrap the rpc sending and receiving into another func.\nfunc startBenchmarkClient(config ...) (*benchmarkClient, error) {\n  printConfig(config)\n  setupEnv(config)\n  conns := setupConns(config)\n  defer closeConns(conns)\n  performRPCs(config, conns)\n}\n. add some comment on returned values (especially the 2nd one).\n. \"Use byeBufCodec if it is required.\"\n. \"connections\"\n. \"Close all connections if performRPCs failed.\"\n. remove?\n. remove?\n. s.stop <-true\n. It returns the connections and corresponding function to close them. It returns non-nil error if there is anything wrong.\n. fix the comments?\n. Add a TODO why we are doing this.\n. TODO: Revisit this for the optimal default setup.\n. bc := &benchmarkClient{\n  ...\n}\nthen the following can use bc directly.\n. if err := performRPC(...); err != nil {\n  ...\n}\n. There is no way to hit that case deterministically. But if you run this test many times, it may hit the line and trigger the problem if there is not this fix.\n. As you already pointed out, if the entire connection is down, we do not need to do this. It is not a special case but a common practice across the code base for better efficiency.\n. This is almost the only way to test the tricky race condition and something similar like this because there is no deterministic way to reproduce that. Typically, I run the tests a thousand times to detect any flakeness before submitting.\n. wait generates StreamError/ConnectionError/nil. No exceptions.\n. HistogramBucket represents a bucket in the histogram.\n. [HistogramOptions.MinValue, HistogramOptions.MinValue + n) and move this to the comments of HistogramOptions.\n. u need to make the receiver consistent. You need to use pointer receiver for consistency.\n. Merge merges h2 into h.\n. can you do \nhs   []lockingHistogram\nwhere\ntype lockingHistogram {\n  mu  sync.Mutex\n  h     *stats.Histogram\n}\n. make([]lockingHistogram, rpcCountPerConn_len(conns), rpcCountPerConn_len(conns))\nspecify the cap to eliminate unnecessary memory footprint.\n. make these 3 lines as a member function lockingHistogram.\n. s/The listener/lis\n. %v for error print\n. to test list has been closed, you can simply close lis again and expect the \"use of closed network connection\" error.\n. s/isWhitelistedHttp2Header/isWhitelistedPseudoHeader/\n. How comes \"Authority\"? There is no such thing.\n. @zellyn Notify() is called repeatedly in a goroutine spawned by grpc internals. Address slice is written when Balancer wants to notify grpc internals address changes which could be driven by other party (e.g., the name resolver or remote load balancer) or itself. The addresses cannot be duplicated (i.e., grpc internals would ignore duplication.). But keep in mind that Address is a struct so that you can make them different even though they have same ip:port string.\nYou can implement ur custom logc in down() handler to count the errors to the addresses.\n. On Fri, May 27, 2016 at 7:24 AM, Zellyn Hunter notifications@github.com\nwrote:\n\nIn balancer.go\nhttps://github.com/grpc/grpc-go/pull/690#discussion_r64911998:\n\n\n// cancellation of ctx when blocking. If opts.BlockingWait is false (for fail-fast\n// RPCs), it should return an address it has notified via Notify(...) immediately\n// instead of blocking.\n//\n// The function returns put which is called once the rpc has completed or failed.\n// put can collect and report RPC stats to a remote load balancer.\nGet(ctx context.Context, opts BalancerGetOptions) (addr Address, put func(), err error)\n// Notify notifies gRPC internals the list of Address to be connected. The list\n// may be from a name resolver or remote load balancer. gRPC internals will\n// compare it with the existing connected addresses. If the address Balancer\n// notified is not in the existing connected addresses, gRPC starts to connect\n// the address. If an address in the existing connected addresses is not in\n// the notification list, the corresponding connection is shutdown gracefully.\n// Otherwise, there are no operations to take. Note that this function must\n// return the full list of the Addresses which should be connected. It is NOT delta.\nNotify() <-chan []Address\n\n\nCalled repeatedly? So, on the first call, you return a chan that you'll\nwrite to whenever the Balancer wants to notify grpc internals of changes.\nOn the second call, you return a different chan that\u2026? Or are you expected\nto write to the first chan once, then close it?\nBy \"called repeatedly\", I meant something like\n\nfor {\n  addrs, ok := <-lb.Notify()\n  if !ok {\n    break\n  }\n}\nWhether it returns the same chan or not is implementation detail. I believe\nmost of impls should return the same chan always (e.g., roundRobin Balancer\nI made).\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/690/files/db887c9bcd224a919fae77c63283655504aa2fcd#r64911998,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AJpyLDwyvyW5W5EcwrVr1YIgPvpl-H9Cks5qFv6DgaJpZM4Ih1rh\n.\n. On Fri, May 27, 2016 at 9:09 AM, Zellyn Hunter notifications@github.com\nwrote:\nIn balancer.go\nhttps://github.com/grpc/grpc-go/pull/690#discussion_r64928582:\n\n\n// cancellation of ctx when blocking. If opts.BlockingWait is false (for fail-fast\n// RPCs), it should return an address it has notified via Notify(...) immediately\n// instead of blocking.\n//\n// The function returns put which is called once the rpc has completed or failed.\n// put can collect and report RPC stats to a remote load balancer.\nGet(ctx context.Context, opts BalancerGetOptions) (addr Address, put func(), err error)\n// Notify notifies gRPC internals the list of Address to be connected. The list\n// may be from a name resolver or remote load balancer. gRPC internals will\n// compare it with the existing connected addresses. If the address Balancer\n// notified is not in the existing connected addresses, gRPC starts to connect\n// the address. If an address in the existing connected addresses is not in\n// the notification list, the corresponding connection is shutdown gracefully.\n// Otherwise, there are no operations to take. Note that this function must\n// return the full list of the Addresses which should be connected. It is NOT delta.\nNotify() <-chan []Address\n\n\nWhy not this?\nfor addrs := range lb.Notify() {\n}\nThis is fine. I think pinning Notify to a channel is a reasonable\nrequirement.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/690/files/db887c9bcd224a919fae77c63283655504aa2fcd#r64928582,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AJpyLJTddoGKYiUEaY8VhDSslpUG3VDgks5qFxc3gaJpZM4Ih1rh\n.\n. I tend to leave watchAddrUpdates for a single batch of updates. This pattern usually has better flexibility.\n. The current errCredentialsMisuse does not match here. You need to revise the error msg and comments.\n. PerRPCCredentials defines the common interface for the credentials which need to attach security information to every RPC (e.g., oauth2).\n. rename serverstatus\n. ok, keep \"reflection\". ignore my previous comment.\n. the server reflection service\n. s/InstallOnServer/Install? Or Register?\n. elaborate the comment. Why do we need this?\n. return s.decodeFileDesc(enc)\n. let it return ([]byte, error).\n. remove this? BTW I think the above are also the help functions?\n. return s.fileDescForType(extT)?\n. out := make([]int32, 0, len(exts))\nfor i := range exts {\n  out = append(out, i)\n}\n. WireFormat -> Encoding\n. ditto\n. rename it to ServiceMetadata.\n. this format is wrong.\n. make([]string, len(s.m), len(s.m))\n. ... returns all the registered service names.\n. s/meta/metadata/\n. \"metadata\" as a member var means this is the metadata of this service. I believe nobody like inconsistent, long and inaccurate names. If you think \"metadata\" is not good, please make your proposal instead.\n. I am not a native speaker. But to me \"meta\" is a prefix in English. I do prefer a noun instead of a language prefix here. And I do not think \"metadata\" is long.\n. Calling it Metadata is misleading because it does NOT return the metadata of a grpc server. Instead it returns the metadata of a service registered on the server.\n. @puellanivis If you search \"Metadata\" in golang, you can also find tons of places which use Metadata (e.g., https://github.com/golang/crypto/blob/c197bcf24cde29d3f73c7b4ac6fd41f4384e8af6/ssh/connection.go#L24). The same thing happens in google internal go RPC lib when this concept is needed. This is really a non-issue... To me, meta is fine and I prefer metadata.\n. if pos := strings.LastIndex(...); pos != -1 {\n  ...\n}\n. if fd == nil {\n  return nil, fmt.Errorf(\"unknown symbol: %v\", name)\n}\nreturn proto.Marshal(fd)\n. all done. \n. the comment sounds pointless... maybe delete it.\n. marshalled\n. we only have metadata for a service so that this is not accurate. method is mostly for sanity check. Once the caller inputs a method, this function would check if the input method is a method of the service. \n. is it better by adding an additional return value for error when the following ok is false? It seems clearer in my opinion.\n. maybe also state that ServiceMetadata and AllServiceNames are added mostly for service reflection support. \n. it will be only called once.\n. comment\n. comment\n. comment on whether this is the full method name (package + service + method) or method only.\n. from a service name\n. Please do not change the API. You can use \"f\" to create a net.Dialer and assign to o.copts. \n. respStatus := &testpb.EchoStatus{\n  Code: proto.Int32(2),\n  Message: proto.String(\"test status message\",\n}\n. if _, err := tc.UnaryCall(...); grpc.Code(err) != codes.Code(code) {\n   ...\n}\n. I think the above can be better represented as:\n\nexpectedErr := grpc.Errorf(code, msg)\nif , err := tc.UnaryCall(...); err == nil || err.Error() != expectedErr.Error() {\n   grpclog.Fatalf(\"%v.UnaryCall(, ) = , %v, want %v\", err, expectedErr)\n}\n. grpclog.Fatalf(\"%v.FullDuplexCall(_) = _, %v, want <nil>\", tc, err)\n. ditto -- add \"want ...\"\n. if err := stream.CloseSend(); err != nil {\n  ...\n}\n. same as the unary case.\n. I might have thought about something else you did not intend to tackle here. To be clear, what goroutine is leaked? Is it a real leak or just the stress is too intensive to get that goroutine eliminated in time? \n. github hid part of my comments. Please take a look at my original comment again. In idiomatic golang test, we typically add want <nil> if this function call does not expect any error.\n. want <nil>\n. this func can be simply\n`return grpc.Code(l) == grpc.Code(r) && grpc.ErrorDesc(l) == grpc.ErrorDesc(r)``\n. move this test to rpc_util_test.go. This is not an end2end test.\n. MethodName?\n. merge the info for unary and stremaing like\nUnary []MethodInfo\nStream []MethodInfo\nFor unary rpc, both IsClientStream and IsServerStream are false.\n. Search the method name in info.Methods.\n. MethodInfo contains the information of an RPC including its method name and type.\n. seems the comment is not complete? can you complete the sentence?\n. WithDialer(net.Dialer) is not sufficient to complete the goal in some cases, e.g.,\n1. https://github.com/grpc/grpc-go/issues/639;\n2. ppl use https://github.com/golang/appengine/blob/master/socket/socket_vm.go#L32 to dial to appengine;\n3. We may introduce in-process server later which cannot be dialed with a net.Dialer either.\n. Please do not expose transport package to grpc users. We intend to make transport as a private package which is only used by grpc package.\n. I meant this needs to be \nfunc WithDialer(f func(string, time.Duration, <-chan struct{}) (net.Conn, error)) DialOption\nI understood this introduces a bit code duplication but the callers should not know existence of the transport package.\n. I would suggest that still keeping \"delete\" into this function (use a input bool param to control it), which makes the code easier to maintain.\n. Removing shutdownChan seems hurt the code readability in my opinion -- letting addrConn work with a transport layer connection option sounds strange when reading the code. I would like to keep shutdownChan and assign it to ac.dopts.copts.Cancel before we call transport.NewClientTransport(...). The point is to operate on connection options only when we want to create a transport.\n. I do not think this needs to be exported.\n. I do not think this needs to be exported.\n. you can write these functions in the tests instead of exporting them. Transport is only used by grpc package. internal package is just used to host some test only methods.\n. only non-test usage (i.e., newHTTP2Client) is in transport package -- you do not need to export it. \nTransport package is bit in the grey area -- we may allow some proxy to use it directly. \n. 1. make these function unexported;\n2. yes, write the function you want to use in the tests. end2end tests should behave like how a normal user uses grpc (notice this user does not know transport package).\n. This is pretty similar to what I was going to suggest (which was not going out due to your update). :) Thanks. I will have a close look tomorrow (I try to see if it is possible not to export new api).\n. in the test, you call this to make a custom dialer (for unix). So this is not only for constructing the default dialer.\n. Any reason this is not exported? All the other fields are exported. If it is exported, it seems we do not need ConnectOptions.WithDialer(...).\n. > It's left unexported to encourage users to use the Dial method rather\n\nthan accessing this directly. That method encapsulates usage of the default.\n\nThe only \"user\" is newHTTP2Client(...). This is not a public facing API -- we only need to make the only caller correct. \n. It sounds strange to have a Dial function for an options object. Probably make it something like:\nfunc Dial(copts ConnetionOptions, addr, string, timeout ....) (net.Conn, error) {\n   ...\n}\n. maybe reordering  copts and addr  would be better.\n. sounds it would be better if we call this function \"encodeGrpcMessage\"?\n. We should probably change it to encodeTimeout too. :)\n. s/tearDown/Tear down/\nand remove the coma at the end of the line.\n. The above block is not need because t.Close is called in line#277.\n. this should be as same as line#262-277.\n. How about\ns/origErr/err\ns/OriginalError/Origin\n. revert this? This has been checked in by another pr.\n. no need to check e.Temporary()\n. I haven't look the following. But why did u delete this? This is defined at https://github.com/grpc/grpc/blob/master/doc/connection-backoff.md.\n. put this into \"testdeps\" so that we do not need to repeat it in both test and testrace?\n. Actually I got some permission deny error (it may need root?) when \"make testrace\" on my desktop after adding that line. This will impact our users when they make locally. Can you take a look?\n. It does not fix ... Even if it fixes, it requires a lot of users doing that, which is not good. Perhaps it is not worth doing that.\n. I saw line#586 (before this change) sets it to copts and then copts is passed transport.NewClientTransport.\n. My GOPATH is under my home directory. If your guess is true, your proposal should fix the problem. But it does not.\n. I did a bit investigation. It seems due to some google specific golang setup. But at very least, this will make me unable to \"make\" when I work. :)\n. and other googlers :)\n. Thanks. We can if we do google-internal projects (blaze takes care of everything). We cannot if we use google machines for oss (I believe there are workarounds but I am not willing to teach every single googler that workaround when they meet this problem.). :) \n. when t.Close() is called, we cannot hold lock. I still use switch as u suggested but it does not save any code now. :)\n. not really. ur suggestion is incorrect because unlock is required for t.Close().\n. add some comments about context here so that the author of some custom credentials knows how to handle it?\n. cc.ctx, cc.cancel = context.WithCancel(context.Background())\nso that line#219 is not needed.\n. Your change makes ClientHandshake terminates when ctx is cancelled. But This makes the goroutine created in line 151 leak if conn.Handshake() stucks. It seems this does not really resolve the issue?\n. wait for go1.7 to resolve this issue? the main theme of this PR is to plumb the context in?\n. In that case, can you revise the title of this pull request?\n. It is more general than credentials.\n. ok. done :)\n. cc.mu.Lock()\n. s/ac.cc.conns/cc.conns\n. s/loop/iteration\n. some nit:\nchange \"<=\" to \"<\" so that \"maxRetries\" semantics is consistent across these 2 tests?\n. This is for go1.7. This is the reason you saw the errors on travis. Please use \"golang.org/x/net/context\".\n. we just merged a conflicting change. Please rebase.\n. if err := ...; err == nil {\n  ...\n}\n. Please add some comments.\n. Please still leave minConnectTimeout in grpc package. You only need to add a function in \"internal\" to set its value in grpc's init.\n. why is it needed?\n. In the \"internal\" package, add a var:\nvar TestingSetMinConnectTimeout func(time.Duration)\nThen in the clientconn.go, add\nfunc init() {\n    internal.TestingCloseConns = func(t time.Duration) {\n        minConnectTimeout = t\n    }\n}\nThen in the test, you can call\ninternal.TestSetMinConnectTimeout(time.Second)\nto set it to 1 second.\n. What if this is deadline exceeded instead of canceled? It seems we need to refine the DialContext behavior better.\n. ErrConnDispatched indicates that rawConn has been dispatched out of gRPC and the caller should not close rawConn.\n. credentials: rawConn is dispatched out of gRPC\n. If there are only close and read ops on this chan, we typically use chan struct{} instead.\n. getRawConn\n. why is this needed?\n. grpc.Dial\n. write might be better here (because it does not need to send an rpc in line#2396).\n. This might make the returned error inconsistent. I propose:\nif err != nil {\n  cc.Close()\n}\nselect {\n  case <-ctx.Done():\n    err = ctx.Err()\n  default:\n  }\n. \"else\" is not needed.\n. this will return a non-nil *ClientConn  and non-nil error. \n. ServerName is the user-configured server name.\n. \"%v.authority ...\", conn, ....\n. grpc.Dial ---> Dial\n. done\n. done.\n. The code always checks b.done before checking b.waitCh now.\n. done\n. done\n. done\n. this is just a comment of an unexported function :)\n. done\n. OverrideServer overrides the server name used to verify the hostname on the returned certificates from the server. gRPC internals also use it to override the virtual hosting name if it is set. It must be called before dialing. Currently, this is only used by grpclb.\n. s/resp/status/g? \"resp\" is misleading.\n. why do we call in.GetResponseStatus twice in this func?\n. can we simply use codes.Unknown directly in line#461 and 463?\n. if _, err := ....; err == nil || err.Error() != expectedError() {\n. perhaps add grpc.FailFast(false) as the call option to prevent network hiccup fails the test?\n. set grpc.FailFast(false)?\n. multipleSetTrailer\n. const (\n  argSize =  1\n  respSize = 1\n)\n. set it to non-failfast?\n. We should avoid calling ServerTransport.Drain() multiple times in Line#836. We should move line#830 below line#837 and add the if branch between line#834 and line#835 like:\nif !s.drain {\n  for c:= range s.conns {\n      c.(transport.ServerTransport).Drain()\n  }\n  s.drain = true\n}\n. sleeping is not desirable since it cannot be interrupted by the server stop signal. We should avoid it.\n. Now this should send out all the existing MD instead of returning.\n. Can we avoid this copy? s.header seems read-only after this.\n. race with t.WriteHeader?\n. avoid copying if md is nil?\n. s/withHeader/hasHeader?\n. s/call/called\n. move this after checking \"s.headerOk || s.state == streamDone\n. s/call/called\n. Serve returns when lis.Accept fails with fatal errors.\n. This is not necessary to be net.Error. It could be any error which implements Temporary() interface. https://github.com/grpc/grpc-go/blob/ac7efbd8beb843cdb95d4bd275f0482729c6ad59/transport/http2_client.go#L130 can serve as an example.\n. var (\n   utimeDiffs = latest.Utime.Sec - first.Utime.Sec\n   utimeDiffus = ...\n   ...\n)\n elapseUtime := ...\n elapseStime := ...\n return elapaseUtime, elapseStime\n}\n. elapsedUserCPU, elapsedSystemCPU := ...\n. return &benchmarkServer{\n  port: p,\n  cores: ...,\n  ...\n}, nil\n. add a link to CLI\n. s/the change needed is/we need to make the following changes/\n. s/test/check\nthe last sentence should be replaced by \n\"gRPC CLI is only available in C++.\" \nthen give a link including the corresponding instructions on installation and execution (are there?).\n. ok, move the above to the place I suggested.\n. from 48 to 55, it should be\nRun a hello world server\n$ go run examples/helloworld/greeter_server/main.go\nother things are not needed.\n. Use t.Errorf\n. t.Errorf\n. print lis.Addr().String() instead of \"local server\".\n. The above comments apply to this test case too.\n. if err := <-done; err != nil {\n  return\n}\n. .... populated the server AuthInfo or failed.\n. make(chan error, 1) so that this goroutine can finish even though the test fails at line 101.\n. %s for list.Addr().String()\n. Use tls.ConnectionState directly?\n. use tls.ConnectionState directly?\n. The body can be written in a separate function like:\nfunc serverHandle(hs func(net.Conn) (net.Conn, AuthInfo, error)) {\n    ...\n}\n. and the above client side logic can be wrapped into a function too:\nfunc clientHandle(hs func(context.Context, string, net.Conn) (net.Conn, AuthInfo, error))\n. s/outgoingPayloadStats/outStats/\n. s/incomingPayloadStats/inStats/\n. change \"outgoing/Outgoing\" to \"out/Out\" and \"incoming/Incoming\" to \"in/In\" in all the places.\n. Package stats is for collecting and reporting various network and RPC stats.\n. name it RPCStats since you will add connection stats later?\n. rename it \"InPayload\" since it is already in stats package?\n. RecvTime?\n. InHeader?\n. InTrailer\n. OutPayload\n. OutHeader\n. OutTrailer\n. RPCErr\n. add something to prevent it from calling multiple times?\n. should we export this too?\n. an incoming\n. IsClient is true if this RPCStats is from client side.\n. Client is true if this InPayload is from client side.\n. IsClient indicates if this is from client side.\n. InHeader contains stats when a header is received.\n. why is there a newline ?\n. ditto as above\n. same\n. same\n. header?\n. same\n. an\n. ditto\n. same\n. ditto\n. ditto\n. FullMethod?\n. FailFast\n. ditto\n. ditto\n. ditto\n. s/header/trailer\n. ditto\n. ditto\n. ditto\n. ditto\n. Better use CompareAndSwapInt32.\n. Handle returns the function registered to process the stats.\n. Start starts the stats collection and reporting if there is a registered stats handle.\n. Stop does not\n. Stop stops the stats collection and processing.\n. inPayload?\n. deterministic?\n. put \"inStats != nil\" in the beginning of the condition.\n. outPayload?\n. ctx is used for stats collection and processing. It is the context passed from the application.\n. These 2 APIs are not needed.\n. should we check the value of \"err\"? what if it is not nil?\n. Is it okay to put fail fast into stats.Begin so that we do not need to pass it into the transport layer? You can refactor NewClientStream and newClientStream somehow (e.g., move opt processing to NewClientStream).\n. what if err == nil?\n. can we put fail fast into Begin stats so that it is not needed to pass it into transport?\n. add a TODO to truncate the large payload. Also for outPayout.\n. should this happen after write returns?\n. what if err is not nil?\n. consider the error case\n. remove this?\n. The reason for this change?\n. Can we move this part (from line 736-754 to the caller of processStreamingRPC and processUnaryRPC) since they are common for both cases?\n. You may need to emphasize this kind of fields are immutable. May apply to all the fields?\n. Data is the serialized message payload?\n. Encryption is the encryption algorithm used for RPCs.\n. All stats collection should use the user context (instead of the stream context) so that all the generated stats for a particular RPC can be associated in the processing phase.\n. I would suggest renaming it to \"statsCtx\" because \"userCtx\" is not appropriate name on server side.\n. You need to revise the previous comment correspondingly.\n. TODO: Investigate how to signal the stats handling party.\n. if stats.On() {\n  end := &stats.End{\n    Client: true,\n    EndTime. time.Now(),\n  }\n  if err != nil && err != io.EOF {\n    end.Error = toRPCErr(err)\n  }\n}\n. There is a chance the t.conn is broken when these are called so that they cannot get the values. You should probably cache them when the connection is setup.\n. not  a unary request since it is also used in FullDuplexCall. \n. ditto\n. use errorID+1 instead?\n. capitalize the 1st letter of the second word in the strings.\n. Is there a way we can verify the contexts from the stats of a particular rpc are same?\n. Due the latest change, it seems this function can be simplified as:\ntr, ok := trace.FromContext(stream.Context())\nif !ok {\n  return nil\n}\n...\n. you can t.Fatalf directly so that this function only needs to return lis.Addr().String().\n. use t.Fatalf and this function only returns tls.ConnectionState.\n. t.Fatalf\n. s/using/in\n. s/using/in\n. remove the comment.\n. remove the comment.\n. This is more like a channel to notify the completion of serverHandle. Can you rename it to \"done\"? Applies to all the tests you added.\n. gRPC internals check the Temporary() method of the returned error (if it is not nil). If there is no balancer installed and the error is temporary, grpc interanls redial. The dial is failed otherwise.\n. s/errorWithTemp/testErr\n. return \"test error\"\n. s/mdOk/ok\n. ditto\n. API\n. goroutine is one word.\n. make this a channel of authInfo. The way launchServer does (&serverConnState) is more like C/C++ way instead of Go idiom. In this way, serverHandshake does not need the second param.\n. don't use \"thread safe\".  have sth like\n\"md cannot be accessed by multiple goroutines  concurrently.\"\n. s/Context/Ctx\n. s/Context/Ctx\n. connHandler\n. the RPC stats\n. RPCTagger?\n. I would prefer RPCTagInfo actually.\n. I prefer ConnTagInfo.\n. ConnTagger?\n. RegisterRPCTagger?\n. ConnBegin has the stats of a connection when it is established.\n. ConnEnd has the states of a connection when it ends.\n. use the one passed by NewClientTransport instead?\n. s/concurrency issues/races?\n. \"done\" should be closed if there is an error.\nsAuthInfo, ok := <-done\nif !ok {\n    t.Fatalf(....)\n}\n. ditto\n. ditto\n. done should be closed when there is an error (e.g., line 142 and 147).\n. With this PR, I think we should be able to remove the \"add\" calls at line#607 and line#613. This applies to the server side too.. this is not a good name. how about \"TestFlowControlRace\"?. go test has timeout mechanism. You do not need to have your own here.. add grpc.WithBlock() to make the procedure more clear.. rpcHandler. HandleRPC. RegisterRPCHandler. \"... whether the stats collection and processing is on.. s/reporting/processing. s/user/the user. FullMethodName is the RPC method in the format of /package.service/method.. ... using the rpc handler registered by the user.. ... for RPC stats processing.. It should be called only once. The later call will overwrite the former value if it is called multiple times.. ditto. s/using/on. the handlers.. change the name too.. cc.dopts.sc reads an object instead of a pointer.. I do not think we should introduce an unnecessary dependency to grpc (i.e., json). Are there particular reasons you want grpc internals receives a json object and decode it instead of receiving a native struct object? If we really need json, I think it should be wrapped somewhere else instead of this place.\nYes, we will add that API to extract the service config.\nWith the API in this PR, it is very easy to inject a service config the grpc internals. We do not need any test-only API. Notice that this impl does NOT bind service config with name resolver, which is more extensible and flexible.. As the comment suggested, we will do even though I am not convinced we should provide this feature since it is error-prone and not necessary for almost all the users.. - wait_for_ready: overriding is done in line#124\n\ntimeout: this is handled by the standard context package already -- when there are 2 context.WithTimeout calles, the valid one is the mininal.. So I think you are talking about the related interprocess communications for service config. That is not the thing here -- the writer of this channel is inside the same process. In this way, we avoid the JSON dependency in grpc package. We always try to minimize the dependency of grpc (credentials are in another separate package in grpc-go). I will send u a cl for this JSON thing and other related stuffs.. done. done. done. fixed by adding a \"cancel\" in clientStream.. I am not convinced why this must be JSON exclusive. Why not give users the flexibility to use other formats they want? Probably we should get some voice from the users instead of making a decision for them?. Apparently using custom struct is a better way to deliver the signal to our users that grpc does not put any restrictions on the format you use to transmit service config. Plus, if you choose custom struct, you gain the benefit avoiding unnecessary conversions (yes, it is not approach killer but it is a benefit.). \n\nI do not understand why any benefit you mentioned in the 2nd paragrahp cannot be achieved with my approach. Again, the approach here does not exclude JSON impl and is a superset -- it just does not fuse the API with JSON.\nNot available today. will schedule some time to talk to you next Monday. Thanks.\n. for the related stats handling (e.g., RPCs, connections). replace the following with something like:\nThe returned context is used in the rest lifetime of the RPC.. can we define it as security-protocol agnostic -- it should return AuthInfo instead of tls.ConnectionState?. ditto. should be chan AuthInfo?. this applies to the rest of the tests. Please fix.. s/clientConnState/clientAuthInfo. s/serverConnState/serverAuthInfo. applies to the other tests.. return nil, err?. return nil, err?. ditto. ditto. ditto. ditto. if err := clientConn.Handshake(); err != nil {\n  ...\n}. ... for all the RPCs and underlying network connections in this ClientConn.. call this \"Stringer\" since the package name is grpclog already?. two problems here:\ni) Accept may return a nil serverRawConn when err is not nil;\nii) it may close the connection too quick so that the client has not read the response.. ",
    "menghanl": "This should have been fixed by #1064. Closing stale issue.. We are working on improving the logging system, #922 is the first step.\n. Can you share more information about your program? Like what the error you got was and what server were you connecting to? What we want to know is the root cause of the connection error.\nWe have backoff mechanism if the connection can't be established at the first place.\nOne possible situation where the flood can happen is that the connection is established successfully, and disconnects immediately after that. We should do something (backoff or even stop retrying) depending on the reason.\n. > The root cause of the error is exactly as you described\nI assume by this you mean \"connection is established successfully, and disconnects immediately after that.\"\nThen what is the reason for this to happen in your case? (The server is a misbehavior server? or a non-gRPC server?)\n. @ilius You can append content to context using interceptor: https://godoc.org/google.golang.org/grpc#UnaryInterceptor. @c4milo trace is still available.\nHave you tried this: https://github.com/grpc/grpc-go/pull/210#issuecomment-140599060?. @flyingmutant @pires fixed in PR #690 \n. We are working on new interface/abstractions between the grpc & transport packages to make it possible for custom transports (UDP could be one of them) to be implemented.. Closing stale issue. Please leave a comment if this is still valid.. Closing stale issue.. @Xiaoshuai I believe you can download the source code with this, but then you need to move the code to the right location to match the import path.\nPlease do go get google.golang.org/grpc if possible.. @ldelossa  cancel will result in context.Canceled error.\nAlso, there's no need to keep receiving if an error was already returned by Recv. So the continue inside if err != nil should be replaced with return or other error handling steps.. @varshanbp You can get Peer using function peer.FromContext.. @varshanbp Sorry I'm not clear about how to do this in Java. Please ask in the Java repo, or in grpc-io list.. @dustin-decker \nFor the question on using net.Conn in a grpc.Server:\nserver.Serve() takes a listener as input, you can make a listener whose Accept() returns the net.Conn and then blocks forever.. @hunnain For browser support, please take a look at https://github.com/grpc/grpc-web. > So the msg is not a nil interface{} here but the real object of msg is nil.\n@songshine Can you give an example for this? Or a reproduction of the error?. I think the fix should be\n - remove the Fatalf in server.go and return the error\nSo that the behavior will be, if the error returned by service handler is nil, the response will be passed to Codec to do Marshal, even if the response is nil.\nIn the case of using protobuf and return nil, nil, the marshal will still fail with Marshal called with nil, and the user will be notified of the error, but not a Fatalf.. @immesys This is a dependency management problem, not something that we can solve within gRPC.\nPlease look into the dependency management tool you use to see how to avoid double import.\nThe discussions in etcd repo (https://github.com/coreos/etcd/pull/9155, https://github.com/coreos/etcd/pull/9240) might help.. LGTM\n. All comments are fixed.\n. @rjarmstrong\n: is not allowed in metadata keys.\nAll allowed characters are defined in this doc. I also filed #1299 to update the doc in metadata package.. grpc/grpc#4672 is still not resolved. But since people are asking, let's do percent encoding now to fix the problem. We can change the encoding later when we have a conclusion.\n@peter-edge can you fix the comments and do a rebase? This PR is old... Thanks a lot.\n@inconshreveable sorry for the delay.\n. @peter-edge ping? We need to get this in recently.\n. All fixed. PTAL.\n. All fixed. PTAL.\n. @sreecha FYI stress test code has been merged.\n. @Ralphbupt The example code is a bit outdated.\nOne thing to note is that NewContext and FromContext are deprecated. Please see the doc and use the correct one.\nWe are planning to remove these two functions. See this gRFC and #1219 for details.\nAlso, have you read the metadata doc?\nAnd do you find the example code necessary in addition to doc?. Moved to #660 \n. All fixed. PTAL.\n. Moved to #677 \n. LGTM.\n. Thanks for the review. All fixed. PTAL.\n. Yes, the workers will stay alive.\nAnd right, I should set the default cpu number explicitly if it is not specified in config.\n. All fixed. PTAL.\n. I'm afraid we have to push this to post-GA. We are trying to avoid introducing more features now and stay in bug fixing mode for some time before GA. Sorry for the trouble.\n. @mwitkow here is the proto for server reflection:\nhttps://github.com/grpc/grpc/blob/master/src/proto/grpc/reflection/v1alpha/reflection.proto\n. @mwitkow @brian-brazil \nFYI, in #726, a new API GetServiceInfo() is added to grpc.Server, which can be used to get all registered services.\nBut we didn't find an appropriate way to pass the information to an interceptor. One possible workaround for you is to add an init function taking grpc.Server as an argument and do initialization, similar to reflection.Register().\n. As @JohanSJA mentioned, grpc.Code(err) should be able to give you the error code.\nI'm closing this issue now. Please reopen if it is not resolved. \n. moved to #755\n. It seems your protoc-gen-go is outdated.\nCan you do an update go get -u github.com/golang/protobuf/protoc-gen-go, and see if that works?\n. @jgrusewski  We cannot reproduce the issue with your example application.\nCould there be something wrong with your example?\nCan you check if you still see the leaks with your example?\n. The proxy is now part of the default dialer.\nIf you want to redefine or remove the proxy, you can override the dialer with a DialOption.. @nscavell Right, authorization is currently not supported. #1446 mentioned the same problem. PR #1447 was filed to fix it, but it was not finished.\nWe currently don't have cycles for this now. I proposed a solution in this comment https://github.com/grpc/grpc-go/pull/1447#issuecomment-324463380. So if any one has cycles and wants to send a PR, it will be great.. The change in proto library was reverted https://github.com/golang/protobuf/pull/326. @tamird moved to #720 \n. I think we should not regenerate all the .pb.go files before testing.\nThe .pb.go files are checked into the repo. If they are regenerated and overwritten, we are not doing test on the checked-in code.\nAnd it also introduces latency to the test.\n. > That's true, but the build will fail if the regenerated protos do not match the checked-in protos.\nIt's also possible that the build and tests pass with regenerated files but fail with checked-in files.\nFor example if we change the support package version number in codegen and rpc_util.go, but forget to update the generated files. The build should fail, but it will pass on travis because they are regenerated.\n. gRPC cannot do this check for you.\nI would suggest you do the check by yourself using map Day_WeekDay_name. Add a check for the incoming day.Day to your server code, and return an error if day.Day is not valid.\nAnother suggestion is to use consts defined by protobuf when constructing Day_WeekDay, like Day_Monday, instead of integers.\n. Add server.GetServiceInfo(), to replace server.ServiceMetadata() and server.AllServiceNames().\nPTAL.\n. cbuf is cleaned at https://github.com/grpc/grpc-go/blob/master/stream.go#L230\nIs SendMsg() called concurrently in your program? We currently don't support concurrent writers for stream.\n. SetTrailer will work after #863. Will have another PR for SendHeader.\n. Travis failed on go vet. Should be fixed in #839.\n. The go vet error on generated code cannot be fixed on our side...\nAn issue was already filed golang/protobuf#214\n. We can add this for go 1.7 since the problem with go vet and Errorf() will be fixed.\n. recover() works in the helloworld example, with the following handler:\nfunc (s *server) SayHello(ctx context.Context, in *pb.HelloRequest) (*pb.HelloReply, error) {\n    defer func() {\n        if err := recover(); err != nil {\n            fmt.Println(\"Recovered from err: \", err)\n        }\n    }()\n    panic(\"TISH IS A PANIC\")\n    return &pb.HelloReply{Message: \"Hello \" + in.Name}, nil\n}\nCan you try to remove some of the code in your handler and see if that will make any difference?\n. In #768 (to fix #622), I'm trying to make Dial with WithBlock return fatal connection error if any of the connection fails. With this change, the connection error will be hidden if one connection succeeds.\nWe need to have more thoughts on this.\n(BTW, this conflicts with #800, sorry for the trouble.)\n. Dial withblock tries to connect to the address(es) specified by user or balancer. If one of them fails and the error indicates this connection is not retry-able, an error will be returned.\nAfter Dial finishes, fatal connection errors can be returned when user is making an RPC, but this depends on whether the RPC is fail-fast and whether a load balancer is involved.\n. The purpose is to return fatal errors immediately. Users may want to know about this kind of errors (like tls error) so they can fix them.\nIf these errors are not returned by Dial, the only place users will know about this error is when making RPCs (or checking logs) , and this depends on RPC types and balancers.\n. @mwitkow FYI\n. @mwitkow Added StreamInfo for streaming RPCs, PTAL.\n. If I understand correctly, there are multiple servers, and the client is trying to send message to them. This send should succeed if one of the servers is working.\nIf you want to manage the connections on your own, one way is to create a list of connections, and then create a new gRPC client for each of this connections. Changing the underlying connection for a gRPC client is not supported.\ngo\nfunc (this *YourClient) DialServer(targetUrls []string, options grpc.DialOption) error {\n  var err error\n  for i, addr := range targetUrls {\n    this.Connections[i], err = grpc.Dial(targetUrl, options)\n    if err ... // check error\n    this.Clients[i] = yourpb.NewBrokerUrlContractClient(this.Connections[i])\n  }\n}\nAnother way to do this is to create your own name resolver.\n. Can you elaborate on what you mean here?\nProtoPackageIsVersion is generated by golang/protobuf, and should not be modified manually.\nAs far as I know, they didn't change this version recently. Did I miss something?\n. @onlyjob ProtoPackageIsVersion is golang proto lib version, not the proto compiler version.\n@sunwangme I believe the version you saw in rpc_util.go is SupportPackageIsVersion3 for gRPC generated code, not ProtoPackageIsVersion3. \nIf the code doesn't compile, please do an update and recompile your proto.\ngo get -u github.com/golang/protobuf/{proto,protoc-gen-go}\n. Did you recompile your proto files after update?\nCan you post the error message you got?\n. @ejona86 Please take a look.\n. fixed by https://github.com/grpc/grpc-go/pull/1027. @gyuho The benchmark worker in go repo is a testing server, the testing client is only available in c++, so it's a bit tricky to run.\nCan you try to run the local benchmark first, and see if you can find anything there?\nThe benchmarks are in https://github.com/grpc/grpc-go/blob/master/benchmark/benchmark_test.go.\ncd benchmark\ngo test -bench=.\n. @gyuho I just ran the local benchmark and the benchmark as in dashboard, and didn't see any performance regression. Please keep us posted if you find anything suspicious.\n. If you want to run benchmark using benchmark/worker, please try the following steps:\nmkdir test\ncd test\ngit clone https://github.com/grpc/grpc-go\ngit clone https://github.com/grpc/grpc\ncd grpc\ngit submodule update --init\nmake grpc_cpp_plugin\ntools/run_tests/run_performance_tests.py -l go\nThe grpc-go in dir test/grpc-go will be used to run the benchmark, not the one in your $GOPATH. So changes should be made to test/grpc-go if you want to run benchmark on different versions of grpc-go.\n. @gyuho Any update on this issue?\n. Need to refactor tearDown\nThe other two fixes are moved to #799 \n. ~~This change reverts 78e558b~~\nAdd function cloneTLSConfig, same as https://tip.golang.org/src/net/http/transport.go#L2014.\n. LGTM.\n. LGTM.\n. @iamqizhao \nIt seems this PR failed on TestClientWithMisbehavedServer, which should be fixed by #801.\nBut #801 failed on TestClientRequestBodyError_Cancel, seems related to this error.\nMaybe we should merge this even though it's failing...\n. We have local benchmarks in benchmark/benchmark_test.go. Run them by go test -bench=.\nMore benchmarks can be run using benchmark/worker, please refer to the comments here for how to run them: https://github.com/grpc/grpc-go/issues/771#issuecomment-234400243\n. Can you specify a timeout for the blocking RPC, so it will return deadline exceeded error?\nReusing dial timeout seems not to be a good idea. ac.wait should not exit on dial timeout.\nWith this change, the real timeout for each RPC becomes min(context-timeout, dial-timeout).\n. The issue with put seems to be a real one. I will have a PR for that.\n. @heyitsanthony If a connection error happens when client is writing or reading, the RPC (unary) will retry on other available connections.\nIt seems there is a race between balancer and gRPC internal. The connection is up when calling balancer.Get(), but down immediately after that before writing anything.\nWill think about how to resolve that.\n. Sorry I forgot to mention, I was talking about non-failfast RPCs. Failfast RPCs will fail on connection errors.\n. Yes, failfast RPCs (unary) will not retry when connection error happens.\n. @heyitsanthony What I'm thinking now is that maybe non-failfast RPCs should not wait (even for dial timeout) on the down-ed connection. It should restart immediately and get a new connection. I will try and see if that's reasonable. Will keep you informed.\n. I think this is already fixed.\nI will double check and get back to this.. I believe the original problem that RPCs block on downed connections has be resolved in #830.\nOn the discussions about retrying, we have a new implementation on retry this coming quarter.\nClosing this PR now.. @xiang90 @heyitsanthony \n. Will add tests for put().\n. Can you post the error message you got, and how the RPC was made?\n. context cancellation should have been handled for this. Maybe we missed some corner case.\nDo you have a reproduction for this?\n. Can you elaborate on what the issue is?\n. Did you encounter any problem caused by this?\nBy design, s.m should be read only after server.start, so this lock is not necessary here.\n. I don't recall any label in Server indicating whether the server has started. Not sure how easy it is. Please add the protection if you find it easy to do.\n. Sorry for the delay.\nIf you don't mind, can you resolve the conflict and let's get this in?. @vjpai  Right, Clear() doesn't do real clear... Should be fixed now. PTAL.\n. cc @heyitsanthony @xiang90 \n. Added a test. There are two connections, one of them is down, but the other one is up. All non-failfast RPCs should not block. Other cases should have been covered by existing tests.\n. Not sure what the problem is. If you put the source cloned from github to directory google.golang.org, probably some directory is misconfigured.\nTry to reinstall everything from google.golang.org using VPN, and see if that works...\n. This issue becomes obsolete with the new balancer and resolver APIs (#1388). Closing this now.. I created #859 with my proposed test. Please take a look. You can cherry-pick the test from there if you want.\n. Can you try dial with block\ngo\nclientConn, err := grpc.Dial(\"\", grpc.WithInsecure(), grpc.WithBlock())\n. Empty address is only a special case of invalid addresses.\nShould find a better way to check if an address is valid.\nClose this now.\n. All tests on osx were blocked for some reason. Test for this PR just started.\n. Seems something is wrong with your golang.org/x/net.\nTry to delete the source from your $GOPATH and run go get again.\nYou can first try to only delete source for golang.org/x/net. If this doesn't work, try to delete all source and start again.\n. Not go get -u golang.org/x/net, run go get -u google.golang.org/grpc.\nIf this doesn't work, delete all source, and run go get -u google.golang.org/grpc.\n. Closing this issue.\nWe have a reflection tutorial and an example for this in the helloworld example.. Yes, we should! Done.\n. Log spamming will be alleviated with grpclog changes.\nWe have https://github.com/grpc/grpc-go/pull/922 for the API change.\nI also have the change to use different log levels in gRPC ready, will send that out when the API change is merged.. PR description updated.\n. Closing stale issue.\nAnd https://github.com/golang/protobuf/ would be a better place to discuss issues on protobuf.. Done\n. Is server side interceptor what you are looking for?\nUnaryInterceptor: https://godoc.org/google.golang.org/grpc#UnaryInterceptor\nStreamInterceptor: https://godoc.org/google.golang.org/grpc#StreamInterceptor\n. %q is used because address was a string instead of a struct before balancer was introduced. We should change this.\nOne thing is that %v prints the pointers as 0x1234abcd, just want to make sure this is expected.\n. We are not sure what the real problem is.\nA better place to ask gRPC php questions is repo https://github.com/grpc/grpc, so you can get answers from gRPC php maintainers and other gRPC php users.\n. We have a plan to change the behavior of metadata.NewContext() to do a merge of the metadata before creating a new context.\nDoes this work for your issue?\n. We don't have an issue for that now.\nWe can edit the title of this issue and use this one to track, if you don't mind.\n. I was thinking that client interceptors may also need this functionality, but it seems not a good idea to change the behavior of NewContext()...\nPlease modify this PR to add a metadata.Join() function. Thanks.\n. This can be done by providing a custom dialer (WithDialer) to the client and a custom listener to the server.\nIn this way, you can specify the net.Conn connecting client and server to be a wrapper of something in memory.\n. server.Stop() should be able to stop the server immediately, but sleep cannot be interrupted by stop.\n. For Unary RPCs, you can append your own CallOption grpc.Header() or grpc.Trailer() to opts.\nFor Streaming RPCs, header and trailer can be read from the ClientStream. \n. @rhysh Please subscribe to the github updates.\nPRs with API changes will have [API revision] in the title.\n. @xiang90 We do put API changes in the tag descriptions.\nhttps://github.com/grpc/grpc-go/releases\nFor example, v1.0.3 includes API changes and behavior changes, and it's description is\n```\nAPI changes:\n* ServerStream interface\n - Add SetHeader()\nBehavior changes:\n Concurrent Server.GracefulStop() calls all behave equivalently\n Server.Serve() won't exit when lis.Accept fails with temporary errors\n``\n. Created a tag v1.0.2.\n. Reverted the commit to use stringer, including the memory allocations.\nThe change was intended to add support for structured logging. Will think about that later.. ping. @peter-edgegithub.com/Sirupsen/logrus actually has bothWarnandWarning`.\nAnd also, we want to use V to warp some logs that are off by default.. @MakMukhi is working on this. Will have a PR shotly.\n. Thanks for the review. All fixed. PTAL.\n. The deleted lines were added to disable tests for \"handler-tls\" in those particular tests. I deleted them because I thought since \"handlrEnv\" was commented out, we don't need those any more.\nI have reverted the change so we will keep those lines.\nWe need to merge this PR so we can continue to work on other PRs without breaking the test.\nAnd we should revert after the original problem is fixed. \n. > even though it is not configued to do so.\nWhat do you mean here? What's the config you changed?\n. Currently the default gRPC logger logs everything to stderr.\nIf you don't want this behavior, please set a different logger with SetLogger.\nWe are working on improving the logging system, stay tuned.\n. The new grpclog only logs Errors by default. \nAnd it's behavior can be controlled by environment variables. Closing this now.. It's recommended to use DialContext instead of WithTimeout dial option.\nAdded doc in #1333.\nClosing this PR.. The logging API changes are in: https://github.com/grpc/grpc-go/pull/922\nWe will get it merged by end of next week.\nAfter this, we will change the gRPC code to use the correct log level.\n. @jellevandenhooff \nIf I remember correctly, the zero-delay reconnect issue only happens with a misbehaving server. It should not be an issue in a correctly configured environment.\nAlso, in the current gRPC code structure, it's not trivial to add backoff in this situation.\nConsidering the effort needed to fix and the severity of the problem, we are putting it at a low priority.\nThe logging issue is more annoying for users, so we will get that fixed soon.. Sorry, reopened.. The panic was in proto.Marshal, please file an issue at https://github.com/golang/protobuf.\nAnd a test to reproduce would be very helpful.\n. Thanks for the review. All fixed. PTAL.\n. Please fix the golint errors as well (you need to comment all the exported functions...).\nAnd for the comments, please make them complete sentences. Starts with a capital letter (except for the case where the first word is a variable name) and ends with a period.\n. This check is to make sure the proto package, the gRPC package and the generated code are in sync.\nThis should be fixed by\n- update the go proto package\n  go get -u github.com/golang/protobuf/{proto,protoc-gen-go}.\n- update gRPC package\n  go get -u google.golang.org/grpc\n- rebuild your proto files with protoc --go_out=plugins=grpc:. *.proto.\n. > rebuild your proto files with protoc --go_out=plugins=grpc:. *.proto\nWhat about this?\n. This error indicates your gRPC package is not updated.\nIf you already did go get -u google.golang.org/grpc, you should be able to find this variable in your local grpc package: https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L457.\nAnd if you use go vendor, make sure the vendored grpc is updated.\n. transport.ConnectionError was designed as a gRPC internal only error, which we didn't expect users to create. We removed transport.ConnectionErrorf on purpose to stop users from creating ConnectionError outside of gRPC.\nFor your issue, we think the correct solution should be that, the dialer returns an error with Temporary() method. Just created #974 to resolve this. Please take a look.\n. Added comments to WithDialer.\n. I noticed that you are using vendor, my guess is that something is not updated in your vendored gRPC.\nPlease do an update and try again.\n. I think the gRPC code is not up to date. I'm not sure how you did the update, but you can check you local code to see if it's updated.\nIn the error message you provided, error happens at line 425.\nBut in latest code, transport.NewServerTransport is called at line 443:\nhttps://github.com/grpc/grpc-go/blob/master/server.go#L443\ntransport.NewServerTransport is defined at:\nhttps://github.com/grpc/grpc-go/blob/master/transport/transport.go#L368\n. Can you elaborate? Where did you get the error?\nCan you give more details on what you did?\nA small test to reproduce would also be helpful.\n. This is cause by #974, to fix #971.\nconnection refused is not a temporary error.\nCan you give some details about your use case? Do you have some specific situation where you want the client to retry on a dead server?\n. A custom dialer always returns temporary error should work as a workaround.\nhttps://github.com/grpc/grpc-go/blob/master/clientconn.go#L204\nIf you are using balancer, balancer.Notify() can trigger a new connection to the notified addresses.\n. Create a new context from the old context with WithCancel, and then you can call cancel() to cancel it.\ngo\nctx, cancel := context.WithCancel(ctx)\nstream, err := client.StreamingRPC(ctx)\n// do something on stream\ncancel()\n. What do you mean by \"no address available to dial\"?\nCan you provide a small example to reproduce the panic?. My guess is that addrCh is closed twice because balancer.Close() is called twice.\nThis should not happen if balancer.Close() is only called by gRPC internals.\nDid you call balancer.Close() in your code?. Can you provide a test to reproduce this?\nIf it was closed twice, we need to know why that happened.. Please make sure proto package, gRPC package are all up to date.\nhttps://github.com/grpc/grpc-go#compiling-error-undefined-grpcsupportpackageisversion. I guess your vendored gRPC is outdated.\nAnd did you try to regenerate the file you mentioned vendor/some_grpc_lib/some_lib.pb.go?. In your error message:\n```\ngithub.com/org/repo/vendor/some_grpc_lib\nvendor/some_grpc_lib/some_lib.pb.go:173: undefined: grpc.SupportPackageIsVersion3```\nThe error happens in some file insidevendor` directory. So I guess you are using go vendor.\nThe original issue can be resolved by updating proto package, gRPC package and rebuilding the proto files. If you are using go vendor, you need to make sure the vendored packages are up to date.. Can you provide the log from the client side?\nAnd can this issue be reproduced with clients using other languages (c++, java, or go)? . Why is this necessary?\ne.Error() returns fmt.Sprintf(\"rpc error: code = %d desc = %s\", e.code, e.desc),\ngrpc.ErrorDesc(e) returns e.desc,\nSo e.Error() should already contain grpc.ErrorDesc(e).. You can call SetLogger outside of init(), as long as you can make sure it is called early enough so that gRPC internals can find the logger to use.. We are not setting GOMAXPROCS specifically in our benchmark, and are working on general performance improvements. So there's no point keeping this issue open now.. Serve() returns the fatal error returned by lis.Accept.\nAnd lis.Accept returns use of closed network connection error when it's closed.. Serve() always returns non-nil error. I will create a PR to document that.\nOne way to get a clean log is to filter errors with use of closed network connection, because error net.errClosing is not exported.. #1018 created. Yes, the connection will be closed.\nDid you see any unexpected behavior related to this?. For onnection: https://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L384\nFor stream: https://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L407. Replaced by #1251. This is actually a behavior change.\nIs this very necessary?. I don't know of anyone depending on the string.. Merging this because depending on the string doesn't look like the right thing.. We have local benchmarks in https://github.com/grpc/grpc-go/tree/master/benchmark.\ncd benchmark\ngo test -bench=.. What is the \"caller id\" you want?\nYou can get the peer information using peer.FromContext, and get the peer address.. fixed by #1079 . After #922, we will update log printings in gRPC to use different log levels.\nThis issue should be solved then.. Interceptors are called after the requests are read by the server. It works, but it may be a bit late in your situation.\ntap.ServerInHandle may be what you want.\nInTapHandle ServerOption defined here: https://github.com/grpc/grpc-go/blob/master/server.go#L195. We haven't started on support gRPC-go on iOS yet.\nPlease comment if you find anything more.\nClosing this for now.. The first step (#922) should be done by the end of this week.\nAfter that, we will need another PR to change the logging levels.. Duplicate of #1054. Do you think if this is a gRPC-go issue? Can we close this issue now?. I'm not able to reproduce this...\nI believe the issue behind this is mentioned at https://github.com/golang/go/issues/18806, and it's OS-dependent.\nAlso, there's a fix for it.\nI'm closing this issue now. Please leave your comment here if you think there's some action we need to take.. Do you mean: don't listen on \":nnn\" and don't use ln.Addr().String()?\nAnd are you suggesting that we should take the approach you mentioned in the golang issue: call TrimPrefix() before dialing?. Thanks for the suggestion. We will have a fix for this.. Fix in #1237, PTAL.\nOne thing to note is that, I'm using \"localhost:port\", not \"127.0.0.1:0\" or \"[::1]:0\".\nPlease leave a comment if \"localhost\" doesn't work.. Sorry for the delay.\nIs any TransportCredentials used in your case? Or you are using an insecure connection?. @jboeuf :authority is set to the server name specified by TransportCredentials: https://github.com/grpc/grpc-go/blob/master/clientconn.go#L323. If it's not specified by TransportCredentials, it's set to the DialTarget.\nI'm OK with making this work when WithInsecure.\n@jboeuf Are there any concerns with :authority in the case of insecure connections?. @jboeuf \n\nWe should probably take this one offline\n\nTake which one offline?\n\ndoes the TransportCredentials make sure that the :authority set in this creds is correct (e.g. listed on the server cert for TLS)?\n\nThe official tls library (which we use) checks the server name against cert if server name is not empty. And :authority will be set to the server name.\nIt doesn't do the check if server name is empty, in that case, :authority is same as the dial target.\n. Thanks for doing this. Looks good.. Sorry for the delay. Do you still see this error with the latest gRPC?. Thanks for fixing this. Looks good.. In your case, Dial() returns immediately and connecting the server happens in background.\nWithBlock() dial option can make Dial() block until connections are established.\nhttps://github.com/grpc/grpc-go/blob/master/clientconn.go#L174. InTapHandle server option can do whant you need: https://github.com/grpc/grpc-go/blob/master/server.go#L196.\nServerInHandle takes a context and returns a context. You can set some timeout on the returned context.. LGTM.. Thanks for adding this.\nLooks good.. Do we need anything else other than #1082?. Which version are you using?\nWhat is google.golang.org/grpc/clientconn.go:505 showing in your vendered gRPC?\nIn the current master branch, https://github.com/grpc/grpc-go/blob/master/clientconn.go#L505 is a }.. @xoebus gRPC won't retry if credentials.TransportCredentials returns non-temporary errors. It doesn't depend on the error code returned.. Thanks a lot!\nThere are some unrelated changes though...\n\n2 lines of comments removed in benchmark/grpc_testing/control.pb.go\n2 lines of comments removed in benchmark/grpc_testing/control.proto\n1 line of TODO removed in benchmark/worker/benchmark_client.go\n. fixed by #1290. There's current no way to know that.\nFor example, in the case of connection errors, there's no way to know how many bytes are received by the server.. In the proposal, it's proposed that we need to kinds of mapping functions, MapName before name resolution and MapAddress after name resolution.\n\nCustom dialer only works in the case of MapAddress.\nIf the name cannot be resolved, dialer won't work.. Make sense.\nClose this and added #1098. You can do this with interceptors.\nServer side interceptors: https://github.com/menghanl/grpc-go/blob/master/server.go#L171\nClient side interceptors: https://github.com/menghanl/grpc-go/blob/master/clientconn.go#L252\nIn this case, your service handlers can return the custom error, and in the interceptor, you can convert that error to a gRPC error.. ping. You can override the dialer with WithDialer().\ngo\ngrpc.WithDialer(func(addr string, timeout time.Duration) (net.Conn, error) {\n  return net.DialTimeout(\"tcp\", addr, timeout)\n}). LGTM.. LGTM.. Closing this issue now. Please leave a comment if you still see the problem.. The context scope is for each stream. It's not support to change context for each request.\nCan you explain a bit more about your use case? Do you want to update the context for each request received in the stream, or just to update the context once when the first request is received?. How about override the Context() method of ServerStream?\nCreate a wrapper of ServerStream that contains the token you want to attach to the context. And override the Context() function to always attach the token.\n```go\ntype myStream struct {\n    token string\n    grpc.ServerStream\n}\nfunc (s *myStream) Context() {\n    s.ServerStream.Context().WithValue(tokenkey{}, s.token)\n}\n```\nThe stream interceptor will return the warpper myStream here..  Though this PR doesn't fix the root problem, it changes the failure from half the time to a race condition.\nGiven that we don't have enough time to look into this issue, I'm going to merge this change. Please send another PR if you think that can fix the root cause.. fixed by #1117 . LGTM.. Is #1097 what you are trying to do?\nIf yes, you can do that with interceptors.\nIf no, can you explain a bit more about your use case?. LGTM.. Can you explain a bit more on why you need to access content-type?\nMost of time it just should be application/grpc.. Something is wrong with the connection. I'm not sure what the root cause of this though.\nI would suggest make sure the network settings are correct.. I will submit this on March 23 (Thursday) if nobody objects. . Also add a test similar to the repro from the issue?. Thanks for your fix.\nThe problem exists not only for client streaming, but for all streaming RPCs.\nThis should been fixed by moving stats.End to clientStream.finish() (#1182)\nThe tests you added would be a good test for situation like this.\nIf you don't mind, can you resolve the conflicts in the test so we can get the tests merged? Thanks.. The go version used is 1.5. It doesn't work after #1132.. https://github.com/grpc/grpc/pull/10309 filed for this.. We are holding this off because the proto context package change was reverted https://github.com/golang/protobuf/pull/326. How about implementing GoString() on Stream?\nIt can return the pointer and the method name. Something like\ngo\nfunc (s *Stream) GoString() string {\n    return fmt.Sprintf(\"<stream: %p, %v>\", s, s.method)\n}. @xiang90 Just did a v1.2.1 release on v1.2.x branch. This fix in included in that release.. We don't have support for that.\nBut I believe you should be able to do something like\ngo\ncookies := (&http.Request{Header: metadata}).Cookies(). Can you explain a bit more about what you plan to do based on whether the error is an rpcError?\nWe actually removed rpcError recently and replaced it with status: https://github.com/grpc/grpc-go/blob/master/status/status.go. And the new function FromError does what you need.\nSo you could switch to this if package_A uses a updated gRPC.. This is intentional. Because we think that in most use cases:\n\nClient sends metadata using NewContext\nServer reads metadata using FromContext\n\nThere's no way to make NewContext and FromContext work for both incoming and outgoing metadata, so we decided to make NewContext same as NewOutgoingContext, and FromContext same as FromIncomingContext, hoping that most users only doing sending/receiving metadata are not affected.\nIt seems that you are trying to read the outgoing metadata on the client side, which falls into the use cases that will be broken by the change (sorry about the breakage). Please use FromOutgoingContext to retrieve the metadata set by NewContext.\nMore on that, NewContext is actually deprecated. Please use NewOutgoingContext instead. And if you switch to the new functions entirely, you should have something like FromOutgoingContext(NewOutgoingContext(ctx, md)) that works as expected.. Actually, the mechanism to send metadata is different between client and server. To send metadata on client side, users need to add the metadata to the context which will be used to make the RPC. To send metadata on server side, users need to call functions like grpc.SendHeader or grpc.SetTrailer.\nReading is also different... More details on this are available in this doc.\nSo metadata.NewIncomingContext should only be called by gRPC itself (to add the received metadata to a context so that service handlers can read the metadata). There's no need for such a function to add metadata to a context using metadata.NewIncomingContext.. However, functions trying to read metadata from context will have the issue you mentioned.\nIn this case, I think it's kind of reasonable to add a flag to indicate which metadata to read. Because a context may have both the incoming and outgoing metadata. The caller of this metadata-reading-function needs to specify which one to read somehow.. Yes, grpclb is kind of special and is implemented in grpc package, so we need to edit this file to avoid import cycles at the first place.\nPlease ignoregrpclb.proto when regenerating pb.go files. . This issue will be fixed by #975. This is currently not implemented.\nCan you give more information about what you are planning to do with this API?. I'm not clear what you are trying to get here.\nIs this on server side? Where do you need the service name?. You are reusing the request to get the response, which doesn't seem correct.\nIt shouldn't work. It worked because the proto unmarshaller didn't reuse the underlying buffer.\nIn a performance change, we reuse the buffer when unmarshalling. So this problem is revealed.\nWhat you should do is what we do in our generated code (https://github.com/menghanl/grpc-go/blob/master/examples/helloworld/helloworld/helloworld.pb.go#L86):\ngo\nx := &pb.HelloRequest{Name: []string{\"a\", \"b\"}}\nout := new(pb.HelloRequest)\nerr = grpc.Invoke(context.Background(), \"/pb.Greeter/SayHello\", x, out, conn)\nAlso, do you have any particular reasons to call Invoke instead of calling the generated code? This kind of problems can be avoided if you are using the generated code.. If you really want this to work, you could try to override the codec with the some custom codec (WithCodec) when dialing.\nI think the old codec here would just work: https://github.com/grpc/grpc-go/pull/1010/files#diff-9d9af2a19a6c9f88d8535aa95b1f9c22L68. It seems clearer to do the Is change in another PR, if we decide to have it.\n@dfawley is out this week and will be back next Monday. Let's wait and get his input on the Is function.. Server indicates the RPC is done by returning from the service handler. When the service handler returns, gRPC cancels this stream because the RPC is done.\nSo in your handler for RPC Start, you need to either call Recv() in the same goroutine, or add some synchronization in the service handler to make sure the handler doesn't return until you are done with this RPC.\nAlso, what's the error you were expecting that indicates client disconnected?. cc @bradfitz, PTAL. Let do this change for now as it looks a little bit \"cleaner\".\nWe will switch to the newLocalListener fix if there's any issue reported from the user.. Thanks for the PR.. Sorry, can you clarify you question?\nThe balancer you implemented is both a Balancer and a Resolver, which seems weird...\nIf you need a roundrobin with a static list of addresses, you have answered your own question: create a StaticResolver and use it to initialize the RoundRobin balancer.\nAre you suggesting we should have this implemented in the repo? Or ask for an example?. In your implementation, balancer and resolver (and watcher) are mixed together into one struct.\nBalancer and resolver should be implemented independently.\nA static resolver could look like:\n```go\ntype staticWatcher struct {\n    updates chan []*naming.Update\n}\nfunc (w staticWatcher) Next() ([]naming.Update, error) {\n    u, ok := <-w.updates\n    if ok {\n        return u, nil\n    }\n    return nil, fmt.Errorf(\"watcher closed\")\n}\nfunc (w *staticWatcher) Close() {\n    close(w.updates)\n}\ntype staticResolver struct{}\nfunc (r staticResolver) Resolve(target string) (naming.Watcher, error) {\n    ch := make(chan []naming.Update, 1)\n    ch <- []*naming.Update{{Op: naming.Add, Addr: \"127.0.0.1\"}}\n    return &staticWatcher{ch}, nil\n}\n```\nUse it with RoundRobin:\ngo\nb := grpc.RoundRobin(&staticResolver{})\ngrpc.Dial(grpc.WithBalancer(b))\nWe are also planning on splitting balancer and resolver in gRPC, so there will be separate APIs to set resolver and to set balancer. With the new set of APIs, we will have default resolvers implemented in the repo, and more documentation on resolvers. Hopefully this will be less confusing.. > Commenting out this line in gRPC-Go works as it uses the proxy\nWhat do you mean by this? How does the proxy work if you comment out the support for CONNECT?\nThe support for CONNECT is part of the default dialer.\nIf you don't set http_proxy, it will not try to do CONNECT. If you do want to skip CONNECT, another way is to implement your own custom dialer. But I didn't get it why you want to skip the CONNECT handshake.\nSince CONNECT is just one step of dialing, the dialing timeout applies here. You could do a DialContext, and call it with WithBlock dial option.. If I understand correctly, the HTTP Proxy server forwards the TCP connection to the desired destination. So http2 should work as expected.\nAlso, I may be wrong here, but is there an RFC talking about support for CONNECT with http2?. We don't see the need for this right now. So I'm closing this issue.\nPlease leave a comment if you still think this is necessary.. Do we know what the string added to metadata by Bigtable is? Does it contain special characters (like newline)?. Have you considered put the information in AuthInfo?\nAuthInfo can be retrieved from peer. You can set AuthInfo during handshake. It's one of the values returned by handshake functions.\nTo check, first get the Peer for the RPC, using FromPeer.\n(Note that for Unary RPCs on client side, you need to use this call option to get Peer, and it's only available when the unary call is finished).\nThen with Peer, you can check the AuthInfo (with a type assertion).. We can backport this PR to the 1.4 branch and do a 1.4.1 release.. @gyuho Just released 1.4.1 with this change.. Sorry about breaking backward compatibility on SupportPackageIsVersion.\nSupportPackageIsVersion is just one of the public APIs of gRPC, though this one is a bit special because it involves interaction between gRPC package and proto codegen.\nIf your project uses some APIs in gRPC that got removed or modified in some release, you will need to deal with a similar problem as this. The difference is that, when changing other APIs, we try to not break backward compatibility. But for SupportPackageIsVersion, we have to remove the old one and there's no easy way to walk around.\nSo this problem is actually not specific to SupportPackageIsVersion*, or gRPC. It's a general question about package management for golang.\nThe official solution for this is vendor, and there are also a lit of options at: https://github.com/golang/go/wiki/PackageManagementTools.. As you mentioned, google recompiles proto files on the fly, so SupportPackageIsVersion is not a problem.\nIn google3, if we cannot ensure backward compatibility, we can make a big change to update all users to the latest API. It's time consuming, so most time keeping backward compatibility would be a good idea.\nI understand that those may not be practical for external users. And I think the solution here is probably still vendoring.\nI'm not sure what the issue about \"vendor does not play nicely with bazel\" was, but it seems the bazel team is make progress on supporting vendor:\nhttps://github.com/bazelbuild/rules_go/blob/master/Vendoring.md#vendoring\nhttps://github.com/bazelbuild/rules_go/pull/488. About this specific issue on SupportPackageIsVersion3 & SupportPackageIsVersion4, the PR is: https://github.com/grpc/grpc-go/pull/941. The only thing affected is server reflection. So if you don't care about server reflection, you could ignore the version change (hard code version to 4 in old code generated with version 3), but all reflection requests will fail with \"invalid file descriptor\" error.\nI would expect other things to work fine.\nWe try to keep backward compatibility when making API changes. It's a bit hard to do for SupportPackageIsVersion though. If we need to change it again (hopefully we won't, but...), we will try to change it in a backward compatible way, for example support rolling upgrades as you suggested.\nAgain, sorry for the breakage.. An update on this: it's not hard to add support for SupportPackageIsVersion3 back. So gRPC will support both SupportPackageIsVersion3 and SupportPackageIsVersion4.\nWe will get a PR out for this shortly.. It seems the test is flaky.\nI restarted the test, and it passed.\nFiled issue #1283 to track the flaky test.. Thanks for the PR.\nCan you resolve the conflict? We renamed some files in benchmark.. Thanks for this PR. We definitely want this.\nA problem here is that, you need to run those tests or binaries in certain directory, otherwise they cannot find the testdata.\nOur proposal is:\n1. Add a go package testdata here\n1. Add an exported function to testdata package (testdata.Directory()?), which returns the absolute path of the testdata directory. This function should look similar to the abs function we already have.\n1. In all the places test files are needed, specify them using the newly added function.\nFor example: var caFile = testdata.Directory() + \"ca.pem\"\nWhat do you think?\nIf you agree, do you mind changing this PR to implement it?. Happened again:\nhttps://travis-ci.org/grpc/grpc-go/jobs/370293809\nerror message:\n--- FAIL: TestServerGoAway (0.24s)\n    end2end_test.go:537: Running test in tcp-clear-v1-balancer environment...\n    end2end_test.go:537: Running test in tcp-tls-v1-balancer environment...\n    end2end_test.go:537: Running test in tcp-clear environment...\n    end2end_test.go:537: Running test in tcp-tls environment...\n    end2end_test.go:952: TestService/EmptyCall(_, _) = _, <nil>, want _, Unavailable or Internal. @jtattermusch can you take a look at this?\nI can make the change if you think it's appropriate.. Thanks for the review. All fixed.. We don't have a gRPC option for creating multiple connection to the same server, but the same functionality is supported by balancer.\nBalancer returns a list of addresses to gRPC via Notify(), each address is a string +  metadata.\ngRPC creates a connection for each different Address struct returned. If the addresses contain the same server address but different metadata, gRPC will create multiple connections to the same server.\nLoad balancing between these connection is done by the same Balancer.\nAbout the overhead of ClientConn.\nEach ClientConn maintains it's own connection pool, and some goroutines for balancers. There's some overhead, but the overhead is quite small.\nFor the situation you are interested in, maintaining your own pool of ClientConns would work, too. But then you need to do load balancing between multiple ClientConns.\nI would suggest take a look at the Balancer APIs, and see if it satisfies your requirements.. @gyuho We try to not release too frequently, hopefully not more than one release per week, except for severe bugs.\nWe will do a release later this week, which will include the fix here.. Closing this issue due to lack of input.\nPlease reply back if you still want this.. @enocom That will be great! Thanks a lot!\nWhat was not mentioned above but could still be helpful: https://godoc.org/google.golang.org/grpc#Stream.. I'm not sure why the logging is not working.\nOne easy way to check if things are working is to run GRPC_GO_LOG_VERBOSITY_LEVEL=2 GRPC_GO_LOG_SEVERITY_LEVEL=INFO go test google.golang.org/grpc -v and see if you see INFO and WARNING logs in the output.\nAnd we don't have as thorough logs as in C-core, so it's possible that nothing is logged in this case (it's unlikely, but it's possible).. One thing on logging, logs go to stderr by default. Not sure if that's the reason you didn't see any logs.. On logging, please try glog and see if that makes any different.\nImport package glogger as in the end2end test, and run the binary with --logtostderr and -v=2.. It's generally not recommended to modify generated pb.go files. Because if you update your proto files and regenerate pb.go, you will need to remember to modify them again.\nModifying pb.go files can work as workarounds, but not a general solution.\npb.go files contain helper functions for proto users (as they are generated from proto files...).\nIf you don't want proto message in your services, you can call gRPC APIs directly (for example Server.RegisterService on the server side, but that means you also need to handle some other logic like the interceptors.. Thanks for reporting this.\nIf my understanding is correct, what happened is:\n - roundrobin sends an update to gRPC, lbWatcher starts to process the update\n - roundrobin sends another update to gRPC (while holding the mutex)\n   - this send blocks because the reader lbWatcher is not reading\n   - also, the mutex is not released until the send unblocks\n - lbWatcher calls down when processing the previous update, because it removed some address\n   - down tries to hold the mutex, and blocks\n - watchAddrUpdates is waiting for lbwatcher to read from the channel, lbwatcher is waiting for watchAddrUpdates to release the mutex\nThe problem here seems to be that, watchAddrUpdates should not send to addrCh while holding the mutex. If we release the mutex before sending to addrCh, this deadlock will be resolved.\nIs that correct?\nAnother option would be, making sending to addrCh never block.\nWe could make addrCh a channel with buffer size 1 (make(chan []resolver.Address, 1), and pull the old update from it if there's any before sending\ngo\nselect {\n  <-addrChs:\n  default:\n}\naddrCh <- open\n. Thanks for debugging and reporting the issue.\nI think we could still use cc.conns as the new cc.addrs map you added in this PR if we can keep cc.conns consistent with the addresses specified by balancer.\nThe root issue I'm seeing here is that, resetAddrConn is used to create new addrConn in some cases (balancer adds more addresses), and to replace the old addrConn with a new one in some cases (goaway happens).\nThe bug as you described is, in the latter case, if such an addrConn doesn't exist in cc.conns, we will still add a new addrConn, which is wrong.\nresetAddrConn takes a parameter to specify the error to tearDown the addrConn.\nThe error is always nil if we are adding a new addrConn, and is always non-nil if we are replacing the addrConn.\nThe fix I'm proposing is that, we should check both if tearDownErr != nil and if stale := cc.conns[ac.addr]; stale == nil.\nIf tearDownErr != nil (we are replacing addrConn, not adding a new one), and also stale == nil (the addrConn we are trying to replace was removed), we should not add a new addrConn to cc.conns.\nPlease let me know if this sounds correct.\n. ~~A second thought on this, maybe we should never replace the old addrConn with new one.\nIn the case of connection error happens, we could just reset the underlying connection.\nCreated #1369 for this. Please take a look and let me know what you think.~~\nIgnore this. It's a bit more complicated than that.. Our release cycle is every 6 weeks.\nSo the next release is scheduled July 19 (tomorrow).\nAbout this PR, I updated #1369 with some new changes, so we can replace transport not the addrConn. Please take a look at that.\nIf we cannot include the fix for this in the next release (1.5.0), we can have a minor release (1.5.1) to include this later.. #1369 has been merged, and the problem should be solved.\nCan you try and see if you can still produce the issue?\nAlso, if you don't mind, can you clean up this PR and only include the test you added?. Sorry for delay.\nv1.5.1 was released.. Closing this PR now. Thanks.. The purpose of GracefulStop, as described in the godoc, is to stop the server from accepting new RPCs, but keep working on the pending RPCs.\nIn your case, the streaming RPC is a pending RPC, and the server will not try to close. The behavior you saw is expected.\nIf you want the stop all RPCs, why not call Stop instead?. This can be solved by a connectivity state API. We are planning to support the API this quarter.\nOn the issue you mentioned here, can you give more details about why a blocking dial with timeout does not work for you?. The implementation should call os.Exit().\nIn the default implementation, log.Logger.Fatalf is called, which calls os.Exit() eventually.\nThe package interface level os.Exit() is called in case some implementation doesn't exit on fatal.\nIt is a no-op if the logging implementation does the right thing.. Calling os.Exit() again in the default implementation after Fatal() seems unnecessary to me.\nAlso the logger interface documentation makes it clear that implementation should call os.Exit().\nIt seems the confusion is that, at the first glance, the default implementation doesn't call os.Exit().\nHow about adding more comments in the default implementation and also in the package functions to make what happened on os.Exit() more clear? So that people won't get mislead by it?. We do want the implementation to call os.Exit() in Fatal functions.\nWhat do you think about the comments added in #1365?. This looks like a log spamming on the server side, which will be fixed by #1340.\nThis is actually not an error.\nDid you see any error on the client side?\n. This was fixed by #1195, which is included in release 1.4.0.\nSo right, it's fixed in 1.4.1.. cc @danjacques. @danjacques Just updated the comment on loggerV2 API to allow it to not call os.Exit(). Please take a look.. Correct me if I'm wrong, I think concurrent read from a map doesn't cause panic.\nAlso, registering new services to a server after serve() is called is not a valid use case.\nIt fatals after #828.. The purpose of not support registering after serve() is to avoid the overhead of mutex when reading from the service handler map. We didn't see the need to register new service handlers after the server has been started.\nIf you have a use case where the service handler has to be installed after serve() is called, please file an issue, and we will consider supporting register after serve().. Can you provide more details? What you expect to see? And some examples would also be good.. @Zeymo Without InTapHandler, the client timeout should be propagated to the server, and the timeout should work as expected.\nCan you give more details on what the issue is?. The problem is, we use grpclog in the example, and we changed the behavior of grpclog recently.\nPlease try GRPC_GO_LOG_SEVERITY_LEVEL=INFO go run client/client.go and see if that works.\nAlso, using grpclog in the example seems wrong, we should use non-grpclogs directly, like the go log package. We have #1395 to fix that.. It seems what you want is to tearDown some connections on the client side, is that right?\nIf yes, balancers could do what you want. Just send a list of addresses without the server address you want to gRPC through (Notify())[https://github.com/grpc/grpc-go/blob/master/balancer.go#L110], gRPC will graceful close this transport (in (lbWatcher())[https://github.com/grpc/grpc-go/blob/master/clientconn.go#L613]).\nNote that, we are working on changing the balancer API in gRPC, and Notify() will be deprecated. The new balancer work is tracked in #1388.. The connections are gracefully stopped. They will kept open until existing RPCs on it all finish.\nDo you have a long live streaming on the connection?\nThere's currently no way to force close a particular addrConn on the client side. Do you have a situation where you want the connection to the server to abort immediately? What are you expectations for the existing RPCs to this server?. The goroutines you see are goroutines monitoring the underlying connections for each addrConn. Those are not for each RPC, so it's expected that you still see them when the RPC is done.\nDid you see those goroutines blocking even after the ClientConn is closed?. I'm not sure about what you want. I think the RPC finishes as expected, the goroutine you saw is not related to the RPC.\nDid you see one more goroutine created and blocked for each RPC?\nTo explain more on the goroutine you saw:\nWe maintain a connection pool on the client side to send RPCs, as part of ClientConn. When sending RPCs, one connection will be picked from the pool to use. When the RPC finishes, we don't close the connection because it can be used by other RPCs.\nThe goroutine you saw is bound with the connection, to make sure we always have a working connection to use.\nThe goroutines will exit when the users decide to close the ClientConn.. It's not expected to see those goroutines on the server side. These should be client side only.\nIn your service handler, do you create a new ClientConn and send new RPCs? If yes, did you close that ClientConn?\n\"Users decide to close the ClientConn\" means when user calls ClientConn.Close()\nI'm not sure about the behavior in a NodeJS client, but I would expect it to be some similar to go.\nWe currently don't a best practice doc. But there's nothing special I can think of about terminating a bidirectional stream. The routeguide example we have should already cover everything.. @gitsen This was released in v1.5.2.. It implements status.Status is not accurate.\nIt should be something like It is an error generated from status.Status and can be converted back to status.Status.\nstatus.FromError should work for this error.\nSorry for the confusion. This should be fixed in #1418.. @therc This was released in v1.5.2.. The root cause of this issue is that, the ClientConn returned by a non-blocking dial is not immediately ready to use (connections are not established for example).\nOur exiting handling of FailFast is flawed, so if you make an RPC on a non-ready ClientConn, the RPC may fail as you saw.\nA quick fix for this is to do a blocking dial using WithBlock, so that in the returned ClientConn, there's at least one ready connection to use, then FailFast RPCs will still succeed on this connection.\nOr you can make the RPC non-failfast using FailFast, this should also fix the issue you saw.\nI tried both, and the failure seems to be gone with these. Please try and let me know.\nWe are working on a fix for the FailFast issue, as part of the new Resolver and Balancer implementation (#1388).\nIn the new implementation, we can better handle FailFast RPCs on a connecting ClientConn, issues like this will not happen.. Can you please run gofmt to fix the formatting issues? Thanks.. Returning RPC errors from InTapHandle is not intended. The functionality of InTapHandle is very limited, and the content of the error will be ignored. For your use case, please try interceptors.\nThe comment for ServerInHandle didn't make this clear. I have filed a PR (#1437) to update that.\nSorry for the confusion.\nThe behavior of Serve() is expected, though. Because only this one RPC failed, and the server is still serving other RPCs.. Thanks for the review. All fixed. PTAL.. Now checking all goroutines instead of just looking at delta. PTAL.. Thanks for the contribution and sorry for the late reply.\nOne issue with your proposed fix is that, the new dial option WithProxyURL is too specific for this problem. And it would be better if we can get a more general solution.\nA proxy dialer actually consists of three parts:\n - mapper from the target address to the proxy server address (for example, get proxy env variable)\n - dialer\n - handshaker\nCurrently, we have these three components, but they are not explicitly called out and not exported.\nThe proposal we make here is to make the mapper and handshaker public, so users can compose their custom proxy dialer as they want, using these components.\nThis includes:\n - Add a new proxy package\n - Move code in proxy.go to the new proxy package\n - Export ProxyFromEnv and HTTPConnectHandshake\n   - ProxyFromEnv would return one more http.Header value for credentials\n   - HTTPConnectHandshake would take one more http.Header parameter\n - Add a new function which takes a mapper, a dialer and a handshaker and returns a dialer for gRPC\nWith the above changes, for your particular problem, you would just need to provide your mapper (which always returns your specified URL and http.Header field containing your creds), and build a dialer with the existing handshaker.\nPlease let us know if you want to make the implementation as we suggested.\nIf you don't have time to implement all of those and want a solution for your issue, a walkaround would be to implement your own proxy dialer, which may need copying some of the function from proxy.go, and dial using WithDialer.. Ping. Are you still planning on implementing this?. @shevchenkodenis No worries! Thanks for implementing this!. Ping. Just want to check what the status of this is.. Closing the PR now.\nPlease leave a comment if you want to revive this.. Did you run go build after go get?\nYou don't need to build the package to use grpc. Just get should be sufficient.. @chy168 I just tried the command you posted and it worked for me without errors.\nThe go version I'm using is 1.9 though, maybe that causes some difference.\nAs @irfansharif mentioned, go get should download the gRPC source files for you.\nOne way to verify if the installation was successfully is to run the helloworld example.. This has been fixed by the new balancer behaviors (picker_wrapper in #1506).. Sorry for the late reply, and thanks for the change.\nThe changes in this PR look good.\nBut #1372 is actually about making Serve() not return when GracefulStop() is called, and block until the server is really stopped.\nIt's not just to make Serve() return nil error or not.\nDo you want to try to solve this in another PR?\n. Thanks for the changes. I'm merging this PR as it has been delayed for a long time.\nIf you are still interested, you can add the test I mentioned in another PR.\nThanks!. The pickfirst balancer you found in balancer.go should work, the reason it's not exported is that we will remove it shortly as we are working on new resolver and balancer APIs (#1388).\nWith the new APIs, pickfirst will be the default balancer if no balancer is specified by the user, so you can use pickfirst with your own name resolver. We will either not export pickfirst at all, or move pickfirst into another package and export.\nThe new APIs were added into the repo, but the implementation is still in progress in my branch, and should be finished in a week or two.\n(Note that the resolver scheme registration is also not working because of the same reason.)\nBefore that's available, if you really need a pickfirst balancer, you can have a small patch to export the pickfirst balancer in balancer.go.\nSorry for inconvenience.. It seems failfast is the reason for the flakiness. Some RPCs fail because the connection is down (or not ready at the beginning).\nRPCs are failfast (not waitForReady ) by default. I tried to set default failfast to false in gRPC code, and the stress test didn't fail after running for ~16 minutes~ 57 minutes.\nThe related change in this PR is this line, which actually fixed a broken behavior that failfast didn't fail properly before. \nPlease try to make all the RPCs non-failfast by using grpc.FailFast(false) call option. (You can set default call option for a ClientConn using WithDefaultCallOptions)\n. What you observed is correct. The first failfast RPC could fail because the connection is not ready yet.\nDoes it make sense for you to make blocking dial instead? So the connection will be ready at the time you make the RPC.\nI will also make a change to skip the TransientFailure in transport if it's the first time connecting.. @irfansharif I added a commit to make a copy of the substring. PTAL.. The change in this PR doesn't work because it makes trace.LazyLog not lazy any more.\nAfter this change, with trace on, even if /debug/requests page is never rendered, each request and response will be serialized when LazyLog is called. This adds large unnecessary overhead to each RPC.\nClosing this PR because of that.. Conflicts...\nAlso, should we label this PR as an API change? Even though it's an internal API change?. The change LGTM. Thanks for the fix.\nCan you also copy your test to handler_server_test.go?. Thanks for the quick fix.\nThe release date for 1.7.0 is next Wednesday (Oct 11).\nIs it OK for you to wait for 1.7.0?. Can please also you do a rebase?\nThe test failed because go get staticcheck failed (disabled in #1561). Thanks!. We had a discussion on this, and the solutions would all include API changes and also changes in the generated code. This would result in compatible issues between old gRPC libraries and new generated code.\nAlso, another note is that, the UnknownServiceHandler is supposed handle \"unknown\" services. The use case it was added for was gRPC proxy.\nWhy are you interested in the service and method name in the UnknownServiceHandler?\nWhat I'm thinking is, if you could somehow handle those requests, they are no longer \"unknown\" services, and you could just install normal service handlers for them.\nCan you provide more information on what you plan to do in UnknownServiceHandler? Thanks!. On a second thought on the context. There's actually already a way to get the method name out from the context:\ngo\nfunc unknownHandler(srv interface{}, serverStream grpc.ServerStream) error {\n    stream, ok := transport.StreamFromContext(serverStream.Context())\n    if !ok {\n        return fmt.Errorf(\"stream not found in serverStream context\")\n    }\n    fmt.Println(stream.Method())\n    return nil\n}\nThe problem with this is, transport package is for gRPC internal uses only, and we may break the APIs. So we don't want the users to use it directly.\nSo now the options we could do:\n 1. Add another method to grpc.ServerStream interface. Problem is, this would be an API change.\n 1. Add one more function Method() to the serverStream implementation. Users can do a type assertion first and then call the method.\n 1. Add a gRPC level function that takes a grpc.ServerStream and returns it's method string.\nLet me know your thoughts on the options.. Could you provide a small snippet to show what you did on the client side?\nIt would be even better if you can provide a reproduction of the problem.. Credentials handshake errors are recognized as non-temporary errors by isTemporary function.\nThe purpose was to detect misconfigured certificates and return TLS errors (https://github.com/grpc/grpc-go/pull/768).\nTo make the client retry connections on creds errors, one way is to create a wrapper credentials, whose ClientHandshake always returns err implementing Temporary() bool { return true }.\nA side note: with the new balancer APIs (#1388) we are working on, the feature to return creds errors from Dial introduced in (#768) will NOT continue to work.\nSo in a cleanup PR following the balancer PRs, we may make client always retry on creds errors.. The new balancer APIs are in progress (#1388).\nThere are multiple pending PRs (including #1515, #1551 and #1558) in review.\nEventually, users should use service config to pick the balancer to use. But as the work to support service config is still in progress, there needs to be some way to test balancer implementations. And that's what grpc.WithBalancerBuilder is for.\nWe don't want users to use this in production (because there should be a way for service owners to switch balancer for its clients), so it's marked as testing only.\nWe expect this new balancer APIs to be stable, but we will still mark it as \"EXPERIMENTAL\" for some time before we are confident enough.\nAs for what you should use right now:\nPlease use the new balancer APIs as in the balancer package. The roundrobin implementation and test can work as an example of how to do it right now.. @xiang90 You can send comments to the discussion link for the gRFC. Or simply file an issue in our repo.\n@Civil Yes.. Closing this issue. Please track the process in #1388.. Status is to represent RPC status (the error code, message and details).\nAll errors returned by RPCs should be type Status.\nErrors returned by other function like Dial are not type Status.. It's not explicitly documented now.\nIn the doc for status package, there's this line that conveys the same message:\ngRPC service handlers should return an error created by this package, and \ngRPC clients should expect a corresponding error to be returned from the RPC call\n\nWe will see if there's a better place to document this behavior.. I removed the FailFast(false) calloption from pickfirst and roundrobin tests.\nThis will cover the case that first failfast RPC after non-blocking dial doesn't fail.. @tamird Will do it this week.. @tamird Just cut 1.7.1.. #1606 should have fixed this. Please give it a try.. This could be caused by the switching of resolvers, and #1606 may fix this.\nCan you try again with the latest code?. The dial on this line might have returned an error and nil ClientConn: https://github.com/chrissnell/weather-bar/blob/master/main.go#L53, and I guess the defer following that calls Close on the nil. Please check the error.\nAlso, I don't have the cert file you listed in your config. I removed that line and got connection error.. I tried with the current gRPC master and it worked for me.\nThe error you got should never happen. For scheme \"\", the default resolver will be used.\nPlease make sure your gRPC code is up-to-date, and try again.. This could be caused by the switching of default resolver. Can you try https://github.com/grpc/grpc-go/pull/1606 and see if that fixes the problem?. Are you using your own logger? Info is ignored by the default gRPC logger.\nIs it OK for your logger to ignore all Info logs from gRPC?\nOr, another option is to put those logs behind a verbose level.. @irfn What error did you get? If you got the same f.fr.SetReuseFrames undefined error, your local http2 package is outdated. Please try to update your local packages with go get -u golang.org/x/net/http2. Or simply do go get -u google.golang.org/grpc to update all the dependencies.. If I understand correctly, you are doing test with dial target \"some-service\", while the tls certificate is for \"some-addr-from-resolver.com www.some-addr-from-resolver.com\".\nBy default ServerName in tls.Config will be set to the dial target if it's not overridden (if it's \"\").\nIf you want to override the ServiceName, set the serverNameOverride parameter in NewClientTLSFromFile to the server name you want.. The reason for the change is:\nNormally, balancers do pick from resolved addresses, not server names.\nIf we do ClientHandshake() using the address returned by balancer, we will use the IP not the server name, which results in handshake failures.\nThough balancers returning un-resolved names is supported (it's actually supported by net.Dial which does the name resolution for you), it's not the expected way.\nFor you use case, I would suggest creating multiple CleintConns for those different servers.. The ClientConn is a logical connection to one name. There may be multiple connection to the same name, but they should share the same Authority. If you need to connect to multiple servers (with different Authorities), you should consider creating multiple ClientConns.\nThere's a hacky workaround for this:\nDon't install client transport credentials. Install a custom dialer and do the client side handshake in your special dialer. Your dialer will get the address returned by balancer, and you can use that to do the handshake.. Closing this issue now as it works as intended.. This is not a behavior that we want to support. Each ClientConn should have one authority.\nClosing this PR now.. We will do a cleanup in addrConn/ClientConn, and will see what we can do on the errors.\nTracking in #1742 . Can you explain a bit more on the purpose of this change?\nAlso, IdempotencyLevel is never set to non-default value in the change.. Will do 1.7.2 this week.. @gyuho 1.7.2 is out\ncc @tamird release 1.7.2 fixed an issue in 1.7.1, PTAL. down is called by v1 balancer wrapper when the SubConn state changes from Ready to non-Ready:\nhttps://github.com/grpc/grpc-go/blob/4318e6475c4b2660f7b8ddb77a4504da77311504/balancer_v1_wrapper.go#L243-L249\nIt's called with \"grpc: the connection is closing\" though.\nDid you see that your down function is never called?\nOr you expect the drain error message?. down is called by notify balancer that the connection is not working.\nIt should be only called once. What do you mean see errConnDrain and then the connection is closing?\ndown is not removed. It's moved to the v1 balancer wrapper. When tearDown() is called, the down function should eventually be called v1 balancer wrapper. If your down is not called, that will be a bug.. tearDown calls handleSubConnStateChange, which will eventually trigger v1 balancer wrapper's HandleSubConnStateChange being called with Shutdown state.\nThe HandleSubConnStateChange function in v1 balancer wrapper calls the down function whenever the SubConn's state changes from Ready to non-Ready.\nSo, when a SubConn's tearDown gets called, the corresponding down function should eventually be called by the v1 balancer wrapper (because of the state transition that happens after tearDown).. v1.6.0 calls down(errConnDrain) if down is not nil, which means Up was called.\nIf the ac was never Up, down will be nil, and tearDown won't call it.\nIn balancer wrapper, Up is called when state goes from non-Ready to Ready, and down is called when it goes from Ready to non-Ready. tearDown for a Ready SubConn will call its down function.\nThe behavior should be the same.. If you mean the timing is different, that might be right.\nIn v1.6.0, down is called immediately when tearDown is called, while in balancer wrapper, down will be called when the state change is propagated to the balancer.. Reopen this issue and will add protoc command back to example/readme.. The failfast RPCs should fail immediately with unavailable errors. Fix for this bug is in #1657.\nThe error will be a general error \"all SubConns are in TransientFailure\" now. We will try to include the reason (for example creds handshake errors) in the error message.\nNon-failfast RPCs will keep waiting for connections to be ready, and will fail with DeadlineExceeded (in theory, the connection could recover, e.g. the resolver might return a new address).\nLet us know what you think of this behavior.. Sounds good. Thanks for confirming.\nWe will still work on including the error details in the RPC errors, but this issue should not be a blocker now.\nMoving this to P2.. This is reverted to only fix the connectivity state. Ready for review.. fixed by #1665. With the current code, right, there's no way to set a custom polling frequency for DNS.\nAn easy workaround would be to add a new function dns.NewBuilderWithFreq(freq time.Duration) (similar to this). Users can create a new DNS builder with this, and register it to override the default DNS builder.\nAnother problem here is, it's very hard to find the perfect polling frequency.\nI'm working on a change to make resolver re-resolve the name whenever a connection goes down. With this, we plan to remove the polling frequency in resolver.\nBut the resolver won't re-resolve if all connections are working. Does this work for K8S?. > it will not handle cases when service is scaled up and new pods are ready to serve requests\nIMO a better solution for this would be to push the updates instead of polling. But that's out of the scope of DNS.\ncc @ejona86 for more inputs.. @raliste Are you using an old version of gRPC? We do try to re-resolve whenever a subconn goes into transient failure.. @raliste I'm not sure I understand the problem.\nThe current behavior is, whenever a subconn goes into transient failure, resolver does a re-resolve. This should cover the case where all subconns go into transient failure. Can you give more explanation?. @raliste When a subconn is in transient failure, it will keep retrying. So the state will go back to connecting, and then transient failure, and a re-resolve will happen.\nIf you still have the problem, can you give a bit more information about what you did? Like what's your environment setup? And are you using the default dns resolver?\nIt would also be good to file a new issue instead of discussing here. Thanks!. @elvizlai What you described is tracked by #1795. Will have a fix shortly.. During fixit week, not flaky in 2000 times. Closing now.. Thanks for filing the issue. The fix for identity is in #1664.. @jadekler we still return unexpected code in function convertCode, including OutOfRange.\nhttps://github.com/grpc/grpc-go/blob/583a6303969ea5075e9bd1dc4b75805dfe66989a/go17.go#L76-L85. The draining message was added to support server graceful close. The error message was valid at that time. It was reused when an address is removed by a balancer, ant the message became invalid and confusing in this.\nI filed #1844 to fix the error message. But it would be a behavior change, and since you are doing string matching in tests, you might also need to update your tests accordingly. Let me know what you think on the PR.\nThe fix (https://github.com/grpc/grpc-go/commit/8ff8683602df4fb6a85c22f2da944bfc8fc73c50#diff-3081f1aaaf958d7d5e00ccec7a62ea6fR413) you mentioned was to implement transparent retry.\nWith transparent retry, when an RPC fails because the picked connection is draining, the RPC will be retried one more time. Since ClientConn always tries to pick a healthy connection to send the RPC, in this retry, another connection would be picked and the RPC will succeed if the second picked connection is working. So even though you may see that the problem is \"fixed\", it's not. It's just made to be less likely to happen.. This channel is used to send address updates from balancer to ClientConn. ClientConn will create/close connections based the received addresses.\nThus, if there's a new update to notify ClientConn before the previous one gets processed, the old one is not needed anymore. Only the latest update matters.\nOne other thing to mention here is, balancer.go is the old balancer APIs. If you are trying to create your own custom balancer, please take a look at the new balancer package.. \ud83d\ude02 Fixed.. Could you provide a reproduction for this?\nI tried with a streaming helloworld example, the behavior was as expected (GracefulStop blocks until stream is done).. Do you have some special need for resolver.ClientConn.NewAddress() with an empty array?\nThis should not be necessary because upon shutting down, all the connections will be closed.\nI will make a fix for the divide by zero panic.. I created a PR for the two problems you mentioned: https://github.com/grpc/grpc-go/pull/1684\nCan you give it a try? Thanks!. The fix was merged. But we want to make the behavior more consistent.\nThe change makes NewSubConn() with empty list return an error, and makes UpdateAddress() with empty list tear the addrConn down. The SubConn won't be usable after this.\nEventually we want to make the SubConn with empty address list stay in TransientFailure but still usable, and balancer still can update this SubConn's address.\nThis will be done in a follow-up PR. And let's keep this issue open to track this.\n. Closing this issue during clean-up.\nIt's covered by #1742. Thanks for the investigation!\nIn step 3, when the previous Send failed with an error, why would Send be called again later? Is there another caller of Send?\n(In the code you linked for (x *watchWatchServer) Send, the sendLoop returns when Send fails.)\n\nconcurrent client writes call (ss *serverStream==0x123) SendMsg.\n\nI'm not sure if I understand this sentence correctly.\nConcurrent SendMsg on the same stream is not safe.. @gyuho Just did v1.7.4 and v1.8.1 with this fix.. Can you give some more information about what you plan to do and what you will need in the picker implementation?\nFor example, does a slice of SubConns work for you? Or you will need a map from address to SubConn?. A map[Address]SubConn will be provided when a new picker is built. While building the picker, you can calculate the rendezvous weight and store that in the picker. IMO this should be acceptable.\nI created a tentative solution in #1707, please take a look and let me know if that works.. Did you look at the new balancer package? Or the v1 balancer in grpc package?\nCan you try to use the new balancer and resolver\ngo\nrr := balancer.Get(\"round_robin\")\ngrpc.Dial(\"dns:///your.target.name\", // \"dns:///\" specifies the resolver to use\n    grpc.WithCredentials(...),\n    grpc.WithBalancerBuilder(rr), // use round_robin balancer\n)\nNot that WithBalancerBuilder is for testing only. I'm planning to add a dial option to set the balancer (#1697).. The resolve interval is decided by each resolver implementation. There are resolvers that do pushing instead of polling.\nSo a WithResolveNowInterval(time.Duration) DialOption doesn't look like a good idea IMO.\nA possible solution would be to create another DNS resolver with a custom resolve interval, as I mentioned in https://github.com/grpc/grpc-go/issues/1663#issuecomment-343555883.\nFrom your comment in #1388, you mentioned dead connections will still be retried. This can be solved by #1679. The resolver will re-resolve whenever a connection is down. If the dead server was removed in DNS, the re-resolve will notice that and will remove it from ClientConn.\nMAX_CONNECTION_AGE plus #1679 would also cause the resolver to re-resolve and discover new servers.\nLet me know what you think about this solution.. Does the max age problem still exist? Did you get more logs for this issue?. @mpuncel Yes.. This doesn't work in go1.6 because Context() doesn't exist. We had similar problems like this, so there are go16.go and go17.go.\nOne solution would be to add a contextFromRequest() function, which returns background for go1.6, and req.Context() for >=go1.7.. This might be caused by this select in Write()\nhttps://github.com/grpc/grpc-go/blob/c6b46087ab923e9f453ec433f99174cdd45b9b89/transport/http2_client.go#L640-L648. The dial target syntax is scheme://authority/endpoint_name as defined in grpc/naming.md.\nThe target you dialed to (tcp://0.0.0.0:46658) couldn't be parsed correctly, so no connection can be established.\nI believe tcp:///0.0.0.0:46658 (note the triple /) or just 0.0.0.0:46658 would work. Please give it a try.. Closing this. Filed a new issue #1741 to track scheme \"unix\".. When a ClientConn is closed, the resolver wrapper will also be closed, and the goroutines will exit. It would be good to get more information on what happened before making this fix.\nWhat is TestDialNoTimeout testing? Will the gRPC ClientConn be created? Or the Dial will fail?\nI tried TestDialNoTimeout at etcd's HEAD, but got a leak. Not sure if it's related..\n```\n$ go test -v -run TestDialNoTimeout\n=== RUN   TestDialNoTimeout\nINFO: 2017/12/15 14:48:17 dialing to target with scheme: \"\"\n--- PASS: TestDialNoTimeout (0.00s)\nPASS\nToo many goroutines running after all test(s).\n1 instances of:\ngithub.com/coreos/etcd/clientv3.(healthBalancer).notifyAddrs(...)\n    /usr/local/google/home/menghanl/go/src/github.com/coreos/etcd/clientv3/health_balancer.go:371 +0x1b1\ngithub.com/coreos/etcd/clientv3.(healthBalancer).updateNotifyLoop(...)\n    /usr/local/google/home/menghanl/go/src/github.com/coreos/etcd/clientv3/health_balancer.go:323 +0x481\ncreated by github.com/coreos/etcd/clientv3.newHealthBalancer\n    /usr/local/google/home/menghanl/go/src/github.com/coreos/etcd/clientv3/health_balancer.go:128 +0x375\n1 instances of:\ngithub.com/coreos/etcd/clientv3.(*healthBalancer).updateUnhealthy(...)\n    /usr/local/google/home/menghanl/go/src/github.com/coreos/etcd/clientv3/health_balancer.go:236 +0x101\ngithub.com/coreos/etcd/clientv3.newHealthBalancer.func1(...)\n    /usr/local/google/home/menghanl/go/src/github.com/coreos/etcd/clientv3/health_balancer.go:132 +0x5a\ncreated by github.com/coreos/etcd/clientv3.newHealthBalancer\n    /usr/local/google/home/menghanl/go/src/github.com/coreos/etcd/clientv3/health_balancer.go:130 +0x3b5\nexit status 1\nFAIL    github.com/coreos/etcd/clientv3 0.093s\n``. Do you still see this problem? Do you have any update on this?\nIf this is still happening, there might be a bug when closing resolvers.. Added a DialOptionWithResolverUserOptions` in #1711. Please take a look.. Can you also sign the CLA? Thanks!. Merged! Thanks!. The timeout is set to a test value at the beginning of the test, and reset in a defer.\nWhile defer is running, ClientConn hasn't completely stopped, and addrConn is reading the timeout value.. \n\nReview status: 0 of 2 files reviewed at latest revision, 2 unresolved discussions.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 2 files at r1, 1 of 2 files at r2.\nReview status: 1 of 2 files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 2 files at r2.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. @aequitas What's the target you are dialing to?\nI assume it is \"unix://etcd-address\" for unix domain socket and \"http://etcd-address\" for tcp, is that right?\nIf what I assumed is correct. I think the problem here is that, the etcd dialer you pointed to assumes gRPC doesn't parse the target, so it will still receive host as \"unix://etcd-address\".\nBut this is no longer true because gRPC parses the target and passes ~\"etcd-address\"~ empty string without the scheme to it.\nPlease try this workaround and see if it works: use \"passthrough:///unix://etcd-address\" as the target you are dialing to.. @aequitas At this stage, I think it would be better to file an issue in etcd repo and ask about this. It's not clear to me what might have caused this problem.\nPlease leave a comment here if anything is needed from gRPC. Thanks.. @jadekler The reason is, other client behaviors depend on the fact that retrying always happens, for example, resolver re-resolving (https://github.com/grpc/grpc-go/issues/1795#issuecomment-365098419)\n. FYI, just did 1.7.5 release.. The log you are referring to is a Info log. Does it work for you to ignore all the Info logs?\nOr if you are only interested in Errors,\ngrpclog.NewLoggerV2WithVerbosity(ioutil.Discard, ioutil.Discard, os.Stderr, 0) would work.. The cpu is not saturated with one unary ping-pong.\nIf you create more concurrent RPCs, you will see more QPS.\nRunning our benchmark with 1-byte request 1-byte response for 10 seconds shows over 7000 QPS:\n```\n$ go run benchmark/benchmain/main.go -benchtime=10s -workloads=unary -compression=off -maxConcurrentCalls=1 -trace=off  -reqSizeBytes=1 -respSizeBytes=1 -networkMode=Local\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1B-respSize_1B-Compressor_false: \n50_Latency: 133.1520 \u00b5s     90_Latency: 149.2080 \u00b5s     99_Latency: 188.4030 \u00b5s     Avg latency: 133.4440 \u00b5s    Count: 74822    7748 Bytes/op   201 Allocs/op \nHistogram (unit: \u00b5s)\nCount: 74822  Min:  78.8  Max: 4259.3  Avg: 133.44\n\n[     78.843000,      78.844000)      1    0.0%    0.0%\n[     78.844000,      78.848441)      0    0.0%    0.0%\n[     78.848441,      78.872606)      0    0.0%    0.0%\n[     78.872606,      79.004092)      0    0.0%    0.0%\n[     79.004092,      79.719529)      0    0.0%    0.0%\n[     79.719529,      83.612338)      3    0.0%    0.0%\n[     83.612338,     104.793739)   3831    5.1%    5.1%  #\n[    104.793739,     220.045163)  70441   94.1%   99.3%  #########\n[    220.045163,     847.146781)    536    0.7%  100.0%\n[    847.146781,    4259.308000)      9    0.0%  100.0%\n[   4259.308000,            inf)      1    0.0%  100.0%\n```\nIf concurrent RPC count is increased to 10 (by changing -maxConcurrentCalls=10), QPS is about 40000.\n```\n$ go run benchmark/benchmain/main.go -benchtime=10s -workloads=unary -compression=off -maxConcurrentCalls=10 -trace=off  -reqSizeBytes=1 -respSizeBytes=1 -networkMode=Local\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_10-reqSize_1B-respSize_1B-Compressor_false: \n50_Latency: 218.5850 \u00b5s     90_Latency: 287.4020 \u00b5s     99_Latency: 710.9700 \u00b5s     Avg latency: 237.5280 \u00b5s    Count: 420160   7101 Bytes/op   148 Allocs/op \nHistogram (unit: \u00b5s)\nCount: 420160  Min:  92.7  Max: 4357.5  Avg: 237.53\n\n[     92.660000,      92.661000)       1    0.0%    0.0%\n[     92.661000,      92.665453)       0    0.0%    0.0%\n[     92.665453,      92.689738)       0    0.0%    0.0%\n[     92.689738,      92.822169)       0    0.0%    0.0%\n[     92.822169,      93.544349)       0    0.0%    0.0%\n[     93.544349,      97.482580)       0    0.0%    0.0%\n[     97.482580,     118.958762)     141    0.0%    0.0%\n[    118.958762,     236.073878)  277758   66.1%   66.1%  #######\n[    236.073878,     874.732587)  141704   33.7%   99.9%  ###\n[    874.732587,    4357.502000)     555    0.1%  100.0%\n[   4357.502000,            inf)       1    0.0%  100.0%\n```.  > https fetch failed: Get google.golang.org/grpc?go-get=1: x509: failed to load system roots and no roots provided\ngo get failed to load the roots.\nPlease try to install certs or file an issue for golang if it doesn't work.. This happened again: https://travis-ci.org/grpc/grpc-go/jobs/378915747. This is a duplicate of #1633. The cert is checked, but grpc.Dial doesn't return the error.\nWe will have a cleanup on ClientConn to handle this. Tracked in #1742.. Duplicate of #468. Please set up VPN and try again.\nOr, do a git clone instead:\ngit clone https://github.com/grpc/grpc-go.git $GOPATH/src/google.golang.org/grpc\nThis won't download gRPC's dependencies (and unfortunately, many of the dependencies are hosted on golang.org). So you will need to clone them, too. . The default resolver was changed from \"dns\" back to \"passthrough\" in #1606. The reason is that if we change the default resolver behavior, user's tests might be broken.\nA \"passthrough\" resolver doesn't try to resolve the target, and will return the target back directly. The target will eventually be resolved by net.Dial, but there will be no load balancing in this case.\nTo use the \"dns\" resolver, please try to add \"dns:///\" prefix to your target. (The target format is defined in this doc). \nresolver.SetDefaultScheme sets the default target scheme. If you are sure you want \"dns\" to be the default in your program, doing this in an init() would also be a good idea.\nSorry for the missing of documentation. I will add a doc to DialContext to explain how to pick the resolver (#1785).. stats handler is not fully support in ServeHTTP Handler now.\nTagRPC would need to be called (like this) when a new stream is created.\nAlso, HandleRPC for headers (incoming, outgoing) and trailers (outgoing) are missing.\nSorry, we currently don't have cycles to support this. It would be great if anyone wants to help on this!. What exactly do you mean by \"a server that does not exist\"?\nI tried with target \"nothing.addr\" which clearly doesn't exist, and got error rpc error: code = Unavailable desc = all SubConns are in TransientFailure.\nDo you have logs from your program? You can get logs by setting environment variable GRPC_GO_LOG_SEVERITY_LEVEL=info to get logs including info.\nIt will also be great if you can provide a reproduction for this so we can look into what's happening.. Friendly ping. Did you get time for the repro?. This is a valid concern, thanks for bringing this up.\nAnd there's actually one more case where this could happen.\nThis happens when there's no retry happening in the ClientConn, could be caused by one of the following reasons:\n1. Resolver returned an empty list\n1. All addrConns stopped retrying because of non-temporary errors\nFor \"Resolver returned an empty list\". It could be a valid address list because the resolver may want to delete the previous returned list. To solve the re-resolve problem, there are two possible solutions:\n1. The resolver keeps retry itself (This would be a cleaner solution because some resolvers don't care because they don't pull from the server)\n1. ClientConn start a goroutine to trigger it (This would end up with something like a retry with exponential backoff)\nFor \"All addrConns stopped for non-temporary errors\". I think the solution would be to keep retrying on those connections. (Filed #1856 for this). >  I do not want to specify repo path.\nMy understanding is that the suggested way is to set the go_package option to the import path, which will likely be something containing github.com/user/project.\nAlso, this is not a gRPC specific question.\nPlease file an issue against the protobuf repo: https://github.com/golang/protobuf. Filed #1807 for this issue. Please take a look.. Closing as discussed in #1812. This is caused by a mutex deadlock in the glog logging package. But I could not find why the deadlock happened.\nA solution is to not use glog in our tests.\nRelated: we could try to use testing.T.Logf as the logger for each individual test.. Closing PR. See related discussion in #1812. This will eventually stop triggering ResolveNow.\nWe would still like retrying on those connections with backoff. Closing now.. \nReview status: 0 of 13 files reviewed at latest revision, 14 unresolved discussions.\n\nclientconn.go, line 644 at r1 (raw file):\n\n```Go\n  balancerWrapper *ccBalancerWrapper\nid int64 // channelz unique identification number\n```\n\nchannelzID\n\nclientconn.go, line 831 at r1 (raw file):\n\nGo\n// ChannelzMetrics returns ChannelMetric of current ClientConn.\n// This is an EXPERIMENTAL API.\nfunc (cc *ClientConn) ChannelzMetrics() *channelz.ChannelMetric {\n\nThis function is called Metric_s_, but the return value is type Metric.\n\nclientconn.go, line 1029 at r1 (raw file):\n\n```Go\n  connectDeadline time.Time\nid int64 // channelz unique identification number\n```\n\nchannelzID\n\ngrpclb_remote_balancer.go, line 258 at r1 (raw file):\n\nGo\n  } else {\n      cc, err = Dial(\"grpclb:///grpclb.server\", dopts...)\n  }\n\ngo\nctx := context.Background()\nif channelz.IsOn() {\n  ctx = channelz.WithParentID(context.Background(), lb.opt.ChannelzParentID)\n}\ncc, err := DialContext(ctx, \"\", dopts...)\n\nserver.go, line 107 at r1 (raw file):\n\n```Go\n  serveWG  sync.WaitGroup // counts active Serve goroutines for GracefulStop\nid int64 // channelz unique identification number\n```\n\nchannelzID\n\nserver.go, line 1232 at r1 (raw file):\n\nGo\n  s.mu.Lock()\n  if channelz.IsOn() {\n      channelz.RemoveEntry(s.id)\n\nIs it a problem if RemoveEntry is called multiple times with the same id?\n\nchannelz/types.go, line 3 at r1 (raw file):\n\nGo\n/*\n *\n * Copyright 2017 gRPC authors.\n\n2018\n\nchannelz/types.go, line 127 at r1 (raw file):\n\nGo\n  ChannelzMetrics() *SocketMetric\n  IncrMsgSent()\n  IncrMsgRecv()\n\nThese two msg related functions should not be part of this interface. They should be defined as part of the transport package.\n\nchannelz/service/service.go, line 19 at r1 (raw file):\n\n```Go\n */\npackage service\n```\n\nDelete this file and add it later?\n\ntransport/http2_client.go, line 117 at r1 (raw file):\n\n```Go\n  goAwayReason GoAwayReason\nid int64 // channelz identification number\n```\n\nchannelzID\n\ntransport/http2_client.go, line 318 at r1 (raw file):\n\nGo\n      t.mu.Lock()\n      if t.state != closing {\n          t.id = channelz.RegisterSocket(t, channelz.NormalSocketT, opts.ParentID, \"\")\n\nThis registering should happen at the same place as stats handler. Move this before go t.reader().\nAlso in that case, t.id doesn't need to be protected by t.mu because nobody will modify it.\n\ntransport/http2_client.go, line 615 at r1 (raw file):\n\nGo\n  if channelz.IsOn() {\n      t.mu.Lock()\n      if t.id != 0 {\n\nThe mutex should not be necessary because id won't be changed.\n\ntransport/http2_server.go, line 98 at r1 (raw file):\n\n```Go\n  bdpEst            *bdpEstimator\nid int64 // channelz unique identification number\n```\n\nchannelzID\n\ntransport/http2_server.go, line 238 at r1 (raw file):\n\nGo\n      if err != nil {\n          t.Close()\n      } else if channelz.IsOn() {\n\nMove to the same place as stats.ConnBegin\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 13 files reviewed at latest revision, 8 unresolved discussions.\n\ngrpclb_remote_balancer.go, line 252 at r2 (raw file):\n\nGo\n  // DialContext using manualResolver.Scheme, which is a random scheme generated\n  // when init grpclb. The target name is not important.\n  var err error\n\nThese two var definitions are unnecessary.\n\nserver.go, line 470 at r2 (raw file):\n\n```Go\n}\ntype listenSocket struct {\n```\n\nWhy is this listenSocket needed?\nAnd there is no way to record numbers of bytes/messages received on this listenSocket.\nIsn't it enough to have channelz socket for connections received on this listener.\n\nchannelz/funcs.go, line 40 at r2 (raw file):\n\nGo\n// TurnOn turns on channelz data collection.\n// This is an EXPERIMENTAL API.\nfunc TurnOn() {\n\nIf TurnOn is called multiple times, new storage will replace the old one. Is this by design?\n\nchannelz/funcs.go, line 71 at r2 (raw file):\n\nGo\n// and returns pointer to the data storage for read-only access.\n// This is an EXPERIMENTAL API.\nfunc NewChannelzStorage() {\n\nDoes this need to be exported?\n\ntest/channelz_test.go, line 3 at r2 (raw file):\n\nGo\n/*\n *\n * Copyright 2017 gRPC authors.\n\n2018\n\ntransport/transport.go, line 508 at r2 (raw file):\n\nGo\n  ReadBufferSize int\n  // ParentID sets the addrConn id which initiate the creation of this client transport.\n  ParentID int64\n\nChannelzParentID\n\nComments from Reviewable\n Sent from Reviewable.io \n. Some more comments in grpc/transport package.\nWill review the details in channelz package later.\n\nReviewed 1 of 13 files at r1, 7 of 11 files at r3.\nReview status: 8 of 13 files reviewed at latest revision, 5 unresolved discussions.\n\ngrpclb_remote_balancer.go, line 254 at r3 (raw file):\n\nGo\n  ctx := context.Background()\n  if channelz.IsOn() {\n      ctx = channelz.WithParentID(ctx, lb.opt.ChannelzParentID)\n\nWith a second thought, I think it's a bad idea to pass this to Dial in the context instead of as a DialOption. \nIn fact, the official context doc explicitly says context is not for passing optional parameters to functions: https://golang.org/pkg/context/#pkg-overview.\n\nserver.go, line 1228 at r3 (raw file):\n\nGo\n  s.mu.Lock()\n  if channelz.IsOn() {\n      channelz.RemoveEntry(s.channelzID)\n\nThis removes the server from channelz, and the following lis.Close, c.Close removes the sockets from channelz. This is a bit out of order here. Not sure if this matters much, but it would still be good to keep them consistent.\nAlso, does this have to happen inside the lock?\n\nserver.go, line 1274 at r3 (raw file):\n\n```Go\nif channelz.IsOn() {\n      channelz.RemoveEntry(s.channelzID)\n```\n\nThe server is gracefully stopping, so there might be pending RPCs on this server.\nShould this Remove happen only when the server actually stops?\n\nserver.go, line 1274 at r3 (raw file):\n\n```Go\nif channelz.IsOn() {\n      channelz.RemoveEntry(s.channelzID)\n```\n\nSimilar to Close(), the order of this and closing listener/connection is wrong.\n\nchannelz/funcs.go, line 71 at r2 (raw file):\nPreviously, lyuxuan wrote\u2026\nIt is not a necessary API for users, but since I am making the TurnOn API a one-time change, I need to export this function to facilitate testing (clear existing channelz table for new tests).\n\nNit in the comment, it doesn't return the data storage.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 13 files at r1, 3 of 4 files at r4.\nReview status: 9 of 13 files reviewed at latest revision, 1 unresolved discussion.\n\nserver.go, line 1274 at r3 (raw file):\nPreviously, lyuxuan wrote\u2026\nsame as above\n\nThis is a bit different from above.\nA running server (gracefully stopping with pending RPC) will be removed from channelz. Whether this is correct depends on the definition of a server entry in channelz. But I think the stats page should not remove the server when the server is actually running.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 9 of 13 files reviewed at latest revision, 1 unresolved discussion.\n\nserver.go, line 1274 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis is a bit different from above.\nA running server (gracefully stopping with pending RPC) will be removed from channelz. Whether this is correct depends on the definition of a server entry in channelz. But I think the stats page should not remove the server when the server is actually running.\n\nNever mind. The parent remains in channelz when there's still childred, and there's no timing issue.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 9 of 13 files reviewed at latest revision, 4 unresolved discussions.\n\nchannelz/funcs.go, line 265 at r4 (raw file):\n\n```Go\n// removeChildFromParent must be called where lock on channelMap is already held.\nfunc (c *channelMap) removeChildFromParent(pid, cid int64) {\n```\n\nI think this (and many other functions like this) is doing too much work on it's own, and is too primitive.\nBetter abstraction should be done for them.\nThink more about Object Oriented, and create classes for the components.\nThis function should be refactored into functions and methods. So it would end up with something like\n```go\nfunc removeEntry(id int64) {\n  c := findEntry(id)\n  p := findEntry(c.parent)\n  p.deleteEntry(cid)\nif p is not refered by any other children:\n    removeEntry(pid)\n}\n```\n\nchannelz/types.go, line 66 at r4 (raw file):\n\nGo\n  LastCallStartedTimestamp time.Time\n  // trace\n  NestedChans map[int64]string\n\nI think this type is mixing fields to be set by the implementation of interface Channel and fields to be set by channelz package together.\nIf you don't expect the implementation to know this field, you should keep it separate from the stats to be collected by the Channelz implementation.\n\nchannelz/types.go, line 92 at r4 (raw file):\n\n```Go\n}\nfunc (c *channel) Lock() {\n```\n\nAre those Lock() Unlock() called by anyone?\nIf the caller really needs those functions, I think getter/setter would be more appropriate.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 4 files at r4, 4 of 10 files at r5, 2 of 4 files at r7.\nReview status: 9 of 12 files reviewed at latest revision, 7 unresolved discussions.\n\nserver.go, line 512 at r7 (raw file):\n\nGo\n          <-s.done\n      default:\n          s.channelzRemoveOnce.Do(func() {\n\nHow about registering channelz at the beginning of serve and remove when serve returns?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 2 files at r6.\nReview status: 10 of 12 files reviewed at latest revision, 6 unresolved discussions.\n\nchannelz/types.go, line 41 at r7 (raw file):\n\nGo\n  // canDelete check whether delete() has been called before, and whether child\n  // list is now empty. If both conditions are met, then delete self from database.\n  canDelete()\n\nCall this tryDelete?\n\nchannelz/types.go, line 79 at r7 (raw file):\n\nGo\n// child list, etc.\ntype ChannelMetric struct {\n  ID          int64\n\nComment for each field\n\nchannelz/types.go, line 147 at r7 (raw file):\n\nGo\nfunc (c *channel) delete() {\n  c.closeCalled = true\n  if len(c.subChans)+len(c.nestedChans) != 0 {\n\nCall canDelete?\n\nchannelz/types.go, line 158 at r7 (raw file):\n\n```Go\nfunc (c *channel) canDelete() {\n  if c.closeCalled && len(c.subChans)+len(c.nestedChans) == 0 {\n```\n\nChange this to early return, similar to what's done in delete.\n\nchannelz/types.go, line 201 at r7 (raw file):\n\n```Go\nfunc (sc *subChannel) canDelete() {\n  if sc.closeCalled && len(sc.sockets) == 0 {\n```\n\nChange this to early return, and call this function from delete().\n\nchannelz/types.go, line 231 at r7 (raw file):\n\nGo\n  LocalFlowControlWindow           int64\n  RemoteFlowControlWindow          int64\n  //socket options\n\nTODO: socket options?\n\nchannelz/types.go, line 234 at r7 (raw file):\n\nGo\n  LocalAddr  net.Addr\n  RemoteAddr net.Addr\n  // Security\n\nIs this a TODO?\n\nchannelz/types.go, line 311 at r7 (raw file):\n\nGo\n  CallsFailed              int64\n  LastCallStartedTimestamp time.Time\n  // trace\n\nTODO: trace?\n\nchannelz/types.go, line 356 at r7 (raw file):\n\n```Go\nfunc (s *server) canDelete() {\n  if s.closeCalled && len(s.sockets)+len(s.listenSockets) == 0 {\n```\n\nChange this, too.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 4 files at r7.\nReview status: 11 of 12 files reviewed at latest revision, 14 unresolved discussions.\n\nchannelz/funcs.go, line 183 at r7 (raw file):\n\nGo\nfunc RegisterServer(s Server, ref string) int64 {\n  id := idGen.genID()\n  db.get().addServer(id, &server{refName: ref, s: s, sockets: make(map[int64]string), listenSockets: make(map[int64]string), id: id})\n\nNit: make this consistent with other register functions (create a temp variable for &server{}).\n\nchannelz/funcs.go, line 277 at r7 (raw file):\n\n```Go\n}\nfunc (c *channelMap) findEntry(id int64) entry {\n```\n\n// c.mu must be held by the caller.\n\nchannelz/funcs.go, line 320 at r7 (raw file):\n\nGo\nfunc (c *channelMap) deleteEntry(id int64) {\n  var ok bool\n  if _, ok = c.channels[id]; ok {\n\nDon't look up, just delete?\n\nchannelz/funcs.go, line 343 at r7 (raw file):\n\n```Go\n}\nfunc (c *channelMap) removeEntry(id int64) {\n```\n\nWhat's deleteEntry and what's removeEntry? Add comments?\nAnd why are there two separate functions?\n\nchannelz/funcs.go, line 349 at r7 (raw file):\n\n```Go\n}\ntype int64Slice []int64\n```\n\nTODO: update to sort.Slice?\n\nchannelz/funcs.go, line 381 at r7 (raw file):\n\nGo\n  sort.Sort(int64Slice(ids))\n  count := 0\n  var ok bool\n\nrename this variable\n\nchannelz/funcs.go, line 386 at r7 (raw file):\n\nGo\n          break\n      }\n      if v < id {\n\nhttps://golang.org/pkg/sort/#Search\n\nchannelz/funcs.go, line 400 at r7 (raw file):\n\nGo\n  c.mu.RUnlock()\n  if count == 0 {\n      ok = true\n\nwhy is this true?\n\nchannelz/funcs.go, line 405 at r7 (raw file):\n\nGo\n  var t []*ChannelMetric\n  for _, cn := range cns {\n      cim := cn.c.ChannelzMetric()\n\nSet ID and refname?\nAnd if those are really missing here, it means they are not checked by the tests.\n\nchannelz/funcs.go, line 409 at r7 (raw file):\n\nGo\n      cm.ChannelData = cim\n      c.mu.RLock()\n      cm.NestedChans = copyMap(cn.nestedChans)\n\nShould this copy happen inside the previous c.mu.RLock()?\n\nchannelz/funcs.go, line 427 at r7 (raw file):\n\nGo\n  sort.Sort(int64Slice(ids))\n  count := 0\n  var ok bool\n\nrename\n\nchannelz/funcs.go, line 450 at r7 (raw file):\n\n```Go\nvar s []*ServerMetric\n  for _, cn := range ss {\n```\n\nNit: does cn stand for channel? Rename cn, something general as item would work.\n\nchannelz/funcs.go, line 481 at r7 (raw file):\n\nGo\n  sort.Sort((int64Slice(ids)))\n  count := 0\n  ok = false\n\nUse another variable for the return value? Reusing ok is confusing.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 3 of 4 files at r8.\nReview status: 11 of 12 files reviewed at latest revision, 4 unresolved discussions.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 22 files reviewed at latest revision, 12 unresolved discussions.\n\ncall.go, line 186 at r3 (raw file):\n\nGo\n  c.maxReceiveMessageSize = getMaxSize(mc.MaxRespSize, c.maxReceiveMessageSize, defaultClientMaxReceiveMessageSize)\n  if err := setCallInfoContentSubtypeAndCodec(c); err != nil {\n      return toRPCErr(err)\n\nThis toRPCErr call is unnecessary.\n\nrpc_util.go, line 137 at r3 (raw file):\n\nGo\n  contentSubtype                          string\n  codec                                   baseCodec\n  useResponseContentSubtypeForCodecLookup bool\n\nIs this variable used anywhere?\n\nrpc_util.go, line 264 at r3 (raw file):\n\nGo\n// over the wire will be \"application/grpc+json\". The content-subtype is\n// converted to lowercase before being included in Content-Type. See\n// Content-Type on https://grpc.io/docs/guides/wire.html#requests for more\n\nThis grpc.io link doesn't exist (why???).\nChange to https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\nThere are multiple places need to be changed.\n\nrpc_util.go, line 273 at r3 (raw file):\n\nGo\n// will result in an error with code codes.Internal.\n//\n// If CallCustomCodec is also used, that Codec will be used for all request and\n\nShould we just say CallCustomCodec overrides CallCustomCodec instead of describing the behavior here?\n\nrpc_util.go, line 540 at r3 (raw file):\n\nGo\n// setCallInfoContentSubtypeAndCodec should only be called after CallOptions\n// have been applied.\nfunc setCallInfoContentSubtypeAndCodec(c *callInfo) error {\n\nHow about following?\n```go\nif c.codec != nil {\n  // CallCustomCodec overrides CallContentSubtype.\n  return nil\n}\nif c.contentSubType == \"\" {\n  // No codec was specified by call option, use default proto.\n  c.codec = encoding.GetCodec(proto.Name)\n}\ncodec := encoding.GetCodec(c.contentSubtype)\nif codec == nil {\n  return status.Errorf(codes.Internal, \"no codec registered for content-subtype %s\", c.contentSubtype)\n}\nc.codec = codec\nreturn nil\n```\n\nrpc_util_test.go, line 29 at r3 (raw file):\n\n```Go\n\"github.com/golang/protobuf/proto\"\n```\n\nremove this blank line\n\nserver.go, line 1275 at r3 (raw file):\n\nGo\n  codec := encoding.GetCodec(contentSubtype)\n  if codec == nil {\n      return encoding.GetCodec(proto.Name)\n\nShould this return unimplemented error to the client?\nOtherwise, the server will try to use proto if content subtype is not recognized, and will return internal if unmarshal was unsuccessful.\n\nstream.go, line 146 at r3 (raw file):\n\nGo\n  c.maxReceiveMessageSize = getMaxSize(mc.MaxRespSize, c.maxReceiveMessageSize, defaultClientMaxReceiveMessageSize)\n  if err := setCallInfoContentSubtypeAndCodec(c); err != nil {\n      return nil, toRPCErr(err)\n\nThis toRPCErr call is unnecessary.\n\nencoding/encoding.go, line 90 at r3 (raw file):\n\nGo\n// servers.\n//\n// The Codec will be stored and looked up by result of its String() method,\n\nof its Name() method\n\nencoding/encoding.go, line 93 at r3 (raw file):\n\nGo\n// which should match the content-subtype of the encoding handled by the Codec.\n// This is case-insensitive, and is stored and looked up as lowercase.  If the\n// result of calling String() is an empty string, RegisterCodec will panic. See\n\nName()\n\ntransport/http_util.go, line 170 at r3 (raw file):\n\nGo\n//\n// contentType is assumed to be lowercase already.\nfunc getContentSubtype(contentType string) (string, bool) {\n\nNit: rename this to retrieveContentSubtype\n\ntransport/http_util.go, line 190 at r3 (raw file):\n\n```Go\n// contentSubtype is assumed to be lowercase\nfunc getContentTypeForSubtype(contentSubtype string) string {\n```\n\nNit: rename this to buildContentTypeForSubtype\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 9 of 23 files at r1, 3 of 3 files at r2, 14 of 14 files at r4.\nReview status: 25 of 26 files reviewed at latest revision, 7 unresolved discussions, some commit checks failed.\n\nrpc_util.go, line 287 at r4 (raw file):\n\nGo\n// String() will be used as the content-subtype in a case-insensitive manner.\n//\n// See Content-Type on https://grpc.io/docs/guides/wire.html#requests for more\n\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\nDocumentation/encoding.md, line 71 at r4 (raw file):\n\nMarkdown\nsub-type supported by a registered `Codec`, it will be used automatically for\ndecoding the request and encoding the response.  Otherwise, the request will be\nrejected with status code `Unimplemented`.\n\nChange this to reflect the current behavior and change it back when Unimplemented is implemented?\n\nencoding/encoding.go, line 94 at r4 (raw file):\n\nGo\n// This is case-insensitive, and is stored and looked up as lowercase.  If the\n// result of calling Name() is an empty string, RegisterCodec will panic. See\n// Content-Type on https://grpc.io/docs/guides/wire.html#requests for more\n\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\ntransport/http_util.go, line 52 at r4 (raw file):\n\nGo\n  // This is a valid content-type on it's own, but can also include a\n  // content-subtype such as \"proto\" as a suffix after \"+\" or \";\".\n  // See https://grpc.io/docs/guides/wire.html#requests for more details.\n\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\ntransport/http_util.go, line 161 at r4 (raw file):\n\nGo\n// The given content-type must be a valid content-type that starts with\n// \"application/grpc\". A content-subtype will follow \"application/grpc\"\n// after a \"+\" or \";\". See https://grpc.io/docs/guides/wire.html#requests\n\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\ntransport/transport.go, line 331 at r4 (raw file):\n\nGo\n// a content-subtype of \"proto\" will result in a content-type of\n// \"application/grpc+proto\". This will always be lowercase.\n// See https://grpc.io/docs/guides/wire.html#requests for more details.\n\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\ntransport/transport.go, line 573 at r4 (raw file):\n\nGo\n  // \"application/grpc+proto\". The value of ContentSubtype must be all\n  // lowercase, otherwise the behavior is undefined. See\n  // https://grpc.io/docs/guides/wire.html#requests for more details.\n\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\nComments from Reviewable\n Sent from Reviewable.io \n. \n\nReviewed 4 of 9 files at r5, 1 of 1 files at r6.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. There are two different server transport in gRPC right now. A grpc.Server use the http2 implementation we implemented, while the handler server uses the http2 implementation in x/net/http2 package.\nClient side cancellation will eventually result in a RST_STREAM to the server side. x/net/http2 package handles this and returns errClientDisconnected.\nWhile in the gRPC http2 implementation, the context in the transport will be canceled, which results in the context canceled error you saw.\nIt seems to me that the difference is deep into the transport implementation. We cannot change x/net/http2's behavior, and our handling of RST_STREAM in our transport is also reasonable IMO.\nI'm afraid there's not much we can do here.. > the connection breaks without the client noticing.\nCan you give more details on this? Like what's the environment your code runs on, and is there a proxy between the client and server?\nI tried this locally, the behavior I got is what you expected.\nOne thing I noticed different is that, PING should be sent out before HEADERS for this new stream.\nAlso, aside from the order the frames are sent out, with that PING frame sent out, if the client doesn't get ack without 5 seconds, the connection will be closed.. The error code indicates the server is not installed on your server. Can you double check the server restarted is one with the server registered?\nAlso, can you try to update your gRPC version? The latest is 1.9.2.\nI tried with the latest code, with server stopped, client got error code = Unavailable desc = all SubConns are in TransientFailure. When the server is restarted, all RPCs succeeded.. Which version of gRPC are you using?\nAnd can you double check that you set grpc.WithInsure() on the client side?\nIt would also be helpful if you can paste your client and server code or provide a small reproduction.\nI could not reproduce this issue locally with HEAD. The client I tried could successfully send RPCs after the server is restarted.. I'm not sure I fully understand your question.\nYou should always close all ClientConns after use. Otherwise there mignt be leaked resources including connections and goroutines.\nEach ClientConn maintains all the connections (TCP connections), and will retry if any of them disconnects (goroutines with infinite loops basically). If the ClientConn is not closed, these won't be released.. If an address was returned by a previous Notify and removed in the following Notify, the connection will be gracefully closed.\nFor example, a Notify with [server1, server2] followed by a Notify with [server2] will cause the connection to server1 to be gracefully closed.\nDo you have pending RPCs on the old connection that you are trying to close? The connection will be kept open until all RPCs finish.. Please revert the benchmark commits.\n\nReviewed 1 of 3 files at r1, 2 of 2 files at r3.\nReview status: 3 of 7 files reviewed at latest revision, 2 unresolved discussions, some commit checks failed.\n\ncall.go, line 46 at r3 (raw file):\n\n```Go\nvar unaryStreamDesc = &StreamDesc{ServerStreams: false, ClientStreams: false}\nfunc invoke(ctx context.Context, method string, req, reply interface{}, cc *ClientConn, opts ...CallOption) (e error) {\n```\n\nReturned error doesn't need to be named\n\nstream.go, line 493 at r3 (raw file):\n\n```Go\n}\nfunc (cs *clientStream) CloseSend() (err error) {\n```\n\nAlso, no need to name the returned error\n\nComments from Reviewable\n Sent from Reviewable.io \n. The client side net.Dial could return a <conn>, nil before listener.Accept returns the conn to the server. So from ClientConn's point of view, a connection is successfully created.\nIn your test, if you close the listener before calling DialContext, the test would pass.\nThis doesn't seem like a gRPC specific issue. It's more about the behavior between net.Dial and net.Listener.. If you don't want gRPC to consider this connection as READY, you can set WithWaitForHandshake dial option to tell gRPC to blocks until the initial settings frame is received.\nThis also makes your test pass.. This PR will remove WithResolverUserOptions as discussed in #1812.\nWe will keep this open for a week before merging. And the plan is to get this into the following release (scheduled on Feb 13) if no objection.\ncc @dzbarsky\ncc @euroelessar. Travis reported flaky deadlock in the new test testClientDoesntDeadlockWhileWritingErrornousLargeMessages\nLog data\n\n```\npanic: test timed out after 7m0s\n\ngoroutine 44670 [running]:\npanic(0xa5f640, 0xc420362900)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/runtime/panic.go:500 +0x1ae\ntesting.startAlarm.func1()\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/testing/testing.go:918 +0x14e\ncreated by time.goFunc\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/time/sleep.go:154 +0x78\n\ngoroutine 1 [chan receive, 1 minutes]:\ntesting.(*T).Run(0xc42007c300, 0xb2bdfc, 0x3a, 0xb862c0, 0x52dd01)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/testing/testing.go:647 +0x56e\ntesting.RunTests.func1(0xc42007c300)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/testing/testing.go:793 +0xba\ntesting.tRunner(0xc42007c300, 0xc4203fddc8)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/testing/testing.go:610 +0xca\ntesting.RunTests(0xb86938, 0xdfcc40, 0x5e, 0x5e, 0xc420015ba0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/testing/testing.go:799 +0x4eb\ntesting.(*M).Run(0xc4203fdf00, 0xc400000000)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/testing/testing.go:743 +0x130\nmain.main()\n    google.golang.org/grpc/test/_test/_testmain.go:240 +0x1b9\n\ngoroutine 17 [syscall, 6 minutes, locked to thread]:\nruntime.goexit()\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/runtime/asm_amd64.s:2086 +0x1\n\ngoroutine 5 [runnable]:\ngithub.com/golang/glog.(*loggingT).flushDaemon(0xe001c0)\n    /home/travis/gopath/src/github.com/golang/glog/glog.go:882 +0x9e\ncreated by github.com/golang/glog.init.1\n    /home/travis/gopath/src/github.com/golang/glog/glog.go:410 +0x23b\n\ngoroutine 44626 [select]:\ngoogle.golang.org/grpc.(*addrConn).transportMonitor(0xc42012a900)\n    /home/travis/gopath/src/google.golang.org/grpc/clientconn.go:1215 +0x82a\ngoogle.golang.org/grpc.(*addrConn).connect.func1(0xc42012a900)\n    /home/travis/gopath/src/google.golang.org/grpc/clientconn.go:827 +0x28d\ncreated by google.golang.org/grpc.(*addrConn).connect\n    /home/travis/gopath/src/google.golang.org/grpc/clientconn.go:828 +0x1db\n\ngoroutine 44636 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc420468de0, 0x152a0bfe2438, 0xc420426dc0, 0xc420462900, 0xc4204629c0, 0x4000, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc420020b40, 0xc42016a000, 0x5, 0x4000, 0xc421fe9ffb, 0xfc00d, 0xfe005, 0xc42029a27e, 0xc42029a278, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc420150a20, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420462600, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420468c00, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420462600, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420468c00, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420462600, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420468c00, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420462600, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc42029a210, 0xdc98a0, 0xc420468b10)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44609 [select]:\ngoogle.golang.org/grpc.(*ccBalancerWrapper).watcher(0xc420426940)\n    /home/travis/gopath/src/google.golang.org/grpc/balancer_conn_wrappers.go:122 +0x643\ncreated by google.golang.org/grpc.newCCBalancerWrapper\n    /home/travis/gopath/src/google.golang.org/grpc/balancer_conn_wrappers.go:113 +0x2d4\n\ngoroutine 44638 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc420308660, 0x152a0bfe2438, 0xc420426dc0, 0xc42028eea0, 0xc42028f1a0, 0x4000, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc4202a0500, 0xc42030e000, 0x5, 0x4000, 0xc421ce3ffb, 0xfc00d, 0xfe005, 0xc4203625de, 0xc4203625d8, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc4203fe360, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc42028e7e0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4203085a0, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc42028e7e0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4203085a0, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc42028e7e0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4203085a0, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc42028e7e0, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc4203624b0, 0xdc98a0, 0xc420308570)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44633 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc4204f50b0, 0x152a0bfe2438, 0xc420426dc0, 0xc420260d80, 0xc420260de0, 0x1a8008, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc42024c500, 0xc420408000, 0x5, 0x4000, 0xc4221edffb, 0xfc00d, 0xfe005, 0xc420182776, 0xc420182770, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc42045ca20, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420260c60, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f4ff0, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420260c60, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f4ff0, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420260c60, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f4ff0, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420260c60, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc420182720, 0xdc98a0, 0xc4204f4c90)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44724 [runnable]:\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1(0xc4203b56c0, 0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420410a00)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:635\ncreated by google.golang.org/grpc.(*Server).serveStreams.func1\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:638 +0xbb\n\ngoroutine 44639 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc420309a40, 0x152a0bfe2438, 0xc420426dc0, 0xc420113a40, 0xc420113aa0, 0x1a8008, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc4202a0c80, 0xc420576000, 0x5, 0x4000, 0xc42101bffb, 0xfc00d, 0xfe005, 0xc420362806, 0xc420362800, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc4203feb40, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420113920, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420309980, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420113920, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420309980, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420113920, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420309980, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420113920, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc4203627b0, 0xdc98a0, 0xc420309950)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44607 [IO wait]:\nnet.runtime_pollWait(0x152a0bfe3138, 0x72, 0x4132d3)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/runtime/netpoll.go:160 +0x5e\nnet.(*pollDesc).wait(0xc420316290, 0x72, 0x40fb18, 0xa939a0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/fd_poll_runtime.go:73 +0x5b\nnet.(*pollDesc).waitRead(0xc420316290, 0xdcc120, 0xc420014280)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/fd_poll_runtime.go:78 +0x42\nnet.(*netFD).accept(0xc420316230, 0x0, 0xdca7e0, 0xc4202a5520)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/fd_unix.go:419 +0x2b8\nnet.(*TCPListener).accept(0xc4204d2090, 0xc420032098, 0xc42037be10, 0x43827c)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/tcpsock_posix.go:132 +0x51\nnet.(*TCPListener).Accept(0xc4204d2090, 0xb86080, 0xc420295b80, 0xdd3b40, 0xc420032098)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/tcpsock.go:222 +0x50\ngoogle.golang.org/grpc.(*Server).Serve(0xc420295b80, 0xdd01e0, 0xc4204d2090, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:503 +0x250\ncreated by google.golang.org/grpc/test.(*test).startServer\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:600 +0x8db\n\ngoroutine 44637 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc42031e240, 0x152a0bfe2438, 0xc420426dc0, 0xc4204630e0, 0xc420463200, 0x4000, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc420021400, 0xc422300000, 0x5, 0x4000, 0xc4220ebffb, 0xfc00d, 0xfe005, 0xc42029a416, 0xc42029a410, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc420150ea0, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420462de0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc42031e120, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420462de0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc42031e120, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420462de0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc42031e120, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420462de0, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc42029a370, 0xdc98a0, 0xc42031e0f0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44634 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc420308990, 0x152a0bfe2438, 0xc420426dc0, 0xc420112720, 0xc420112cc0, 0x4000, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc4202a0780, 0xc42038a000, 0x5, 0x4000, 0xc421de5ffb, 0xfc00d, 0xfe005, 0xc4203625d6, 0xc4203625d0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc4203fe6c0, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420112060, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4203088d0, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420112060, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4203088d0, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420112060, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4203088d0, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420112060, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc420362580, 0xdc98a0, 0xc420308870)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44641 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc420309c50, 0x152a0bfe2438, 0xc420426dc0, 0xc420113da0, 0xc420113e00, 0x1a8008, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc4202a1180, 0xc420418000, 0x5, 0x4000, 0xc42111dffb, 0xfc00d, 0xfe005, 0xc4203628ae, 0xc4203628a8, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc4203feea0, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420113c80, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420309b90, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420113c80, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420309b90, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420113c80, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc420309b90, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420113c80, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc420362840, 0xdc98a0, 0xc4204f54d0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44722 [runnable]:\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1(0xc4203b56c0, 0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420410140)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:635\ncreated by google.golang.org/grpc.(*Server).serveStreams.func1\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:638 +0xbb\n\ngoroutine 44631 [select]:\ngoogle.golang.org/grpc/transport.loopyWriter(0x152a0bfe2438, 0xc420426dc0, 0xc4201c6a50, 0xc42037cf88)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:748 +0x6fb\ngoogle.golang.org/grpc/transport.newHTTP2Client.func3(0xc4204b2000)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:290 +0xc4\ncreated by google.golang.org/grpc/transport.newHTTP2Client\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:292 +0x19b9\n\ngoroutine 44608 [select]:\ngoogle.golang.org/grpc.(*ccResolverWrapper).watcher(0xc4203b3f80)\n    /home/travis/gopath/src/google.golang.org/grpc/resolver_conn_wrapper.go:108 +0x5e5\ncreated by google.golang.org/grpc.(*ccResolverWrapper).start\n    /home/travis/gopath/src/google.golang.org/grpc/resolver_conn_wrapper.go:94 +0x4d\n\ngoroutine 44635 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc42031ef30, 0x152a0bfe2438, 0xc420426dc0, 0xc420463bc0, 0xc420463c20, 0x1a8008, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc420021680, 0xc420444000, 0x5, 0x4000, 0xc420e17ffb, 0xfc00d, 0xfe005, 0xc42029a526, 0xc42029a520, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc4201510e0, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420463a40, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc42031ed20, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420463a40, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc42031ed20, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420463a40, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc42031ed20, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420463a40, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc42029a490, 0xdc98a0, 0xc4204f52c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44710 [runnable]:\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1(0xc4203b56c0, 0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420458c80)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:635\ncreated by google.golang.org/grpc.(*Server).serveStreams.func1\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:638 +0xbb\n\ngoroutine 44510 [runnable]:\ngoogle.golang.org/grpc/transport.loopyWriter(0x152a0bfe2438, 0xc4204e2d80, 0xc420307f20, 0xc420045f88)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:748 +0x6fb\ngoogle.golang.org/grpc/transport.newHTTP2Server.func2(0xc4204e4180)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:261 +0xc4\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:263 +0x160c\n\ngoroutine 38640 [semacquire]:\nsync.runtime_Semacquire(0xc42016492c)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/runtime/sema.go:47 +0x30\nsync.(*WaitGroup).Wait(0xc420164920)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/sync/waitgroup.go:131 +0xbf\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages(0xc4203ae0c0, 0xb15d61, 0xb, 0xb10060, 0x3, 0xb10063, 0x3, 0x0, 0x0, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6041 +0x44c\ngoogle.golang.org/grpc/test.TestClientDoesntDeadlockWhileWritingErrornousLargeMessages(0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6006 +0x17a\ntesting.tRunner(0xc4203ae0c0, 0xb862c0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/testing/testing.go:610 +0xca\ncreated by testing.(*T).Run\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/testing/testing.go:646 +0x530\n\ngoroutine 44709 [select]:\ngoogle.golang.org/grpc/transport.(*recvBufferReader).read(0xc42230d3b0, 0xc420128870, 0x5, 0x5, 0x43e01a, 0xc420365a00, 0xc420458af8)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:132 +0x5ac\ngoogle.golang.org/grpc/transport.(*recvBufferReader).Read(0xc42230d3b0, 0xc420128870, 0x5, 0x5, 0x0, 0x0, 0xc42037c5e8)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:121 +0xdb\ngoogle.golang.org/grpc/transport.(*transportReader).Read(0xc42031bd10, 0xc420128870, 0x5, 0x5, 0xb872d8, 0xc420292440, 0xc420458b04)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:408 +0x74\nio.ReadAtLeast(0xdc9ba0, 0xc42031bd10, 0xc420128870, 0x5, 0x5, 0x5, 0xc42037c6c0, 0x9285c9, 0xc4204e4180)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/io/io.go:307 +0xb2\nio.ReadFull(0xdc9ba0, 0xc42031bd10, 0xc420128870, 0x5, 0x5, 0x8, 0x0, 0x2)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/io/io.go:325 +0x73\ngoogle.golang.org/grpc/transport.(*Stream).Read(0xc420458a00, 0xc420128870, 0x5, 0x5, 0x20, 0xc42037c828, 0x4122f8)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:392 +0x186\ngoogle.golang.org/grpc.(*parser).recvMsg(0xc420128860, 0x400, 0x0, 0xc42029bd70, 0x6, 0x10000000043dc97, 0xc4204f4090, 0xc42037c8d0)\n    /home/travis/gopath/src/google.golang.org/grpc/rpc_util.go:336 +0xa3\ngoogle.golang.org/grpc.(*Server).processUnaryRPC(0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420458a00, 0xc4203b3830, 0xdf2a78, 0x0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:843 +0x3ee\ngoogle.golang.org/grpc.(*Server).handleStream(0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420458a00, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:1142 +0xa8b\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1(0xc4203b56c0, 0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420458a00)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:637 +0xb9\ncreated by google.golang.org/grpc.(*Server).serveStreams.func1\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:638 +0xbb\n\ngoroutine 44511 [select]:\ngoogle.golang.org/grpc/transport.(*http2Server).keepalive(0xc4204e4180)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:955 +0x66a\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:264 +0x1631\n\ngoroutine 44630 [IO wait]:\nnet.runtime_pollWait(0x152a0bfe2e38, 0x72, 0x4132d3)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/runtime/netpoll.go:160 +0x5e\nnet.(*pollDesc).wait(0xc420317cd0, 0x72, 0x40fb18, 0xa939a0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/fd_poll_runtime.go:73 +0x5b\nnet.(*pollDesc).waitRead(0xc420317cd0, 0xdcc120, 0xc420014280)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/fd_poll_runtime.go:78 +0x42\nnet.(*netFD).Read(0xc420317c70, 0xc420454400, 0x400, 0x400, 0x0, 0xdcc120, 0xc420014280)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/fd_unix.go:243 +0x1d9\nnet.(*conn).Read(0xc4204d20e8, 0xc420454400, 0x400, 0x400, 0x102, 0xc42045440d, 0x20454405)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/net/net.go:173 +0x97\nnet.(*TCPConn).Read(0xc4204d20e8, 0xc420454400, 0x400, 0x400, 0xc420335598, 0x603f79, 0xc420335598)\n    :69 +0x73\ncrypto/tls.(*block).readFromUntil(0xc4201c6360, 0x152a0bfe3798, 0xc4204d20e8, 0x5, 0xc4204d20e8, 0x140000000002)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/crypto/tls/conn.go:476 +0x10d\ncrypto/tls.(*Conn).readRecord(0xc420296a80, 0xb87217, 0xc420296b88, 0x5c48a4)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/crypto/tls/conn.go:578 +0x13d\ncrypto/tls.(*Conn).Read(0xc420296a80, 0xc4222ec000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/crypto/tls/conn.go:1113 +0x186\nbufio.(*Reader).fill(0xc4204a8360)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/bufio/bufio.go:97 +0x197\nbufio.(*Reader).Read(0xc4204a8360, 0xc4200fc2d8, 0x9, 0x9, 0xc420335b88, 0x43e23c, 0xc420308188)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/bufio/bufio.go:209 +0x6ac\nio.ReadAtLeast(0xdc8da0, 0xc4204a8360, 0xc4200fc2d8, 0x9, 0x9, 0x9, 0xc420335c01, 0xc420335c98, 0x922047)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/io/io.go:307 +0xb2\nio.ReadFull(0xdc8da0, 0xc4204a8360, 0xc4200fc2d8, 0x9, 0x9, 0x0, 0x0, 0x465a72)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/io/io.go:325 +0x73\ngolang.org/x/net/http2.readFrameHeader(0xc4200fc2d8, 0x9, 0x9, 0xdc8da0, 0xc4204a8360, 0x0, 0x0, 0xc4204b2000, 0xc420335d1e)\n    /home/travis/gopath/src/golang.org/x/net/http2/frame.go:237 +0xa3\ngolang.org/x/net/http2.(*Framer).ReadFrame(0xc4200fc2a0, 0xc4203098c0, 0xc4203098c0, 0xc420335ec8, 0x1)\n    /home/travis/gopath/src/golang.org/x/net/http2/frame.go:492 +0xf2\ngoogle.golang.org/grpc/transport.(*http2Client).reader(0xc4204b2000)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:1180 +0x20f\ncreated by google.golang.org/grpc/transport.newHTTP2Client\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:258 +0x137c\n\ngoroutine 44640 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc4204f55f0, 0x152a0bfe2438, 0xc420426dc0, 0xc4202611a0, 0xc420261200, 0x1a8008, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc42024c640, 0xc42057a000, 0x5, 0x4000, 0xc420f19ffb, 0xfc00d, 0xfe005, 0xc4201828d6, 0xc4201828d0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc42045cc60, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420260fc0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f5500, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420260fc0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f5500, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420260fc0, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f5500, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420260fc0, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc420182880, 0xdc98a0, 0xc4204f53e0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44708 [select]:\ngoogle.golang.org/grpc/transport.(*recvBufferReader).read(0xc42230d220, 0xc420128830, 0x5, 0x5, 0x43e01a, 0xc4200f69c0, 0xc420458878)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:132 +0x5ac\ngoogle.golang.org/grpc/transport.(*recvBufferReader).Read(0xc42230d220, 0xc420128830, 0x5, 0x5, 0x0, 0x0, 0xc42037c5e8)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:121 +0xdb\ngoogle.golang.org/grpc/transport.(*transportReader).Read(0xc42031bb30, 0xc420128830, 0x5, 0x5, 0xb872d8, 0xc42046f480, 0xc420458884)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:408 +0x74\nio.ReadAtLeast(0xdc9ba0, 0xc42031bb30, 0xc420128830, 0x5, 0x5, 0x5, 0xc42037c6c0, 0x9285c9, 0xc4204e4180)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/io/io.go:307 +0xb2\nio.ReadFull(0xdc9ba0, 0xc42031bb30, 0xc420128830, 0x5, 0x5, 0x8, 0x0, 0x2)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/io/io.go:325 +0x73\ngoogle.golang.org/grpc/transport.(*Stream).Read(0xc420458780, 0xc420128830, 0x5, 0x5, 0x20, 0xc42037c828, 0x4122f8)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/transport.go:392 +0x186\ngoogle.golang.org/grpc.(*parser).recvMsg(0xc420128820, 0x400, 0x0, 0xc42029bd70, 0x6, 0x10000000043dc97, 0xc4204f4090, 0xc42037c8d0)\n    /home/travis/gopath/src/google.golang.org/grpc/rpc_util.go:336 +0xa3\ngoogle.golang.org/grpc.(*Server).processUnaryRPC(0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420458780, 0xc4203b3830, 0xdf2a78, 0x0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:843 +0x3ee\ngoogle.golang.org/grpc.(*Server).handleStream(0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420458780, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:1142 +0xa8b\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1(0xc4203b56c0, 0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420458780)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:637 +0xb9\ncreated by google.golang.org/grpc.(*Server).serveStreams.func1\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:638 +0xbb\n\ngoroutine 44512 [runnable]:\nsync.(*Mutex).Unlock(0xc42001e808)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/sync/mutex.go:102\ncrypto/tls.(*Conn).Read(0xc42001e700, 0xc4204aa000, 0x8000, 0x8000, 0x4000, 0x0, 0x0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/crypto/tls/conn.go:1155 +0x569\nbufio.(*Reader).fill(0xc4204136e0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/bufio/bufio.go:97 +0x197\nbufio.(*Reader).Read(0xc4204136e0, 0xc42025dff7, 0x9, 0x9, 0x3ff7, 0x0, 0x0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/bufio/bufio.go:209 +0x6ac\nio.ReadAtLeast(0xdc8da0, 0xc4204136e0, 0xc42025a000, 0x4000, 0x4000, 0x4000, 0xc4202ea1c0, 0x8f00004000, 0x0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/io/io.go:307 +0xb2\nio.ReadFull(0xdc8da0, 0xc4204136e0, 0xc42025a000, 0x4000, 0x4000, 0x400000000001, 0x8f, 0x0)\n    /home/travis/.gimme/versions/go1.7.6.linux.amd64/src/io/io.go:325 +0x73\ngolang.org/x/net/http2.(*Framer).ReadFrame(0xc4202ea1c0, 0xc420307d70, 0xc420307d70, 0xc4204bfe30, 0x1)\n    /home/travis/gopath/src/golang.org/x/net/http2/frame.go:500 +0x2cd\ngoogle.golang.org/grpc/transport.(*http2Server).HandleStreams(0xc4204e4180, 0xc4204dc000, 0xb860d8)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:396 +0x9e\ngoogle.golang.org/grpc.(*Server).serveStreams(0xc420295b80, 0xdd3540, 0xc4204e4180)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:645 +0x180\ngoogle.golang.org/grpc.(*Server).handleRawConn.func2()\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:590 +0x54\ngoogle.golang.org/grpc.(*Server).handleRawConn.func3(0xc42033b720, 0xc420295b80, 0x152a0bfe79e0, 0xc4204e4180)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:598 +0x35\ncreated by google.golang.org/grpc.(*Server).handleRawConn\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:600 +0x8cd\n\ngoroutine 44632 [select]:\ngoogle.golang.org/grpc/transport.(*quotaPool).get(0xc420456dc0, 0x4000, 0x152a0bfe7950, 0xc4204f4a50, 0x152a0bfe2438, 0xc420426dc0, 0xc420260960, 0xc4202609c0, 0x8, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/control.go:191 +0x42e\ngoogle.golang.org/grpc/transport.(*http2Client).Write(0xc4204b2000, 0xc42024c3c0, 0xc4222fc000, 0x5, 0x4000, 0xc420a0fffb, 0xfc00d, 0xfe005, 0xc4201826a6, 0xc4201826a0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/transport/http2_client.go:695 +0x554\ngoogle.golang.org/grpc.(*clientStream).SendMsg(0xc42045c6c0, 0xad33a0, 0xc420456ec0, 0x0, 0x0)\n    /home/travis/gopath/src/google.golang.org/grpc/stream.go:397 +0x5f2\ngoogle.golang.org/grpc.invoke(0x152a0bfe3730, 0xc420260840, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f4990, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:56 +0x164\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc420085400, 0x152a0bfe3730, 0xc420260840, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f4990, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:33 +0x217\ngoogle.golang.org/grpc.Invoke(0x152a0bfe3730, 0xc420260840, 0xb23162, 0x23, 0xad33a0, 0xc420456ec0, 0xaca440, 0xc4204f4990, 0xc420085400, 0x0, ...)\n    /home/travis/gopath/src/google.golang.org/grpc/call.go:41 +0xe0\ngoogle.golang.org/grpc/test/grpc_testing.(*testServiceClient).UnaryCall(0xc42032c090, 0x152a0bfe3730, 0xc420260840, 0xc420456ec0, 0x0, 0x0, 0x0, 0xc420182650, 0xdc98a0, 0xc4204f4960)\n    /home/travis/gopath/src/google.golang.org/grpc/test/grpc_testing/test.pb.go:386 +0x107\ngoogle.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages.func1(0xc420164920, 0xdd2e20, 0xc42032c090, 0xc420456ec0, 0xc4203ae0c0)\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6034 +0x1ed\ncreated by google.golang.org/grpc/test.testClientDoesntDeadlockWhileWritingErrornousLargeMessages\n    /home/travis/gopath/src/google.golang.org/grpc/test/end2end_test.go:6039 +0x423\n\ngoroutine 44723 [runnable]:\ngoogle.golang.org/grpc.(*Server).serveStreams.func1.1(0xc4203b56c0, 0xc420295b80, 0xdd3540, 0xc4204e4180, 0xc420410500)\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:635\ncreated by google.golang.org/grpc.(*Server).serveStreams.func1\n    /home/travis/gopath/src/google.golang.org/grpc/server.go:638 +0xbb\nFAIL    google.golang.org/grpc/test 420.132s\nok      google.golang.org/grpc/test/bufconn 1.128s\n?       google.golang.org/grpc/test/codec_perf  [no test files]\n?       google.golang.org/grpc/test/grpc_testing    [no test files]\nok      google.golang.org/grpc/test/leakcheck   9.181s\nok      google.golang.org/grpc/transport    194.468s\nmake: *** [testrace] Error 1\n```\n\n. @dim13 any update?\nIt seems the failed tests were caused by travis. This issue was hopefully fixed by #1859 . Please try to update your local packages with go get -u golang.org/x/net/http2. Or simply do go get -u google.golang.org/grpc to update all the dependencies.\nFor the accessability issue,  does this help? If not, please try setting up VPN.. With #1855, the connection errors are included in the returned RPC errors.\nCallers interested the errors could try a non-blocking dial and check the errors returned from RPCs.. \nReviewed 6 of 6 files at r1.\nReview status: all files reviewed at latest revision, 8 unresolved discussions.\n\nclientconn.go, line 1384 at r1 (raw file):\n\nGo\n// Deprecated: This error is never returned by grpc and should not be\n// referenced by users.\nvar ErrClientConnTimeout = errors.New(\"grpc: timed out when dialing\")\n\nWhy is this error moved here?\n\nstream.go, line 131 at r1 (raw file):\n\nGo\n  if mc.Timeout != nil && *mc.Timeout >= 0 {\n      // The cancel function for this context will only be called when RecvMsg\n      // returns non-nil error or another error is encountered, which means the\n\nWhat does this mean?\nAlso move the comment to where cancel is defined? (And move the definition of cancel closer to if?)\n\nstream.go, line 419 at r1 (raw file):\n\nGo\n          // because the generated code requires it.\n          if err == io.EOF && !cs.desc.ServerStreams {\n              err = nil\n\nreturn nil in the second if err == io.EOF on line 481 instead of setting it back to nil here?\nSo you can save this if, and finish() will always be called with io.EOF in the successful case.\n\nstream.go, line 421 at r1 (raw file):\n\nGo\n              err = nil\n          }\n          cs.finish(err)\n\nSo we will call finish() with nil in some cases. It should be OK though.\n\nstream.go, line 491 at r1 (raw file):\n\nGo\n  }\n  cs.sentLast = true\n  cs.t.Write(cs.s, nil, nil, &transport.Options{Last: true})\n\nAdd comment why we don't care the returned error.\n\nstats/stats_test.go, line 1228 at r1 (raw file):\n\nGo\nfunc TestClientStatsFullDuplexRPCNotCallingLastRecv(t *testing.T) {\n  count := 1\n  testClientStats(t, &testConfig{compress: \"gzip\"}, &rpcConfig{count: count, success: true, failfast: false, callType: fullDuplexStreamRPC, noLastRecv: true}, map[int]*checkFuncWithCount{\n\nAlso delete noLastRecv field from rpcConfig.\n\ntest/end2end_test.go, line 1167 at r1 (raw file):\n\nGo\n      Payload:            payload,\n  }\n  if err := stream.Send(req); err != nil && err != io.EOF {\n\nShould we send multiple times and expect io.EOF eventually?\n\ntest/end2end_test.go, line 3800 at r1 (raw file):\n\nGo\n  cc := te.clientConn()\n  tc := testpb.NewTestServiceClient(cc)\n  ctx, cancel := context.WithCancel(context.Background())\n\nConsider also defering this cancel? Otherwise a fatal below may cause this to be leaked.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \n\nReviewed 3 of 3 files at r2.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. @vitalyisaev2 Thanks for pointing this out!\nThe route_guide example does Recv until a non-nil error is returned, so it follows that\n\n\nThe user calls RecvMsg (or Recv in generated code) until a non-nil error is returned.\n\n\nUsing context.Background() is not something we would recommend, so I filed #1877 to add timeout to all context in the examples, please take a look.\nI also filed #1878 to check other background context used.. @alecthomas If what you need from the resolver is to return the hosts passed in back as resolved addresses, the solutions you suggested would all work.\nYou could also do grpc.Dial(\"etcd:///etcdhost1,etcdhost2\"), the resolver builder will receive a struct as\ngo\nresolver.Target{\n  Scheme: \"etcd\",\n  Authority: \"\",\n  Endpoint: \"etcdhost1,etcdhost2\",\n}\nNow you need to make sure the etcd resolver builder knows how to parse \"etcdhost1,etcdhost2\".\nThis will be similar to a passthrough resolver that also parses the endpoint string.. So you want to specify the etcd servers that the resolver should query to resolve the service.\nThen what you suggested grpc.Dial(\"etcd://etcdhost1,etcdhost2/foo\") is the right way.\nIn this target string, \"etcdhost1,etcdhost2\" becomes the authority, and from the naming doc\n\nThe authority indicates some scheme-specific bootstrap information, e.g., for DNS, the authority may include the IP[:port] of the DNS server to use.. This happened on my fork, no in the main repo.. Cannot reproduce with over 10000 runs. Closing.. Only looked at package alts. Will take a look at other internal packages later.\n\n\nReviewed 2 of 31 files at r1.\nReview status: 2 of 31 files reviewed at latest revision, 13 unresolved discussions.\n\ncredentials/alts/alts.go, line 22 at r1 (raw file):\n\nGo\n// encapsulates all the state needed by a client to authenticate with a server\n// using ALTS and make various assertions, e.g., about the client's identity,\n// role, or whether it is authorized to make a particular call.\n\nDeclare this package as experimental?\n\ncredentials/alts/alts.go, line 53 at r1 (raw file):\n\n```Go\nvar (\n  enableUntrustedALTS = flag.Bool(\"enable_untrusted_alts\", false, \"Enables ALTS in untrusted mode. Enabling this mode is risky since we cannot ensure that the application is running on GCP with a trusted handshaker service.\")\n```\n\nWhat's the purpose of this flag?\nAlso, I kind of don't like the idea of adding a flag for it.\nHow about moving this to constructors NewClientALTS() and NewServerALTS() as a function parameter?\n\ncredentials/alts/alts.go, line 62 at r1 (raw file):\n\nGo\n// AuthInfo exposes security information from the ALTS handshake to the\n// application.\ntype AuthInfo interface {\n\nShould we add to the comment that:\nThis interface is to be implemented by alts. Users should not need a\nbrand new implementation of this interface. For the situations like\ntesting, the new implementation should embed this interface. This allows\nalts to add new methods to this interface.\n\ncredentials/alts/alts.go, line 80 at r1 (raw file):\n\n```Go\n}\n// altsTC is the credentials required for authenticating a connection using Google\n```\n\nNit: this still says \"Google Transport Security\".\n\ncredentials/alts/alts.go, line 129 at r1 (raw file):\n\nGo\n  opts := handshaker.DefaultClientHandshakerOptions()\n  opts.TargetServiceAccounts = g.accounts\n  opts.RPCVersions = &altspb.RpcProtocolVersions{\n\nMake a static variable for versions struct so we don't need to re-create it every time?\n\ncredentials/alts/alts.go, line 139 at r1 (raw file):\n\nGo\n      },\n  }\n  chs, err := handshaker.NewClientHandshaker(ctx, hsConn, rawConn, opts)\n\nWill chs be properly closed if something goes wrong (e.g. this function returns non-nil error)?\nAlso see https://github.com/grpc/grpc-go/pull/1854 for possible leaks on streaming RPCs.\n\ncredentials/alts/alts.go, line 151 at r1 (raw file):\n\nGo\n      return nil, nil, errors.New(\"client-side auth info is not of type alts.AuthInfo\")\n  }\n  match, _ := checkRPCVersions(opts.RPCVersions, altsAuthInfo.PeerRPCVersions())\n\nDoes the underlying handshaker need the RPCVersions if the check is done here?\nIMO the check should happen before we create the secConn, so we can avoid the overhead in error cases.\nIt would also be a bit clearer if this check is done in chs.ClientHandshake, so we can save the type assertion.\n\ncredentials/alts/alts.go, line 173 at r1 (raw file):\n\nGo\n  defer cancel()\n  opts := handshaker.DefaultServerHandshakerOptions()\n  opts.RPCVersions = &altspb.RpcProtocolVersions{\n\nMake a static variable?\n\ncredentials/alts/alts.go, line 183 at r1 (raw file):\n\nGo\n      },\n  }\n  shs, err := handshaker.NewServerHandshaker(ctx, hsConn, rawConn, opts)\n\nSimilar to client side, shs clean up on errors cases.\n\ncredentials/alts/alts.go, line 195 at r1 (raw file):\n\nGo\n      return nil, nil, errors.New(\"server-side auth info is not of type alts.AuthInfo\")\n  }\n  match, _ := checkRPCVersions(opts.RPCVersions, altsAuthInfo.PeerRPCVersions())\n\nSimilar to client side on where the check should happen.\n\ncredentials/alts/utils.go, line 53 at r1 (raw file):\n\nGo\n  // The following two variables will be reassigned in tests.\n  runningOS  = runtime.GOOS\n  readerFunc = func() (io.Reader, error) {\n\nNit: rename this function to something more meaningful.\n\ncredentials/alts/utils.go, line 86 at r1 (raw file):\n\nGo\n  manufacturer, err := readManufacturer()\n  if err != nil {\n      log.Fatal(err)\n\nThis fatal will print the error returned by os.Open or cmd.Output.\nDo a Fatalf with more information? Or include more information in readerFunc.\n\ncredentials/alts/utils.go, line 99 at r1 (raw file):\n\nGo\n      return name == \"Google\"\n  default:\n      panic(platformError(runningOS))\n\nWhy is this a panic not a Fatalf?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 31 files reviewed at latest revision, 4 unresolved discussions.\n\ncredentials/alts/alts.go, line 87 at r3 (raw file):\n\nGo\n  SecurityLevel() altspb.SecurityLevel\n  // PeerServiceAccount returns the peer service account.\n  PeerServiceAccount() string\n\nWill we want to extend the return value?\n\"account\" sounds like more than just a string to me...\nIf so, make it a struct instead?\n\ncredentials/alts/alts.go, line 91 at r3 (raw file):\n\nGo\n  LocalServiceAccount() string\n  // PeerRPCVersions returns the RPC version supported by the peer.\n  PeerRPCVersions() *altspb.RpcProtocolVersions\n\nAll functions in this interface will be exposed to users, including this one.\nThis function is needed by checkRPCVersions().\nWill users also need this? If no, this should be moved to a separate (probably also unexported) interface.\n\ncredentials/alts/alts.go, line 131 at r3 (raw file):\n\nGo\nfunc (g *altsTC) ClientHandshake(ctx context.Context, addr string, rawConn net.Conn) (_ net.Conn, _ credentials.AuthInfo, err error) {\n  // Make sure flags are parsed before accessing enableUntrustedALTS.\n  once.Do(func() { flag.Parse() })\n\nShould this be moved to newALTS()?\n\nnewALTS() also reads *enableUntrustedALTS\ninit should be done in the \"package entrance\", which should be newALTS() for this package.\n\n\ncredentials/alts/utils.go, line 78 at r3 (raw file):\n\nGo\n      }\n  }\n  vmOnGCP = isRunningOnGCP()\n\nAlso move this into the sync.Once?\nThis and flag.Parse() are all part of the initialization process.\nOtherwise if users import but don't use alts, they may still get Fatal.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 1 files at r4.\nReview status: 1 of 31 files reviewed at latest revision, 1 unresolved discussion.\n\ncredentials/alts/utils.go, line 78 at r3 (raw file):\nPreviously, cesarghali (Cesar Ghali) wrote\u2026\nisRunningOnGCP was actually in init() but now I remember why I moved it here, it was one of Doug's suggestions :)\n\nI meant moving this to the same sync once for flag.Parse(). Because we don't need to do this if no alts functions are ever called.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 31 files reviewed at latest revision, 1 unresolved discussion.\n\ncredentials/alts/alts.go, line 117 at r5 (raw file):\n\nGo\n  once.Do(func() {\n      flag.Parse()\n      vmOnGCP = isRunningOnGCP()\n\nHmm, this is not thread safe... newALTS could be called in parallel.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \ncredentials/alts/alts.go, line 117 at r5 (raw file):\nPreviously, cesarghali (Cesar Ghali) wrote\u2026\nMy understanding is that Do blocks if it's called multiple times and only the first time will trigger the function:\nhttps://golang.org/src/sync/once.go?s=1137:1164#L25\n\nOK, right. I missed that.\n\nComments from Reviewable\n Sent from Reviewable.io \n. @zllak Thanks! Seems the bot is happy now. :). This proposal SGTM.\nAssuming this interface will be defined with golang/protobuf. The gogo/protobuf implementation would also need to take golang/protobuf.Message as input, and also return golang/proto.ExtensionDesc.\nI'm not familiar with those. Will it be easy to convert between golang/protobuf types and gogo/protobuf types?\n\nSome other thoughts:\nAnother possible abstraction is to separate the reflection service handler with the layer that provides the data. The reflection protocol is designed for protobuf, for example extension, but it's still potentially useful for other codecs.\nThis would also require the underlying message interchange format to support similar features as protobuf's file descriptor.\nWe can hold this and implement the proto only change you suggested first.. The ClientConn is closed before requests are sent on wire, so the requests are never sent out.\nSent only pushes the requests to a buffer, it doesn't make sure they are sent on wire.\n - In the failed tests, the ClientConn is closed (in the defer) before the requests got sent out.\n - In the case of CloseAndRecv, the client blocks until it receives something from the server. So the requests are guaranteed to be sent out.\n - In the delayed case. ClientConn is closed 1 ms later, which is long enough to send the requests.\nFor a streaming RPC, the client is expected to call Recv to read the response and also the final status.\nIf you really don't care about the server's response, you need to explicitly cancel the RPC so the remaining goroutines will be cleaned up.\nSee this comment https://github.com/grpc/grpc-go/pull/1854#issue-167849429 for more details.. I agree that documentation needs to be updated. We can track that with another issue (#1880) if you are OK. Re-purposing this issue might be confusing for other people.\n\nCloseSend is to only close the send direction, but keep receiving. The function itself is not that useful in the case of client only streaming RPC (the one you are using), because the nature of this RPC is to send a list of requests, stop sending, and receive.\nIt would make more sense if you think of a bidirectional streaming RPC, where the client could send a list of requests, stop sending (also tell the server it's done), and receive a list of responses.\nAlso, actually, CloseAndRecv is a CloseSend plus a Recv:\nhttps://github.com/grpc/grpc-go/blob/583a6303969ea5075e9bd1dc4b75805dfe66989a/examples/route_guide/routeguide/route_guide.pb.go#L317-L326. @vitalyisaev2 Thanks for taking a look!\nThat's a good point. I have updated the code accordingly. PTAL.. My understanding is that you want to do message encoding on your own, and send the encoded bytes directly.\nOne trivial way to do this to define proto messages that contain bytes only. There will still be some encoding going on in protobuf, but the overhead should be low.\nAnother way is to create and register your byte slice codec (how to use codec), the codec you will define should be simiar to this.\nTo send and receive message, you will need to convert the stream to grpc.ClientStream, and call SendMsg and RecvMsg directly.\nFor example:\ngo\nbytes := encode(msg)\nstream.(grpc.ClientStream).SendMsg(&bytes)\n. The dialContext function in transport/http2_client.go is called only when there's no dialer set. But it's never going to happen because the dialer will always be set. It's either the user's custom dialer, or the default dialer you just fixed.\nThe failed test looks unrelated to this change.. @hexfusion Will do the merge and backport next Monday.. @hexfusion @gyuho \nBackported. Please see https://github.com/grpc/grpc-go/releases/tag/v1.11.3. Yes, the context can be checked to see if the RPC is done.. This is gRPC-Go repo.\nFor gRPC-C++, please file issues against https://github.com/grpc/grpc directly.. Newline (\\n) is not allowed in ascii metadata.\nYou need to either remove the newline from metadata value, or append \"-bin\" suffix to the metadata key if the newline is important.\nSee custom-metadata related definition here.. @stevvooe There could be a solution for this issue:\nFor unix:///foo, the parsed target will be unix, which is not registered.\nIn this case, if we keep the unparsed target string unix:///foo and pass it to the default passthrough resolver, the dialer will see unix:///foo eventually, and should be able to dial successfully.\nFiled #1943 for this.. \nReviewed 4 of 14 files at r3, 4 of 18 files at r5.\nReview status: 8 of 17 files reviewed at latest revision, all discussions resolved.\n\nclientconn.go, line 878 at r5 (raw file):\n\nGo\nfunc (cc *ClientConn) incrCallsStarted() {\n  cc.czmu.Lock()\n  cc.callsStarted++\n\nMake this and other fields like this atomic?\n\npicker_wrapper.go, line 78 at r5 (raw file):\n\n```Go\n}\nfunc doneWrapper(acw *acBalancerWrapper, done func(balancer.DoneInfo)) func(balancer.DoneInfo) {\n```\n\nNit: call this doneChannelzWrapper()\n\nrpc_util.go, line 278 at r5 (raw file):\n\nGo\n  return nil\n}\n\nRevert blank changes in this file\n\nserver.go, line 1338 at r5 (raw file):\n\n```Go\ns.mu.Lock()\n  if channelz.IsOn() {\n```\n\nDelete this? It's already removed by the code above.\n\nbenchmark/benchmain/main.go, line 462 at r5 (raw file):\n\nGo\n              results.AllocedBytesPerOp(), results.AllocsPerOp(), sharedPos)\n          fmt.Println(s.BenchString())\n          // fmt.Println(s.String())\n\nDelete\n\nbenchmark/stats/stats.go, line 145 at r5 (raw file):\n\nGo\n  stats.maybeUpdate()\n  s := stats.result\n  // res := s.RunMode + \"-\" + s.Features.String() + \",\"\n\nCleanup this file\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 5 of 18 files at r5.\nReview status: 8 of 17 files reviewed at latest revision, 6 unresolved discussions, some commit checks failed.\n\ntransport/flowcontrol.go, line 126 at r5 (raw file):\n\n```Go\nfunc (f *trInFlow) getSize() uint32 {\n  return atomic.LoadUint32(&f.limit) - atomic.LoadUint32(&f.unacked)\n```\n\nMinus is not atomic, the result is potentially racy. Is it OK?\n\ntransport/http2_client.go, line 111 at r5 (raw file):\n\nGo\n  kpCount           int64\n  streamsStarted    int64\n  streamsSucceeded  int64\n\nComments on what succeed means on transport level.\n\ntransport/http2_client.go, line 626 at r5 (raw file):\n\n```Go\n}\nfunc (t http2Client) closeStream(s Stream, err error, rst bool, rstCode http2.ErrCode, st *status.Status, mdata map[string][]string, streamSucceeded bool) {\n```\n\nstreamSucceeded counts for the stream with EoS bit set for both end points, the caller of this function doesn't have the information on whether EoS was sent, right?\nHow about adding two fields to Stream for EoS sent/received, and check that in this function?\n\ntransport/http2_client.go, line 1243 at r5 (raw file):\n\nGo\n      LastMessageReceivedTimestamp:    t.lastMsgRecv,\n      LocalFlowControlWindow:          int64(t.fc.getSize()),\n      //socket options\n\nNit: either mark those as TODOs, or just remove them.\n\ntransport/http2_client.go, line 1272 at r5 (raw file):\n\nGo\n  t.controlBuf.put(&outFlowControlSizeRequest{resp})\n  select {\n  case sz := <-resp:\n\nCould this <- be blocked for a long time? It that OK?\n\ntransport/http2_client.go, line 1269 at r6 (raw file):\n\n```Go\nfunc (t *http2Client) getOutFlowWindow() int64 {\n  resp := make(chan uint32)\n```\n\nMake this buf size 1?\nOtherwise the writer goroutine may block forever if t.ctx timeout. Not sure if that can happen though...\n\ntransport/http2_server.go, line 952 at r5 (raw file):\n\nGo\n// closeStream clears the footprint of a stream when the stream is not needed\n// any more.\nfunc (t *http2Server) closeStream(s *Stream, rst bool, rstCode http2.ErrCode, hdr *headerFrame, streamSucceeded bool) {\n\nSame as the comments on client side.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 5 of 5 files at r6, 3 of 4 files at r7, 1 of 3 files at r8.\nReview status: 12 of 16 files reviewed at latest revision, all discussions resolved, some commit checks failed.\n\nchannelz/types.go, line 246 at r8 (raw file):\n\nGo\n  // The number of streams that have been started.\n  StreamsStarted int64\n  // The number of streams that have ended successfully with the EoS bit set for\n\nUpdate the comment\n\ntransport/http2_client.go, line 114 at r8 (raw file):\n\nGo\n  streamsStarted int64\n  // The number of streams that have ended successfully with the EoS bit set for\n  // both end points.\n\nUpdate the comment\n\ntransport/http2_server.go, line 119 at r8 (raw file):\n\nGo\n  streamsStarted int64\n  // The number of streams that have ended successfully with the EoS bit set for\n  // both end points.\n\nUpdate\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 8 of 12 files at r9, 1 of 1 files at r10.\nReview status: 12 of 16 files reviewed at latest revision, 3 unresolved discussions.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \n\nReviewed 6 of 6 files at r11.\nReview status: 12 of 16 files reviewed at latest revision, 3 unresolved discussions.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 12 files at r9.\nReview status: 13 of 16 files reviewed at latest revision, 3 unresolved discussions.\n\ntest/end2end_test.go, line 535 at r12 (raw file):\n\n```Go\n}\ntype listenerWrapper struct {\n```\n\nMove this and dialer wrapper to file rawConnWrapper.go and rename the file. Those are all conn related utilities.\n\ntest/end2end_test.go, line 538 at r12 (raw file):\n\nGo\n  net.Listener\n  mu    sync.Mutex\n  conns []*rawConnWrapper\n\nWhy is this a list in listener but only one pointer in dialer wrapper?\n\ntest/end2end_test.go, line 563 at r12 (raw file):\n\n```Go\n// Close stops the listener.\nfunc (l *listenerWrapper) Close() error {\n```\n\nThis is not necessary.\n\ntest/end2end_test.go, line 579 at r12 (raw file):\n\n```Go\n// Addr reports the address of the listener.\nfunc (l *listenerWrapper) Addr() net.Addr { return l.Listener.Addr() }\n```\n\nThis is not necessary.\n\ntest/end2end_test.go, line 676 at r12 (raw file):\n\n```Go\nfunc (te test) startServerWithConnControl(ts testpb.TestServiceServer) (listenerWrapper, listenFuncReplace) {\n  listen = listenWithConnControl\n```\n\nThis global function overriding is not necessary.\nAdd another argument to listenAndServe as the listen function.\n\ntest/end2end_test.go, line 825 at r12 (raw file):\n\nGo\n  // overwrite the dialer before\n  opts = append(opts, grpc.WithDialer(dw.dialer))\n  var err error\n\nJust build opts and then return te.clientConn(opts...), dw\n\ntest/rawConnWrapper.go, line 84 at r12 (raw file):\n\nGo\n// greet initiates the client's HTTP/2 connection into a state where\n// frames may be sent.\nfunc (rcw *rawConnWrapper) greet() error {\n\nSeems this function is never called, right?\nThere are other functions not called in this file.\n\ntest/rawConnWrapper.go, line 240 at r12 (raw file):\n\n```Go\n}\nfunc (rcw *rawConnWrapper) writeHeadersGRPC(streamID uint32, path string) {\n```\n\nThis and the following header functions are not called.\n\ntest/rawConnWrapper.go, line 261 at r12 (raw file):\n\n```Go\n}\nfunc (rcw *rawConnWrapper) writeData(streamID uint32, endStream bool, data []byte) error {\n```\n\nThis and the padded data functions are not called.\nDo you need to manually write data bytes?\n\ntest/rawConnWrapper.go, line 282 at r12 (raw file):\n\n```Go\n}\nfunc (rcw *rawConnWrapper) WriteGoAway(maxStreamID uint32, code http2.ErrCode, debugData []byte) error {\n```\n\nUnexport?\n\ntest/rawConnWrapper.go, line 289 at r12 (raw file):\n\n```Go\n}\nfunc (rcw *rawConnWrapper) WriteRawFrame(t http2.FrameType, flags http2.Flags, streamID uint32, payload []byte) error {\n```\n\nUnexport?\n\nComments from Reviewable\n Sent from Reviewable.io \n. We will keep the behavior of Dial to avoid possible breakages.\nIn the planned NewClient API (#1786), we will make it compliant with the RFC.. \nReviewed 4 of 4 files at r1.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nchannelz/service/service.go, line 37 at r1 (raw file):\n\n```Go\n// RegisterChannelzServiceToServer registers the channelz service to the given server.\nfunc RegisterChannelzServiceToServer(s *grpc.Server) {\n```\n\nDelete this function? Or delete the following New function?\n\nchannelz/service/service.go, line 42 at r1 (raw file):\n\n```Go\n// NewCZServer returns a ChannelzServer implementation.\nfunc NewCZServer() pb.ChannelzServer {\n```\n\ns/CZ/Channelz, or just NewServer().\n\nchannelz/service/service.go, line 52 at r1 (raw file):\n\nGo\n  switch s {\n  case connectivity.Idle:\n      return &pb.ChannelConnectivityState{pb.ChannelConnectivityState_IDLE}\n\nAvoid using unnamed fields?\ngo vet ./channelz/service/ shows warnings. But why is travis not complaining?\n\nchannelz/service/service.go, line 72 at r1 (raw file):\n\nGo\n  c.Data = &pb.ChannelData{\n      State:          connectivityStateToProto(cm.ChannelData.State),\n      Target:         cm.ChannelData.Target,\n\nCould any of those pointers be nil?\n\nchannelz/service/service.go, line 137 at r1 (raw file):\n\nGo\n  switch a.Network() {\n  case \"udp\":\n  // TODO: Address_OtherAddress{}. Need proto def for Value.\n\nindentation\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 2 of 2 files at r2.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nComments from Reviewable\n Sent from Reviewable.io \n. > I've only seen CloseSend in defer statements in all example code\nCan you point me to the example you are talking about here? Calling CloseSend in defer doesn't sound right to me. And I believe we don't have any defer CloseSend in this repo.. @zhixinwen Do you still see this problem?. Closing this issue for now. Please reply back or file a new issue if you encounter problems later.. @crosbymichael Done: https://github.com/grpc/grpc-go/releases/tag/v1.10.1. @stevvooe That sounds great! Thanks for the update!. Since go 1.9, x/net/context is a type alias to context, so you should not notice any difference when using.\nThere has been discussion on switching to the new context package, see https://github.com/grpc/grpc-go/issues/711.\nThe short conclusion is, there's currently no plan to make this change, because this is a non-issue with type alias. And the change will cause breakage for people using go versions without type alias.. Closing this issue now.\nIf you find a way to reproduce the issue, please reply to this.. Unfortunately, there is not much we can do here. Please try to set up VPN or do a manual clone as @douglarek suggested.\nSee also: https://github.com/grpc/grpc-go/issues/1780#issuecomment-355365355. The error message does show it's a timeout error:\n\ndial tcp 216.239.37.1:443: i/o timeout. \n\nReviewed 10 of 20 files at r1, 4 of 7 files at r2.\nReview status: 14 of 18 files reviewed at latest revision, all discussions resolved.\n\nchannelz/types.go, line 423 at r2 (raw file):\n\nGo\n// Security defines the interface that security protocols should implement in order\n// to provide security info to channelz.\ntype Security interface {\n\nMove this to a sub package?\nchannelz package defines the API between the clientconn and the channelz store.\nThis is more like a helper function for the clientconn to get info from creds. It's not part of the API.\n\nchannelz/types_linux.go, line 24 at r2 (raw file):\n\n```Go\n  \"syscall\"\n\"golang.org/x/sys/unix\"\n```\n\nWill this import cause the build to fail on non-linux? Or just function calls will return error?\n\nchannelz/types_linux.go, line 30 at r2 (raw file):\n\nGo\n// getter function to obtain info from fd.\ntype SocketOptionData struct {\n  Linger      *unix.Linger\n\nThis field is not defined in _windows.go, but is referenced in other files (e.g. service.go), even on windows.\n\nchannelz/util_default.go, line 1 at r2 (raw file):\n\nGo\n// +build !darwin,!dragonfly,!freebsd,!linux,!netbsd,!openbsd,!solaris !go1.9\n\nRename this file. default sounds too normal.\n\nchannelz/util_unix_go19.go, line 1 at r2 (raw file):\n\nGo\n// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n\nWhy do we need this build constraint?\nAnd why is this not just windows or not?\n\nchannelz/util_unix_go19.go, line 29 at r2 (raw file):\n\n```Go\n// GetSocketOption gets the socket option info of the conn.\nfunc GetSocketOption(socket interface{}) *SocketOptionData {\n```\n\nThis is also a helper function, not part of the API.\nIn other words, this is not necessary for channelz, the clientconn could generate the same result without calling this function.\n\nchannelz/service/service.go, line 38 at r2 (raw file):\n\n```Go\nconst (\n  secToNano  = 1e9\n```\n\nMake a function to do the time conversion.\n\nchannelz/service/service.go, line 145 at r2 (raw file):\n\nGo\n          LocalCertificate:  v.LocalCertificate,\n          RemoteCertificate: v.RemoteCertificate,\n      },\n\nMove }'s to one line\n\ncredentials/credentials.go, line 35 at r2 (raw file):\n\n```Go\n\"google.golang.org/grpc/channelz\"\n```\n\nDelete blank line.\n\ncredentials/credentials.go, line 172 at r2 (raw file):\n\nGo\n      return nil, nil, ctx.Err()\n  }\n  return tlsConn{conn, rawConn}, TLSInfo{conn.ConnectionState()}, nil\n\nNames of the fields.\n\ncredentials/credentials_util_go19.go, line 5 at r2 (raw file):\n\nGo\n/*\n *\n * Copyright 2014 gRPC authors.\n\n2018\n\ncredentials/credentials_util_go19.go, line 36 at r2 (raw file):\n\n```Go\nfunc (c tlsConn) SyscallConn() (syscall.RawConn, error) {\n  conn, ok := c.rawConn.(syscall.Conn)\n```\n\nSince we only care about method SyscallConn, do\ngo\nrawConn.(interface{\n  SyscallConn() (syscall.RawConn, error)\n})\nSo we don't need the _pre19.go file for this.\nBut this works only if syscall.RawConn is defined pre 1.9.\n\ncredentials/credentials_util_pre_go19.go, line 5 at r2 (raw file):\n\nGo\n/*\n *\n * Copyright 2014 gRPC authors.\n\n2018\n\ncredentials/credentials_util_pre_go19.go, line 30 at r2 (raw file):\n\nGo\ntype tlsConn struct {\n  *tls.Conn\n  rawConn net.Conn\n\nWhy keeping the rawConn?\n\ntest/channelz_test.go, line 125 at r2 (raw file):\n\nGo\n          ccs = append(ccs, cc)\n      }\n\nRevert this.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 23 files at r1, 10 of 19 files at r3.\nReview status: 14 of 17 files reviewed at latest revision, 2 unresolved discussions, some commit checks failed.\n\nchannelz/types_nonunixes.go, line 71 at r3 (raw file):\n\nGo\n// Windows OS doesn't support Socket Option\ntype SocketOptionData struct {\n  Linger      *lingerDummy\n\nThis is a strong assumption that non-unix systems will have similar concepts with unix.\nLooking at the proto def for socket option, the real data field is a google.protobuf.Any. We should hide the Linger details in the platform specific files, and add a method that generates the Any proto (move MarshalAny function calls from service.go to platform specific files).\n\nchannelz/service/service.go, line 38 at r2 (raw file):\nPreviously, lyuxuan wrote\u2026\ndone\n\nHmm, what I meant is a function that returns time.Duration, or even ptypes.DurationProto.\nAnd one function should be enough.\n\ncredentials/credentials.go, line 237 at r3 (raw file):\n\nGo\n// Security defines the interface that security protocols should implement in order\n// to provide security info to channelz.\ntype Security interface {\n\nThe names for the interfaces are OK in channelz package, but inappropriate in credentials package.\nRename this or move it to channelz/subpackage\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 23 files at r1, 3 of 19 files at r3.\nReview status: all files reviewed at latest revision, 3 unresolved discussions, some commit checks failed.\n\nchannelz/util_test.go, line 31 at r3 (raw file):\n\n```Go\n\"google.golang.org/grpc/channelz\"\n```\n\nRemove this blank line.\n\nchannelz/util_test.go, line 65 at r3 (raw file):\n\nGo\n      t.Fatalf(\"net.Listen(%s,%s) failed with err: %v\", network, addr, err)\n  }\n  done := startServer(ln)\n\nsomething is wrong with this startServer function\n\nchannelz/util_test.go, line 73 at r3 (raw file):\n\n```Go\n  }\nl := &unix.Linger{Onoff: 1, Linger: 5}\n```\n\nThe content of this struct will be modified by the syscall later, right?\nJust leave it empty? Or use new.\n\nchannelz/util_test.go, line 110 at r3 (raw file):\n\n```Go\n  }\nln.Close()\n```\n\ndefer\n\nchannelz/service/service.go, line 153 at r3 (raw file):\n\nGo\n      anyval, err := ptypes.MarshalAny(v.Value)\n      if err != nil {\n          return &pb.Security{Model: &pb.Security_Other{Other: &pb.Security_OtherSecurity{\n\nMove this code around so you create one local variable and only set its field when error is not nil.\n\nchannelz/service/service.go, line 168 at r3 (raw file):\n\nGo\n  var opts []*pb.SocketOption\n  if skopts.Linger != nil {\n      additional, err := ptypes.MarshalAny(&pb.SocketOptionLinger{Active: skopts.Linger.Onoff != 0, Duration: ptypes.DurationProto(time.Duration(secToNano(int64(skopts.Linger.Linger))))})\n\nMove this and other code that accesses internal fields of SocketOptionData to _unix.go and _nonunix.go files. So you won't need the dummy non-unix types, either.\n\nchannelz/service/service.go, line 168 at r3 (raw file):\n\nGo\n  var opts []*pb.SocketOption\n  if skopts.Linger != nil {\n      additional, err := ptypes.MarshalAny(&pb.SocketOptionLinger{Active: skopts.Linger.Onoff != 0, Duration: ptypes.DurationProto(time.Duration(secToNano(int64(skopts.Linger.Linger))))})\n\nAnd put the fields of the struct to multiple lines.\n\nchannelz/service/service_test.go, line 35 at r3 (raw file):\n\nGo\n  \"golang.org/x/sys/unix\"\n  \"google.golang.org/grpc/channelz\"\n  pb \"google.golang.org/grpc/channelz/service_proto\"\n\nchannelzpb \"google.golang.org/grpc/channelz/\"\n\nchannelz/service/service_test.go, line 41 at r3 (raw file):\n\n```Go\nfunc init() {\n  proto.RegisterType((*OtherSecurityValue)(nil), \"grpc.credentials.OtherSecurityValue\")\n```\n\nMove this next to the definition of OtherSecurityValue, and add a comment that it's needed by UnmarshalAny\n\nchannelz/service/service_test.go, line 180 at r3 (raw file):\n\nGo\n  if protoDuration := protoLinger.GetDuration(); protoDuration != nil {\n      if dur, err := ptypes.Duration(protoDuration); err == nil {\n          linger.Linger = int32(int64(dur) / 1e9)\n\nSimilar to previous comments on making a function to convert duration, this and the following 1e3, 1e9 could also be a function.\n\ncredentials/credentials_util_go19.go, line 35 at r3 (raw file):\n\n```Go\n}\nfunc (c tlsConn) SyscallConn() (syscall.RawConn, error) {\n```\n\nAdd a comment that this implements interface syscall.Conn\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 10 of 13 files at r4.\nReview status: 18 of 19 files reviewed at latest revision, 1 unresolved discussion, some commit checks failed.\n\nchannelz/util_test.go, line 65 at r3 (raw file):\nPreviously, lyuxuan wrote\u2026\nfixed\n\nBut what's the purpose of function startServer and why the for loop plus the channel.\nIt seems to me what you actually want is just\n```go\nln, _ := net.Listen(network, addr)\ngo func() { ln.Accept() } ()\nconn, _ := net.Dial()\n```\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 13 files at r4, 8 of 8 files at r5.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\ncredentials/credentials.go, line 237 at r5 (raw file):\n\nGo\n// ExtraSecurityInfo defines the interface that security protocols should implement in order\n// to provide security info to channelz.\ntype ExtraSecurityInfo interface {\n\nNit: If there's no plan to use this and the following types for anything else, I would include the word \"channelz\" in all the names, like ChannelzSecurityInfo.\n\nComments from Reviewable\n Sent from Reviewable.io \n. Breaks darwin builds. Revert in #2096.. go\n    opts := []grpc.DialOption{grpc.WithTransportCredentials(clientAlwaysFailCred{})}\n    ctx, cancel := context.WithTimeout(context.Background(), time.Second)\n    defer cancel()\n    cc, err := grpc.DialContext(ctx, te.srvAddr, opts...)\n    if err != nil {\n        t.Fatalf(\"Dial(_) = %v, want %v\", err, nil)\n    }\n    defer cc.Close()\nThe Dial is not a blocking dial.\nWill try to extend deadline to 10 seconds.. Benchmark result for non-sticky RPCs:\n```\n==== before ====\ngo run benchmark/benchmain/main.go -benchtime=10s -workloads=all -compression=on -maxConcurrentCalls=1000 -trace=off -reqSizeBytes=1 -respSizeBytes=1 -networkMode=Local\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1B-respSize_1B-Compressor_true: \n50_Latency: 14071.5150 \u00b5s   90_Latency: 16441.2350 \u00b5s   99_Latency: 20031.4030 \u00b5s   Avg latency: 14210.7170 \u00b5s  Count: 703661   13866 Bytes/op  173 Allocs/op \nHistogram (unit: \u00b5s)\nCount: 703661  Min: 613.9  Max: 45838.0  Avg: 14210.72\n\n[     613.868000,      613.869000)       1    0.0%    0.0%\n[     613.869000,      613.875089)       0    0.0%    0.0%\n[     613.875089,      613.918257)       0    0.0%    0.0%\n[     613.918257,      614.224279)       0    0.0%    0.0%\n[     614.224279,      616.393728)       0    0.0%    0.0%\n[     616.393728,      631.773371)       0    0.0%    0.0%\n[     631.773371,      740.802589)       0    0.0%    0.0%\n[     740.802589,     1513.731516)       0    0.0%    0.0%\n[    1513.731516,     6993.172135)       1    0.0%    0.0%\n[    6993.172135,    45837.972000)  703658  100.0%  100.0%  ##########\n[   45837.972000,             inf)       1    0.0%  100.0%  \nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1B-respSize_1B-Compressor_true: \n50_Latency: 16236.1490 \u00b5s   90_Latency: 49637.4370 \u00b5s   99_Latency: 76499.9850 \u00b5s   Avg latency: 23501.6690 \u00b5s  Count: 425264   41246 Bytes/op  40 Allocs/op  \nHistogram (unit: \u00b5s)\nCount: 425264  Min: 215.3  Max: 131301.7  Avg: 23501.67\n\n[      215.284000,       215.285000)       1    0.0%    0.0%\n[      215.285000,       215.291979)       0    0.0%    0.0%\n[      215.291979,       215.347665)       0    0.0%    0.0%\n[      215.347665,       215.791987)       0    0.0%    0.0%\n[      215.791987,       219.337250)       0    0.0%    0.0%\n[      219.337250,       247.625060)       0    0.0%    0.0%\n[      247.625060,       473.334737)       6    0.0%    0.0%\n[      473.334737,      2274.282144)      53    0.0%    0.0%\n[     2274.282144,     16644.120436)  221560   52.1%   52.1%  #####\n[    16644.120436,    131301.690000)  203643   47.9%  100.0%  #####\n[   131301.690000,              inf)       1    0.0%  100.0%  \n==== after ====\ngo run benchmark/benchmain/main.go -benchtime=10s -workloads=all -compression=on -maxConcurrentCalls=1000 -trace=off -reqSizeBytes=1 -respSizeBytes=1 -networkMode=Local\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1B-respSize_1B-Compressor_true: \n50_Latency: 13.8357 ms  90_Latency: 15.9277 ms  99_Latency: 19.8321 ms  Avg latency: 13.9379 ms     Count: 717441 13868 Bytes/op    174 Allocs/op \nHistogram (unit: ms)\nCount: 717441  Min:   3.8  Max:  33.6  Avg: 13.94\n\n[       3.845331,        3.845332)       1    0.0%    0.0%\n[       3.845332,        3.845338)       0    0.0%    0.0%\n[       3.845338,        3.845377)       0    0.0%    0.0%\n[       3.845377,        3.845641)       0    0.0%    0.0%\n[       3.845641,        3.847429)       0    0.0%    0.0%\n[       3.847429,        3.859533)       0    0.0%    0.0%\n[       3.859533,        3.941452)       0    0.0%    0.0%\n[       3.941452,        4.495892)       0    0.0%    0.0%\n[       4.495892,        8.248425)     553    0.1%    0.1%\n[       8.248425,       33.646118)  716886   99.9%  100.0%  ##########\n[      33.646118,             inf)       1    0.0%  100.0%  \nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1B-respSize_1B-Compressor_true: \n50_Latency: 16425.5590 \u00b5s   90_Latency: 48938.5340 \u00b5s   99_Latency: 75172.7280 \u00b5s   Avg latency: 23827.6740 \u00b5s  Count: 419785   42069 Bytes/op  40 Allocs/op  \nHistogram (unit: \u00b5s)\nCount: 419785  Min: 184.6  Max: 141370.9  Avg: 23827.67\n\n[      184.603000,       184.604000)       1    0.0%    0.0%\n[      184.604000,       184.611045)       0    0.0%    0.0%\n[      184.611045,       184.667724)       0    0.0%    0.0%\n[      184.667724,       185.123712)       0    0.0%    0.0%\n[      185.123712,       188.792190)       0    0.0%    0.0%\n[      188.792190,       218.305535)       2    0.0%    0.0%\n[      218.305535,       455.743926)       9    0.0%    0.0%\n[      455.743926,      2365.964217)      39    0.0%    0.0%\n[     2365.964217,     17733.915193)  232117   55.3%   55.3%  ######\n[    17733.915193,    141370.921000)  187616   44.7%  100.0%  ####\n[   141370.921000,              inf)       1    0.0%  100.0%\n```. \nReview status: 0 of 6 files reviewed at latest revision, 3 unresolved discussions.\n\npicker_wrapper.go, line 73 at r4 (raw file):\nPreviously, MakMukhi (mmukhi) wrote\u2026\nHow about atomic.Swap instead of atomic.Load ?\n\nDone.\n\npicker_wrapper.go, line 80 at r4 (raw file):\nPreviously, MakMukhi (mmukhi) wrote\u2026\nSame as above\n\nDone.\n\nstickiness_test.go, line 172 at r4 (raw file):\nPreviously, MakMukhi (mmukhi) wrote\u2026\nHow about: (picked[0] != 0 && picked[1] == 0) || (picked[0] == 0 && picked[1] !=0) ?\n\nDone.\n\nComments from Reviewable\n Sent from Reviewable.io \n. The map used by Register and Get() is currently not protected by mutex.\nThe expectation was that all balancers should already be registered by the time you do something with gRPC. The registration should happen at init time (in an init() function or some where early enough).\nDid this only happen in your tests because tests run in parallel? Do you need to register balancers while the client is running?. You can use Peer to retrieve peer information, note that the field is populated after the RPC finishes.\nhttps://github.com/grpc/grpc-go/blob/7f73c863c0d52a3900bdd76ccaf08191ed51b714/rpc_util.go#L231-L235. Thanks for point this out!\nPlease see https://github.com/grpc/grpc.github.io/pull/651 for the fix.. \nReviewed 8 of 14 files at r1, 2 of 6 files at r2, 6 of 6 files at r3.\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nrpc_util.go, line 423 at r1 (raw file):\n\nGo\n// The caller owns the returned msg memory.\n//\n// If there is an error, possible values are:\n\nAre those errors still true?\n\ninternal/msgdecoder/msgdecoder.go, line 19 at r3 (raw file):\n\n```Go\n// Package msgdecoder contains the logic to deconstruct a gRPC-message.\npackage msgdecoder\n```\n\nThis is internal to transport. I wonder if moving this to grpc/transport/internal/msgdecoder would still work..\n\ninternal/msgdecoder/msgdecoder.go, line 86 at r3 (raw file):\n\n```Go\n// Decode consumes bytes from a HTTP2 data frame to create gRPC messages.\nfunc (m *MessageDecoder) Decode(b []byte, padding int) {\n```\n\nAdd some tests to cover the following cases.\n\ninternal/msgdecoder/msgdecoder.go, line 127 at r3 (raw file):\n\n```Go\nfunc (m *MessageDecoder) parseHeader(b []byte) {\n  buf := getMem(b[1:5])\n```\n\nNit: calculate length := binary.BigEndian.Uint32(b[1:5]) before this line, call getMem(length), and you don't need len(buf) later.\n\ninternal/msgdecoder/msgdecoder.go, line 155 at r3 (raw file):\n\n```Go\n// GetMessageHeader creates a gRPC-specific message header.\nfunc GetMessageHeader(l int, isCompressed bool) []byte {\n```\n\nCall this BuildMessageHeader? Or CreateMessageHeader?\n\ntransport/stream.go, line 3 at r3 (raw file):\n\nGo\n/*\n *\n * Copyright 2014 gRPC authors.\n\n2018? Not sure.\n\ntransport/stream.go, line 64 at r3 (raw file):\n\nGo\n  t := l.tail\n  l.tail = r\n  t.Next = r\n\nUsing a field of an exported field from another package doesn't look like a good idea to me.\nIMO, the linked list related fields/structs should be kept in the same package.\nDefine a recvMagListNode struct with a next pointer as a container for *msgdecoder.RecvMsg?\nOr https://golang.org/pkg/container/list/ if you don't mind the extra type assertion.\n\ntransport/stream.go, line 107 at r3 (raw file):\n\nGo\n// waiting, otherwise, it writes on the chan directly.\n//\n// put must synchronize with GetNoBlock().\n\nWhat does this comment mean?\nput and getNoBlock are already protected by b.mu.\n\ntransport/stream.go, line 122 at r3 (raw file):\n\nGo\n// getNoBlock returns a msgdecoder.RecvMsg and true status, if there's\n// any available.\n// If the status is false, the caller must then call\n\nIt seems the only use case now is\nif not getNoBlock() {\n  getWithBlock()...\n}\nWhy not merge these two functions?\n\ntransport/stream.go, line 139 at r3 (raw file):\n\n```Go\n}\n// GetWithBlock() blocks until a complete message has been\n```\n\nNit: getWithBlock\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 6 of 6 files at r4.\nReview status: all files reviewed at latest revision, 6 unresolved discussions.\n\ninternal/msgdecoder/msgdecoder.go, line 19 at r3 (raw file):\nPreviously, MakMukhi (mmukhi) wrote\u2026\nActually, we'd like it to not be a transport internal, since we don't want to make the transport aware of gRPC-level encoding.\n\nSGTM.\n\nComments from Reviewable\n Sent from Reviewable.io \n. If I didn't miss anything, WithCancel automatically figures what to do with parent context's deadline, because WithDeadline does: https://golang.org/src/context/context.go?s=12285:12353#L373\nThe following seems to be working for me. What I did is to remove grpc.WithTimeout and call context.WithTimeout on c.ctx:\n```diff\ndiff --git a/clientv3/client.go b/clientv3/client.go\nindex 95afd1f66..56ff2acb6 100644\n--- a/clientv3/client.go\n+++ b/clientv3/client.go\n@@ -232,9 +232,6 @@ func (c Client) processCreds(scheme string) (creds credentials.TransportCreden\n// dialSetupOpts gives the dial opts prior to any authentication\n func (c Client) dialSetupOpts(endpoint string, dopts ...grpc.DialOption) (opts []grpc.DialOption) {\n-       if c.cfg.DialTimeout > 0 {\n-               opts = []grpc.DialOption{grpc.WithTimeout(c.cfg.DialTimeout)}\n-       }\n        if c.cfg.DialKeepAliveTime > 0 {\n                params := keepalive.ClientParameters{\n                        Time:    c.cfg.DialKeepAliveTime,\n@@ -349,6 +346,11 @@ func (c Client) dial(endpoint string, dopts ...grpc.DialOption) (*grpc.ClientCo\n    opts = append(opts, c.cfg.DialOptions...)\n\n\nvar cancel context.CancelFunc\nif c.cfg.DialTimeout > 0 {\nc.ctx, cancel = context.WithTimeout(c.ctx, c.cfg.DialTimeout)\ndefer cancel()\n}\n        conn, err := grpc.DialContext(c.ctx, host, opts...)\n        if err != nil {\n                return nil, err\ndiff --git a/clientv3/client_test.go b/clientv3/client_test.go\nindex d09e64330..68b435313 100644\n--- a/clientv3/client_test.go\n+++ b/clientv3/client_test.go\n@@ -23,6 +23,7 @@ import (\"github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes\"\n\"github.com/coreos/etcd/pkg/testutil\"\n\n\n\"google.golang.org/grpc\"\n )\n\n\n\nfunc TestDialCancel(t testing.T) {\n@@ -84,6 +85,7 @@ func TestDialTimeout(t testing.T) {\n                {\n                        Endpoints:   []string{\"http://254.0.0.1:12345\"},\n                        DialTimeout: 2 * time.Second,\n+                       DialOptions: []grpc.DialOption{grpc.WithBlock()},\n                },\n                {\n                        Endpoints:   []string{\"http://254.0.0.1:12345\"},\n```. @enocom No, we just didn't think it was necessary.\nI added a vet target, and made make all run everything.\nThis also makes the test commands consistent.. Can you provide more information on what you want?\nNot sure if this is what you want, you can generate a TransportCredentials from tls.Config using NewTLS.. Yes, your understanding is correct. A ClientConn is one service name.\nAlso see this comment: https://github.com/grpc/grpc-go/issues/1627#issuecomment-340599404. I believe interceptor does what you want.\nFor unary RPCs, see https://godoc.org/google.golang.org/grpc#WithUnaryInterceptor. If interceptor is set, the generated code calls interceptor, and the interceptor will then call the real invoke function.\nFor example, to modify the context:\ngo\nfunc fooUnaryInterceptor(ctx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {\n    ctx = PrepareContextFunc(ctx, method)\n    return invoker(ctx, method, req, reply, cc, opts...)\n}. fixed by #2023. @gyuho Thanks for clarifying!\n@ericchiang \nIn the newer releases, we include the last connection error in RPC errors, (#1855, part of release 1.11). So if you have logs for RPC errors, you will see information about connections there, which could help you tell if the error was fatal or not.\nThe proposal about log levels in this issue sounds good. We have plan to define our logging policy (how to pick log levels), we will make sure this issue gets covered.. The logging doc was submitted in #2033, and sweeping through our logging is tracked in #2146.\nClosing this issue now.. This was reverted in #2049. Reverted in #2083. Do you have another http2 client running in the same process? The leak goroutine stack trace seems to be generated by net/http.(*http2Transport).newClientConn, and doesn't include any gRPC functions.\nI couldn't run your test locally, there's no clear indication on how server is started. I tried with my local helloworld, where the server is started in another process, and a similar test finished with no error.\nOne other possible cause is, conn.Close() doesn't wait until all goroutines exit, so there might be some goroutine running after it's called. Those goroutines will eventually exit.\nI'm not familiar with goleak package, but it seems goleak.VerifyNoLeaks does retry a few times before returning an error.. @gyuho Sorry for the delay. Just did: https://github.com/grpc/grpc-go/releases/tag/v1.12.1. The service name is generated from proto files. It's currently not possible to rename a service. You could modify your proto file and regenearte pb.go files.. Closing issue now. Please comment if you have further questions!. Can you also provide the message you are trying to return in both cases? A code snippet to create the status in both cases would also be helpful.\n\nResulting status-message http2 field is not valid escaped utf-8 sequence in violation of http2 protocol spec for grpc.\n\nIIUC, encoded messages should all be ASCII. It's not clear to me how this will result in violation of http2 protocol. An example would help here.. I see. There are two problems here:\n\n\nMissing a check of invalid utf-8 chars when creating the status (e.g. status.New())\n\n\nIn the case of status with details, the panic caused by an error returned by proto library\n\n\nFor problem 1, we could add a check for user input, and strip invalid utf-8 chars (by replacing them with Unicode replacement character). The same approach should happen on the receiving side, too.\nProblem 2 won't be a problem in the invalid utf-8 cases after we solve problem 1.\nI still agree that panic is not a good idea here, but we didn't want to silently ignore the error. There should be another way to notify the user of the marshal error.. fix reverted in #2127. The vet.sh change is to make travis run make proto, otherwise only cron jobs will run it. The changes will be reverted before submitting.. The change in this PR won't be tested on travis because make proto only runs in cron jobs.\nCan you make a change (https://github.com/grpc/grpc-go/pull/2079/commits/59431fb9ee27a3f96972aa5b859f19177b843d67) to force travis to run the tests? This change will need to be removed before merging.. The build failure was fixed by #2085. Please rebase and try again. Thanks!. @carl-mastrangelo  Thanks!. The port number might be important for the grpclb server, and the grpclb server could strip it if it wants.. The vet.sh change will be reverted before merging.. This is working as intended.\nA ClientConn can be used to do multiple RPCs. To clean up the ClientConn and all underlying connections, you need to call ClientConn.Close.\nOne way to understand this is to view Close as something you want to defer.\nClientConn tries to maintain a working connection pool all the time, so it reconnects whenever a connection is broken. MaxConnectionIdle will cause the server to close the connection, but the ClientConn will recreate it (reconnecting happens with backoff). \nOne note on closing streams: CloseSend() doesn't close the stream. It's only a half close. You can find more discussion on CloseSend in #2015.\nTo fully close a stream, please see the doc.\nWe know that CloseSend and some other functions are confusing for users, and there's a plan to provide an API to close the stream (#1933). And we will also update the doc (https://github.com/grpc/grpc-go/pull/2071).. To protect your server, you will need authorization.\nYou could start with this doc, or search for other related topics on the internet.. It's not clear to me what you did and what you were expecting. Can you explain more? Some code snippet of how you are calling gRPC would help a lot.\n\nWhat version of gRPC are you using?\nVersion 2.0\n\nAlso, there's no gRPC version 2.0. Are you using another library that uses gRPC?. @euroelessar Sorry, just release 1.12.1 but missed this...\nWill make another release with this.\nThere's also another release request (#2125), but I'm a bit unsure about the mentioned PR. If you don't mind, I will want for their reply and combine the releases.\nExpect a minor by tomorrow.. @euroelessar Please see https://github.com/grpc/grpc-go/releases/tag/v1.12.2 for the release.. What kinds of logs are you expecting?\nAlso, does GRPC_GO_LOG_VERBOSITY_LEVEL work for you? You can try this command and see if it works.\nStart the server:\nsh\ngo run examples/helloworld/greeter_server/main.go\nRun the client:\nsh\nGRPC_GO_LOG_SEVERITY_LEVEL=info go run examples/helloworld/greeter_client/main.go\nWith GRPC_GO_LOG_SEVERITY_LEVEL=info, the output I got is:\nINFO: 2018/05/22 16:47:05 parsed scheme: \"\"\nINFO: 2018/05/22 16:47:05 scheme \"\" not registered, fallback to default scheme\nINFO: 2018/05/22 16:47:05 ccResolverWrapper: sending new addresses to cc: [{localhost:50051 0  <nil>}]\nINFO: 2018/05/22 16:47:05 ClientConn switching balancer to \"pick_first\"\nINFO: 2018/05/22 16:47:05 pickfirstBalancer: HandleSubConnStateChange: 0xc420184060, CONNECTING\nINFO: 2018/05/22 16:47:05 pickfirstBalancer: HandleSubConnStateChange: 0xc420184060, READY\n2018/05/22 16:47:05 Greeting: Hello world. There's currently much less info logs on the server side than the client side. But if you made it work on the client side, it should be the same thing on the server side.. There's currently no DEBUG level.\nMaybe try this: GODEBUG=http2debug=2, this shows frame level information, so you will know what's sent and what's received.\nAnd also try to turn on client side logging on the client side?. \nReview status: 0 of 19 files reviewed at latest revision, all discussions resolved, some commit checks failed.\n\nchannelz/types_nonlinux.go, line 1 at r1 (raw file):\n\nGo\n// +build !linux\n\nAdd an init() to log something.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: all files reviewed at latest revision, all discussions resolved.\n\nchannelz/util_test.go, line 2 at r3 (raw file):\n\nGo\n// +build linux\n// +build go1.10\n\nWhy is this 1.10?\n\nchannelz/service/service_test.go, line 1 at r3 (raw file):\n\nGo\n// +build linux\n\nOnly the socket related tests are platform specific, other tests should cover all platforms.\n\ntest/channelz_linux_go19_test.go, line 1 at r3 (raw file):\n\nGo\n// +build go1.10,linux\n\nThe file name is go19.\nAnd why is this go1.10 and above only?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 19 of 23 files reviewed at latest revision, 2 unresolved discussions.\n\nchannelz/util_test.go, line 2 at r3 (raw file):\nPreviously, lyuxuan wrote\u2026\nThis is because SyscallConn() function for net.TCPListener is added after go1.10. https://github.com/golang/go/commit/eed308de31e32a42012fd916d70cfed19280bbe7\n\nAdd comment to explain this.\n\nchannelz/service/service_linux_test.go, line 20 at r4 (raw file):\n\nGo\n *\n */\n\nAdd file level comment for what's different between this and non-linux.\n\ntest/channelz_linux_go19_test.go, line 1 at r3 (raw file):\nPreviously, lyuxuan wrote\u2026\nWrong file name, should be go1.10. Good catch.\n\nIs this 1.10 the same reason as above? Add a comment as well.\n\nComments from Reviewable\n Sent from Reviewable.io \n. Those are all cpu time related syscalls.\nWe should move only that function to a separate file, and add build constraints to that file.\nOr, since they are all the same function, we should move those to a separate internal/ package.. \nReview status: 0 of 7 files reviewed, 2 unresolved discussions (waiting on @lyuxuan and @menghanl)\n\ninternal/benchmarkutil/benchmarkutil_linux.go, line 21 at r2 (raw file):\n\n```Go\n */\npackage benchmarkutil\n```\n\nName this package syscall?\n\ninternal/benchmarkutil/benchmarkutil_linux.go, line 44 at r2 (raw file):\n\n```Go\n// Getrusage returns the resource usage of current process.\nfunc Getrusage(rusage *Rusage) {\n```\n\nMake this function return *Rusage instead?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 7 files reviewed, 2 unresolved discussions (waiting on @menghanl)\n\ninternal/benchmarkutil/benchmarkutil_linux.go, line 21 at r2 (raw file):\nPreviously, lyuxuan wrote\u2026\nboth syscall and unix package functions are included here.\n\nWhat I was thinking is, this will be the grpc syscall package. Whenever there's a requirement for syscall in grpc, the code should go here.\nunix should be considered as part of syscall. (And it's under x/sys)\n\nComments from Reviewable\n Sent from Reviewable.io \n. Thanks!. Thanks a lot for this change! Some nits/minor comments.\n\nReview status: all files reviewed at latest revision, all discussions resolved.\n\nbalancer_switching_test.go, line 392 at r2 (raw file):\n\n```Go\n}\nfunc installFakeGRPCLB() {\n```\n\nDelete this function?\n\nbalancer_v1_wrapper.go, line 83 at r2 (raw file):\n\n```Go\n  targetAddr string // Target without the scheme.\n// To aggregate the connectivity state.\n```\n\nNit: move these three lines below mu to indicate that they are protected by the mutex.\n\nbalancer/balancer.go, line 230 at r2 (raw file):\n\n```Go\n}\n// ConnectivityStateEvaluator gets updated by addrConns when their\n```\n\nHow about renaming this to ConnectivityStateEvaluator?\nAnd update the comment?\n// ConnectivityStateEvaluator takes the connectivity states of multiple SubConns\n// and returns one aggregated connectivity state.\n//\n// It's not thread safe.\n\nbalancer/balancer.go, line 239 at r2 (raw file):\n\n```Go\n}\n// RecordTransition records state change happening in every subConn and based on\n```\n\nAlso on comments, how about this:\n// RecordTransition records state change happening in subConn and based on that\n// it evaluates what aggregated state should be.\n//\n//  - If at least one SubConn in Ready, the aggregated state is Ready;\n//  - Else if at least one SubConn in Connecting, the aggregated state is Connecting;\n//  - Else the aggregated state is TransientFailure.\n//\n// Idle and Shutdown are not considered.\n\ninternal/internal.go, line 31 at r2 (raw file):\n\n```Go\n// set by clientconn.go\nvar (\n```\n\nMerge all three into one var ( ... ) block?\nAnd comment that those two are only used by grpclb?\n\nbalancer/grpclb/grpclb_test.go, line 475 at r2 (raw file):\n\nGo\n  var i int\n  for i = 0; i < 1000; i++ {\n      if _, err := testC.EmptyCall(ctx, &testpb.Empty{}, grpc.FailFast(false)); err == nil {\n\nHow about creating a new ctx with 2 seconds timeout before entering the loop?\n\nbalancer/grpclb/grpclb_test.go, line 484 at r2 (raw file):\n\nGo\n  }\n  select {\n  case <-ctx.Done():\n\nWhy is this check necessary?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 12 of 13 files reviewed at latest revision, 2 unresolved discussions, some commit checks failed.\n\nbalancer_v1_wrapper.go, line 91 at r4 (raw file):\n\nGo\n  // - Build hasn't return, cc doesn't have access to balancer.\n  startCh chan struct{}\n\nNit: to show mu also covers the following fields, I would remove this blank line.\n\nComments from Reviewable\n Sent from Reviewable.io \n. Thanks for the fixes.\nSorry I merged another grpclb PR #2101 that caused conflicts.\nThree conflicts if I didn't miss anything:\n - Fix import for grpclb.go\n - Rename rpcStats to rpcStatsForTest in grpclb_test.go\n - Also rename newRPCStats to newRPCStatsForTest in grpclb_test.go. fixed by #2281. was reverted in #2134 . Ping. Can you provide a reproduction so we can look at it? Thanks.. Still looking at main retry logic in stream.go.\n\nReviewed 6 of 10 files at r1, 1 of 2 files at r2.\nReview status: 7 of 10 files reviewed at latest revision, all discussions resolved.\n\nclientconn.go, line 445 at r2 (raw file):\n\n```Go\n}\n// WithEnableRetry returns a DialOption that enables retries if the service\n```\n\nHow about moving this to an environment variable instead?\nThe advantage would be, no code change will be needed when we enable retry in the future.\n\nclientconn.go, line 1054 at r2 (raw file):\n\nGo\n  if sc.RetryThrottling != nil {\n      cc.retryMu.Lock()\n      cc.retryTokens = sc.RetryThrottling.MaxTokens\n\nThere are things similar to this done in handleServiceConfig.\nMove those to this function, too? Or add TODO?\n\nclientconn.go, line 1633 at r2 (raw file):\n\n```Go\n}\n// throttleRetry subtracts a retry token from the pool and returns whether a\n```\n\nMake a util struct for retryTokens and the following functions?\n\nservice_config.go, line 100 at r2 (raw file):\n\nGo\n  // If token_count is less than or equal to maxTokens / 2, then RPCs will not\n  // be retried and hedged RPCs will not be sent.\n  RetryThrottling *RetryThrottlingPolicy\n\nunexport this field. The only reason to export fields in this struct is to use WithServiceConfig, but we don't want users to use that.\n\nservice_config.go, line 106 at r2 (raw file):\n\nGo\n// service config here:\n// https://github.com/grpc/proposal/blob/master/A6-client-retries.md#integration-with-service-config\ntype RetryPolicy struct {\n\nThis type doesn't need to be exported, either. Unless we want to provide another way to config retry other than service config.\n\nstream.go, line 433 at r2 (raw file):\n\nGo\n  // TODO(retry): Move down if the spec changes to not check code before\n  // considering this a failure.\n  pushback := 0\n\nNot sure if this is what the TODO is talking about.\nThis variable is not used until line 463. Move this down after the ifs?\n\nstream.go, line 439 at r2 (raw file):\n\nGo\n      var e error\n      if pushback, e = strconv.Atoi(sps[0]); e != nil || pushback < 0 {\n          grpclog.Infof(\"Server retry pushback specified to abort (%q).\", sps[0])\n\nThis is inaccurate if e != nil.\n\nstream.go, line 468 at r2 (raw file):\n\nGo\n      max := float64(rp.MaxBackoff)\n      cur := float64(rp.InitialBackoff)\n      for i := 0; i < cs.numRetriesSincePushback; i++ {\n\nTo avoid the for loop, how about comparing\n  cs.numRetriesSincePushback with\n  \\log_{multipler}\\frac{max}{init} math link\nThe latter value can be calculated every time service config gets updated.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 5 of 7 files at r3, 1 of 3 files at r4, 1 of 2 files at r5, 7 of 9 files at r7, 1 of 1 files at r8, 1 of 1 files at r9.\nReview status: 14 of 16 files reviewed, 5 unresolved discussions (waiting on @dfawley and @menghanl)\n\nstream.go, line 329 at r9 (raw file):\n\n```Go\n}\ntype fatalErr struct {\n```\n\nDelete this struct.\n\nstream.go, line 339 at r9 (raw file):\n\nGo\n  if err != nil {\n      if a.done != nil {\n          a.done(balancer.DoneInfo{Err: err})\n\nnewStream() could be retried, so it shouldn't call done\ndone is already called by finish.\n\nstream.go, line 427 at r9 (raw file):\n\n```Go\n  }\n// Wait for the current attempt to complete so trailers have arrived.\n```\n\nThis comment is inappropriate. The wait happens in the TrailersOnly call.\n\nstream.go, line 444 at r9 (raw file):\n\nGo\n  hasPushback := false\n  if cs.attempt.s != nil {\n      if to, toErr := cs.attempt.s.TrailersOnly(); toErr != nil {\n\nTrailersOnly also returns true when there's a connection error. But the stream is not necessary trailers only.\n\nstream.go, line 581 at r9 (raw file):\n\nGo\n  // RecvMsg() returned a non-nil error before calling this function is valid.\n  // We would have retried earlier if necessary.\n  return cs.attempt.s.Trailer()\n\nCould s be nil?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 6 of 8 files at r10.\nReview status: 13 of 15 files reviewed, 2 unresolved discussions (waiting on @dfawley and @menghanl)\n\nstream.go, line 266 at r10 (raw file):\n\nGo\n  // finish() will affect stats and tracing.\n  // TODO(dfawley): move to newAttempt when per-attempt stats are implemented.\n  cs.attempt.statsHandler = sh\n\nMove statsHandler and traceInfo as parameters for newAttemptLocked()\n\nstream.go, line 560 at r10 (raw file):\n\nGo\n  // We would have retried earlier if necessary.\n  if cs.attempt.s == nil {\n      return nil\n\nShould this return the previous received trailer?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReviewed 1 of 7 files at r3, 1 of 8 files at r10, 1 of 1 files at r11.\nReview status: :shipit: complete! all files reviewed, all discussions resolved\n\nComments from Reviewable\n Sent from Reviewable.io \n. Because a server can serve on multiple listeners (multiple ports).\n```go\n    s := grpc.NewServer()\n    pb.RegisterGreeterServer(s, &server{})\nlis1, err := net.Listen(\"tcp\", port1)\nlis2, err := net.Listen(\"tcp\", port2)\nerr = s.Serve(lis1)\nerr = s.Serve(lis2)\n\n``. I would argue that the current behavior is working as intended.passthrough` is designed to be a resolver that does nothing more than passing the input address directly back to the ClientConn.\nIf you see \"name resolution\" as a process that, takes an input string as name, and returns a list of addresses, what you want is actually already a different \"name resolution\" than passthrough. \nEven though it's not as complicated as other systems such as \"DNS\", it still does something that only your resolver knows (in this case, splitting on \",\"s).\nI would suggest that you create your own resolver, and register it (maybe under a different scheme?). Feel free to copy the passthrough resolver and modify it.. It looks to me like your vendored gRPC is an older version, but is somehow referencing an newer version of protos.\nCan you try to update your vendored gRPC and see if that works?\nEDIT:\nIt seems there's no generated pb.go code in the vendor directory, so this import in vendor/.../grpclb.go will be referencing the one outside of vendor directory.\nIt's not clear to me where you ran protoc --go_out=plugins=grpc:. *.proto from. If you don't want to update vendored gRPC for some reason, I think generating pb.go files in the vendored gRPC folder will also fix the problem.. Can you double check it's #2106? Why is that important for you?\nThe original plan is to not include it in the minor release. But if you really need it, we can add it.. Since you've found a workaround, I will not include #2106 in the next minor release (the reason is to minimize the change in case there are unintentional changes).\nIf you later find it still bothers you, please reply here.. Root cause of the regression: https://github.com/golang/protobuf/pull/628\nUTF-8 validation was reverted in proto2, and kept in proto3.\nBut to get the right behavior, all proto3 users need to regenerate pb.go files (reason here).\nstatus.proto is proto3, so regeneration is necessary (done here).\nThe original fix should be good after this.. This can be done using interceptors:\nhttps://godoc.org/google.golang.org/grpc#WithUnaryInterceptor\nhttps://godoc.org/google.golang.org/grpc#WithStreamInterceptor\nIn the interceptor, call context.WithTimeout to set the timeout, and then make the RPC with the new context.. Closing this now. Please reply if you this is still needed.. Same change as in https://github.com/grpc/grpc-go/pull/2109\nWas reverted by https://github.com/grpc/grpc-go/pull/2127\nThe original change was good. See https://github.com/grpc/grpc-go/pull/2127#issuecomment-395845059. The generated pb.go file seems outdated. Can you update it?\nYou probably also want to sync to head because other pb.gos are also outdated.\nAnd also make this vet.sh change again: https://github.com/grpc/grpc-go/commit/59431fb9ee27a3f96972aa5b859f19177b843d67?. I assume you mean multiple RPCs in parallel, not one stream RPC sending messages in parallel, right?\nCan you try to set these environment variables to get logs: GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info?. I would prefer dependent.\nMaintaining copies sounds like a bad idea to me because you will need to keep the copies consistent, which means, whenever you make a change, you will need to find all the copies and update those as well.\nI'm not very familiar with Bazel, but it should be able to skip those proto files that are unchanged when building. And \"Caching + incrementality\" is listed as a benefit in this post: https://blog.bazel.build/2017/02/27/protocol-buffers.html. \nThe bazel repo would be a better place for this question.. From which directory did you run protoc --go_out=plugins=grpc:. *.proto?\nIf you are in directory $GOPATH/src/google.golang.org/grpc/examples/helloworld, the command would be protoc -I helloworld/ helloworld/helloworld.proto --go_out=plugins=grpc:helloworld.\nAlso, did you see any error output?. Looking at the generated code, I'm pretty sure you are somehow using an outdated go proto generator.\ncd $GOPATH/src/github.com/golang/protobuf to check you local code is as expected (it's on git master branch and maybe no local changes?).\nOr if you are sure you don't have anything that you want to keep in your local cache, just delete them and redo go get.\ngo get -v -u github.com/golang/protobuf/{proto,protoc-gen-go}. @malengatiger \"transport is closing\" error is often caused by the server closing the connection. Try to look at the server side logging and see if there's any transport error there.. Thanks for the fix for the issue.\nThere's something related to grpclb I think you may want to know:\nWe recently moved the grpclb package to a sub package of balancer, and grpc package no longer installs it by default. So your clients will need to import _ \".../grpclb\".\nWe also moved the generated proto files, which may affect you if you import the proto from grpc repo. You will not immediately notice this because we kept the old package to avoid breakage. But it will be great if you could import the new generated code instead.\nThe changes were made in #2107 and #2137, and the reason is to make it easier to sync proto with the official grpclb proto file.\n. @MaerF0x0 transport.ErrConnClosing is never returned by gRPC package, it will be wrapped into a status error. So if you were expecting this error from an RPC, you can safely delete this if condition now.. The error message means, github.com/gogo/protobuf/proto in the vendor directory is not in sync with proto/felixbackend.pb.go. Most likely, your github.com/gogo/protobuf/proto is outdated.\nPlease try to refresh the dependencies. I'm not very familiar with glide, but there might be a command to do that.\nThis is not a gRPC issue. The felix repo is probably a better place to ask.. I noticed that you set WithDialer, is there any particular reason for that?\nIn the current implementation, proxy doesn't work if a custom dialer is set.\nI think it would work if you do\ngo\ngrpc.DialContext(context.TODO(), \"127.0.0.1:30103\", grpc.WithInsecure()). Try dialing to the server name, without setting custom dialer:\ngo\ngrpc.DialContext(context.TODO(), \"gRPCServer\", grpc.WithInsecure())\n(Note that even though the above works, I would suggest dialing to \"passthrough:///gRPCServer\" instead of just \"gRPCServer\". The \"passthrough:///\" prefix is to pick the passthrough resolver. It's not necessary because passthrough is the default now. But the default may change to DNS in the future.)\nRun the client with proxy environment variable set:\nsh\nHTTP_PROXY=\"proxyIP:port\" go run client.go\nThe behavior should be, a connection will be created to \"proxyIP:port\", and then a HTTP connect request will be sent to the proxy, which should contain \"gRPCServer\" as the URL.. @leaxoy Do you have any updates on this? Thanks.. My understanding is that, you want to specify a set of services and when new gRPC servers are created, those services will be automatically installed on the new servers. Is that right?\nIf that's the case, it seems that you can have a wrapped NewServer function, which calls grpc.NewServer and do service registration on the returned server.. This is caused by a change in gofmt: https://github.com/golang/go/issues/26228\nWe will need to make this change when we upgrade travis to go 1.11.. The resolver depends on etcd to get the server addresses, can you double check it gets the correct addresses first?\nAlso, can you run the client with GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info and get the logs?. GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info go run client.go, and then you should see logs printed by the client.. Short answer is, in your resolver implementation, the watch() method should be on each resolver, not on the builder.\nBecause\n Builder builds Resolvers.\n Different clients calls the same Builder to build different Resolvers (this happens as part of grpc.Dial(). \n* So the Builder is shared by all the clients, but different clients should have different Resolvers.\nIn your resolver implementation, the watch() method and all the updates happen in the Builder, so when you only have one client, things should work fine.\nBut when you call grpc.Dial the second time, the Builder will be modified by your build function, and things start to go wrong.\nA working resolver structure in my mind would look like\n```go\ntype resolver struct {\n  cli etcdClient\n  cc  resolver.ClientConn\n}\nfunc (resolver) watch() {\n  // Similar to your existing code.\n}\ntype builder struct {...}\nfunc (builder) Build() {\n  cli := etcd.New()\n  r := &resolver{cli: cli}\n  go r.watch() // watch is done by resolver, not the bulder\n}\n```\nYou can also take a look at the dns resolver as an example.. Thanks @aaronbee for answering the question.\nClosing this issue now.. Your understanding is correct.\nThe RPC deadline applies across all retry attempts for a given RPC.\nThere was a discussion on gRPC deadline vs attempt deadline:\nhttps://groups.google.com/d/msg/grpc-io/zzHIICbwTZE/0bDxQwPECQAJ (search for deadline)\nShort answer is, this case will be covered by hedging.\nHedging support is not available now.. As you already mentioned yourself, a retry interceptor would work in this case.\nWhat I image this interceptor could do is to get the timeout from the context, and make several RPC attempts with a shorter timeout. Those attempts can happen in a for loop, and at the same time, the interceptor should keep an eye on the overall timeout.\nThere's no default timeout applied to the RPCs.. Closing this issue now.\nPlease reply if you have further questions.. The vet failure should be fixed by https://github.com/grpc/grpc-go/pull/2263. Please rebase and try again.. It seems that what you need is to know the underlying connection error when a ClientConn is not working. Does that sound right?\nThere's #2055 to return the latest connection error in the ClientConn, but we haven't finalized on how to do that. Let us know if this solution will work for you.. With #2055 or something similar, you can wait on the ClientConn's connectivity state change. When it goes into transient failure, you can call the new function to look at the connection error, and choose to keep or close the ClientConn.. helloworld.pb.go needs to be regenerated. Please run make proto and try again.\nAlso, can you sign the CLA? Thanks.. @Arbusz can you fix the failure and also sign the CLA? Thanks!. It sounds to me that TLS can solve your problem.\nMore information about auth can be found at: https://github.com/grpc/grpc-go/blob/master/Documentation/grpc-auth-support.md. This looks like a duplicate of #906.\nWe are working on a transport redesign to enable custom transports, including in-process transport. Please see more detailed updates in the issue.\nClosing this now. Please reply if you have other concerns.. @jrick Can you try if #2270 is fixed by this? Thanks!. @jrick Thanks a lot for verifying!. This has been discussed in #1663, and the decision is to not add a custom polling frequency.\nPlease take a look at that and see if it answers your question.. The drop field is defined here. The grpclb server sends a list of servers where the client can send the RPCs. Some of those servers can be drops.\nOne use case is that when the backend servers are overloaded, and the grpclb server wants the clients to drop some loads.\nThis assumes that the client is a well-behaving client. So servers still need to be able to correctly handle the situation where client doesn't respect drop.. If all servers in serverlist are valid, traffic will all be sent to those servers.\nBut 50% of the RPCs will be dropped if half of the servers are valid, and the other half are drops.. Can you check the server side logs and see if there's any errors? Like creds handshake errors?\nAlso, on client side, try adding WithWaitForHandshake dialoption and see if it makes any difference.. @ahmetb Just a note on location of the proto file:\nThe official copy of health.proto is in grpc-proto repo, and we have checks to make sure our pb.go file is update to date with that one.. This shouldn't be necessary: https://golang.org/pkg/sync/#Mutex. This looks like typed nil again!\ndefer cleanup() function does\ngo\nif client != nil { client.Close() }\nWhere client is \ngo\nvar client ClientTransport\n...\nclient, err = newHTTP2Client() // This function returned (*http2client)nil, err\nSo the deferred if check sees true, and calls Close() on the typed nil.. Did you try setting PermitWithoutStream to true?\nWith this set to false, the client won't send ping when there's no stream (stream only exists when the RPC is in progress, so when the unary RPC finishes, there's no stream), and the server won't check against the EnforcementPolicy when there's no stream.. With the follow configuration, client will get EnhanceYourCalm goaway.\nClient\ngo\ngrpc.WithKeepaliveParams(keepalive.ClientParameters{\n    Time:                time.Millisecond,\n    Timeout:             time.Millisecond,\n    PermitWithoutStream: true,\n})\nServer\ngo\ngrpc.KeepaliveEnforcementPolicy(keepalive.EnforcementPolicy{\n    MinTime:             2 * time.Second,\n    PermitWithoutStream: true,\n})\nBesides PermitWithoutStream, Timeout on client side is also important.\nThe client will send a ping, wait for Timeout, and check if there's any activity (ping ack is an activity). So the client won't send a second ping before this timeout. The default timeout value is 20 seconds.\nAnother easy way to reproduce this is to set PermitWithoutStream: false on server side, and don't do any RPC on the client side. So whenever the server receives a ping, the ping will be considered invalid.. > is there a way I can keep PermitWithoutStream to false and still see the goaway messages?\nThe purpose of PermitWithoutStream is to control whether keepalive pings are sent when there's no active streams. So there is no way to do this.\n\nI also tried examples/route_guide/client\n\nWhich RPC did you try? GetFeatureis a unary RPC, and is similar with HelloWorld. If you tried streaming RPCs and sent requests, keepalive pings won't happen (because there's no need to ping when there's activities on the connection).\nYou can try to do a RouteChat, keep the stream and don't send requests.\n\nFor more on keepalive, please take a look at https://github.com/grpc/grpc-go/pull/2342 and see if that helps.. If the vendor directory that got removed is this: https://github.com/u-root/u-bmc/pull/37, it seems there's no gRPC in it.\nBut it does contain protobuf, and we use protobuf to find the registered methods and do reflection. Could it be that the protobuf version was old and missed some features?. Do you have other replace rules? The error message is about golang.org/x/lint, not grpc-go.\nAnd this is not a gRPC issue. It's a go module issue, probably related to: https://github.com/golang/go/issues/26904. It sounds reasonable to ignore empty serverlists, and is an easy fix.\nBut if the balancer never needs to send empty serverlists, and will not send empty serverlists, it is also unnecessary for the client side to handle empty serverlists.. @markdroth Thanks for the fix. I will close this now.. This change caused performance drop.\n./benchmark/run_bench.sh -r 10 -c 200 -req 100 -resp 1 -rpc_type unary -d 20\nQPS dropped from 166k to 160k.\nReplaced with #2351.. This also improved performance.\nReason could be: less values (e.g. compressor) are needed by the closure (function df()), so time spend on memory operations is less.\nWith the change:\n./benchmark/run_bench.sh -r 10 -c 200 -req 100 -resp 1 -rpc_type unary -d 20\n================================================================================\nr_10_c_200_req_100_resp_1_unary_1538781162\nqps: 193278\nLatency: (50/90/99 %ile): 9.587852ms/15.678145ms/23.563613ms\nClient CPU utilization: 2m51.318190995s\nClient CPU profile: /tmp/client_r_10_c_200_req_100_resp_1_unary_1538781162.cpu\nClient Mem Profile: /tmp/client_r_10_c_200_req_100_resp_1_unary_1538781162.mem\nServer CPU utilization: 2m42.053437419s\nServer CPU profile: /tmp/Server_r_10_c_200_req_100_resp_1_unary_1538781162.cpu\nServer Mem Profile: /tmp/Server_r_10_c_200_req_100_resp_1_unary_1538781162.mem\nWithout the change:\n./benchmark/run_bench.sh -r 10 -c 200 -req 100 -resp 1 -rpc_type unary -d 20\n================================================================================\nr_10_c_200_req_100_resp_1_unary_1538781200\nqps: 164940.9\nLatency: (50/90/99 %ile): 11.187639ms/18.423748ms/28.158704ms\nClient CPU utilization: 2m27.89375611s\nClient CPU profile: /tmp/client_r_10_c_200_req_100_resp_1_unary_1538781200.cpu\nClient Mem Profile: /tmp/client_r_10_c_200_req_100_resp_1_unary_1538781200.mem\nServer CPU utilization: 3m8.813832391s\nServer CPU profile: /tmp/Server_r_10_c_200_req_100_resp_1_unary_1538781200.cpu\nServer Mem Profile: /tmp/Server_r_10_c_200_req_100_resp_1_unary_1538781200.mem. This change doesn't show performance improvement in other benchmarks (where server and client run in two processes, not sure if that's the reason).\nIt doesn't cause any problem either, so this change is still good.. Thanks for reporting.\nCan you provide the code to reproduce? It will help a lot. Thanks.. I had a fix in #2371, please try if that works.. Thanks for the change!\nDo you have a particular reason for this other than the TODO comment? The TODO actually means do this when we need the same functionality somewhere else.\nThere's no much point if the struct is only used in this one place, as it adds an unnecessary type assertion. This can be better solved with generics. But I would imagine when generics become a thing, there will be some standard libraries that we can use.. The code you linked is to support HTTP CONNECT proxy. It seems your proxy is not a HTTP CONNECT proxy server. If you don't want this connection to go through your proxy, can you just unset http_proxy?. If you don't want to have the default proxy behavior, it can be turned off by setting the custom dialer to one without the proxy code.. The handshake done in the default dialer is to support HTTP CONNECT Proxy as defined in https://github.com/grpc/proposal/blob/master/A1-http-connect-proxy-support.md\nHTTP CONNECT creates a HTTP tunnel, after that, we send HTTP2 traffic through the tunnel.. By \"grpc proxy\", it actually means proxy that supports grpc traffic.\nIt can be\n - a HTTP2 proxy (which means it parses the request header, looks at the path, and decides where to forward the request).\n   - In this case, the proxy needs to do HTTP2 because it needs to parse HTTP2 headers.\n - a HTTP tunnel.\n   - In this case, the proxy supports HTTP CONNECT (this is http1.1), and then forwards the raw bytes. Because it doesn't parse HTTP2 headers, it doesn't need to know about HTTP2\nOnly if you want to support both of these on the same port, you will need to support both HTTP2 and HTTP1.1. But I believe you will only need one of them.. Closing this issue now. Please reply if you have further questions. Thanks.. This is not an expected behavior. Reconnecting will start over with the TLS handshake.\nDoes this only happen when reconnecting in an existing ClientConn? If you restart the ClientConn, will the connection be successful?\nIt will also great if you can provide the test to reproduce so we can look into that. Thanks!. gRPC uses HTTP2, so http1.1 request/response won't work.\nTo do REST with gRPC, you will need a layer to do the conversion.\nE.g. https://github.com/grpc-ecosystem/grpc-gateway. This appears to be working for me. Did you get any problem caused by this?\nOn the code:\nwe make a copy of default callInfo, and modify it with the call options set by user. So at the line you saw, the value of c.maxReceiveMessageSize is already set to the value from your call option.. Thanks for reporting!\n\n407 Proxy Authentication Required\n\nAuthentication is not supported in the current proxy dialer.\nI have a fix in #2426. Please give it a try if you have time. Thanks.. I'm not familiar with squid proxy, but from the error message, I think it supports HTTP CONNECT. In that case, it's not about HTTP2 support, because it's just a HTTP tunnel.\nDoes it support basic authentication?\nThe error message (Proxy-Authenticate: Basic realm=\"outbound_proxy\") shows it does, but I still want to confirm.\nAnd if you can get the server side logs for this request, it would also help a lot debugging the problem. Thanks!. Supporting digest auth would require more time to do, and we currently don't have the cycles. Sorry about that. I filed #2440 to track it.\nIn the mean time, if you really need this, you can try to install a custom Dialer and do digest auth with it.. And change labels to keep this in the release notes.. https://github.com/grpc/grpc-web might be a better place for this question.\nAlso found this: https://github.com/grpc/grpc-web/issues/285. Not sure if this is related though.. Glad that worked.\nClosing this now. Please reply if you have further questions.. transport is closing is usually caused by peer, turning on logging reveals what the cause is:\nhttps://github.com/grpc/grpc-go/blob/master/README.md#the-rpc-failed-with-error-code--unavailable-desc--transport-is-closing. > without the right credentials\nWhat exactly went wrong in the configuration?\nIn my mind, this can be one of:\n1. Server expects auth, but client doesn't do auth (connect using WithInsecure).\n   * The client doesn't do creds handshake at all. What we see is that the connection is closed, for some unclear reason. It could be authentication or something else.\n1. Client uses the wrong configuration (e.g. wrong certificate).\n   * The RPC should fail with an error like rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: connection error: desc = \"transport: authentication handshake failed: .... This can be done by running GracefulStop in a goroutine, and do a select-wait on timer followed by a force Stop in the main goroutine.\nSomething like:\n```go\nstopped := make(chan, struct{})\ngo func () {\n  s.GracefulStop()\n  close(stopped)\n}\nt := time.NewTimer(10 * time.Second)\nselect {\ncase <- t.C:\n  s.Stop()\ncase <- stopped:\n  t.Stop()\n}\n```. This is a request for protobuf, not gRPC. https://github.com/golang/protobuf is the golang protobuf repo.\nAlso, this seems like a dup of https://github.com/golang/protobuf/issues/261.. > GRPC_GO_LOG_VERBOSITY_LEVEL not takes effect\nWhat did you see that makes you believe it's not taking effect?\nIf you didn't see a whole lot of logs, that's because we don't have that many verbose logs.\nI also just tried (by adding a testing log with verbose level 2), and it works.\nAlso you may want to set GRPC_GO_LOG_SEVERITY_LEVEL=info, too.. Did you import github.com/grpc/grpc-go/tree/master/grpclog/glogger to install glog?\nIn that case, the log levels and verbose levels are controlled by glog command line flags: https://github.com/golang/glog/blob/master/glog.go#L399.. All oauth2 can be implementations of PerRPCCredentials. The important part is to return the Metadata (HTTP headers) to be sent together with the RPCs.\nI'm not very familiar with oidc, but I think they would require browsers to work. If your app is going to run in terminal, you will need to solve that.\nI imagine you can create one PerRPCCredentials at init time, ask user to auth, and reuse it for all ClientConns in the binary.. Which json library are you using?\nThe repeated fields are tagged with json:\"...,omitempty\", so it should be omitted in this case.\nIf this is still not helping, try the proto json package instead: https://godoc.org/github.com/golang/protobuf/jsonpb#Marshaler.Marshal. > \"hybrid\" (default; removed after the 1.17 release): wait for handshake before considering a connection ready, but wait before considering successful.\nDid you miss a \"not\"?. One possible reason I can think of is, internal package is not vendored correctly. The vendored client.go is trying to import internal from gopath, and caused the build failure.\nCan you check your vendor directory and see if you find internal there?. I just noticed that you are importing \"github.com/grpc/grpc-go/health\", but the canonical import path is \"google.golang.org/grpc/health\". Same for your vendored directory hierarchy.\nPlease try fixing your vendor directory hierarchy and import path.. Authority is the server name, not the IP (and a name could be resolved to multiple IPs).\nThe client side credentials, for example TLS, needs to verify that the name is covered by the certificates returned from the server.\nAlso, if you need the IP, you should be able to get it from net.Conn.. For the error, \n\ncertificate is valid for 10.1.230.47, not 127.0.0.1\n\nServer certificate is for 10.1.230.47, but you are doing grpc.Dial(127.0.0.1). So the hostname verification failed.\nIf you cannot do grpc.Dial(10.1.230.47), you can override the server name when creating client credentials. Note this should only be used for tests, not for production.. I'm not very familiar with consul or your setup. But an easy fix for you:\ngo\nconn, err := grpc.Dial(\"consul://127.0.0.1:5000/10.1.230.47:50051\", opts...\nAnd in your consul name resolver (line conf.Address = target.Endpoint), change it to\ngo\nconf.Address = target.Authority\nYou can read more about the name syntax here: https://github.com/grpc/grpc/blob/master/doc/naming.md#name-syntax. The certificate should (normally) be signed for the server name, because\n1. the backend IPs can change, but the server name usually don't change\n   - this is the same reason that you want to hide the server addresses, and only expose consul address\n2. you can use the same certificate for all your backend IPs, instead of one for each backend\nA name resolution system can be seen as a map from server names to IPs (like a map[string][]IP).\nIf I understand correctly, in your system, a consul server is responsible for a bunch of backend servers. When dialing, you connect to this consul server, get all the backend server IPs registered in the consul server, and connect to the backends.\nThis makes it a map from consul-server-IP to backend-server-IPs (map[consul-IP][]backend-IP).\nSo the consul-server-IP is essentially the name in the name resolution system, and since the certificate should be signed for the name, the certificate should be signed for the consul-IP.\n(Some other suggestion: instead of one consul server for one group of backend servers with the same name, the consul server can be used to manage multiple server names. So it looks more like a real name resolver (like DNS).\nWhen you register backend servers to the consul server, it can be registered with a name (e.g. service.hxzhao527.com). The certificate can be signed for the same name (service.hxzhao527.com). On the client side, dial to consul://127.0.0.1:5000/service.hxzhao527.com.). leakcheck is not meant to be a public package and was moved to internal (#2129).\nI just checked that all imports in gRPC are pointing to the right location.\nAre there imports in your code or other packages that's still referencing the old location?. This sounds to me like something is misconfigured.\nCan you confirm that the client is connecting to the right server, and the service is registered on the server?\n(One thing I noticed: the client is called ApplicationRegisterServiceClient, but the error message is about a ServiceNameDiscoveryService. Those seem to be two very different services.). I'm assuming this is just a client and a server, no proxy or anything in between.\n\nIs the client connected to the server as expected? (Is the connection established between the client and the server)\nIf not, you might be dialing to a wrong address\nTurn on logs could help clarify things out.\nIf the client is connected to the expected server (the service is registered on this server), but still get Unimplemented error\nThis sounds like a strange problem. \nAre the generated codes in different languages in sync?\nAnother suggestion is to look in the generated functions that register services (for go, it would be the RegisterApplicationRegisterServiceServer function. There's should be similar ones in Java and c#).\n\n(Another totally random guess:\nin proto.ApplicationRegisterService/applicationCodeRegister, the first letter of the method name is not capitalized. IMO this should work find and shouldn't cause any problems. But if you are still stuck, it's worth trying to change it to ApplicationCodeRegister.). The log says a connection to localhost:11800 was successfully created. But it seems the RPC still failed with Unimplemented.\n\nAnother suggestion is to look in the generated functions that register services (for go, it would be the RegisterApplicationRegisterServiceServer function. There's should be similar ones in Java and c#).\n\nWhat I meant is to check the function that registers the service on the server side, what you checked is the client side function that makes the RPC.\nWith the information I see now, I cannot tell why it doesn't work.\nMaybe try to reproduce with simple services (like helloworld). And if you can still reproduce it, put it in a repo, so I can try it myself?. > According to the official demo, goalng can connect c# server. It makes me very confused.\nGolang client should be able to talk to c# servers. Interoperability between all languages are expected to work.\nI'm going to close this issue now. Please comment here when you have a reproduction or new findings. Thanks!. This file is generated, and shouldn't be manually edited.\nAlso, if you use go1.9 and above, this shouldn't be a problem because \"golang.org/x/net/context\" is just a type alias.\nThe codegen change is already submitted upstream. We are waiting for a new release to update out dependencies.. No, let's not do this. Helloworld should be simple.. This shouldn't be a problem since you are using go1.11, and x/net/context has been made a type alias of context for go version>=1.9 (https://github.com/golang/net/blob/master/context/go19.go).\nYour vendored x/net/context is pretty old (last updated 2 years ago).\nUpdating your vendored x/net/context, or just removing it from vendor should fix this problem for you.\nOn updating context in the generated code, the change for the codegen has been submitted. We are waiting for a release from protobuf repo.. Let's add a README.md to examples/rpc_errors to explain the example.. Can you try https://godoc.org/github.com/golang/protobuf/jsonpb instead of json?\nOne possibility is that json package is not handling enums correctly.. https://github.com/grpc/grpc-go/blob/97da9e087c5cf7a11263637efb410e08c51a7a0f/clientconn.go#L907-L911\nThe race happens on the same line, but the line is already protected by mutex.. This race seems to only exist in the version you use. It should be fixed in newer version.\nThe write to ac.dopts.copts.KeepaliveParams needs to hold ac.mu, but in the version you use, it happens outside of the lock:\nhttps://github.com/grpc/grpc-go/blob/97da9e087c5cf7a11263637efb410e08c51a7a0f/clientconn.go#L904-L914\nBut it's fixed now:\nhttps://github.com/grpc/grpc-go/blob/c71aa62423b37215980f9c3141eef06f4c35e998/clientconn.go#L997-L1002\nPlease try to upgrade your grpc version and try again.. Closing this now. Please reply if you have further questions.. Should be fixed by https://github.com/golang/sys/commit/c6cbdbf9e68a68b19802a5c44e537c284b9c8e1a#diff-5f6b0bf9df53057228c1438f608c3b0b\nTravis reruns look good.. Root cause might be: the lock should be more than just a read lock\nhttps://github.com/grpc/grpc-go/blob/97da9e087c5cf7a11263637efb410e08c51a7a0f/clientconn.go#L907-L911\nhttps://github.com/grpc/grpc-go/issues/2542#issuecomment-450455657. Taking the whole lock sounds good.. > https fetch: Get honnef.co/go/tools?go-get=1: net/http: TLS handshake timeout\nThis could be caused by connection problems. Can you try again with VPN?. > go: cloud.google.com/go@v0.26.0: unknown revision v0.26.0\nv0.26.0 release exists, but not found for some reason:\nhttps://github.com/googleapis/google-cloud-go/releases/tag/v0.26.0\nDid you run everything with VPN? IIRC, to access cloud.google.com you will need VPN.. Closing this now. Please reply if you have further questions.. Returning a status error is the right way to close stream with an error.\nBut it's never executed because ok is always true for md, ok := metadata.FromIncomingContext(stream.Context()).\nThere's metadata sent by gRPC (not by users), e.g.\nmap[:authority:[x.test.youtube.com] content-type:[application/grpc] user-agent:[grpc-go/1.18.0-dev]]\nInstead of checking ok, please look up the key in metadata.. Client side stream.send() should fail with error io.EOF, which means the stream is broken. To get the status of the stream (why the stream failed), please call receive on the stream.\nSee the doc of SendMsg and RecvMsg here: https://godoc.org/google.golang.org/grpc#ClientStream. When I said send() fail with error, I meant \"stream.Send()` for messages.\nThe stream creation will be successful, but the stream will be closed by the server, and then you won't be able to send messages on the stream.\n\nBasically\n_ , err = client.StreamData(context.Background())\nthis will only return err if there is any network errors right ? All the RPC implementation errors will be communicated with the stream. Is that correct understanding ?\n\nThis is right.\nStream creation doesn't need ack, so stream is created when the headers are sent out.\nErrors of the stream will be delivered to client when client is receiving.\n. Ping. Any update on this? Thanks.. Can you provide more details about where the memory is allocated? Maybe the stack trace you got from pprof?\nIt would also help if you can give us an example to reproduce the issue. Thanks!. @rohanil ping. Did you have time to take a look and collect pprof data? Thanks!. Where did you run go get -u google.golang.org/grpc and what's its output?\nAlso what does make build do? Is it just go build?\ngoogle.golang.org/genproto is specified in our dependencies, so this shouldn't be a problem.. I'm not sure what the root cause is, maybe try go mod tidy and see if it helps.. Closing now. Please reply if you still have the problem. Thanks!. I tried the command and it worked for me. It's probably related to your environment.\nAlso gRPC repo is not a good place for compiling issues.\nIf you still have this problem, please file an issue against https://github.com/golang/go. Thanks!. Can you turn on logs and see if the client stays READY all the time: https://github.com/grpc/grpc-go#how-to-turn-on-logging?. > I am assuming its reconnecting so quickly it goes back to READY immediately?\nRight. We do exponential backoff, and the first retry happens very fast (1 second initial backoff).\nThe backoff is also reset when a connection is created, which will happen in your test because server only closes connections after they are created for 1 second. So you will always see the initial 1 second backoff.\nThere's currently no way to stop reconnecting.\nThere's a pending issue to support IDLE: https://github.com/grpc/grpc-go/issues/1719, (also https://github.com/grpc/grpc-go/issues/1786), but we currently don't have time to work on that.\nIf you don't want to keep the idle connections, and know that you won't need it for some time, you can close the ClientConn and recreate it later. This will introduce some overhead though.\n. Thanks for filing the bug.\nThe message limit is to protect receiver's memory, so the safe solution is to check both (wire length and after decompression).\nNote that because of the limitation of the API, this only works with the new encoding.Compressor, not the old grpc.Decompressor.\n. It seems the working addresses are from SRV records, so they are marked as grpclb addresses. In 1.18.0, we filter out grpclb addresses if the balancer is not grpclb.\nDid you use SRV purposely, or it's used by kubernetes?\nThis looks like a duplicate of https://github.com/grpc/grpc-go/issues/2575. Can you see if the answer there is helpful? Thanks!. How about the following?\n```go\npackage bar\nimport (\n    \"foo\"\n    pb \"generated/proto/package\"\n)\nfunc init() {\n    foo.RegisterFuncs = append(foo.RegisterFuncs, register)\n}\n// register registers a new StorageBrokerService\nfunc register(s *grpc.Server) {\n    pb.StorageBrokerService(s, &implementation{})\n}\n```\n```go\npackage foo\n// RegisterFuncs is a slice of RegisterFunc\nvar RegisterFuncs []RegisterFunc\n// RegisterFunc is the function that register gRPC services.\ntype RegisterFunc func(*grpc.Server)\nfunc LoadgRPCServices() {\n    for _, f := range RegisterFuncs() {\n        f(gRPCServer)\n    }\n}\n```\n. Maybe pass the config to the register functions? It should have the same effect.\ngo\n// RegisterFuncs is a slice of RegisterFunc\nvar RegisterFuncs []func(s *grpc.Server, config map[string]interface{})\n\nWill all services share the same config?\n - If so, this is a very strong assumption of all services.\n - If not, what's the point of the init we are doing here? Since you will need to call NewFunc() for a certain service with it's corresponding config, or call all NewFunc()'s with a config that's a superset of all configs, and do lookup inside NewFunc().. :authority is set to the dial target unless it's overridden explicitly by credentials. The target is parsed as defined in https://github.com/grpc/grpc/blob/master/doc/naming.md, and Endpoint will be used as :authority.\nIn your example, the target doesn't follow the URI scheme. So target parsing will fail. But :authority should be set to the unparsed string tcp://127.0.0.1:10125.\nIf I didn't miss anything, this is the behavior at head.\nThere was a bug in target parsing, fixed in https://github.com/grpc/grpc-go/commit/90dca43332f6cc944c37e16f32a82c41639e7705. It's only available after release 1.11.0.\nCan you update your gRPC and try again? Thanks!. Thanks for reporting. This is indeed a bug.\nI suspect we either set the SubConn in TransientFailure when it isn't, or we forgot to set the connection. Anyway, IMO the last connection error shouldn't be nil here.\nIs there any other logs that you got to tell whether there's an actual error (like connection broken) when this happens?. Also, it could be related to https://github.com/grpc/grpc-go/pull/2565 (see https://github.com/grpc/grpc-go/issues/2406 for more information).\nCan you try setting GRPC_GO_REQUIRE_HANDSHAKE=off and see if that changes anything?. There are more than one things happening, but the root cause is failing to reset reconnecting backoff, which is fixed in #2669. Please try the fix and see if it helps.\nBecause of the bug, reconnecting doesn't happen in time, resulting in longer than expected time in transient failure. \nAbout latest connection error: nil. This error message was introduced for errors like TLS misconfig, so when the connection fails, the RPC error will contain useful information.\nIt can still potentially be nil when connection is closed by servers. It shouldn't be noticeable because reconnecting will happen immediately. But because of the backoff bug, it's much more easily encountered.\nOne possible fix is to set the error whenever connection goes into transient failure, so it will never be nil. I will save that in another fix.. @aanm \nIf GRPC_GO_REQUIRE_HANDSHAKE=off is required, it sounds like that the server didn't finish the handshake in time.\nHow is your server setup? Is it a proxy, a gRPC server, or server behind a mux (so port is shared by gRPC and http)?. @bbassingthwaite-va \nThat is caused by #2565 (also see #2406).\nWith that, gRPC-Go clients have the same behavior as gRPC-Java, thus you need to set the same match with writers as mentioned: https://github.com/soheilhy/cmux#limitations\nAn issue (https://github.com/soheilhy/cmux/issues/64) was filed before the gRPC-Go changes were checked in. Will send a PR to update their README as well.. @aanm That is caused by #2565 (also see #2406).\ncri-o will need to set match with writers as mentioned in https://github.com/soheilhy/cmux#limitations for gRPC-Java clients.\nFor now, you can set GRPC_GO_REQUIRE_HANDSHAKE=off before it's done.. Sorry for the late reply.\nWhat's the dial target you passed to grpc.Dial()? The target is used as the authority for TLS verification.\nIf it worked before, I'm assuming you were dialing to the IP directly, is that right?\nIn that case, how did the ClientConn get a different IP later?. It's unexpected to create a ClientConn to an IP, and then connect to a different IP using the same ClientConn.\nA normal use case would be, Dial to a server-name, then resolve to IPs. The IPs can be more than one, and also can change at any time. So the SubConns are just connections to different backends of the same server (thus they share the server name).\nAlso I think certificates should normally be signed for an abstract name, like \"example.com\", not IP.\n\nSome ideas:\nThere's an option for balancer to control what credentials to use for each SubConn.\nIt's supposed to be used to set a different type of credentials (but still verifying the same authority).\nBut if you set it to a TLS creds with a hardcoded authority and ignore the one passed in authority, it should work. You may need to copy the base balancer to do this.\nYou can configure the client to skip the verification (InsecureSkipVerify). This won't protect you from man-in-the-middle attacks, but if you trust your connections, it may be fine.\nYou can also configure the server so it uses a different certificate based on the server name clients sends (SNI).\nIf some of the nodes are normal nodes, and others are fallback nodes, you only need to configure the fallback nodes to support all the normal nodes. Also it still doesn't sound right if the certificates are signed for IPs (you will have a server using certificates signed for other node's IP). If the normal nodes have server-names each, the fallback node will just support all the names.. Closing for now. Please reply if you have more questions. Thanks!. @xiang90 \nA ClientConn abstractly represents a client for a service (example.com). It may contain multiple connections, but all of them are connected to the backends serving the same service, and all the IPs are resolved from example.com.\nBased on this assumption, the authority is set to example.com.\nTo use TLS, the user would sign the certificate for example.com,and deploy it on all the backends. It also makes adding backends easier because they can deploy the same certificate instead of signing a new one.\nWith that being said, dialing to an IP but connect to a different IP works with dialer or balancer, but it breaks the address hierarchy that was assumed, so extra work need to be done to fix the gap, like the balancer option to configure credentials I mentioned above.. Thanks for the fix.\nIt seems there's some problem with the email in the commit (see the reply above). Can you fix that? Thanks!. Can you also get server side logs to see if it contains more information?\nThe handshake deadline exceeded error looks very suspicious. It would help if we know why it happened.. One more thing to try: can you set GRPC_GO_REQUIRE_HANDSHAKE=off on client side and see if that changes anything? (#2406). There's a bug in the reconnect logic, the backoff time is not reset after creating a connection.\nThe fix is in #2669. Can you guys try it and see if it fixes the problems you are seeing? Thanks!. > Actually, loopyWriter gets the message from the queue and writes it asynchronously.\n\nIt is it correct?\n\nThat's right. This was done for performance reasons. And because of that, it's hard to measure the \"send latency\".\nOne more question, how are you planning to use this latency if it can be measured? \nEven if we know when the message is sent on wire, it doesn't mean when it will be received, or in some error cases it may never arrive.\nA more reliable way would be to record the time message is sent, time it's received (at the receiving side), and then calculate the latency.. @jadekler This is ready for review. PTAL.. Link to the spec or issue?. Thanks for your review.\nWhat do you mean here?\n. I got it. Thanks.\n. Right, changed to \"gRPC will automatically convert uppercase keys to lowercase\"\n. Can you remove \"grpc\" from the function names and variable names?\nAnd make the the function names more meaningful (like percentEncode?)\n. Can you remove the assigning here if it's not necessary?\n. s/codd/code\n. Use this link?\nhttps://github.com/grpc/grpc/blob/master/doc/connection-backoff.md\n. Ptr is short for pointer. I will remove them.\n. I removed the constants and used string instead.\nI also added a check in parseTestCases() on the given test names so unsupported tests will be reported early.\n. Move this comment to a better place\n. tc.UnaryCall(...) returns a response and an error, but this function returns an error.\n. This new error contains more information on the error rpc. \n. Panic will stop the worker as well. But the worker should keep running even if benchmark client stops.\n. This function is called by rpc handler to create clients. The error returned here will be returned by the handler directly.\n. I didn't find a way to specify which cores to use in go.\nI will add a TODO so we can add this when the function is available in go.\n. The original function call hides the implementation detail about what time.Nanosecond actually is.\nIt improves the readability and is consistent with the other part of this function.\n. BalancerGetOptions\n. It should respect ...\n. It watches the name resolution updates.\nor\nIt starts a goroutine to watch the name resolution updates.\n. remove this print?\n. Never mind, it's an error, we should keep this.\n. s/rr.addrs/rr.connected\n. The old waitCh is closed, but a new one is not created.\nMove the if condition at line 274 to the beginning of the for loop?\n. No, we should not move that part of code.\nWe should add code to create waitCh before unlock().\n. also specify the capacity here\n. s/rr.addrs/rr.connected\n. Maybe we should not return here if the watcher is not dead.\n. need to close the watcher created in Start()\n. Should we rename put() here, so we can remember it's related to balancer?\n. Should we not call put() when we know this rpc will retry, and only call put() before this function returns?\nSo that put() is called when this rpc is done using the ac, and we can defer the execution of put().\n. Never mind, retry should get a new transport and a new put function.\n. rename this function or add comments to it\n. Do something with this error\n. why do we need a watcher here?\n. check if an ac for this addr already exists\n. move this if statement after checking if state==shutdown\n. It should return error if and only if Watcher cannot recover.\n. this close() is not needed\n. want not nil\n. downErr is not exported and not shown in godoc.\nWe should add something like \"the error argument in function down is net.Error\"\n. Move this for loop into watchAddrUpdates()?\nSo we can do go rr.watchAddrUpdates() and we don't need to return an error.\n. Add comments about when and what error should be returned by Get()?\nWe are actually assuming the error is ErrClientConnClosing. \n. Should check if the returned channel is nil here.\n. Should we check if t == ac.transport before doing the real update?\nEDIT: the use of t is safe here as it can only be updated in the same goroutine.\n. Thanks for your review.\nThe reason for not change the function name is this is a public api, and we are trying to minimize breakage.\nHow about changing the interface name from TransportAuthenticator to TransportCredentials? So this function will look like\nfunc WithTransportCredentials(creds credentials.TransportCredentials) DialOption\n. Can you add more explanation to the comment about what's done by this function?\n. More explanation in the comment?\n. What about removing the underscore and adding a comment about why the error is not checked?\nSeems to me the underscore is unnecessary here..\n. t.Fatalf(\"Read got err %v, want <nil>\", err)\n. t.Errorf(\"grpcMessageEncode(%v) = %v, want %v\", input, actual, expected)\n. t.Errorf(\"grpcMessageEncode(%v) = %v, want %v\", input, actual, expected)\n. We can avoid calling append() by\nopen := make([]Address, len(rr.addrs))\nfor i, v := range rr.addrs {\n  open[i] = v.addr\n}\n. @iamqizhao  The reason for not returning an error is that even if the returned error is nil, caller still needs to check whether the returned metadata is nil.\n@puellanivis  The returned value is needed by reflection. So only checking if the service is valid is not sufficient.\nServer reflection is implemented in a different package. So even if this function is only used by server reflection, it still needs to be exported. The problem is this function becomes confusing for users other than reflection.\nWe will try to come up with another way to accomplish this.\n. Will do, in the new api :)\n. Done.\n. Done.\nDial() will return the error received from TransportCredentials.ClientHandshake().\n. It's consistent with timeoutEncode().\nHowever, maybe it's better to call it statusDescEncode, because it's called as\ngo\nt.hEnc.WriteField(hpack.HeaderField{Name: \"grpc-message\", Value: grpcMessageEncode(statusDesc)})\n. Yes, this can be moved inside resetTransport. And actually, this tearDown() is redundant now, because the only error returned by resetTransport is errConnClosing.\nThe reason I don't want to remove this is that resetTransport may also return fatal connection errors like in #768, bad certificates error, and we may need to do different things based on the error and the place this function is called. Keeping it outside makes it easier. If we can make sure we always do the same thing here, this can be moved into resetTransport.\n. The caller of this function will not reset channel proceed so proceed will not be readable by other goroutines.\nIs this the reason?\n. Read is not necessary if Write already failed.\n. Done. Trace needs to start before the for loop, so it's split out.\n. Yes, this tests the expected behavior.\n. Type TestService_StreamingInputCallServer is defined by generated code, so this line cannot be changed.\n. It seems this change is a bit too invasive.\nThe purpose for this test is actually not for \"real timeout\". It's to test what gRPC's behavior is when ClientHandshake returns context.DeadlineExceeded.\nMy proposal is to use a custom testing-only creds instead of \"tls\".\nThis creds can return context.DeadlineExceeded for the first time (ignoring context), and return nil error for the second time.\nIn this way, your changes on minConnectTimeout and to split startServer are not necessary. The change to export backoff.BaseDelay is probably also not necessary.\nLet me know if you need help.\n. creds can be other credentials, not just tls\n. One side effect is that minConnectTimeout is shared by all tests. Other tests may fail if they run in parallel with this test.\n. Is this function needed for you original issue? If not, please leave it to another fix.\n. miss spelling in function name, s/wit/with\n. grpclog.Fatalf(\"%v.UnaryCall(_, %v) = _, %v, want _, %v\", tc, req, err, expectedErr)\n. Remove \"stream\" from fatalf\n. Remove \"stream\" from fatalf\n. Remove \"stream\" from fatalf. And you missed the variable for the first %v\ngrpclog.Fatalf(\"%v.Recv() returned error %v, want %v\", stream, err, expectedErr)\n. Extra space at the beginning.\n. How merging works is not important. The point is that all metadata will be sent, not just the last one.\nWhat's the thing you find confusing here? If you think \"merged\" is not a good word, what about all md's will be combined? Or other suggestions?\n. Done.\n. done\n. Done. :)\n. That will be a massive API change, we don't want to do that unless necessary. In current state, this change has no influence for most users.\nAnother important reason is that we want to limit the use of OverrideServerName(). It should only be used in grpclb.\nCalling OverrideServerName() after the creds is used in dialing can have unexpected behaviors, and we want to avoid the confusion.\n. By iff I mean \"if and only if\".\n. Not sure if I understand your comment correctly.\nIf you mean we don't need function NewClientTLSFromCert(), that's probably true. It's just a wrapper of NewTLS(). But it helps to cover the implementing details of TLS creds.\nIf you mean we don't need the newly added API OverrideServerName(). grpclb needs to override the server name without knowing the real type of the creds (it could be something other than TLS), the new API is necessary to accomplish this.\n. Double different\n. move to after b.done\n. b.waitCh = nil\n. s/2014/2016\n. use ok?\n. use ok\nand add else { break }\n. fatalf\n. s/Inject/inject\n. s/Grpc/GRPC\n. golint is complaining about this line\ndon't use underscores in Go names; var stream_req should be streamReq\n. timeElapsed collected here is not time elapsed since last reset.\nResetting is done after merging histograms, but timeElapsed is calculated before merging.\n. This is not accurate. Serve() also returns if the error is not a net.Error.\n. Use Timer instead of sleep.\n. I'm not sure if reusing timer will cause any issues here.\nI propose making timer a local variable and creating a new one every time.\nTo interrupt the timer, instead of resetting the timer, we can add a context to server and do a select\ngo\nselect {\n  case <- timer.C:\n  case <- s.ctx.Done():\n}\n. The former one looks good to me, it's probably better to add parentheses:\nServe returns when lis.Accept fails (except for temporary network errors)\n. I think it should be ok to remove this line...\n. Just use cancel as the variable name?\n. I changed it to the diff I made in helloworld example.\n. Done.\n. remove this TODO\n. define this as a file level const\nconst tlsDir := ...\n. do listen() on localhost:0 and get the listening address from lis\n. Move this to the place right after listen().\nIf something fails before this line, lis will not be closed().\n. You can simplify the error message to be just\nt.Fatalf(\"Failed to listen: %v\", err)\n. check the error here?\n. TLSInfo.AuthType always returns \"tls\".\nHow about comparing something inside ConnectionState instead? like Version?\n. the select is not necessary, just <-done.\n. defer close this connection\n. The fields below the newline are only available when Client is false.\n. indentation of these 3 lines.\n. add period at the end.\n. add period at the end.\n. add period at the end.\n. add period at the end.\n. variable expectedCode is not necessary.\nif grpc.Code(err) != codes.Unimplemented\n. This function won't test TLS even if useTLS is true.\nHow about changing the function to func DoUnimplementedMethod(cc grpc.ClientConn) and use the cc created in main()?\n. var req, reply proto.Message\n. The URL is in the format of /package.service/method, so you are actually testing unimplemented service here.\nChange it to /grpc.testing.TestService/UnimplementedCall\n. if md, ok := ... and put the following metadata code inside if ok\n. if md, ok := ... and put the following metadata code inside if ok\n. It's not guaranteed that the keys are consecutive. If I use a list, I need to check if the value is nil.\n. Changed it to TagConn()\n. done\n. The default value is not empty string. Is this on purpose?. channel has a special meaning in go... So we use connections instead.\nChange to func newConn(...)?. And print them in logParameterInfo().. s/his/this. ... the RPC has no deadline?. s/provides/providers?. mention the service providers as in MethodConfig?. if sc, ok := ..., ok {. Is this needed?. ok is not necessary, if sc is closed, the value received should be nil.. if sc, ok := ...; ok {. The watcher not started here. Remove from the comment?. Change this to:\ngo\nif cc.dopts.balancer == nil && cc.sc.LB != nil {\n    cc.dopts.balancer = cc.sc.LB\n}\nSo the following if can remain unchanged.. Rename this to scChan?. This cancel will cancel the RPC when newClientStream returns.. Name the return value as mc instead of sc?. s/sc/mc. The renaming is unnecessary.. List all the test cases.. Merge this into the previous var( ... ).. why is this named clientNewPayload, will we have one for server?. and \"client\" in the name here.. Is this file necessary? We already have the commands in the md file.\n(And why do we need the gofmt here?. Add alias for this import because it is a proto.. Wrap this in a function createMock returning a mocking client?. Add a link here to the folder.\nAnd a one line file summary?. add a space before nil. How will this work? How to control the server behavior?. In the documentation, the request size and the response size are the same with the sizes used in interop package (different from the number you are using here though).\nCan you reuse the function DoLargeUnaryCall defined in interop instead of defining a new function here?. revert this line.. The comment should start with the variable name:\n// statusExists records if grpc-status field exists.. And reorder the variables.. Maybe we should not try to read the content of state if stats.err != nil.\nFor example in the case where http-status is not a valid number and strconv.Atoi() returns an error.. Is it easy to use net/http package instead?. Is it OK to remove this?\nstream.Read() should be a blocking call.. Check if the err is a StreamError, and compare err.Code?. Move this above traceInfo?. Add \"for a unary RPC\" at the end so it's consistent with Header and Trailer?. Use net.SplitHostPort?\nhttps://golang.org/pkg/net/#SplitHostPort. Add grpc.FailFast(false)?. Is it better to use a *peer.Peer here?\nSo that we don't need to copy peer in recvResponse.. We can still make a copy and return the copy of Peer to the user, so we can avoid the double pointer.\nI was thinking that we should avoid copying Peer inside gRPC.\nAssume most users don't care about peer, so they won't use this new CallOption. In that case, we don't need to copy Peer and return it. But the copy in recvResponse will still happen.. Can you also test that WithAuthority has no effect if TransportCredentials are present?\nI think just copying TestTLSServerNameOverwrite and add WithAuthority to Dial would do the work.. Also specify this field doesn't work when TransportCredentials exist.. Can this be moved inside if size > 0?. About the naming, seems to me this is more like a UnknownServiceHandler. Or probably DefaultHandler?. And in the comment, we should mention how the custom handler will be used (and that it's bidi streaming).\nAll unknown gRPC services/methods will be handled by this handler as bidirectional streaming RPCs.. This is for unknown service, different from the following one for unknown method.. I was saying the content of the string for LazyLog is wrong... It will only say \"unknown method\", even in the case of \"unknown service\".\nThere's no requirement on the description. But I would prefer to keep it unchanged, in case someone is doing string matching on that.. Function processUnknownStreamHandler is not so necessary now.\nI think we could just check s.opts.unknownStreamDesc and call s.processStreamingRPC here directly.. We don't actually need the dummyService if it only contains the nil.\nHow about passing in a nil, and in processStreamingRPC, add a check on srv.. I'm not against adding functions. But it seems this function is not necessary though...\nAnd the boolean handled always equals to s.opts.unknownStreamDesc != nil. Seems to me bidi-ServerStream is confusing.\nHow about just bidi-streaming RPC service handler?. Add comments here to explain what rstStream means and our special handling for streams quota pool.\n. Explain a little bit about the race.. rstStream indicates whether a RSTStream needs to be sent to the server to signify that this stream is closing.\n\"is sent\" is a bit confusing.. How about TestExceedDefaultMaxStreamsLimit, as opposed to the previous test TestExceedMaxStreamsLimit.. Add something saying that \"the server won't send the settings frame for MaxConcurrentStreams\"?. Move this check before goroutine is created?. We don't need to flush the ping, we will flush the headers later.. CAS. CAS(a, 1, 0). Done. Done. The change containing this type was reverted. I'm trying to avoid introduce http as a dependency of the exported APIs.\nI'm using http.Header in the implementation though...\nI can change this if you think the dependency is OK.. gRPC should check those env variables by default. And this will be the implementation of the default behavior in gRPC.. Done.\nWas thinking that we will eventually get rid of the specific code for old versions, and to git rid of that, we can simply delete those files ;). Done. Done. Done. Refactored as discussed. s/MAX_CONNECTION_AGE/MaxConnectionAge. Just do maxIdle.Stop(), we don't care about the channel anymore.. typo connection. why is the size 24?. Done. Also check for connection closed.. Moved inside the if condition.. I think the purpose of this is to return an error in certain conditions, to test that the client gets the correct error.\nChanged the comment to include this.. Done. The names are inconsistent.\nEither rename timeElapsed to elapsedTime, or rename the two new variables?. Use itoa?\nhttps://golang.org/pkg/strconv/#Itoa\nAnd the following line, too.. Change this error message?. Should we also have a NewOutgoingContext?\nSo we can make NewContext call NewOutgoingContext,\nand probably guide new users to use the new function instead?. move to rpc_util.go. Deprecated: use WithMaxReceiveMessageSize instead.\nAnd return WithMaxReceiveMessageSize(s).. remove the empty line here to indicate that mkp is protected by mu?. Make this atomic read?\nThe lock is probably even not necessary.... Can we just start a gRPC server here?. Why do we still make a clone here?. Comment this should never be called with nil?. gRPC itself should not return code InvaludArgument.\n(If this is consistent with other languages, probably we should comment that).. Move those to clientconn.go, around line 106? Since they are shared by Unary and Streaming.\nRemove \"client\" from the variable name.\nAnd do\ngo\nconst (\n    defaultMaxReceiveMessageSize = 1024 * 1024 * 4\n    defaultMaxSendMessageSize = 1024 * 1024 * 4\n). Remove this const if it's not used.. What if mc.MaxReqSize == nil && cc.dopts.maxSendMessageSize >= 0?. We should probably make those pointers, too.... Also add the comment about this behavior on ServiceConfig.Methods.. If this is recv on server side, InvalidArgument makes sense.\nBut if this is recv on client side, InvalidArgument is weird.... Remove \"server\" from the variable name.\nAnd\ngo\nconst (\n    ...\n). Deprecated: ... \nAnd return MaxReceiveMessageSize(m). Mention the default value as the server side?. InvalidArgument is also weird here. This is to send response.... The error message is not updated for this.. And the error code InvalidArgument here, too.\nThis is to send response.... This is probably too specific. It should be OK to do something similar to the error handling on recvMsg.\nMost of the following code is copied from line 690...\n```go\n    if err == io.EOF {\n        // The entire stream is done (for unary RPC only).\n        return err\n    }\nif st, ok := status.FromError(err); ok {\n    if e := t.WriteStatus(stream, st); e != nil {\n        grpclog.Printf(\"grpc: Server.processUnaryRPC failed to write status %v\", e)\n    }\n} else {\n    switch st := err.(type) {\n    case transport.ConnectionError:\n        // Nothing to do here.\n    case transport.StreamError:\n        if e := t.WriteStatus(stream, status.New(st.Code, st.Desc)); e != nil {\n            grpclog.Printf(\"grpc: Server.processUnaryRPC failed to write status %v\", e)\n        }\n    default:\n        panic(fmt.Sprintf(\"grpc: Unexpected error (%T) from recvMsg: %v\", st, st))\n    }\n}\nreturn err\n\n```. I did a brief check on the error returned by sendResponse, the error should be one of:\n\nio.EOF\nstatus\ntransport.ConnectionError\ntransport.StreamError. remove this TODO. Yes, if will be copied from the URL.. Done.. I tested it in the proxy_test but forgot to set the response body...\nChanged the close() to defer, and it works now.. done. done. done. Done. Removed.. Done. Removed.. b.clientStats is protected by the mutex. I don't want to put the rpc call between the lock/unlock.. The timestamp here is a copy of the ptypes Timestamp.... Done. In that case, these two values will not be protected by the same pair of lock/unlock.. Done. Done. Move these two tests to http_util_test.go instead of deleting them?. Move the two variables for server to server.go?. Nit: move these two new functions before the const SupportPackageIsVersion4.. cancel was defined at line 113.\nAnd this cancel shouldn't be called when newClientStream returns.\nIt will be stored in the stream at line 228, and will be called when the stream finishes.. Can you also remove this? This was moved to the if statement above.\nI believe this is a git merge error.. group these two with\ngo\nvar(\n  ...\n). s/channel/connection\nThe word channel is misleading in golang.... GetRequestMetadata also takes parameter uri.. Make authData a map[string][]string and reuse authData?. Why is this fatalf better than grpclog.Fatalf?. This is an API change, we would rather avoid changes like this.. The server side generated code is for testing only, and client side code was moved to grpc package to avoid the dependency problem. They can be considered hand-written. So for grpclb.proto, we could just generate the messages, not the services (call protoc without the grpc plugin).\n\nAnd also, it may be a good idea to use go generate to generate this pb.go files. We can add the //go:generate comment to those go files that import pb packages.. Instead of returning the same msg every time, is it possible to store the message sent by last Send() and return it here?. Nit: this is not sent message.\nHow about trying to send message larger than ...?. WithMaxReceiveMessageSize is not a DialOption anymore.\nWe could say, \"use WithDefaultCallOptions(WithMaxReceiveMessageSize(s)) instead\",\nor \"use CallOption WithDefaultCallOptions instead\".\nAlso, see my following comment, WithDefaultCallOptions should be renamed.. Rename this to MaxReceiveMessageSize, by convention we don't name CallOption WithSomething.. Rename to MaxSendMessageSize.. Format\ngo\nvar defaultServerOptions = options{\n  maxReceiveMessageSize: defaultServerMaxReceiveMessageSize, \n  maxSendMessageSize: defaultServerMaxSendMessageSize,\n}. The comment,\n... max message size in bytes the server can receive., since we name the option ...Receive..... Similar to the above comment\n... the server can send.. trying to send.... trying to send.... Let's just need to check io.EOF here. If n>0, err will be ErrUnexpectedEOF.\nFrom the doc of ReadFull:\nThe error is EOF only if no bytes were read.\nIf an EOF happens after reading some but not all the bytes,\nReadFull returns ErrUnexpectedEOF.. Add comment like \"should not panic\". return \"near-misses\". Should this be `s.requestRead`?\n\nAnd where is s.requestRead called?. s/form/from\nAlso, explain a bit more here?\nWe won't remember what we decoupled when we get used to the new flow control structure.. remove this comment. Use google.golang.org/grpc/benchmark/... instead of benchmark/benchmark17_test.go benchmark/benchmark.go?. Also, write result to a temp file instead of results?\nThis should be OK if using temp file takes too much effort.. This line generates the bench name. Spaces are replaced with _ automatically. So, the string generated is:\nUnary-Trace maxConcurrentCalls: 1, reqSize: 1, respSize: 1\nthe result shows:\nUnary-Trace_maxConcurrentCalls:_1,_reqSize:_1,_respSize:_1-12\nHow about remove the  and : from Sprintf, and reformat it as:\nUnary-Trace-maxConcurrentCalls_%v-reqSize_%v-respSize_%v,\nresult will be:\nUnary-Trace-maxConcurrentCalls_1-reqSize_1-respSize_1. Add one more parameter tracing to BenchClientUnary and remove this function?\nWe could probably add a parameter for Unary and Streaming as well. But let's not do that now. Since Unary and Streaming RPC benchmarks can be different.. Define slices for maxConcurrentCalls, reqSize, respSize and do 3 loops?\ngo\nfor _, c := range maxConcurrentCalls {\n  for _, req := range reqSizes {\n    for _, resp := range respSize {\n    }\n  }\n}. grpc.EnableTracing = enableTrace. We probably should keep Unary and Streaming separate, as two sets of benchmark tests, because we may want to send multiple requests in the streaming call. But we can keep this in this PR, and change it later when we need.\n. s/1 * megabyte/megabyte. Or 1 << 20. If we just need to split host and port, we can use this function https://golang.org/pkg/net/#SplitHostPort?. remove this comment and the one on port number?. what happens if name is an IP address?\nHave a special static watcher for IP, whose next returns once and blocks forever afterwards.. How about adding a NewResolver() function and unexport this type?. The watcher type doesn't need to be exported.. Instead of adding one more parameter to the function, why not add Network into struct ServeInfo?\nSo you don't need to change all the caller of StartServer. What's the purpose of this?\nThis function should always set nw.Kbps to the parameter kbps passed in.\nIf we want to run benchmarks only with kbps -1, we should make sure this function is called with kbps=-1, instead of setting nw.Kbps to -1 inside the function.. I would prefer to name the returned conn just conn. clientConn usually means a gRPC client.. Return nil, err instead of fatal?. Change this dialer, too.. Pass a nil network instead.\nIf you move nw into ServerInfo as what I suggested in previous comment, this line doesn't need to be changed.. See my comment in server/main.go.. Changing 1 to 0.999 seems arbitrary.\nIn what situation will this be 0?. Can return directly here:\ngo\nreturn nw.TimeoutDialer(net.DialTimeout)(\"tcp\", address, timeout)\nAlso, nit, I would do this in remove some of the indentation\n```go\n    conn := NewClientConn(\n        target, grpc.WithInsecure(),\n        grpc.WithDialer(func(address string, timeout time.Duration) (net.Conn, error) {\n            return nw.TimeoutDialer(net.DialTimeout)(\"tcp\", address, timeout)\n        }),\n    )\n. Same as runUnarygo\nreturn nw.TimeoutDialer(net.DialTimeout)(\"tcp\", address, timeout)\n``. Check the error before this.. Also mention the default frequency?. Nit: why is the name \"setXXX\"?\nIt's like a c function that takes a pointer to a struct and updates the struct's member fields.. How about also making this error a var?. We don't need thisdonechannel.\nClosingupdateChanwould work..make([]*Update, len(update), len(update))?. Is the \"compile\" in the function name on purpose? Or you wanted \"compare\"?. If we are trying to build aSetfor updates, this map should probably be amap[Update]bool.\nThis won't work forUpdates with same address but different metadata.. Is the slice size related to the length ofsrvs`?\nIf we don't know anything about the slice, we could just do var newAddrs []*Update.\nappend should still work.. Related to logs: don't use grpclog.Printf.\nDo a rebase and you will have grpclog.Infof, grpclog.Warningf, grpclog.Errorf.. Do we still need this sort?. Return here?. Make this a map[string]error and also compare the errors?. Should we make this script more general instead of spcific for travis? So it's easy for us to run and compare two results locally?\nFor this particular line, we can parse $TRAVIS_COMMIT_RANGE in travis.yaml and pass base commit and current commit as command line arguments. If directory benchmark/compare already exists, why do we need to go get again?. Should the path be $GOPATH/google.golang.org/grpc/benchmark/compare?. How about writing the result to some file in /tmp directory?\nThis may be work better if we run this locally, so we don't have these two extra files in our working directory.. What's the purpose of this?\nAlso, the output on travis from this is ls: cannot access benchmark/compare/: No such file or directory. This actually assumes the script is run inside grpc directory.. Why copying the directory?. It seems compareLatency should be passed using a flag instead.. Print the error content as well\nfmt.Printf(\"failed to read file: %v\\n\", err)\nAnd should probably just return here.. Just var part1, part2 []string, the default value is nil.. Does https://golang.org/pkg/strings/#Fields do what you want here?. I would make the value of this map a struct instead of a string slice, just to make it easier to maintain.\nThe values in this struct could be parsed values, like floats or ints.. Will we apply settings more than once?\nI'm thinking if we could use store instead?. Do we need to also return the stream quota?. Done. PTAL.. return ff.rr.Notify(). cc.resetAddrConn([]Address{{Addr: target}}, cc.dopts.block, nil). Not pickfirst.. cc.conns is a map, not slice. This would work:\ngo\nvar currentAc *addrConn\nfor _, currentAc = range cc.conns {\n\ufffc   break\n\ufffc}. Do not export this function.. if _, isPickFirst := cc.dopts.balancer.(*pickFirst); isPickFirst {. s/PickFirstBalancer/pickFirstBalancer\nAnd add period at the end.. This comment would look wired without the context of this PR. Try to make it more general, just describe what's happening.\nSomething like \"Not pickFirst, create a new addrConn for each address.\". Move this comment to type pickFirst.. Extra space before Get.. Although it's a wrapper around RoundRobin?. remove this select.. Move this before the for loop? We only need to calculate this once.. Make curAddr a pointer and set it to nil?. Done.. Done.. Done.. Done. Done.\nReturn defaultBuilder if map lookup is not ok.. Done. Done. Done. This should be documented in the connectivity semantics doc. Also, it's not clear when a SubConn should go IDLE now.... Done. Done. Done. Done. Most balancers don't care about the final status of RPCs.\ngrpclb is the only one cares right now.. Done. Done. Done. Done. Done. Done. The resolver is a bit different from balancer I think.\nFor balancer, users cannot override the default balancer. The default will always be pickfirst.\nFor resolver, the default is whatever the users set here.\nIf we read the map in SetDefaultScheme function, we need to make sure the builder was already registered.\nWe could make this SetDefaultBuilder(b Builder).\nThen if users set the default resolver to one with scheme \"blah\", and then dial with \"blah:///\", we should find the correct resolver to use. So we need to register the default Builder as well. . Starting up a new resolver means we need to support switching resolver.\nThis sounds more complicated than just calling a function on the existing resolver.... If we have to tear down the ac, we will still keep the acWrapper. So this function doesn't need to return a new acWrapper to balancers.\nI think we should try to keep ac in the same state before this.\n - If the ac was IDLE, that means there was no \"currently-connected address\". We update the address list and don't connect.\n - If the ac was not IDLE, then it's other already connected or is trying to connect. We will auto-connect with the new addresses.. Done. I would like to keep this in this PR.\nWe should implement the stats interface using x/net/trace to do tracing like this.. Done. Done in https://github.com/grpc/grpc-go/pull/1430. Done.. Done. Can you remove the trailing space? It's causing gofmt failures.. Remove this.. Name this as pickFirstUpdateAddresses() and comment that it's only used when balancer is pickfirst.. The error comes from user's handler, why would it be a transport.StreamError?. If we want to run go generate, we need to get stringer as well: golang.org/x/tools/cmd/stringer\nBut then this is not just make proto... It's make generate.... Remove this empty line. How about just var res []*Update?\nLength of updates may be much shorter than len(newAddrs)+len(w.curAddrs).. Add a local variable for a+\":\"+strconv.Itoa(int(s.Port))?. Local variable for a+\":\"+w.port?. Done.. Done.. Done.. For Idle and Shutdown, it won't trigger a callback.\nFor the other 3 cases, yes, they can all happen.\nWe actually can have Ready->Connecting->Ready.\nI updated the comment to mention this can trigger state update for the subconn.\nI also added a TODO to check all the state transitions.. Also make this 1.9*?. We need to also call cancel() before break, otherwise the last context will still be leaked.. Good catch! I believe this needs to be protected by the mutex. Will fix it soon.. Fixed. Please take a look.\nThanks for pointing this out!. We are going to change this to 1.9* shortly.\nI wonder if there's an easy way to find what the latest go version is.... Should we keep this?\nThough this works on travis, someone trying to run make proto locally may fail because of these two missing packages.... Done. Done. Done.\nKept the for loop for t.Errorf.\nThe test checks how many times Errorf gets called.. Done. Done. Done. Done. :man_facepalming: version is missed here.\nAnd make $check_proto true so travis will test it?. Rename to NewBuilder.\nOtherwise it will be dns.NewDNSBuilder().. Why not d.watcher()?. Nit: add period at the end.... Nit: add period at the end.... Why not i.watcher()?\nAnd, we don't need this goroutine here. It should be OK to call NewAddress() directly here.. Ideally, the same builder can be used to build multiple resolvers. So each resolver shouldn't modify the internal state of the builder.\nMaking a copy of freq should be good here.. Is this reset necessary?\nWhen resolveNow is called, the watcher will read from d.rn, unblock and reset the timer.. Do an unblocking write?. Handle ServiceConfig before addresses, so if the ServiceConfig changes balancer, we can avoid a bit overhead.. And don't call NewServiceConfig if it's empty?. If the reason this function takes a []byte and returns a []byte is that it calls json functions, how about making this function takes a string and returns a string, and doing the conversion between []byte and string within this function?. What if the result contains both balancer addresses and backend addresses?\nThere's a fallback mechanism that use backends directly if cannot connect to the remote balancers.. Call NewAddress() in ipResolver.ResolveNow()?. https://golang.org/pkg/strings/#HasPrefix. If it's not hard to explain, add a comment.. Why do we need this goroutine?\nIs it to support ResolveNow()? If so, we could call NewAddress() in ResolverNow(), right?. Nit: comment still shows d.b.freq.. Include the error returned by SplitHostPort in the return value.. cliHostName?. time.Sleep(time.Millisecond)?. Add an init() to register the builder.. It seems nobody calls these two functions.. Move these two cancel functions to a defer\ngo\ndefer func() {\n  if err != nil {\n    cancel()\n    // Don't call connectCancel in success path due to a race in Go 1.6:\n\ufffc    // https://github.com/golang/go/issues/15078.\n    connectCancel()\n  }\n}. This if will always be true. The only other state is unreachable set by notifyError, and that code was deleted.\nActually, is errorChan === t.ctx.Done()?. Remove which have currently released. Done. Done. Comments updated, PTAL.\nMost users don't need to call NewBuilder. But if they want to have a custom balancer on top of roundrobin, they can call this function to create a builder. (I did this in new grpclb at first, but removed it later).\nBut there's already another way to get a roundrobin builder: balancer.Get(\"roundrobin\"). I will unexport this function.. Done. Done. Done. Done. Done. Done. Done. Done. Done.\nThis could happen to the newer balancer in my naive balancer switching code. But I think we should make sure this never happens.. Done. Done. Done. Names removed.. Done. \ud83e\udd23. Mutex removed.. Done. Done. Done. The connection state for roundrobin in the case of no resolved address is available is transient_failure. So failfast RPCs will fail in this case.... Done. Done. Done.. Done. Done. Done. scState is taken. Renamed it to scStateUpdate and scStateUpdateBuffer.. Done. We will have more than one balancers (and more than one balancerWrappers) in the future.\nI was thinking each balancerWrapper gets notified of all the subconn state changes, and each of them decides to call balancer functions or not.. Marked this as a TODO.. I want to avoid calling any balancer methods after balancer is closed, so in the balancer implementation, you don't need to check the balancer state in each method.\nMade the suggested change.. When updating addresses for a SubConn, if the address in use is not in the new addresses, the old ac will be tearDown() and a new ac will be created. tearDown() generates a state change with Shutdown state, we don't want the balancer to receive this change.\nAdded a comment to explain this.. It's done now. \ud83d\ude01. If you are OK, I would like to keep this explicit. It's not that bad I think.. newPickerWrapper now.. Done. Done. Done. Done. put is to collect the stats of RPCs sent on the picked transport. This transport was not picked for the RPC eventually, so let's not call put here?. Done. Names removed.. Made them 50ms now.. Done. Done. cc.conns is set to nil in cc.Close, so the error returned when cc.conns == nil is different from the error returned when ac == nil...\nAdded a TODO for error cleanup.. It's done now.. Done. Done. Names removed.. Done.. Done.. Done. It's done.. s/Stringer()/String() ?\nStringer is the name of an interface with a String() function.. Done. type balancer will conflict with the imported balancer package. I renamed them to rrBuilder and rrBalancer.... In the normal cases, when handshake returns an error, rawConn gets closed.\nIn this test, handshake returns a special error, which tells gRPC to not close the connection.\nThis is to sleep and wait for gRPC to see the error, and then test if the connection was closed.\n100ms is picked randomly.... A server can actually Serve on multiple listeners. This only works in the case of one Serve.. If service == nil, this function returns \"method\", is this right?\nWhen searching in the map, we search for /service/method first, then /service/, but never for method. Do we need to store the value in the map if service == nil?. go\nsc := ServiceConfig {\n\ufffc   LB:      rsc.LoadBalancingPolicy,\n\ufffc   Methods: make(map[string]MethodConfig),\n}. Move the all service config related code (structs and functions) to a new file service_config.go and also add unit tests for the json parsing function.\nPut all the parsing code in a function\ngo\nfunc parseJsonServiceConfig(js string) (ServiceConfig, error)\nIn ClientConn, you can do\ngo\nif sc, err := parse(js); err == nil {\n  cc.mu.Lock()\n  cc.scRaw = js\n  cc.sc = sc\n  cc.mu.Unlock()\n}. It should be OK to test service config with only one of the test envs. Then we don't need this wrapper function and loop.\nJust start the test with\ngo\nte := testServiceConfigSetup(t, tcpClearRREnv). Move the code around so the declaration of a variable is closed to where it's used?\nYou actually don't need this string variable if you don't need to check against it.. go\nfor {\n  if cc.GetMethodConfig(\"/grpc.testing.TestService/EmptyCall\").WaitForReady != nil {\n    break\n  }\n  time.Sleep(time.Millisecond)\n}\nProbably also add some timeout:\ngo\nvar processed bool\nfor i := 0; i < 1000; i++ {\n  if cc.GetMethodConfig(\"/grpc.testing.TestService/EmptyCall\").WaitForReady != nil {\n    processed = true\n    break\n  }\n  time.Sleep(time.Millisecond)\n}\nif !processed {\n  t.Fatalf(\"service config is not processed by gRPC after 1 second\")\n}. Return error early instead.\ngo\nsc, err := parseServiceConfig(js)\n    if err != nil {\n        return err\n    }\n    cc.mu.Lock()\n    cc.scRaw = js\n    cc.sc = sc\n    cc.mu.Unlock()\n}. Move this struct to the new file, too?. Typically, String() function only returns string.\nRename this to getPath().. Indentation for this and some of the following tests.. Leakcheck is unnecessary for tests like this.. Did you forget to push some commits?\nThis code is still here.. This struct is same as the real ServiceConfig struct except for the second field. I prefer to not have this temporary struct, and reuse the existing structs.\njson does a partial parsing if the struct only contains part of the fields, so we could do Unmarshal twice, first time for the service/method name, second time for the remaining configs. The second Unmarshal can go directly to the existing MethodConfig struct.\nhttps://play.golang.org/p/GCSXcP1yQk\n``go\nconst testJSON ={\"name\": [ {\"service\": \"foo\", \"method\": \"bar\"} ], \"waitForReady\": true}`\ntype jsonName struct {\n    Service string json:\"service,omitempty\"\n    Method  string json:\"method,omitempty\"\n}\ntype methodConfigName struct {\n    Name *[]jsonName\n}\ntype MethodConfig struct {\n    WaitForReady bool\n    Timeout      string\n}\nfunc main() {\n    buf := []byte(testJSON)\nvar mcn methodConfigName\nerr := json.Unmarshal(buf, &mcn) // for name only.\nif err != nil {\n    panic(err.Error())\n}\nfmt.Printf(\"%+v\\n\", mcn)\n\nvar mc MethodConfig\nerr = json.Unmarshal(buf, &mc) // for everything other than name.\nif err != nil {\n    panic(err.Error())\n}\nfmt.Printf(\"%+v\\n\", mc)\n\n}\n``. Added a const (avar` actually) to roundrobin_test.\nIn theory, a user can re-register a roundrobin and Get will return the new one. So a exported variable in balancer package won't work.. Done.. Done. No defer!. Done. This is not the real check. This is to make sure each server will be picked (all the subConns are connected).\nThe following loop does the real roundrobin check.. The returned string is in the format of \"/service/method\".. Set this to \"\" in scWatcher? Just to be \"consistent\".. Still doesn't look right.... One way to test this is to make GracefulStop() block by having a pending streaming RPC, and check that Serve() doesn't return.\nWe have end2end tests that does similar things.. Changing this option also changes the output file path. (Run make proto to see the result).\nIt seems our current proto structure (or just the way we run protoc) doesn't work well with this new go_package. We need to fix that.. The fix for this is to change the --go_out parameter for go generate in grpclb_test.go\n```diff\ndiff --git a/grpclb/grpclb_test.go b/grpclb/grpclb_test.go\nindex 46c1fe5..cc8d75c 100644\n--- a/grpclb/grpclb_test.go\n+++ b/grpclb/grpclb_test.go\n@@ -16,8 +16,8 @@\n  \n/\n-//go:generate protoc --go_out=plugins=:. grpc_lb_v1/messages/messages.proto\n-//go:generate protoc --go_out=Mgrpc_lb_v1/messages/messages.proto=google.golang.org/grpc/grpclb/grpc_lb_v1/messages,plugins=grpc:. grpc_lb_v1/service/service.proto\n+//go:generate protoc --go_out=plugins=:$GOPATH grpc_lb_v1/messages/messages.proto\n+//go:generate protoc --go_out=plugins=grpc:$GOPATH grpc_lb_v1/service/service.proto\n// Package grpclb_test is currently used only for grpclb testing.\n package grpclb_test\n``. Good catch. Reverted.. The state is set bytransportMonitortoTransientFailurebefore this function is called.\nI removed this comment and add a comment intransportMonitor.. done. done. This can also be used to override the registered grpclb builder with a new fallback timeout.\nUpdated the comment.. done. done.lb.fullServerListis the received response from remote server (which is a slice ofServer`s).\nNobody should modify the content of this slice.. done. done. done\nWe don't check this returned error, though.... Close() is guaranteed to be called from one goroutine (never in parallel).\nSo this should be OK.. done. done. done. Removed. done. error is an interface, so errPicker is also an interface, and interface types are not valid receivers...\nNeed to define another struct:\ngo\ntype errPicker struct {\n    error\n}. done. Nit: define a testing const, and print %q to avoid escaping \"s.\n```go\nconst testMethod = \"/package.service/method\"\nif !ok || method != testMethod {\n\ufffc       t.Fatalf(\"Invoke with method %q, got %q, %v, want %q, true\", testMethod, method, ok, testMethod)\n}\n``. It feels like this ispicker's functionality.\nMoved tonewPicker()function.. Thisdeferneeds to be inside theif, and there's one error returned outside of theif.. done. This could happen for the fallback case (use the backend addresses from the resolved list if no response is received from the remote balancer).. It could happen for fallback.. done. removed \ud83d\ude05.NewSubConn()doesn't return any usefull errors now.\nIt's here in case we want to add some fields to theNewSubConnOptions, the returned error may become more meaningful.. Whenlb.doneChgets closed (inlbBalancer.Close()),lb.ccRemoteLBis also closed.\nSo this RPC will fail in that case.. Changed toErrorfand continue.. done. Hmm, right, context.\nThere's no need to create a new context here.SendLoadReport()should block on the stream's context.. done. done.\nAdded a unexportedwithContextDialer(). Does this need to be exported?. What's the purpose of thisfinish()?finishclosess.done, which may result in race inwait()`.\nSee following comment on ErrStreamDrain.\nIf we do a cleanup on stream goaway, this finish() may become necessary.. Do we need this anymore? Seems it's only returned by transport but never check.\nIIRC, the purpose of this error is exactly what Unprocessed() is for.\nAnd if we no longer need this error, we can also (maybe) remove s.goaway channel.\nBut should probably make this a TODO for another cleanup PR.. This is actually not true anymore...\nCan you remove these two lines and update the link for install protoc?\nmarkdown\nFirst [install protoc](https://github.com/google/protobuf/blob/master/README.md#protocol-compiler-installation). A side note:\nbalancerBuilder == nil means there's no custom balancer specified, by gRPC will still pick a balancer to use (the default is pickfirst, service config could also specify a balancer).\n. In most balancers, addr.Addr will be a IP address, not a server name.\nWe don't want to use the IP as the Authority to do handshake.. want state to be not Ready?. context will be leaked if t.Fafalf before this.\nMove this before the if check?. Not cancel on t.Fatalf?\ndefer?. Rebased on top of the retry PR. This was removed.. Also mention what happens if the request compressor is not registered?. Reorder if and else?\ngo\nif rc := stream.RecvCompress(); dopts.dc != nil && dopts.dc.Type() == rc {\n    dc = dopts.dc\n} else if rc != \"\" && rc != encoding.Identity {\n    comp = encoding.GetCompressor(rc)\n}. reorder if and else?\ngo\nif rc := stream.RecvCompress(); s.opts.dc != nil && s.opts.dc.Type() == rc {\n    dc = s.opts.dc\n} else if rc != \"\" && rc != encoding.Identity {\n    decomp = encoding.GetCompressor(rc)\n}. Don't defer.\ndefer is slow (https://github.com/golang/go/issues/14939). Delete this?. How about this: create a file staticcheck.ignore, and do \nstaticcheck -ignore \"$(cat staticcheck.ignore)\" ./.... We should try the cmp package.. done. done. It's up to the resolver on how to deal with parallel ResolveNow(). In the dns resolver implementation, a new ResolveNow() is ignored if the previous one hasn't been processed.\nFrom ClientConn's point, if multiple ResolveNow() result in multiple NewAddress() calls, only the last update will be kept. And we also ignore this update if it's the same as the cache we have.. Done again... And also the following one.. Done.. Done again.... Everything following until the blank line...\nI move the code around and added a comment.. Done. Moved err logging to the beginning of the for loop.. lbPicker is splitted into three different pickers:\n - errPicker\n - rrPicker: only used for fallback, never collects rpc stats\n - lbPicker: does grpclb two layers roundrobin\nThe new lbPicker only sets done function if returned error is nil.. Added withResolverBuilder dial option.. Done. Done. If WriteStatus is called after this unlock, before ht.do, the panic will still happen.. Also update the comment.. Thanks for taking a look. Fixed.. Done. Done * 2. Done. Done. The idea was this is like resolver.Build(UserOptions).\nKeeping this as it is now.. Done. Done. Caller of this function should do that. Comment updated.. The idea here is, if balancer is set by a dial option, all switch balancer call will be no-op.. How are we going to use this Unmarshaler?\nAre we going to call json.Unmarshal() instead of this? Also test json.Unmarshal()?. From Unmarshaler's godoc:\nBy convention, to approximate the behavior of Unmarshal itself, Unmarshalers implement UnmarshalJSON([]byte(\"null\")) as a no-op.\nhttps://golang.org/pkg/encoding/json/#Unmarshaler. Done. Right!\nFixed and added a test.. Done. Done. Done. @dfawley I updated the following if else while doing a rebase. Please take another look. Thanks!. Added a defer close, but kept the order of preface and create transport.. Done. Done. Done. This API change looks good to me, and nobody else should be calling this function.\nA slightly different way for this would be to pass in a struct, for example *transport.ServerConfig, instead of a stats.Handler directly. Also we could unify the constructor of http2server and serverHandlerTransport into transport.NewServerTransport.\nBut let's leave these for future changes.. A proposal to resolve the possible race you mentioned: move this HandleRPC call out from this func and call it after ht.do returns:\ngo\nerr := ht.do(func() {})\nif err == nil {\n  if ht.stats != nil {\n    ht.stats.HandleRPC(s.Context(), &stats.OutTrailer{})\n  }\n  // Other stuff\n}\nThis should work because the func is scheduled by ht.do doesn't return in the middle. So we can assume it would eventually happen, it would be OK to handle OutTrailer before it actually happens.. Similar to WriteStatus, move this out from func:\ngo\nerr := ht.do(func() {})\nif err != nil {\n  return err\n}\nif ht.stats != nil {\n  ht.stats.HandleRPC(s.Context(), &stats.OutHeader{})\n}\nreturn nil. Remove these two comments?. parseTarget parses scheme://authority/endpoint_name into \ngo\ntype Target struct {\n    Scheme    string\n    Authority string\n    Endpoint  string\n}\nThe result of \"unix://etcd:0\" is actually {Scheme:\"unix\", Authority:\"\", Endpoint:\"\"}.\nThe following dial won't work because t.Endpoint would be empty.\n~Also, if you want this dialer to get \"unix://etcd:0\", you would need to pass \"passthrough:///unix://etcd:0\" to grpc.Dial. Does this work for you?~\nEDIT: with #1889, only passing \"unix://etcd:0\" should work now.. Done. https://golang.org/pkg/net/url/#Parse would be better here.. Make this static so we don't re-compile the regex in every dial?\nAlso, do ^[a-z]+:/ so that unix:/addr would also work?\nWe would like this to work with unix:addr as well.\nBut url.Parse doesn't parse \"example.com:443\" correctly. It also fails to parse \"192.168.0.1:443\". url.Parse(\"unix://addr\") gives me\ngo\n(*url.URL)(0xc4200b0000)({\n  Scheme: (string) (len=4) \"unix\",\n  ...\n  Host: (string) (len=4) \"addr\",\n  Path: (string) \"\",\n  ...\n})\nwhile url.Parse(\"unix:///addr\") gives me\ngo\n(*url.URL)(0xc42009c000)({\n  Scheme: (string) (len=4) \"unix\",\n  ...\n  Host: (string) \"\",\n  Path: (string) (len=5) \"/addr\",\n  ...\n})\nAnd url.Parse(\"unix:/addr\") gives me\ngo\n(*url.URL)(0xc4200c2000)({\n  Scheme: (string) (len=4) \"unix\",\n  ...\n  Host: (string) \"\",\n  Path: (string) (len=5) \"/addr\",\n  ...\n})\nHow about dialing to host if path is empty string?\nSo that path takes priority over host, and unix://authority/addr would also behave correctly.\ngo\naddr = t.Path\nif addr == \"\" {\n  addr = t.Host\n}. It would also be great if you could make line 440-451 a separate function (in file rpc_util.go) and write some unit tests for it.. Nice catch!  timer is hard.\nFixed, PTAL.. It worked for me: https://play.golang.org/p/RUdTtKqBBem\nThe result I got was\n```go\n(url.URL)(0xc4200c2000)({\n  Scheme: (string) (len=4) \"unix\",\n  Opaque: (string) \"\",\n  User: (url.Userinfo)(),\n  Host: (string) \"\",\n  Path: (string) (len=27) \"/tmp/etcd-unix-so-325611655\",\n  RawPath: (string) \"\",\n  ForceQuery: (bool) false,\n  RawQuery: (string) \"\",\n  Fragment: (string) \"\"\n})\n```. I would prefer to not export this function. It's a helper function for the default dialer, not a general purpose function for users.. Stop the timer, or\ngo\nvar i int\nfor i=0; i<500; i++ {\n  if ok, err = f(); ok {\n    return\n  }\n  time.Sleep(10 * time.Millisecond)\n}\nif i>=500 {\n  t.Fatal()\n}\nAlso, t.Fatal also prints the line number. So it would help us to locate the failure if this function returns an error, and t.Fatal gets called in the test function.. Yes, it's updated as part of the service config.. Done. The key comes from metadata, it has to be string. No hashing should be applied to the stickiness key.. Done. Done. If this is not necessary, should we skip this step?\nIt makes the example look more complicated than what it actually is.. Are those for debugging? Delete them?. Nit: move this Listen below the options initialization, before Serve.. This file and some other new files are not used.\nAlso, you could reuse the test TLS files in testdata. Sample of usage: client, server.. IMO, must is too strong here.\ninterceptors are actually not necessary. It's just convenient to install interceptor to validate all RPCs.\nHow about moving the metadata related sentences above? And mention interceptor as a suggestion after that?. Nit: could instead of must?. Nit: could instead of must?. Rename this to altsHSAddr and add work alts to the flag name?. Add alts to the name?. It could be nil. We need , _ otherwise it may panic.. Done. But what if we later want to add another field to resolver.BuilderOption that's not to be configured by the user?. Also mention service handlers for streaming RPCs? It shares the same guidelines, but I think it would be better to document them explicitly.. What if the balancer wanted to remove the last addrConn from cc? It will also be zero in that case.. What's the advantage of using status error here?\nWhat other error codes will we return?. Move this above the pacakge comment to be consistent with other go generates?\nAnd I'm curious if this will be rendered in godoc if we leave it this way.... Done!. TLS makes things tricky again...\nWhat if the connection failed because TLS misconfigure?. Yes..\nRemoved.. The test will continue to run after this. Is that what you want?. Delete v as you did above?\nif getTotal() == defaultWindowSize. cause client to run out of ...?. Done!. Done! Also changed the test a bit.. Then the function still needs to return the stream as well.\nWill keep the current code.. Done. Moved.. Deleted this TODO.. This is \"all streams\", including those in activeStreams + those out of stream quota + those with no data to send, is that right?. I would make wanting (and capable) explicit.\nHow about just with data to send and with stream level flow control quota?. Explain a bit about the gosched variable in this function?. Explain a bit more on this? E.g.\nFullMethodName is the method name that NewStream() is called with, the canonical format is /service/method.. Also change the error message when it's deadline?. Should we just delete the comment on this line?. Resolver is not necessary in this test.\nJust Dial(\"nonexist:///non.existent\") would work.. defer cc.Close() (after the error check). done. done. iff?. Name the struct fields?. Name the struct fields? So you can also leave out sync.Mutex{}.. The error message is outdated.. Any reason that the parameter is not a pointer?. Fixed. Doing init(). Is the main purpose of this function to reset backoff or to wake up addrConns?. Also set ac.connectRetryNum = 0?. Delete this line?. Nit: this break is unnecessary, right?. Fprintf, or just Printf?. Check error and exit with non-zero value.. Call the returned variable errored? Or failed?\nconnErr looks like a type error to me.. Done. Done. Done. Done. Done. Done :). This indeed makes me feel sad. But done anyway.. Done. Skipped for GRPCLB.. @cesarghali what do you think about this?. Renamed to CredsBundleModeBackendFromBalancer.... Done. Those consts are kept in internal but renamed to non alts/tls specific names.. Renamed to NewWithMode.. Done. Removed. Un-exported. The GCP checking in google default creds is removed.. Removed. @cesarghali I removed this checking so we don't need to export IsRunningOnGCP. Let me know if you think this is not OK.\nI also modified the error string that will be returned by ALTS if not running on GCP.. Done. Done. oauth.NewApplicationDefault calls FindDefaultCredentials, and seems it handles non-GCP cases: https://github.com/golang/oauth2/blob/d2e6202438beef2727060aa7cabdd924d92ebfd9/google/go19.go#L47:6. This is returning code unknown?\nAlso use codes.Unknown instead.. Let's mark this as for testing only.. Why is this field exported?\nAlso, why not just use the proto message here?. Why are the fields exported?. I remember this was discussed, and is expected.\nIf we want to remove the port, how about https://golang.org/pkg/net/#SplitHostPort?\nOr we require the port number in serviceName.. Or leave this field nil?. If we use proto message in the server struct, this won't be necessary.. Do we need this now?. Merge this and TLS flag into one that takes a string?\nOtherwise we will need to deal with *alts==true && *tls==true, and it's hard to add another one (insecure maybe), just like in interop client.. What does this mean? Is this a special case or just an error?\nIf this is an error, it's not necessary because the following parsing would report this as well.. Use https://golang.org/pkg/net/#SplitHostPort. Check the error here.. Done. Done. I think the false case is more interesting because the other settings will be ignored.\nUpdated, PTAL.. Done. Done :smile_cat: . Done :smiley_cat: . Done :joy_cat: . We tried to avoid this mutex here for performance reasons, and that's why we do those atomic actions.\nThe state transition in gRPC is limited comparing with the http2 spec:\n - the client will never be in read-done mode because that means the end of the stream, so the transition can only be active -> [writeDone] -> done\n - the server will never be in write-done mode, and it can only be active -> [readDone] -> done\nHowever, because the Stream struct is shared by the client and the server, so the state is made a atomic, and the logic is handled in the client/server specific code. We have plans to split Stream for client and server, and will be able to better solve this problem.\nI would suggest let's revert this back to the previous state if you are OK with it.. This should work, but let's do a RST_STREAM instead.\nThe ideal process of a normal RPC is, the client sends end-of-stream when it's done sending, and the server finishes the RPC with trailer (and also is end-of-stream).\nSo when the client receives an end-of-stream before it sends end-of-stream, it is unexpected. So RST_STREAM sounds better here.\ncloseStream is already sending RST_STREAM to terminate the stream when necessary. The parameters rst and rstCode are for that. So if you set rst to true, things should happen automatically. We can use http2.ErrCodeNo for the error code.\n. This print is unnecessary.. Check the error returned by send.. Why is this deleted?. Also do\ngo\nhealthWatchChecker(t, stream12, healthpb.HealthCheckResponse_NOT_SERVING)\n?\nThis is to check the update to service1 would affect both streams watching service1.. I believe this is unnecessary and also kind of incorrect.\nsrv.server is supposed to be the service implementation for the method. But for unknown method, such implementation doesn't exist, and passing in a \"random\" pointer doesn't seem like a good idea.\nThe old behavior, passing in nil pointer, would make it clear that the implementation of the service doesn't exist.. Done. Make this a if == Serving not switch case.. Done. balancer/roundrobin doesn't do NewSubConn. All it does is Pick.\nWe can make this an option that defaults to false.\nThe current API base.NewBalancerBuilder() doesn't take config. I think it's OK to add another one with config.. Nit: rename this field? This sounds like a []Address to me.. Copy some of the comments here? And explain why it's important to keep cc.resolverWapper nil.. The name... This looks like a sync.Once...\nHow about resolvedEvent?. hmm, fixed.\nManually tested and confirmed it's working.. Do we still modify user's code? Is git status --porcelain at the beginning of this file still necessary?. Was this changed manually? (Same question for route_guide mock). Do an ok check to avoid panic is Security is not the expected type.. Are the expected suites a subset of the whole CipherSuiteLookup? I would prefer to not export the map for testing.. Done. Done.. Done. Done. Hmm, good idea.\nBut they are all named Conn (net.Conn vs syscall.Conn), and are \"duplicate fields\".... Not related to the changes in this PR.\nCan this just be verifyResultWithDelay(func() error)? (No bool in return value).\nIt seems the function either return true, nil or false, <non-nil>.. I wanted to keep example data separate from test data.\nAlso I renamed the package to exampledata.. Done....... grpc.examples.echo should be good.\nI didn't add option go_package =, I think we don't need it for this.. And the directory name needs to be renamed to grpc_examples_echo, to match the go package name.. Done. Done. Done. Done. Done.. Done. Done. Returning nil will not cause error because we check it. nil will result in a re-pick.\nWe will only get nil here when things are not in sync (between the wrapper and the v1-balancer). This means the connectivity state has changed, and a new pick was (or will be) updated. So I think re-pick makes sense.\n. Added an internal function pointer.. Done.. There can be a race if this is not in the lock\ngoroutine-1: HasFired()=false\ngoroutine-2: Fire()\ngoroutine-2: take lock, set Not_serving, release lock\ngoroutine-1: take lock, set status (to Serving). done. Added else.. Just return unimplemented for all streaming RPCs, if we are not going to use them.. Move this after the RPC goroutines and move time.Sleep() out?\nSo the code reads as\n```\ncc = Dial()\ngo RPC1()\ngo RPC2()\ngo RPC3()\nsleep(2s)\ngo startServer()\nwg.Wait()\n```\nI think it's a bit clearer this way.. Maybe inline this function? It will add code duplication, but I think it's easier to read that way. And examples are meant to be read by people.. This is making it more like a test. I think just printing the error should be OK.. I think it's OK to always print it. You can print the response when there's no error.\nAlso, it's probably a good idea to add a README.md to explain what's done and why.. Done. Done. Let's use wait for ready only. Failfast is a deprecated term.\nWe should probably also deprecate Failfast API and add wait for ready.. Add a comment before this line:\n// Binary wait_for_ready is an example for \"wait for ready\".. Copy the comment of FailFast here.\nAnd mark FailFast as deprecated with\n// ...\n// By default, RPCs are \"Fail Fast\".\n//\n// Deprecated: use WaitForReady. Add package comment\n// Binary client is an example client.\nFor servers\n// Binary server is an example server.. Use service echo.. Let's make address and port on the server side configurable.\nhttps://github.com/grpc/grpc-go/blob/master/examples/features/metadata/client/main.go#L35\nhttps://github.com/grpc/grpc-go/blob/master/examples/features/metadata/server/main.go#L39. Add something like:\nClient cancels the RPC by cancelling the RPC context. Both client and server will receive status with code Cancel.. fmt.Printf. Let's make the client take address, not port.\nhttps://github.com/grpc/grpc-go/blob/master/examples/features/cancellation/client/main.go#L35. Package comment\n// Binary client is an example client.. Package comment\n// Binary server is an example server.. Package comment.. Nit: this is not necessary. You can just block forever (with select {}).\nCtrl-C will be caught by the default handler and the program will quit anyway.. Same as client.\nAnd we don't really care about cleanup since the program exits.\nYou can also start channelz server last, so the last line can be s.Serve(lis).. GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info go run server/main.go\nBut then it could be misleading, users may think channelz also requires those levels.\nAdd another section maybe?. break?. Delete empty lines.. Nit: this can be a const.. This is not a pointer, right?\nhttps://github.com/grpc/grpc-go/blob/36f3126920fe326b7874e730e3cc26546186d8f0/rpc_util.go#L328-L330. You don't need reflection. fmt.Printf(\"%T\", m) prints the type of m.. Empty line.. %T. %T. %T. Actually, this read lock is correct. It protects the reading of ac.cc.mkp. We should keep it.\nThe problem is the writing of ac.dopts.copts.KeepaliveParams. It needs to hold ac.mu.\nIn the older version mentioned in the issue (#2542), the writing happens outside of ac.mu (unlock before this line, lock after this line).\nhttps://github.com/grpc/grpc-go/blob/97da9e087c5cf7a11263637efb410e08c51a7a0f/clientconn.go#L904-L914\nBut this is already fixed in the new version\nhttps://github.com/grpc/grpc-go/blob/c71aa62423b37215980f9c3141eef06f4c35e998/clientconn.go#L997-L1002\nSo I think the race only exists in the old version, and the changes here are all unnecessary.. This doesn't seems necessary to me. ac.mu is held before adjustParams is called. ac.cc.mu doesn't affect anything in this line.\nThe race seems to only exist in the old version.. We don't need s either, so we return \"\" to save (a very small amount of) memory... See the comment here.\nHow about changing the comment to returns (\"\", \"\", false)?. This might be too fancy: atomic? So we can run subtests in parallel?. staticcheck is complaining about unused credsNone...\nAdd a case credsNone:, and in default just fatal?. Is this also EXPERIMENTAL?. Revert this. GetServerSockets is the function name.. Revert. SocketData is the field name.. Revert this file. TestService is the service name.. Revert HPACK.. Link to https://github.com/grpc/grpc-go/blob/master/Documentation/keepalive.md. How about select {} here, and run both client and server with GODEBUG=http2debug=2? So you can see ping frames.. Done. Done. Done. Done. GODEBUG=http2debug=2 for server too?. This needs to be Fatalf. Travis isn't happy about it. It is... It is the \"this\" to \"add\".... I have cleanup changes in another branch, was trying to keep this change simple so we can cherrypick it to previous releases.. If reconnecting failed, we will continue above, and won't reach this.\nThis only happens when we successfully create a connection, and we want to skip any backoff if this connection breaks.. Note that t.Write doesn't actually write the request on wire. transport has a queue of messages, and Write only pushed the message to the queue. So returning from Write doesn't mean the message is sent.\nThe gap between begin and HandleRPC actually only measures the time spent on encoding and compression.. This time is not when the first byte is received. The bytes for this message could have been in transport's receiving queue for a while, it's just not read by gRPC.\nIn many cases, especially for streaming RPCs, similar to send(), this only measures time spent on decoding and decompression.. Done.. 1s should be long enough for client to notice the connection is down.\nI'm adding more checks to make sure the right backend is picked.. That's right, there's will only be one SubConn.\nThe following for loop breaks after the first iteration.\nIt's done this way to avoid rewriting the SubConn state change handling code.\nAnd also we can reuse the pickers (both roundrobin and the fancy grpclb picker).. Done. Done. If the field was named NoBytesSent, we would want to set it to true (It's great that's not the case :) )\nDeleted and explain in a comment.. Didn't test this because returning a non-Ready SubConn from picker is very racy.. This is just being paranoid.\nIdeally, after taking ac.mu, the first thing should be checking stats==Shutdown so we can be sure whatever we read in ac is sane.\nAn made-up situation (not the case today) is, if we somehow set ac.addrs = nil when closing addrConns. Then the for loop in tryAdd won't execute, and we will not detect Shutdown.\nI almost wanted to do the following so we don't need the manual checks everytime. But it needs a lambda...\ngo\nfunc (ac *addrConn) withLock(realWork func()) {\n  ac.mu.Lock()\n  defer ac.mu.Unlock()\n  if ac.state == Shutdown {\n    return\n  }\n  realWork()\n}. Done.\nI didn't do that because the first parameter is addrs... But I think it's easier to read with addrs in the name.. A thought: if we also want to use this interface for interceptors, we need to make sure all ClientConn features are available.\nMethod ClientConn() *ClientConn should work.. Let's also check firstIntKey to make sure it's not in context (interceptors are not accidentally called more than once).. This works but looks like an overkill. Recursion like this is hard to understand and maintain.\nThis function builds the chain top-down. It's only necessary when you don't have the real invoke.\nSince we have invoke, we should be able to build the chain bottom-up.\ngo\nfunc chain(its []UnaryClientInterceptor, finalInvoker UnaryInvoker) UnaryInvoker {\n    ret := finalInvoker\n    for i := len(its) - 1; i >= 0; i-- {\n        it, inv := its[i], ret // Trap values in local variables.\n        ret = func(ctx context.Context, method string, req, reply interface{}, cc *ClientConn, opts ...CallOption) error {\n            return it(ctx, method, req, reply, cc, inv, opts...)\n        }\n    }\n    return ret\n}\nIt also works with 0 interceptors, so in ClientConn.Invoke, you could just do \ngo\nchain(interceptors, invoke)(ctx, method, args, reply, cc, opts...). This will be called every time handleServiceConfig is called. return pointer from parseServiceConfig?. Add a field to ServiceConfig for the raw js string.. We still give users a chance to pass invalid string here.\nHow about returning a dial option and an error from ValidateServiceConfig(), so the only way to create a default service config dial option is to validate it first?\nThe name ValidateServiceConfig would be inappropriate, time to bikeshed.. Also mention that all interceptors will be concatenated, and WithUnaryInterceptor will be prepend.. Delete body of setUp and call setUpWithOptions.. Being a bit paranoid:\nWe should also make sure this interceptor is called. One way is to append something to the end of reply, and then check that reply == expectedResponse + surfix.. ",
    "bradfitz": "I don't have access to see the commit, so I can't review.\n. This discussion should happen in the normal grpc discussion places first, not in grpc-go.  Once a language-neutral plan is agreed upon, then grpc-go can implement it.\nIn the meantime, users outside of grpc-go can implement a net.Listener/net.Conn that sniffs the beginning traffic and routes depending on whether it looks like a TLS handshake (with the ALPN for \"h2\") or an plaintext HTTP/1 request \"GET / HTTP/1.1\" etc.\nBut let's please not do some gprc-go-specific hack here if other languages don't do the same.\n. It's hard to design something until there's a defined problem.\n. @tv42, yeah, grpc-go was developed in parallel with bradfitz/http2. They share a framer, but they have different server implementations. So it's not exactly clear whether and where it would be possible to make one defer to the other. I'd personally like to see them unified someday, perhaps with grpc-go just being a handler under http2 itself, using the http2 server code. But then, grpc might be faster doing it by hand. I'm not sure.\n. I actually don't know. I just assumed there was one, since they have a fancy website (http://www.grpc.io/) and all.\n. Looks like https://groups.google.com/forum/#!forum/grpc-io \n. @philips, probably. But grpc-go and Go's http2 server support developed independently with different goals (both using the same http2 Framer package). Whether they should be merged is an open question, but one I haven't had time to consider.\n. My plan is to add a ServeHTTP method to *grpc.Server.\nIt will still be usable as it is today, but you will gain the ability to use it with the net/http package and put it in an HTTP mux, etc.\n. Okay, that wasn't too bad. Few hours of work and https://github.com/bradfitz/grpc-go/commit/72479a46a520238e2d11926370911d9247801d71 seems to work. *grpc.Server now has a ServeHTTP method. It needs tests and WriteHeader implemented (for streaming RPCs probably?) and then I'll send a PR.\n. Note that only the http2 spec says that field names must be lowercase (https://httpwg.github.io/specs/rfc7540.html#rfc.section.8.1.2). The hpack spec says nothing about it. I believe in theory other protocols could use hpack but make different decisions about the case of fields.\nSo I'm inclined to say it's the caller's responsibility.\nBut perhaps we can add a boolean to hpack.Encoder like AllowCapital bool defaulting to true from NewEncoder so people CAN do weird things if they want, like the options in the http2 framer to send illegal frames.\n. Related: #75, where I'm adding a ServeHTTP method to *grpc.Server, so you can do whatever muxing you want before gRPC takes over.\n. Use context.WithCancel and call the CancelFunc to abort a long-lived operation.\n. Done.\n. PTAL\n. @iamqizhao, yeah, I saw that. I can't reproduce yet locally. I'm trying to figure out what's different between travis and my machines.\n. @iamqizhao, also, all these tests are way too noisy with expected log spam mixed in with real errors. We need to quiet these tests up so real problems are easy to see.\n. The problem with this test is that it creates more goroutines than the race detector is capable of tracking.\nI will shorten this test when running in race mode.\n. Done.\nRemoved example changes for now. They can come in a separate change later.\n. Done. PTAL.\n. PTAL. I've updated this.\nI also got sick of Github's crap pull request code review system, so I'm mirroring this review here:\nhttps://go-review.googlesource.com/#/c/19272/\nFeel free to leave comments there instead of here and I'll keep updating both this pull request & that review (increasingly automated)\nIn particular, Gerrit lets you easily see the diff between versions. Here you can see that I just deleted the net.Listener implementation you didn't like: https://go-review.googlesource.com/#/c/19272/2..3/server.go\n. Rebased. Gerrit mirror is updated as well: https://go-review.googlesource.com/#/c/19272/4\n. Travis finally scheduled. Build passed.\n. /cc @iamqizhao \nI'm sending this separately from my other CL. This should be fixed regardless.\n. @zellyn, correct. But it's true that I didn't audit all files outside of end2end_test.go. Doing that now and already found at least one.\n. /cc @iamqizhao \n. Yeah, for private auto-generated code, I don't care too much about style.\n. You can do this yourself once #75 and #514 land.\n. Actually, if you need this to fail at compile time, this is fine.\n. Squash these commits into one. We don't need to see typo fixes in the git history.\nAlso, you're right that this shouldn't introduce a dependency.\nEspecially a dependency just for an interface. An interface can be defined locally if needed. And this interface is too large and not stable. It's marked experimental.\nI don't think we want this. Let's identify the problem before we jump to a solution.\nPlease close this PR and let's discuss on a bug, if there's not already one open.\n. @iamqizhao, Go's http2 server does this aggregation automatically. It has a write scheduler that only flushes only when a packet is full or there's nothing else to send. So if we switch, I think this should get a lot easier to control. (or ideally: not control, if it just works)\n. I think this is only a problem with the tests. e.g. this fixes CompressOK:\npatch\n@@ -1283,10 +1308,12 @@ func testCompressOK(t *testing.T, e env) {\n                t.Fatalf(\"TestService/UnaryCall(_, _) = _, %v, want _, <nil>\", err)\n        }\n        // Streaming RPC\n-       stream, err := tc.FullDuplexCall(context.Background())\n+       ctx, cancel := context.WithCancel(context.Background())\n+       stream, err := tc.FullDuplexCall(ctx)\n        if err != nil {\n                t.Fatalf(\"%v.FullDuplexCall(_) = _, %v, want <nil>\", tc, err)\n        }\n+       defer cancel()\n        respParam := []*testpb.ResponseParameters{\n                {\n                        Size: proto.Int32(31415),\n. I have fixes for these locally too. I can include them in my CL if you want, or send separately.\n. I'll send mine, since it has the leak checker in the same CL. I'll send it first, before my bigger changes.\n. I noticed more failures on the Travis run once I enabled this:\n--- FAIL: TestHealthCheckOnFailure-4 (5.31s)\n    end2end_test.go:356: Running test in tcp-clear environment...\n    end2end_test.go:356: Running test in tcp-tls environment...\n    end2end_test.go:356: Running test in unix-clear environment...\n    end2end_test.go:356: Running test in unix-tls environment...\n    <autogenerated>:31: Leaked goroutine: goroutine 10361 [select]:\n        google.golang.org/grpc/transport.(*http2Server).controller(0xc2080e6000)\n            /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:626 +0x84f\n        created by google.golang.org/grpc/transport.newHTTP2Server\n            /home/travis/gopath/src/google.golang.org/grpc/transport/http2_server.go:134 +0xb66\n@iamqizhao, this one doesn't look as obviously easy to fix. I'm going to disable the leak checker for now and get it submitted, and then you can fix this bug.\n. Should we re-open this bug? I didn't mean to close it in the commit message but somehow managed to.\n. @mwitkow, the ones I fixed I think only affect tests, not prod. I don't think you'll see a difference yet. @iamqizhao is still looking for the transport.(*http2Server).controller leak.\n. @mwitkow, but please comment here with details of which goroutines of yours you see leaking.\n. @iamqizhao, this also has a few minor test/style cleanups and the --only_env flag for testing. Hope that's okay.\nI'm off early for birthday activities now.\n. PTAL\n. PTAL. Mirrored in Gerrit for better code review at: https://go-review.googlesource.com/#/c/19311/\n. The actionable thing is to remove all time.Sleep calls in tests and add proper synchronization.\nPlease re-open and assign to me. You can add it to a Low-Priority milestone or label perhaps?\n. Whoops, sorry!\nLGTM\n. @mwitkow, yes, net/http.Server accepts connections and sets TCP keep-alives on connections, before http1 or http2 is determined.\n. Your proposed wording update invites more questions: if it immediately closes all open connections, how does it then later abort all active RPCs?\nWhat does aborting mean?\n. Great. Let's put that in docs.\n. I left code review feedback on https://go-review.googlesource.com/19426\nYou guys don't do code review for gRPC?\n. @iamqizhao, I'm sending you smaller CLs out of my big one, so #514 is hopefully more reviewable.\n. /cc @dvyukov\nDmitry, what are the limits on how many goroutines the race detector can track?\n. More cleanups taken out of #514 \nIf this is merged I will rebase #514 again.\n. @iamqizhao, are you ready for this one if I update it to tip?\n. Rebased. No new changes.\n. Yes, context propagation would be great.\nI would also like to address #289 at the same time as any API changes, and change the grpclogger.Logger interface to be much smaller. Currently it is:\ngo\n// Logger mimics golang's standard Logger as an interface.                                                                                                                                                           \ntype Logger interface {\n        Fatal(args ...interface{})\n        Fatalf(format string, args ...interface{})\n        Fatalln(args ...interface{})\n        Print(args ...interface{})\n        Printf(format string, args ...interface{})\n        Println(args ...interface{})\n}\nBut I'd like to get it down to a more Go-like (small) interface. Something like:\ngo\ntype Logger interface {\n     Log(context.Context, error)\n     Fatal(context.Context, error)\n}\nAs a transition, we can use contest.TODO and errors.New+fmt.Errorf to fix existing callers. Notably, error can be stringified. That's all that matters.\nThen we create a new grpcerrors package with our error types, for other logger implementations to do what they want with, converting them to key/value pairs, assigning priority levels, etc.\nMaybe we'll even define some optional interfaces in grpcerrors for error types to implement. (but won't be required... using fmt.Errorf is always an option).\nAnd we can keep all the helper funcs in package grpclog like Printf, Fatalf etc, at least for now.\n. Note that my existing PR #546 doesn't change how logging works. It just makes it expected log output quiet during end2end tests, so unexpected output becomes noticeable. But PR #546 can go in independent of this.\n. I definitely mean error and not string. Making it a string doesn't fix the structured logging problem. An error is just an interface value, so we can add types over time, and also means we can do things like lazy stringification.\n. The context could contain the requested log level, which the log sites could inspect to decide whether to log. It's the value being logged that has a level, inspectable via an optional interface on the error values (but people would use helpers).\n. @peter-edge, I don't think anybody was proposing the latter.\n. @dsymonds, we're not going to put log level in a context.\n. > The context could contain the requested log level\n@dsymonds, perhaps that quoted comment of mine was misinterpreted. It doesn't contain the log site's log level. It can contain the requested level of verbose logging from the caller. (e.g. by default, normal log level, but certain requests or during debugging or sampling could request verbose logs as opt-in on that context).\n. @dsymonds, yeah, and I think the majority of it will remain so. But it's an option if/when we need it.\n. @iamqizhao, can we hold on the FUD until there's data to suggest otherwise? Yes, it's a newer implementation, but the x/net/http2 server is not new at all, and the new ServerTransport implementation isn't much glue between the two.\nThere might be a bug in the glue, but I don't think it calls for warning about performance problems.\nBut by all means, if people find problems, please report them.\n. Which numbers on https://github.com/cockroachdb/rpc-bench don't look good?\n@tamird \n. The http.Handler-based server only works with TLS right now.\nThe \"PRI\" you're seeing is the dummy kinda-not-really-HTTP request that is announced by http2 clients during upgrade.\nIf you used TLS, though, the x/net/http2 package (or Go 1.6's default server) would do the upgrade for you. But neither x/net/http2 or Go itself automatically upgrades unencrypted connections to http2, so you're seeing the PRI request.\nI plan to make this work for gRPC, but it's not a super high priority at the moment.\n. Can you run the server with Go 1.7 and env GODEBUG=http2debug=2 and report what it says when grpcServer.ServeHTTP fails?\n. > I guess next step would be to add proper servehttp_test.go tests to cover the malfunctions?\nIt did have tests. I'm curious what could've regressed.\n. I sent #1406 with some docs.\n. A more complete bug report would be helpful if you have time.\ne.g. minimal repro steps.\nI'll try to create one from inspecting the code, though.\n. @iamqizhao, it seems that at least parts of transport.ServerTransport interface can be used more concurrently than documented. Can you help clarify that the expected rules are so we can document them? In transport/http2_server.go I see mutexes around all three Write methods, suggesting that all methods may be called concurrently, so I expect those are the implicit rules.\nI can likewise serialize all work in the ServeHTTP transport, but I also want to clarify the ServerTransport docs, and add a new end2end test exercising concurrent calls (for whatever concurrent calls are permissible).\n. I have a repro:\n``` go\ntype concurrentSendServer struct {\n        testpb.TestServiceServer\n}\nfunc (s concurrentSendServer) StreamingOutputCall(args *testpb.StreamingOutputCallRequest, stream testpb.TestService_StreamingOutputCallServer) error {\n        var wg sync.WaitGroup\n        for i := 0; i < 2; i++ {\n                wg.Add(1)\n                go func(i int) {\n                        defer wg.Done()\n                        stream.Send(&testpb.StreamingOutputCallResponse{\n                                Payload: &testpb.Payload{\n                                        Body: []byte(fmt.Sprint(i)),\n                                },\n                        })\n                }(i)\n        }\n        wg.Wait()\n        return nil\n}\n// Tests that two concurrent Send calls from a streaming server don't race.                                                                                                                                   \nfunc TestServerStreaming_Concurrent(t *testing.T) {\n        defer leakCheck(t)()\n        for _, e := range listTestEnv() {\n                testServerStreaming_Concurrent(t, e)\n        }\n}\nfunc testServerStreaming_Concurrent(t *testing.T, e env) {\n        et := newTest(t, e)\n        et.testServer = concurrentSendServer{}\n        et.startServer()\n        defer et.tearDown()\n    cc := et.clientConn()\n    tc := testpb.NewTestServiceClient(cc)\n\n    req := &testpb.StreamingOutputCallRequest{}\n    stream, err := tc.StreamingOutputCall(context.Background(), req)\n    if err != nil {\n            t.Fatalf(\"%v.StreamingOutputCall(_) = _, %v, want <nil>\", tc, err)\n    }\n\n    var ngot int\n    for {\n            reply, err := stream.Recv()\n            if err == io.EOF {\n                    break\n            }\n            if err != nil {\n                    t.Fatal(err)\n            }\n            ngot++\n            t.Logf(\"Got: %q\", reply.GetPayload().GetBody())\n    }\n    if ngot != 2 {\n            t.Errorf(\"Got %d replies, want %d\", ngot, 2)\n    }\n\n}\n```\nUpdated docs, test, and fix coming.\n. @tamird, I'll keep my new implementation which fixes the race, but I'm adjusting my test per @iamqizhao's view of the rules (no concurrent Sends allowed). Do you have a link to your code so I can see what you're doing that you hit a race regardless?\nIs it https://github.com/cockroachdb/cockroach/blob/18a69ab17df31db2eb4b4c4cfbc7157c2a9ebbb0/gossip/server.go that I'm looking at, or which version had problems?\n. After debugging and chat, I think the problem was you were doing stream sends concurrent with returning from your method.\nWe're going to close this, but feel free to re-open or file a new bug if you have a minimal repro showing a race.\n/cc @iamqizhao \n. @tamird, fixed. But I forgot to update the transport tests. Fixing that too before the second version.\n. PTAL\n. We could use the Github Milestone feature to track prioritization. And/or Labels.\n. /cc @iamqizhao \n. Please don't merge this branch. I will investigate and write a proper fix & new tests.\nCan you merge https://github.com/grpc/grpc-go/pull/556 first, though? It will make it easier for me to add new tests for this issue without git merge conflicts.\n. No rush. I'm in India for a conference anyway. When you merge #556 I'll send you a fix for this issue with a new test. I see I was stressing the wrong test. I suspect I'll be able to see the flakiness if I run TestRPCTimeout-4 in a loop instead. I misunderstood your previous message (I blame jet lag).\n. @iamqizhao, yes, leave it to me. I have my talk here in India in a few hours and then I'll fix this.\n. Btw, I have some pending work on my laptop I did on the plane back home today. Will send it off tomorrow.\n. Update your http2 dep. Either:\n$ go get -u golang.org/x/net/http2\nOr:\n$ go get -u google.golang.org/grpc\n. I vote no. Pin the hash you need after it's tested with your application. It's not like we're maintaining many branches. There is exactly one.\n. Sorry, this still sounds like a solution in search of a problem. git log is a pretty good changelog.\n. It makes sense when you're doing releases and maintaining branches. It doesn't makes sense when you're doing neither. Maybe one day, but not yet.\n. LGTM\n. > Failed on travis (it seems there are still unhandled errors):\nOh, indeed. That test was flaky. It was doing an exact match on the string, but there are multiple places the context cancellation can fail from, and they stringify differently. I've changed it to only look at the error type and code and not the stringification and now it's no longer flaky. (tested locally with hundreds of runs; previously it failed very quickly with that)\nPTAL\n. /cc @iamqizhao \n. Whoops, I forgot to run all the tests. Fixing...\n. PTAL. The TestInvokeLargeErr test is fixed now. It wasn't speaking valid http2 before but it wasn't being detected until now.\n. The http2.MetaHeadersFrame change exposed an existing problem but it is not the problem itself.\nThe http2 spec is very clear:\nhttp://httpwg.org/specs/rfc7540.html#rfc.section.10.3\n\nSimilarly, HTTP/2 allows header field values that are not valid. While most of the values that can be encoded will not alter header field parsing, carriage return (CR, ASCII 0xd), line feed (LF, ASCII 0xa), and the zero character (NUL, ASCII 0x0) might be exploited by an attacker if they are translated verbatim. Any request or response that contains a character not permitted in a header field value MUST be treated as malformed (Section 8.1.2.6). Valid characters are defined by the field-content ABNF rule in Section 3.2 of [RFC7230].\n\ngrpc & grpc-go should not be putting user's raw error messages without escaping into header fields.\nThe grpc wire protocol says:\nhttp://www.grpc.io/docs/guides/wire.html\n\nNote that HTTP2 does not allow arbitrary octet sequences for header values so binary header values must be encoded using Base64 as per https://tools.ietf.org/html/rfc4648#section-4. Implementations MUST accept padded and un-padded values and should emit un-padded values. Applications define binary headers by having their names end with \u201c-bin\u201d. Runtime libraries use this suffix to detect binary headers and properly apply base64 encoding & decoding as headers are sent and received.\n\nI think grpc-go might not handle the *-bin headers in enough cases? Maybe it's only implemented for metadata headers?\n$ git grep -- -bin\nmetadata/metadata.go:   binHdrSuffix = \"-bin\"\nmetadata/metadata_test.go:              {\"key-bin\", \"abc\", \"key-bin\", \"YWJj\"},\nmetadata/metadata_test.go:              {\"key-bin\", binaryValue, \"key-bin\", \"woA=\"},\nmetadata/metadata_test.go:              {\"key-bin\", \"Zm9vAGJhcg==\", \"key-bin\", \"foo\\x00bar\", nil},\nmetadata/metadata_test.go:              {\"key-bin\", \"woA=\", \"key-bin\", binaryValue, nil},\nmetadata/metadata_test.go:              {[]string{\"k1\", \"v1\", \"k2-bin\", binaryValue}, New(map[string]string{\nmetadata/metadata_test.go:                      \"k2-bin\": binaryValue,\nI'm happy to implement this if you'd like, @iamqizhao, or you can if you want to. Let me know.\n. @peter-edge, I believe there's already an open bug for transparent base64 escaping. If you can't find it, please open one. (I'm on a poor connection)\n. And I'm currently on Australia time. If we're distributed across the globe we'll have good code review latency with somebody always awake. :-)\n. Comments in https://go-review.googlesource.com/#/c/20077/1\n. /cc @iamqizhao for review & merge.\n. No, I've been busy with Go 1.7 stuff for the May 1st freeze. Hopefully I can spend a bit of time on this soon, but then I'm out for a month starting in a couple weeks.\n. > is there some way in which we could help? :)\nSure. Look at the http2 code and find ways to improve it, while not hurting readability or maintainability.\n. Honestly I forget. That code was mostly written for correctness and not performance. There's probably a lot of low-hanging fruit in there but there might not even be enough benchmarks yet to start using pprof & friends. You'd probably have to write at least a few new ones.\n. @tamird, the grpc-go team is undergoing a changing of the guard at the moment so this isn't much of a priority.\nAnd I don't officially work on grpc-go. It's not surprising that ServeHTTP is slower considering it's shoehorned into the existing ServerTransport interface somewhat awkwardly.\nI started working on a new design for all of grpc-go in https://github.com/bradfitz/grpc-go but it's not ready yet for wider discussion or testing. It was mostly a proof of concept. But it's something I intend to get back to. @philips et al at CoreOS are also interested.\n. @iamqizhao, I am against this change for a number of reasons:\n1) it does too many unrelated things in one pull request\n2) the first commit (\"isItem is ...\") has a poor commit message subject and actually does two things. One is okay (but unrelated to performance), and the other I disagree with its direction (value vs pointer receivers) even though I agree with making things consistent.\n3) the handler_server.go change is not safe. It touches code around a TODO without addressing the TODO.\nI think 1) alone is enough to reject this request. The pieces can be submitted separately for independent review, but it's too much distraction trying to review them together.\n. My comments were deleted when this pull request was updated. Because Github totally sucks for code review.\nI think we should switch gRPC 100% to Gerrit at some point.\n. LGTM. @iamqizhao?\nI still want to delete all this code, but this is fine and better for now.\n. I object to this CL for the same reasons listed in https://github.com/grpc/grpc-go/pull/587#issuecomment-202619710\nPlease break this up so they can be reviewed independently.\n. Okay, @iamqizhao can review this one. I have no opinion on it.\n. @iamqizhao, shouldn't WithPerRPCCredential also lowercase the map keys it sends?\n. LGTM. Left some comments.\n. @iamqizhao, I see it. I filed a bug. Sorry about that.\n. > But I do think HTTP/2 framer should give more detailed info about the error instead of simply returning PROTOCOL Error.\nIt can: https://godoc.org/golang.org/x/net/http2#Framer.ErrorDetail\nErrorDetail was added to not break compatibility and change the values being returned.\n. We could detect it in writing, but we could also just fix it. See #609, which isn't doing the -bin suffix thing yet.\n. What do the other languages do if the message contains bogus bytes?\n. @peter-edge, thanks. I look forward to it.\n. @iamqizhao, it doesn't list all ASCII because it's not a table of ASCII. It's a table for whether it's a valid token, as its name suggests. See:\ngo\n// validHeaderFieldName reports whether v is a valid header field name (key).\n//  RFC 7230 says:\n//   header-field   = field-name \":\" OWS field-value OWS\n//   field-name     = token\n//   token          = 1*tchar\n//   tchar = \"!\" / \"#\" / \"$\" / \"%\" / \"&\" / \"'\" / \"*\" / \"+\" / \"-\" / \".\" /\n//           \"^\" / \"_\" / \"`\" / \"|\" / \"~\" / DIGIT / ALPHA\n// Further, http2 says:\n//   \"Just as in HTTP/1.x, header field names are strings of ASCII\n//   characters that are compared in a case-insensitive\n//   fashion. However, header field names MUST be converted to\n//   lowercase prior to their encoding in HTTP/2. \"\nfunc validHeaderFieldName(v string) bool {\n    if len(v) == 0 {\n        return false\n    }\n    for _, r := range v {\n        if int(r) >= len(isTokenTable) || ('A' <= r && r <= 'Z') {\n            return false\n        }\n        if !isTokenTable[byte(r)] {\n            return false\n        }\n    }\n    return true\n}\n. > All RPCs fail when key in the returned may contains slash or colon. This used to work, so is it not supposed to?\nA header field name cannot contain a colon.\n. > And it implies a leaky abstraction, where key-values break when underlying transport changes, doesn't it?\nThat interface could definitely use some more docs saying what the keys are allowed to be. And the callers of that interface should verify the docs were obeyed and fail earlier, before it gets to the wire.\n. > I see. BTW, this is a google example on checking illegal chars on reading path does not give good error info. Can you add it? We can definitely add the checking in gRPC but it seems it should go into http2 frame code. \nIt should be already. ErrorDetail would say that.\n. It's a little gross because hpack allows a larger set of things than http2 (which just uses hpack) allows, and a bunch of the code in gRPC and elsewhere is using hpack directly to encode things. I suppose I can make the hpack encoder assume http2 rules by default, unless you opt-in to the full hpack encoding possibilities.\n. > @bradfitz in that case, it seems it makes sense grpc http2 transport enforces the rule by itself instead of relying on http2 framer.\ngRPC doesn't look at the error return value from hpack.Encoder.WriteField anyway, so me returning an error for invalid http2 field Name values wouldn't help anyway...\ntransport/http2_client.go:      // HPACK encodes various headers. Note that once WriteField(...) is\ntransport/http2_client.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \":method\", Value: \"POST\"})\ntransport/http2_client.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \":scheme\", Value: t.scheme})\ntransport/http2_client.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \":path\", Value: callHdr.Method})\ntransport/http2_client.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \":authority\", Value: callHdr.Host})\ntransport/http2_client.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \"content-type\", Value: \"application/grpc\"})\ntransport/http2_client.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \"user-agent\", Value: t.userAgent})\ntransport/http2_client.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \"te\", Value: \"trailers\"})\ntransport/http2_client.go:              t.hEnc.WriteField(hpack.HeaderField{Name: \"grpc-encoding\", Value: callHdr.SendCompress})\ntransport/http2_client.go:              t.hEnc.WriteField(hpack.HeaderField{Name: \"grpc-timeout\", Value: timeoutEncode(timeout)})\ntransport/http2_client.go:              t.hEnc.WriteField(hpack.HeaderField{Name: k, Value: v})\ntransport/http2_client.go:                              t.hEnc.WriteField(hpack.HeaderField{Name: k, Value: entry})\ntransport/http2_server.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \":status\", Value: \"200\"})\ntransport/http2_server.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \"content-type\", Value: \"application/grpc\"})\ntransport/http2_server.go:              t.hEnc.WriteField(hpack.HeaderField{Name: \"grpc-encoding\", Value: s.sendCompress})\ntransport/http2_server.go:                      t.hEnc.WriteField(hpack.HeaderField{Name: k, Value: entry})\ntransport/http2_server.go:              t.hEnc.WriteField(hpack.HeaderField{Name: \":status\", Value: \"200\"})\ntransport/http2_server.go:              t.hEnc.WriteField(hpack.HeaderField{Name: \"content-type\", Value: \"application/grpc\"})\ntransport/http2_server.go:      t.hEnc.WriteField(\ntransport/http2_server.go:      t.hEnc.WriteField(hpack.HeaderField{Name: \"grpc-message\", Value: statusDesc})\ntransport/http2_server.go:                      t.hEnc.WriteField(hpack.HeaderField{Name: k, Value: entry})\ntransport/http2_server.go:              t.hEnc.WriteField(hpack.HeaderField{Name: \":status\", Value: \"200\"})\ntransport/http2_server.go:              t.hEnc.WriteField(hpack.HeaderField{Name: \"content-type\", Value: \"application/grpc\"})\ntransport/http2_server.go:                      t.hEnc.WriteField(hpack.HeaderField{Name: \"grpc-encoding\", Value: s.sendCompress})\n. @ThomasBets, a *bytes.Buffer is documented as never returning an error on Writes. (It only returns error values to be compatible with the io.Writer etc interfaces). And *hpack.Encoder (so far) just returns errors from its underlying Writer, so it was a safe move to ignore those errors so far.\n. I haven't been able to reproduce it easily. Any tips?\n. I tried to reproduce it again yesterday but again couldn't.\nI'll need to spend more time on this but I've been short on time.\nFeel free to skip that test for that environment for now.\n. Agree. If I introduced a bug the answer is to fix it, not ignore it.\nI'll investigate.\n. Okay, I think I see what's happening. The body Read and Close calls are happening from different goroutines which isn't really expected. \nI'll fix either side or both, and docs and tests.\nBut not at a computer at the moment. Will look in a few hours after dinner.\n. Why does your proposed patch do more than just comment out the one bit? Why all the deleted lines?\n. Sent out the fix: https://golang.org/cl/31636\n. I don't know what version of Go you're talking about here. If you find a problem with the net package in Go 1.8 compared to Go 1.7, please file a bug at https://github.com/golang/go/issues/new . Or if you find something underdocumented, likewise file a bug.\nI don't know what's changed in gRPC lately to know what's biting you. I don't regularly work on gRPC.\n. Yeah, @F21, I confirm that TestServerCredsDispatch loops forever with that failure log message when using Go 1.8rc1.\n. Wait, I also see this on Go 1.7.. @MakMukhi, are you the owner of this?\n. This is still happening (I'm at 50955793b0183f9de69bd78e2ec251cf20aab121):\n$ go test -v -run=TestServerCredsDispatch google.golang.org/grpc/test\n=== RUN   TestServerCredsDispatch\n2017/01/26 17:28:34 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::]:60765: connect: network is unreachable\"; Reconnecting to {[::]:60765 <nil>}\n2017/01/26 17:28:35 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::]:60765: connect: network is unreachable\"; Reconnecting to {[::]:60765 <nil>}\n2017/01/26 17:28:37 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::]:60765: connect: network is unreachable\"; Reconnecting to {[::]:60765 <nil>}\n2017/01/26 17:28:40 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::]:60765: connect: network is unreachable\"; Reconnecting to {[::]:60765 <nil>}\n2017/01/26 17:28:44 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::]:60765: connect: network is unreachable\"; Reconnecting to {[::]:60765 <nil>}\n2017/01/26 17:28:51 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::]:60765: connect: network is unreachable\"; Reconnecting to {[::]:60765 <nil>}\n2017/01/26 17:29:01 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::]:60765: connect: network is unreachable\"; Reconnecting to {[::]:60765 <nil>}\n....\n. If anybody knew what the issue was, it'd probably be fixed. :). I'll trust you if you think they're different. I haven't looked. I forked my bug report off into #1058.. @bdarnell, sorry for the threadjack, but does Cockroach use streaming RPCs? I ask because I also ran into some gRPC problems but while debugging I just decided to delete gRPC-go's http2 stack and retrofit gRPC-go atop Go's native http2 support. The retrofit worked for me, but I didn't need streaming RPCs so I didn't implement them yet. Similarly, h2c support is easy but not yet done, because I didn't need it. (I just needed to do simple RPCs out to Google services over https). Okay. If you're interested, you can follow along at https://github.com/bradfitz/grpc-go/issues/3 and https://github.com/bradfitz/grpc-go/issues/4. @menghanl, please reopen this. You're relying on undefined behavior.\nThe Go bug is about defining it, but that would only help Go 1.9+. You need to fix it for earlier Go versions.\nThe fix is easy: don't listen on \":nnn\" and use ln.Addr().String().. Yes, sorry: you can listen on \":nnn\" but if you do so, you're not allowed to use ln.Addr().String() as the dial address, because it's currently undefined.\nSo you need to clean it up, mapping \"0.0.0.0\" to \"127.0.0.1\".\nInstead of listening on \":nnn\", I recommend using:\ngo\nfunc newLocalListener(t *testing.T) net.Listener {\n        ln, err := net.Listen(\"tcp\", \"127.0.0.1:0\")\n        if err != nil {\n                ln, err = net.Listen(\"tcp6\", \"[::1]:0\")\n        }\n        if err != nil {\n                t.Fatal(err)\n        }\n        return ln\n}\n. In Go's experience, not all machines have \"localhost\" both defined and defined to be the correct loopback address.\nThat's why all the Go unit tests use the newLocalListener func I posted above.. LGTM\nBut....\nIn Go's experience, not all machines have \"localhost\" both defined and defined to be the correct loopback address.\nThat's why all the Go unit tests use the newLocalListener func I posted in https://github.com/grpc/grpc-go/issues/1058#issuecomment-301143668.\nBut you can start with this.\n. /cc @dfawley . I would prefer it as-is without the FUD. The words \"may\" and \"significantly\" in your proposed addition are at opposite sides of the certainly spectrum.\nIf you want to cite actual features they'll lose, that's fine. But which?\n. What is \"keepalive\"? Go's HTTP/2 server keeps connections alive too, by default.\nWe also have idle timeouts (https://tip.golang.org/pkg/net/http/#Server ... https://tip.golang.org/pkg/net/http/#Server.IdleTimeout)\nWe don't do BDP estimation, that's true.\nIf you want a caveat, I'd prefer a caveat emptor wording that says it's a different HTTP/2 server implementation and \"performance and available features may vary\", without saying ServeHTTP \"may\" only suck more. I'd argue in a number of ways that Go's http2 server is more solid.. I've pushed an addition with a possible caveat paragraph:\nhttps://github.com/grpc/grpc-go/pull/1406/commits/25d5150ac4ae27d0a0ed927a3fcd52dfb1c06e91. Sure. Will do.\n. I don't think gRPC the protocol supports such a thing, so client libraries don't have a way to do it.\nWhere this is more useful is you're now able to use an net/http.Handler-based router that routes based on the content-type of requests, routing gRPC in one direction (even sub-muxing on service or path), and other stuff to your main website.\n. Some net.Conn impls block unreasonably long on Close. Until Go 1.6, even Go's *tls.Conn could hang forever in Close if there was a blocking Write in-flight (which wouldn't be woken up by the Close, since the Close was previously blocked forever on a mutex)\nSpinning up a goroutine is nearly free.\n. > In CockroachDB we're incrementally migrating from net/rpc to grpc\nNice!\n. Surprising who? This code is only used by tests, and the tests don't synchronize it with anything else.\nWho else is using this? grpc-go really needs a graceful shutdown mechanism, and this isn't it (or at least not yet). I don't want to fix that in this bug, but Stop by itself is pretty useless today. It's mostly a cleanup mechanism for tests, since normal servers don't call Stop. They'd really want graceful shutdown if they know they're going to shut down (else they can just crash/exit).\n. Is your Go net/http Server using TLS? http2 is only enabled in the standard library with TLS.\n. @tamird, I modified the greeter_server/main.go like:\n``` go\n                http.Handle(\"/\", myMux{s})\n...\ntype myMux struct {\n        gs *grpc.Server\n}\nfunc (m myMux) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n        log.Printf(\"Got request: %#v\", r)\n        m.gs.ServeHTTP(w, r)\n}\n```\nAnd hit it with greeter_client and got this output logged:\n2016/02/02 05:19:06 Running hello server on localhost:50051 ...\n2016/02/02 05:19:10 Got request: &http.Request{Method:\"POST\", URL:(*url.URL)(0xc820278300), Proto:\"HTTP/2.0\", ProtoMajor:2, ProtoMinor:0, Header:http.Header{\"Content-Type\":[]string{\"application/grpc\"}, \"User-Agent\":[]string{\"grpc-go/0.11\"}, \"Te\":[]string{\"trailers\"}}, Body:(*http.http2requestBody)(0xc820270810), ContentLength:-1, TransferEncoding:[]string(nil), Close:false, Host:\"localhost\", Form:url.Values(nil), PostForm:url.Values(nil), MultipartForm:(*multipart.Form)(nil), Trailer:http.Header(nil), RemoteAddr:\"127.0.0.1:43332\", RequestURI:\"/helloworld.Greeter/SayHello\", TLS:(*tls.ConnectionState)(0xc820244d10), Cancel:(<-chan struct {})(nil)}\nSeems to be working for me. That was with Go 1.6. I also tried with Go 1.5, which required that I changed the example code to:\ngo\n    var httpServer http.Server\n    httpServer.Handler = myMux{s}\n    httpServer.Addr = *listen\n    http2.ConfigureServer(&httpServer, nil)\n    log.Fatal(httpServer.ListenAndServeTLS(file(\"server1.pem\"), file(\"server1.key\")))\n... and then it logged the same output using Go 1.5.\nWhat is your code?\n. Because I only implemented ServeHTTP for TLS. That's all Go's http2 currently does.\n. Oh, you mean I should use the root CA option? I remember seeing that somewhere.\nI could do that.\n. Oh, I missed that you even have \"unix\"+\"tls\" in the line noise that was line 329 before. :) I'll restore.\n. t is used by the testing.TB already. t would shadow it. t0 is intentional and idiomatic: it's \"time zero\" (the start time).\n. Windows users should still be able to filter. If they run a unix dialer environment and it fails, that's fine. Only developers will use this and they'll know.\n. This should still work for Windows users. I think the name \"unix-\" is clear enough that unix- ones won't work on Windows.\n. I renamed it to deadline and got rid of t0.\n. Oh, yeah, I changed my mind about something earlier. Removed.\nI have grander plans for all this logging stuff, but this is a baby step.\n. Thanks! Please file a bug. Clearly we need both a new test, as well as documentation on the ServerTransport interface about the expectations of implementations.\nThe fix seems easy enough, though.\n. Sure, I'll take a look. By TestPingPong_4 do you mean TestPingPong with -cpu=4? And are you able to reproduce easily on your own machine?\nI can't reproduce so far, with either:\n$ go test -v   -run=PingPong -cpu=4 ./test -only_env=handler-tls  -count=200 -race\nor\n$ go test -v   -run=PingPong -cpu=4 ./test -only_env=handler-tls  -count=200\n. The thing you linked to is about TestRPCTimeout-4 failing, not PingPong.\nOr were you trying to debug and found TestRPCTimeout similar to TestPingPong somehow?\n. Indeed.\n. But you would have to adjust the call sites too, which you don't. So this change is does not even seem correct.\n. Oh, I see. Both are already only used as pointers. Weird.\nI'll defer to @iamqizhao. I agree that it's weird having the receiver type here be values, but it's also weird that the code is passing around pointers for these. I think for code clarity reasons I'd prefer to just change them all to be values. As a small bonus, putting an empty struct value like flushIO into an interface value is free, and doesn't require allocations like it is now:\ntransport/http2_client.go:                              t.controlBuf.put(&flushIO{})\ntransport/http2_client.go:                              case *flushIO:\ntransport/http2_server.go:                              t.controlBuf.put(&flushIO{})\ntransport/http2_server.go:                              case *flushIO:\nChanging it to pointers makes it look like it has mutable state.\n. > Unfortunately you lose too much type safety when you implement interfaces with values in Go.\nI don't understand this comment. Can you give an example?\n. I might add a comment to this line saying // capital \"Key\" is illegal in HTTP/2\n. Add some prose before this, explaining at a high level what this test is testing.\n. base64 should things should have keys ending in -bin per http://www.grpc.io/docs/guides/wire.html\n. what is with all the conversions? neither are needed.\nSee https://blog.golang.org/constants\n. write an actual self-contained sentence first.\n. just buf\n. drop the underscores on the LHS\n. don't do this conversion. unnecessary allocation. iterate over the bytes instead.\nbut do a pass over it first to see if it's valid before you do any allocations.\n. just msg for the variable name. Or v.\n. this one's unnecessary, no?\n. I knew exactly why you were doing it, but it's not popular Go style. Also: bytes.Buffer does not return errors and is even documented as such, so it's just noise.\n. No, the loop on line 319 only runs buf := make(...) once, and each iteration exhausts some of the front of buf until it's gone. This refills it.\n. No, that's unsafe. It would let the receiver mutate the next receiver's memory.\nEven if it currently happens to be safe, it's impossible to reason about, and it's a hard-to-debug disaster waiting to happen.\n. ",
    "thelinuxfoundation": "Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @iamqizhao isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @ZhouyihaiDing isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @shevchenkodenis isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @irfansharif isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @irfansharif isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @sunshangpp isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @peter-edge isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @menghanl isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @dfawley isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @dfawley isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @lyuxuan isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @thinkerou isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @k3rn3l-p4n1c isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @fho isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @MakMukhi isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @lukatendai isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @lukatendai isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits was unable to be associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @minaevmike isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits was unable to be associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @tsuna isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @SeerUK isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @reterVision isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @muirrn isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @dnephin isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @groob isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @cesarghali isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @cesarghali isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @cmceniry isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. Thank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @zolotov isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nThe Linux Foundation CLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @slomek isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @coocood isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @blairkutz isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @gm42 isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @peter-edge isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @etdub isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @zllak isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @codetriage-readme-bot isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @jhump isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @btc isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @HusterWan isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @johanbrandhorst isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @ilius isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @Chyroc isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @igorbernstein2 isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @jsha isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @jusunglee isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\nUser @mwitkow isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\nUser @ains isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @gnoack isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @weiyougit isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @krzysztofdrys isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @dsymonds isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @ashi009 isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @dsymonds isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @Arbusz isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @JelteF isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @virtuald isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @davidklassen isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @zchee isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @steve-gray isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @canguler isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @sword2ya isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @kazegusuri isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @dreambo8563 isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @thaJeztah isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @tomwei7 isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @tomwei7 isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @prannayk isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @90wukai isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @ianlancetaylor isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @dntj isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @zelahi isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @mklencke isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @taralx isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nOne or more commits are not associated with a GitHub user. The email used in the Git commit must match a verified email address in a GitHub user profile. For more information, please see https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/\n\nRegards,\nCLA GitHub bot\n. \nThank you for your pull request. Before we can look at your contribution, we need to ensure all contributors are covered by a Contributor License Agreement.\nAfter the following items are addressed, please respond with a new comment here, and the automated system will re-verify.\n\nUser @AngusReid isn't covered by a CLA. They will need to complete the form at https://identity.linuxfoundation.org/projects/cncf\n\nRegards,\nCLA GitHub bot\n. ",
    "stapelberg": "Ah, after having a closer look it seems like you can just not specify that field, since it\u2019s variadic :).\nIn any case, an example would make that clearer.\n. I\u2019d be interested in this as well.\n. +1 to what @philips said. I\u2019m exactly in the same boat with http://robustirc.net/, where I\u2019d be happy to switch to gRPC, but not if it involves a separate port.\n. More than a year has passed. What\u2019s the current status? Flying blind is killing one of my biggest motivations to use gRPC in the first place.. Thanks for looking into this!\n\nIn theory we could truncate the binary-serialized message when storing it, but I don't believe the Go protobuf library handles truncated messages, so we wouldn't be able to render it in text form later if we did that.\n\nThis is not what I observe: https://play.golang.org/p/u9xZoYLx6J results in:\n2017/09/08 18:06:23 unexpected EOF\n2017/09/08 18:06:23 res: {Path:[foo bar baz]}\ni.e., proto.Unmarshal returns an error, but as a side-effect it also fills in the message as good as it can.\nI think it\u2019s safe to unmarshal a truncated message and display the result textually.. ",
    "matttproud": "@dsymonds, thank you for your patience.  I was able to come up with a few proposals on the transport closing behaviors w.r.t. locking and such.  I came up with using sync.Once in the HTTP2 transport client close mechanism: if you think an error or panic is better suited or silently doing nothing, I am all ears.  I would just like to know why to better understand your reasoning.  :)  Either way, I think the general direction we are taking improves the comprehensibility w.r.t. intent.\n. @iamqizhao, since we both agree on dulling the sharp edge of the the double-shutdown case, I have rescoped this pull request to reflect that.  I will approach the clientStream and serverStream work in a separate exploration.\n. @iamqizhao, cool!  As of the latest snapshot, we now have the following:\n1. A single unit test dedicated to single and duplicate closure behaviors.\n2. Generalized validation for client and server close-correctness in all tests.  This makes sense to have for completeness.  I opted for not generalizing the check function, because I want line numbers to be as proximate as possible for failures.\n3. Annotations added to the mutexes in question from previous review comments to clarify their sensitivity.\n. @iamqizhao, this should work for you now, rebased off of master.\n. You are welcome!\nOn Wed, Feb 25, 2015 at 9:47 PM Qi Zhao notifications@github.com wrote:\n\nLGTM. Thanks for your contribution!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/52#issuecomment-76052998.\n. I have gone ahead and renamed it.  As for moving it, I am curious for your suggestions:\n\nWe are benchmarking an unexported, internal function encode at the moment, which belongs to the top-level package.  There will be more that we benchmark, but this is just the beginning.\nThe only way we can benchmark it while moving it to another package appears to be \u2026\n1. Exporting the function (highly undesirable due to public API cluttering).\n2. Moving it to an internal package, provided that meets the Golang versioning compatibility requirements for grpc and is actually implemented in upsteam Go (a quick search at stdlib in 1.4.1 answers: no).\nWhat are your thoughts?\n. Yeah, there is value in instrumenting these smaller units to get a better approximation of the whole.  I started withencode, because of your TODO in the function itself.  More broadly, I have a few ideas about end-to-end benchmarking.  I just wanted to start with the simpler cases and expand from there.\n. Returned to rpc_util_test.go.\n. @Sajmani, speaking from the perspective of internal, first-party use, I would want the fidelity of a lossless API (I am sure plenty of others would agree if you put the question to them).  Most of the heretofore discussions around service level indicators (SLI) have not had the luxury to speak about things in terms of reporting error and just assume perfect measurement, for better or worse.\n. Would the conn.Close() in defer on 122-24 fail to be called because of variable shadowing of err in the original version?\n. Excellent point, and that completely moots this!  I had not bothered to closely look at the return signature's named returns because of\u2014well\u2014a general sentiment in the community to use named returns very sparingly and had assumed that they were not being used here (esp. in new code)!  Mea culpa; this changes everything!  :)  We are in complete agreement, and much of this pull request is irrelevant.\n. It shouldn't be.  Can you explain how?\nIf multiple calls to t.Close() were to occur concurrently and race, closeOnce.Do would run only one of them, while all of the others would block until the runner's completion.  This is to say the shared state would mutated in only one, and the synchronization would force the other Goroutines to wait until the mutation propagated and became visible.  Unfortunately the documentation for sync.Once does not convey the behavior very well, but an examination of its source confirms: http://golang.org/src/sync/once.go?s=1139:1166#L25.\nYou can try a variant of it with the race detector locally: http://play.golang.org/p/YRnQOdJESC.\n. Ahh!  Good to know.  Does this new version look better?\nPerhaps we could massage the behavior into transport.Stream a little more elegantly through the type system and use those throughout the respective clients and servers:\n```\ntype (\n    clientStream Stream\n    serverStream Stream\n)\nfunc (s clientStream) Close() {}\nfunc (s serverStream) Close() {}\n```\nLet me know what you think!\n. Sure, entirely.  I was just proposing that we could make the API a bit less brittle and more consistent in case of multiple closures: consistent return values and blocking until close.\nLet me take a few days offline to polish the rest of my thoughts offline, and I'll re-upload a draft into there.  I'll go ahead and archive this until then.  What do you think?\n. Err has the possibility of being non-nil in the first case you show from an anomaly in the connection shutdown routine, so we wouldn't want to just return nil on first close.  I am not attached to using sync.Once for this; the original just looked a bit brittle around the edges for customers.\nI'll stick with my plan on archiving this, splitting out the distinct hunks, etc.  Sound OK?  Namely the extraction of the client and server stream termination routines?\n. Curious: how is the deadlock possible?  That would be worth documenting.\n. How about calling it ErrClosing, and have it convey that it is closing or\nhas been closed?\n. Sounds perfectly reasonable.  It would be a corner case error that nobody\nshould have to explicitly handle: it is from their own misuse.\n. Even in context of the back-and-forth we went through in the review process?  It seems like a nuance worth conveying (at least somewhere).  Do you have a suggestion on how to reword it?\n. Done.\n. Done.\n. See my question above.  I would like the nuance to be explicitly mentioned\u2014somewhere.\n. While it is a common good-thing-to-do, to be sure, I had wanted to balance a way to call out special attention to the possibility (as you proposed).  Since there seems to be no amenable way of expressing this, I will leave the burden of self-discovery to future contributors.\n. OK.  server.Close has been reverted.  I have kept the client tests, since ClientTransport.Close() conveys the possibility of error\n. If we inline this, the errors (should they occur) will lose their line\nnumber context sensitivity and report the line from the function we have\ndelegated the error checking to, not the place where the error happened\n(meaning the test case's line no.).  Are you sure?  Or is there a way to\nrefactor the test without getting this defect?\nRun this example locally to see what I mean:\nhttp://play.golang.org/p/kXWjbRJbpQ.\n. Will do.  Thank you for clarification.  If we had  > 1 client transport per\ntest, the test feedback would get ambiguous.\n. I have used a variant of this, passing off the first error it encounters instead of failing.\n. Renamed to a variant thereof.\n. Your message is empty.\n. Same empty message.\n. Same empty message.\n. Same empty message.\n. I have reworded it: short circuiting the test case.  I have replaced with\nclearer verbiage to convey test case termination.\n. Works for me.  Done.\n. Done.  This is used to scaffold strictly the the Buffer message type for now.  I had included 'simple' originally due to just using an array of zero-ed bytes as opposed to another predefined byte payload or even randomly-generated.\n. Converted.\n. Sure!  In the original, you are using binary.Write(binary.Write impl.).  Each time a message is encoded, the binary.Write function body needs to \u2026\n1. allocate two byte arrays and one slice (three allocations in on heap in total)\n2. perform type switching\nAdmittedly the compiler and runtime could improve the memory behavior (keep on stack) and the type switch through some sort of a priori branch inferencing in future versions, but the encoding/binary package explicitly states:\n\nThis package favors simplicity over efficiency.\n\n\u2026 whereas we know a priori the storage size of the length (four bytes), which means \u2026\n1. we can specify the VarDecl with that size once and hold the number of allocations to one\n2. we specify the exact encoding mechanism binary.BigEndian.PutUint32 as opposed to being forced by the runtime to type switch\n\u2026\u00a0without those overheads.  The net effect of this is faster encoding, reduced memory usage (size), and reduced heap allocation (object count), all of which mean increased throughput (bytes/second).  With this being library code, the memory stewardship component is salient.\nIn the commit message, I excerpt the result of this (before versus after) using the benchcmp tool:\n```\n$ # Return to previous commit but benchmark.\n$ go test ./... -test.bench=\"Benchmark\" > /tmp/before\n$ # Return to working copy.\n$ go test ./... -test.bench=\"Benchmark\" > /tmp/after\n$ benchcmp /tmp/before /tmp/after\nbenchmark               old ns/op     new ns/op     delta\nBenchmarkEncode1        1451          942           -35.08%\nBenchmarkEncode10       1466          1067          -27.22%\nBenchmarkEncode100      1966          1648          -16.17%\nBenchmarkEncode1000     4390          3910          -10.93%\nbenchmark               old MB/s     new MB/s     speedup\nBenchmarkEncode1        5.51         8.49         1.54x\nBenchmarkEncode10       11.60        15.93        1.37x\nBenchmarkEncode100      54.40        64.92        1.19x\nBenchmarkEncode1000     229.59       257.77       1.12x\nbenchmark               old allocs     new allocs     delta\nBenchmarkEncode1        6              3              -50.00%\nBenchmarkEncode10       7              4              -42.86%\nBenchmarkEncode100      8              5              -37.50%\nBenchmarkEncode1000     8              5              -37.50%\nbenchmark               old bytes     new bytes     delta\nBenchmarkEncode1        384           328           -14.58%\nBenchmarkEncode10       400           344           -14.00%\nBenchmarkEncode100      736           680           -7.61%\nBenchmarkEncode1000     2560          2504          -2.19%\n``\n. Curious: Have you benchmarked for throughput the overhead of the additional defer call on this procedure and elsewhere?\n. Nit: As a convention, I would drop the receiver name since it is unused here and elsewhere.\n. Nit: I would group the imports by \u2026\n1. standard library\n2. first-party\n3. third-party\n. Since these will be allocated per-request, have you checked to ensure that struct alignment is optimal to trim extra bits for space savings?   \n. I realize that you are creating this as a rough cut for a proposal, but perhaps givegolint` a quick run to bring everything to an equal level of polish.\n. Thought: This models itself after a trace.  Perhaps the type should be renamed?\n. ",
    "wonderfly": "I've addressed the majority of the comments except 'unwrap', PTAL.\n. Addressing comments. PTAL.\nA route guide documentation will be added in a follow up CL: so that I can copy/paste the checked in code snippet into the doc instead of changing code in two places.\nA decision has been made that this CL will be checked into the go repo, while having a link in grpc-common that points to this code.\n. Addressing comments. Looks like the generated code got checked in after all as it's required for Travis tests.\n. GitHub is not picking up my latest commits because my fork stopped working as the upstream went public and I recreated it. Should I start a new PR instead?\n. Addressing @iamqizhao 's comment. PTAL.\n. Done\n. Done\n. Done\n. Done\n. Done\n. Done\n. Agreed. Done.\n. Done\n. Done\n. Done\n. Done\n. Done\n. Done\n. Done\n. Done\n. I agree it's dicey to some extent, but I'd rather follow what other languages have done just for consistency. I've modified the proto definition to make this behavior more clear. This is an example API so may not be worth rediscussing the API design and asking others to make the change.\n. Done\n. Done\n. Done\n. Done. Also removed other occurrences in client.go.\n. Changed to CoordFactor instead.\n. Done\n. Done\n. Done\n. Done\n. Sorry, what do you mean exactly?\n. Done\n. Done\n. Does it still make sense now that 'serverAddr' includes both host and port? The route guide in other languages don't have a serverAddr flag at all, what consistency are we trying to get here?\n. Done\n. Done\n. What's strange about these vars? It's the standard \"haversine\" formula, as is referenced in the leading comment.\n. Ah, you were referring to 'tls_server_name', instead of 'server_addr'. Yes that makes sense. I changed it too.\n. Agreed. I forgot to update the flag name. Done.\n. Done\n. I was trying not to call log.Println(feature) when there's an error so I returned early. Changed it to if{} else{} to make the flow control more clear.\n. Done\n. Done\n. Done\n. This seems to be an incorrect diff. The real diff as shown in the commits is from 'Fatal' to 'Fatalf'.\n. ",
    "jayantkolhe": "@LisaFC \nfyi\n. @LisaFC @iamqizhao \n. ",
    "gainward": "You mean generating code from protos, right? I was running through a quick example and had things blow up when I tried:\nbash\n$ protoc --go_out=plugins=grpc:. todo.proto \nprotoc-gen-go: program not found or is not executable\n--go_out: protoc-gen-go: Plugin failed with status code 1.\n. Okay, that's fair. I hadn't noticed the language subdirs in grpc-common and was expecting detailed generation instructions in here.\nThis is just more of a user flow issue, then, and I'm happy to close it.\n. Bleh, I have no idea why that is. The tests all pass on my local machine, and seem to pass on travis even though I haven't changed anything substantial. Perhaps it's flaky?\n. Okay, fair point. I've updated the code to pipe timeouts down into newHTTP2Client and extended the TransportAuthenticator interface to include a DialTimeout function.\nI also did some rejiggering of ConnectionError to include the error it's wrapping, since this seems to be its overwhelming use. I think it's idiomatic Go--os.PathError takes a similar approach--and it seemed cleaner than shoehorning Temporary() and Timeout() into CE.\nI've also reintroduced the !Temporary() check, since we can get our hands on the net.Error now. Tomorrow I'll take a look at properly testing that functionality.\n. Done.\n. It didn't seem necessary anymore, since piping the timeout through to net.Dial was sufficient for the case I had in mind (server not listening). Were you thinking of bounding the total amount of time Dial could be called during an attempt to reset the transport?\nIf you think it'd be easier to take over, that's okay with me. I can also squash the commits if that would make things look prettier.\n. Gah, my mistake. Ack.\n. ack\n. ack\n. ack\n. It should come out to time.Now(), given the zero value for timeout. However, we check down below that timeout > 0 before running through the timeout check.\nPerhaps it would be better written as\nvar totime time.Time\nif cc.dopts.timeout > 0 {\n  totime := time.Now().Add(cc.dopts.timeout)\n}\n...\n  if !totime.IsZero() && otherTimeoutCheck {\n      // etc\n  }\nWould you prefer that approach?\n. Cool, that reads much better. How did you feel about checking if the ensuing time.Sleep(dur) call would take us past the timeout and, in that case, stopping prematurely?\n. ack\n. whoops, missed a return statement there.\n. ack\n. ack\n. ack\n. Happy to blow this away and handle it in a separate PR. Sound good to you?\n. Hrm, I'm not sure I need to start up a server here, since the easiest way to test this timeout is to attempt connecting to an unopened socket.\nSo, how to go about getting a socket we're sure is unopened--in the flaky case, perhaps some other test leaves one open on accident? I've implemented a simplistic open/close/test-with-recently-closed-address logic, but I'm not really happy with it. If you have a better idea, I'm all ears :) but it's 2AM here and I'll have to take another look in the morning.\n. ack\n. ack\n. Shouldn't matter. If the user hasn't specified a timeout, it'll just be 0, which is the default for a new net.Dialer anyway.\n. I was basically mirroring the Dial/DialTimeout approach taken in the net package. If you feel it just adds to bloat, though, I'm happy to just update NewClientTransportto take a timeout.\n. You've already removed time.Since(start) from dopts.Timeout, so I think it's sufficient to say dopts.Timeout > 0 && dopts.Timeout < sleepTime\n. s/dialer.Dialand/dialer.Dial and/\n. remove todo\n. If we limit opts.Timeout to >=0 elsewhere, this can be better expressed as:\nconn, connErr = ccreds.DialWithDialer(&net.Dialer{Timeout: opts.Timeout}, \"tcp\", addr)\nsince ccreds.Dial would just use the same dialer by default, anyway.\n. Can we bound dopts.Timeout >=0 here? That would simplify your dialing if/else statements.\n. Gotcha, this makes more sense. Maybe also check cc.dopts.Timeout >0 instead of dopts.Timeout. That's more of a stylistic choice, but since we're checking cc.dopts.Timeout in the right half of the expression, it makes sense.\n. You can collapse this if/else into a single call to net.DialTimeout as well.\n. Ah, I meant instead of. So\nif cc.dopts.Timeout > 0 && cc.dopts.Timeout < sleepTime + time.Since(start) {\ninstead of\nif dopts.Timeout > 0 && cc.dopts.Timeout < sleepTime + time.Since(start) {\nBut you're right that we're constrained cc.dopts.Timeout >= dopts.Timeout. I just thought it made more sense to use the latter var, since we're explicitly checking that it was set.\n. ",
    "tv42": "Sorry, I submitted the form too early. Still diagnosing, this might be user error. Will report back soon.\n. Okay here's the better diagnosis: the client lib assumes addresses always have a port. It calls net.SplitHostPort and fails unless the address was host:port style. There's several calls, removing the error path from this makes the greeter example work over unix domain sockets, with a custom Dial:\nhttps://github.com/grpc/grpc-go/blob/master/call.go#L118\n. Oh, and I can't re-open this issue.\n. All I have to do to make it work is put a dummy \":kludge\" suffix on the address, and strip it out in my TransportAuthenticator.Dial. But that really shouldn't be necessary ;)\n. Oh, absolutely. I am abusing the TransportAuthenticator point to hook up a custom dialer; proper custom dialer support would definitely be better.\nOh and just to be clear, just passing \"network, addr string\" to net.Dial would cover the unix domain socket part, but still leave out all the things that are truly custom transports, e.g. reliable stream transports over UDP.\n. I just realized the grpc tls alpn is just HTTP/2 drafts, not a separate protocol. Scratch that plan; the splitting of grpc vs other needs to happen more per-{request|stream|something}.\n. @bradfitz That part doesn't help differentiate web traffic from grpc traffic, when they both happen to run over TLS & even the same ALPN. That really is the interesting part.\nActually, I don't see the grpc protocol spec saying anything about what part of it is so special that it can't just look like POST requests to net/http. Anyone have a short answer to that?\n. @acasajus Sure, it would, but right now grpc seems to sort of reimplement part of net/http / bradfitz/http2, so it's not clear how one would pass a request to the other. It's not like one could have a http.Handler inspect the headers and call one of two alternate http.Handlers.\n. As the originator of this issue, I'd like to say this: I really don't care about running gRPC over HTTP/1.1. If you want that, please make a separate issue.\nWhat I want is to have a single TCP port that can be accessed both with a modern browser and a gRPC client, where both behave sanely. That can mean always TLS and all gRPC over HTTP/2; whether the browser uses HTTP/2 or HTTP/1.1 is not relevant.\n. I guess the only thing grpc can do is avoid that name.. for a while I misread the message as being about the Error vs Errorf confusion, but it's not even that. Maybe go vet should learn to look for the first format string parameter, and start its logic from there. Of course, that doesn't belong in grpc.\n. If what you're saying is going to hold, then it sounds like grpc.Errorf should not be exported.\nHowever, consider the implications: if Codes are only supposed to be created by convertCode/toRPCErr, that means the only way to return a codes.AlreadyExists from something that is not an OS-level file system operation or such is to use platform-dependent erorrs like syscall.EEXIST (attempt to make os.IsExist(err)==true). At that point, it seems like the Code mechanism is unusable, and people would be better off with enum error types in their response messages. And in that case, why are there so many Codes?\nAnd yes, more clarity into intents of the mechanism is most welcome.\n. Don't know how well this aligns with your plans, but adding a Conn() net.Conn method to ServerTransport, with a trivial implementation in http2Server, is enough to do this, and also provides RemoteAddr etc.\n. I don't see credentials.TransportAuthenticator interacting with the server side at all. At most it's used to wrap a net.Listener (with no place to stash extra data). The plan for its future is unclear. Is there something planned that is just not implemented yet?\nI agree that exposing net.Conn feels dirty, but the alternative is a lot of case-by-case things (TLS ConnectionState! RemoteAddr! Client public key for a non-TLS protocol!). Needing to support every underlying transport explicitly sounds wrong.\nPerhaps the really modular way is something like func (s *Server) ServeConn(ctx context.Context, conn net.Conn) error, that lets the caller pass whatever is needed in the context. This would do what one iteration of the Serve for { Accept() } loop does now.\nIf pb.RegisterXxxServer was done differently, I'd be happy creating the struct that's the receiver of all the RPC methods separately for every connection, instead of sharing the same value for all connections, and storing any state I want in there -- but it seems like Context is more where the world is heading. In my own explorations of RPC in http://godoc.org/github.com/tv42/birpc , imitating net/rpc, I separated Registry from Codec to make that nicer -- though I ended up implementing an API I don't personally like anymore, filling method arguments based on type with reflect.\n. I still don't see how you want the RPC method to actually access the ConnectionState. RPC methods don't get passed the TransportAuthenticator. TransportAuthenticator is also not a per-connection value, so I don't see how adding methods to that is helpful -- except if they also take either net.Conn or Context as an argument.\nIf you're still talking about exposing net.Conn, then I personally don't even need anything more from TransportAuthenticator, I can hardcode my app to type assert to *tls.Conn etc.\nIf you're talking about passing Context to a TransportAuthenticator method to look up some sort of client credentials, then where does the TransportAuthenticator implementation get to do context.WithValue? Also, at that point, it might as well be a standalone function -- I'm not at all convinced the client credentials are all the same type, so the calling code already needs to know what it's asking for, and then can call therightpkg.AuthFromContext(ctx).\nI'm really keen to know if you have a plan for all this, e.g. based on some Google internal service or Java code, that's just slowly getting written in Go..\n. My use case is this: I'm porting a peer-to-peer communication protocol to gRPC.\nAn ealier version did it by streaming protobufs both ways over a HTTP POST, with the URL path deciding which \"method\" to call. From that perspective, gRPC sounds pretty familiar, right? To me, gRPC seemed like a good way to avoid boilerplate or writing my own custom framework.\nI identify which peer I'm talking to by the client certificate (and respectively, the peer verifies the server identity with the server cert). The RPC methods naturally need to know which exact peer is calling them.\n. And just to state this explicitly one more time, my current needs would be 100% covered with either of these:\n1.  func (s *Server) ServeConn(ctx context.Context, conn net.Conn) error, with a context derived from that being passed to the RPC method, would let me pass in whatever I need. I'd just do my own for-accept loop, and prepare a context with the information I'll need later.\n2. a Conn() net.Conn method in ServerTransport and http2Server would let me dig out what I need.\nOf these, 1. seems prettier to me.\n(I typoed option 1. earlier and left out the net.Conn, sorry about that; updated the comment above.)\n. Example use of ServeConn:\ngo\nfor {\n    c, err := l.Accept()\n    if err != nil { ... }\n    ctx := context.WithValue(context.Background(), myKey, metadata(c))\n    go srv.ServeConn(ctx, c)\n}\n. @acasajus For 2., the authentication also needs to pass information to the RPC method, e.g. who the current user is. Making the function both take and return a Context would be enough.\n. Here's a concrete proposal. Let me know what you think:\n1. Add a WithServerTransportAuthenticator ServerOption that takes a credentials.TransportAuthenticator and just remembers it in options.\n2. In Server.Serve, if a credentials.TransportAuthenticator was set with the above, wrap the listener with TransportAuthenticator.NewListener.\nThis is optional and unrelated to the other changes, but this seems like the intended purpose of NewListener.\n3. Make http2Server.operateHeaders take a context argument, and pass it in from http2Server.HandleStreams.\n4. Make http2Server.HandleStreams take a context argument; change transport.ServerTransport. Make Server.Serve pass it in.\n5. Add a NewServerConn(ctx context.Context, conn net.Conn) (context.Context, error) method to credentials.TransportAuthenticator. Existing implementations can just pass the context through unchanged.\nIn Server.Serve after a successful Accept, if a credentials.TransportAuthenticator was set, wrap the context with TransportAuthenticator.NewServerConn. On error, close the connection.\nThis lets the TransportAuthenticator set things in context based on the connection information.\nOptionally, to make access control even easier, let the TransportAuthenticator guard the method calls. Something like NewServerCall(ctx context.Context, call Xxx) (ctx context.Context, error), if that returns an error don't call the RPC method but just return error to client.\nFor example, TransportAuthenticator.NewServerCall implementations could just return grpc.Errorf(codes.Unauthenticated, \"TLS client certificate expired\"), or set a context deadline based on account policy.\nI'm not 100% sure about the right arguments, your judgement is appreciated.\nThat last part might also tie into how a Dapper-like tracing system would plugin -- perhaps there should be something to see the end of the request, too?\n. Maybe that error in 5. above should be just ok bool, as I don't see where the error message could go, at that stage. Or maybe make ctx==nil special? Your call. My prototype didn't actually include that error there, but I can see how closing the error could be desirable. What would happen if NewServerConn just said conn.Close()?\nNewServerCall can report the error to the client, so there error makes a lot more sense.\n. FWIW the numbered branches are historical, the one without the number is latest: https://github.com/bazil/grpc-go/commits/auth\n. ",
    "louiscryan": "LGTM\n. +ejona86\ndon't think this is a protocol issue at all. This is an issue with blocking writes in general, if the application can't be unblocked then we have a problem.\nI do agree that if the write can't complete, possibly for flow-control reasons, then the RST_STREAM might be necessary to unblock if partial writes have occurred but that's already an allowed feature in the spec.\nThis problem becomes exacerbated for streaming RPCs where the semantics of the call deadlines are ill-defined. ",
    "acasajus": "Since there's no grpc/grpc-common#152 Can there be a failover stream handler? If the grpc server cannot process the stream (the service/method doesn't exist for instance) we could have another handler that devs can define to process it.\n. From grpc/grpc-common#152: There's a mandatory content type for grpc. So streams that do not have it can be served by the default http handler.\n. Any news on this? Should we go ahead and make a PR?\n. @tv42 Doesn't the mandatory \"Content-Type: application/grpc\" work for differentiating between grpc and plain http requests? Granted that's once the handshake has passed.\n. The default approach to this would be to be able to define a set of handlers execute before the actual RPC request that can modify the context to add whatever info we need. It would be nice to have a set of handlers for the connection and another for each stream.\nIn the set of handlers that execute when a new connection is received we could do authentication and stuff like that. \nAnd in the set of handlers that execute when a new stream is created we could do authorization.\n1. Accept new connection\n2. Execute handlers for new connection. We can do authentication here. If the client is not valid just close the connection or something.\n3. Wait for streams\n4. For each new stream execute handlers. We can do authorization here. If the client is not allowed to execute whatever he wants to do, close the connection or something\n5. Execute whatever RPC method\nMaybe the stream handlers can be defined in the service? \n. @tv42 agreed :+1: \n. For handlers per connection there's no service yet defined so somehow we'd need to add some info to the context so the handler can check if the authentication credentials are valid.\n. What if you want to pass something to all the services in a server? We need to add that to all the services?\n. Point taken :)\n. Now that is alpha It could be fixed to keep consistency with the rest of the language implementations :)\n. I'd prefer Service because it describes a Service instead of: A Handler interface describes a Service and is implemented by the user. \nBut I'm ok which Handler since it's different from Server as you say.\n. ",
    "xiang90": "I am also interested in this one.\n/cc @xiang90 @yichengq\n. @iamqizhao Have you had the discussion yet? Any decision? Anything I can help to make this happen?\n. @iamqizhao I can work on it. \nMotivation: \n1. currently etcd and a few other my side projects have a http1 endpoint as it API.\n2. I want to support GRPC in the next few releases without adding a new port or break compatibility.\nSo ideally, grpc can forward http1 requests to my old handlers. But can you give me some direction to do that? I can start to explore and hopefully have a pr for it.\n. @iamqizhao \nIt seems @bradfitz's http2 server supports both http1 and http2 at the same time. Here is the related code\nhttps://github.com/bradfitz/http2/blob/91f80303028022bc2034c277126b405a2257d990/server.go#L185-L192\nAre we able to do the similar thing?\n. @bradfitz Agreed. I will move the discussion to grpc.\n. @iamqizhao @philips \nI tend to agree on this https://groups.google.com/forum/#!topic/grpc-io/JnjCYGPMUms. (grpc-java plan)\nCan we do something similar in grpc-go?\nThanks.\n. @iamqizhao \nI have tried the way you suggested, which implements a HTTP 1.1 based hand shaking. https://github.com/xiang90/grpchttp/blob/master/server/server.go#L46-L62\nBut this also requires EVERY client want to talk to the gRPC sever implementing the handshake mechanism. It basically means my gRPC server becomes a special one. (I might make a mistake or misunderstand your suggestion.)\nOne of the important point of using gRPC is for its generalization. Users can get easily get working gRPC clients in different language very easily. \nI looked at the code, and agreed that if we want to get better support we might need to redesign the gRPC server (http2 server?) a little bit. But I think it worth the effort since it will largely improve the adoption of gRPC for existing applications.\n. > I do not understand why you care other servers.\nI care about other clients. Every client that wants to talk to my server needs to implement the handshaking, since my server is a special one. I do not want make my server a special gRPC server.\n. > I personally do not like this approach because it adds some overhead (path\n\ndispatch) to every single rpc regardless whether you use http1 or not.\n\nPath dispatching only happens for dialing, correct? If we can make this low cost, do you want to have a try?\n. > I assume your clients use your client library and your client library\n\nshould implement the handshaker. So this is transparent to your clients.\nNo?\n\nNot necessarily. For example, in etcd we will maintain the go client (go grpc client and the upper level client on top of it). \nThe community will maintain the clients in other languages. They need to implement the handshaker for etcd special gRPC server. And I am not sure it is easy to do in all available gRPC client bindings. It would be a overhead.\n. > I think I get lost. What is your proposal to change http2 package? How can\n\nyou make it happen only for dialing?\n\nI briefly looked through the spec (I may misunderstand the spec). The path is / Service-Name / {method name}. We can assume gRPC-go owns all the registered server-name path. When dialing, we can pass a service name path and we will be able to distinguish it from other general http request. HTTP handler can then pass the conn to gRPC-go. This means the gRPC-go sever will not control the whole life-cycle of a connection. \n. > Got it. But again, it is not an grpc-go specific question and this is not\n\nthe right place to discuss and have solution. The topic should go to\ngrpc-io. In Go world, the handshaker is a mechanism to achieve that (if\nboth your client and server are written in gRPC-Go) if you need it now.\n\nI totally agree with you. That is why I do not think your suggestion is a very generic solution. \nI did ask the same question in grpc-io group. Java is using path dispatching. And they suggest me to explore the same thing in grpc-go. I think it might be doable and reasonable.(Again I am not super familiar with gRPC or gRPC-go, if you think it is reasonable I would love to explore more)\n. > Connection and service are two completely orthogonal concepts.\nI understand that. \n\nWhen clients dial, there is 0 knowledge about what\nservices will run on this connection.\n\nWhy do we care? We just want to distinguish it from non-grpc connections. We can simply pass an arbitrary registered service name path when creating the initial gRPC-go http2 connection. \n. > At very least, your understanding about\n\npath dispatching seems not correct according to your previous reply.\n\nWhich part? Can you give me some explanation? \nI think there are two ways: \n1. use the http server do path dispatching for every gRPC call\n2. use the http server do path dispatching for the first one and use this https://github.com/grpc/grpc-go/blob/master/transport/transport.go#L172 to dispatch further calls inside grpc-go as it is today.\n. > Your client needs to send this name to the\n\nserver and you run into the same issue. All of your client impl need to\nknow what is to send first after a conn is setup. Also if your server is\nexpecting this name, it is a \"special\" one according to your previous def.\n\nThere is a difference. If the connection is set up without the path, we can delay transferring the ownership of the connection until the first stream is created. And I think we can safely assume if the first stream has gRPC prefix, all following streams within the connection will have.\n. > In other words, if all grpc versions implements handshaker, do u still\n\nthink your server is \"special\"?\n\nIf all grpc versions implements the handshaker and there is a common documentation talking about the suggested approach for running http1.1/http2.0/gRPC on the same port, I think I am happy to say it is generic approach. And I believe the developers will not consider my server special and a crazy hack.\n. @iamqizhao \naudit: before each actual client rpc invocation, the control will hand over to application pre-defined interceptor first. The interceptor can record the related metrics of the invocation.\nsynchronization: before each actual server rpc handling, the control will hand over to application pre-defined interceptor first. The interceptor can decide when to actually call the handler to process the request. \nAll of them can be done by manually adding hooks into the generated stub statically. But there is no good way to do it dynamically.\nmaybe something like:\nclient.Intercept(method string, interceptF func(args ...interface{}))\n. > Why don't you call it before you make the rpc call?\nIf I have 10 places to do one rpc call, I need to write the code 10 times. To solve that, I need to wrapper round the actual client, which leads to my first point in the issue:\nAt the client side, we can actually wrapper the client interface with a thin layer. But we still cannot make this happen dynamically at runtime.\nFor example, if I have a instruments framework, ideally all the grpc calls can be examined by the framework transparently. The application can be unaware of the auditing and the framework can change the way to do the auditing without modifying the application code.\n. @iamqizhao I just found there is something related in java implementation: \nhttps://github.com/grpc/grpc-java/blob/master/core/src/main/java/io/grpc/ClientInterceptor.java\nPersonally, I think adding a thin wrapper around the generated interface might be good enough for the common cases. But adding the ability to intercept the rpc invocation and handling might still be useful, at least in the case I described.\n. > not really. You can wrap it into a struct so that you only write the code\n\nonce. Again, wrapping is not in the generated code. The same approach\napplies to the server side too.\n\nI understand that. But I cannot create the struct dynamically as I mentioned.\nEven if golang supports making a struct at runtime, this will add a great amount of\ncomplexity comparing to add a \"hook place\" in the generated code.\n\nWe do not have plan to to support generic interceptor (except java) in grpc.\n\nCan you talk a little bit about your plan? Thanks!\n. @iamqizhao Thanks. Closing.\n. @iamqizhao Why do we use glog as the default logger? For performance reason? As https://github.com/grpc/grpc-go/issues/217 said, it registers some unwanted flags.\n. @iamqizhao Yes. \nTake a look at\nhttps://github.com/golang/glog/blob/master/glog.go#L398-L411\n. @mwitkow-io @juliusv @dsymonds\nHere is the result of #131: we use invoke directly and so bypass the generated wrapper at client side in order to add our own hooks and intercept the rpc calls.\n. @juliusv \n\nHis worry is that misbehaving hooks will cause bad and surprising behavior.\n\nWhy cant we assume the user would do the correct thing? \n\nIt seemed @dsymonds was most open to an approach where metrics would be counted in a very primitive fashion by grpc itself\n\nWon't this approach actually complicate gRPC the most? Or gRPC itself do not have to implement and maintain all of metrics. This sounds even more difficult than agreeing on a hook interface. \n. @iamqizhao mentioned that there will be an official API soon.\n. @iamqizhao \nhttps://github.com/grpc/grpc-go/blob/master/codes/codes.go#L50-L55\nFrom the doc, it says the server might return an unknown error in some cases. So if the application receives an unknown error, there is still no way to know if it is a connection error.\nWhat I want to achieve is to determine if the rpc fails because of client side issues (when the local network is down) or a connection failure (when the server is down).\n. > grpc internal always tries to address connection error by itself\nSo it will retry forever regardless the connection error?\n. @iamqizhao Will grpc-go provide the binary-logging feature? Or we should implement it as an interceptor? \n. Thanks!\n. @iamqizhao You can access github, but not google service in China mainland. \n. @tamird We have to call stream.Close or cancel the context of the entire stream to stop recv. But this will destroy the stream...\n. @tamird \nhttps://github.com/coreos/etcd/blob/master/clientv3/lease.go#L343-L344\n. @tamird Sorry. I should have checked your previous comment more carefully. On server side, we cannot... We just spawn a routine to close the stream at server side when receive the rpc call.\n. @kelseyhightower Thanks! This is what I wanted to add after I tried to understand errors and had to read the grpc code a little bit. \n. @iamqizhao Are we open to add a Flush() method to stream to enable user controlled batching?\n. @ashishgandhi It works fine I believe. You need to return the error correctly, not nil response + nil error.\n. @ryszard \nWhile waiting for the \"official\" doc, probably you can check out\nhttps://github.com/philips/grpc-gateway-example/blob/master/cmd/serve.go.\n. @iamqizhao I do not want to change sendMsg.\nThe generated Stream API is like Send(pbMessage). pbMessage is a struct type. For efficient broadcast, I want to send []byte directly. So I have to use sendMsg which accepts interface{} and my custom codec that can bypass encoding []byte by doing a type assertion.\nMy worry is that in the documentation it says sendMsg is used by generated code. So I wonder if I can use sendMsg directly and if gRPC go will maintain that API. If not, how shall I achieve same thing?\n. @iamqizhao Thank you!\n. > And it is true that they are designed to be called by generated code.\nI think you misunderstood my intention. I was saying \"It makes users to feel these funcs are designed to be called by generated code only.\" \nIf this is ONLY designed for generated code usage, we should document it explicitly. If it is not, it is confusing and we should change it.\n. @iamqizhao Good suggestion. Will do.\n. @iamqizhao Fixed. PTAL.\n. @iamqizhao All fixed.\n. @menghanl Fixed. PTAL.\n. Can you share some benchmark result with testing enviornment? So other people can know what to expect and to compare with?\n. @iamqizhao Thanks for the information!\n. Can you please use English for better communication on github?\n. More context, in etcd we have one connection per client. And that client has multiple APIs. Each APIs keeps several long-live streams to the server. If one API meets a connection error, it will close the connection. Then other APIs' streams will receive error (transport is closing) from Recv(). If gRPC does not clean up routines, each API needs to parse if the returned error is transport is closing and then cancel ctx accordingly. Or APIs can just try to grab the newly created connection and restart.\n. @iamqizhao It does not I believe. I will bump gRPC and give it another try. Thanks!\n. @iamqizhao It is fixed. Thanks!\n. @heyitsanthony @iamqizhao Thanks!\n. @heyitsanthony This change looks good to me.\n@iamqizhao By reading the code, I found another issue. So if we have multiple addresses, the routine will try to connect them one by one until succeeds. However the main routine will exit at the first error. This seems to be broken.\n. @iamqizhao Should it exit at the first error? Or it should continue to try other address?\n. @iamqizhao ACK. Works for me.\n. the direction looks good to me.\n. @menghanl @iamqizhao Can you take a look? Thanks!\n. @menghanl @iamqizhao Is there anything we can do to get this merged soon?\n. >  I'm trying to make Dial with WithBlock return fatal connection error if any of the connection fails. \nCan you explain the reason for this? Why should withBlock only try the first passed in endpoint?\n. @iamqizhao OK. My original thought is that blocking dial should try its best to finish the work. And dial timeout is used to limit the time it can spend on the best effort. To collect errors, we can have an error chan. But I guess I can understand your reasoning too. @heyitsanthony Probably for us, we can just use non-blocking dial? Is this a blocker?\n. @menghanl @iamqizhao Can you take a look? We want to get this merged soon. This blocks etcd release, blocks kubernetes + etcd3 integration.\n. @runeaune \nWhat does your workload look like? A lot of small payload? Or very large payload?\n. @iamqizhao This is a pretty significant performance regression for us. It would be great if you can take a look. Also do we have any documentation around running grpc benchmark tool?\n. @iamqizhao We will do more investigation tomorrow. We will let you know the result. Probably you can hold off the optimization for now.\n. @menghanl We found it is probably a compiler related issue. We are not sure how this change (changing error from struct to pointer) triggered it. Probably we should report to go upstream.\n. @iamqizhao Yes. The regression was from 1.6.3,, 1.7. The performance dropped 50% or so for no reason other than updating Go. We changed some code around the rpc path (like reverting the error pointer change or adding some random printf in the unreachable code path). And magically, we got the performance back... So we do not really understand why. We have not dig into the root cause.\n. @iamqizhao @menghanl This is a minimal change, can you take a look?\n. > If a connection error happens when client is writing or reading, the RPC (unary) will retry on other available connections.\nIn some cases, we do not want the RPC to be retried after finishing writing. Basically, at that point we do not know if the RPC succeeds or now. Retrying write request can have bad side effect.\n. > Sorry I forgot to mention, I was talking about non-failfast RPCs. Failfast RPCs will fail on connection errors.\nSo for non-retryable RPC, we need to use failfast option?\n. @menghanl \nIdeally, we want gRPC to retry when there is a connection error in the dial phase or before the request finishes sending, and not to retry when the request is already sent. Basically we want at most once semantics working with endpoint choosing/dialing.\nIs it possible today? Or we cannot use the balancer.\n. >  I would like to emphasize that even though grpc write returns successfully there is no guarantee your request msg can reach the server (it still could fail on any part of the path client host <--> network <--> server host).\nI understand this. And this is what I would expect.\nHere is our use case:\nWrite requests have side-effect. We cannot retry on any write requests that potentially succeeds (already wrote out, waiting for reply). \nWe do want to retry write request if there is a connection error. Say users give 3 endpoints, and 1 is bad. If the write goes to the bad one, we should retry.\nIn my opinion, each RPC can have several states:\n1. waiting for get an endpoint from balancer\n2. waiting for dialing the endpoint (we might ignore this if the returned endpoint is connected)\n3. writing the requests\n4. waiting for the response\n5. finishing up\nBasically I hope the retry happens before we entry state 3, but not after. fast-fail will return from state 2 without retry.\n. @iamqizhao \nYou are right that someone has to handle failures. But for us, we write a client that using grpc-go. For any error happens after state 2 (successfully dialed), we can directly return them to our users since etcd does not know what happened either just like grpc. For error happens before state 2 (including), we need to handle it. It is unreasonable for us to return an error to our users if it is just a connection failure. \nMy general feeling is that for the things that we are certain, we should retry. Or every libraries that use grpc-go will have to implement the similar logic.\n. lgtm\n. > Thanks for the reporting. This is not the right fix though. Let us take this over. will send the fix shortly.\nCan you please be more clear about why this is not the right fix next time? So that other people can understand it better. Thank you.\n. Thanks for the explanation.\n. @bdarnell \nhttps://github.com/coreos/etcd/blob/master/clientv3/retry.go#L26-L42. I feel this is pretty tedious though.\nRight now, fail-fast fails on all errors including retryable temporary one. It is on the user to define the retry policy to retry on another endpoint or the same endpoint. \nFail-fast makes sense when we want at most once semantics. It is impossible for gRPC to retry temp errors for requests that has been written out. gRPC can do better to retry requests that failed in dial stage though.\n. @bdarnell \nYou are right. \n\nWe're not currently using Balancers; maybe that changes things?\n\nI believe so. A customized balancer can bring a down connection back for connecting. But we should fix this bug still.\n. /cc @heyitsanthony \n. OK. Will fix at the application side.\n. /cc @heyitsanthony \n. @menghanl I do not think it is helpful. github issues are very busy. It is hard to track. The API change should be announced in the release note at least I think.\n. @menghanl Awesome. Thanks!\n. > we can set this up in the benchmark by setting core_limit = 1 in go QPS worker scenario config. this worth doing?\nI do not think we should do this. This seems like a heck way to improve benchmark. For a lot of applications, we cannot really set core_limit to 1. I believe people are more interested in seeing the benchmark in a more default like setup.. > (the default for go).\nThe default is max, not 1 since go1.5. See https://golang.org/doc/go1.5#introduction. @vjpai I disagree. If you disable gc, ping-pong can run faster, should we also do that? we should not change runtime to make it run slightly faster. Also that option is going to go away, see godoc if you're not familiar with it.. @vjpai \n\nbut tuning the GC certainly is\n\nWhat do you mean be tuning GC? To get the max performance for the pingpong test? Or to use mircobench to understand the system?\nDo you have any mechinism to do this for the ping pong test yet? \n\nWith regard to GOMAXPROCS\n\nHow do you know PROCS=1 works the best? Why not PROCS=2? Even for the ping-pong test, how do you know it can only utilize one core?\nFurther more, what the original issue suggested is to set the maxproc to 1, which seems to increase 5% for a specific go version on a specific machine. I still strongly disagree with that.\nAlso in your last reply, you changed your position from\n\nThe goal of ping-pong latency testing is to know the ultimate limit of what's achievable with a given API, not the practical reality.\n\nto \n\nExperimenting with this value gives us insights both into our own system and as potential feedback to the Go scheduling team.\n\nThey are VERY DIFFERENT thing. I think you already confused yourself.\nIf you want to try different go runtime paras to figure the limit of gRPC-go or the limit of go runtime, that is probably find. As @iamqizhao mentioned, we need to the infrastructure to enable that. Or the best way is to leave this as default value instead of a vague value that can be changed eventually.\n. In summary, I do not think we should tune runtime to achieve a higher bench result. If we want to understand the system better with microbench, we can tune paras. But we need to have the mechinism first instead of changing to a single value that seems to work the best.. @apolcyn Sure. We should finish the discussion about tuning paras before setting this to a fixed value of 1. Also we need to constantly evaluate these values.. @ctiller \n\nOur standard for these benchmarks has always been that it's ok to tune any\nknob that a user could reasonably be expected to tune in order to\ndemonstrate the best that we can do.\n\nWhat do you mean by\n\na user could reasonably be expected to tune? \n\nDo you mean that users can also make the tune easily (in this case setting go proc == simply adding a evn var)? Or they think the tuning itself is reasonable (setting go proc = 1 is reasonable to users)?. >   an application that's expected to run in production\nChanging to number of core to 1 is not realistic to the majority of Go production code I believe. I work on etcd, and it uses gRPC. We will never suggest any production users to change Core to 1 for gRPC. /cc @tamird I think Cockroachdb falls in the same bucket.\nIf you think it is realistic to change Go proc =1 for gRPC, give me examples.\nGo is not like C++, where I can play around freely with the threading library and interact with raw OS.. @apolcyn \nI understand we can tune go GC. That is a reasonable thing to do for a lot of cases.. > If you were working on an application that connected to a single peer and exchanged messages would you set max procs to 1\nProbably not. Making procs to N can make gc run concurrently with application. Also my application might want to process the data in async manner. \nAlso do you have any real world example? Are these application the large majority?\nIt seems to me that you are simply making it up to justify your decision.. >  don't need a high thread count\nI think we are talking about exact 1 here and a go program.\nBut I am curious that Lyft's proxy only handle at most one request concurrently with 0 pipelining? They set java thread to exact 1?\n/cc @mattklein123 is this true?. /cc @menghanl @MakMukhi\ncan you take a look?. @menghanl thank you so much for the quick review.. @menghanl can we get a release soon? this currently blocks our CI to get things done :P. for read request, we do want to retry. i think we need to provide options to users.. @dfawley awesome. good to learn.. one related question: how should we provide feedbacks on the new balancer API?. /cc @gyuho . @menghanl \nGreat! That matches my expectation.\nCan we document this somewhere? (Or if this is already documented, can you point me to the doc?). > Also, we are implementing retry logic in gRPC itself, hopefully it will be done in the next few weeks.\nYou mean implementing retry policy? gRPC-go has naive retry support with failfast=false on connection issues as it is today, no?. > on-fail-fast is not supposed to retry; it's supposed to wait until a connection is available, and then send the RPC\nthis makes much more sense to me. can you update the documentation here: https://godoc.org/google.golang.org/grpc#FailFast to clarify that failfast wont retry on transit failures? thank you.. > but that is actually a bug\n@dfawley will the bug be fixed in the up coming release. We now have to work around it by disabling  failfast, and write our own connection ready waiting logic.. @dfawley much better. thanks a lot! . @dfawley @menghanl \nany update on this one?. @menghanl \nCan you report back to upstream then, or can you parse it at gRPC-go side and unify the errors?\nWe probably should try the best to provide a consistent error no matter what is the underlying transport. . /cc @menghanl can you take a quick look at this PR? it affects many projects right now.. @menghanl \n\nIt's unexpected to create a ClientConn to an IP, and then connect to a different IP using the same ClientConn.\n\nWhy cannot we dial to an IP, and then supply the balancer with alternative IP addresses as different backends? I think this is also a valid use case.. one thing I was trying to figure out is that:\nis this error a rpc error from server side or this is a client-side/network error?\n. This is what I am doing. However, in the doc it also says https://github.com/grpc/grpc-go/blob/master/codes/codes.go#L53. \n. remove these empty lines?\n. Constructing?\n. should be rather than will be? (or gRPC will automatically convert uppercase to lowercase?)\n. surfixed -> suffixed\n. extra empty line?\n. extra empty line?\n. align with other fields?\n. alignment issue here\n. alignment issue.\n. <tab>ctx\n<space><space><space><space>someRequest\nso the fields are not properly aligned.\nswitch to use all tab or all space to be consistent?\n. the method naming is probably confusing... I would expect isItem to return true, or false. I would not expect this method to exist as a marker item.\n. @tamird If this is a marker method, I feel so. There are some code in stdlib using similar approach (https://golang.org/src/go/ast/ast.go: type Expr interface is an example)\n. it seems like the whole timeout var can be removed.\n. oh... right.\n. is this relevant to this commit?\n. ",
    "bakins": "Why not use HTTP Upgrade?\n. ",
    "philips": "@xiang90 @bradfitz Where is this discussion happening in the \"normal grpc\" channels?\n. Started the conversation here: https://groups.google.com/d/msg/grpc-io/JnjCYGPMUms/JDmi9-UpDuoJ\n. On Mon, May 18, 2015 at 11:02 AM, Qi Zhao notifications@github.com wrote:\n\nAs I mentioned before, this will involve some redesign of http2 package. In\ngeneral, I suspect whether it is worth doing a lot of redesign and\nrewriting to accommodate http1 traffic and I strongly lean to serve http2\nand http1 traffic on different ports.\n\nWould you take patches to make this library do that?\nI think the major reason people find gRPC interesting is because it is ran\nover http2; and the only reason http2 is interesting is because it is\nbackwards compatible with http1+tls. \"Backwards compatibility\" is very\nimportant; I am sure http2 would have been much more elegantly designed if\nthey decided to run it over a different port. As an example we want to have\na gRPC option for etcd but we need to have it run on the same port so when\npeople upgrade to etcd v3.0.0 with gRPC they don't need to reconfigure load\nbalancers, firewalls, or certificates.\n. Don't we want to end up essentially having this library be a path dispatcher on top of the generic HTTP2 library? As explained here: https://groups.google.com/d/msg/grpc-io/JnjCYGPMUms/L2c9m5KiR3gJ\nI guess what I am saying is that grpc-go should not implement a Server type at all and should essentially be a http.Handler type. Right?\n. @bradfitz OK, that makes sense. We anxiously await some direction on this so we can use grpc-go in our projects. In the meantime we will continue our development on different ports in etcd.\n. Where are we on this? Is the consensus that we put up a PR that documents using cmux?\n. @iamqizhao Do you have any updates on your approach to gRPC and HTTP 1.1 on the same port?\n. Is there a high-level description of how this would be used?\n. Thank you for the context, makes sense.\nOn Thu, Aug 6, 2015 at 2:11 PM Qi Zhao notifications@github.com wrote:\n\nHi Philips,\nA half-baked doc is at\nhttps://github.com/a11r/grpc/blob/doc2/doc/naming.md. And I will have a\ncomplementary doc for go specific.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/280#issuecomment-128509305.\n. What needs to be done here? It is the only issue opened against the \"beta\" milestone.\n. @iamqizhao Where is the roadmap? Is this something we can help with?\n. Looking back at this issue this is another example of something that would be solved by building on top of net/http2 directly instead of patching.\n. I still feel we must say explicitly what that list of exceptions is if we want the community to help out.\n. @iamqizhao We can just link to this list and say you can only rely on those non-builtin packages: http://godoc.org/google.golang.org/grpc?imports\n. @dsymonds I am trying to find some way of pointing at this list mentioned in the README now: \"A new addition to the list requires a discussion with gRPC-Go authors and consultants.\" Isn't this link that list? http://godoc.org/google.golang.org/grpc?imports\n. I am trying to make it non-ambiguous so we know when we have to have the discussion or not with new contributors.\n. LGTM, thank you\n. cc @xiang90 @gyuho @heyitsanthony \n. +1 on having a common implementation of this so the community can interop.\n. Well, I can't define external because I don't know the rules. Clearly we should document the rules so we can point to the rules when people open PRs and hopefully avoid duplicate work.\n. define official. I don't know what official means.\n. THat is a great way to put it.\n\nOn Mon, Sep 21, 2015 at 10:53 AM Qi Zhao notifications@github.com wrote:\n\nIn README.md\nhttps://github.com/grpc/grpc-go/pull/345#discussion_r40002547:\n\n@@ -18,6 +18,11 @@ Prerequisites\nThis requires Go 1.4 or above.\n+Design Constraints\n+------------------\n+\n+The google.golang.org/grpc http://google.golang.org/grpc Go package must not depend on any external Go libraries. This is to simplify use when vendored into other Go projects.\n\nI meant https://github.com/golang.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/345/files#r40002547.\n. \n",
    "mwitkow": "This would be great for us.\nWe'd like to host our /statusz (debug pages) end /metrics (Prometheus metrics endpoint) on the same port to \na) make our DNS SRV based service discovery easier (not needing to write two ports)\nb) leverage Prometheus scraping to detect whether the actual service endpoint is up\n@bradfitz any further thoughts of merging grpc-go with your http2 library to facilitate shared dispatch?\n. Does gRPC do automatic negotiation of compression? If this is added for other languages (Java, C++), will they be transparently negotiating it?\n. Nice to see lameduck mode being planned :) \nAre you guys planning in any way to bridge it with other mechanisms, such as:\nhttps://github.com/facebookgo/grace\n. @iamqizhao, can you provide any tips as to how to get started here? Is the logic inside http2Server? \n. That sounds great!\nCan we use this bug for tracking client-side LB work in grpc-go or do you track it somewhere else? \n. I hope you enjoyed your vacation!\nGreat to hear this picking up steam, we'd be really interested in seeing a DNS SRV ReplicatedClientChannel :)\n. Any news here? We could lend a hand implementing DNS SRV, @miekg :)\n. @iamqizhao \nI know you guys are working on this as high priority. Any ETAs on when the Picker and Naming APIs will be considered stable?\n. @iamqizhao An abstraction over picker would be amazing. Would help me move https://github.com/mwitkow/grpc-proxy (which we use in prod) onto a non-hacked version of gRPC :)\n. @iamqizhao can you please send the current ammended LB proposal my way as well? :)\n. Trace looks amazing! I'm really happy that go-rpc will have RPC debug pages  straight out of the box :)\nHowever, the trace.Trace interface doesn't expose access to the Family, Title, Start orElapsedfields of the privatestruct trace.trace. Nor does it have the ability to add callbacks onNewTraceandTrace.Finished()` to increment/decrement relevant metric counters.\ngRPC already has traceInfo struct, that is used here:\n/home/michal/code/go/src/google.golang.org/grpc/call.go:121\ngo\n    if EnableTracing {\n        c.traceInfo.tr = trace.New(\"Sent.\"+methodFamily(method), method)\n        defer c.traceInfo.tr.Finish()\nHowever, if you were wrapping trace, it would mean that the actual trace.trace data would be duplicated. \nI think it may be better to extend trace package to have such callbacks.\n- @dsymonds and @miekg for context.\n. Also, it seems that the traceInfo is only currently rolled out on the Client side, (call.go, stream.go) but not on the Server. Any idea where that could be plugged in? :)\n. So, I decided to take a first stab at this in https://github.com/grpc/grpc-go/pull/299\ngRPC seems to allow users to override the marshalling/unmarhsalling codec with pretty much whatever they want. As such, users can already significantly impact the performance of gRPC by deciding to use something wonky.\nIn the PR, I use a simple callback-based approach, that leaves a lot of flexibility to implement the monitoring as the user wishes. The choice of which implementation is used is made through  server.options or client.DialOpts. \nAn example server-side implementation is provided for Prometheus,  mimicked after the level of instrumentation one can find in Google servers by default. heres an example of a ~~varz~~ metrics page with examples of streaming and unary server-side instrumentation:\n\n. @iamqizhao even if bufSize is configurable, as far as I understand the proposal here, it would control the publication for both Tracing and Monitoring. As such, if I wanted full fidelity for Monitoring (which should be lightweight - incrementing a counter), I would have lossless Tracing (which can be expensive - writes to disks or something). The tracing could be fairly costly. Or alternatively I could disable Tracing altogether to retain Monitoring.\nI tend to agree with @matttproud regarding lossless monitoring. As a SRE or a systems operator, I want to have full confidence that the counter I'm observing has been incremented on every RPC. If it was lossy, I probably want the monitoring to export a counter that said that a certain number of values was dropped... but that breeds a chicken and egg problem.\n. @iamqizhao, what's your take about the change proposed in https://github.com/grpc/grpc-go/pull/299 ?\nI know @matttproud commented on it, and @pires said that API would be great for adding InfluxDB support as well (another popular monitoring framework).\nGiven that https://groups.google.com/d/msg/grpc-io/K8Ar3wom5CM/ushngUvb5GYJ suggests that third party monitoring support is on the books, can we get some traction on the subject?\nOr if the PR doesn't fit your needs, can you at least comment if the edge cases of client-side monitoring are covered in it correctly? We're already using it and I would sleep better knowing that it's correct. \n. So, we'd like to use Context as a way of passing into Service implementation all the things necessary to process the RPC in it's scope. \nWe currently have two use cases: one an auth object and the server hop-tracing.\nFor the first one we'd like to parametrize the Context with a RpcAuther that will handle things like: answering whether the given Bearer Token is good enough for this call, propagate it to other RPC calls. \nIf we could seed the RpcAuther into the server's Context, it would be immediately available to all Services handled by that server.\nSecond use case is tracing what the name of the server handling the request is, so we can have an easy to use trace of which backends were used for this particular query. Again, it seems like it would make sure to have this available in the Context.\nThe alternative is to inject the values into the struct that implements the Service. This unfortunately leads to boilerplate (you need a rpcAuther and a serverName field in every Service that you implement, and we have tons).\n. Ok, neither of our use cases is unsolveable without this.\n. Added the client side stream monitoring, but I am not entirely correct if I got all the edge cases. @iamqizhao can you take a look? Even if this is not gonna make it upstream, we still intend to use it internally :)\n. Glad to hear that right after coming back from vacation :)\nTwo things to discuss:\n- would you prefer the NoOpMonitor to be removed and substituted with if monitor != nil? There's hardly any allocation done per-RPC so the performance impact should be negligible, but the code is clearer.\n- would you prefer a collapsed ClientMonitor and ServerMonitor interfaces? Their signatures would be the same.\n. @iamqizhao I have refactored the PR to:\n- remove NewClientMonitor and NewServerMonitor to be unified and have the same signature\n- rebase on top of current master, which includes the Picker changes that broke\nCould you PTAL? :)\n. @iamqizhao, I understand that you guys have tons of other work. It would be at least useful to know whether this PR is considered for acceptance? \nWe're thinking about relying on it for our SLO monitoring (using different error.Codes to differentiated between user faults and our system faults), and knowing whether this PR has a chance of being upstreamed would be incredibly useful.\n. @iamqizhao Great to hear that. I have rebase over the latest master.\nI'm wondering how to add unit tests that will make the monitoring still work correctly across major refactors such as the ones I'm currently rebasing on. Maybe retrofit some integration tests with monitoring counters?\n. That's ok :) LB and naming is something we're incredibly interested in (and willing to put some manpower behind DNS SRV implementation).\nCan you provide some pointers regarding how to implement the things in the end2end test?\n. @iamqizhao, any updates here? we've been happily using this monitoring for the last 2 months in prod and are willing to help out getting it upstreamed :)\n. @iamqizhao, any updates on the metrics API? We've been happily using this in prod for a while now :)\n. I moved this implementation into a server-side interceptor under:\nhttps://github.com/mwitkow/go-grpc-prometheus\n. 1. Then the Header CallOption documentation is wrong as it only states\n   unary RPCs.\n2. I'm slightly confused how can you pass in the Context to the call. Can\n   you provide an example?\nThe only thing the call accepts on a generated Client interface is the\nProto msg and CallOptions? I haven't seen a CallOption for Context.\nIs it a DialOption? If so, this kinda would break my abstraction, as the\nApplication layer should rarely know what address it needs to dial.\nOn Fri, 21 Aug 2015 20:52 Qi Zhao notifications@github.com wrote:\n\n1.\nThis belongs to application logic. In your service impl handler, you\n   can simply take the context and pass it the downstream rpc calls. In this\n   way, the upstream rpc is chained with dwonstream rpcs.\n   2.\nIt works for both. I am not sure what you missed. Can you take a look\n   at\n   https://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L767\n   as an example for metadata on streaming rpc?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/301#issuecomment-133529396.\n. Is this something that could be slated for GA? We'd be happy to contribute to make this happen.\n. For our own internal purposes we managed to work around this problem in the following way:\n\nWe implemented a custom credentials.AuthInfo:\n``` go\ntype CallAuthInfo struct {\n    RemoteAddr net.Addr\n    Parent credentials.AuthInfo\n}\nfunc (t CallAuthInfo) AuthType() string {\n    return \"callinfo\"\n}\n```\nAnd a custom credentials.TransportAuthenticator called CallInfoAuthenticator, which among other things does:\n``` go\nfunc (c *callInfoAuthenticator) ServerHandshake(rawConn net.Conn) (net.Conn, credentials.AuthInfo, error) {\n    callInfo := CallAuthInfo{RemoteAddr: rawConn.RemoteAddr()}\n    if c.tlsCreds != nil {\n        conn, info, err := tlsCreds.ServerHandshake(rawConn)\n        if err != nil {\n            return nil, nil, err\n        }\n        callInfo.Parent = info\n        return conn, callInfo, nil\n    }\n    return rawConn, callInfo, nil\n}\n```\n. Peter, while this would be pretty awesome, I think a similar idea was discarded in https://github.com/grpc/grpc-go/issues/131\n. Something similar would be incredibly useful for us as well. The gRPC Java implementation has interceptors, @iamqizhao maybe it's something worth reconsidering?\n. @iamqizhao FTR, we're using a similar approach to inject an authentication token extractor for our incoming RPCs, so that particular logic could be shared between all calls in a given Service.\nWhat we did was:\n- changed the grpc code generator to generate a public MyService_serviceDesc instead of a private _MyService_serviceDesc\n- iterated over all the Handlers and the Streams of the ServiceDesc and wrapped them in a function\n- the function would extract the Bearer token from the Context, reject the RPC if it was missing or invalid, and return to the handler a Context with the decoded token bound in it\nDoing that was pretty messy, mostly due to the need to change the gRPC proto generator, but the payoff has been great: we've removed code duplication dozens of services, and found that certain ones of them were doing the verification in an incomplete fashion.\nAs such, I do agree with @peter-edge that there is a pretty good case for Interceptors, or at least public ServiceDesc handlers.\n. dang, this is embarrasing. Apologies.\n. For reference: https://github.com/mwitkow-io/grpc-proxy/blob/master/proxy.go#L187\nI can't rely on getting the Picker from the user, as the public interface for \"dialing\"  is ClientConn. Thus, I made ClientConn the interface I expect the users of the gRPC proxy to use to specify the destinations of their RPCs.\nBasically the problem I'm facing is there is no way of getting a transport.ClientTransport out of the ClientConn, which is needed for raw shoveling of data frames.\n. I actually would really like to piggy-back on the Picker, as a combination of a Proxy + naming-backed pools of machines would be very powerful. A RR-ed ClientTransport I'd get from the Picker is exactly what I'd need for providing load balancing with no effort :)\nThe problem is that there is currently no way to get a ClientTransport either out of a Picker or a ClientConn. The logic currently is hidden behind NewClientStream that wraps it up in a bunch of private encoding abstractions.\nEither one of the two would be immensely helpful:\n- ClientConn.Picker() -> Picker\n- ClientConn.NewTransport() -> (ClientTransport, error)\nThis means that the ClientConn which is the user-visible manifestation of a \"gRPC backend\" (if I understand the intention), with all the LB and backend healthchecking, would be able to return a transport.ClientTransport, for raw consumption.\n. @iamqizhao, can you PTAL at the issue? I'd really love to get this in so that the gRPC proxy doesn't need a patched gRPC client.\n. @iamqizhao I think there is a slight misunderstanding here.\nIn my implementation, I'm not skipping the grpc.ClientConn, I'm purposefully using it as an abstraction representing a \"backend\" (which IMHO fits the general gRPC approach). Why do you think a proxy should custom implementation of a transport Picker? I see no benefit, and mostly drawbacks of duplicated code, or dropping functionality. I understand you're busy, but would appreciate your thoughts about it so that the Proxy code is as reusable by others as possible.\nAs to the Picker, how about the other approach: \nNewTransport(ClientConn) -> (ClientTransport, error), similarly to NewClientStream and Invoke, but returning the grpc.transport abstraction. This seems to me less messy then exposing a Picker, and in-line with the current public abstractions. What do you think?\n. @bradfitz as he's the maintainer of http2/hpack: should hEnc.WriteField force the lower case encoding of field names?\n. @bradfitz thanks a bunch for finding these! we spent a while trying to get down to where we were leaking goroutines in our prod servers. Rolling restarts got tiring after a while :)\nWill test these out ASAP :)\n. @iamqizhao, I do agree that having a periodic health check would solve the problem. However, I don't agree that expecting a user to do user-code level solutions is appropriate when the issue is on the transport level.\nThe issue with GCE LB, and many other routers, terminating long running connections is due to Connection Tracking that stateful routing/firewalling mechanisms have. If the tracking expires, the connection gets dropped, without a TCP RST packet sent to either the server nor the client. This is incredibly hard to debug if no KeepAlive is present, since the gRPC transport layer has no idea that it's sending stuff into a \"void\", as the underlying TCP socket will never return a timeout (the default system timeout is like 2h nowadays). This manifested itself in two ways for us:\n- RPCs would reach the server and finish with OK, but no response would be passed to the client calle\n- the client caller would hang forever (in absence of context timeouts) since none of the ConnectionTimeout gRPC errors would be raised, and the request would be consumed by the \"void\" TCP stream and never reach the server\n@bradfitz, does HTTP2 support TCP KeepAlives like HTTP1.1? If so, can we document that using KeepAlives for using gRPC over public internet?\n. Do we document it somehow for other users, so they don't have to waste time\ntroubleshooting this? \u263a\ufe0f\nOn Tue, 9 Feb 2016, 19:02 Qi Zhao notifications@github.com wrote:\n\nThere are 3 different layers relevant to this issue:\ni) transport layer (e.g., TCP): You can use grpc.WithDialer to pass a\nDialer with KeepAlives on;\nii) rpc layer (e.g., PING frame if we use http2):\na) user-initiated PING: we will have API to support this;\nb) grpc-initated PING: under debating.\niii) application layer: health check service as I mentioned\nIn principle, we lean to letting users initiate this health signal by\nthemselves instead of grpc lib because it could generate non-trivial\ntraffic which could be charged.\nOverall, I do not see any problem here.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/536#issuecomment-182006950.\n. Cool, thanks!\n\nOn Tue, 9 Feb 2016, 20:45 Qi Zhao notifications@github.com wrote:\n\nI think we should. Need talk to the team.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/536#issuecomment-182056434.\n. @bradfitz is there some way in which we could help? :)\n. Any pointers as to how to start? Is there any particular test you pprof to find bottlenecks? :)\n. @dfawley this is the bug I mentioned in person at CoreOS Fest. Would be fantastic if we could reach serving parity on the HandlerTransport to the one with a pure socket listener.. I agree, having multi-interceptor design would be great.\n\nWe're currently in the process of moving from our own internal implementation of interceptors to upstream. @iamqizhao @menghanl, can we have this multi support upstream or should we roll our own?\n. You mentioned contrib inside grpc go, you still keen on doing that?\nOn Thu, 12 May 2016, 19:15 Qi Zhao, notifications@github.com wrote:\n\nPlease roll your own. It is important to minimize the public API for\ngRPC-Go at the current stage (we already have massive public API).\nOtherwise I can see we would easily run into API stability and maintenance\nproblems when more and more new feature requests pop out in the future. I\nalso think providing multiple ways to users for a single purpose sometimes\nhurts the user experience.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/653#issuecomment-218840989\n. Awesome\n\nOn Thu, 12 May 2016, 20:41 Qi Zhao, notifications@github.com wrote:\n\nI think contrib will be out soon. It is outside the library repo -- we\nwill have a new org and multiple repos for contrib.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/653#issuecomment-218863887\n. It would be awesome to have a common implementation. \nWe've moved our gRPC Prometheus monitoring stuff into https://github.com/mwitkow/go-grpc-prometheus, which complies to the standard single-interceptor interface.\n. When moving away from our \"hacked\" interceptor design we had for months, I realised the problems with interceptors go slightly deeper:\n- it's hard to chain (and this doesn't seem like it will be merged)\n- accessing and modifying the ServerStream.Context is hard, as it is read-only and not overwriteable\n- per-message access in ServerStream is hard, as you need to hack things together\n\nI started https://github.com/mwitkow/go-grpc-middleware, with a chaining (basically @peter-edge's equivalent of middleware) and WrappedServerStream implementation that we needed so far. I'd be happy to move it into grpc-contrib once that lands.\n. @zinuga Do you think we can include https://github.com/mwitkow/go-grpc-middleware into grpc-ecosystem? :). I'm not the gRPC maintainer, but have used gRPC extensively. \n1. AFAIK you can use the connections concurrently between clients. The only problem may be the limitation of number of concurrent HTTP2 streams you can do, but these count in thousands.\n2. Yes.\n3. Yes and no.\n   -  gRPC go has a Picker interface which deals with choosing a channel (tcp connection) to send it down to. Ideally you'd do all your pooling here. Don't know what the state of the Picker implementation is. @iamqizhao ?\n   - gRPC 1.1 will have a way of healthchecking a backend, making Picker implementations better. See https://github.com/grpc/grpc-java/issues/1648#issuecomment-207581337\n. @iamqizhao thoughts on enabling authority access in MD? \n. @iamqizhao can you help with the errors?\nend2end_test.go:1685: TestService/UnaryCall(_, _) = _, rpc error: code = 13 desc = pseudo header field after regular, want _, <nil>\nThis seems to be an error from http/Framer.ReadFrame(). However, I fail to see how an decodeState.mdata field would be parsed to framer.Reader. Do the end2end tests pass teh same context to responses and back to clients?\n. @iamqizhao, did some investigation. Apparently: pseudo header field after regular can happen whenever users specify a reserved http2 field (e.g. :authority). This breaks every client gRPC says, so I added code to guard against that.\nThe reason why the test was failing, was because all the end2end tests forward the received context (and thus the new :authority field).\n. Also it seems that TestCompressOK test is flaky. It seems that if you provide Metadata to a compress call, it will fail to call\ngo\n    if s.sendCompress != \"\" {\n        fmt.Printf(\"Calling send compress!\\n\")\n        t.hEnc.WriteField(hpack.HeaderField{Name: \"grpc-encoding\", Value: s.sendCompress})\n    }\nnever gets called, hence the filtering of reserved fields fails.\nhttps://github.com/grpc/grpc-go/issues/686\n. @menghanl Can you please take a look? This has been out for a month now\n. @iamqizhao how's the GA going? :) Any chance we could get this in? :)\n. @menghanl any news here? :) Can we get this merged? :)\n. @iamqizhao is it worth me taking another look at this to get it merged? It's something we use with grpc-proxy.\n. Closed in favour of:\nhttps://github.com/grpc/grpc-go/pull/1082. The problem is that the grpc-encoding client doesn't receive a grpc-encoding header. \n. The problem is: \nIn Server.processUnaryRPC the call to set the compression mode of the stream (      stream.SetSendCompress(s.opts.cp.Type())) is made too late, and some SendHeader calls already completed.\n. Server reflection would be awesome!\n@menghanl is there any draft proposal you could share? \u263a\ufe0f\nOn Wed, 25 May 2016, 21:32 Qi Zhao, notifications@github.com wrote:\n\nI am not strongly against this. We can probably make it happen along with\nthe server reflection support @menghanl https://github.com/menghanl is\nworking on. It may provide richer information -- e.g., beside all method\nnames it may also provide the type info of the input&output params.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/689#issuecomment-221698105\n. Cool, thanks that will definitely work for us :)\n. @menghanl I hit a problem with ServiceInfo: there's no marker on it whether the given RPC is a stream or a unary one. It would be fantastic if we surfaced similar info as the StreamServerInfo, which is how https://github.com/mwitkow/go-grpc-prometheus differentiates between types.\n. @mischief proxying over HTTP CONNECT can be done using a custom dialer. That's why I wrote https://github.com/mwitkow/go-http-dialer\n. Thanks @menghanl \nWould it be possible to surface the IsServerStream or IsClientStream similar to grpc.StreamServerInfo? Prometheus interceptor relies on these to decide how to label the RPC\nhttps://github.com/mwitkow/go-grpc-prometheus/blob/master/server_reporter.go#L16\n. Awesome, thank you! :)\n. @iamqizhao great to hear. \n\nWill the fix involve returning grpc Code errors as part of .Send() so that the client can see that the server doesn't accept any more traffic.\n. Would happily provide a patch, but want to make sure this was acceptable :)\n. What if it was opt in? I.e. by default we use the single Codec() if exactly application/grpc was used (without the + part).\nBy default all implementations already do a check on the validContentType. \nExtending that code point to add an optional substring past + (if it existed) shouldn't be a performance problem for typical case. Then you just take the extra content type and put it in Stream and do a if != \"\" check before doing any codec lookups.\nThe performance impact would be totally opt-in by specifying both WithMultiCodecs() and application/grpc+json.\n. I can hack something together over the weekend. Are there any pointers as to how to run the benchmarks? :)\n. Got a half-broken branch somewhere. However, it's problably worhtwhile syncing with the efforts in:\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-WEB.md\n@wenbozhu thoughts?. Indeed, but the spec allows you to specify an Accept header that specifies\nencoding (json or PB). I assume the underlying grpc library would need to\nchose the respective marshaller before putting it down to the framing\nlayer.\nOn Fri, 10 Feb 2017, 17:08 Wenbo Zhu, notifications@github.com wrote:\n\n@mwitkow https://github.com/mwitkow Not quite, grpc-web addresses the\nwire encoding of grpc frames, not the encoding of the payload of grpc\nframes.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/803#issuecomment-279002202, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJNWo_EkxFKtfYV6l30cerrwHM03XIu2ks5rbJmHgaJpZM4JYyAf\n.\n. @nodirt does classic AppEngine do HTTP2 now? :)\n. I was thinking about developing something like polyglot in Golang, but unfortunately it seems that there are no dynamic protobuf serialization libraries in Golang (Java/C++ Dynamic Message equivalents).\n\n@hsaliak Is that something that is planned? :)\n. @MakMukhi any chance this gets prioritised? :). Could be down to golang/protobuf serialization being based on reflection.\nGogo protobuf is a drop-in replacement that uses code-generation for proto serialization. \nhttps://github.com/gogo/protobuf\nhttps://discuss.dgraph.io/t/gogoprotobuf-is-extremely-fast/639\nWould be interested in someone re-running these benchmarks with the codegen :)\n. @yugui you may be interested in this as the maintainer of grpc-gateway.\n. This would be really useful for @yugui and grpc-gateway use case.\n. Ok, while this would be nice to handle inside gRPC, we just wrapped the grpc.Dialer and net.Listener in its own monitor layer:\nhttps://github.com/mwitkow/go-conntrack\n. @ejona86 @zinuga can we get some traction here?\nThere's a PR that is attempting to fix this https://github.com/grpc/grpc-go/pull/1126 but we are also hitting this. Hand-cranking a initialWindowSize number to something magical is our workaround, but this is pretty bad for gRPC rep: https://news.ycombinator.com/item?id=14211213. @menghanl Could we please get a parameter for this? We've been hitting rst_stream on our requsts to Datastore over gRPC and would like to validate if that's the case.. PTAL :). @menghanl addressed comments and moved inline.. @tamird @menghanl I squashed the commits, addressed the code and comment reviews. Can we get this in? :). @menghanl addressed the final coment.. Thanks @menghanl, that was all that was needed!\nI've updated the gRPC-Proxy repo with the new handler:\nhttps://github.com/mwitkow/grpc-proxy\nI'll be working on a stand-alone binary sometime in the next 2 weeks.. Go already has an abstraction of net.Dialer which can be usedto address both points of the https://github.com/grpc/proposal/pull/4/files proposal: allowing to overwrite the address and perform TCP-level operations before handing off to gRPC. Can we please use that instead to make the library more Go-canonical and just write sugar-APIs on top to make it look like gRPC C++?\nWe had exactly the same problem: dialing remote gRPC endpoints over HTTPS connect. We've built https://github.com/mwitkow/go-http-dialer and end-to-end integraiton tests gRPC plaintext and gRPC tls https://github.com/mwitkow/go-http-dialer/blob/master/test/grpc_e2e_test.go\n. The dialer gets the hostname in single connection mode so you definitely\ncan mapname as well (that's what we do).\nUnless you're taking of mapname before hitting grpc.Naming then you're\nright. But few people would be using Grpc.Naming and grpc LB with a proxy\nconnection.\nOn Tue, 28 Feb 2017, 18:23 Menghan Li, notifications@github.com wrote:\n\nIn the proposal, it's proposed that we need to kinds of mapping functions,\nMapName before name resolution and MapAddress after name resolution.\nCustom dialer only works in the case of MapAddress.\nIf the name cannot be resolved, dialer won't work.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/1095#issuecomment-283121701, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJNWoyK3e-xGVs8BB-p7DNkamKaNLa8sks5rhGYzgaJpZM4MNw9W\n.\n. @peter-edge nope. The grpc-proxy code doesn't rely on content encoding at all, it just passes all metadata through:\nhttps://github.com/mwitkow/grpc-proxy/blob/master/proxy/codec.go#L17. @mehrdada yes, I've considered it, and I do think such per-call option tweaks should be achieved by.. well a CallOption. It is a significantly more type-safe way of doing it.\n\nThis really doesn't seem like a massive ask, and actually would breed a nicer interface for the community of grpc-go interceptor authors to use.. @mehrdada can you provide the reasoning as to why this is bad? Not having it makes it hard to add extensions to grpc-go and build a viable ecosystem.. Just to clarify, I'm not advocating exposing the before and after methods of the CallOption, interceptors are very much capable of that. In fact, that's exactly what the retry middleware does: it strips out its own CallOptions that drive the it's own before/after behaviours:\nhttps://github.com/grpc-ecosystem/go-grpc-middleware/blob/master/retry/retry.go#L31\nIn terms of type safety, I do think that having your own CallOption type inherit from grpc.CallOption is much more type safe. It's narrowing down what the user can pass you trivial to separate the two:\nhttps://github.com/grpc-ecosystem/go-grpc-middleware/blob/master/retry/options.go#L111\nMoreover, this allows for much nicer call patterns that are more obvious to the user when they see CallOption:\ngo\nstream, err := s.Client.PingList(parentCtx, goodPing, grpc_retry.WithMax(5))\ninstead of\ngo\nstream, err := s.Client.PingList(grpc_retry.ContextOption(ctx, grpc_retry.WithMax(5)), goodPing)\nJust to clarify: it is already possible for external developers to rely on CallOption anonymously, like I do. The reason why I'm proposing implementing:\ngo\ntype NoOpCallOption struct {}\nfunc (o NoOpCallOption) before(c *callInfo) error { return nil }\nfunc (o NoOpCallOption) after(c *callInfo)        {  }\nis to make that already-possible practice safer (and avoid things lke https://github.com/grpc-ecosystem/go-grpc-middleware/issues/37). The only way to avoid this practice to be used is through making CallOption private, but that would break existing code of end users anyway.\n. @dfawley can we get your option but with using a context.Context for timeouts. E.g:\nctx := context.WithTimeout(parentCtx, 1 * time.Second)\nerr := stream.SendMsg(ctx)\nand the err optionally returning a context.Deadline exceeded which can be retried by the sender.\nThis also means that all cleanups on parentCtx will work.. @MakMukhi the http.Handler transport does respect it:\nhttps://github.com/grpc/grpc-go/blob/master/transport/handler_server.go#L263\n. Indeed, should have run goimports on this before exporting.\n. in thsi case the receiver name is needed, because it is returning itself, updated in other places.\n. I haven't thought about it, but: the first 3 types are string, which is 16 bits each * 3 = 48.\nThe last item is time.Time which is 24 bytes.  So that's 72bits  = 9B total. So there is no padding necessary.\nI could turn the time.Time into a pointer and reduce it to 56bits, but that is even more wasteful due to pointers.\nDoes that answer your concerns?\n. Indeed. Mea culpa :)\n. I didn't want to bring confusion as it seems the prevelant opinion in the bug thread is that Tracing data structures should be different from monitoring ones: https://github.com/grpc/grpc-go/issues/240#issuecomment-120210769\n. No, I haven't. But the gRPC codebase seems to be full of defers. Should I put a if monitor.(type) == monitoring.NoOpMonitor?\n. Monitoring on service + method name allows you to do richer queries in Prometheus. E.g. count all errors in a given Service, regardless of Method.\n. Ack.\n. Yes, they allow you to create a monitor for a given RPC.\n. Thanks.\n. other things related to what happened to the RPC. Handled, Received Sent. Erred seemed to fit.\n. Ack.\n. Yup, see later ocmment.\n. In case of the NoOpMonitor there is only one allocation: the one that happens on Server create or on Client side connection create. Otherwise, per-RPC, the same object is reused.\nI initially made the if monitor !=nil but the code became pretty hard to read, as there were many corner cases. I can move back to that approach if you prefer it.\n. Discussed below.\n. But then they would clash with ClientMonitor. Or would you prefer me to collapse them into one?\n. Indeed, I thought this would be needed for http2 Handler implementation, but that required additional code anyway.\n. lame ducking FTW! thanks @iamqizhao!\n. done.\n. I know, I extracted it into a single function with an error listing the full method name (service + method). The code otherwise was completely duplicate.\nWould you prefer me to return it to the previous state?. Just out of curiosity, is there a gRPC-standard requirement for the description sent as part of these errors (i.e. is it mandate that you need to diff between method and service existence?). Ok, reverted.. Done.. Yeah, but then it is unclear what to do with dummyService service desc. Moving it to the top of the file would pollute it with something ambiguous.\nI think that keeping this in a separate function with a clear name of what it does makes sense, given that this handleStreams function is already fairly busy. Unless you guys have a policy of keeping the number of functions low, in which case I can inline it.. Ack, added a nil check to processStreamingRPC.. @menghanl asked me to move it in like this, that's because srv can be nil. changed.. Fixed the doc.. isn't this expensive per call?. I think this should only support subtypes, i.e. application/grpc[+subtype] and by default be an empty string.. Um. I do think we should keep the existing behaviour of using application/grpc if the user hasn't specified proto as their codec explicitly. \nOtherwise other user's code may silently break and it's unclear how it would interact with other gRPC implementations.. Um. Wouldn't it be better to just extend the Codec interface to have a method that exposes the Subtype? Otehrwise using this will be kinda hard.. Maybe SupportedCodecs?. if you add a codec.ContentSubtype method as proposed above, you will have symmetry in the client and server code paths.. Can we add a \nvar _ CallOption = EmptyCallOption{} to make sure this always satisfies the contract?. ",
    "soheilhy": "I just came across this issue. I think I have solution for this: cmux sniffs the connection payload and  multiplexes HTTP and gRPC connections transparently from the same listener similar to what @bradfitz mentioned earlier in the thread.\nI modified server.go as an example:\nhttps://gist.github.com/soheilhy/bb272c000f1987f17063\nTo test this version of server.go, please run:\n% go run client/client.go\n2015/07/31 10:49:36 Getting feature for point (409146138, -746188906)\n...\n% go run client/client.go -tls\n2015/07/31 10:49:39 Getting feature for point (409146138, -746188906)\n....\n% curl localhost:10000                                                                                                           \nhello http\n% curl -k https://localhost:10000\nhello http\nRight now, it can't serve HTTP2 content (i.e., won't work if you try https://127.0.0.1/ using Chrome) because https://github.com/bradfitz/http2 does not have h2 support. I'm waiting for @bradfitz to consider https://github.com/bradfitz/http2/issues/69. If that gets a greenlight, cmux can serve pretty much anything  along side gRPC.\nThanks for your time!\n. No, it sniffs the first header frame only, and after that it multiplexes the connection. The assumption is that one connection is either gRPC or otherwise HTTP. It my benchmarks there is no meaningful overhead for long-lived connections. If you have a specific benchmark scenario, I am happy to evaluate it.\n. That's right, but AFAICT cmux correctly handles this (as all gRPC examples I could find work with cmux). The SETTINGS frame is always followed by a HEADERS frame and as soon as cmux sees the header frame, the connection is all gRPC's. Please note that cmux is just sniffing the connection, so that when gRPC server starts reading from the connection it will read from the beginning of the stream: sees the SETTINGS frame and will send its ACK, etc.\n. Exactly, it buffers the frames until it sees the first HEADERS frames with a matching field or sees a HEADERS frame with an END_HEADERS flag. Thank you!\n. ",
    "peter-edge": ":)\n. Also of note, https://github.com/iamqizhao/glog can be dropped in as the default logger after this, if glog wraps the global functions in an interface.\n. I'll respond to these separately:\ni) It's not a DialOption or ServerOption really, we could make it some other type of option. On global - I agree, at the application level. The problem is that grpc-go is a library, not an application, and so there needs to be the ability to set logging separately. Go's http package, for example, does not lock in any type of logging.\nOne compromise that could work, although is not optimal at least in my opinion, is we could have a global method on grpc such as grpc.SetLogger, then at least a logger could be chosen. But again, my concerns about application vs. library tell me at least that this should be a non-global (I really dislike globals :) ).\nii) I'm not sure I follow the indirections - if we're worried about the function stack getting one level deeper, there's a ton of other places in the code that this needs to be optimized. ie I don't think this is a concern. I just added a commit that actually uses glog now as the default.\niii). This one is definitely different. Most people in the go community don't even seem to use golang's standard logger - docker, for example, uses logrus. Other log packages allow more control, especially in terms of output. For my application, I actually add semantics to all my logs so I can idiomatically read them back, and if I were to use grpc-go, where my only option is to send string logs to stderr, I would have to actually throw the logs away. grpc-go has many applications in the future, including in the docker package, and this limitation might really hurt adoption.\nIf there is an issue with another logger, this is on the user - if I choose to use a logger implementation that is not standard and there is a bug, that's my problem, not grpc's.\nOverall, I think the closest equivalent library would be net/http, and it does not lock in a logging implementation. Not having the option to use a different logger will definitely affect adoption of grpc-go IMO, and hurts at least my ability to use it :)\n. I did a rebase from master and I'm not sure what happened, something did not go right. Alternatively if we end up agreeing on something, I can just open up a new PR to not muck with the commit history.\n. The other issue with globals I forgot to mention is for testing - it really does limit what you can do with the log output during testing, you might actually want to see if certain things are logged without reading os.Stderr. In my case, I actually test for log events.\n. Alright, so, should I do the SetLogger/SetClientLogger? Happy to refactor this to do that. Thanks for all the quick replies, super appreciate it.\n. Of note, it would be nice if I could put this right in the grpc package, but this would lead to cyclic dependencies (transport <-> grpc, for one) - maybe a bigger overall structural issue we could address?\n. Go standards might actually dictate to name it log, but I agree that it's confusing. Naming it 'grpclog' is a decent compromose - rpclog doesn't have any corollaries in this package, we should stay consistent. I redid this.\n. Ya I really don't care, just need something soon. What you doing?\n. This was a quick and dirty one :)\n. Closing\n. @dsymonds this is probably a better PR to comment on. Is there any other documentation on this? I cannot seem to get tracing on my end.\n. Have you successfully done this? What is it doing? Because I tried this and got an error. Do you mean that on whatever process a gRPC client is running, an http server will also implicitly be run, because this seems bad?\nI've been using grpc-go for a while and I really have no idea how to use this feature, I really think some documentation/examples are needed.\n. Alright, sounds good. Ya I think this functionality is super cool, just had no idea how to use it. Thanks, and sorry if I sound grumpy, just waking up :)\n. @dsymonds is there any other documentation you can point me to? I'm not sure how to use this still\n. Ya, so in between my comment 30 minutes ago, and now, I figured it out https://github.com/pachyderm/pachyderm/commit/5960171ba3401a18cd2adc5a07a278ea0cd1cc9e, sorry for the ping.\nSome general questions:\n- What about optionally pushing tracing info somewhere, hopefully async?\n- What about being able to query this via gRPC itself somehow? Maybe similar to how grpc/health is being set up?\nI'm not sure if any of this is being proposed, net/trace is cool and all but it's nice to be able to query this separately/have the raw data. Are you two open to a discussion about it? :)\n. Also the core thing that caught me up was http.DefaultServeMux, I didn't realize this was where it was being registered (which might be more obvious to a lot of golang developers)\n. pprof was obvious to me from the start, I would say something as simple as adding \"use http.ListenAndServe or http.Serve with a nil hander (per the semantics of http.DefaultHttpHandler) to serve these on your server\" would be enough. And probably a top-level mention of AuthRequest, in the package comments instead of just on the variable.\nIn grpc-go, it would be really cool to just have some one-liners in the existing README about credentials, health, metrics (when that PR is merged), trace, etc, just to point people in the right direction.\n. @yangzhouhan is there an example of how to use tracing with grpc?\n. I don't want to test any of the grpc code, what my goal is is to be able to write tests against a client, that call a server implementation. This is especially hard with, for example, streaming calls. The method signature is significantly different on both sides.\nI want to write tests against the client, that call a server implementation, does this make sense? It's very similar to what people try to do with http libs.\n. That's what I'm doing now - it's fine. I was trying to make it one level nicer, but I can u sweats d the argument to not add more complication.\n. *understand the argument (phone typos)\n. Shameless plug: I have a mostly-complete structured logger using protocol buffers at https://go.pedge.io/lion, it's actually the third (and best!) iteration of the system, and allows you to read back logs into protocol buffers if they are serialized in that manner. And if using glog, it's fast. And can be faster. Thought you might be interested in the long term @aybabtme @iamqizhao \n. Ya protolog is meant to switch out the backing implementation, it just has the concept of events as protocol buffer messages. protolog allows the old-school WithField/WithFields as well, but having an event for every log entry is really nice. example: https://github.com/pachyderm/pachyderm/blob/master/src/pps/run/protolog.proto https://github.com/pachyderm/pachyderm/blob/master/src/pps/run/runner.go#L56\n. This is the Logger interface in lion https://github.com/peter-edge/lion-go/blob/master/lion.go\n. https://github.com/grpc/grpc-go/pull/523 @bradfitz \nProtolog has been renamed to lion\n. On ii) - I can rename it to glogger again, so there's no confusion. In general though, that glog package would only ever be imported once, probably in a main package, it shouldn't be referenced in general i.e.:\nimport _ \"go.edge.io/dlog/glog\" // should be the only time you have to import\nOn moving it into grpc - I'm generally fine with this, I just wanted to use dlog in the future for other libraries (mostly my own), so making it grpc-specific is no fun for that. But with that said, totally get the \"no external dependencies\", and since for my own stuff I just use protolog...eh why not, let's just move it in.\nI'll move dlog to grpc/grpclog, and add all the package comments/import etc, how does that sound? I'll probably just do a new PR so there's no wasted commits. If you give me the go (pun intended?), I'll write up this PR now.\n. I would say there's an argument to be made that grpc does too much logging in some places, but I think that's another concept that should be tackled - what this PR (and the equivalent one over on grpc-gateway) is trying to cover is that every library is choosing their own logger, and it makes end-user applications hard to coalesce all logging to a preferred system. Just my two cents.\n. @iamqizhao I just put the other PR together for your review since it only took a few minutes\n. @dsymonds I'm pretty sure you saw this since you're a collaborator but just in case, this was our original discussion re: grpclog: https://github.com/grpc/grpc-go/pull/190\nThe main thing is it's hard to capture grpc's logs in some systems (well I guess that's empirical evidence, but ya, the systems I've worked on :) ) because not everyone uses glog. So you end up either ignoring grpc's logs (the usual solution), which isn't great, whether grpc is logging too much or not, or you have to redirect glog into your own logging system.\n. I totally agree actually - I think when adding a long-term log message, it should be something actionable or meaningful. And on that line, totally happy to help grpc-go with this, because it is definitely a bit noisy.\nThis was sort of the motivation (plugging my own package) for go.pedge.io/protolog - it allows you to do generic logging, but the main use is to make a proto message for everything you log (log \"events\"), and you get somewhat of an artificial barrier to adding a new log message - emotionally, you think more about \"do I really want to log this/make a new log message for this\" before adding a log line. Small example (not using protolog much yet, but we're getting there): https://github.com/pachyderm/pachyderm/tree/master/src/pps/server\nBut ya, end of plug :)\n. For sure :)\n. Can we do this? :)\n. Might be a nice time to revisit the idea? :)\n. @ecnahc515 @mwitkow-io I'm not sure if this is relevant for your case, but something I'm using:\nhttps://github.com/peter-edge/go-proto/blob/master/rpclog/rpclog.go\nhttps://github.com/pachyderm/pachyderm/blob/master/src/pps/persist/log_api_server.go\nThought I'd link it in case you could use parts of it.\n. Me neither, but in the mean time until we can (maybe?? Hopefully?? :) ) get interceptors.\n. I'm happy to spend a few hours to put some proposal together next week\n. This seems really similar to https://github.com/grpc/grpc-go/pull/349 - can we have a bigger overall discussion about this?\n. heh\n. A quick thought: adding log level to contexts could be expensive, especially since a new context struct is created every time a value is added. Consider a debug message that is called in a tight loop - it is a lot cheaper to have the log library do:\ngo\nfunc Debug(whatever ...interface{}) {\n  if level > Level_DEBUG {\n    return\n  }\n  Print(Level_DEBUG, whatever...)\n}\nThan:\n``` go\nfunc Print(ctx context.Context, etc ...interface{}) {\n  ...\n}\ngrpclog.Print(context.WithValue(ctx, \"logLevel\", \"DEBUG\"), etc...)\n```\n. Ah, ok\n. I was, but it's in internal code, I have to figure out how to share it. It's not a secret, just need to go through my notes.. Re 606, saying this is expected behavior - is it? I think grpc should be able to handle errors with newlines, without the end user caring about the transport - can't it just be escaped? @iamqizhao \n. I could use a good grpc bug to get into, lemme see what I can do :) but tomorrow, I am on Europe time \n. +1 for GOOG :)\n. @bradfitz just did https://github.com/grpc/grpc-go/pull/609 which fixes the immediate issue, and at least allows newlines in error messages again (I'm getting this error in other repositories now too, no fun)\n. https://github.com/grpc/grpc-go/pull/610\n. More specifically, if you return this error from a server implementation for a unary call, then you get malformed http2:\ngo\nfunc (a *apiServer) Something(ctx context.Context, request *Request) (*Response, error) {\n  return nil, errors.New(`protoc -I/tmp/protoeasy-input933135460 -I/go/src/go.pedge.io/protoeasy/vendor/go.pedge.io/pb/proto --go_out=Mgoogle/api/annotations.proto=github.com/gengo/grpc-gateway/third_party/googleapis/google/api,Mgoogle/api/http.proto=github.com/gengo/grpc-gateway/third_party/googleapis/google/api,Mgoogle/api/label.proto=go.pedge.io/pb/go/google/api,Mgoogle/api/monitored_resource.proto=go.pedge.io/pb/go/google/api,Mgoogle/datastore/v1beta3/datastore.proto=go.pedge.io/pb/go/google/datastore/v1beta3,Mgoogle/datastore/v1beta3/entity.proto=go.pedge.io/pb/go/google/datastore/v1beta3,Mgoogle/datastore/v1beta3/query.proto=go.pedge.io/pb/go/google/datastore/v1beta3,Mgoogle/devtools/cloudtrace/v1/trace.proto=go.pedge.io/pb/go/google/devtools/cloudtrace/v1,Mgoogle/example/library/v1/library.proto=go.pedge.io/pb/go/google/example/library/v1,Mgoogle/iam/v1/iam_policy.proto=go.pedge.io/pb/go/google/iam/v1,Mgoogle/iam/v1/policy.proto=go.pedge.io/pb/go/google/iam/v1,Mgoogle/logging/type/http_request.proto=go.pedge.io/pb/go/google/logging/type,Mgoogle/logging/type/log_severity.proto=go.pedge.io/pb/go/google/logging/type,Mgoogle/logging/v2/log_entry.proto=go.pedge.io/pb/go/google/logging/v2,Mgoogle/logging/v2/logging.proto=go.pedge.io/pb/go/google/logging/v2,Mgoogle/logging/v2/logging_config.proto=go.pedge.io/pb/go/google/logging/v2,Mgoogle/logging/v2/logging_metrics.proto=go.pedge.io/pb/go/google/logging/v2,Mgoogle/longrunning/operations.proto=go.pedge.io/pb/go/google/longrunning,Mgoogle/protobuf/any.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/api.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/descriptor.proto=github.com/golang/protobuf/protoc-gen-go/descriptor,Mgoogle/protobuf/duration.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/empty.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/field_mask.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/source_context.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/struct.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/timestamp.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/type.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/protobuf/wrappers.proto=go.pedge.io/pb/go/google/protobuf,Mgoogle/pubsub/v1/pubsub.proto=go.pedge.io/pb/go/google/pubsub/v1,Mgoogle/pubsub/v1beta2/pubsub.proto=go.pedge.io/pb/go/google/pubsub/v1beta2,Mgoogle/rpc/code.proto=go.pedge.io/pb/go/google/rpc,Mgoogle/rpc/error_details.proto=go.pedge.io/pb/go/google/rpc,Mgoogle/rpc/status.proto=go.pedge.io/pb/go/google/rpc,Mgoogle/type/color.proto=go.pedge.io/pb/go/google/type,Mgoogle/type/date.proto=go.pedge.io/pb/go/google/type,Mgoogle/type/dayofweek.proto=go.pedge.io/pb/go/google/type,Mgoogle/type/latlng.proto=go.pedge.io/pb/go/google/type,Mgoogle/type/money.proto=go.pedge.io/pb/go/google/type,Mgoogle/type/timeofday.proto=go.pedge.io/pb/go/google/type,Mpb/common/common.proto=go.pedge.io/pb/go/pb/common,Mpb/geo/geo.gen.proto=go.pedge.io/pb/go/pb/geo,Mpb/geo/geo.proto=go.pedge.io/pb/go/pb/geo,Mpb/money/money.gen.proto=go.pedge.io/pb/go/pb/money,Mpb/money/money.proto=go.pedge.io/pb/go/pb/money,Mpb/net/net.proto=go.pedge.io/pb/go/pb/net,Mpb/phone/phone.proto=go.pedge.io/pb/go/pb/phone,Mpb/time/time.proto=go.pedge.io/pb/go/pb/time:/tmp/protoeasy-output472003187 /tmp/protoeasy-input933135460/tmp.proto: exit status 1\ntmp.proto:5:1: Expected \";\".`)\n}\n. I went through the error string and found out that if an error has a newline in it, it causes this error: https://github.com/peter-edge/grpcerr/commit/595207d8d635e2a1423ef33173a2590e6a61ffd2\nSo this errors:\ngo\nfunc (a *apiServer) Something(ctx context.Context, request *Request) (*Response, error) {\n  return nil, errors.New(\"1\\n2\")\n}\n. The error comes from https://github.com/grpc/grpc-go/blob/e5f60381dde4777aae49b7d678850c6fa25bbbc6/transport/http2_client.go#L756, I'm not a world expert on http2 (@bradfitz lol) so is there something that needs to be handled with newlines when sending from the server-side?\n. I commented over on 576 FYI, def like de duping :)\n. Just saw this - note that I just did https://github.com/grpc/grpc-go/pull/609, which allows errors with newlines (the original issue) - some combination of 608 and 609 would be good.\n. Of note, 608 and 609 don't really conflict, so they could be merged separately.\n. @bradfitz I'm gonna file an issue over on grpc/grpc once I do some investigation into grpc/grpc and grpc-java and I'll cc you when I get there. I'm context switching like a maniac today so let me pop some stuff off my stack first\n. Well shoot, it looks like the discussion is over, looks like they are doing this already in grpc-java https://github.com/grpc/grpc-java/pull/1517/files so I don't think it's worth it to discuss base64 vs percent encoding, right?\nSo if the decision has already effectively been made, is it fair for us to just:\n- Keep it as grpc-message\n- Percent encode/decode the message?\n. I can't say I'm the biggest fan of it, I'd personally prefer base64, but @bradfitz I would argue it's not worth going against the tide here right?\n. I did this https://github.com/grpc/grpc-go/pull/610\n. Of note, I don't like this, I wish it was a standard encoding, but see the comments on the other PR.\nThere's performance improvements that can be had, most obviously to pre-size the buffers, but this at least is a start.\n. https://github.com/grpc/grpc-go/issues/489\n. @dsymonds I would note it would be nice to at least get something in sooner than later, this is popping up in a lot of places, when I'm executing code in libraries that worked before, I'm getting PROTOCOL_ERROR a lot now.\n@iamqizhao maybe we re-open https://github.com/grpc/grpc-go/pull/610 just as a tracker?\nAlso note I think the same thing is affecting https://github.com/grpc/grpc-go/issues/613 at https://github.com/grpc/grpc-go/blob/5309b8314a606dba2e2e9c4f782073445eea1c5d/transport/http2_server.go#L450 re: https://tools.ietf.org/html/rfc7230#section-3.2.6\nOverall, ya I don't like this, but two things:\n1. It's a moderately time-sensitive bug, because it's causing a lot of trouble\n2. I do think the whole encoding discussion should be revisited - I don't think this is a great solution, and whether it's base64 or url encoding, something more standard would be good. I quit in 2013 so I obviously can't push this discussion internally - can one of you or @bradfitz? :)\n. I really don't know - this just started happening recently, we could look back through the commits\n. All comments should be addressed\n. This is in no way definitive, but I'd bet 10 bucks it started here https://github.com/grpc/grpc-go/commit/ac5c8d797284f225f2934c2e9fd5ad8aa23b13ef, which is a commit I think you'd want to keep, so probably better to fix after the fact, ie handle the real problem (ie a PR such as this/discuss encoding/etc).\n. Well shoot, this might be related to https://github.com/grpc/grpc-go/issues/576\n. I assume the discussion on chaining interceptors is officially over? :)\n. It would be just as easy to allow multiple interceptors, and if there are >1 interceptors when you parse the options for the server, combine all the interceptors into one with a wrapper interceptors that calls all the given interceptors in order, this satisfies everyone\n. Why can't we do it here if it's so super super easy? It will mean more fragmentation if we don't \n. +cc other people discussing interceptors @maniksurtani @nictuku @zellyn @bradfitz \n. Hey @chancez, I think this would just be exposing newMultiUnaryServerInterceptor and newMultiStreamServerInterceptor as public functions, right? Am I missing something?\nThat is another way to do it, but I would argue it would be better to embed this directly as part of the library - there's no cost to doing so (if you specify one interceptor only, the code path is the same as before), and it prevents fragmentation in regards to what end users do. I think a lot of people will expect to be able to specify multiple interceptors.\n. ping, probably specifically @iamqizhao , any thoughts?\n. One repo across all languages would be a different pattern right? It's going to be not as fun with go get and go build/test commands as it is right now with grpc-go.\nYes, I have a lot to add, heh.\n. What about splitting it as it is now, i.e. grpc-contrib/grpc-contrib-java/grpc-contrib-go?\n. I'd like to put something together for this if there is still interest. We're running into wanting this in https://github.com/yarpc/yarpc-go/issues/911 where an exposed transport (such as grpc) needs to handle multiple inbound encodings, and our choices are to do grpc at a lower level (not use large portions of grpc-go) or to get this added. We want to add as few custom headers as possible for our usage of grpc.. I've put together a PR for this, we're discussing it there as of now, this seems like a grpc-go specific issue - grpc as a protocol supports multiple encodings, we just should implement it I think.. Can we make this more \"standard\" with regards to popular logging packages, ie github.com/Sirupsen/logrus and github.com/uber-go-zap? Things in particular:\n\nRename Warning to Warn\nRemove the V function\n\nI don't think https://github.com/golang/glog was ever meant to be this widely used, and it would be nicer for this to be more in line with existing loggers.\nAlso to note: do we have to support both Foo and Fooln? They generally end up doing the same, but this is an arguable point.. I can confirm this is very breaking, I had to hunt down this PR using git blame unfortunately :(. I would definitely support this.. I've added an integration test for this. This should be ready for review now.. I responded to the inline comments. The short is that codec.String() is already effectively codec.ContentSubtype(), and the goal with the client-side CallContentSubtype function is to be able to decouple the codec from the subtype, ie to allow passthrough codecs while specifying the subtype outside of grpc-go. It would have been preferable way back in the day to decouple encoding and transport, but alas :) this was hard to forsee at the time.\nNote grpc explicitly uses content-type to specify the encoding http://www.grpc.io/docs/guides/wire.html. I can do this, but just to be clear - do you want a global variable for codec registration? I don't think I understand the proposed approach, but I doubt this is it. If you can give me a fuller explanation of what you'd like to see, I'd be happy to work on it.\nAdditionally, we still need the ability to overwrite the content subtype regardless of codec, are you ok with this being in the submission?. @dfawley ping :). @dfawley would you have some time to video chat?\nI'd love to explain the thinking here - the short is that the Codec API isn't expressive enough for all situations, and grpc-go has a tight coupling between content-type and transport that some users need to get around. We use a passthrough codec ([]byte in, *[]byte out) and need to set the content-type manually, and we could not use a global map either. I know @mwitkow also has code that relies on using passthrough codecs (the proxy code) but not sure if he needs access to content-type.. Ah ok. Well regardless, I think @mwitkow's use case and my use case shows there is a desire to bypass the Codec completely, and I can argue it's not expressive enough - for YARPC for example, we do encoding completely separate from the transport.. Hey,\nThanks for the detailed response. It took me a minute to understand how we could use it given the error returned if the content subtype is not present in the Codec map, but I think I see what you're saying - if you specify both content subtype AND custom codec, then it will use that codec blindly, correct?\nTwo last questions:\n\nCould I also add a WithContentSubtype DialOption, that calls WithDefaultCallOptions(CallContentSubtype(...))? I don't need this, as I can just do the latter, but it would be nice.\nShould we have grpc-go always send content subtype now, ie right now just application/grpc is being sent in the default case, should I change this to application/grpc+proto? Re: compatibility - gRPC libraries should handle this per the wire format specification, but who knows. I would obviously prefer the default change, but that is a selfish answer.\n\nOne other thing (and this is kind of separate from this PR, so we can have this discussion somewhere else) - would you be open to having a function func WithDefaultMetadata(md metadata.MD) DialOption { ... }? This would allow me to set the custom headers we need once, as opposed to on every call. I think there is a use case for static metadata  - we need more than just content subtype, we have required headers rpc-service, rpc-caller for our case. I can do this as a separate PR, let me know.. OK great! I will get on this and hopefully have something for you tomorrow.. Got held up on a few other things, will get this out soon, apologies for delay.. Hey,\nI can work on this soon, next couple weeks? Is that OK?. I've put together two branches to make this more easily reproducible.\nSetup steps:\ngo get -u go.uber.org/yarpc\ncd \"${GOPATH}/src/go.uber.org/yarpc\" # assuming your GOPATH has one directory\nI pushed a branch grpc-metadata-bug-pre that just adds log statements to show the metadata before grpc.Invoke, and then inside the handler.\nTo run:\ngit checkout grpc-metadata-bug-pre\nSUPPRESS_DOCKER=1 make -C internal/examples protobuf-grpc\nOutput:\n$ SUPPRESS_DOCKER=1 make -C internal/examples protobuf-grpc\ngo install ../service-test\ngo build -o protobuf/protobuf protobuf/main.go\nTRANSPORT=grpc service-test  --dir protobuf\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1] user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1] user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1] user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1] user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf]]\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1] user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example] :authority:[127.0.0.1]]\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1] user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client]]\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1] user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:11:33 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1] user-agent:[yarpc-go/1.9.0-dev] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client]]\nI pushed another branch grpc-metadata-bug that has the same log statements, but also does glide update, which pulls in 1.3.0.\nTo run:\ngit checkout grpc-metadata-bug\nSUPPRESS_DOCKER=1 make -C internal/examples protobuf-grpc\nOutput:\n$ SUPPRESS_DOCKER=1 make -C internal/examples protobuf-grpc\ngo install ../service-test\ngo build -o protobuf/protobuf protobuf/main.go\nTRANSPORT=grpc service-test  --dir protobuf\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1:57527] user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0]]\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1:57527] user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0]]\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1:57527] user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0]]\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1:57527] user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0]]\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1:57527] user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0]]\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1:57527] user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0]]\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf] yarpc-grpc-reserved-service:[example]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1:57527] user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0]]\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[:authority:[127.0.0.1:57527] user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0]]\n2017/05/04 15:08:34 HERE IS THE OUTGOING METADATA THAT IS ADDED TO CONTEXT USING metadata.NewOutgoingContext: map[yarpc-grpc-reserved-service:[example] yarpc-grpc-reserved-caller:[example-client] yarpc-grpc-reserved-encoding:[protobuf]]\n2017/05/04 15:08:34 HERE IS THE INCOMING METADATA FROM metadata.FromIncomingContext: map[user-agent:[yarpc-go/1.9.0-dev grpc-go/1.3.0] :authority:[127.0.0.1:57527]]. I think I have begun to identify the problem - if NewOutgoingContext is called twice before serialization in http2_client.go, you end up with an outgoing context along the lines of:\ncontext.Background.WithDeadline(2017-05-04 15:54:14.289231314 +0200 CEST [999.772844ms]).WithValue(metadata.mdOutgoingKey{}, metadata.MD{\"yarpc-grpc-reserved-caller\":[]string{\"example-client\"}, \"yarpc-grpc-reserved-encoding\":[]string{\"protobuf\"}, \"yarpc-grpc-reserved-service\":[]string{\"example\"}}).WithValue(metadata.mdOutgoingKey{}, metadata.MD{}).WithValue(grpc.rpcInfoContextKey{}, &grpc.rpcInfo{bytesSent:false, bytesReceived:false}).WithValue(peer.peerKey{}, &peer.Peer{Addr:(*net.TCPAddr)(0xc420360300), AuthInfo:credentials.AuthInfo(nil)})\nNote the two WithValue calls with mdOutgoingKey{}, with the second key having nothing for a value. metadata.NewOutgoingContext is called both in our code, and in github.com/grpc-ecosystem/grpc-opentracing/go/otgrpc/client.go:65, ie we use github.com/grpc-ecosystem/grpc-opentracing. grpc-opentracing is not adding anything, but when metadata.FromOutgoingContext is called in http2_client.go for serialization, the second (empty) value is being returned, not the earlier value that we did in our own code that set yarpc-grpc-reserved-encoding etc.\nIn the old version pre-1.3.0, there are still two WithValue calls, but the metadata is copied to the second call, ie calling metadata.NewContext twice properly propagated the information:\ncontext.Background.WithDeadline(2017-05-04 16:02:25.348512784 +0200 CEST [999.435702ms]).WithValue(metadata.mdKey{}, metadata.MD{\"yarpc-grpc-reserved-caller\":[]string{\"example-client\"}, \"yarpc-grpc-reserved-encoding\":[]string{\"protobuf\"}, \"yarpc-grpc-reserved-service\":[]string{\"example\"}}).WithValue(metadata.mdKey{}, metadata.MD{\"yarpc-grpc-reserved-encoding\":[]string{\"protobuf\"}, \"yarpc-grpc-reserved-service\":[]string{\"example\"}, \"yarpc-grpc-reserved-caller\":[]string{\"example-client\"}}).WithValue(peer.peerKey{}, &peer.Peer{Addr:(*net.TCPAddr)(0xc4203ee030), AuthInfo:credentials.AuthInfo(nil)})\nNote this is printed from adding log.Printf(\"%+v\", ctx) right before the call to metadata.FromOutgoingContext (or metadata.FromContext in the old code) in http2_client.go around line 430.. Well and there we go: it's because grpc-opentracing needs to be updated, so this was an in-memory issue: https://github.com/grpc-ecosystem/grpc-opentracing/blob/master/go/otgrpc/client.go#L53\ngrpc-ecosystem repos should be updated to reflect grpc-go changes, can we do this?\nThis is going to be problematic (and why this breaking metadata change is really tough on consumers) because grpc-opentracing must update to the new metadata API, however this means that when users update grpc-opentracing, they must also update grpc-go (which may have other changes they do not want). grpc-opentracing does not have any releases/doesn't use semvar, so this is even tougher. I think as part of this, it would be great if we could get grpc-opentracing on semvar too?\nI hope this issue helps, let me know if there's anything I can do to assist.. Yea I get it, just wanted to alert. Let me know if I can help in any way, I've tried to go through a few of the grpc-ecosystem repositories and send PRs.. All comments addressed. Sgtm, yea I dropped the ball on this too, my bad. I trust you, it's your repo, go for it :-). I completed The Linux Foundation CLA, I don't know if that check can be run again but it should be good to go.. Fixed\n. grpcMessage is there to match that this is meant to be specifically for the grpc-message field - calling this percentEncode could actually be more confusing as when I hear that, I think URL encoding etc...this is something super custom (which again I don't like, but that's what it is).\n. It's for errcheck, and a style that I like - by doing a blank assignment, I'm explicitly saying \"I know there is an error returned here, and I'm purposefully ignoring it, this is not a bug\". It tells future contributors that this is not an issue that should be checked, that we are extremely confident this error will not happen and that checking it would be not worthwhile and get in the way of cleaner code (ie no returned error from this function). Does that make sense?\n. ya, this is more OCD than anything heh :)\n. Ya I was gonna ask, which was more important (the allocation or doing two passes) - will do.\n. So much work! :) Will do.\n. I will if it's a universal thing, but see my last comment on this for @menghanl , will just copy/paste it here:\nIt's for errcheck, and a style that I like - by doing a blank assignment, I'm explicitly saying \"I know there is an error returned here, and I'm purposefully ignoring it, this is not a bug\". It tells future contributors that this is not an issue that should be checked, that we are extremely confident this error will not happen and that checking it would be not worthwhile and get in the way of cleaner code (ie no returned error from this function). Does that make sense?\n. fixed\n. fixed\n. fixed\n. fixed\n. fixed\n. Ya, see TODO and other comment about this. What do you mean? Currently, it's just \"application/grpc\", now this ends up either being that, or with subtypes.. I'm going to keep this on server setup (which is a one-time operation), but not on the read. HTTP headers are case-insensitive, but it seems like HTTP/2 transmits them as lowercase https://evertpot.com/http-2-finalized. I was keeping these separate for now - you want some concept of a default codec anyways as this is what is currently implemented, and you might as well not do a map lookup.. I think this should stay as \"application/grpc\" by default, as this is the existing behavior. If we change this, this could have unintended consequences for existing users, even if this means some grpc implementations are not handling the Content-Type header properly.. Lowercasing every time is expensive, and I don't know if it will matter in general.. Ya I don't know, I think we should move this to the full version. I am worried too about breaking other existing libraries, but they would have to incorrectly handle content-type to break (which they may be doing unfortunately). Adding the subtype gives more information, and we need it for example for yarpc.. I don't understand the concern - this is making it so the subtype does not rely on the codec. This is useful for situations where you might have a passthrough codec (as you do in your proxy code, or as we have in yarpc-go right now) where you do the encoding outside of grpc-go, but still want to set the subtype (ie what we need to do). grpc-go tightly ties the encoding to the transport, and there's no real way to decouple these at this point in terms of the grpc-go code.. Still don't understand - trying to make it so that codec can be not used effectively (and just have a passthrough) - codec.String() is effectively used as subtype right now as it is, so I wouldn't want to add another function that also does this.\nThe goal is to give the ability for the client to specify the subtype without relying on codec.String(), ie not relying on the codec at all, which is needed when the encoding and transport are decoupled (grpc-go relies on the marshaling/unmarshaling being done internally using just the Codec interface).. I wanted to keep the naming similar.. In terms of explicitly specifying the codec - don't understand either, other than breaking existing clients that don't properly implement the grpc wire protocol (which I mentioned as a concern). Right now, proto is the subtype specified even though users may not realize it (since grpc-go at least makes protoCodec the default codec). This is just specifying that over the wire.. See comments on wire protocol - right now, grpc-go effectively is always saying that proto is the default subtype.. This is just using the same variable name as validContentType, which is similar.. This is specifically the codec registered for \"proto\", not really the default codec - the name \"default\" is a little overridden in this repo, and I want the logic in setCallInfoContentSubtypeAndCodec to know this is specifically a \"proto\" codec. I think this should stay as is, does that sound reasonable?. See the comment on the defaultCodec stuff - we define the codec to be the proto codec if nothing is set, and if you want to override this, you can do it with the options anyways (for the server you use CustomCodec for example). I need more direction here - will google.golang.org/grpc depend on google.golang.org/grpc/encoding? Should I add another public method GetRegisteredCodec(contentSubtype string) Codec to encoding? Also maybe GetRegisteredProtoCodec() Codec which must return nil?\nIf Codec is still in google.golang.org/grpc, this will result in a cyclical dependency, as I need access to the registered codecs there, but encoding will rely on the base package.. Done. errEmptyCodecString how about that?. Done. Done. I think we should explicitly error - if you use the CallContentSubtype option, passing an empty string ie CallContentSubtype(\"\") should be a bad input, defaulting to \"application/grpc\" seems like undefined behavior to me, right?. Let's chat offline perhaps? I feel moderately strongly we should be erroring if we use an empty string to this option or the other option.. Same discussion regarding default codec - there's already ways to set default codecs, this way is also backwards-compatible in assuming no codec means the proto codec.. Done. Done. I added some clarification. Ugh do I really have to do this? It makes other things confusing IMO :-) I can if you want, no worries. Same comment. Sure but this is not the purpose of this variable, see the logic in setCallInfoContentSubtypeAndCodec (I think you saw this below).. This is done, however i did not s/Codec/encoding.Codec/ in grpc. I think this should be left to another PR as if this causes problems, it should be independently roll-backable.\nNote it's your repository so this is up to you :-) but I am very against not following SemVer and breaking the API between major releases, which moving Codec from grpc to a new encoding package is. This has caused us and others problems in the past.. ",
    "mattn": "http://play.golang.org/p/ENZjynLjV_\nAh, right. protobuf's textWriter seems to avoid to print non-ASCII as @dsymonds says. Should be used unicode.IsPrint instead?\n. fair enoguh.\n. goveralls run go test. is this same on grpc-go?\n. then do goveralls google.golang.org/grpc/...\n. go get before execute goveralls.\nlike Makefile is doing as testdep https://github.com/grpc/grpc-go/blob/master/Makefile#L40-L41\n. could you please try with -v?\n. ",
    "jpfuentes2": "\nsince the rpc is synchronous/blocking\n\nThat's what I got from looking at the route_guide example: the server's end points access a slice without locks. Is this considered quasi similar to an actor model? Is there documentation about this fact (I might have missed it)?\n. > There is no lock for that because it is read-only once it is initialized\nGot it, thanks.\n. ",
    "trevorgray": "I also wasn't able to find this documented anywhere but this issue. Would you be able to add this to the godoc documentation to make it more clear that it's safe to call a client from multiple go routines concurrently?\n. ",
    "timbunce": "Trying to summarize for myself... \n@rubenv asks:\n\nCan I use a client from different threads in parallel?\n\n@iamqizhao replies:\n\nOn client, if you want to perform multiple rpc in parallel, you should spawn multiple goroutines to do that since the rpc is synchronous/blocking\n\nThat's helpful, thanks, but doesn't quite clarify if a single client object can be used by all those \"multiple goroutines\". It sounds like the answer is yes. This example suggests the same. Is that right?\nHowever, a stream can't be shared.\n@trevorgray, these kinds of concurrency topics are apparently still not documented, per #682.. > You do not need to make a new connection for every RPC. A connection can be multiplexed by multiple clients or RPCs.\nI'm unclear exactly what's meant by \"multiplexed\" here. Using a pool to avoid concurrent use?\nWhere is this covered in the documentation? Perhaps I'm missing it.\nI've not been able to find docs that address sharing of connections, or clients, between goroutines.\n\ngrpc.io email list would be the better place for these questions.\n\n(In my projects I've found it helpful to view questions in issues, and on the mailing list, as implicit bug reports on the documentation. That's reinforced by trying to answer questions with links to useful docs. If I can't then I'd create a PR for a doc change that would answer the question and reply with a link to that.). grpc-go-pool is a gRPC pool implementation. As a novice I look at it and think that's maybe what I want. But I also have a nagging doubt that it's not needed at all.. > We currently don't support concurrent writers for stream.\nWhere is this documented? (See also #682). I was just at GopherCon '17 and saw Alan Shreve's \"grpc: From Tutorial to Production\" talk along with a few hundred other developers. It was a very good talk with a nice structure and flow that covered the topic well.\nExcept for one thing... Here's a version of his slides. Notice that the server-side cache as no concurrency protection:\ngo\ntype CacheService struct {\n    store map[string][]byte\n}\nfunc (s *CacheService) Get(ctx context.Context, req *rpc.GetReq) (*rpc.GetResp,\n    error) {\n    val := s.store[req.Key]\n    return &rpc.GetResp{Val: val}, nil\n}\nfunc (s *CacheService) Store(ctx context.Context, req *rpc.StoreReq) (*rpc.StoreResp, error) {\n    s.store[req.Key] = req.Val\n    return &rpc.StoreResp{}, nil\n}\nThere was no mention of concurrency safety in the talk. (There wasn't time for questions at the end so I couldn't query that at the time.)\nAm I right in thinking that that code is unsafe, or am I missing something?\n/cc @inconshreveable\n. ",
    "dfawley": "\nThat's helpful, thanks, but doesn't quite clarify if a single client object can be used by all those \"multiple goroutines\". It sounds like the answer is yes. This example suggests the same. Is that right?\nHowever, a stream can't be shared.\n\nThat all sounds right to me.\n\n@trevorgray, these kinds of concurrency topics are apparently still not documented, per #682.\n\nDocumentation is a well-known deficiency that I hope to address sometime this quarter.\n. Yes, compression is supported now.. This error is unrelated to the original issue created >2 years ago.  We would prefer not to have one issue for \"encountered an error running go get\".  Please file a new issue if you're still having trouble, and make sure to provide as much information about your environment as you can.\n. @pvox, this is the grpc-go repo, but it looks like this is about java.\nIf you're looking for help, you may want to start by asking your question in https://groups.google.com/forum/#!forum/grpc-io.  The gRPC team monitors that list.. We're open to this idea, but we would like to see a design proposal first (either as a grfc or a write-up in this bug), and would need a contribution to implement it as well.\n. Closing stale PR.. Thanks for helping, @irfansharif.  @kissgyorgy, in the future, please create new issues (https://github.com/grpc/grpc-go/issues/new) unless the symptoms you see are closely related to the original issue.  Thanks.\n. See also #1229 (esp. this comment and below), which is about Send() deadlines but the same concepts/techniques apply.. @mangrep can you be more specific about exactly what command(s) you are running and what error(s) you are getting?. @mangrep this is the Go repo, not Java.  Can you file an issue asking for help there?  https://github.com/grpc/grpc-java/issues/new. @shuaichang is the current behavior causing problems for you?  IIUC, this is a pretty minor issue in that it would allow the server to more quickly clean up when a client disconnects, but is not a high priority issue.\nMaybe you're more interested in #1104?. @shuaichang thanks for the explanation.  That feature request is #1104, not this issue.  This issue is about sending an HTTP2 GOAWAY frame before closing a connection, regardless of whether we allow existing streams on that connection to continue.\n. Proposal:\nReturn a gRPC status error (code: Internal) from grpc.Invoke or ServerStream's SetHeader, SendHeader, and SetTrailer methods before transmitting any additional data if the metadata in the request is malformed.\nThis avoids changing the API and doesn't require a new validation function that has to be called manually.  Note that anyone relying upon sending metadata forbidden by the spec but accepted by the current version of grpc-go would be broken by this change.\n. That PR was not merged.. A different sample is necessary with the recentish changes to status.\n(And now with the helpers added in #1358.). @enocom,\n\nIs that the gist of the issue here?\n\nCorrect!  Trailers are still useful.  However, for detailed error information, status.WithDetails should be used instead.  (Trailers can be used to transmit additional data at the end of an RPC even on success.). Error details may now be transmitted using status.FromProto instead of trailer metadata, and received via status.FromError().Proto().Details.  An example for this could be added to the route_guide example, possibly by returning an error instead of an unnamed feature when nothing is found here: https://github.com/grpc/grpc-go/blob/master/examples/route_guide/server/server.go#L82.\n. On the wire, it's proto-serialized and sent with the trailer metadata as \"grpc-status-details-bin\".  It's not available through the metadata handed back to the user, however -- it's returned to the client in the error. \n I believe support for this is available in all languages now.\n. It's a google.rpc.Status proto:\nhttps://github.com/grpc/grpc-go/blob/master/status/status.go#L131\nhttps://github.com/googleapis/googleapis/blob/master/google/rpc/status.proto#L80\nThe C and Java repos have also implemented this:\nhttps://github.com/grpc/grpc/blob/master/include/grpc%2B%2B/impl/codegen/call.h#L67\nhttps://github.com/grpc/grpc-java/blob/master/protobuf/src/main/java/io/grpc/protobuf/StatusProto.java:\nI believe their implementations do not create or parse that field as a proto by default, to avoid the proto dependency.  They have libraries to create/parse the field instead.  But in Go, grpc already has a dependency on proto, and status is generally handled differently, so it made more sense to implement it through the errors returned by the library.\n. This was fixed under #1148. We discussed this today at some length.  Without major API changes or hacks (i.e. passing a magic wrapper type to stream.SendMsg), there isn't much we can do here.  It does sound like a good idea, though, so I'll leave it open.\n. A few thoughts:\n\nThis function must return an RPC error (now a status error).\nThe transport package isn't intended to be used outside of grpc.  We'd want to send back a different grpc.ErrConnClosing or equivalent instead.  Looks like we already have such a thing, but it isn't a status error (oops):\n\nhttps://github.com/grpc/grpc-go/blob/c5c761dbca07f4d0efafd3f430a40dcdf1c1163f/clientconn.go#L41\nHow about if we make that a status error instead (status.Error(status.Unavailable, \"grpc: the client connection is closing\")) and return it?\n. Fixed by #1521.. Closing stale PR.. @bradfitz if you are still interested in fixing this, we'd love to have it.  Thanks!. #1297 is a newer attempt to fix this.. This would require another API change, so let's push this off until v2 (which is not currently planned).. Note: this is important for retries, which will consider an RPC committed if it receives headers separately from trailers in error cases.\n. @peter-edge did you say you were able to accomplish this?  Is there anything you could share about it?. Also: https://github.com/golang/go/issues/14141. This should be fixed by #610. Lowering priority to P2.  Including the trailer field names in the header is a \"SHOULD\" in the HTTP/1.1 RFC (ref).  Proxies are wrong to rely upon this optional behavior.\nNote that if a proxy only forwards the trailer fields named in the headers, then that will be a big and hard-to-debug problem.  We can't pre-announce all of the trailers we will be sending -- we let users specify trailers during streaming RPCs, and we conditionally send some gRPC trailers (e.g. error details).. Actually, that RFC section says the \"Trailer\" header field should be set in chunked encodings.  gRPC does not use chunked encoding, so I don't think that section even applies to us.\n. We can't remove this dependency without breaking backward compatibility, so we have to wait until v2 (for which we have no plans at this time).. @KeKe-Li did you go get -u google.golang.org/grpc?\nIf so, try to repro with a clean GOPATH.  The build is green, so there must be something wrong with your environment.. I agree, but unfortunately we won't have any time to spend on this for at least another quarter.  It will be a pretty sizable effort.  It's possible we may get some contributions for this before then, but I wouldn't want to set any expectations at this point.\n. @mwitkow, @tamird, @glerchundi, I have been poking at this periodically and learning more about it since June.\nThe goal of throwing away the grpc http2 server and using x/net/http2 instead would be a major undertaking, and would require adding significant new features and hooks and (likely) optimizations into x/net/http2 to reach parity with the existing grpc http2 server.  The handler-based transport was/is an experiment (and is documented as such), and isn't something fully supported.  The core grpc-go team does not currently have the cycles to support this and keep up with our existing commitments -- let alone to completely replace our default transport with it -- but we would be happy to review PRs that improve features or performance of the handler-based transport.\nRe: @bradfitz's comment:\n\nIt's not surprising that ServeHTTP is slower considering it's shoehorned into the existing\nServerTransport interface somewhat awkwardly.\n\nI agree this is an awkward fit: the handler-based transport is mimicking a full transport for each RPC.  As we continue to dig in and clean up some of the code and interfaces between the grpc layer and the transport layer (e.g.s #1745, #1854), it could happen that the handler-based transport becomes a more natural fit, and performance may be improved as a result if that is a factor in its performance differences.\n. This approach does not work, as I found in PR #1238.  I will fix this with my updates to that PR.. This issue came up in discussions about issue #976.\nIn light of that issue, we've been re-evaluating the way DialContext establishes connections with targets returned by the balancer.   We would like to unify the approach for this with the ongoing balancer watcher, rather than having two separate methods of connecting.  PR #1112 implements this, but one notable side effect is that we will no longer return a specific connection error from a blocking DialContext -- only a timeout from the context would occur.  We believe this is reasonable behavior and that returning connection errors from Dial is problematic for two reasons: 1. ongoing balancer updates could result in new connection failures, of which the client is never appraised, and 2. it is difficult to reasonably define how Dial should behave in the event of multiple errors, or with both errors and successes.  The current behavior fails immediately upon any error connecting to any target; we are sure this is undesirable -- other targets could succeed, or may already have.  With PR #1112, we will unblock after the first successful connection is made or the timeout elapses.  The behavior of Dial to a single target server, not via a balancer, is unchanged.\nPlease update this issue if you have concerns about this change.\n. Closing stale PR.. @kyleconroy That's exactly why it was considered stale.  It was significantly old and would not merge cleanly.  The team is spending some time each week to go through old PRs in an effort to get the backlog either merged or closed.\nIn the case of this PR specifically, I'm not really sure why the change was requested in the first place; it appears to be attempting to show off a feature of proto, not grpc, so the team agreed that it wasn't a required change.  We do appreciate your effort, regardless.  Please bear with us while we try to get the open PRs and issues whittled down appropriately over the next few weeks.. ClientConns can safely be accessed concurrently, and RPCs will be sent in parallel.\nThe only reason I can think of to use a pool of grpc clients to the same backend(s) is if you're running into stream limits or per-connection throughput limits, possibly imposed by a proxy outside your control.. gRPC implements http2 discovery using external means as described in Section 3.4 of the HTTP/2 RFC, and not the version identification method described in sections 3.1-3.3.  I.e. you tell a gRPC client to connect to a known-gRPC server and they speak the http2 wire protocol from there.  TLS is off by default, but can be enabled with some options.\nThis is not true if you are using the Go http.Handler server implementation instead of grpc-go's server.  In this case, TLS would need to be enabled, because h2c isn't supported by x/net/http2.\n. > So I should create a single connection when a client process bootstraps and close it when the process terminates? Could anything environmental 'break' a connection such that I would need to detect and re-connect?\nTechnically, a ClientConn (inappropriately named IMO) can consistent of many connections that come and go for various reasons.  However, the ClientConn should manage the connections itself, so if a connection is broken, it will reconnect automatically.  And if you have multiple backends, it's possible to connect to multiple of them and load balance between them.  You can learn a lot more about this at \nhttps://github.com/grpc/grpc/blob/master/doc/load-balancing.md and\nhttps://github.com/grpc/proposal/pull/30\n. We recommend disabling tracing in gRPC (grpc.EnableTracing=false) and using the stats handlers to record this data instead.  We would like to implement a library that provides integration with x/net/trace via the stats interface, but don't have the bandwidth to do this any time soon.\n. After more deliberation, we decided that truncating the messages should be easy and shouldn't be too controversial, so we will just do that for now and file a separate issue to track the implementation of tracing through the stats API.. ...and after the first attempt in #1508, we realized it's very difficult to fix the memory usage problem.  To truncate the message before storing it, we must first (s)print it -- but in most cases we'll never display the message, which is why the string method is evaluated lazily.  The best I believe we can do without changing what we render in the tracing (e.g. omit the message contents) is to truncate the message when displaying it to protect the browser when it renders them.  In theory we could truncate the binary-serialized message when storing it, but I don't believe the Go protobuf library handles truncated messages, so we wouldn't be able to render it in text form later if we did that.\nWe'll also be disabling tracing by default to prevent this problem from impacting users that don't actually need tracing.\n1509\n. That's encouraging.  So maybe the tracing implementation that is based on the stats handler interface can do something a little smarter.  #1510. > Why clutter the code with type aliases when we can just drop support for Go < 1.7? Anybody stuck on Go 1.5 or 1.6 can vendor the current grpc version.\nIt's not that simple.  Changing the types before Go 1.9 is the oldest supported version breaks backward compatibility with previously-generated pb.go files, which is something we have heard is a priority to maintain.. > Recompiling\u00a0pb.go\u00a0files doesn't sound like a crippling problem. I think we should just rip off the band-aid.\nIt's a harder problem when you're pulling in 10s of external projects, all of which check in their own generated pb.go files without go:generate tags.. If you are on Go 1.9, is this even an issue for you?  If so, please explain why.\nOtherwise, I believe this should only be changed when we drop support for Go 1.8 - i.e. all grpc-go supported versions have type aliases.  If we change it before then, we will break interactions between old pb.go files and grpc-go.\n. AIUI x/net/context for Go1.9 builds is a type alias to context.  So if you (or anything else) import x/net/context everything should \"just work\".\n. (See: https://github.com/golang/net/blob/master/context/go19.go). Sigh.  My understanding is GAE is supposed to get 1.9 much faster than it took to get the 1.8 release.\nI would love to get rid of the x/net/context import, but we have to balance this inconvenience with breaking everyone using existing .pb.go files.\nFor people stuck using older versions of Go, 1.8 introduced a fix feature for context.  go tool fix -force context <src path> should update everything quickly.\n. FYI, support for Go 1.6-1.8 will be removed on or after November 1st, and \"context\" will be imported directly at that time.  This will be safe because \"x/net/context\" is a type alias in 1.9 and later.. @tamird, I like the idea of this, but I don't like that it will block unrelated PRs until the files are regenerated.\nWe also run our tests nightly on head via a cron job.  Would you be able to update this PR to check for that case and only run the tests then?\nhttps://docs.travis-ci.com/user/cron-jobs/#Detecting-Builds-Triggered-by-Cron\nThanks!. @tamird, any thoughts on my comments?. @tamird ping, any updates?. We will implement this soon with the same approach for the travis checks, but using go generate in make proto instead.  Thanks for the PR.. https://godoc.org/google.golang.org/grpc#Stream\nThis is a bit difficult to document, because the user typically works with the generated code that wraps this, and doesn't produce godocs.  I hope to add some basic documentation in the next 2-3 months to cover this and other FAQs.. I did a good amount of refactoring with #1533 to help improve context propagation and cancellation.  If there's anything more of concern here, please let us know.. > Could it be that there was an issue with the automatic adjustment of the window in the older version\n\n(the setting has initial in its name for a reason I presume)?\n\nWithout #1073 (now #1248), there is no automatic window size adjustment in grpc-go.  The \"initial\" name was chosen in anticipation of this new feature.  So you will probably want to either use that feature or wait for #1248 to land soon.\nWindow sizes are now configurable with #1210, so I think we can close this and continue any other network performance discussions in other issues.. Thanks for the fix.  I have a slightly different version that also fixes a new instance since this PR was submitted.  #1213. Unless this is required for something new, we have no plans to implement this.. Sorry for the delay in response.\nFor now, this is not possible given the current API.  However, we will be investigating changing this as part of #1455 -- one idea is to see if the codec implements \"Free([]byte)\", and if so, call that when we are done with the buffer it returned.\n. As of now, the only way to do something like this would be to use a stats handler (unintuitively), which can tag a connection's context.  This context is propagated to all incoming calls, and data could be shared this way.\nThis is not a unique request, but we generally recommend against keeping state on the server at a connection-level.  The same client could connect to multiple servers or the same server multiple times, which could lead to races or data loss if you aren't careful.\nFeel free to follow-up here if you have further questions.. Sorry for the long delay here.  The stream's Send and Recv methods should return an error when there is a connection problem.  At that point you should be able to do your cleanup/reconnect.  If you still have any questions/comments about this, please let me know and I'll try to answer them.  Thanks!\n. FWIW, retries and hedging are coming to gRPC-Go natively in a month or two.  Relevant gRFC.\nRegarding the initial issue, we have since created the bufconn package to at least bypass the network stack (message are still [de]serialized and everything goes through the http2 transport, but this should help with overhead somewhat).  Otherwise, the team still doesn't have the bandwidth to take on an in-process transport any time soon.\n. @Random-Liu,\n\nAny updates on this?\n\nWe were literally talking about this yesterday, so we haven't forgotten about it.  However, it's still not at the top of our priority list.  It will probably be another 6 months before we can get to it.  Our other priorities right now are channelz, performance, and retry.  It would be a fairly meaty project if an outside contributor wanted to take it on, but we would be able to advise.\n\nThe main advantage of inprocgrpc is that it doesn't incur serialization/de-serialization overhead\n\nSkipping serialization/deserialization seems dangerous in Go, because proto messages are mutable.  This means that if the server modifies the request message, then the client would see the result of that modification.  (The same is true for streaming in either direction.)  This shouldn't typically be happening, but it's a notable difference between a real server and inprocgrpc.. > The library avoids serialization/de-serialization, but it does copy.\nAha, I see now.  So you do a proto.Clone() for proto messages, and a best-effort shallow copy otherwise (ref).  FWIW, it may be better (if possible) to serialize and deserialize using the configured codec in the fallback case, in order to avoid this potential problem.  But I also agree with the comment that says the fallback path should basically never be exercised, as most people use proto with grpc.\n. FWIW, in the next quarter, I plan to redesign the interface/abstractions between the grpc & transport packages to make it possible for custom transports to be implemented.. @Daniel-B-Smith yes, I believe it would unblock any efforts to implement that (with the caveat that the core grpc-go team is not committed to implementing it).  I'll have an update on the progress of this issue shortly.... I am far enough along with a prototype that I'm ready to share some details on this effort.  Please feel free to review and provide feedback.\nFirst, some of my starting goals:\n\nShould encapsulate gRPC semantics (hide HTTP/2 semantics and grpc wire protocol semantics).\nTransports should be message-based, not byte-based, to facilitate an in-process transport that uses proto.Clone() for efficiency.\nNote that retry support requires byte-based support from all transports, because we need to cache multiple sent messages, which our streaming API does not make possible without serialization.\nUnexpected asynchronous events should be delivered by callbacks, not channels.\nChannels may be more idiomatic for Go, but require a goroutine to monitor them, which leads to worse performance (in the case of streams) and more complex code (see #2219 which refactors our transport monitoring).\nSending and receiving data should remain synchronous to mirror grpc's external API.\nDialers (Client) and Listeners (Server) should return gRPC transports, not net.Conns.  How to connect (e.g. net.Dial) is an implementation detail enacapsulated in the transport.  Likewise, handshaking happens internally to Dialers and Listeners.\n\nOn to the API.  Disclaimer: I have PoC-implemented only the client-side of this interface so far.  However, a client is fully implemented and passes all tests, so this API is sufficient for all our current needs.\nPackage structure (under google.golang.org/grpc/):\ntransport: Definition of types shared between client and server\ntransport/client: Definition of client-side transport and stream\ntransport/server: Definition of server-side transport and stream\n```go\npackage transport\n// OutgoingMessage is a message to be sent by gRPC.\ntype OutgoingMessage interface {\n    // Marshal marshals to a byte buffer and returns information about the\n    // encoding, or an error.  Repeated calls to Marshal must always return the\n    // same values.\n    Marshal() ([]byte, *MessageInfo, error)\n}\n// OutgoingMessage is a message to be received by gRPC.\ntype IncomingMessage interface {\n    // Unmarshal unmarshals from the scatter-gather byte buffer given.\n    Unmarshal([][]byte, *MessageInfo) error\n}\n// MessageInfo contains details about how a message is encoded.\ntype MessageInfo struct {\n    // Encoding is the message's content-type encoding.\n    Encoding string\n    // Compressor is the compressor's name or the empty string if compression\n    // is disabled.\n    Compressor string\n}\n```\n```go\npackage client\n// TransportBuilder constructs Transports connected to addresses.\ntype TransportBuilder interface {\n    // Build begins connecting to the address.  It must return a Transport that\n    // is ready to accept new streams or an error.\n    Build(context.Context, resolver.Address, TransportMonitor, TransportBuildOptions) (Transport, error)\n// TransportBuildOptions, elided, is a struct with an []interface{} for opaque,\n// custom-transport options and possibly declared fields with known-common options.\n\n}\n// A Transport is a client-side gRPC transport.  It begins in a\n// \"half-connected\" state where the client may opportunistically start new\n// streams by calling NewStream.  Some clients will wait until the\n// TransportMonitor's Connected method is called.\ntype Transport interface {\n    // NewStream begins a new Stream on the Transport.  Blocks until sufficient\n    // stream quota is available, if applicable.  If the Transport is closed,\n    // returns an error.\n    NewStream(context.Context, Header) (Stream, error)\n// GracefulClose closes the Transport.  Outstanding and pending Streams\n// created by NewStream continue uninterrupted and this function blocks\n// until the Streams are finished and the transport has closed.\n// Close may be called concurrently.\nGracefulClose()\n\n// Close closes the Transport.  Outstanding and pending Streams created by\n// NewStream are canceled.\nClose()\n\n// Info returns information about the transport's current state.\nInfo() TransportInfo\n\n}\n// TransportInfo contains information about the transport's current state.  All\n// information is optional.\ntype TransportInfo struct {\n    // RemoteAddr is the address of the server (typically an IP/port).\n    RemoteAddr net.Addr\n    IsSecure   bool // if set, WithInsecure is not required and Per-RPC Credentials are allowed.\n    AuthInfo   credentials.AuthInfo\n}\n// A TransportMonitor is a monitor for client-side transports.\ntype TransportMonitor interface {\n    // Connected reports that the Transport is fully connected - i.e. the\n    // remote server has confirmed it is a gRPC server.\n    //\n    // May only be called once.\n    Connected()\n    // OnError reports that the Transport has closed due to the error provided.\n    // Existing streams may or may not continue, but new streams may not be\n    // created.  When all existing streams have completed, the Transport will\n    // fully close.\n    //\n    // Once called, no further calls in the TransportMonitor are valid.\n    OnError(error)\n}\n// Stream is a client-side streaming RPC\ntype Stream interface {\n    // SendMsg queues the message m to be sent by the Stream and returns true\n    // unless the Stream terminates before m can be queued.  May not wait for m\n    // to be sent.  May not be called simultaneously from multiple goroutines.\n    SendMsg(m transport.OutgoingMessage, opts StreamSendMsgOptions) bool\n// RecvHeader blocks until the Stream receives the server's header and then\n// returns it.  Returns nil if the Stream terminated without a valid\n// header.  Repeated calls will return the same header.\nRecvHeader() *ServerHeader\n\n// RecvMsg receives the next message on the Stream into m and returns true\n// unless the Stream terminates before a full message is received.  May not\n// be called simultaneously from multiple goroutines.\nRecvMsg(m transport.IncomingMessage) bool\n\n// RecvTrailer blocks until the Stream receives the server's trailer and\n// then returns it.  Returns a synthesized trailer containing an\n// appropriate status if the RPC terminates before receiving a trailer from\n// the server.  Repeated calls will return the same trailer.\n//\n// If all messages have not been retrieved from the stream before calling\n// RecvTrailer, subsequent calls to RecvMsg should immediately fail.  This\n// is to prevent the status of the RPC from changing as a result of parsing\n// the messages.\nRecvTrailer() Trailer\n\n// Cancel unconditionally cancels the RPC.  Any pending SendMsg and RecvMsg\n// calls should become unblocked.  Queued messages may not be sent.  If the\n// stream does not already have a status, the one provided (which must be\n// non-nil) is used.\nCancel(*status.Status)\n\n// Info returns information about the stream's current state.\nInfo() StreamInfo\n\n}\n// ServerHeader contains header data sent by the server.\ntype ServerHeader struct {\n    Metadata metadata.MD\n}\n// StreamInfo contains information about the stream's current state.\n//\n// All information is optional.  Fields not supported by a Stream returning\n// this struct should be nil.  If a transport does not support all features,\n// some grpc features (e.g. transparent retry or grpclb load reporting) may not\n// work properly.\ntype StreamInfo struct {\n    // BytesReceived is true iff the client received any data for this stream\n    // from the server (e.g. partial header bytes), false if the stream ended\n    // without receiving any data, or nil if data may still be received.\n    BytesReceived *bool\n// Unprocessed is true if the server has confirmed the application did not\n// process this stream*, false if the server sent a response indicating the\n// application may have processed the stream, or nil if it is uncertain.\n//\n// *: In HTTP/2, this is true if a RST_STREAM with REFUSED_STREAM is\n//    received or if a GOAWAY including this stream's ID is received.\nUnprocessed *bool\n\n}\n```\n```go\npackage server\ntype TransportListener interface {\n    // Accept blocks until a new Stream is created.  grpc calls this until\n    // error != nil.\n    Accept() (Stream, error)\n// GracefulClose causes the TransportListener to stop accepting new\n// incoming streams, and returns when all outstanding streams have\n// completed.\nGracefulClose()\n\n// Close immediately closes all outstanding connections and streams.\nClose()\n\n}\ntype Header struct {\n    MethodName string\n}\ntype Trailer struct {\n    Status   status.Status\n    Metadata metadata.MD\n}\ntype Stream interface {\n  // Implementation details TBD; expected to be similar in structure to client.Stream.\n}\n```\n/cc @johanbrandhorst @jhump \n. An important detail I left out is how to inject these custom transports.\nServer-side, the story is simple: instead of giving grpc a net.Conn-based Listener, you would pass it a server.TransportListener that spits out server.Streams instead of net.Conns.  That TransportListener would be responsible for managing the underlying connections.\nClient-side, it's a bit more complicated: either the resolvers or just the schemes (TBD) will indirectly determine which transport to use.  I.e. the \"dns\" scheme/resolver would output \"tcp\" addresses that by default use our default transport layer, whereas the \"inproc\" scheme/resolver (not yet implemented) would output \"inproc\" addresses that would use our inproc transport.  You could register (per-ClientConn or globally, also TBD) a map from scheme/address type to client.TransportBuilder that determines which transport to use for those addresses.\n. @annismckenzie I'm targeting something in the next month or so.  I am taking a couple weeks off soon, but hopefully I can wrap things up pretty quickly when I return since I already have a working prototype.\nI'm not sure what you mean about adding \"another 2 layers\".  This is basically just drawing a more consistent line between our existing, hard-coded HTTP/2 transport and the grpc package, and making it pluggable.. @annismckenzie no worries, I am just trying to understand your concerns with the design, if you have any.  I'm hopeful a few folks here will at least briefly look it over to let me know if there are any problems with it or improvements that can be made.  Thanks!. Sorry for the delay!  Priorities shift and there is a lot going on, but I have not lost sight of this.  This is old news, but there is a gRFC and also a proof of concept / prototype in my fork for the client-side work.  I will be working on this with the aim to finish it up this quarter.. As discussed, please maintain compatibility with older clients by creating a new interface and keeping the old one, and wrapping the old Logger with a new Logger.. @gbbr, that sounds right to me.  To hopefully explain this further: we don't know which server will be attempted before invoker is called -- the interceptor is almost the first thing we do after the user's code.  And note that with retry, you may end up hitting multiple different servers in the same call, so the Peer is only valid after the RPC has completed (or has been committed).\nThanks for clarifying the solution here.\n. We discussed this a bit today and weren't sure exactly what the right behavior is in this case.  As mentioned above, backoff applies only to cases where the server does not respond, not to the case where it can be reached but then immediately fails.  Related, the pick-first balancer, which restarts from the top of the list of backends upon any backend failure, would continually pick this same problem server if it is at the top.  We will need to do some more investigation on this.. This and similar problems have come up a few times lately, so we would like to do something soon.  The tentative plan is:\n\n\nAdd a dial option to allow the user to choose to wait until a settings frame is received from the server before considering the connection established.  This is what the java client does, and it will prevent this problem when connecting to a server that is accepting connections, but isn't a gRPC endpoint or isn't healthy enough to send settings frames.\n\n\nMake this option opt-out after one release, instead of opt-in.\n\n\nRemove this option after one more release.\n\n\nIf a server is healthy enough to send a settings frame, and then disconnects, and that happens repeatedly...I'm not sure there's much we can do about that without changes to the gRPC spec.\nOne incompatibility with this behavior that I'm aware of is cmux -- there is a workaround, but it would require a small code change.  If anyone knows of other possible problems with this approach, please let us know.\n. Let's consolidate this with #1444.. @apolcyn what is the state of this PR?. We've done some pretty significant refactoring and performance tuning in the past couple months, so I'm going to close this now.. @FX-HAO,\n\nIt seems i need a connection pool to reuse the connections.\n\nYou only need to create a single client, and you can use that one simultaneously for all of your RPCs.  You should not need to maintain a pool.. @apolcyn, do you remember why you filed this bug?  Is this a request to add a benchmark?  Or metrics gathering? Or something else?  Thanks!. This problem was fixed by #1139.  Thanks.. Thanks for the contribution.  I'm OK with the approach taken by this PR, and we will merge it if you can resolve the conflicts first.\n. @jordanorelli, ping - are you still interested in finishing this off?. @tamird, would you mind resolving the conflicts and then re-assigning this to me?  We'd like to move ahead with this.. @apolcyn what is the state of this PR?. I took your branch and rebased it (locally) and the tests still passed.  Merging.. @apolcyn what is the state of this PR?. @apolcyn ping. Java has this feature, and we will need to implement it in Go (probably in Q3).. If your connection window is filling up because of slow or blocked readers, then you could try using a bigger ratio of connection:stream window sizes (either raise the connection limit or lower the stream limit -- or both).  This would enable more streams to be in flight at any time.\nReading the description of the problem in cockroachdb/cockroach#14948, it sounds like you have a dependency between multiple readers from the same connection -- with this approach, I believe it's possible to hit this kind of deadlock under enough load no matter what strategy we use for flow control aside from an infinite connection window.  I'm not sure what the motivations are for not reading data that's pending, but if you could pull it off the stream and store it in another buffer (or throw it away if you're running out of memory and re-request it later if necessary?), that would be the only way to completely avoid the problem.\n@apolcyn's PR #1073 could help, but there are a couple caveats:\n\nOur defaults will still have the same ratio as what you're setting (32MB:2MB vs. 1MB:64KB), so you would still need to increase the connection window size to make the deadlock less likely.\nThis only helps with large messages -- if you're sending many small messages, this will not result in an increased stream window size, and throughput will still be a problem without manually increasing the window size.\n\nWe've also been talking about dynamic window size based upon the BDP of the connection, but there are no plans yet to implement them.\nEDIT: @apolcyn also had a suggestion to use multiple connections for this kind of use case.  With that approach, not reading from one connection would not impact with the others.\n. @MakMukhi  is taking over @apolcyn's change.  Reassigning and increasing priority.\n. This benchmark is flawed.  Every call to the throttler's Write method incurs the network latency before it returns (see lines 128, 89 & 102 in the linked file), whereas a normal net.Conn's Write function will return as soon as the data is committed to send.  When the grpc server sends the response to the client, it writes control frames (to absorb the request payload), headers, data frames, and trailers in separate calls to Write.  Since each call has a fixed 32ms delay, that explains why our performance appears so poor in these benchmarks.\n@MakMukhi  is going to see whether he can improve the throttler to only perform the sleep in the middle of transmitting the data between the writer and reader (instead of in-line in the Write call), and if so, we will provide updated numbers with this benchmark.\nIn the process of brainstorming performance ideas, we have come up with another improvement for unary RPCs -- we will send a very large window update to the server for the stream immediately after the request is sent.  This will enable the server to send the entire response once it is ready, without waiting for a window update after the first 64k is received by the client.  This should eliminate one round-trip for large response messages.\n. I took @rsc's benchmark and copied it into a branch here.  I modified the throttler to make the reader apply a delay corresponding to when Write was called, plus a latency, and I wrapped both the client and the server net.Conns.  Here are my numbers with a one-way latency of 76, which matches up almost exactly with the real-world samples gathered by @MakMukhi:\n```\n459.528466ms    76ms    GRPC\n459.264934ms    76ms    GRPC\n460.386324ms    76ms    GRPC\n458.850075ms    76ms    GRPC\n460.898012ms    76ms    GRPC\n2.904975973s    76ms    HTTP/2.0\n2.595898404s    76ms    HTTP/2.0\n2.595900343s    76ms    HTTP/2.0\n2.594949489s    76ms    HTTP/2.0\n2.594792549s    76ms    HTTP/2.0\n471.882055ms    76ms    HTTP/1.1\n156.190616ms    76ms    HTTP/1.1\n157.006169ms    76ms    HTTP/1.1\n156.707524ms    76ms    HTTP/1.1\n156.684356ms    76ms    HTTP/1.1\n```\nSo we're approximately 3x the time of HTTP1, which is about twice as good as the previous measurement showed, but still more than what I would expect, which would be 2x of HTTP1, because of flow control.\n. Some explanation of the numbers: the reason it's 3x instead of 2x is because of the size of the request (1MB) very closely matching our connection window.  Only once 25% of the connection window worth of data is received by the server does it send a window update back to the client.  The timing looks like this:\n```\n 0ms - Client initiates connection.  Server conn window: 1MB, stream window: 64KB (defaults).\n     - Client sends headers and the first 64k- of request data, where\n        represents a tiny header we put on the data.\n     - Server conn window: 960KB, stream window: 0.\n32ms - Server receives first 64KB- of data.\n     - Server sends 1MB+-64KB window update for the stream to let the\n       client know to send the entire message.\n64ms - Client receives window update.  Server conn window: 960KB, stream window: 960KB+.\n     - Client sends remaining request data -- except for , before running\n       out of connection window quota.\n     - Server conn window: 0MB, stream window: \n96ms - Server receives data.\n     - Server sends 1MB connection window updates (split in 4 updates)\n128ms - Client receives window updates.  Server conn window: 1MB, stream window: .\n      - Client sends remaining  of payload.\n160ms - Server receives remainder of request payload.  Sends response.\n192ms - Client receives server response; done.\n```\nIf we set the initial connection window size to 2MB manually (via grpc.InitialConnWindowSize(2<<20)), that removes one hop, and we see ~132ms.  If we also set the initial stream window size to 2MB (via grpc.InitialWindowSize(2<<20)), then we can perform the entire RPC in a single hop.\n. @robpike, @adg, @glycerine\nI'm concerned that if we optimize performance for this one workload, we may do so at the expense of other, more-realistic scenarios.  Flow control exists as a way to trade-off bandwidth for application memory usage and tail latency.  It would be trivial to change the defaults of grpc-go to maximize bandwidth utilization at the cost of the other two, but that is not our goal.  While 64MB seems like a reasonable setting for the window size when you think about it in terms of a single connection, if you consider that servers may have thousands of open connections at once, that quickly turns into a production problem waiting to happen.\nAt the end of the day, grpc-go needs to be a reasonable out-of-the-box solution for most users, while providing configuration knobs for advanced users to optimize for either speed or memory footprint based upon their individual needs.  Also note that any changes to the default behavior has to be made very cautiously, as they will impact all our users.\nTo that end, our plans for the next month or so are as follows:\n\n\nAdd a collection of benchmarks to measure our performance for different types of workloads under various network conditions (high and low bandwidth and latency).  Performance metrics will include throughput, QPS, and tail latency (as well as localized metrics like allocations, syscalls, and CPU usage).\n\n\nProvide a setting to limit grpc's memory usage per connection.  The default setting for this is TBD.\n\n\nProvide a setting to limit grpc's memory usage per stream.  The default setting for this is likely to be the measured BDP of the connection to optimize for maximum performance per stream (and if not, we will provide a way to configure it that way).\n\n\nImplement other optimizations where we find opportunities, like batching pending window updates and pre-emptively sending a large window update along with a unary request.\n\n\nDocument how grpc-go manages flow control, how all the various settings impact that, and how to optimize settings for different use cases.\n\n\nFurther in the future, provide a setting to limit grpc's memory usage per-Server and per-Client.\n\n\nThis bug will remain closed due to the misinformation / FUD caused by the benchmark it was predicated upon.  Significant improvements have already been made for these types of scenarios and settings were added to allow users to easily trade-off memory usage to achieve better performance.  I'll file a few extra issues to track things more granularly than #1280 and link them here later today.  Thanks for your understanding and patience.\n. Sorry, I forgot to circle back and link the issues here.  You should refer to our performance label for an up-to-date list of our perf issues.\nSince this bug was popular, we've added auto-window-size updating based upon BDP estimation, which we believe should completely resolve this issue.  Most of our performance work remaining at this point is optimizations to improve CPU utilization by reducing syscalls and avoiding allocations (thereby reducing GC), and reducing lock contention to support higher QPS on each connection.\n. Sounds like this issue is caused by having an older x/net/context package in combination with a newer version of Go.  This issue should resolve itself when we drop 1.6 support and switch to using the Go context.. @nicolasnoble, what is the status of this?  We noticed the counterpart changes in c and java are also pending. If it's OK to move forward with this change, can you please update it to include all the new/moved files?  We will prioritize the approval then.  If it's not appropriate, I would like to close it until it's ready.  Thanks!. Nice catch.  You're right -- normally it would be safe to cancel the context immediately after the client is created.  Unfortunately, making that change stimulates a bug in Go 1.6's implementation of Dial that results in subsequent I/O errors: https://github.com/golang/go/issues/15078\nNot cancelling is not a major problem, because the resources will eventually be freed after the context leaves scope, but it's unusual enough that I'll add a comment to the code explaining why we aren't calling it.. Note that this is low priority because users can implement this feature without support from the grpc library.  E.g. every time a stream is started/stopped, increment/decrement a WaitGroup (potentially using an interceptor).  When you are ready to gracefully stop the client, stop using the client for new RPCs, and in a goroutine do: wg.Wait(); client.Close()\n. @prune998 \n\nI'm not sure I understand how to cleanly stop a Grpc Streaming feed from the client out\nof the comments here...\n\nIf you mean a single stream, that is not the topic of this issue.  To stop a stream, you can simply cancel the context given to NewStream.  Or to \"cleanly\" stop it, the client would typically CloseSend.  Then the server's Recv would see an io.EOF, at which point, the server handler would return the status of the RPC (or nil for success).  The client's Recv would then see the status (or io.EOF for success).\nThe feature in this bug will not stop any streams.  It will prevent new ones from being created and block until the existing streams finish naturally.\n. Do you have any specific comments about either how you're trying to use it and what problems you're having, or how you would propose to change it?  Thanks.\n. I think it's possible that's #1738.. LGTM, but travis is unhappy.  Looks like maybe an unrelated flaky test?. LGTM. LGTM. LGTM. We would like to provide some official examples or documentation for this scenario, but don't currently have the cycles.  In the meantime, if you search the web for \"grpc oauth golang\", you should find some samples to get you started.. LGTM. Are you setting deadlines on your RPCs, and are you attempting 100+ concurrent RPCs?  If so, this may be a known issue fixed by #1124.  Please give it a try.. Thanks for the follow-up.\nIt's unlikely the original issue was seen because of malformed metadata, because it sounds like it was happening sporadically.  Is this still happening?  If so, we could use some help in reproducing it (please see @MakMukhi's comment: https://github.com/grpc/grpc-go/issues/1134#issuecomment-288591079).  Thanks!\n. > Switching the md key to \"authorization-bin\" seems to have fixed the issue.\nWe think that checking the return value of WriteField on the client side (ref) may be able to surface these types of issues before attempting to send the RPC that contains illegal characters in metadata.. @raliste, Did you set the corresponding server options?\nhttps://godoc.org/google.golang.org/grpc#MaxRecvMsgSize\nhttps://godoc.org/google.golang.org/grpc#MaxSendMsgSize. @sslavian812, it looks like you're working in Python, but this is the Go repo.  Please be patient and someone should respond to your question on SO.  If not, you can also email the grpc.io group to ask for help, or if you think it's a bug, file an issue against python in the main grpc repo.. LGTM. LGTM. LGTM. Yes, that is unrelated; this issue was about go1.5.  The error from yours says:\nFetching https://golang.org/x/oauth2?go-get=1\nParsing meta tags from https://golang.org/x/oauth2?go-get=1 (status code 404)\npackage golang.org/x/oauth2: unrecognized import path \"golang.org/x/oauth2\" (parse https://golang.org/x/oauth2?go-get=1: no go-import meta tags ())\nI don't know what could be causing that.. Best guess is maybe https://github.com/grpc/grpc/pull/10309, which changed to use go-latest instead of go1.5, and go-latest is unstable.  Is it possible for you to test with the go1.8 version and see if that's more reliable?\n. Thanks for pointing this out.  I'm going to work on having separate keys in the context so that the server doesn't unintentionally forward the metadata from the inbound RPCs into its outbound RPCs.  I'm also going to look into fixing the consistency problems with binary data in the metadata without making any substantial API changes.\n. PR #1157 is for this and is waiting for review.  If you'd like you can patch that change and test with it.\n. Fixed in #1189 and #1180. > if the logic uses typical Go idioms for error propagation, the server will fail with the same response code as received by the client call.\n\nWhile there are some cases where this appropriate -- like when forwarding/proxying a request to another server -- it should be the exception, not the rule.\n\nPart of idiomatic Go error propagation includes determining whether the error is appropriate to forward directly or if it might need to be wrapped or translated into another type of error instead.  Deciding how to translate errors should be the responsibility of the person implementing the middle service, not gRPC.  Mechanically, this translation can be readily accomplished via several methods without changing the gRPC core or API, e.g.s:\n\nclient interceptors\nwrapper types/libraries\ntranslation functions (func (error) error) called in-line\n\nI am not convinced the value of a gRPC-provided method of doing this would be worth the cost of API complexity or ongoing maintenance overhead.\nFWIW, I've personally had success using a generic client wrapper library that wraps RPC calls using lambdas (func(context.Context) error) and does this kind of error translation -- and also adds other useful features like rate limiting and retrying.\nThank you for taking the time to write up your suggestion.\n. > Perhaps this should be title, \"It would be helpful to have separate error types for server and client errors, to enable easier error processing\"\nThat title would put more emphasis on the proposed solution than the problem.  This also wouldn't make anything easier, really.  It would make it harder to accidentally forward the error code from an outbound call in a service handler.\nHaving separate types would be easy to implement, but this would be a behavior change.  I'm sure there are proxying/forwarding services out there that we would break if we did this.\n. Thank you for your contribution, and sorry for the delay.\nWe debated this for some time, and have decided we do not want to accept this change.  Specific implementations of compressors are not a core part of gRPC, and we would prefer not to maintain a library of them.  Additionally (but not the main factor), including these directly in the grpc package adds dependencies that are not essential for grpc functionality, and that should be avoided as much as possible.\nEven so, we appreciate that it's not as trivial as it should be to install a new compressor/decompressor.  We'd like to make a change in the future that removes the Compressor/Decompressor interfaces and instead is built upon the standard package's NewWriter/NewReader functions in the different compression packages directly (or possibly with very minor wrapping).  Until then, if you would like, you could add this to your own repo, and we can link to it from our grpc/grpc-contrib repo.\nThanks again for this PR.\n. I created #1193 to track the idea expressed above.. Do we still want this, or should we close it?. I would appreciate some advice for how/where to test this change.\n. PTAL, thanks!. This is considered a fix for a fairly serious security bug.  Automatically forwarding information between services was never intentional.  Even so, I am sorry this change caused problems for you, and I apologize for not announcing it more broadly before it was merged.  We are working to improve our communication, and we will use this as an example of things we can do better.. I don't believe there's any way to do that using the stream or req parameters to the handler, but you can do it if you have a field in your server struct with the Server returned by grpc.NewServer().  Then you could call Stop() or GracefulStop() on that Server as desired.\n. We are going to implement this in the next few weeks with a slightly different approach.  Thank you for submitting this.. Please resolve the conflicts and we will make sure to prioritize the re-review of this.. > I notice that this code is still using a Go struct instead of JSON for the service config. This means that it's not easily extensible -- i.e., it doesn't provide a way for third parties to add new fields to be used in their own interceptors. I assume that we're going to fix that in a separate PR?\n@markdroth - We also aren't passing the service config to the interceptors.  Are these requirements?\n. Please see #922, and if you still think changes are necessary after that is done, please leave your thoughts in that PR.. From https://github.com/google/go-genproto/blob/master/README.md:\n\nIMPORTANT  This repository is currently experimental\n\nI'll update grpc to use google.golang.org instead since it was only recently added.  Thanks.\n. Interoperability is not affected by this change.  The metadata is sent and received on the wire the same way.  This only changes the way the metadata is represented in contexts within Go binaries.. Some minor changes to this proposal:\n```go\npackage grpc\nfunc RegisterCompressor(name string, f func(io.Writer) io.WriteCloser)\nfunc RegisterDecompressor(name string, f func(io.Reader) (io.ReadCloser, error))\nfunc UseCompressor(name string) CallOption\n```\nUsage:\n```go\nfunc init() {\n  grpc.RegisterCompressor(\"gzip\", func(w io.Writer) (io.WriteCloser) { return gzip.NewWriter(w) })\n  grpc.RegisterDecompressor(\"gzip\", func(r io.Reader) (io.Reader, error) { return gzip.NewReader(r) })\n}\ncc, err := grpc.Dial(..., grpc.WithDefaultCallOptions(grpc.UseCompressor(\"gzip\")))\n// All calls use \"gzip\" unless overridden:\nres, err := client.SomeMethod(..., grpc.UseCompressor(\"deflate\"))\n```\nThe legacy grpc.WithCompressor will continue to be used (unless overridden by the UseCompressor call option).\nIf grpc.WithDecompressor is specified, we will always use that decompressor instead of looking for the registered decompressor, to avoid a behavior change.  No compressors/decompressors will be installed by default, but we will add a package that implements gzip compression/decompression (mainly as an example), which can be imported to self-register gzip support.\nThis effectively matches how codecs will be implemented in the near future.\n. @mehrdada yes, this is how I envision this check will happen eventually.. Thanks for the repro case.  Usage of the standard library context isn't supported by grpc-go, but I may be able to make a change here that will avoid this particular panic.\n. As for the main issue reported here, my guess about what's happening:\nThe client is panicking after starting the RPC, which means the client-side code in grpc to manage the stream for the RPC is being aborted unexpectedly.  Because of this:\n\nthe server never gets the context cancellation, and\nthe connection's flow control is never updated after the server's response is sent.\n\nThat would explain why the server stops processing requests on that connection.  If a new connection is established, I would expect it to be handled correctly.\nNot panicking should eliminate the problem.\nLooking at the output from your repro case, though (with #1258 applied), it seems like client-side cancellation may not be getting propagated to the server, or flow control still ends up getting blocked even with proper error handling.  We still ended up with 73 goroutines on the server, and when I quit it, it spammed errors like this:\ngrpc: Server.processUnaryRPC failed to write status connection error: desc = \"transport is closing\"\ngrpc: Server.processUnaryRPC failed to write status stream error: code = Canceled desc = \"context canceled\"\n. It looks like the client's implementation is to NOT cancel RPCs after the first 500.  At that point, the RPCs wait until completion, which is why the server has 73 goroutines running.  There should be ~67 server handlers in flight at any time (1 sent per 15ms, 1s per request), and there are a handful of other goroutines running.  So I think there is nothing else left to do on this issue.  Thank you for the report and repro code.\n. Sorry for the delay here.\nThis is definitely something we need to support, but we'd like to take a slightly different approach for the implementation.  We would like to move to a registration pattern instead, i.e. grpc.RegisterCodecs(...Codec) (leveraging the String method in the Codec interface for the name).  The registration pattern is important for proper handling of received messages in various formats, and it also unifies the API for the server and client.  With the registration pattern, the associated CallOption-generating function should require the name of the codec to be used, with \"proto\" as the default for backward compatibility.\nIn PR #1165, @lyuxuan is introducing the concept of a \"default call option\", which should ultimately replace the current dial options (e.g. WithCodec) that only apply at call-time.  This change should wait until that is introduced, at which time the existing WithCodec can be re-implemented as an append to the default call options (which avoids the need for storing the setting in two places).\nIf you're interested in reworking this change to follow that approach, we would really appreciate the contribution.\nThanks!\nDoug\n. For the first question: yes, a global map is what I meant.  But, we should capture the currently-registered codecs when creating a new Server / ClientConn, to avoid the need for a lock on every RPC to access the map.  Let me know if I missed any other details in the description.\nFor the second: I'll have to think about this a bit more and do some research into the spec/other language implementations.  It doesn't seem correct to me that the sub-type can be set manually to something that is not a valid codec, or else the receiving end won't be able to handle the results (unless that codec is registered as pass-through).  I'll have to get back to you on this...\n. Hi Peter,\nThanks for meeting to explain your needs in more detail.  We've discussed this a bit more internally, and have come up with the following proposal (with only slight modifications to the previous version, and hopefully sharing a decent amount of similarity with this PR):\n\nA global registry (not captured by the ClientConn/Server to avoid essentially unnecessary copying).  I.e., in grpc/codec.go:\n\n```go\nvar codecs = map[string]Codec{}\n// RegisterCodec registers the provided codec for use with all gRPC clients and servers.\n// RegisterCodec should only be called from init.\nfunc RegisterCodec(c Codec) {\n  codecs[c.String()] = c  // basically.\n}\n```\n(Sorry, we're going to be using global registration a decent amount for other things to maintain API similarity with C++.)\n\n\nRegister our proto decoder in a grpc package init function.\n\n\nAdd a new CallOption to specify the content subtype by string.  This will (mostly) cause gRPC to look up that string in the map and use the corresponding codec, or error if a codec with that name is not found.  (See below for details.)\n\n\ngo\nfunc CallContentSubtype(contentSubtype string) CallOption\n\nKeep the existing custom codec {Dial,Server}Options for backward compatibility and going forward for less-\"traditional\" use cases such as yours.  Add a new CallCustomCodec CallOption, and re-implement WithCodec (the DialOption) using it and WithDefaultCallOptions:\n\n```go\nfunc CallCustomCodec(c Codec) CallOption {\n  ...\n}\nfunc WithCodec(c Codec) DialOption {\n  return WithDefaultCallOptions(CallCustomCodec(c))\n}\n```\n(Optionally, add a WithCustomCodec that behaves the same as WithCodec, for naming consistency.)\nBehavior-wise:\nIf the CallContentSubtype CallOption is used:\n- The string sent on the wire will be application/grpc+<contentSubtype>.\n- If CallCustomCodec is not used as a CallOption, grpc will look up the contentSubtype specified in the map.  If no entry is present, we will return an Internal error (or Unimplemented? -- TBD).  If it is present, that codec will be used.  The response message's content subtype will independently be looked up in the map, and an error will be returned if it is not supported.\n- If a CallCustomCodec is used as a CallOption, grpc will blindly use that codec for all request and response messages.\nIf the CallContentSubtype CallOption is not used:\n- If CallCustomCodec is not used as a CallOption, contentSubtype will default to \"proto\".\n- If CallCustomCodec is used as a CallOption, contentSubtype will be copts.customCodec.String().\n- The string sent on the wire will be application/grpc+<contentSubtype>.\nAnd lastly, the content-type from the wire should be available via the header metadata (as in this PR).\nLet me know if you have any questions/concerns/objections.\nIt would probably be best to create a new PR for this implementation, even if you base it on the same branch.\nThanks!\n. > if you specify both content subtype AND custom codec, then it will use that codec blindly, correct?\nExactly!  That also matches the current behavior of the custom codec (we have no current checks on the content subtype vs. codec name), so I thought that would be ideal.  For most users, we would not recommend using custom codecs anymore as Dial/Call/Server options, but using the registry instead.\n\nCould I also add a WithContentSubtype DialOption,\n\nFrom a purist point of view, I would rather not have DialOptions that are really CallOptions in disguise, except for the one that allows you to set default CallOptions.  Otherwise, users could get confused about the actual behavior of these things.  DialOptions should affect dialing.  There's unfortunately precedence for the other way, but I would like to phase those things out eventually.\n\nShould we have grpc-go always send content subtype now\n\nIf possible, yes.  It would depend on interop with Java/C.  I will dig around and figure out whether this is doable.  If not, then we'll match the existing behavior and never set content subtype unless the explicit option to set it is provided.\n\nwould you be open to having a function func WithDefaultMetadata(md metadata.MD) DialOption { ... }\n\nI was just thinking the other day that it's a little weird that we send outgoing metadata via the context instead of using call options.  Following my above opinion about CallOptions vs DialOptions, I would be open to a func CallMetadata(md metadata.MD) CallOption that could be used with WithDefaultCallOptions to do exactly what you want.\n. @peter-edge - ping, just curious what your plans are for this PR, schedule-wise.  Thanks!. That should be fine, thanks!. I don't believe the base64 package can handle padded OR unpadded values.  So it looks like we'll have to try both.. Yes, that seems reasonable, thanks for the suggestion.. Sorry for dropping this -- were you able to resolve it?. Thanks for the report and debugging info.  This is definitely a problem, but I don't see any way this can be handled gracefully at this point.  We are planning to remove the NewContext and FromContext functions from the metadata package completely in order to force users to decide which metadata they intended to set/get.  Unfortunately, an API breakage is necessary here, because this is a security problem with the API itself.\n. Docs were added to ServeHTTP and clarified elsewhere.  The status package was not changed -- similar documentation is pretty common in Go.\n. Sorry for closing this issue too quickly.  We thought the whole issue was about trying to salvage the stream even though the receiver isn't reading off the channel.  It does seem reasonable to have a way of cancelling a send without relying upon the context's deadline, which the server can't influence.\nIf I understand your objective correctly, it's possible to achieve the behavior of adding a deadline to an individual SendMsg call as follows:\ngo\ndone := make(chan struct{})\ngo func () {\n  stream.SendMsg(response)\n  close(done)\n}()\nt := time.NewTimer(5*time.Second)\nselect {\n  case <-t.C:\n    return status.Errorf(codes.DeadlineExceeded, \"too slow\")\n  case <-done:\n    if !t.Stop() {\n      <-t.C\n    }\n}\nThis is clearly too much boilerplate for sending a message with a timeout.  We could simplify this significantly if we added an explicit Cancel method to the ServerStream type.  The resulting code would look something like this:\ngo\nt := time.AfterFunc(5*time.Second, stream.Cancel)\nstream.SendMsg()\nif !t.Stop() {\n  <- t.C\n}\nWould adding the ability to cancel the stream without needing to return from the handler be sufficient to resolve this issue?  (Note that the client side can already implement this pattern, because it created the context used to perform the call, so it can use the associated cancel function.)\n. > However for the specific use case,\n\nstream := createStream().withWriteTimeout(\"1s\")\nwould be even more usable.\n\nOnly the client creates the stream, so this wouldn't work for the server.\nIf we do something to make timeouts first-class, it should really be set per-send.  Unfortunately, our hands are a bit tied here: any change will break the API, because {Client,Server}Stream are interfaces -- even adding a variadic SendOptions-type parameter to Send, which would be backward compatible if the stream wasn't in an interface.\n. Retrying on a stream after a timeout is basically impossible.\nTechnically it is possible in certain circumstances, but if you sent part of the message on the wire before the timeout, but not all of it, the stream must be reset (i.e. closed).  Because of this, we would need to distinguish to the caller whether or not that happened, which is confusing.  And if a timeout ever happens, it means the receiver isn't receiving its pending data for whatever reason, so what would be the goal of continuing to use the stream in that case?  End the stream and let the client initiate a new one if desired.  It's also somewhat arbitrary to give up in this case if the goal is not to end the stream: if there is sufficient flow control for the message, the send will return quickly, but there's no guarantee the client ever requests the data.\nUsing a context to specify timeouts is fine, but SendMsg doesn't currently accept a context.  Backward compatibility must be broken to support per-send timeouts.  For this reason, unfortunately, it's unlikely anything will happen for this any time soon.\nI think for now the best idea is to wrap the code in https://github.com/grpc/grpc-go/issues/1229#issuecomment-300938770 in a function and call it where desired.  It can be written fairly generically, too:\n```go\n// DoWithTimeout runs f and returns its error.  If the deadline d elapses first,\n// it returns a grpc DeadlineExceeded error instead.\nfunc DoWithTimeout(f func() error, d time.Duration) error {\n  errChan := make(chan error, 1)\n  go func () {\n    errChan <- f()\n    close(errChan)\n  }()\n  t := time.NewTimer(5*time.Second)\n  select {\n    case <-t.C:\n      return status.Errorf(codes.DeadlineExceeded, \"too slow\")\n    case err := <-errChan:\n      if !t.Stop() {\n        <-t.C\n      }\n      return err\n  }\n}\nfunc (s server) MyHandler(stream pb.Service_MyHandlerServer) error {\n  for  {\n    if err := DoWithTimeout(func() error { return stream.SendMsg() }, 5time.Second); err != nil {\n      return err\n    }\n  }\n  return nil\n}\n```\nOr with a context instead:\ngo\n// DoWithContext runs f and returns its error.  If the context is cancelled or\n// times out first, it returns the context's error instead.\nfunc DoWithContext(ctx context.Context, f func() error) error {\n  errChan := make(chan error, 1)\n  go func () {\n    errChan <- f()\n    close(errChan)\n  }()\n  select {\n    case <-ctx.Done():\n      return ctx.Err()\n    case err := <-errChan:\n      if !t.Stop() {\n        <-t.C\n      }\n      return err\n  }\n}\ntl;dr: this is something we would really like to support, but it requires breaking the API, and there is a fairly reasonable workaround.  This makes it both hard and low priority, so I'm going to close this for now.\n. Good catch, thanks.  I updated the examples.. > Is there a way to forcibly close the connection from the server side, so that conn.Send will return abnormally and be able to bounce the sender goroutine?\nYou can use the examples in this comment as ways to add a timeout to your send attempt.  Note the important bits of starting a timer and exiting the RPC handler.  When the handler returns, the RPC ends and all Send/Recv calls in any goroutines will unblock with errors indicating the stream is closed.. What is the exact API you are proposing, so I can better understand your intentions?  Status should be immutable, so a WithDetails is preferred over SetDetails.\n. Details() is redundant with s.Proto().Details.  But then again, Message() is redundant with s.Proto().Message.  I'd be OK with having this accessor for convenience and consistency.  (Note that Details is actually a slice of Anys.)\nThe reason I abandoned a constructor that takes details and went with FromProto/ErrorProto was because of the Newf/Errorf variants.  Given this, I am still inclined to require the use of ErrorProto for users that want to return details in the error.  There's not really much convenience or utility that this kind of constructor would provide.  If we could also perform ptypes.MarshalAny and fill in the Details field for the user according to best practices, that could be nice, but then ErrorWithDetails() would have to return (error, error), which would be awkward and maybe not all that convenient after all.\n. Yes, because of [Un]MarshalAny, we will need to pass errors back to the caller.  I hadn't noticed the DynamicAny type that can be passed to UnmarshalAny, so I think we can decode the any messages for the user.\nHow about this API?\n```go\n// WithDetails returns a new status with the provided details messages appended to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\nfunc (s *Status) WithDetails(...proto.Message) (s, error)\n// Details returns a slice of details messages attached to the status.\n// If any errors are encountered, it returns nil and the first error encountered.\nfunc (s *Status) Details() ([]proto.Message, error)\n```\nI think this would be useful because it hides both the Status and Any proto types from the user.\nWithDetails is a little more Java/builder style, but it is more convenient to use than status.AddDetails(s, ...).  Usage would be:\n```go\n// setting:\ns, _ := status.New(codes.Unavailable, \"msg\").WithDetails(m1, m2)\nreturn s.Err()\n// getting:\nds, _ := s.Details()\nfor d := range ds {\n  switch m := d.(type) {\n    case ed.RetryInfo:\n      // ...\n  }\n}\n```\nEdit: fix \"setting\" code.. > I am not sure the code sample you have would work though, I think go would\n\ncomplain that we are using Err() on the pair (s, err) returned from WithDetails method.\n\nYes, you're right.  Also, Err() only returns one value.  You'd have to do this instead:\ngo\ns, _ := status.New(codes.Unavailable, \"msg\").WithDetails(m1, m2)\nreturn s.Err()\nI'd rather not have a package that deals with error_details.proto messages in grpc itself -- this would introduce an unnecessary dependency, and even create a wrapper library for it.  It should be straightforward to construct the necessary error_details protos and serialize them using an API that works with all protos.  If simple convenience wrappers are helpful for an application, those could be implemented either in the same package that needs them or in a separate library.  Also, I believe the marshal/unmarshal errors will always be technically possible (e.g. if the message is too big).\nThe Details/WithDetails functions would be useful for everyone using gRPC that wants to attach/read error details, so if you are interested in adding those, we would appreciate the contribution.  Thanks!\n. I had a few small comments that I made on that commit, but it looks great overall, thanks!\nRegarding the tests, the success cases are plenty (maybe even too many?), so maybe add a few tests for the various places where errors can occur.  Also, I might recommend expressing them a little differently for that purpose:\n```go\n    m1 := &epb.ResourceInfo{ResourceType: \"book\"}\n    ...\ntests := []struct {\n    s *Status\n    want proto.Message\n    wantErr string // or just a bool?\n}{\n    s: New(codes.OK, \"ok\").WithDetails(m1),\n    want: m1\n}, {\n    s: nil\n    wantErr: \"...\"\n}, {\n    s: New(codes.OK, \"ok\").WithDetails(nil),\n    wantErr: \"...\"\n\n...\n```\nThanks again!\n. > A question: regarding returning error on nil input, no other function in the package returns a separate error, I am wondering if it would be more consistent with the rest of the functions if we return a bool in place of an error as in the FromError function.\nAn error here makes more sense, so the user can see the error produced by MarshalAny.  We just need to return a special error in this case.  That error should basically never happen unless the user does something very wrong.  (For FromError(), a bool is sufficient, because the error either is or is not a status type.)\n\nps: I cannot do\u00a0s: New(...).WithDetails(...),\u00a0because WithDetails returns two values.\n\nDerp.  You would need to wrap it in a closure that does not return an error (and obviously don't give it any invalid input as I did in the 3rd case -- t.Fatal instead if there is an error.)\n. Lots of good points.\nstatus.FromProto(p).Proto() != p, actually (because that's a pointer comparison), just things that proto.Equal().  Right now, FromProto(nil).Proto() == nil, but that's not something anyone should be relying upon.  So changing status.FromProto() like that doesn't seem unreasonable.\nHowever...  WithDetails() doesn't really make sense on an OK status, either -- OK statuses get converted into a pure-nil error in Err() before they are handed over to grpc, so the details will be lost then.  So we could make WithDetails() error any time s.Code() == codes.OK.  That would help prevent the misconception that the details of an OK status can be sent to the client from the server.  What do you think?\n. No, the proto message itself doesn't document that status of OK with details is nonsensical.  The best it does is call them \"error details\" in the documentation, and an \"OK\" status logically isn't an error (and the comments in google.rpc.code states this, too).  So while the type itself doesn't document them explicitly that way, I think it's fine for gRPC's helpers to not be able to generate an OK status with details, since the details will be lost when converted into an error.\n. With #1754, this last part (Is) should be satisfied (via status.Code(err) == code instead).. @mwitkow, could you clarify this better, please?  From what we can tell, we never set Delay to true, so we will always flush at the end of every message.  (In fact, #524 is requesting the ability to set Delay to true when performing a Send.). Those functions deal with addrConns, which are individual connections, not the ClientConn, which represents a connection to zero or more servers.  It sounds like you're interested in just knowing whether the ClientConn is valid?\nCan you attempt to re-establish the stream even though the server is still down, but set non-fail-fast and a very long deadline?\n. We are planning to implement the connectivity state API next quarter (by September).. Looks like there's a dupe of this.  #1208.. My guess is this is likely #1219.  To add the metadata, are you using metadata.FromContext in a client-side interceptor?  If so, it will need to be changed to use FromOutgoingContext.. That would clear the outgoing metadata.  To add an entry to the outgoing metadata, you should do something like this:\ngo\nmd, _ := metadata.FromOutgoingContext(ctx)\nmd = metadata.Join(md, metadata.New(map[string]string{\"key\": \"value\"}))\nctx = metadata.NewOutgoingContext(ctx, md)\n. It sounds like it's something else in the context, not the metadata, then...\nCan you log the context?  It may also be helpful if you could provide a reproducible test case so we can dig into it.\n. Almost definitely what's happening here is the client interceptor that adds its metadata is reading your incoming metadata (because it is using metadata.FromContext), appending its value to the metadata there, then using that as the outgoing metadata.  This is the main root cause, and that should be fixed by GoogleCloudPlatform/google-cloud-go#627.\nIn addition, it seems there's characters in the incoming metadata that isn't allowed in text-only outgoing metadata, and that's why we're getting the error we're getting.  It would be interesting to know the nature of the redacted authorization/authority values (e.g. are there characters in there outside the range of the base64 encoding set?).  Also, you said earlier that you have clients that are running both the latest grpc version and older versions, and only the older versions fail.  Can you compare the incoming metadata at the server from those two clients and see if you can discern a difference between them?\n. The fix to the bigtable client library was merged, so this should be resolved now.. This change isn't possible without changing the API for codecs.  I.e. we hand that buffer straight to the codec, which is free to take a reference to it, and pass it along to the application.\nWe are focusing on performance more in the upcoming weeks/months, and have some changes we are considering in this area at that time.\n. The flaky test was fixed; @MakMukhi still wanted to investigate why a full second was insufficient for something expected to take microseconds.\n. It seems like scheduling issues were the root cause of this.. This was fixed by #1498 . @mehrdada - I wonder if this is the same root cause as #1192?  Slowing down sending could mean it's an issue with flow control.. Upon further consideration, I don't think this is a good idea.  The \"extra\" CallOptions can do things like specify metadata, which may affect the behavior of the server.\nI believe what you really want is #906 to cut out unnecessary gRPC processing while still doing the things it needs to do that affect client/server interaction.\n. You don't need this feature to do that.  You can put your clients and services in one binary and have the server serve to a free port on localhost, with the clients dialing that same port.\nOr, you could try out our bufconn package that we use for some of our testing to avoid the network stack entirely.  To use it, you pass a bufconn.Listener to a grpc.Server's Serve method, and pass the listener's Dial function as a custom dialer via the WithDialer call option.  (Wrap it to ignore the args.)\n. This doesn't deal with the goAway error.  How about this version?\ngo\nvar err error\nselect {\ncase <-s.ctx.Done():\n    err = ContextErr(s.ctx.Err())\ncase <-s.goAway:\n    err = ErrStreamDrain\ncase <-s.headerChan:\n    return s.header.Copy(), nil\n}\n// Even if the stream is closed, header is returned if available.\nselect {\ncase <-s.headerChan:\n    return s.header.Copy(), nil\ndefault:\n}\nreturn nil, err. \"Typically used by\" does not mean it may not be used elsewhere.  The context.Background() you see in grpc.NewServer is just a signal we use internally for one small purpose. Server's Stop() and GracefulStop() methods are intended to be used to close the connection.  If you need to trigger server close on this other context, you can do the following:\ngo\ns := grpc.NewServer()\ngo func() {\n  select {\n    case <-myctx.Done():\n      s./*Graceful*/Stop()\n  }\n}()\n. Remaining work: a benchmark that sends messages in an unconstrained fashion (as opposed to ping-pong). Actually, we have the network parameters in there already.  But we're still missing unconstrained-streaming-RPC benchmarks.. That's a reasonable suggestion, but it would require some changes to the API, which we would rather avoid unless there's a strong need.\ngRPC implements support for load-balancing internally, including custom load balancing logic (godoc) -- if you were already aware of this, could you explain why that is not sufficient for your needs?\n. Great to hear, and thanks for following up.  Note that we will be changing this API in the next couple months to enable some additional required gRPC features like swapping the balancer at runtime based on service config updates.  There should be a gRFC out for this very soon (keep an eye out for pull requests in the proposal repo).  Migrating an existing balancer to the new API isn't expected to be difficult.. I think we may actually be able to do this without significant compatibility changes.  Reopening to investigate.. > Add an exported function to testdata package (testdata.Directory()?)\nNit/bikeshedding: how about Path()?\n\nFor example: var caFile = testdata.Directory() + \"ca.pem\"\n\nNote: prefer filepath.Join() to manual concatenation for cross-platform compatibility.\n. This same flake happened again:\nend2end_test.go:961: TestService/EmptyCall(_, _) = _, <nil>, want _, Unavailable or Internal\n. Thanks for the PR.\nWhat is the motivation for the change in the switches?  I believe the existing style is more preferred to avoid an extra level of nesting.\n. I see, thanks.  If you're going for consistency, how about changing the other ones to this style instead?  IMO and as I understand Go style, it's better to avoid unnecessary indentation.\n. Thank you for the cleanup PR.. Interceptors should really be CallOptions, not DialOptions, because they only apply to the calls themselves.  If we added a CallOption for interceptors, would that satisfy your needs?\n. This looks like #1233 to me.  Please see the discussion and proposal there and chime in if you'd like to take over the implementation from @ghasemloo or if you have other input.  Thanks!\n. This worries me a bit, because it means an import somewhere deep in a dependency tree could install an interceptor without you realizing it, and be able to do anything it wanted to.  Security-related issues aside, we'd probably also need to add support for multiple interceptors, otherwise things would break in hard-to-debug ways as soon as you tried to use more than one of these.\nIf you wanted this for your project, you could write a grpc-wrapper package that handles registration of DialOptions and wraps grpc.Dial() using those extra options, but I don't think this is something we want to put in the main grpc libraries at this time.\nClosing this request for the above reasons.  If I'm misunderstanding your request, please feel free to comment further.  Thanks for the suggestion!. Please explain in some detail what this issue is regarding to help us understand it better.  Thanks!\n. Can you explain more specifically what you are looking for?  I'm not sure I'm understanding exactly.\nI'm not as familiar with proto3 yet, but in proto2, it was possible to use tags in brackets on the fields in the proto and create a custom plug-in to generate your pb.go files with additional checks like I believe you are asking for.\nAlso, if this request isn't specifically about the service stubs being generated for grpc, then this would not fall under grpc's control.  It would be up to https://github.com/golang/protobuf to assess your request and implement.\n. Yes, what you have above is the normal/recommended way of doing this.  If you run into problems with that, feel free to comment here.  Thanks!. The server does not send an 'ack' back to the client at the start of a streaming RPC, so it is not possible to block and determine whether the server sent back an error at that time.  It is the user's responsibility to call Recv to receive the error.  Without significant changes to the API, I'm afraid there isn't anything we can do here.\nThis has recently been documented in the ClientStream interface here, and we plan to improve our documentation on github in the next quarter or so.\nThanks!\n. Nice work!\nUnfortunately, I do see something troubling.  From this PR:\n   50%     17.8378ms     17.7488ms    -0.50%  \n   90%     23.6225ms     20.0661ms   -15.06%  \n   99%     39.6329ms     24.1002ms   -39.19%  \n  100%     43.4342ms     29.7268ms   -31.56%\n\nAverage     18.2027ms     16.4898ms    -9.41%  \nObviously you didn't change anything that would impact performance, and these are very large differences.\nWe may not be able to use Travis for timing tests (which is probably fine; most changes shouldn't have a noticeable effect on real-world latency numbers), or we may need to find some way to simulate the clock instead.  ns/B/alloc per op looks stable, though!\n. Conflicts need to be resolved here -- you can remove the Stop/Reset happening at the top of the functions.  We reset the timer now right before the tests begin.. Should this be closed in favor of #1493?. Support for status details is present in the default server implementation (ref).  Are you using the http.Handler method for serving?. Using the standard set of codes is always the first thing you should attempt -- having a global set of codes that all applications in your organization support has many benefits.\ngrpc.ErrorDesc and friends all have \"deprecated\" in their comments and pointers to the status package.  There is also #1233 to make details a bit more friendly to use.  Do you have any other ideas that might help?\n. > I think the only real thing missing is the ability to add service-specific error codes, complete with string display\nIt's possible to use interceptors on the server and client to convert other types of errors to/from gRPC status errors by marshaling/unmarshaling them into the details.  If the status package is insufficient for your needs, this is what I would recommend.\nWith the Details/WithDetails added in #1233, I don't think there's anything else to be done here.  Let me know if you still have any problems or concerns.. @c4milo, what is your server returning?  It should do something like:\ngo\ns := status.New(codes.InvalidArgument, \"bad user\")\nif d, err := s.WithDetails(<detail proto message(s)>); err == nil {\n  return d.Err()\n}\nlog.Warning(\"!?\")\nreturn s.Err()\n@ghasemloo Hrm, maybe we should make WithDetails return the unmodified s on error.  The most common usage pattern is surely:\ngo\ns, _ := status.New(codes.InvalidArgument, \"bad user\").WithDetails(<detail proto message(s)>)\n// If an error happens, what can you do?  Just ignore it.\nreturn s.Err()\n. @ghasemloo, that would be great, thanks.  Yes, it's probably best for the user to log or inc a metric on error, but still return s.Err() from the server handler.\n. > or this just does not work using GRPC's HTTP handler.\nSorry, yes, that is it.  We did not add support for this feature through that path of serving yet.  It should be fairly straightforward to add it if you are interested in contributing (or anyone else, if you're not).. @c4milo Sure thing.  I can't remember why I didn't add this when I implemented status details in grpc's server.  But I don't think it was because it was going to be any more difficult.\n. FYI, #1438 has been merged (thanks, @c4milo), so we now support detailed errors with the http.Handler-based transport.. > Can concurrent goroutines share use of a single client object?\nShort answer: yes.\nThe only part of grpc-go that isn't concurrency-friendly is calling Recv (or Send) multiple times concurrently on a single stream.  (Recv and Send may be called concurrently with each other.). Are there still any problems related to this?  We've made many changes and bug fixes to flow control and the transport in general since the last update here.. It seems the leakcheck is actually just broken.  It is sensitive to the PC in each goroutine and not just the goroutine itself.  Logging \"interesting\" goroutines before and after a test, with a fake \"interesting\" one, yields (before):\n    time.Sleep(0x2540be400)\n        /usr/lib/google-golang/src/runtime/time.go:57 +0xb8\n    google.golang.org/grpc/test.leakCheck.func1()\n        /usr/local/google/home/dfawley/go-leakcheck/src/google.golang.org/grpc/test/end2end_test.go:4262 +0x30\n    created by google.golang.org/grpc/test.leakCheck\n        /usr/local/google/home/dfawley/go-leakcheck/src/google.golang.org/grpc/test/end2end_test.go:4263 +0x39\n    :true]\n\nand after:\nend2end_test.go:4276: goroutine 7 [sleep]:\n    time.Sleep(0x2540be400)\n        /usr/lib/google-golang/src/runtime/time.go:59 +0xf9\n    google.golang.org/grpc/test.leakCheck.func1()\n        /usr/local/google/home/dfawley/go-leakcheck/src/google.golang.org/grpc/test/end2end_test.go:4262 +0x30\n    created by google.golang.org/grpc/test.leakCheck\n        /usr/local/google/home/dfawley/go-leakcheck/src/google.golang.org/grpc/test/end2end_test.go:4263 +0x39\n\ninterestingGoroutines() should probably output a list of \"created by ....\" lines instead of full stack traces, and leakCheck() shouldn't store them in a map -- in case multiple were created and we have more of them after the run than before.  That should also allow us to pare down the list of \"uninteresting\" goroutine names that we're maintaining here.\n. Please resolve the conflicts and let us know when this is ready to review again.  Sorry, thanks.\n. Our next release will be on 8/30, which will include this PR.\n. It sounds like what you want is to perform a non-blocking dial, but set grpc.FailFast(false) on all of your outgoing RPCs.  Would this solve the problem for you?  Note that you can use grpc.WithDefaultCallOptions(grpc.FailFast(false)) to avoid needing to pass the CallOption every time.. Would this work?\ngo\nctx := context.WithTimeout(context.Background(), time.Second)\nc, err := grpc.DialContext(ctx, ..., grpc.WithBlock())\nif err != nil {\n  c, err = grpc.Dial(...) // Not blocking\n}\nIt's a bit of boilerplate, but you could factor it into a wrapper function...\nWe're going to be making some changes to the dialing process (and in the implementation of fail-fast AKA wait-for-ready) in the next several weeks to more closely match the specification and the C/Java implementations of gRPC, so I'm somewhat hesitant to make any other changes to it at this time, as it will only increase the chances that we will have to break something.\nNote that even fail-fast (non-wait-for-ready) RPCs are supposed to block if the ClientConn is in the CONNECTING or IDLE state (ref).. @MakMukhi will be working on the ConnectivityState API (#1245) and is estimating about 2 weeks for that work.\n. Closing this in favor of #1208 (#1245 was a dupe of it).\n. Per #1359, I'd rather not take this approach for now and implement the Connectivity State API instead.  That, or fixes to how fail-fast works in grpc-go (ETA 1-2 months as part of a big change with gRFC coming soon) should make this kind of feature unnecessary.  Thanks!\n. Without further clarification on this, it sounds like you want to have your interceptor only be called for certain services/methods.  The method name is passed to the interceptor, so your interceptor could match on that to control its behavior.\nE.g. https://godoc.org/google.golang.org/grpc#UnaryClientInterceptor\n. Server-side, the interceptor receives a UnaryServerInfo that contains the method name, so you could match on that to control interceptor behavior.\n. Yes, thanks!. grpc.Code(error) implements the last example, but is marked as deprecated.\nI am thinking that status.FromError(<non-status error>) could reasonably return a non-nil status containing {Code: codes.Unknown, Msg: err.Error()} and false.  If it did, it would be sensible to do a switch on status.FromError(err).Code().  Essentially, FromError would be a general error-conversion function, with the boolean indicating whether the provided error was already a status or if the returned status was fabricated.\nRight now, status.FromError(err) returns a status that represents a success if err is a non-status error, which could be misused and cause problems.. It is a behavior change, but if anyone is relying upon status.FromError(<non-nil, non-status error>) == nil (when it also returns , false), they're probably doing something wrong (mostly because (*status.Status)(nil).Code() == OK, which is in conflict with the non-nil error provided)...so I think this should be safe and probably even desirable.\nEdit: FWIW, all of the usages sourcegraph found are inside a ; ok check.. > (because status.FromError returns two values, that is generally the issue with returning two values in go :).\nI somehow keep overlooking this.  I'm now really wishing this general-purpose conversion function wasn't a \", ok\".\nHow about this?\ngo\n// Convert returns a Status representing err.  If err was not produced by this package,\n// it returns a Status with an Unknown code and a message of err.Error().\nfunc Convert(err error) *Status {\n  s, _ := FromError(err)\n  return s\n}\nThis would have more utility than Is (which also doesn't feel very Go-idiomatic to me).. There are a number of features that are not supported or available when serving via ServeHTTP(), and this is one of them.\nThe implementation for Drain() in grpc's transport sends a GOAWAY to the client.  In a multitenancy environment (ServeHTTP()), this would be inappropriate (if it is even possible), as it would affect the other handlers.  The gRPC spec has no other way to notify the client that it should stop sending new RPCs.\nThe right way to do what you want is to perform a graceful stop on the http server (using Shutdown) instead of the gRPC server.  Alternatively, you can use the standard Serve(net.Listener) function instead of ServeHTTP(), which has many advantages (e.g. performance).\nWe may look into better support for the HTTP server handler model, possibly by hijacking the net.Conn away from the http server, but it's not straightforward.\nIn terms of what to do with ServeHTTP()+GracefulStop() (because I don't particularly like the panic, either), I'm not sure there is much we can do.  If we don't panic, there is no other way to report an error back to the caller.  And we don't return an error, so if we silently can't perform the operation (tell the client to stop sending new RPCs), then that would be bad, too.  So it seems panic is the only option.. Thanks for the report.  We discussed this and found that the assumption is that a context deadline/cancellation error originates because of the client, so there's no need to notify the client of the error, but in the case of InTapHandle returning a different context, that cannot be safely assumed.\nWe need to determine what the right error to return is in this case before moving forward.  DeadlineExceeded / Canceled (conditionally) seem like obvious front-runners, but if the client didn't set a deadline or cancel the RPC, are those appropriate to return?\n. To fix this, we will remove the s.ctx being passed to the wait() for acquiring the transport's writable channel.  This will ensure that we always write the status even when the context is cancelled.\nhttps://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L762\n. Also, related, it looks like we latch the context before the InTapHandle into the receive buffer, which means that a streaming RPC that is blocked on Recv will block longer than the TapHandler's context (or indefinitely if no deadline is set on the RPC).\n. I am working on this, and assuming all goes well, expect to be done in a day or two.. Thanks for the PR.  This has been bugging me for awhile: all \"grpc-\" headers should be reserved, not specific ones.  I created #1391 to do that instead.. FYI: we merged #1391, but are temporarily reverting it, because this header is needed by tracing code.. I'm playing around with an API change that would improve performance of this.  The idea is to make a CallOption for metadata, so adding extra metadata is as easy as injecting an extra call option.. FYI: This is being reverted temporarily while we implement a solution to allow stats handlers to set the grpc-tags-bin and grpc-trace-bin headers.  #1400. Yes.  This PR is a breaking change.  grpc-gateway needs to be updated to use metadata.FromIncomingContext or FromOutgoingContext, whichever is appropriate based upon its usage.\n. We don't have a policy of bumping the major version number on API changes.  gRPC 2.0 is reserved for when wire protocol changes are made that will require a new version of gRPC across languages.  We have changed our API in 1.x and expect to do so in upcoming releases as well.\nThis change was broadcast months in advance through all means available (grpc-io mailing list, proposals repo, here).  And it was listed under \"API Changes\" in the release notes of 1.6.  Sorry for the breakage this caused, but I don't think there's anything else we could have done here.\n\nEDIT: I locked this thread because I don't think it's productive to have any further general discussion about versioning or API breakages here.  Please file a new issue against the repo if you want to discuss our policies/etc.. As an update to my comment above:\n\n\nI was mistaken.  grpc-go 2.0 may happen before there is a non-backward compatible wire protocol change.\n\n\nWe have no intentions to break backward compatibility in 1.x, although there are circumstances in which we may need to do so.  Please see https://github.com/grpc/grpc-go/blob/master/Documentation/versioning.md, and please file new issues or comment on #1583 if you have concerns.\n. Also, as discussed, please move all the go away handling code into handleGoAway.. Please comment if you still believe this is a leak after making sure to close the clientconns properly.. The details field in the Status proto is a \"repeated Any\", which can hold any arbitrary serialized data you would like, and includes a string field to specify its type.\n\n\nhttps://developers.google.com/protocol-buffers/docs/reference/google.protobuf#google.protobuf.Any\nI believe this should satisfy your requirements for custom data types.  If not, could you please explain why?  Thanks!\n. If you control the clients and servers, you can put literally any data in the details field you wish -- including raw JSON error strings directly -- and/or, you can serialize/deserialize them by any means you choose.\nIf that still doesn't meet your needs, you can also define your own custom trailer fields for your specific detailed error data and types.\nAdding a feature like this would require a gRPC spec change and cross-language support, which would first require a gRFC to be submitted and accepted.  If you feel strongly about this, feel free to send your proposal to the grpc.io forum for discussion.\n. I think the docstring here is sufficient to at least point users toward the http package so they can figure out the rest.  If more help is necessary, a user manual or examples would be more appropriate IMO.  Although, TBH, with the downsides of this method of serving, I would like to discourage its use by all but the biggest power users that already know how this stuff works.\n549 did specifically mention adding usage examples.  @bradfitz, do you still have the examples referenced in #549?:\n\nThe examples were removed from earlier versions of #514 to reduce the size of that change.\n\nThanks!\n. > The words \"may\" and \"significantly\" in your proposed addition are at opposite sides\n\nof the certainly spectrum.\n\nI don't see these as opposites.  \"May\" describes certainty, while \"significantly\" describes severity.\n\nIf you want to cite actual features they'll lose, that's fine. But which?\n\nKeepalive, keepalive enforcement (ping flood prevention), max connection age and idleness detection, flow control settings and optimizations (e.g. BDP estimation and large message optimizations), stream-level flow control tied to application reads, GracefulStop(), InTapHandle(), stats hooks, status details, and possibly others.\nI'd rather keep the FUD levels high here until such time that this path is well-understood and well-supported by the maintainers, which is currently not the case.\n. > What is \"keepalive\"? Go's HTTP/2 server keeps connections alive too, by default.\nSettings in gRPC to control pings to check liveness (ref), and matching server enforcement rules.\nEven if the http2 package supports idle timeouts, we aren't configuring them right now in gRPC.  And AIUI, we (within gRPC) can't if we're going through the ServeHTTP method of serving, because we don't have access to the server itself with this mechanism -- unless we hijack the net.Conn away from the server?\n\nServeHTTP \"may\" only suck more. I'd argue in a number of ways that Go's http2 server is more solid.\n\nI'm not making any value judgments.  But for now, the team is working almost exclusively on the gRPC transport, not the http2 one.  We've repeatedly heard that performance is a problem with gRPC through a handler, but we haven't had time to look into this, nor do I expect we will in the next quarter or two.  So for now I'd rather lead people (strongly) toward using our server.  If our server 1 year from now just fires up x/net/http2, then that would be great, but that is not the current state of things.\nI'm OK with the text of the paragraph that is there, but I would like to add a sentence to let people know it is not (\"well\"?) supported right now.\n. @bradfitz asked me to finish this off, as he won't have enough cycles to do so personally.  I'll submit it and add the extra sentence I was requesting.  Thanks for the PR.. Thanks for the preview.  For the future, there is no need to create a PR until you have something you are ready to submit -- for previews, you can point people directly to a branch in your fork on github.\n. Could you please look into the test failures?  Thanks!. Can you provide a little more background into why this change is necessary?  grpc should be ensuring Start() is called and completes before calling Notify() -- and if it's not, that's a bug.. Notify is not intended for use outside gRPC.  Otherwise, updates from the resolver could be lost.  Why do you need access to this channel?. @markdroth interesting, thanks for calling this to my attention.  I think this change is OK for us as long as nobody accesses the peer while the RPC is happening.  Our API is synchronous, so this would be unexpected.  For retries, it would end up being set to the last attempt, which is presumably what we want.  For hedging, we will have to move things around a bit, but this won't make things much worse than they already are in that regard.\n. >\n\nSince len(b) returns a int. It seems wrong because length > math.MaxUint32\nwill always returns false.\nGood point.  Only on those systems where \"int\" is 32 bits.  Presumably\nthose systems can't have slices bigger than 4GB, so I suppose this is\nfine.  Why not leave \"length\" an int (instead of a uint, with casting),\nthen?\n. @mehrdada the compressor itself should not need to worry about this.  The writer it uses should give it an error back when it overflows.  (Similarly with decompressor and the reader pulling from it.)  This will happen eventually, but we need to make a lot of changes here and I'd like to make them in phases.. > Not Done. I only removed part from client. Server will receive a string\nof compressor name and it may not be set by UseCompressor.\n\nThe idea is to fail this way if the user made a call with UseCompressor\nand had not registered a compressor with that name.  It shouldn't matter\nwhether they also had done WithCompressor when dialing -- if they ask for\na specific compressor but forgot to register it, we should error.\n\nAlso, for the end2end_test.go/TestCompressServerHasNoSupport, if the\nserver register \"gzip\" during init(), this test won't pass. Do I need to\ncopy gzip compressor and name it \"gzip_tmp\" for this test?\n\nTo make this test work, the client will need to use a compressor with a\ndifferent name.  Because of the global registry, the only way to do this is\nwith the WithCompressor DialOption (or have different binaries for\nclient and server, but that's out of scope).  I would just make a dummy\n\"nop\" compressor that does nothing and set it with WithCompressor to make\nsure the client sets a compressor the server does not support.\n. Sorry for dropping this.  I have decided we should go ahead with this as-is -- any changes to introduce conditional compression will have to be separate from this mechanism, which is too low-level to perform that function effectively.  Please resolve the conflicts and I'll give it a final pass.  Thanks!. Can you rebase again?  Travis is failing because one of our dependencies is offline (we have a commit that removed the dependency a few minutes ago).\n. @dzbarsky FYI, we will be changing the connectivity API slightly in this PR (to support upcoming work revamping the resolver and balancer logic), in case you are using it already.\n. Anything that applies at call time should ideally be a CallOption, with WithDefaultCallOptions used to specify things as defaults per ClientConn.  I'm not sure what it would take to do the work, but I would happily review, approve, and appreciate a PR to do this (with any DialOption for that matter).\n. Let's track the wait for ready bug via #1463.. Ping -- could you please update this so that Travis passes (gofmt/goimports)?  Thanks.. Looks like there are still compilation problems.  Note that you can enable travis on your fork of grpc in your repo via https://travis-ci.org/profile/dzbarsky so you don't need to push updates to a PR to get travis to run.\n. @dzbarsky, do you plan on picking this back up soon?  Thanks!. We don't have the cycles to pick this up, and may not for some time.  Let's keep tracking using the issue (#1431).  Thanks!. This is a bug.  See PR #1433 for a possible fix.  However, we still aren't completely sure if using the port from the host is correct, or if we should use the target's port instead (which is what the code was attempting to do before).. There are no plans to change this beyond what #1433 did.  Please comment here if you think there's any issues using it as implemented.  Otherwise, I'll close this, as the PR has been merged.  Thanks.\n. We will not be backporting it unless there is an urgent need.   We will be doing the 1.6 release next Wednesday.\n. LGTM, thank you for the PR!. Thanks for finding this and for the fix!\nIt looks like the second commit was done by xuechen.wzm -- if you redo this commit with your other username and git push -f, I believe that should take care of it.. I'm not sure if this will work.  I thought that since there was a commit by another author, it would say you hadn't signed the CLAs.  But it's not complaining at the moment.\nI'll be squashing and merging anyway, and since both authors are you, LGTM and let's hope it actually works.  Thanks again for the fix.\n. Probably the best way to do this kind of check would be to implement this yourself in either your server handler or an interceptor.  Note that there could be races where the client disconnects and attempts to reconnect before the server notices the client has disconnected.  This would result in the client's reconnect being rejected until the server notices the first connection has been lost.  Note also that this will only limit one client per server, so if you have multiple instances of this server, the same client could connect to all of them and initiate a SendABC and SendXYZ on each one.\n. Per the gRPC spec, Unavailable is expected to be returned when the server shuts down or when some data is transmitted before the connection breaks.  However, the Wait For Ready spec does not specify what kind of failure should be returned if there is an error attempting to issue an RPC to a ClientConn for which there is no current connection available.  Unavailable seems like the most reasonable choice for this condition, but it does prevent this kind of use case, which is important.  It's also important to have a different error code (and not just string matching or a sentinel error), because gRPC's retry mechanism will be based upon the code alone.  We'll follow up with other gRPC implementations and see how they handle this.. Actually, I think the root cause of this issue is that our non-Wait For Ready / FailFast behavior is wrong.  We are supposed to block if there is no active connection (i.e. the ClientConn's state is Idle or Connecting) until the context expires:\n\nRPCs SHOULD NOT fail as a result of the channel being in other states (CONNECTING, READY, or IDLE).\n\nAt that time, the error returned should be either Canceled or DeadlineExceeded.\nIf you turn off Fail Fast (i.e. if you enable Wait For Ready), then RPCs will always block until the context is canceled or expires, or the ClientConn is closed.\nIf you are maintaining multiple connections to multiple backends, you should do that within a single ClientConn, and gRPC should always pick one that is up for your RPCs.\n. Let's track the wait for ready bug via #1463.  Closing this as a duplicate of that, since it is the root cause.\n. > I don't understand how #1463 is the root cause of this issue.\nAre you using a load balancer?  If so, depending on the implementation (our RoundRobin balancer does this), it will return an error when there are no backends here:\nhttps://github.com/grpc/grpc-go/blob/fa59b779e8729b2ae0d0cfca12a91220f690fa6f/clientconn.go#L866\nIf you are not using a balancer, then it does seem that we follow the right behavior (though I was fairly sure I observed the opposite).\nIs your connection actually in the transient failure state, possibly?\nNote that when retry support is added (ETA ~1 month), gRPC will transparently retry RPCs that didn't leave the client or were not seen by the server application.  So this should not be something you need to distinguish externally after that time.\n\nwe need to choose which backend(s) to use on an RPC-by-RPC basis. \nIs there some way I'm not seeing to configure gRPC to do this?\n\nYou should be able to implement a custom balancer to do this if round-robin is not suitable for your needs.  (Note that the API is going to change around here pretty significantly in the next month or two - see #1388.)\n. In theory, the errors we return are supposed to match the spec.  (We are aware we don't get this right in all instances.)  The spec doesn't specify what should be returned in this particular situation.  I'm checking with C/Java.\nThis leaves us with either a gRFC to change the error (if it's UNAVAILABLE in the other languages as well), which could be tricky, or a Go-specific feature to help identify whether data was transmitted before any error occurred.\nI'll reopen this, because it seems it's not related to our \"wait for ready\" implementation.. > does the error code from the inner RPC get propagated back to my client or does it get turned into Unknown?\nThis is entirely up to the implementation of the middle service. If it does:\ngo\nfunc MyHandler(...) (..., error) {\n  if _, err := innerSvc.Call(...); err != nil {\n    return err\n  }\n}\nThen the status code will be propagated.  This is not typically recommended; the middle service should translate the error into something that makes sense to its client -- usually by switching on the status code returned from the inner service.. It looks like clients are only waiting for a connection to be made and for the client preface and a settings frame to be sent to the server -- never waiting for the server to send a valid settings frame back -- before attempting to use the connection.  It may make sense to wait for that settings frame before using the connection.  We'd need to do this through a DialOption, because this will break cmux users that don't have the \"workaround for java\" in place.\nFurther, we noticed there are no deadlines on the reads/writes happening during connection initialization, which is problematic -- we should set these to the deadline of the context during this phase.\n. cc @vtubati\nThe changes to implement this are not significant, but we have higher priority things in flight right now.  I expect this to be done within a month.. Thanks for the ping.  We should hopefully be able to have this done by the end of next week.. We haven't made much progress on this, but it's at the top of our priority list.  Also, we have a slightly different plan:\n\n\nAs before, add a DialOption to block in the client until a settings frame is received from the server before sending RPCs on the connection.\n\n\nThe new part: even if the option is not set, do not consider the connection \"good\" (and consequently reset the backoff timer) until a settings frame is received from the server.  We would still proceed to send RPCs on this connection immediately, as we do today.  But if a failure occurs before a settings frame is received, we will resume connecting to alternate backends and backoff with the same deadline as if the initial server never connected at all.\n. Note: this is PR #1648 if you are curious.. If I understand your concerns correctly, then I believe it should be fixed for everyone.  We will not consider a connection \"successful\" (from a backoff perspective) if the server never sent the HTTP2 preface to the client.\n\n\nThe option is there to prevent RPCs from being assigned to the channel until after the handshake has been received.  This can be set if you want extra-stable behavior so RPCs don't fail due to a connection that fails in this way.\n. This is definitely possible, but it would take a moderate amount of redesign to implement.  I don't imagine this is something we would be able to get to in the next ~6 months, but it is something we'd like to do eventually.  In the meantime, one workaround is to create multiple connections and send different workloads on each by priority, and leverage external QoS to prioritize them.\nNote that we don't transmit RPCs serially in the order in which they are initiated.  Small requests that start after large ones should typically be able to complete before the large requests.\n. Thanks for the contribution!  We discussed this internally and decided we preferred something more like PR #1454.  Please take a look and comment if you think that won't fix the problems you are encountering.\n. #1498 resolved this for unary calls; leaving open to track the same problem while doing handshaking and sending the client preface.\n. Actually, let's use #1444 for that half.  Closing this issue.. Specific areas where we would like to do this:\n\nCodec output buffer (requires codec API extension)\nReading from transport (I believe this is #869)\n. My thinking for the codec was to add an optional method to it that, if implemented, would be called when the buffer it returns from Marshal is no longer needed by grpc.  Then the codec could re-use that memory.  Without something like this, it's impossible for the codec to safely reuse old memory buffers.\n\nThe http2 transport uses byte buffers to hold data it reads from the network; this can and should be re-used with a pool as well.. cc @ejona86 . grpc-go combats this by first sending a GOAWAY with MAX_INT32 as the stream ID.  Upon receiving that, the client should not initiate any new streams.  We know that the client received the first GOAWAY by sending a ping along with the GOAWAY and not triggering the second GOAWAY (or closure if no active streams were created yet) until the ping ack is received.. As part of this fix, we should see what the Go client is doing in the same situation -- will the client close the transport when it is done with it after having received any GOAWAY, or are we keeping it open until the server closes it?. @dzbarsky can you provide any extra info to help us track down the source of the bug?  Do you see this when you call RecvMsg?\n. Ping - are you still seeing this?. Tracking with #1469. We will tentatively leave grpclb in the grpc package for now.\nUsers that wish to implement a grpclb server will need to import the .pb.go files from grpc (see grpclb/grpclb_test.go for an example).\nHowever, we may wish to change this in the future, as the current situation is not ideal.\n. Looks like Travis doesn't support Go1.9 yet:\n$ GIMME_OUTPUT=$(gimme 1.9.x | tee -a $HOME/.bashrc) && eval \"$GIMME_OUTPUT\"\nI don't have any idea what to do with '1.9.x'.\n  (using type 'auto')\n$ export GOPATH=$HOME/gopath\n$ export PATH=$HOME/gopath/bin:$PATH\n$ mkdir -p $HOME/gopath/src/google.golang.org/grpc\n$ rsync -az ${TRAVIS_BUILD_DIR}/ $HOME/gopath/src/google.golang.org/grpc/\n$ export TRAVIS_BUILD_DIR=$HOME/gopath/src/google.golang.org/grpc\n$ cd $HOME/gopath/src/google.golang.org/grpc\n0.02s\n$ gimme version\nv1.0.0\n$ go version\ngo version go1.7.4 linux/amd64\nBut, https://github.com/travis-ci/travis-build/pull/1148 just added it.... Thanks for the PR.  Would you mind changing the doc to use this instead?\ngo generate google.golang.org/grpc/examples/helloworld/...\n. Looks good, thanks!. The issue with this change is that it drastically increases allocations and copying for large messages without compression enabled (the justification for the optimization it rolls back).  We should be able to do better for both use cases.  @MakMukhi is working on such improvements, but I don't believe we can take this PR as-is in the interim, given the significant cost it adds to non-compression workloads.\nIf you need a short term improvement, maybe return an empty header slice from encode(), and include the header info in the data buffer when compression is enabled?. @irfansharif,\n\nThe cleanest answer for both ends I think would be the presence of a msg.MarshalTo([]byte)\nhelper (with a corresponding msg.Size() but unfortunately not currently present in golang/protobuf\n(is in gogo/protobuf however).\n\nA MarshalTo(io.Writer) would work, too, although readers/writers have their issues due to the extra copying they require.  Even though the default proto library doesn't support that, we have been looking into extending the codec API so that one could take advantage of such features of other codecs.\nAlso, given the existing codec API, we're likely going to add an optional Free() method to it, so that we can recycle the buffers themselves, which should be a big win for large message workloads.  (This would require much of the code you proposed removing in #1478 for the proto codec.)\nRight now, @MakMukhi is working on a pretty big change of the write path to fix a correctness issue (#1453).  We need to see how that shakes out a little more before deciding what the next set of optimizations will look like, exactly.\nIn the meantime, I'd be happy with a PR that, in encode(), combines the header and payload only when compression is enabled.  I think if they are combined into the header, with the payload nil, then the transport Write methods will not allocate any additional memory with compression enabled.  For non-compression cases, we could keep a buffer pool in the transport, and instead of appending to the header in place, copy the header and the first part of the payload into a recycled buffer.\nThanks again for your PRs and interest in helping.  We're more than happy to accept the help, but please check with us before spending much effort to make sure it's in agreement with the direction of the project.\n. Good catch, thanks for the report.  The goroutine should block on either doneChan or ctx.Done to avoid blocking indefinitely.  Or lbwatcher could close doneChan as it exits if it hasn't already (The channel returned by Notify() should close when the balancer is closed).\n. Actually, lbWatcher already does close doneChan as it exits:\nhttps://github.com/grpc/grpc-go/blob/master/clientconn.go#L584\nThis was fixed by #1424.  The fix was not in 1.5.2 (1.6 will be released on Wednesday, so we do not plan to backport this to the 1.5.x branch).\n. Fixed by #1490 . Already in progress: #1475\n. > Also change/remove this: https://github.com/grpc/grpc-go/tree/master/examples#prerequisites?\nNice.  I only grepped for 1.6, not 1.5!  Done.. Another option for this would be to make the CallOptions implement various interfaces that allow the interceptor to retrieve the data from the CallOption.  This approach would allow us to add/remove support for introspecting CallOptions in the future.  I have something like this kicking around in a branch of my fork:\nhttps://github.com/dfawley/grpc-go/blob/metadata_callopt/rpc_util.go#L157-L181\n. Please rebase - this seems to include #1445 itself, too.. > Please try to make all the RPCs non-failfast by using grpc.FailFast(false) call option. (You can set default call option for a ClientConn using WithDefaultCallOptions)\nNote that if you're doing unary RPCs, then this could be dangerous right now if they are not idempotent because of #1532.  I revived my branch that fixes this problem and hope to have it checked in this week.\nEDIT: added \"if they are not idempotent\". > I think this is not suitable for a production use case, should be disabled by default.\nWe agree; tracing was disabled by default in #1509.\nWe may delete the global entirely and just provide a stats handler package the user can install if they want net/trace integration.. The overall gRPC team dictates our versioning system (frequency and numbering), and it is not based on semantic versioning.  I think a fair request would be for us to document our versioning strategy.\n. Fixed by #1537. Yes, definitely a dupe.  Thanks for the report.\n. This proposal would mean changing the generated code to fill in the new field.  This means new generated code would not be compatible with old gRPC libraries, but new gRPC libraries could support older versions of the generated code.  That seems reasonable, but this isn't something we will be able to prioritize for at least a quarter.\n. Thanks.  This needs gofmt run on it, and it may need a rebase to fix some code for a new golint check.\n. I'm not sure how you did your merge, but there are way more diffs in here than what you changed yourself.  Can you fix that, or if you're having trouble, maybe start from a new branch (sync'd to our latest upstream/master).\nFWIW, I recommend rebasing your branch instead of merging the master commits on top: https://www.atlassian.com/git/tutorials/merging-vs-rebasing.  The commit log in github gets a little weird when you do that, but it keeps things more sane in the diffs themselves IMO.\n. It looks like your server's Chat handler is only attempting to echo the message back on the same stream that it is serving.\nYou need something in your server that shares the message with all the other connected streams as well.\n. @theobouwman that would be up to your implementation.  The basic idea is that in the server, there is one call to Chat() outstanding for every open stream.  You need to broadcast messages from each invocation of Chat() to every other one.  I.e. each Chat() should have a background goroutine subscribed to messages coming from other Chat()s and calling Send() for each one received, and a foreground goroutine calling Recv() and publishing the messages it receives to the other Chat()s.  Hope that helps.. This would be covered by retry support (gRFC A6), but it isn't otherwise a part of gRPC.. > Conflicts...\nResolved.\n\nAlso, should we label this PR as an API change? Even though it's an internal API change?\n\nI don't think so.  I think the labels on PRs should be applied so that end users can understand the release notes.  If we put non-user-impacting API changes in with ones that do impact users, that dilutes the significance of the section.\n. This is a known issue #1444.. Thanks for filing this.\nWe looked into net.Buffers a month or two ago, and IIRC, we discovered that only net.TCPConns could end up taking advantage of this.  If transport-level security is employed, that's done with TransportCredentials, whose handshakers return a wrapped net.Conn.  So I think to make this work, we'll need to do something higher-level.  I.e. if the net.Conn in use supports \\<some interface> then call that method for writes.  Then those wrapped net.Conns can implement that method and use net.Buffers.WriteTo when they write to the raw net.TCPConn they (presumably) wrap.\nOh, or we can wait for https://github.com/golang/go/issues/21676, which I just read (thanks for the link). \n That looks promising.. So if we have to copy it at all, then what is happening inside the bufio.Writer is already ~optimal, right?\nMaybe this should be a feature request for the http2 package, then, to support net.Buffers?\n. We talked about this today and agreed that there is no way to make effective use of net.Buffers.WriteTo() -- this has to be done internal to the framer.  Note that if you are using any kind of transport encryption, because of your [3], above, the wrapped net.Conn's writev() can't be used currently.  Also, with encryption, even once [3] is fixed, you may still need to either encrypt many smaller buffers in the wrapped writev() or concatenate them, which would require more memcpy()s.\nTo reduce the number of syscalls, you may take advantage of the new functionality in #1544.  We'll probably be increasing the default to 128KB (from 32KB) as well, as we think this setting will be optimal for most of our users.\n. My fault for not keeping up with this.  I've created a replacement PR for it at #1813.\n. Will be fixed by #1558 . Something like this happened again in last night's Go1.6 cron build:\npanic: close of closed channel\ngoroutine 821 [running]:\npanic(0xa3f9c0, 0xc820010660)\n    /home/travis/.gimme/versions/go1.6.4.linux.amd64/src/runtime/panic.go:481 +0x3ff\ngoogle.golang.org/grpc.(grpclbBalancer).Start.func2.3(0xc8205c6a90, 0x1, 0xc820158480, 0xc8204dd0e0)\n    /home/travis/gopath/src/google.golang.org/grpc/grpclb.go:486 +0xbc\ncreated by google.golang.org/grpc.(grpclbBalancer).Start.func2\n    /home/travis/gopath/src/google.golang.org/grpc/grpclb.go:488 +0x15af\nFAIL    google.golang.org/grpc/grpclb   12.427s\n. Still will be fixed by #1558.. I'm not familiar with the Java API.  You say you're using the async task, and you have a shutdown call in onPostExecute, but your stub is created using newBlockingStub.  Is this correct?\nAnother thing you could try is setting the MaxAge values to something very short to ensure that the keepalive mechanism is properly killing the connections from the server side.. @sandeepkdpl, sorry we lost track of this.  Are you still seeing problems related to this issue?\nTo answer your question above, setting MaxConnectionAge to something short like 1 second would cause the server to only accept new RPCs from the client for 1 second, then put it into a \"draining\" state where the existing RPCs would be processed until MaxConnectionAgeGrace, at which point the connection would be hard-closed.. That's great, thanks!. Since it kills the whole transport, any errors here are probably fine to log instead of returning them to the caller of Send/Recv.  So let's log them.\nAlso, we are not capturing any errors when writing the headers here:\nhttps://github.com/grpc/grpc-go/blob/6f3b6ff46b28cdc2e871df2e1176e48d4204bd33/transport/http2_client.go#L1200-L1202\n. Can you try using ReadBufferSize and WriteBufferSize (and the corresponding DialOptions if you're trying to optimize a client) to increase the size of the IO buffers to 128KB or 256KB, and let us know if that makes any difference?. Dialing for every RPC adds a significant amount of syscalls and network round-trips, and would be expected to impact performance.\nIf your total transmission size for each connection is only 1KB, then changing the read/write buffer sizes won't help.  Also if you only do unary requests and responses of that size, one at a time on a connection, it won't help either.  But, you can share one ClientConn across many goroutines, at which point the buffer sizes would be helpful to increase, even with small messages.  (I would recommend doing this.)\nDo you still have concerns related to performance after making these changes?. See #711 for some discussion on why we are not going to do this until Go 1.9 is grpc-go's oldest supported version (tl;dr: we need everyone to have type aliases to avoid breaking backward grpc/protoc compatibility).. Rolling back the PR that removed 1.6 support is obviously trivial.  However, there is an ongoing maintenance cost in the form of fragmented (conditionally-built) code and needing to maintain workarounds for Go 1.6 bugs (notably https://github.com/golang/go/issues/15078).\nIs vendoring the previous release not an option for you if you need 1.6 support?. > How about some nominal support for 1.6 - that is, still drop \"support\" for 1.6, but any bugs that arise are fixed with only a best-effort guarantee.\nSuch a change wouldn't actually help with any of my maintenance concerns.  We'd have to revert the PR that dropped 1.6 compatibility and continue to maintain a more complex code base.  I also fully expect our dependencies to drop 1.6 compatibility in the near future (x/net/trace broke 1.6 builds a few months back, but reverted it, and IIRC there was a comment somewhere that said they were going to roll forward after the final 1.9 release).  If our dependencies don't support 1.6, then we can't.\nI don't really see any compelling reasons to maintain Go1.6 compatibility at HEAD.  Anyone that wants to run with 1.6 can pretty easily vendor to the previous grpc-go release (1.6, interestingly enough).  Is there something that I'm missing here?\n. We debated this for awhile and decided to roll back the change early next week and do one more release with Go1.6 support (in 6 weeks).\n. @jba, this is discussed in #711, and we will not be doing this until our oldest supported version is 1.9 (see: my comment).  Thanks.. FYI, gomock no longer works with Go1.6 either: https://github.com/golang/mock/issues/116\n. Note that in 21 days, and after our next release (Oct 23rd), we will be dropping support for Go1.6-1.8.\nSince this thread is locked, head over to issue #711 for related discussion if desired.\n. This is honestly the first I've heard of this, so thanks for filing this.  It looks like something we probably will implement, but don't have any plans to do so in the current quarter.\n. We will re-add go1.6 support as a short-term workaround for AppEngine users still stuck on 1.6.  #1580 \n(The transport package was not intended to be used outside the grpc package, so hopefully this dependency is indirect.)\n. Can this be closed now?. /cc @jayantkolhe . I believe this is the same as #1532.\nAlso, we are implementing retry logic in gRPC itself, hopefully it will be done in the next few weeks.\nhttps://github.com/grpc/proposal/blob/master/A6-client-retries.md. > You mean implementing retry policy? gRPC-go has naive retry support with failfast=false on connection issues as it is today, no?\nTechnically, yes, it does currently have retry support, but that is actually a bug.  Non-fail-fast is not supposed to retry; it's supposed to wait until a connection is available, and then send the RPC.  It will be fixed to follow this behavior, and we will also be adding transparent retry support (as part of gRFC A6) if we are sure the request was not received by the server.\n. Thanks for noticing that problem with the doc.  #1586 fixes it.. Yes, I'm hoping to have that done in a week or two.\n. I signed it?. Actually, this wasn't doubling performance, it was more like a ~30% slowdown with header decoding enabled.  Also, I didn't fully understand what was meant by not decoding the headers (because I assumed the http2 library always did some decoding on its side), so now that I understand that it can be completely disabled, this seems reasonable.. @petermattis Are those latest numbers with gRPC or your x/y implementations?\nI'm assuming the latter because of this comment:\n\nUnfortunately, those changes don't move the performance needle at all for gRPC.\n\nSo you're saying optimizations in header decoding doesn't help gRPC at all?  If so, that probably means the bottleneck is in the sending path right now.\nMaybe we can turn this into an issue/PR against http2.  Reusing header frames seems like a useful feature for everyone.. Thanks for the cleanup change!. With the Go Custom Transports proposal, we will no longer have a method like this.. Ping: rebased & resolved conflicts.\n. Rolled back by #1619. You can specify your own Dialer using https://godoc.org/google.golang.org/grpc#WithDialer, which would be the preferred way of doing exactly this.  Thanks for the submission, though!. Sounds good, thanks @rakyll.  @broady, any input on OpenCensus's Go1.6 support?. ~~Also, are you setting DATASTORE_EMULATOR_HOST in your environment?  If so, it looks like that specifies the target for grpc to connect to -- can you share its setting?~~\nEDIT: Sorry, nevermind that question - I see this is for debugging and there is a different path in another package that calls grpc.Dial if it's not set.. How many of these errors do you get?  If it's not many, than one or a few of your clients had an error connecting to the server.  This makes your RPC fail, because \"fail fast\" is the default behavior for gRPC.  You can fix this by doing grpc.Dial(..., grpc.WithBlock()), which will wait until a connection is established before proceeding, or c.SetRequest(..., grpc.FailFast(false)), which will make your RPC wait until the connection is ready before failing.  I recommend the latter unless you know you want fail-fast behavior, because if an intermittent problem happens in the middle of the connection, the latter would recover better.\nAlso, this is creating 200 simultaneous connections to the server.  Do you want this?  You can do many simultaneous streams over a single connection.  I.e.\ngo\nconn, err := grpc.Dial(\"localhost:8800\", grpc.WithInsecure())\nc := pb.New<Something>Client(conn)\nfor i := 0 ; i < 200 ; i ++ {\n  go func() {\n    r, err := c.SetRequest(context.Background(), t)\n  }()\n}\nconn.Close()\nAlso, I highly recommend setting a deadline on every RPC, always:\ngo\nctx, cancel := context.WithTimeout(context.Background(), time.Second)\nr, err := c.SetRequest(ctx, ...)\ncancel()\nEDIT: Call ClientConn.Close(). Also, @MakMukhi noticed that you are also not closing the ClientConn in your example before it leaves scope.  You should make sure you call ClientConn.Close() when you are done with the connection, or it will leak resources.\n. $ go test -benchtime=3s -bench=Context benchmark/primitives/*.go\ngoos: linux\ngoarch: amd64\nBenchmarkCancelContextErrNoErr-12               100000000           52.2 ns/op\nBenchmarkCancelContextErrGotErr-12              100000000           51.9 ns/op\nBenchmarkCancelContextChannelNoErr-12           200000000           19.9 ns/op\nBenchmarkCancelContextChannelGotErr-12          100000000           36.1 ns/op\nBenchmarkTimerContextErrNoErr-12                100000000           53.1 ns/op\nBenchmarkTimerContextErrGotErr-12               100000000           52.6 ns/op\nBenchmarkTimerContextChannelNoErr-12            200000000           20.5 ns/op\nBenchmarkTimerContextChannelGotErr-12           100000000           37.0 ns/op\nBenchmarkCancelNetContextErrNoErr-12            100000000           51.8 ns/op\nBenchmarkCancelNetContextErrGotErr-12           100000000           51.7 ns/op\nBenchmarkCancelNetContextChannelNoErr-12        200000000           20.3 ns/op\nBenchmarkCancelNetContextChannelGotErr-12       100000000           36.1 ns/op\nBenchmarkTimerNetContextErrNoErr-12             100000000           52.5 ns/op\nBenchmarkTimerNetContextErrGotErr-12            100000000           53.4 ns/op\nBenchmarkTimerNetContextChannelNoErr-12         200000000           20.5 ns/op\nBenchmarkTimerNetContextChannelGotErr-12        100000000           36.7 ns/op. I just realized there's no point in benchmarking x/net/context, at least not with more work:\n\nFor Go versions > 1.6, x/net/context is implemented by context (and as of 1.9 it is a type alias)\nFor Go 1.6, this benchmark cannot be run, because it imports context\n. For uses not including Details, this is fairly easy:\ngo\ns, _ := status.FromError(err)\nnewErr := status.Errorf(s.Code(), \"more info: %v\", err)\n\nIf you may have details in your error, then it's possibly unintuitive, but not really onerous.\ngo\np := s.Proto()\np.Message = fmt.Sprintf(\"more info: %v\", err)\nnewErr := status.ErrorProto(p)\n. Please let us know if that isn't sufficient for your needs.. Did you do go get -u google.golang.org/grpc?\nEDIT: (Note the -u). > Shouldn't it be && \"$GOARCH\" != \"386\" ]] ?\nYES.  Good catch, thanks.  Updated.. I highly recommend not setting any window size options.  These settings are intended for performance tuning only, and are generally not necessary now that we automatically adapt the window size based on the connection's BDP.\nThis API in Java is an application-level control only, and probably isn't doing what you think it's doing.  It only controls when new messages are delivered to the application's asynchronous callbacks from grpc, and does not directly result in any push back on the server on a per-message basis.  E.g. if handling an RPC involves starting up an expensive background process, then a server may wish to disable auto-inbound flow control to delay the next message from the grpc library until it explicitly signals that it is ready for it, rather than when the stream message handler returns.\ngrpc-go's streaming API is blocking, so such a mechanism is not necessary.  The application chooses when to receive the messages by calling Recv() on the stream when it is ready.\n. Follow-up to answer your real question:\nIf you want message-level flow-control with grpc, you will need to implement it yourself via fields in your request/response messages.  I.e. you can have the receiver send explicit \"ack\" signals back to the sender when it is ready for another message.\n. #1615 . Thanks for the PR.  We would prefer to implement this by making the entire proto file descriptor available to interceptors, which would include this information.  (#1526)  Unfortunately, that requires changes to generated code, which means it's blocked on us (this PR would have the same constraint).  We hope to get to this early next quarter.  Please let us know if you have any concerns with this.. This was actually intentional to avoid an unnecessary part of that change that also relied on other changes from earlier commits.  From the commit message:\nThis commit cherry-picked the connectivity state change from b3ed81a,\nexcluding the connect() function cleanup.\nDo you need some changes related to that in a 1.7.x release?  If so, can you explain what/why?  Thanks.. Thanks for filing the bug.  I believe the issue is that, while Marshal works fine with the same message being used in parallel many times, we're Unmarshaling into the same destination in parallel, which doesn't even make sense to do:\nhttps://github.com/grpc/grpc-go/blob/08a45354194cd00490a5d926825670fb928b5f76/codec_benchmark_test.go#L97\n(protoStruct is a shared reference to the proto message in the slice.)\nI will send out a fix in a moment that Unmarshals into a local.. @tamird, I agree, thanks for the reminder.. We plan on making some changes here in the next month or two that might make it unsuitable for export right now.  What exactly do you want this for?\nFWIW, after #1547 gets hashed out, it will be possible to retrieve a grpc.Codec for the proto codec via encoding.GetCodec.. FWIW, you are free to use the code in your project as long as you comply with the terms of the license at the top.\n. > Got rpc error: code = Unknown desc = context deadline exceeded of type grpc/*status.statusError.\n\nOur question is whether this error from RPC is expected? Should it be converted to\ncontext.DeadlineExceeded from gRPC side?\n\nRPCs should always return an error implemented by the status package.  However, the code should be codes.DeadlineExceeded in this case, not Unknown.  We'll look into this more.  Thanks for filing the bug.. Yes, it looks like you were missing the \"plugins=grpc\" bit.  With that, the client and server types and functions should be generated.  If there's still something (docs/etc) we need to fix, please let me know.  Thanks!. Thanks for the bug.\nLooks like an encoding of \"identity\" is a thing in our spec: https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\nSo we need to support it.  I'll take a look.. I have a fix for this almost ready, I should be able to get it out for review tomorrow.. #1664 is out for review, and should hopefully be the complete fix for this (and other small but previously-undiscovered issues).  Things are a bit confusing with both the old and new compressor APIs in flight, so I added a doc to help explain it (and to help me implement it!).\n. I ran a little experiment to determine whether this is caused by a constant overhead or if it's a per-server problem that will result in poor scaling.  I made the greeter_server accept a parameter for how many servers to run simultaneously (branch).  Results:\n$ for i in 1 2 5 10 25 50 100 250 500; do  { ./gs --num_servers=$i & pid=$!; sleep 10; awk 'BEGIN {i=0} /^Pss/ {i=i+$2} END { print \"'$i:' \", i }' /proc/$pid/smaps; kill $pid; } 2>/dev/null; done\n1:  7853\n2:  8051\n5:  8231\n10:  8400\n25:  8441\n50:  8875\n100:  9615\n250:  11037\n500:  13835\n(The output lines represent the number of servers and the total Pss for the greeter_server process.)\nThis shows that any scaling problems could only happen if you run your services in different binaries.\nLooking at all allocations, go's pprof only sees 34kB in greeter_server when a single server is started. This meshes well with the findings above.  I'm not sure where the 7MB is going, but I'm guessing it's entirely about holding the binary in memory.\nAs far as binary size, our external package dependencies are pretty minimal:\n$ go list -f '{{ join .Deps \"\\n\" }}' google.golang.org/grpc | grep -v google.golang.org/grpc\nThe only packages not in the standard library are a few things in golang.org/x/ and protobuf.\nIt looks like pulling in x/net/trace alone into an otherwise-small binary makes it ~10MB.  But even if we remove it (which we've considered doing), pulling in x/net/http2, which we cannot remove, the size goes up to ~8MB.\nI don't think there's anything else we can do here.  If you have any other ideas, though, please let us know.\n. Looks like we can't run with -race on GOARCH=386, so travis is failing.. Thanks for the contribution!  My fix for this is #1664.  There were a number of minor problems and inconsistencies with the existing code in grpc for applying the compressor/decompressor; I believe I've ensured everything works correctly and consistently now.\n. Resolving with \"working as intended\", since we don't intend to add support for custom polling frequencies to the default DNS resolver -- please let us know if you don't think that's appropriate.. @tsuna, I did not specifically test this using a Python client, but I added a test case that sets \"identity\" in the grpc-encoding header and makes sure the server behaves correctly.\nWould you be able to patch this change into your environment and test it?\n. We have interop tests, both in open source and inside Google, and I am following-up to figure out why they didn't catch this issue.\n. There's already an issue for this: #1654. Connections are not shared between ClientConns.  This doesn't sound at all expected, and I'd be surprised if we don't have at least one test that creates multiple connections and closes in between -- would it be possible for you to put together a small repro case that demonstrates this?. > If we didn't close connections, it's likely that grpc reused the underlying connection\nWe do not reuse connections across ClientConns.  If the dialing context is being canceled, that would explain the error -- are you reusing the same context in subsequent grpc.DialContext calls perhaps?\n. It seems we didn't follow our own advice for testing. \ud83d\ude04 . @kaoet can confirm whether 0d399e6 is where the problems started for you, or if @MakMukhi's suggestion works for you?  If so, this is the intended behavior (allow unlimited streams if the server does not explicitly set a limit).  Thanks!. OK, thanks for trying.  If you run into more problems, please comment here and we will reopen the issue.  Also note that we're making some performance improvements to the flow control mechanism in the next couple weeks that should allow servers (and clients) to spend less time blocked acquiring quota, which may help here as well.\n. @bboreham, thanks for the diagnosis.  We have some improvements here (that have been very close to landing for a couple weeks now) that will help with this contention substantially.  #1512 is the tracking issue for this.\n. I wouldn't worry about http2_server.go; we are making some bigger changes there that should actually render this problem obsolete.  Thanks!. PR #1478 was attempting to do the same thing, but also for proto.Unmarshaler.  Could you add that, too, and run our benchmarks before/after the change to make sure this does not hurt performance for non-self-marshaling proto types?\n. This can't be done until gRPC itself stops using the deprecated functions.  If you would like to make those changes as well, we can approve them; otherwise, we will wait until we have the cycles to do that.. The build failure on 1.9 is the gofmt check failing -- can you rerun it please?  This also needs a rebase to resolve the conflict in balancer_switching_test.go  Thanks!. Failed again:\n--- FAIL: TestFlowControlLogicalRace (27.67s)\n    end2end_test.go:4640: StreamingOutputCall; err = \"rpc error: code = Unavailable desc = the connection is draining\"\n. Have you turned on info logging by importing the glogger package or using the environment variable GRPC_GO_LOG_SEVERITY_LEVEL=\"INFO\"?\nIf killing the server manually works, however, my guess is MAX_CONNECTION_AGE isn't configured correctly or is not working correctly -- it should kill the connection and appear the same as an error to the client.\n. @ejona86 can you advise on this?. This sounds like an Envoy problem, then, since it works without Envoy.. Apparently this is caused by https://github.com/golang/go/issues/10527; listener.Accept() can return a connection even after listener.Close() has returned.  Our tests are assuming this is impossible.\nThe workaround I will employ is to add a WaitForHandshake to the DialOptions for these tests; that way RPCs won't be scheduled until after the server's preface has been received, which will never happen because the server was closed.  (Actually, I believe I'll also have to add/move a check to ensure that is the case.  I.e. the following:)\nhttps://github.com/grpc/grpc-go/blob/cce0e436e5c428f2094edd926779788f1024938d/server.go#L524-L530. Actually, it looks like this protects us from starting a http2 transport for the connection:\nhttps://github.com/grpc/grpc-go/blob/cce0e436e5c428f2094edd926779788f1024938d/server.go#L558-L562. We decided off-list to add these constants outside of gRPC for the time being, since gRPC doesn't need them (yet?).. Not sure if we have another issue that encapsulates this, but we are working on it (and have been for a week or two).\n. We'll track this under #1512. Thanks.. Send will make sure the data gets flushed for each message.  Not before returning, but the message will be put on the wire very soon with no need to manually flush anything.\n\nOn clients, if you want to make sure the data has been sent before calling Close on the ClientConn, then wait for Recv to return io.EOF.\n\nOn servers, you should use GracefulStop to make sure in-flight RPCs finish before closing the server.\n. Yes.  CloseAndRecv is basically just a wrapper for CloseSend and RecvMsg.  As a special case, RecvMsg for client-only-streaming RPCs will block until the final status of the RPC is received from the server.\n. For the client side of streaming RPCs, unfortunately things like this are not straightforward.\n\n\nThere is no Stream.Close() method that the user must call to signify the end of the stream.\n\nInstead, the final call to Stream.RecvMsg(), which returns the status, is essentially the end.\nSome users don't call RecvMsg to retrieve the final status.  So if we tie a callback to that, it may never be called.\nWe could tie it to when the server sends the trailers and ends the stream.\nHowever, this could be confusing, because RecvMsg will typically be called after this happens.  This means if OnStreamClose() is used, for example, to free resources used by the interceptor, that would be incorrect.\n. ~~stats.Handler is notified of stream closure, so I think for now the best way to implement your tracing is to use the stats.Handler instead if you need end-of-stream notifications.~~\n\nEDIT: see below. > 2. From a finalizer (e.g. runtime.SetFinalizer) on the underlying client-stream. (Should be safe as long as the grpc library itself does not use a finalizer on the stream it returns. This stream never \"escapes\" since it is wrapped, so calling code cannot set a finalizer.)\n...\n\nThe second bullet handles the case where the client never exhausts the response stream. Yes, I know it is not pretty. But I haven't found any other way to know with certainty that a client stream is over.\n\nWe had this same thought a few months ago when the issue of stats.Handler not firing for users that don't do the final Recv() first came up.  Since there's no stream.Close() that we tell users must be called, unfortunately, I think this is the only way.  If/when this becomes an important thing to fix, we probably would install a finalizer (vs. calling via the transport, before the final Recv), so I hope that doesn't break your wrapper.\n... And now I believe my comment on Dec 14 is bad:\n\nstats.Handler is notified of stream closure, so I think for now the best way to implement your tracing is to use the stats.Handler instead if you need end-of-stream notifications.\n\nThis has the same drawbacks as just wrapping the Stream, so it won't help.  Your best bet if you can't be sure the application will do the final Recv() is to add a finalizer to your wrapped Stream like @jhump suggests.. \ud83e\udd22 \ud83d\ude00 . Thanks!  I had a feeling that was the problem -- we have no .go files in that path, so the error is accurate; it's just not clear why it would be an error.. Note: TestGracefulStop is failing ~1/10k runs with:\ngracefulstop_test.go:209: EmptyCall() = rpc error: code = Unavailable desc = transport is closing; want <nil>\nThis is most likely a long-standing issue, and #1745 should be a big improvement over the prior state for these types of races already.  With @MakMukhi making some major changes to the transport in the next couple weeks, we'll wait until that is done to dig more into this.. @menghanl can you provide an ETA on or work estimate for this feature request?. I don't really like the idea of a package that exists only to work around a bug in godep.  How about if we check this in on the 1.7.x branch, which is the one you need, and hope they fix the issue before you move to a later gRPC release?. \"context\" was imported by goimports instead of \"x/net/context\" in the new test file.  Fixed and push -f'd\n. Indeed, there is no lock on the map or the slices it contains.  :disappointed:\nIt should be a trivial fix.  I'll take care of it.. IMO SendAndClose is poorly named, and the server handler generated for client-streaming RPCs should have returned the response message and error like unary handlers, but the API is unfortunately not something we are able to change at this point.\nIt looks like there is a bug here preventing client-side Sends from returning an error when the stream is closed.  We'll look into it.  Note that the same thing also appears to be true when the client calls CloseSend and then performs subsequent Send calls.\nAs a minor side note, I would not recommend deferring stream.SendAndClose; instead return either an error or stream.SendAndClose(response) (which itself returns an error if a problem is encountered) from your handler.  Otherwise, if there is an error, your server will transmit the response message to the client, even though it will be ignored (due to the code in CloseAndRecv).\n. I did some digging, and it looks like an error will eventually be returned from Send, but only after the flow control limit is reached, which could be as much as 4MB.\nLike #1738, there will be a lot of changes going into the transport Real Soon Now, so we will wait until those are in before putting in a fix for this.\n. This should also be fixed by #1792 (the same root cause).\n. I think this is what #1748 fixes. Unless you were relying on the behavior of a client staying connected to an unresponsive server indefinitely, this shouldn't have changed anything user-visible.\nCan you provide a pointer to tests that are failing?  Or what your tests are doing when they time out?  I.e. are they stuck in Dial or do they time out making RPCs or elsewhere?\n. Without knowing more specifically what the tests are doing, I'm afraid we can't be of much help.  Even logging output will probably not help without knowing the intent of the test.  If you can dig a little more and provide some details as you discover them, that would be helpful.\nThat commit did have an intentional and potentially impactful change, which is that the client now kills a connection if the server's HTTP2 preface is not received within 20 seconds (the time scales up on reconnect attempts).  If RPCs were pending on that connection when it was killed, they will fail.\n. The client will never fail permanently.  RPCs assigned to the connection after the connection is established will fail if the handshake is not completed in time.  The client will then attempt a new connection.  I don't imagine this would be a problem unless you go through a proxy that accepts the connection but doesn't handshake on its own, and whose remote server isn't yet running.\nThis behavior cannot be disabled.  However, you can do one of two things:\n\nUse grpc.WithWaitForHandshake() when you call grpc.Dial.  This will prevent RPCs from being assigned to the connection until the connection comes up.  If you also use WithBlock(), Dial will not return until at least one connection has finished handshaking.  Otherwise, you can use the connectivity API to block until the state is Ready.\nSet grpc.WaitForReady() when performing calls; this should cause a single transparent retry to be performed, which I believe may be enough if this is indeed the cause.  Otherwise, manually retry this RPC until it succeeds or a timeout occurs.. It's always possible a bug was introduced, but note that all of grpc's existing tests passed without any meaningful modification as part of that commit.  To move forward, we really need a better understanding of your tests.  I.e. what operation are they performing that isn't working as expected?\n. If you mean for reconnecting, the only thing you can tune is:\n\nhttps://godoc.org/google.golang.org/grpc#WithBackoffMaxDelay\nThere's also WithBackoffConfig, but it only allows you to tweak that one setting.\nHow much worse is this now, and what do you mean by \"high/low percentiles\"?\n. Hi @dzbarsky, have you seen these problems since patching https://github.com/grpc/grpc-go/commit/e975017b473bd9b2a3d5b23428a8549fe20b1153?. @dzbarsky, any update on this?. Sorry for the regression.  I'll fix it and add a test case.. Thanks for the fix.  Can you please complete the CLA?\n(Also, it seems we have a flaky test unrelated to this change.  I'll file an issue and investigate.). Thanks!  I restarted the failing job (flakiness was fixed by #1767 BTW), which should pass, then I will merge.. I've also encountered the following error:\n--- FAIL: TestGracefulStop (0.00s)\n    gracefulstop_test.go:209: EmptyCall() = rpc error: code = Unavailable desc = transport is closing; want <nil>\nFAIL\nThis looks like a production code problem vs. a test problem.. > This looks like a production code problem vs. a test problem.\nThis is probably what I was explaining in #1738:\n\nAt this point I believe it is still racy, just less so.. Regarding this point:\nOne more, less-good option; adding a default deadline to WaitForStateChange (it uses context.Background(), which never times out).\n\nWaitForStateChange accepts a context.Context parameter.  In your thread1, this is the one provided by the user when they called grpc.DialContext.  This can only be context.Background() if that's the one the user provided (as you show happening above in the call to pubsub.NewClient) or if grpc.Dial is called without the grpc.WithTimeout() DialOption.  Otherwise, Dial[Context] should never hang indefinitely.\n. Panicking should be avoided if at all possible.  SetHeader and SendHeader could return errors.  Unfortunately, grpc's ServerStream.SetTrailer doesn't return an error.  In this case, one other option would be to make the transport force the stream to result in an error with an error message indicating that the server attempted to send malformed metadata.. @MakMukhi is there any way to attempt to stimulate this race in a regression test?  Why didn't our tests ever run into this problem?. I'm going to reopen this to track adding regression test cases as discussed offline:\n\n~~Use WithWaitForHandshake() in a case where the remote server never sends the preface, and ensure RPCs are never assigned to that connection (i.e. its state never becomes Ready).  Without WithWaitForHandshake(), RPCs should be assigned to it (i.e. its state does become Ready after Accepting the connection).~~\nAdd a sleep after the server handshake is received in TestDialWaitsForServerSettings, longer than the minConnectTimeout, and make sure the connection is still valid.\n\nEDIT: 1 doesn't really need to be a separate case that TestDialWaitsForServerSettings isn't already covering.  That one ensures Dial doesn't return until the handshake happens, which is essential the same thing.. FYI: v1.9.1 has been released with the fix to this issue.\n. Please let us know if that didn't help or you need more information.  Thanks!. #1693 is about flakes in this test with this error.  Here it failed 4/5 times, which is unusual.  I restarted all the failing jobs, and we'll see what happens.\n. > it looks like there's no consistent error statuses being returned.\nThat's what I was afraid of..\n\nWould you prefer me to continue by returning Unauthenticated for now?\n\nYes, I think that's the best we can do without changes elsewhere - thanks!. FYI: I filed https://github.com/golang/oauth2/issues/264 as a feature request for oauth2.. Note that if we provided this feature, it would need to block until the RPC is considered \"committed\" for the purposes of retry.  This means either response-headers have been received or the outgoing message buffer overflowed.  (See: https://github.com/grpc/proposal/blob/master/A6-client-retries.md#when-retries-are-valid). Actually, this is possible via peer.FromContext(stream.Context()). FYI: v1.9.1 has been released with this fix.. It finishes for me, you just have to be patient:\n```\n$ go run benchmark/benchmain/main.go -benchtime=10s -workloads=streaming -compression=off -maxConcurrentCalls=200 -trace=off -reqSizeBytes=1048576 -respSizeBytes=1048576 -networkMode=LAN -cpuProfile=speedup.cpu\nStream-traceMode_false-latency_2ms-kbps_102400-MTU_1500-maxConcurrentCalls_200-reqSize_1048576B-respSize_1048576B-Compressor_false: \n50_Latency: 40.3235 s   90_Latency: 41.0967 s   99_Latency: 41.1228 s   Avg latency: 36.3483 s  Count: 200  13201654 Bytes/op   16305 Allocs/op \nHistogram (unit: s)\nCount: 200  Min:  14.5  *Max:  41.1***  Avg: 36.35\n\n[         14.456700,          14.456700)    1    0.5%    0.5%\n[         14.456700,          14.456700)    0    0.0%    0.5%\n[         14.456700,          14.456700)    0    0.0%    0.5%\n[         14.456700,          14.456703)    0    0.0%    0.5%\n[         14.456703,          14.456743)    0    0.0%    0.5%\n[         14.456743,          14.457320)    0    0.0%    0.5%\n[         14.457320,          14.465627)    0    0.0%    0.5%\n[         14.465627,          14.585274)    0    0.0%    0.5%\n[         14.585274,          16.308545)    3    1.5%    2.0%\n[         16.308545,          41.128673)  195   97.5%   99.5%  ##########\n[         41.128673,                inf)    1    0.5%  100.0%\n```\n(Note the \"Max\" above, asterisked.)\n1MB = 8Mb.  200 streams * 2 directions * 8Mb per message = 3200Mb.  LAN mode allows 100Mbps.  This means our best case scenario* would be 32s.\nI'm not sure why we're 10s worse than optimal at this point (78% of max), but I don't see anything broken with the -networkMode flag.  It's just that -benchTime=<short> is not really compatible with high levels of concurrency and large messages relative to the effective throughput.\n\n\n\nThis is if we have perfect fairness across the outgoing client streams and all 200 requests complete at the same moment, with all 200 responses starting afterwards.  Interestingly, the overall benchmark would improve if we sent the streams serially so that we could maximize the bi-directional utilization of the network.\n  . Passthrough sends the entire target string (after \"passthrough:///\") to the dialer as the address, verbatim.\n\n\n\nA DNS resolver would resolve the target (after \"dns:///\"), turn it into IP addresses & ports, and those would be handed to the dialer.\nIt just so happens that the default dialer (net.Dial) is able to do DNS name resolution, and so passing \"google.com:22\" will work just fine with it.\nHowever, this passthrough method means we can't do things like load-balance across all the DNS results for that name.\nYou can see more about the resolver/balancer design in the gRFC here: https://github.com/grpc/proposal/blob/master/L9-go-resolver-balancer-API.md. Looks like there is some kind of lint error still.  Also, can you update the benchmark numbers please?\n. > XXXX(set by SetParallelism)/XXXX (extra work besides critical section)\nCould you reverse the order of these so when the tests are printed, they're easier to compare?\nAlso, please either remove the extra \"/Mutex\" and \"/CAS\" from the name, or merge the two benchmarks into a single top-level benchmark function.. Looks good, but could you reverse the order of the parameters so when the tests are printed, they're easier to compare?  I.e. put the iteration count before the parallelism.  The output would look like this instead (but with the /x/y/ numbers reordered to /y/x/, which I did not want to do):\nBenchmarkStructStoreContention/CAS/100000000/100000-12              3000       4731686 ns/op\nBenchmarkStructStoreContention/CAS/100000000/10-12                  3000       4677181 ns/op\nBenchmarkStructStoreContention/CAS/10000/100000-12              30000000           483 ns/op\nBenchmarkStructStoreContention/CAS/10000/10-12                  30000000           475 ns/op\nBenchmarkStructStoreContention/CAS/0/100000-12                  100000000          155 ns/op\nBenchmarkStructStoreContention/CAS/0/10-12                      100000000          154 ns/op\nBenchmarkStructStoreContention/Mutex/100000000/100000-12            3000       4745838 ns/op\nBenchmarkStructStoreContention/Mutex/100000000/10-12                3000       4687433 ns/op\nBenchmarkStructStoreContention/Mutex/10000/10-12                30000000           479 ns/op\nBenchmarkStructStoreContention/Mutex/10000/100000-12             5000000          3442 ns/op\nBenchmarkStructStoreContention/Mutex/0/100000-12                30000000           365 ns/op\nBenchmarkStructStoreContention/Mutex/0/10-12                    200000000          100 ns/op\n(Note how it's easier to see how the workload scales with less/more parallelism, because the relevant lines are adjacent.). Also.. I assume this is an anomaly?\n\nBenchmarkStructStoreContention/Mutex/10000/10-12                 30000000           479 ns/op\nBenchmarkStructStoreContention/Mutex/10000/100000-12              5000000          3442 ns/op\n\nThe Mutex doesn't scale that poorly for the other workloads (with higher or lower contention).. > Well, it's not anomaly, quite repeatable. I also remembered it's of magnitude 1000 when we did experiments before.\nYes...it looks like we have a 4x slowdown between the 10 & 100k parallelism for the max-contention case as well, so this is on par with that.. Have you seen: https://github.com/grpc/grpc-go/blob/master/Documentation/compression.md?\nUseCompressor is marked as experimental mostly as a hedge in case we discover that something doesn't work and it needs to be changed.  There are no expected changes there, but it's possible.\nPer our versioning doc deprecated functions will stick around until the next MAJOR release (which we do not have planned).  Also, a backward-incompatible release is likely to live in another import path (e.g. \"grpc/v2\").  If you have no tolerance for breakages, use the deprecated API.\n. > To get the gzip compressor registered, is the idea that you do:\n\nimport _ \"google.golang.org/grpc/encoding/gzip\"\n?\n\nYes, exactly.\n\nAlso it feels like there should be a constant for \"gzip\" rather than having me type it out and possibly misspell it.\n\nSGTM. If you modify your client to do an r.CloseAndRecv(), you will see the Unimplemented error.  What happens is this:\n\nClient dials server\nServer accepts connection\nClient initiates RPC (sends headers)\nClient application calls Send() to queue up a message; Send returns after flow control checks ensure it can be sent\n  -- This is where your client terminates --\nServe receives the header referring to an unregistered RPC, and ends the stream with trailers that include status=Unimplemented\nClient receives trailers\n\nHowever, if those trailers are never read via a Recv() (or in this case, since it's client-streaming, CloseAndRecv()), the application will never see them.\nI think there may also be an API problem here.  If the server terminates the stream, subsequent Send calls should arguably fail.  This would introduce a race between 6 and 4, above, and it would be a behavior change, so I'm not sure if this is something we should/can change.  I'll have to think more about it.\nMay I ask why you don't want to use reflection?. That seems reasonable to me.  Obviously the server application could theoretically return an Unimplemented error from a registered handler, but if you know it doesn't, then that should work fine.\nHowever, I think this is a bug.  We should return an error (io.EOF) from Send when the stream has been terminated by the server.  Send actually will return an error today, but only after the flow control is exhausted.  We will fix this issue, but I think your idea above is fine as a workaround until that happens.\nNote that because the network is asynchronous, there will still be an inherent race between steps 4 and 6, so you will not necessarily get an error upon the first write, just the first write after the Unimplemented is received.\n. Actually, this may be even worse than I previously thought.  If we are deducting these Sends from the transport's flow control, but then not writing the data on the stream and not adding our quota back to the flow control, then we will eventually run out of flow control on the connection.. > If we are deducting these Sends from the transport's flow control, but then not writing the data on the stream\nIt looks like this is not the case; we are unconditionally writing the data to the stream even after it was terminated, so the server will send flow control window updates, and we will not run out of send quota.. Actually, this bug is a duplicate of #1755, so I'll close this now.\nThe tl;dr here is that after #1792 is submitted, Send() itself will eventually give you an error (io.EOF), at which point Recv() will let you know it was Unimplemented.  So you shouldn't have to do a probing no-payload use of the RPC as suggested above.. > Should https://github.com/grpc/grpc-go/blob/master/Documentation/grpc-metadata.md be updated as part of this?\nYes please!. > @dfawley Yes, I certainly shall. Should I benchmark the adding as well as retrieving operations (FromOutgoingContext), or are we only concerned with the adding?\nCan't hurt if you'd like to, but I don't think it's necessary.\n. > Extracting it with FromOutgoingContext is quite slow in comparison, though, since that's where the joining is happening now.\nThis is expected, and so we won't see any actual speedup until we change the transport to not call metadata.FromOutgoingContext when writing the outgoing headers.  We will instead need to get access to the rawMD, and output the metadata without merging it all into a single map.\n. I was thinking we'd add either: metadata.FromOutgoingContextRaw(context.Context) (MD, [][]string) or metadata.FromOutgoingContextRaw(context.Context) RawMD to avoid that problem.  What do you think?. @zhexyany, can you quantify the performance penalty you're seeing?  That would help us prioritize this against other performance changes we are making.  Thanks!\n. @zhexuany sorry, I botched the @mention above.... Is this needed only for per-RPC balancing decisions, or when the balancer is created?\nThe balancer.BuildOptions is passed a Build-time.  If you only need this when RPCs are performed, one option would be to pass data from the client to the Picker using the context.\ncc @ejona86 . push -f?. @jhump to maintain backward compatibility, we will have to continue supporting interceptors exactly the way they currently work.  It might be possible to introduce a new type of interceptor that works for both unary and streaming RPCs, but that would be a different feature request.\nIndependently, we're talking about some CallOption changes as a follow-on to #1794 that would fix #1494.. > I think that new type of interceptor would basically look exactly like the stream interceptor.\nMost likely yes.  I meant \"type\" as in \"variety\", not as in \"Go type system\".  A new DialOption to register an interceptor for unary and streaming, that has (maybe?) the same signature as streaming interceptors, seems like the most reasonable option to me.\n\nI'll file a new issue to describe this feature request.\n\nSounds good, thanks.\n\nThe name of this issue does not indicate that it is client-only. Is that the intent?\n\nThis issue is intended to be for an internal-only cleanup to delete the bulk of call.go.  No APIs or behavior should be affected (unless there are bug fixes / minor tweaks to error cases like the example in the first comment).\n. @jhump - How are you injecting an alternate transport into grpc?  Do you have a client interceptor that doesn't use the provided invoke function?. Will do!. I'd like to redesign our interceptor API.  Please add your list of requirements/nice-to-haves in this bug and I'll compile them here:\nRequirements:\n- Single interceptor type for unary and streaming\n  - If it's difficult to implement, provide an optional convenience wrapper to generate a stream-style interceptor from a single function (like how unary interceptors work today).\n- Include access to ServiceDesc/FileDescriptorProto (#1526)\n- Chainable / able to easily register multiple handlers\nNice to Haves:\n- Single API for stats handlers and interceptors\n- Global registration pattern\n- Able to specify via CallOptions\n- Function signature includes explicit context.Context (from @skipor)\n- Same/similar type signature as RPC handler itself (Java does this)\nThis is currently just a quick list based on my memory at the moment.  I intend to add to this as I run into more things.\nThis will be a breaking change (breaking only features marked \"EXPERIMENTAL\" in the docs), but we will make sure to leave the current version around for one or two releases to allow time for users to migrate.  We will also be sure to make a gRFC proposal for this.. cc @ejona86: do you have the same concerns with this as #1711?. Seems like there may be an issue here.  The CloseStream here, which is what happens when the client receives a message that is too large, will not result in a RST_STREAM being sent back to the server, which means the server will block indefinitely once the stream's flow control is exhausted.. Thanks for all the feedback!\n\nDue to internal optimizations we can't serialize all options to a string due to shared data structures between different resolver objects.\n\nYour resolver builder is a single struct type that is registered and accessed globally (e.g. from the dns resolver: link).  You could store the shared data structures there.  Each Resolver returned by the Builder would then be able to reference the shared data.\nWould something like that work for your use case?\n. \nReview status: 0 of 22 files reviewed at latest revision, 12 unresolved discussions.\n\ncall.go, line 186 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis toRPCErr call is unnecessary.\n\nDone.\n\nrpc_util.go, line 137 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nIs this variable used anywhere?\n\nNo; removed.\n\nrpc_util.go, line 264 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis grpc.io link doesn't exist (why???).\nChange to https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\nThere are multiple places need to be changed.\n\nDone.\n\nrpc_util.go, line 273 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nShould we just say CallCustomCodec overrides CallCustomCodec instead of describing the behavior here?\n\nIt doesn't completely, though.  We still use the string from this CallOption.\n\nrpc_util.go, line 540 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nHow about following?\n```go\nif c.codec != nil {\n  // CallCustomCodec overrides CallContentSubtype.\n  return nil\n}\nif c.contentSubType == \"\" {\n  // No codec was specified by call option, use default proto.\n  c.codec = encoding.GetCodec(proto.Name)\n}\ncodec := encoding.GetCodec(c.contentSubtype)\nif codec == nil {\n  return status.Errorf(codes.Internal, \"no codec registered for content-subtype %s\", c.contentSubtype)\n}\nc.codec = codec\nreturn nil\n```\n\nYou're missing a return if c.contentSubType==\"\"; otherwise, done.\n\nrpc_util_test.go, line 29 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nremove this blank line\n\nDone.\n\nserver.go, line 1275 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nShould this return unimplemented error to the client?\nOtherwise, the server will try to use proto if content subtype is not recognized, and will return internal if unmarshal was unsuccessful.\n\nOur current behavior is to ignore the field entirely, so this would represent a behavior change.\nMaybe we can do that in a future commit?  I would like to try to avoid behavior changes in this PR as much as possible.\n\nstream.go, line 146 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis toRPCErr call is unnecessary.\n\nDone.\n\nencoding/encoding.go, line 90 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nof its Name() method\n\nDone.\n\nencoding/encoding.go, line 93 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nName()\n\nDone.\n\ntransport/http_util.go, line 170 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit: rename this to retrieveContentSubtype\n\ncontentSubtype()?\n\ntransport/http_util.go, line 190 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit: rename this to buildContentTypeForSubtype\n\ncontentType()?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 25 of 26 files reviewed at latest revision, 7 unresolved discussions, some commit checks failed.\n\nrpc_util.go, line 287 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\nDone.\n\nDocumentation/encoding.md, line 71 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nChange this to reflect the current behavior and change it back when Unimplemented is implemented?\n\nDone.\n\nencoding/encoding.go, line 94 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\nDone.  Sheesh, where did all these links come from?\n\ntransport/http_util.go, line 52 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\nDone.\n\ntransport/http_util.go, line 161 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\nDone.\n\ntransport/transport.go, line 331 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\nDone.\n\ntransport/transport.go, line 573 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md#requests\n\nDone.\n\nComments from Reviewable\n Sent from Reviewable.io \n. :+1: Glad you were able to work this out.\nI also hope you upgraded to a fairly recent release of golang; we intend to also drop support for Go1.6 in the not-too-distant future.. @benlangfeld, I tried to improve the commit message in #1838 for our release notes.  I do not suspect this is related to kubernetes/helm#3409.  That mentioned \"the transport is closing\" but here you would get \"context canceled\" errors instead of being able to receive the RPC data.. How do you feel about this doc in #1813 instead?\nhttps://github.com/dfawley/grpc-go/blob/95452f5cf729b0afbd80437d09f861227858bf66/Documentation/encoding.md\n. Thanks for the review; I've made some changes to the PR:\nhttps://github.com/dfawley/grpc-go/blob/f47e722e94c7e4a8f11e9fbb132d0f98a68804b7/Documentation/encoding.md. I'm going to close this now that #1813 is submitted with better separation of sections as you suggested.  Let me know if you disagree.\n. We do run go vet, but it appears there's a bug preventing it from detecting this problem.  Filed https://github.com/golang/go/issues/23539 against vet to track.. Hi @gyuho, have you had a chance to look into this yet?  Thanks!. I ran 5 runs of the unary benchmark with 1 byte request/response and 100 concurrent streams for 2 minutes each.\nBefore:\n50_Latency: 1465.2040 \u00b5s    90_Latency: 1635.4170 \u00b5s    99_Latency: 2209.4680 \u00b5s    Avg latency: 1492.8950 \u00b5s   Count: 8034655  7352 Bytes/op   141 Allocs/op   \n50_Latency: 1468.3980 \u00b5s    90_Latency: 1634.9030 \u00b5s    99_Latency: 2206.0530 \u00b5s    Avg latency: 1496.6130 \u00b5s   Count: 8015025  7352 Bytes/op   141 Allocs/op   \n50_Latency: 1460.1300 \u00b5s    90_Latency: 1624.9120 \u00b5s    99_Latency: 2201.9330 \u00b5s    Avg latency: 1487.6050 \u00b5s   Count: 8063496  7352 Bytes/op   141 Allocs/op   \n50_Latency: 1469.1540 \u00b5s    90_Latency: 1633.3620 \u00b5s    99_Latency: 2201.2620 \u00b5s    Avg latency: 1496.0120 \u00b5s   Count: 8018548  7352 Bytes/op   142 Allocs/op   \n50_Latency: 1469.0910 \u00b5s    90_Latency: 1630.6830 \u00b5s    99_Latency: 2203.9370 \u00b5s    Avg latency: 1496.4050 \u00b5s   Count: 8016386  7352 Bytes/op   142 Allocs/op\nAverage of averages: 1493\nAfter:\n50_Latency: 1497.4120 \u00b5s    90_Latency: 1664.6800 \u00b5s    99_Latency: 2243.7390 \u00b5s    Avg latency: 1527.6940 \u00b5s   Count: 7852325  7720 Bytes/op   145 Allocs/op   \n50_Latency: 1498.5790 \u00b5s    90_Latency: 1664.7820 \u00b5s    99_Latency: 2229.4360 \u00b5s    Avg latency: 1526.6400 \u00b5s   Count: 7857824  7721 Bytes/op   145 Allocs/op   \n50_Latency: 1500.0920 \u00b5s    90_Latency: 1667.4590 \u00b5s    99_Latency: 2235.4930 \u00b5s    Avg latency: 1528.6580 \u00b5s   Count: 7846750  7721 Bytes/op   145 Allocs/op   \n50_Latency: 1499.1370 \u00b5s    90_Latency: 1664.3580 \u00b5s    99_Latency: 2229.4210 \u00b5s    Avg latency: 1527.3680 \u00b5s   Count: 7853837  7720 Bytes/op   145 Allocs/op   \n50_Latency: 1490.3740 \u00b5s    90_Latency: 1657.7140 \u00b5s    99_Latency: 2233.4980 \u00b5s    Avg latency: 1519.9220 \u00b5s   Count: 7892692  7720 Bytes/op   145 Allocs/op\nAverage of averages: 1508\nSo with this change we are approximately 1% slower on this unary microbenchmark.  I'll try to see if there's any obvious low-hanging fruit, but my initial thinking is that the 1% is worth it to unify the code (e.g. we can focus our performance work even better with less code to optimize), and it should have very little impact in real-world scenarios.\n. Benchmark commit reverted via rebase. Hopefully that doesn't break reviewable's/github's brain too much.  And since I was rebasing anyway, I also updated with master at the same time because there was a conflict.\n\nReview status: 3 of 7 files reviewed at latest revision, 2 unresolved discussions, some commit checks failed.\n\ncall.go, line 46 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nReturned error doesn't need to be named\n\nDone.\n\nstream.go, line 493 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAlso, no need to name the returned error\n\nDone.\n\nComments from Reviewable\n Sent from Reviewable.io \n. What does your workload look like?  E.g. how many streams/RPCs are you performing concurrently / per second?  64k \"encodeTimeout()\" objects in use seems to imply you have 64k streams during the profiling window.\nAlso, I'm a little confused by this report, because the option to pprof makes it list \"in use_objects\" not \"un used objects\".  A bit more information to help us understand why you think this output is bad and how we could produce something similar on our own would be helpful.\n. @zhexuany, ping in case you missed my last post.. @slomek,\nWhat are you using for your load balancer?  With grpclb, this feature exists (it's implemented in Go but not other languages, hence the language about \"Future Work\").\nIf you're using your own load balancer protocol, you would need to implement this as a feature in your own custom resolver and balancer.  You can do this by having the resolver emit Addresses with Metadata to indicate whether it's a fallback address.  The balancer could then make use of this and the connectivity state in order to initiate direct connections to backends when the remote load balancer is unreachable.\nLet me know if you have any more questions about this!. @tux21b, I think you ultimately want a fix to #1741, which would make us support dialing unix sockets automatically.\nIn the meantime, our target parsing behavior is as described in our spec here: https://github.com/grpc/grpc/blob/master/doc/naming.md\nYour \"unix:///run/example.sock\" is being parsed as:\n- \"unix\" as the schema\n- \"\" as the authority\n- \"run/example.sock\" as the endpoint_name\nBecause you want your dialer to see the entire target name, you can use the passthrough resolver like so:\n```go\nimport \"..../grpc/resolver/passthrough\"\ngrpc.Dial(\"passthrough:///unix:///run/example.sock\")\n```\nWe will support the unix scheme out of the box when #1741 is fixed, at which point you would use unix:////run/example.sock (note the four slashes up front: two after schema, one after authority, and one as the beginning of the endpoint name) or dns:///hostname.com or dns:///<numeric IP> (dns is installed by default).\nSorry for the change of behavior.  The previous implementation did not follow the grpc spec, which was preventing us from supporting other features that we needed.  Let me know if you have any other questions.. Something strange happened with these diffs.. this looks like an older version (e.g. check the comment on FromError).. @thurt, issue #1802 looks very similar, and should cover your needs.  Can you take a look and see if you agree?. > Note: after further investigations, not a message size, but rather non-zero processing\n\ntime on servers side caused deadlock occurring.\n\n@dim13 - are you sure both are not required (which was our analysis)?  Can you test with #1859 and see if you still encounter the problem?  Thanks!. Note that currently on server-side, RST_STREAM may not be ordered with the trailers, so it could be sent first, which would be a problem.. \nReview status: all files reviewed at latest revision, 8 unresolved discussions.\n\nclientconn.go, line 1384 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhy is this error moved here?\n\nI was tired of looking at it.  I'd delete it, but it's possible it would break somebody, so I don't think we should (and it's harmless).\n\nstream.go, line 131 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhat does this mean?\nAlso move the comment to where cancel is defined? (And move the definition of cancel closer to if?)\n\nPTAL\n\nstream.go, line 419 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nreturn nil in the second if err == io.EOF on line 481 instead of setting it back to nil here?\nSo you can save this if, and finish() will always be called with io.EOF in the successful case.\n\nI can do this, but it needs an explicit finish() call below in that case, too, because the defer can only finish() on error.  Changed to reflect.  It's maybe cleaner because the special-case stuff is closer together now?\n\nstream.go, line 421 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSo we will call finish() with nil in some cases. It should be OK though.\n\nYes, that's fine. finish() converts io.EOF to nil anyway.\n\nstream.go, line 491 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAdd comment why we don't care the returned error.\n\nDone\n\nstats/stats_test.go, line 1228 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAlso delete noLastRecv field from rpcConfig.\n\nDone.\n\ntest/end2end_test.go, line 1167 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nShould we send multiple times and expect io.EOF eventually?\n\nDone.\n\ntest/end2end_test.go, line 3800 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nConsider also defering this cancel? Otherwise a fatal below may cause this to be leaked.\n\nI had a defer here once, where did it go?  Re-added.\n\nComments from Reviewable\n Sent from Reviewable.io \n. Do you believe this will lead to problems?  Can you explain?\nSelects are inherently non-deterministic, as races between the incoming signals will determine the result.. I see what you're saying.  We generally have protections in place before or after our selects in case one term should have priority.  For instance, code at the top of Write() in http2Client and http2Server checks the stream/transport contexts already.  (And interestingly enough, the exact error returned no longer matters with #1854.)  We also have error latching in place for errors in the read path.\nWe do have some cleanups planned here to greatly simplify most of our selects (boiling them down to a single context check and the condition they're waiting on).\nIf you can identify real problems that are caused by the code, please don't hesitate to file issues, but I don't think there's anything actionable here at this time.. @pohly - Serve() vs. [Graceful]Stop() issues should all be fixed by #1485 (in 1.8.0 and later releases) and/or #1745 (in 1.9.0 and later releases).  The 1.7.x branch is sufficiently old that we would rather not back-port a fix to it.  Are you able to update gRPC in Kubernetes?. > One can argue that Server works as intended. But then how can a user of the Server prevent this unexpected behavior, given the current API?\nIt seems like you should be able to synchronize your test code such that Serve() is not called if Stop() has already been called, if you don't want the listener to be closed.  Or you could re-initialize your socket for every test case.  I'm a little confused though, because it sounds like you intend to re-use the socket in subsequent cases, but any call to Serve() will eventually result in the listener being closed (via Stop()).  I believe the listener is closed here for consistency reasons: in case the user isn't synchronized, then calling Serve() and Stop() at roughly the same time will always result in a closed listener.  Let me know if I'm misunderstanding something.. OK, I understand better now.\nThere's an interesting comment on line 173 here  (this is the Close() method being invoked for unix socket listeners on posix systems):\n// There's a race here--we can't know for\n    // sure whether someone else has come along\n    // and replaced our socket name already--\n    // but this sequence (remove then close)\n    // is at least compatible with the auto-remove\n    // sequence in ListenUnix. It's only non-Go\n    // programs that can mess us up.\n    // Even if there are racy calls to Close, we want to unlink only for the first one.\nSo one fix is to manually call Close on your listener before your test case exits.  Then a late call to Start() with the old listener would not unlink the file again.\nAnother option would be to make your Serve() goroutine signal when it finishes (e.g. by closing a channel in your ManagerImpl struct after Serve() returns), and have your Stop() function wait on that after calling m.server.Stop().  Then you know Serve() will have completed, also avoiding this problem.\nEither way, you should make sure your listener is closed before you create a new one with the same filename, which is the source of your problem.\nI don't think we can change grpc in a way that fixes your problem.  Close should always be called on every listener you create to avoid leaking resources, so if grpc didn't call Close on it, you would still have to.  This would mean that you'd have to do the following to be correct:\ngo\ngo func() {\n  if err := m.server.Serve(s); err == ErrServerStopped {\n    // grpc doesn't close our listener for us, so we have to do it ourselves.\n    s.Close()\n  }\n}()\n...and you'd have the same race.\n. > Yes, that looks like the easiest solution.\nIt is easy to do, but I also have reservations about leaving rogue goroutines hanging around after a test case finishes.  You may want to check out our leakcheck package, which ensures no unexpected goroutines are running before calling a test a success.  (This would also take care of this race.)\n\nI wonder whether it is worth mentioning this in the API documentation\nfor Serve(). Calling it in a goroutine can't be that unusual.\n\nFor most tests, Serve needs to be called in a goroutine.  I'm not sure how we can doc this, though.  This behavior is specific to unix sockets.  The docs do indicate that lis is always closed.\n. These are all OK, as they are not returned by grpc itself.. > Do you mean DataLoss?\nYes, sorry, I meant DataLoss in end2end_test.go. > I would just like to mention that for streams simple cancellation (context.WithCancel instead of\n\ncontext.WithTimeout) could be a little bit more idiomatic. Steams may be potentially infinite, so\ntimeouting on working stream may cause data inconsistency and so on.\n\nI disagree; even streaming RPCs should always have deadlines set.  One problem that could occur without a deadline is that if the RPC is wait-for-ready, it will block indefinitely if all servers are down.  Since your stream can't be expected to live forever (due to inevitable connection errors), if you want it to be persistent, you'll need to have code to recreate it when it fails.  This same code can recreate it when the deadline is reached as well.\n. @vitalyisaev2,\n\nUnfortunately I'm not aware of 'wait-for-ready` RPC semantics\n\nThere are other ways an RPC can get stuck without a deadline.  If the resolver never resolves the target name when the client is created, even fail-fast (non-wait-for-ready) RPCs will block indefinitely.  In general, it's a bad idea to perform unbounded, blocking operations.  If you have other things in place to prevent it from blocking forever (like SIGINTing the whole process), then I guess it should be OK.  But I'd prefer for our \"official\" examples of streaming RPCs to use a timeout.\n\nthe stream will be canceled and next time download will start from the very beginning.\n\nThis may be a problem with the API design; if you're transmitting large files over an unreliable network, you should ideally be able to resume them from the middle after reconnecting.\n. See also: https://groups.google.com/d/msg/grpc-io/XcN4hA9HonI/5PskjxpYAAAJ. @immesys, Random thought along the lines of @MakMukhi's reasoning: you aren't connecting through any kind of proxy, are you?. Hmm, sounds like there should be no pings in the system except BDP pings, then.  Hopefully you can try the suggestions from @MakMukhi and get back to us.  Thanks!. Is there any way to figure out why this was happening to @immesys so reproducibly, but none of our tests encountered it?. @hexfusion, @menghanl is out of the office for a about 2 more weeks.  I believe before he left, he said he wanted to implement \"unix\" scheme support a little differently (i.e. by registering the scheme as a resolver), but let's wait until he gets back to make any decisions here.  Thanks!. Note that it's possible to create your own Status proto message and use it directly: https://godoc.org/google.golang.org/grpc/status#FromProto.  This means you don't need to use the ptypes functions at all, even when you have details to include with your status.  We'll still serialize the status using proto.Marshal() when writing to the wire, however.\nUnfortunately, grpc has always been coupled with proto, which is why status was implemented this way.  I'm not sure if there are any acceptable solutions here that both remove the tight coupling and also maintain backward compatibility.  I'm happy to hear about how we might fix this without needing a version 2.0, but our team doesn't have the bandwidth to take this on for some time.. I don't think we've seen this in a long time; closing.. @yogeshpandey we do that step.  When I approved the tests were still running in Travis.  They're all green now, so I'll do the merge.. Recv is blocking. io.EOF from Recv means the client called CloseSend (or SendAndClose).\nThere is a manual for our generated code on grpc.io, but I agree we should have better godocs or pointers to the manual from the generated code.. > So it's not really possible to do send and receive both from the same goroutine?\nRight, this is not recommended (but it is technically possible if you control both server and client and can ensure they work together properly).  Send() is also blocking, so if the other side is not receiving, it will eventually run out of flow control and block until the other side does call Recv().  This could lead to a deadlock if you aren't careful.\n\nIt seems that I should exclude io.EOF in that small goroutine from closing the channel. So that the client can still receive notifications when it no longer wants to change it's subscription. Or explicitly document that the server closes the stream when the client does.\n\nEither way seems reasonable for this use case.  I don't think there's much value in calling CloseStream from the client except when it's ready to end the stream, and then you can get the final status in case it's potentially interesting (which you can't get if you have to cancel the context to end the stream).. @srini100, note that we've recently added some stream documentation here:\nhttps://github.com/grpc/grpc-go/blob/master/Documentation/concurrency.md\nIs that sufficient for all of these issues?\n. @lyuxuan unfortunately the info in travis is lost when you re-run the job -- better to copy/paste the failure text.  Was this the same race detector error?. @aamitdb We don't intend to make any changes to this API; however, we do intend to replace it eventually.  When we do so, we will make sure we do at least one major release containing both the new and old APIs to provide an opportunity to migrate.  The new API should be very similar, so isn't expected to take much effort to change.  We would like to leave these marked as experimental until then.\nThanks!. My guess is this is fixed by #1889, since you are dialing \"unix://...\" (via dialer.DialAddress).  Can you test with that PR included, and we can cherry-pick it into a 1.10.1 release if needed?. @stevvooe can you try turning on info logging and see if anything useful comes out?  Thanks!\nGRPC_GO_LOG_SEVERITY_LEVEL=INFO (unless you have a custom logger installed).. Is this with #1889?\nWhat is the actual address that is being passed to grpc.Dial()?  Is it \"unix:///run/containerd/containerd.sock\"?\nIn this case, you are confusing our target parsing logic.  We now follow the naming scheme defined in the grpc spec here: https://github.com/grpc/grpc/blob/master/doc/naming.md#name-syntax.  The only exception is our default scheme is \"passthrough\" instead of \"dns\", for historical-compatibility reasons.\nBecause you actually want your dialer to see \"unix://</whatever>\", you need to manually specify the passthrough resolver like so:\n\"passthrough:///unix://</whatever>\"\n(When #1741 is fixed, you won't even need a custom dialer for unix sockets, so this whole mess would not be a problem.)\n. > Will all due respect, I am confusing nothing. We updated the package and now the dialer code\n\ndoesn't work. Whatever changes were put in have created an incompatibility that is breaking\nour code.\n\nThe behavior change brings us in line with the gRPC spec. I'm sorry for the breakage, but your input happens to exactly follow the format the spec defines, so it unfortunately doesn't end up following the fallback behavior intended to maintain backward compatibility for most users.\nYou should be able to fix this by prefixing your target with \"passthrough:///\".  Then the full text after that will be handed directly to your custom dialer, as it was before.  Let me know if that doesn't work and I'll take another look.\n. > Is the unix dialing scheme even supported?\nunix scheme support is covered by #1741.  Until it is done, you still need to inject your own dialer, and either:\n\nDon't use \"unix://\" at the start of your target, or\nUse \"passthrough:///\" at the start.\n\nOnce unix support is done, you will not need a custom dialer.\n\nFor example, if you parse off the scheme for unix:///foo, the correct answer for the path is /foo, not foo.  That doesn't seemed to be clear in the spec, but that is the correct way to handle URIs in most other systems.\n\nIn the spec, it says: scheme://authority/endpoint_name\nIt's true that http URLs grab the separating slash along with the path, but in this case, pulling the slash along with it would not be usable; the endpoint name is typically a DNS name like \"google.com\".  Parsing it out as \"/google.com\" would be a problem.\nFWIW, there was a gRFC published for these changes before they were implemented, since they were fairly significant.  You can find them at the proposal repo here: https://github.com/grpc/proposal/blob/master/L9-go-resolver-balancer-API.md. @stevvooe When the unix scheme is supported, you'd use it like so:\nunix:////a/b/c (i.e. unix:/// specifies the unix scheme with no authority and /a/b/c specifies the path)\nYou can use your existing custom dialer similarly:\npassthrough:///unix:///a/b/c will pass unix:///a/b/c to your dialer, which is free to interpret that as it wishes (e.g. remove unix:// and treat the path as /a/b/c).. cc @markdroth. Apparently C implements it this way, despite what is in the gRPC naming spec (which is what we used for our implementation).  We'll look into this: #1911.. @stevvooe I filed #1911 to track the differences between Go and C.\nIn the meantime, please use passthrough:///<whatever> to make sure your dialer gets \"<whatever>\".. Hi @ArkadyB, this sounds more like an envoy question than a gRPC one.  I'd recommend filing your question over there instead: https://github.com/envoyproxy/envoy\n. > I am pretty confident my change could not have induced the test failures. \nThe Go1.9 run says rpc_util.go needs to be run through gofmt (this is the only build that does the lint/etc checks).  The other failure is a known flake.\n. Sorry for the delay, and thanks for the implementation.  This is, indeed, what I meant.  But seeing it in practice, I'm not sure how I feel about this from a usability perspective.  It may be better if the <Foo>CallOption types themselves were exported; then the user could do:\nfor _, co := range callOpts {\n  switch o := co.(type) {\n  case *grpc.HeaderCallOption: *o.HeaderAddr() = ... // or *o.HeaderAddr = ?\n  case *grpc.TrailerCallOption: ...\n  }\n}\nWith this version, I think users would need to do:\nfor _, co := range callOpts {\n  if o, ok := co.(interface{HeaderAddr() *metadata.MD}); ok {\n    *o.HeaderAddr() = ... // or *o.HeaderAddr = ? or o.SetHeader()?\n  } else if o, ok := co.(interface{TrailerAddr() *metadata.MD}); ok {\n     ...\n  }\n}\nWith the types being exported, it would be more documented as well (i.e. the exact method signatures would show up in godoc).\nWhat do you think?\nI'd also like to mark anything we do here as experimental, and give it some time before we decide to definitely go with that approach.\n. > As far as marking it experimental, would it suffice to just document all of the new exported types like so?\nSounds good.\n\nOr should I also make note of it in the call option factory function (since its return type is part of the experimental API)?\n\nNo; those should continue to return CallOptions as their type.  If you want to mention in the docstring that it returns a particular type, then you could write something like:\n// Returns a CallOption implemented by HeaderCallOption (experimental).\nBut that would be optional IMO.\n. Sorry, but this is not actually grpc related.  This is some kind of interaction between the cloud libraries, opencensus, and dep.  The error is coming specifically from this file: https://github.com/google/google-api-go-client/blob/master/transport/grpc/go18.go.\ncc @jba in case he can help.. A couple observations/quesitons:\n\nThe transport package is supposed to be grpc-internal-only; this would presumably expose an API for users?\nIf grpc-go natively had an in-process transport, would this still be useful?\n. @Kevin005 could you file an issue for this, please?. Thanks for the PR.\n\nI'm fine with this change, but with the caveat that this API is experimental and will be redesigned in the somewhat-near future.  There is a fundamental problem with this stats API and the capabilities of replay.  I plan to address this in a gRFC at some point in the future.\nIn a revamp, I don't intend to pass either begin or end time, since those are literally time.Now() right before the stats handler is called; the stats handler can keep track of time on its own instead, if needed.. > Are there preliminary comments about the \"capabilities of replay\" you mention?\nSorry, I meant to say \"retry\", but it's specifically the replaying of messages that are a problem.  The gRFC for retry is here for reference.  The issue with stats is that we include the message payload in the call to the stats handler, but the payload is no longer owned by gRPC after the Send() call that contains it returns.  However, we may need to retransmit that message on subsequent retry attempts.\nGenerally, though, the stats API wasn't designed with retry in mind.  E.g. there is no distinction between an RPC call being performed by the application and individual attempts being sent on the wire.. Note that 8089 is specifically about the \"file\" scheme; we are implementing 3986, which is support for URI schemes generally.  I would rather not make assumptions about the format of the target to determine whether to fallback to a default; that just makes things more complicated.\n. @stevvooe,\n\nIf we look at godoc.org/google.golang.org/grpc/naming#Update for example, it doesn't really have all the necessary inputs for dialer. Also, if we look at godoc.org/google.golang.org/grpc#BalancerConfig, the dialer there doesn't match the dialer from the stdlib, at all.\n\nThe naming package and grpc.Balancer are part of the old API.  The new functionality is in the balancer and resolver packages.  Sorry for the confusion; we will update the docstrings for the old API (to mark it as deprecated instead of experimental) soon.. Exactly.  In retrospect, we should have kept this behavior out of Dial entirely, but now that it's done, we shouldn't make further changes to avoid breaking those relying on the current behavior.. Servers dictate how many streams are allowable.  In gRPC, clients limit themselves to 100 streams until they receive the first settings frame from the server.  If that contains a SETTINGS_MAX_CONCURRENT_STREAMS, then we honor that; otherwise we remove our limit and go unlimited due to this code here:\nhttps://github.com/grpc/grpc-go/blob/2c2d834e8e13f239343a3441b76b0efcc167a0d7/transport/http2_client.go#L946-L955\nYou can configure your server using the grpc.MaxConcurrentStreams ServerOption (which is unlimited by default for Go servers).. This SGTM as a short-term fix to this problem with the caveat that the interceptor API is marked as experimental, and I fully intend to redesign it in the near future.  Let's track that effort in #1805.\n. @jhump sorry for the delay.  I talked about this for a long time yesterday with others on my team.  This limitation seems somewhat related to the general problem in Go's proto library of the service descriptor protos being hard to access (https://github.com/golang/protobuf/issues/489).  The only way to get them is via the filename containing the service.  If there was another way to look them up using the service or package name, then I don't think we would need anything special in the grpc codegen for this at all.  (Does that sound correct to you?)\nAs a similar approach, but without waiting for the proto redesign in https://github.com/golang/protobuf/issues/364, perhaps we could add a grpc service registry in the same vein.  At init() time, we would register a map from service (or service/method) name to the parent FileDescriptorProto.  This would avoid the need to store the data in the context on the critical RPC path.  (This is still just a brainstormed idea / strawman and not necessarily the approach I think we should take.). > Do you know if there is already an issue in the protobuf project about this (registry of known services)?\nI added a question in this old/closed issue (https://github.com/golang/protobuf/issues/489#issuecomment-453595624) to see if there is already something like that planned for the v2 API.  I believe the answer to that question will be \"no\", at which point we could reopen the issue or file a feature request for it.\nWe can see what happens from there, then?  If it can be added to the v1 API, and reasonably soon, then maybe we can close this issue.  But if not, or if they prefer not to implement it, we could add a registry to grpc.\n. Update: https://github.com/golang/protobuf/issues/489#issuecomment-453672590\nLooks like the v2 API will support exactly this need.  Do you still need this functionality before that is ready, and is there a low-impact, temporary solution that could work for you if so?. Given that proto API v2 will make this change unnecessary, and I believe that's ultimately the \"right\" fix, I'm going to close this.  Please feel free to comment if you have any other concerns.. @mastersingh24,\n\nbut I guess WithBlock() negates those options\n\nWithBlock() makes Dial() block until a connection is successfully established, and either returns nil or context.DeadlineExceeded on success/timeout, respectively.  It is conceptually similar to making a non-FailFast (AKA wait-for-ready) RPC.\nNote that FailFast is the default, and always should be, so you don't need to explicitly set it.. We can't do this without changing the stats handler API (which we need to do anyway because of an incompatibility with retry).\nHowever, we can avoid this contention by deriving the transport context from context.Background() instead (without wrapping in a Cancel/Deadline context).  This will make canceling a child context not recurse into the parent.\n. @cstockton Are you thinking of CloseSend on the client or the server?  This bug covers client-side; I don't think you could finish the clientStream or abort that goroutine when CloseSend is called, since the RPC is still active and closing send to signal to the server that the client is done is a common use case.  On the server, this goroutine doesn't exist, so nothing there is necessary.. #2599 should fix this, for the most part.  Still not returning the raw HTML, however, but I think we should cover that with a separate feature request instead of as part of this bug.. Thanks for filing the issue.  As you note, it's improper to append directly to the slice, as that can modify the parent slice.. Thanks for the PR.\nInstead of this approach, what if WithDetails() does a type assertion, and if the parameter is already an any.Any, it appends it directly.  Would that satisfy all the same use cases?\n. @johanbrandhorst Thank you for the code samples.  However, the second sample doesn't really look right for what is proposed in this PR.  You should show FromError being used, not WithDetails.  And please consider the definition of the custom error type itself as well, which must include Error, GetCode, GetMessage, and GetDetails methods, with an implementation of the type returned by GetDetails having GetTypeURL and GetValue methods.  This part seems extremely complicated to me.  You'd be better off IMO doing example 1 (or using FromProto), but putting the reusable parts of the logic into a separate function.\nAlso, what is the concern with using FromProto?\n. OK, so you want this so applications can return a gogo/status.Status.Err() from RPC handlers, and then we can convert it into a grpc/status.Status from within gRPC.  Correct?\nI would prefer to do this slightly differently:\n\nExport the existing status() *Status from grpc/status.statusError (as Status())\nChange grpc/status.FromError to check for a Status() *Status method and call that, instead of type asserting to *statusError.\nImplement Status() *grpc/status.Status in gogo/status.\n\nThis was actually part of an earlier implementation, but I made it more restrictive, because this use case was not apparent at the time.\nI think this would have all the same benefits for you, but it would keep things simpler and easier to understand in our API.  What do you think?. This is still not true, unfortunately.  I'm not sure when we will drop 1.6 support, but hopefully it can be soon.  There are several code cleanups that can come with it as well.  If you still wish to clean things up, I'd just make it say that support for 1.6 will be dropped \"soon\".\n. > Still trying to figure out how we can manage refreshes.\nI'm not sure what you mean by \"refreshes\".  Do you believe this is a gRPC thing or a bigtable client library thing?  If gRPC, could you please clarify?  Otherwise, if you have everything sorted out from the gRPC side, please close this.  Thanks!\n. @maksharma if you haven't solved your problems yet, it might be better to open an issue against the google-cloud-go repo.  Thanks!. @mpuncel yes, that's right -- the first API was grpc.Balancer, which was marked as experimental before GA.  This API had some limitations that prevented us from implementing service configs (i.e. we could not dynamically switch the balancer).  We have no firm plans to remove it, but I wouldn't rely upon it.\nAny changes to the new API are expected to be minor.\n. > Just curious, is there a specific case where this clashes currently?\nYes, we caught this when importing the change internally, because we produce errors that comply with another status package via the same method name.  This same kind of problem could apply to gogo's status package as well if a status system like this existed in another RPC system, so I think this is a reasonable change.\n. @wy-zhang \nCan you provide a minimal reproduction test case?\nI don't think I'm following the scenario very well.  Do RPCs succeed for the first 20 minutes?  That server error, if it happens for every connection, should mean the connection is never fully established.  It could be unrelated.  Does this reproduce if you run the client and server on the same machine and dial using localhost?\nAre you connecting through a proxy or firewall that might be enforcing a maximum connection age?\n. @euroelessar yes, we will make a patch release with this.. > Or maybe we should just deprecate grpc.DialContext and recommend users who want to\n\ntimeout the dial use the WithDialer option and use a timeout inside of that.\n\nThis has different connotations.  A grpc.ClientConn is not a single connection, it is a connection pool.  If you apply a timeout in your dialer, that applies to that particular connection attempt.  If you apply a timeout to Dial, that applies across all of the initial connection attempts.\nIn #1786, I have proposed deprecating Dial/DialContext and replacing them with a NewClient[Conn] instead, which would make that distinction more apparent, and eliminate blocking client creation altogether.  (Users would still be able to use the connectivity state API to achieve the same result, if desired.)\nIt would probably be appropriate to mention the non-blocking default behavior in the docstring.\n. @alltom,\nTo answer your specific question:\n\nI thought that Dial might return an interface that I could implement on top of a peer-to-peer connection, but it looks opaque. Is there an override point that I could use to dial in a custom way?\n\nYou can provide the client with a custom dialer that returns a net.Conn interface:\nhttps://godoc.org/google.golang.org/grpc#WithDialer\nLet us know if you have further questions about this.. @asimonov, I'm not familiar with the Python API.  You could ask on grpc-io or in an issue on the grpc repo.. In order to look into this, we will need more information:\n\nWhat does your program do, specifically?\nWhat other errors are you seeing?  The goroutine trace above only shows that the server is blocked on flow control quota, which might mean the client was not receiving the server's messages quickly enough.  These are not errors, and shouldn't lead to an error.  If there is a FATAL file with errors in it, there should be more lines in there.\n\n1857 is most likely unrelated.\n. The base package is a convenience package to help you implement a balancer when your use cases are simple.  You should implement your own balancer.Builder -- the Balancer it returns can maintain state for that ClientConn.. List of packages missing package-level comments:\n\n[x] balancer/grpclb/grpc_lb_v1\n[ ] benchmark/client\n[x] benchmark/grpc_testing\n[ ] benchmark/server\n[ ] benchmark/stats\n[ ] benchmark/worker\n[x] channelz/grpc_channelz_v1\n[x] credentials/alts/internal/proto/grpc_gcp\n[ ] examples/helloworld/greeter_client\n[ ] examples/helloworld/greeter_server\n[ ] examples/helloworld/helloworld\n[ ] examples/helloworld/mock_helloworld\n[ ] examples/route_guide/mock_routeguide\n[ ] examples/route_guide/routeguide\n[x] health/grpc_health_v1\n[ ] interop\n[ ] interop/client\n[x] interop/grpc_testing\n[ ] interop/http2\n[ ] interop/server\n[x] reflection/grpc_reflection_v1alpha\n[x] reflection/grpc_testing\n[x] stats/grpc_testing\n[x] stress/grpc_testing\n[ ] stress/metrics_client\n[ ] test\n[ ] test/codec_perf\n[x] test/grpc_testing\n\nNote that a handful of these are generated.\n. @zelahi sounds good, thanks!  One for all of them is fine, or if you want to break it up into smaller chunks, that's fine too.  One per package is probably too much, though.\nObviously the generated packages don't need comments (unless the generator was updated to produce them).. We should document balancer.Register to make it clear that it must be called during initialization time.. @tailnode sorry for closing this too early, then.  Please update if you find out more, or if you can provide a reproducible test case, and we will reopen it.. Please rebase and ping when done.  Thanks!. If you are able to reproduce this, please let us know and we'll look into it.  Thanks!. > But what if we later want to add another field to resolver.BuilderOption that's not to be configured by the user?\nI can't come up with anything that makes sense for this.  Building the resolver is essentially the first thing we do, so it seems like its configuration can only come from the user.  But if that is the case, we have (at least?) three options:\n\nA different dial option for every resolver.BuilderOption\nDocument the options the user shouldn't set (i.e. we will ignore/override the user's setting)\nresolver.UserBuilderOptions as a sub-field of BuilderOption  (As an aside, it's bugging me that this wasn't \"BuilderOptions\".  It will break people using it, but...should we change this now before even more people are using it?)\n\nI think I would prefer 2, 3, then 1, in that order.\n. @cstockton you can call the context's cancel() function to fully terminate the stream on the client side, without needing to receive all of the data from it.. OK, I see what you're saying now.  #1933 covers this same concern.  Note that our hands are a bit tied with the current API, due to the use of interfaces, so any changes here will be harder to use than we'd like.  I.e. you'll need to do something like one/either of stream.(io.Closer).Close() or grpc.CancelClientStream(stream).\n. We can export this function, also as a temporary experimental feature, to unbreak you.  Note that I intend to redesign interceptors and stats handlers (#1805) in the next month or two, because of several deficiencies in their design.\n. I have a feeling Travis won't like this, since our tests still call many \"deprecated\" functions.  You might want to limit it to just the one function for now.. @Chyroc \nThanks for the extensive fixes!\n\nshould i fix protoc-gen-go or add it to staticcheck -ignore ?\n\nWe should revert the deprecation tag on it until after protoc-gen-go is updated to not use it.  I wouldn't want to set off other people's staticchecks.\n. @gertcuykens Both sides of a bi-di stream are Streamers and Receivers.  The type names used here are <Service>_<Method>{Client,Server}, because distinct types are needed for the two sides, one used by the client and one by the server.\nI wouldn't argue that the names chosen are the best options possible, but it's also not possible to change them without breaking backward compatibility.\n. > I expected a blocking dial to return some indication of why it had failed, instead of only timing out. In particular, including the most recent transport error in the returned error value would be very useful for reporting connection problems.\nThe trouble is that this isn't really a \"Dial\" function, per se, despite the name.  grpc.Dial creates a ClientConn containing a managed pool of connections.  The blocking behavior of Dial is essentially to start up this pool and wait until a connection is established to one, or the deadline is met.  Authentication problems could be a misconfiguration of a server or a client, and they could resolve themselves transparently to the client (e.g. if the server restarts and receives new certs).  For this reason, we do not give up when authentication problems are encountered; we keep trying until the deadline is reached.\nChanging the returned error may be possible, but it would be a behavior change.  Anyone checking for Dial returning context.DeadlineExceeded could be broken by that change, because context.DeadlineExceeded is a value and not an interface, so it cannot be extended.\nThe longer-term plan is to deprecate Dial and replace it with a NewClient that is both clearer in its behavior, and can have the semantics we prefer.\n\nMy system isn't using fail-fast because we want the gRPC library to retry past transient network issues.\n\nThe difference between fail-fast (the default) and wait-for-ready RPCs is that WFR will remain pending through connection establishment errors.  Once they are assigned to a connection, RPCs that encounter transient network errors will still result in failures.  Auth failures are considered connection errors, so all your WFR RPCs will deadline exceed when this happens and you can't connect to any backend.\nNote that our implementation of fail-fast used to be wrong ~5 months ago: it would not block when the client was in a \"connecting\" state, it would just fail.  Now that that is resolved, it may be suitable for your use case.\n. Update: I don't think we should change the behavior of Dial to return anything besides context.DeadlineExceeded in this case.  That would be a behavior change and not a true bug fix, and it could break some users.\nHowever, I do think we can and should modify the text of the error that comes from a wait-for-ready RPC when it exceeds its deadline while waiting for a subchannel to include the details from the last connection error, like we do with non-WFR RPCs when we encounter transient failure.\nHow does that sound, @jmillikin-stripe?\n. No.  In that case, you'd want to do a non-blocking dial, and then poll connectivity state until you get either TransientFailure or Connected (or some deadline).  On TransientFailure, we could then add a method to our connectivity state API similar to what Java has* to provide the relevant connection error.\n: https://godoc.org/google.golang.org/grpc#ClientConn.GetState and https://godoc.org/google.golang.org/grpc#ClientConn.WaitForStateChange\n*: https://grpc.io/grpc-java/javadoc/io/grpc/ConnectivityStateInfo.html#getStatus--\n. I created a PR (#2055) as a proof of concept for this.  In light of some offline discussions, I still plan on reworking it a bit.. In light of #2266, if the TLS error is non-temporary, then using WithBlock in combination with FailOnNonTempDialError should get you the behavior you want.. Yes, that's right.  It's only failing in 1.10 because we don't run the vet/lint/etc checks other builds.. What version is your local go?\n...Now I'm even more confused.  The output shows it grepping for \"unsafe\", but if I look in your branch, your vet.sh doesn't have the change to do that yet.\nI think we should remove the \"-l\" parameter from the command-line so we can see the full diffs.  You can do that in your branch, anyway, to get travis to output it.\n. > I'm running go1.10\nMissed that.  Are you not running 1.10.1?  That's what travis used.. > I'm not entirely sure if this is the correct solution, but figured I start the conversation with this PR. Please advise\nThis is the right fix.  It was not correct to replace the call to newClientStream with cc.NewClientStream; only NewClientStream calls should have been redirected.. @pavelkalinnikov done.  We just needed the CLA signed.  Thanks @igorbernstein2!. Please make sure your proto package is up-to-date: go get -u github.com/golang/protobuf/proto. Great, thanks for sharing the fix.. Closing for now until I get a chance to revisit.. @carl-mastrangelo there are some golint problems if you check the travis output.  Can you please take a look?  Thanks.. \u200bCould you put the go:generate command in another, existing, alts .go file\n(whichever one makes the most sense)?\n. > this message would be print even if err is nil. /transport/controlbuf.go@master#L371\nThis function (run) is an infinite loop, and does not return unless there is a non-nil err.\nThat said, it's not useful to log (especially as an error) things that are normal termination conditions.  We should update this accordingly.\nWhat you've done looks mostly OK.  I have two concerns:\nThe first is that you don't call Close() on the ClientConn in the event of an error.  Better to defer Close instead:\ngo\nfunc YourFunction() (err error) {\n  conn, err := grpc.Dial(...)\n  if err != nil {\n    return err\n  }\n  defer func() {\n    closeErr := conn.Close()\n    if err == nil {\n      // Otherwise, we will return the primary error and ignore the error from Close.\n      err = closeErr\n    }\n  }()\n...\nThe second is that you have used context.Background() for your RPC context.  This can be problematic.  It would be better to either use a context with a deadline or make a child context with a cancel.  I.e.\ngo\nctx, cancel := context.WithDeadline(context.Background(), 5*time.Second)\n// or ctx, cancel := context.WithCancel(context.Background())\ndefer cancel()\nFailure to cancel the context, or Close() the ClientConn (which you do here), or Recv from the stream until receiving a non-nil error (which you do not do here), will leak a goroutine.\nOne last note is that ClientConns are intended to be long-living.  So if you have other RPCs you want to make to the server, or if you want to do this multiple times, it's better to create it once and keep it for as long as you need it, rather than per-RPC.\n. > But it's unclear to me if or how a server could indicate this.\n@jpbetz We intuit this by seeing a GOAWAY with a stream ID that indicates the stream was not processed or by getting a RST_STREAM with error code of REFUSED_STREAM.  You cannot control the former; the latter can be controlled by setting https://godoc.org/google.golang.org/grpc#InTapHandle, but I would not recommend using this if possible.\nFull retry support is in progress.  Unfortunately this has been going much slower than I had hoped, due to other demands on my time more than anything else.  I hope to have it done within 2-4 weeks.  Until it's fully ready, the best way to accomplish this would be by using interceptors.  There is a package that does this available here: https://github.com/grpc-ecosystem/go-grpc-middleware/tree/master/retry.\nLet me know if you have any other questions about this.. Note per the linked issue that the status code here is supposed to be Unimplemented instead of Internal.\nEDIT: for future reference, this can be easily tested via:\n$ go run interop/client/client.go --server_host=dsldksd.googleapis.com --server_host_override=dsldksd.googleapis.com --server_port=443 --use_tls=true\nFATAL: 2018/12/05 16:33:19 /TestService/UnaryCall RPC failed: rpc error: code = Internal desc = transport: received the unexpected content-type \"text/html; charset=UTF-8\"\nexit status 1. @menghanl - Why?  Is there a related issue?  Or what happened?. Ping @dreamflyfengzi - if you can provide a reproduction case for this, it would help us to diagnose and fix it.  Also, can you confirm what version of grpc you were using?. @dreamflyfengzi thanks for the update.  Could you try with the latest release, and if you're still seeing a problem, share the code you're using to get these results (ideally reducing it to a minimal reproduction case first)?. \nReview status: 7 of 10 files reviewed, 8 unresolved discussions (waiting on @menghanl)\n\nclientconn.go, line 445 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nHow about moving this to an environment variable instead?\nThe advantage would be, no code change will be needed when we enable retry in the future.\n\nDone.\n\nclientconn.go, line 1054 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThere are things similar to this done in handleServiceConfig.\nMove those to this function, too? Or add TODO?\n\nDeleted setSC after unexporting retry configuration fields from service config structs.\n\nclientconn.go, line 1633 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMake a util struct for retryTokens and the following functions?\n\nDone.\n\nservice_config.go, line 100 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nunexport this field. The only reason to export fields in this struct is to use WithServiceConfig, but we don't want users to use that.\n\nDone.\n\nservice_config.go, line 106 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis type doesn't need to be exported, either. Unless we want to provide another way to config retry other than service config.\n\nDone.\n\nstream.go, line 433 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNot sure if this is what the TODO is talking about.\nThis variable is not used until line 463. Move this down after the ifs?\n\nI meant to write \"not check pushback\" instead.  It is important to check the server pushback value before calling throttleRetry() (which has side-effects), but there was talk of changing the spec to call throttleRetry() first instead.  Comment updated.\n\nstream.go, line 439 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis is inaccurate if e != nil.\n\nIf e != nil, that means a non-numeric response was provided, which the spec says is a valid way for the server to indicate retry should not be performed.\n\nstream.go, line 468 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nTo avoid the for loop, how about comparing\n  cs.numRetriesSincePushback with\n  \\log_{multipler}\\frac{max}{init} math link\nThe latter value can be calculated every time service config gets updated.\n\nI removed the loop and replaced it by math.Pow(), but left out the optimization that avoids the need to call Pow() when we've hit the max backoff, since it should be quite rare and a minor win.\n\nComments from Reviewable\n Sent from Reviewable.io \n. This should be ready to review again, thanks!. \nReview status: 14 of 16 files reviewed, 5 unresolved discussions (waiting on @menghanl and @dfawley)\n\nstream.go, line 329 at r9 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDelete this struct.\n\nDone.\n\nstream.go, line 339 at r9 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nnewStream() could be retried, so it shouldn't call done\ndone is already called by finish.\n\nThis actually was necessary, because I wasn't calling finish on the attempt in retry, which is another bug.  Done.\n\nstream.go, line 427 at r9 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis comment is inappropriate. The wait happens in the TrailersOnly call.\n\nDone.\n\nstream.go, line 444 at r9 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nTrailersOnly also returns true when there's a connection error. But the stream is not necessary trailers only.\n\nI know, but names are hard.  NoHeaders?\n\nstream.go, line 581 at r9 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nCould s be nil?\n\nYes, if the latest newStream attempt failed.  Done.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 12 of 15 files reviewed, 2 unresolved discussions (waiting on @menghanl)\n\nstream.go, line 266 at r10 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMove statsHandler and traceInfo as parameters for newAttemptLocked()\n\nDone.\n\nstream.go, line 560 at r10 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nShould this return the previous received trailer?\n\nNo...but I added a commitAttempt here since it will prevent races when users don't use the API correctly.  And updated the comment.\n\nComments from Reviewable\n Sent from Reviewable.io \n. :relieved:. @annismckenzie \n\nSetting the env variable will turn retry support on? Or does that only work when using a Service Config which is also not documented anywhere?\n\nYes, you will need to do both (set environment and enable in service config).  Service configs are documented generally here, and in Go are set via the resolver (by calling ClientConn.NewServiceConfig).  Our default DNS resolver will provide a service config if one exists in your TXT records (more info in the SC-in-DNS gRFC).  Note that for backward compatibility reasons, the DNS resolver is not the default resolver in grpc-go, so you need to use grpc.Dial(\"dns:///<target>\") to use it if you decide to go that route vs. making a custom resolver.  The retry settings of service config are documented the the retry gRFC.  Let us know if anything isn't clear or you need more specific help.\n\nthe stab at the docs also isn't meant to be condescending\n\nIt's totally fair.  We know our documentation is not ideal.  Any specific suggestions you have about places where we can improve them and how to do so (e.g. what's better: godoc or our repo's Documentation directory or grpc.io) would be appreciated.. Hi @johanbrandhorst,\n\nI'm guessing this is something we can all agree is a good idea even if it might not be of the highest priority.\n\nI agree this sounds good in theory.  In practice, however, I don't think it will ever be possible to delete the grpc HTTP2 stack and replace it with uses of net/http.  There are a number of features in the grpc transport that are not present in the net/http client/server.  For example: keepalive, max-idleness detection, and hooks for stats and interceptors.\nIn the upcoming quarter, I intend to redesign the interface between the grpc layer and the transport, make it public, and introduce a way to inject alternate transports.  This should make it straightforward to implement a transport based on the net/http package instead.  I don't think that could be our default for the reasons mentioned above, but we may be able to support it as an alternative.. @nhooyr It may be possible, but you're talking about upstreaming a lot of features that the Go team may not want in their server.  Also, we have done a lot of performance tuning work, so we would have to make sure performance was the same or better before switching.  In the end, the effort would be a lot of work, and we have higher priority things on our plate.  When we complete the refactor to allow for alternate transports, something built on the standard http2 server could be an optional one.\nI'm not sure if you're aware, but we do have a ServeHTTP that makes our server work with the standard http2 implementation if you absolutely need it, but it's experimental.\n. Let's track this effort under #906, which predates this issue.. Since we claim to support appengine, I think it's about time for some appengine tests.\nLooks like that's possible locally without too much trouble: https://cloud.google.com/appengine/docs/standard/go/tools/localunittesting/\nSo in Travis, we could add a target that does:\nFILE=\"go_appengine_sdk_linux_amd64-1.9.64.zip\"\ncurl -O \"https://storage.googleapis.com/appengine-sdks/featured/${FILE}\"\nunzip \"${FILE}\" # Sadly this cannot be a pipe w/curl because of the zip file format\ngo_appengine/goapp test ./...\nLooks like there's also a problem that go get doesn't naturally fetch the appengine-conditional dependencies, and I couldn't figure out how to force it to (I tried go get -tags 'appengine appenginevm' but that didn't help).  oauth has appengine dependencies that goapp test makes it require.  I'm sure there's some way to do this, but I had to manually get the dependency:\ngo get -u -d google.golang.org/appengine/...\n. Let's wait until we do a release (next week!) with #2137 before doing this.  We can drop a note in the release notes as a heads-up.. Let's only do this for one go version, and add a new travis instance for it.  Otherwise our testing time will double, which would not be desirable.. @josephusmv Sorry, we would prefer to have one set of possibly-confusing function names than two different APIs to document and support.  We should fix any problems of confusion surrounding the current API with better documentation instead.  Thanks for the suggestion.\n. Even if you are notified as a message is written to the wire, as suggested in @MakMukhi's comment above, it is still not an indication that the server will ever receive it.  It could be lost if a network error occurs after transmission.  It could be lost if the server crashes.  Or, it could sit in a buffer on the server, waiting to be Recv()d when stream cancellation happens.  Cancellation is ~immediate, meaning it will abort the whole RPC ASAP and result in lost messages from the server's internal buffers.\nThe only way to ensure the server received your messages is to either build it into your protocol (via streaming ACK responses) or wait for successful RPC completion.  This is a fundamental part of the way gRPC works, and all gRPC implementations have this behavior.\nIf you want your stream to outlive another context, don't derive your stream's context from the other context.  It is admittedly a bit unusual in Go for a context to outlive a function call, but cancellation propagation is a core gRPC feature, this was the most natural way to express it, and this mechanism works similarly in the other languages as well.\n. > A call to Send|Write blocks until a corresponding system call has completed.\nThe gRPC-Go streaming API is not intended to be conceptually at the same level as file descriptor operations.  Applications that write to FDs need to know when syscalls are completed for various reasons.  gRPC streams are a much higher level API.  Applications that write to grpc streams have no reason to worry about syscalls; they should only be concerned with message delivery guarantees.\n\nto understand delivery guarantees one must dig through thousands of lines of code\n\nThe comments on SendMsg explain this (possibly changed after this issue was filed):\n// SendMsg does not wait until the message is received by the server. An\n    // untimely stream closure may result in lost messages. To ensure delivery,\n    // users should ensure the RPC completed successfully using RecvMsg.\n\nEven with a separate background context- how do you eventually give up and measure the progress you have made thus far, given you have no way to check how many of the existing buffered messages resulted in a system call?\n\nYou would need server-streaming ACKs if you are concerned with partial RPC progress (regardless of our API).\n\nperformance gains that sacrifice correctness\n\nMy point is that it doesn't make any difference to the application whether the syscall happens before or after SendMsg returns, in terms of knowing whether the server received -- or will ever receive -- the message.  Given that, there is no reason to block.  It does not impact correctness.\n. @yangliCypressTest do you have any updates on this issue?. @vaishalig2693 - Those logs are client-side logs, not server-side.\nThese errors are happening extremely early in the connection.  You could try setting grpc.WithWaitForHandshake in your DialOptions and see if anything else shows up in the logs.  My guess is the connection was never actually valid.  Are you using TLS or WithInsecure?\n. @vaishalig2693 - everything should work correctly with WithInsecure if the server is configured accordingly.  My guess is you are not connecting to the correct host/port, or the server is expecting your client to use TLS, not plaintext.\n. @jaytaylor this should still be possible.  If you are doing port-sharing on the server, you can use the http server handler implementation (which is experimental) or something like cmux, depending on your needs.\nYou can configure your client to do TLS and Apache can terminate it, and then from there to to the grpc server you can use another secured connection or just forward RPCs as plaintext (via WithInsecure) if your network is considered secure or if you are on the same machine (e.g. UDS or an in-process connection).\nIf you do get this working, please share what you can about your solution.  Thanks!. @jaytaylor Unfortunately I'm not familiar enough with Apache to help with this.  The TLS certificate used for the gRPC client should always be the one that the terminating endpoint expects to see.  Since that's Apache in your case, you need to make sure Apache and the gRPC client are paired -- how to do that in Apache and what options you have is where my knowledge ends.  From the client-side, however, you'd need to use WithTransportCredentials something like this:\ngrpc.Dial(..., grpc.WithTransportCredentials(credentials.NewClientTLSFromFile(certFile, \"\"))). @jaytaylor were you able to get things working with TLS and Apache?. Hi @jaytaylor, I don't know of any list of LBs, but nginx as of 1.13.10 does support grpc (the issue you linked is 2 years old).  Envoy should also work well.  Thanks for following up with the extra details.  Let us know if you have any further issue/comments/concerns.. @gyuho, no, there are no current plans to do this.  Is there a reason you want to perform this check when you get keepalive pings as opposed to alternatives (e.g. polling periodically)?  It may be possible to do something similar using a custom handshaker on the server that kills the connection when certain conditions arise, or by making an RPC initiated by the client that takes the place of the keepalive pings.  I'm not sure I fully understand your design, however, so this may or may not help.. @guyho,\nIf you were to kill the connection, how would the client know which node to reconnect to?\nCould you perform this check in a server interceptor and kill the watch stream (instead of the connection itself) when the partitioning happens?  Note that your streaming interceptor has full control of the stream and can terminate it early with an error at any time -- not just at the start of the RPC or when traffic happens.  E.g. something like this might work:\ngo\nfunc interceptor(...) error {\n  errChan := make(chan error, 1)\n  go func() {\n    errChan<-handler(srv, ss)\n    close(errChan)\n  }()\n  select {\n    case <-partitioned:  // written to by something monitoring for partitioning\n      return partitionedErr\n    case err := <-errChan:\n      return err\n    }\n  }\n}\n\nWe haven't tried godoc.org/google.golang.org/grpc/credentials#TransportCredentials but seems like this is for connection handshake?\n\nTransportCredentials is intended for auth, but could be used to do anything at a connection-level that was desired as well.  That would be a bit of an abuse, though, so probably not a great idea.  Instead, if you really wanted connection-level control, the Listener you give to gRPC could be wrapped in something that wraps the Conns it hands out that do checks like this.. Yes, I think the interceptor pattern would be recommended for this.  Keepalives should only be necessary for our internal implementation, so I would like to avoid exporting a hook for them unless there is a strong enough need to justify it (i.e. there's no other/better way to accomplish the same thing).  Thanks for the request.\n. @cesarghali do you know anything about this?  Do you know if the Go TLS library supports the ability to reload?. It looks like the extent of support in C is to fetch the credentials for every new incoming connection attempt.  This would not mean that existing connections would be affected.\nThe same technique should be possible with the grpc credentials package as well.\n. True.  It should be pretty straightforward to implement this and use it with grpc-go:\ngo\ngrpc.DialContext(...,\n  grpc.WithTransportCredentials(\n    credentials.NewTLS(tls.Config{GetCertificate: myGetCertFunc, ...})))\nI'm going to close this because I don't see much value in any additional support for this in grpc-go.  If I'm missing something, feel free to comment and we will reconsider.. Note:\nThis should be backward compatible as long as the user does not have a custom dialer, because the default dialer of net.Dial(\"tcp\", addr) will behave indistinguishably from our DNS resolver for both hostname[:port] and IP[:port] uses.  We should not use \"dns\" as the default if a custom dialer is used, however, as most users in that scenario will need the target passed through to their dialer verbatim.\n. On second thought, it seems more prudent to avoid changing behavior, even if it's minor, due to backward compatibility.  When there is a grpc.NewClient, the default resolver for that path will be \"dns\".. SendMsg does not return an RPC status -- it returns io.EOF and the client needs to call RecvMsg to determine the status.\n\"To ensure delivery...\" should end with \"before cancelling the context\" instead of \"before closing the stream\".  (The stream is already closed when RecvMsg returns io.EOF and it's not necessary to cancel the context after then - but it's still a good idea anyway.). @elliots The default DNS resolver does not support an authority, so it should error if one is set.  If I understand correctly, you should either always install your custom resolver or set the authority only when it is installed.\nNote that https://github.com/grpc/grpc/pull/15618 is currently updating this documentation to explicitly call out authority as unsupported for DNS with the exception of the c-ares DNS resolver.. @elliots - Yes, thanks!  If you send us a PR, we can review it and take in this functionality.. Fixed by #2265.. Thanks, this is very cool!  However, I don't think we want to merge something like this in its current form, but rather wait for the redesign mentioned in #2112 and then implement something using that system instead.  I'm working on that right now and hope to have something to share in the next couple weeks.  I will close this PR in the meantime, but thank you again for sharing it.\n. Do you have time to look into this this week, Mak?. It failed last night in our cron build.. This error means your x/sys/unix library is out of date.  Make sure go sees an updated version when building.  There are many different possibilities:\n\n$GOPATH/src is out of date: go get -u -d google.golang.org/grpc/... will update it.\nvendor directory is out of date: update your vendored copy of x/sys/unix to a newer verison.\nUsing a version control system like glide: configure it to pull a newer version.\n\nIf using go modules (aka \"vgo\"), then this should not happen unless you've modified our go.mod file, or you have a main go.mod file that pins a specific version of x/sys/unix, or if you are forcing vendoring (via go [build|install|run] -mod vendor).\n. I think we could move some of the implementation of status into internal/ and play tricks to allow ourselves to efficiently access the proto when we know it won't be changing, but still force external users to use it immutably.\n. > I see a lot of people getting it wrong in various ways (e.g. binding to :0 instead of a loopback address\nThis is a problem with Go, though, not grpc; binding to :0 is/should be a loopback address.  net.Listen should work with it.  This is presumably https://github.com/golang/go/issues/22826.\n\nor starting the server before registering service handlers\n\nThis approach wouldn't help solve that problem, though.\n\nIt's not a lot of code, but it cuts down the error rate a lot.\n\nLet's wait until we have a real in-process transport, and write a doc on \"how to start test servers & clients\".  It should be pretty short right now and even shorter then - not significant enough to justify a new library to maintain IMO.. Let's revisit this after the in-process transport functionality is in place.  I think once we have that, the process for starting/connecting to a service in tests should be just as simple as this package provides.\n. I'm able to reproduce this now, and can isolate it to a single run of this test case (i.e. there is no state leaking between test functions or test cases).  It takes many iterations to reproduce, but it should be enough to debug.. Fixed by #2199. That's a good point.  We should have a form of CallCustomCodec that accepts an encoding.Codec instead.  (We can't alter it in-place to take an encoding.Codec, as that would break backward compatibility.)\nNote that the only difference between the two interfaces is the String vs. Name method, which was changed to make it unify with encoding.Compressor, so it should be easy to make one type that satisfies both.. If you need CallCustomCodec (are you sure you need it?), then you should just go ahead and implement grpc.Codec and use it.\nOtherwise, the recommend approach is to use CallContentSubtype and implement your codec using the new interface.  See the relevant docs here: https://github.com/grpc/grpc-go/blob/master/Documentation/encoding.md\nConverting to the new interface should be as easy as changing String() to Name().. @prannayk - This would be a problem for type safety, shifting compile-time type checks to runtime.  Also, call option functions don't return errors, so the error at runtime would have to be a panic, and panics in libraries like this should be avoided at all costs.\nWhat I suggested above should be a pretty simple change to make; I'll just take care of it now to close this out.\n. If you want to control the behavior of the RPCs, another idea would be to write your own balancer.  You can use the simple pick-first or round robin implementations as a starting point if you don't need anything fancy.  Then, in Pick, instead of returning balancer.ErrNoSubConnAvailable, return balancer.ErrTransientFailure and fail-fast RPCs will fail immediately, or any other error for both fail-fast and wait-for-ready RPCs to fail immediately.  (See calling code (ref) for details.)  Note that this may have some adverse effects, e.g. the first RPCs dispatched to the connection will fail until a connection is established.. Glad to hear!  Let us know if you have any other questions or suggestions.. Fixed #2188.. I fixed the parse problems (unquoted [s at the start of a command, which yaml doesn't parse), so this is ready to be reviewed now.. cc @ejona86 . > this disconnects the stream, and when the client re-connects, it happens again.\nThis is unexpected..  It's very possible that, with 20s set on the client and 20s set on the server, a small amount of network jitter will cause an error to happen once.  However, the C client should be slowing its rate down when it sees ENHANCE_YOUR_CALM.  We do this in Go here:\nhttps://github.com/grpc/grpc-go/blob/f3361fda2fe0734510c562684480f18ebb63ac33/clientconn.go#L1196-L1202\nCan you try setting a 10 second MinTime on the Go server and see if the GOAWAYs still happen?. > Just for my understanding, do you mind explaining why the issue was fixed by setting the server's KeepaliveEnforcementPolicy.MinTime to a value smaller than the keepalive time on the client?\nIf both are exactly the same, then a little network jitter will mean that the server believes the client is being abusive.  Example:\nClient and server configured for 20s ping/enforcement time\n- T=0s: client sends ping\n- T=0.5s: server receives ping\n- T=20s: client sends ping\n- T=20.4s: server receives ping\nBecause the server received the second ping 19.9s after the first ping, it will treat it as abusive and tell the client to enhance its calm.  If the server's enforcement time was 10s, then 19.9s is well over that amount and everything is fine.\nThe client is supposed to reduce its ping frequency when this happens.  If that is working correctly, the errors should stop.  If you are seeing C clients continually getting these errors, we can file a bug against the C repo, as it's likely the problem lies there.\n. @yashykt could you look into this from the C-client-side, please? It seems the client isn't scaling its ping timing when it gets pushback from the server.. @yashykt is that by design?  It seems like the scaling should be honored regardless of the pings-per-data-frame setting.. > I am guessing that scaling is being honored. It's just that even after scaling, we are sending pings without data even though are spaced out. The server is probably not expecting that.\nThis doesn't sound right, because our server doesn't check for data frames in its enforcement.  Also, PermitWithoutStream is set, meaning pings even while idle is OK.\n@RahulMitra, can you try this out, please?  Set GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA to 1 and have client and server both set to 20s ping time/enforcement as in the initial report.. @RahulMitra any updates on your testing?. @RahulMitra thanks so much for the update and reproduction code!\nDoes this sound right, @yashykt?  The user is setting PermitWithoutStream, which means the grpc-go server doesn't consider a ping to be problematic unless it falls inside the MinTime window, regardless of data frames being sent.  Here is our logic that implements that check:\nhttps://github.com/grpc/grpc-go/blob/8fbeaf87a68ad9f4e7eaf69d605d0a888277de40/internal/transport/http2_server.go#L686-L705\nI wonder if this setting is just covering up the problem by disabling pings entirely.  Note that the client never sends data; I'm not sure if the server does in the test implementation.  When does the PINGS_WITHOUT_DATA setting allow the next ping?  Is it on sending data or receiving data or both?\nEDIT: actually, it doesn't matter.  If the server is sending data (which is the only way data could be sent in this example), then we reset our strike counter.  So if you only send one ping for every data frame you receive, and we reset our strike counter for every data frame we send, it is impossible to receive pings too frequently.  So from this example, it's not possible to determine whether the scaling is happening or not.  But my guess is it's not, since PINGS_WITHOUT_DATA of 0 should not be a problem for Go servers.\n. @yashykt thanks!  I'll close this one out, since I think the Go side is OK here.\n. @takakawa I think this is working as intended from the grpc side.  The fail-fast setting is intended to fail RPCs quickly if there is any temporary problem with the connection.  I would recommend using grpc.FailFast(false) on any RPCs for which you are willing to tolerate connection errors like these.  Wait For Ready RPCs, on the other hand, will retry once when data is written to the wire but the server does not process it, or indefinitely if the connection was chosen but receives a GOAWAY before the HTTP/2 stream can be created for the RPC.  If you need retries beyond that, Go has recently implemented configurable client-side retry support as documented in gRFC A6 (in PR #2111).\nPlease let us know if you still think there is a problem here.  Thanks!\n. Thanks for the extra information @mastersingh24.  What seems to be happening is that, when there is an error, ServeHTTP is not sending a \"trailers-only\" response like Serve; instead it's sending headers and then trailers as separate HTTP HEADERS frames.  In the headers, it pre-declares some trailers (Grpc-Status/etc), and in the trailers is the RPC's status.  This is actually a perfectly valid way of responding to the request, but there is a small downside in that it prevents client-side retry support from working.\n. It would be a little ugly, but I think instead of calling writeCommonHeaders here:\nhttps://github.com/grpc/grpc-go/blob/46ee6abebf62241b49973422c42250327bb94169/internal/transport/handler_server.go#L198\nwe could just write the \"Content-Type\" header (if common headers haven't been written yet), and never call Flush here, which actually says it's done to force headers and trailers to be separated (which we don't want; maybe I'm missing something else?):\nhttps://github.com/grpc/grpc-go/blob/46ee6abebf62241b49973422c42250327bb94169/internal/transport/handler_server.go#L200-L203\n. No, canceling the context kills the stream ASAP, resulting in the loss of buffered messages.  The only way to ensure delivery of messages is to finish the RPC completely (call RecvMsg until it returns a non-nil error) or build it into your protocol (i.e. the server sends ack responses to individual request messages).. @zjshen14 I believe you are running into a TCP feature, not a problem with grpc.  You cannot re-use a port within a certain amount of time.  Your best bet would be to pick a unique port for your tests (listening on port 0 will find one for you).  For more information about that, this is a good resource: https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linux\nIf you are not listening on the same port in your tests, we may need some more details to help debug this.. Yes, that makes sense, thanks for the suggestion.. Needs goimports/etc.. @huyntsgs How are you setting your environment variables?  Are you using set/setx?  setx GRPC_GO_LOG_SEVERITY_LEVEL ERROR will make it apply for all processes.  (They must be restarted first.)\nAlso, verbosity should be set to \"0\" for \"least verbose\".\n. @huyntsgs Did you use \"setx\"?  Otherwise the setting will only take effect for the current window.. Are you using a custom logger?  Our logger does not print \" [INF] GRPC: ...\"  If anything you are importing is calling SetLoggerV2 in an init, then you will get that logger implementation instead of ours.. @ShiningRush We moved this package to internal to prevent others from importing it, and to make room for new pluggable transports in gRPC.  Our transport package was never intended for external use.  Do you work on micro, or are you a user?  If the former, we would be interested in learning more about its use cases and finding another, more maintainable way to accomplish the same functionality.. @aaronbee transport.ErrConnClosing isn't returned to users through the grpc API.  It is converted into a status package error instead.  Referencing it was not correct.  All errors returned by RPC methods must be implemented by the status package (or io.EOF).  (#531 tried to change that in 2016, but was rejected.)  It can be matched by checking for status.Convert(err).Code()==codes.Unavailable\n. @ShiningRush do you know why micro does not use grpc.NewServer+Server.Serve, instead of trying to integrate with transport directly (essentially reimplementing Serve and below)?  That is the expected way of serving with grpc.\nNote that @asim, who works on micro, is aware of this breakage per his twitter post; I'm not sure about his plans to fix it, but it sounds like he intends to maintain a fork of grpc-go.\nAs a short-term workaround, you could vendor grpc-go to a version prior to e85134f.\n. Please see #2112.. Thanks for the fix @dsymonds.  Could you make sure you accept the CLA, please?  The project is CNCF, now, so there are new CLAs to sign, unfortunately.. @dsymonds You will need to sign up at linuxfoundation.org for the CLAs to be satisfied.  Everyone on the grpc team at google had to do this.  Steps:\n\nhttps://identity.linuxfoundation.org/projects/cncf\nClick \"Sign up ... as an employee\"\nSign in with your @google.com account\n(This is from an old email; I hope it's accurate): Use \"add identity\" and associate your github account with your new linux foundation account.\nDrop a comment here to kick the CLA check\n. @jtattermusch @hsaliak any idea what is going on here and how to fix it?. Cool, thanks for resolving the CLA issue.. If you set HTTP_PROXY or HTTPS_PROXY in your environment, grpc-go will use that by default.  Does that work for you?. @leaxoy could you please clarify this a bit more?  What functions/types would be in the registry package, and how would it interact with the grpc package and with the application?\n. Thanks for the report @fastest963.  We'll revert the change now and look into the root cause and follow-up later.. I think we can fix this so the old behavior keeps working -- i.e. FailOnNonTempDialError would be a fail-fast for blocking Dial calls.  Blocking dials already poll connectivity status; we just need to check the last error when the state is transient failure and exit if it is of the non-temporary variety.. Increasing priority to keep it on our radar since this was an unintentional behavior change.. Closing; if you would like to complete this, let us know and we will reopen.. Please let us know if you have any further questions, thanks!. Thanks, looks good.  Travis isn't running, which usually means you need to rebase your branch with upstream.  Yours looks fine, though, so I'm not sure what's wrong.  Try adding an empty commit and doing another push?. Hi @JelteF, please let me know if you have any issues making travis happy.  Also, could you please agree to the CLAs?  Thanks!. Hi @JelteF, could you please agree to the CLAs so we can merge this?  Thanks!. > Also, thinking about it, should a note be added to the documentation that FailOnNonTempDialError is only useful if WithBlock is also used? It's not immediately clear to me that it does anything if not using WithBlock...\n\nThat would be great, thanks.\nThe whole idea of a blocking Dial is a bit unusual for the gRPC model, where a ClientConn is actually a long-lived pool of connections that may be unavailable at times.  The initial connection attempt is only interesting in that if it fails, it's possible your configuration was incorrect, vs. transient network/unavailability problems. \n Note that in all other languages, gRPC does not attempt to connect until the first RPC is dispatched, and the initial connection is not treated specially.\n. From #1663, your options here are to set max connection age or make a custom resolver that behaves the way you like.  Note that the default DNS resolver will be changed soon to not poll at all.\n. gRPC in Java and C do not poll DNS; they only update when connection errors happen.  We already update when connection errors happen, but we need to implement something to keep retrying if no addresses are returned.  Once that happens, we will remove polling from our DNS resolver.\n. Closing; see comments in #2279.. You can see how we install protoc in our .travis.yml -- it's in vet.sh and that does:\nsh\n      PROTOBUF_VERSION=3.3.0\n      PROTOC_FILENAME=protoc-${PROTOBUF_VERSION}-linux-x86_64.zip\n      pushd /home/travis\n      wget https://github.com/google/protobuf/releases/download/v${PROTOBUF_VERSION}/${PROTOC_FILENAME}\n      unzip ${PROTOC_FILENAME}\n      bin/protoc --version\n      popd\nThat said, most open source users check in their generated pb.go files to avoid the need to regenerate them for every build.  We only use protoc in our CI to make sure they are not stale.. @zchee can you try again?  I've never seen a page not found error.  What is the URL when it gives that error?\n. We can't do this until Go 1.6 support is removed.  Closing for now.. cc @cesarghali - there are some affected ALTS files. I reverted all of the alts/core changes for now.. @dominikh I think I ran something like -check '*,-SA1019' when I got this error (I was just guessing at usage).  I would rather avoid shadowing altogether, so I'm OK with this check.\n. /cc @johanbrandhorst \n@MOZGIII - See #2174.  It was not merged because we don't want so much special casing going on inside our transport (which is already in need of some cleanup).  Instead we'll be adding support for pluggable transports (#906), and then a separate transport can be implemented for wasm.\n. @rfarnham\nYou need to make sure the RPC handler corresponding to the stuck client will terminate when sendWithTimeout returns an error.  How to communicate that back to your handler will depend on your implementation.  It looks like you have a centralized registry of active streams that you're looping over in a separate goroutine when sending, so you'll need something like a channel created for each RPC that the main handler goroutine blocks on.  When the main RPC handler returns back to gRPC, the RPC is terminated and all Send/Recv calls are unblocked.  Let us know if you need any further help.\n. @EdSchouten,\nThat is an interesting idea!  Unfortunately, I don't think we will be able to do what you are asking for directly in the default grpc proto generator.  There is another option available: UnknownServiceHandler.  This will be invoked for any unregistered methods on your server, and can be used to make your server act as a proxy for those methods.  This handler would call NewStream on the client and then forward all operations between that stream and the incoming stream.  The messages sent/received would need to be passed between the server and client internally as []bytes, because otherwise the proto codec would need to know the message types.\nTo do this in a more type-safe way, I think it could be done in a separate protoc plugin.  It would essentially generate a function like this:\ngo\nfunc FooForwardingService(foopb.FooClient) foopb.FooServer\nWhere the implementation of each method performs a call on the client and forwards all the operations.  The reason I would rather have this in a separate plugin is because the code would not be needed by the majority of our users.\nThanks for the suggestion, and let us know if you need help with anything else!\n. Closing from inactivity.  Please let us know if you are continuing to have problems.. cc @kgolab. The behavior of -u is subtly different with modules.  It causes go to update all of the dependencies of the thing you are updating.**  With Go1.11 module support, you can just use go get google.golang.org/grpc to make sure the dependencies are downloaded -- but that isn't even necessary.  go build <your package> will automatically download the dependencies needed if you have a proper mod file for your package set up (go mod init/go mod tidy).\n** EDIT - Actually, this behavior is the same -- go get -u would always update grpc's dependencies.  The difference is that now you should never need to do this.  Everything should work using the versions explicitly listed in grpc's go.mod file.\n. Given this error, it's strange that other packages can be fetched.  Try go get -u with those other packages to make sure it's trying to use the network to update them instead of accepting the currently-installed versions.\nDid you see https://github.com/golang/go/issues/6391?  Seems like your system's certs are just not in the place where Go expects them.. Closing from inactivity. Please let us know if you are continuing to have problems.\n. To be in-line with the other grpc implementation languages, we will be removing polling from the DNS resolver; updates will be only performed when a connection error occurs.\nSee #1663 for more information.. @mikeraimondi \nSorry for the delay here.  This implementation LGTM; could you please rebase and resolve any conflicts, and then see if there's a reasonable way to test this?  Maybe something in internal/transport/transport_test.go that creates a transport and then reads the socket options back to make sure it has been set?. @mikeraimondi the 1.11 failure is a known flake (#2380 should fix it).\nThis recent set of changes LGTM.  Hopefully there won't be many conflicts to resolve in 2 weeks when this is good to submit.  Thanks again for your contribution!\n. @mikeraimondi - Go 1.6 and 1.8 have been removed from our CI.  Please rebase your branch to master, and I think everything should be good to go.  Thanks!. Your proposal works, and might be the best short-term solution.\nTwo downsides: 1. compile-time type safety is effectively lost (if the user typos a method name, everything still compiles), and 2. we can't force users to do this, and users that don't will break when new RPCs are added to services.\nAn alternative would be to support registering methods one-by-one and recommend using that instead of service-level registration.  The implementation of pb.RegisterFooService could optionally be changed to accept an interface{} and register the methods that are supported by the parameter (this obviously also loses type safety), or we could still provide the unimplemented service as above.. @prannayk - Sounds good to me, thanks!  The grpc code gen lives in the proto repo - let me know if you need any help with that, and please cc me on any PRs you send there.. Correct.  I think the only way this could be a nil pointer dereference is if t is nil.. Good catch.  Best fix is probably to declare client as an *http2Client (or to not declare it if possible and use inference).. What is in your vendor/ directory before you remove it?\nNote that if the same proto package (pb.go) is imported multiple times from different locations (including a vendor and a non-vendor directory), that can cause problems.. I'm able to reproduce this with those steps.  It doesn't make any sense to me at this point, but with a repro case we should be able to track down the problem.\nWith grpcurl:\n```\n$ grpcurl -plaintext localhost:8080 describe bmc.ManagementService\nFailed to resolve symbol \"bmc.ManagementService\": Symbol not found: bmc.ManagementService\n<>\n$ grpcurl -plaintext localhost:8080 describe grpc.reflection.v1alpha.ServerReflection\ngrpc.reflection.v1alpha.ServerReflection is a service:\n{\n  \"name\": \"ServerReflection\",\n  \"method\": [\n    {\n      \"name\": \"ServerReflectionInfo\",\n...snip...\n```\nStrange that the reflection proto descriptors are fine but the ManagementService ones are not.. OK, this is what's happening:\nProto works by having a global registry of symbols.  This \"global\" registry lives in the protobuf package.  Vendoring the protobuf package means that every pb.go file that uses the vendored copy will register its symbols with the vendored package, but everything outside of that path (e.g. grpc) will still attempt to use the non-vendored package.\nThe right solutions are:\n\nDon't vendor protobuf (use Go modules instead if version control is needed), or\nVendor grpc as well as protobuf, but be wary of anything that isn't vendored that might be using proto, like the well-known types in google.golang.org/genproto (actually those will cause a compile error if they are not vendored also, but this won't be true for all protos).\n. I don't think there's any way for grpc to notice a situation like this.  The best it can do is return an error from the reflection service when it fails to find a symbol in the global proto registry, and that is what it's doing.\n\nNote that vendoring is not really recommended for reasons like this, and vendoring in Go 1.11 with modules enabled works differently - it forces the use of a single copy of a package to avoid situations like this.\n. @akshayjshah,\nThis issue has come up a few times in the grpc team.  C/C++, Java, and Go all treat the status details slightly differently based upon our interpretations of the design.  Unfortunately, no gRFC was published for this feature (IIUC it was proposed before the gRFC process existed), and it was never incorporated into the main docs, either.  We have an AI to work out the differences and decide on a path forward, hopefully in this quarter.\n/cc @ejona86 @markdroth \n. Sorry, but no.  Even if you were to write them, we would/may overwrite them.\nCan you use custom trailers instead of grpc-status-details-bin, since you aren't able to use protos on all your platforms?. For future reference, this is for https://github.com/dessant/lock-threads. Are you certain gzip is being used between your GRPC-Gateway client and server?  Compression is only used if the client specifically initiates it.  The client initiates it by setting the UseCompressor CallOption (or the deprecated WithCompressor DialOption).  Registering a compressor just makes it available for decompressing if needed and makes it possible for UseCompressor to use it.\nSee https://github.com/grpc/grpc-go/blob/master/Documentation/compression.md for more details.. Adding the same comment from PR #2356 for future reference:\nUnfortunately, we do not want to enable concurrent SendMsg calls at this time.  There are a few reasons I can provide for this:\n\nBecause ordering often does matter for streams, it could be confusing for users if we allowed this.\nStreams not attempting this type of operation become penalized by the cost of taking this mutex.\nSimilarly, in a heavily loaded system, attempting to send in parallel will not end up leading to any performance gains; the CPU and network are already fully saturated, and the cost of synchronization becomes a negative.\nIf we were to allow this, the issue of fairness arises pretty quickly thereafter.  sync.Mutexes have no concept of fairness, so it's possible that even if the user implements their own message reordering on the receiving end, one message could be postponed until many subsequent messages (hundreds/thousands/more) have been sent.  This could result in problems for users not expecting that possibility - e.g. if they are buffering a message until all prior messages are received.\n\nIf this is a performance bottleneck for your use case, I would recommend something like what @snowzach described in #1879.. @steve-gray \nThat SendMsg is not thread-safe is not an oversight.  The use case of unordered messages on a stream is uncommon.  Adding synchronization to support a rare use case is not worth the tradeoffs that it entails.  And it also doesn't help with cases like @suyashkumar's, where ordering is necessary, but the same limitations prevent maximum throughput.\nIf your messages are order-independent, you could also try creating multiple streams to carry the messages.  This will actually scale better long-term as well, in case you fully saturate your connection and find you need multiple connections to provide even more throughput.\nFor cases where ordering does matter, solutions like @snowzach's are the only way to help, currently.\nNote that the writing of the data to the connection doesn't actually happen in the SendMsg goroutine (as of ~1yr ago).  The data is queued and a centralized goroutine picks it up and writes it.  The SendMsg goroutine only blocks until sufficient flow control is available for the queue before returning.  This means that, if your messages are large, you typically will be in the N+1th call to SendMsg while the data from the Nth call is being written.  If you can marshal/compress the data faster than you can write it, then that will saturate the network.  If it's slower, the marshal/compress would need to happen in multiple goroutines to saturate the network.\nIn general, we believe that resolving these types of performance problems is best left up to the application.  Adding extra parallelism/synchronization in grpc incurs a penalty to users that do not require it.  That said, we're sensitive to applications needing to implement complex logic to get the most out of grpc, so where new APIs or libraries can be provided to help with that, we may do so.\n. I filed issue #2432 as a proposal for a way to address this limitation.  Please feel free to comment.. @steve-gray thanks for the contribution.  Unfortunately, we do not want to enable concurrent SendMsg calls at this time.  There are a few reasons I can provide for this:\n\nBecause ordering often does matter for streams, it could be confusing for users if we allowed this.\nStreams not attempting this type of operation become penalized by the cost of taking this mutex.\nSimilarly, in a heavily loaded system, attempting to send in parallel will not end up leading to any performance gains; the CPU and network are already fully saturated, and the cost of synchronization becomes a negative.\nIf we were to allow this, the issue of fairness arises pretty quickly thereafter.  sync.Mutexes have no concept of fairness, so it's possible that even if the user implements their own message reordering on the receiving end, one message could be postponed until many subsequent messages (hundreds/thousands/more) have been sent.  This could result in problems for users not expecting that possibility - e.g. if they are buffering a message until all prior messages are received.\n\nIf this is a performance bottleneck for your use case, I would recommend something like what @snowzach described in #1879.  Thanks again for the PR.\n. Thanks for the report @kushtrimjunuzi.\nWithout digging in, this seems likely: 2562048H overflows int64.  MaxInt64 -> Hours should be 2562047h47m16.854775807s.  Because of the round-up, we overflow when converting back.. > In clientconn_state_transitions_test.go, each time we set up a server to send\n\nSETTINGS, we should also set up the server to read. This allows the client\nto successfully send its SETTINGS.\n\nThis condition may happen in practice, though.  Should we have a test case for each server behavior?. Yeah the github UI is a pain.  I tried to mark everything as resolved that is resolved.. @nikkolasg if you can provide a reproduction test case, please let us know and we will reopen the issue and investigate.  Thanks!. @euroelessar the 4MB window size limit is set based on standard TCP settings -- most TCP connections can't effectively utilize any larger of a window size.  What is the discrepancy between our out-of-the-box 40MB/s and what you're seeing with iperf or when you set Initial[Conn]WindowSize?  What iperf tests are you running when you see these results?. Looks like 4MB is definitely too conservative of a default value - we are open to increasing this, or possibly reading the TCP settings and mirroring the receive window size when setting our flow control.\n@euroelessar could you do a cat /proc/sys/net/ipv4/tcp_rmem so we can see what your TCP receive window size can scale to (on both the client and server if they are not homogeneous)?  Thanks!\n. @ashi009 what are you trying to do and why?  We've considered it grpc's job to parse the target string into a resolver.Target, and pass the parsed result to the resolver.. @ashi009 - In that case, you should be able to use url.Parse to parse the target string and get the scheme.. I discussed this with the other grpc language leads, and they don't offer anything similar.  Target URIs aren't intended to be parsed by users.  We are investigating some different options to configure connection settings based on the scheme, but haven't made any concrete plans yet.\n. Minor update:\n\nDuring development for the 1.19 release, support for changing this behavior via the environment variable will be removed entirely. Also, the grpc.WithWaitForHandshake() DialOption (was \"experimental\"; now \"deprecated\") will be removed.\n\nThis will be removed shortly, but not in the 1.19 release.. Made #2408 to include the new RPC (returns Unimplemented). Travis passed.  Github doesn't seem to know, though.  I'm going to force the merge.. Updated to use a grpcsync.Event instead of manually doing all the things it already does.  Made grpcsync.Event use an atomic in the fast-path for performance.  PTAL.. > Is there any way to tweak limits server-side\nYou can use this to adjust the limits: https://godoc.org/google.golang.org/grpc#MaxConcurrentStreams\n\nand get the client to create an additional H2 connection if current streams' count is close to the limit?\n\nThe client application can't observe the limit, currently.  You would either have to communicate it another way or hard-code it.  Creating extra connections can be done by having a custom balancer that creates more SubConns when load is heavy.  The balancer could measure in-flight RPCs (increment on Pick, decrement on done) and adjust connections accordingly.\nThere are discussions going on about detecting this situation and automatically creating new connections, but this seems like a pretty significant feature so I wouldn't expect anything for some time.\n. > How heavy, do you have got any benchmark. H2 still need multi connections to improve performance?\nWe don't have any benchmarks for this.  Multiple connections might be needed in this case to overcome the max concurrent streams limit (e.g. if it's being set by an L7 proxy in the middle) -- gRPC can configure this limit for direct connections, and so this kind of thing shouldn't be necessary.  There are other reasons a single connection could be a bottleneck, but often one connection should be able to maximize throughput.\n\nGot any achievement or plan?\n\nNo.  There are no current plans or a schedule to implement anything, just some high level discussions.. related to #711. This function blocks when the sender does not have sufficient flow control to send the current message.  That happens when the remote side (the server in this case) isn't reading from the stream.  Is the server actively calling stream.RecvMsg?. Do you have info logging enabled on the client and server?  Is your connection persistent or is the connection dying and the client reconnecting when this error occurs?\nCould you also update your grpc-go version to our master/HEAD for testing?. @yjp211 have you had a chance to look at any of the debugging info for this?. Great, thank you for following up.  Let us know if you have any trouble in the future.. @mitar I don't think go get works like that.  What you're describing is more like how modules work (experimental in go1.11).  Otherwise, you will need to vendor your dependencies (manually or by using a tool like dep) if you want to use an unsupported version of Go.\n. > This would require updating the grpc.Stream interface to get the required codec and compressors for the PreparedMsg. Right now there is no method to access those universally for any stream.\nWe can do this without changing grpc.Stream.  I was originally thinking we would type assert s to grpc.clientStream, but this would make it so Streams implemented by interceptors couldn't support this operation.  Instead, we could store this information in the rpcInfo that we already store in the stream's context instead:\nhttps://github.com/grpc/grpc-go/blob/e441557ee90e1b93ac1f42ab8904845fd8d6e637/rpc_util.go#L670-L684\n...or add a new key/value in the context, but this one seems fine to extend.\nAn interceptor could mask this context completely, but that should be rare, and there are limits to what we can do given that we can't add methods to grpc.Stream.\n. Once the server returns an error from the method handler, the stream is closed and sends/receives on the stream in other goroutines will fail.\nI'm not sure why you're wanting to receive messages after returning an error from the server, but note that, in general, when a client sends a message there is no guarantee the server will receive it unless the RPC completes successfully or an ack message is sent back to the client from the server (this must be done in your protocol; grpc does not include message-level acks).\nPlease let me know if that doesn't answer your question or if you have any follow-ups.  Thanks!\n. > My streaming server does a few things. It is a streaming audio server, so it (a) does storage and (b) does ASR transcription. Storage is the higher priority task. If (b) fails I still want to store all the incoming audio and notify the client that an error has occurred. I thought GRPC errors was the way to go but it seems like building this as application error is a better choice.\nIf you want to keep streaming from the client despite an ASR transcription error, then you probably would encode that in your protocol by using a bidi streaming RPC and having the server send a message to the client indicating the error, but not ending the stream.\n\nThis leaks if I return error from the main GRPC thread. Any suggestions how to not make this goroutine leak? One idea is to make the main GRPC thread never return an error and hence Recv() shall always finish (assuming we make sure GRPC thread always returns)?\n\nThe Recv call should unblock and error after the server handler returns an error.  Are you not seeing this?  Is it happening because your channel is full and you are no longer reading from it?  If so, then I think what you suggested makes sense: prevent exiting from the main service handler function until you get an error from the recv channel.. Let us know if you have any further questions.. @aschlosberg thank you for the contribution.\nUnfortunately, this is not something we would like to accept at this time.  As discussed previously in a similar PR (#2186), I don't believe the native grpc API for creating and connecting to a server is sufficiently complicated to require a wrapper library.  Wrapper libraries introduce the burden of learning another API, so it must be justified by hiding significant complexity in what it wraps.  In addition, the process for testing a grpc service should be even simpler once we have implemented the in-process transport (after https://github.com/grpc/proposal/pull/103 is done).\nIf you are interesting in contributing to grpc-go, we do have a number of open issues tagged \"Help Wanted\".  Please feel free to peruse the list and drop a comment in anything that sounds interesting and we can help make sure the issue is still relevant and provide any guidance needed to get you up to speed on it.  Thanks!. If you are asking for something to help you construct errdetails proto messages more easily, that is outside the scope of grpc, and something like it would belong more naturally in the googleapis repo where the messages are defined.  I would suggest opening an issue there.\n. @ch3rub1m Good point.  The difference is that gRPC error details are based upon Status messages, whereas the details within a Status are arbitrary proto messages.  errdetails protos are not specifically supported.  Also note that the status package is not designed to help you work with generic Status proto messages more easily, and codes exists mostly because it predates the google/go-genproto repo.\n. @vishal-uttamchandani In gRPC, the client is responsible for ending connections, since it's typically the client that knows whether more requests might be sent.  Do you have a use case where this doesn't work well?. @vishal-uttamchandani connection-level controls on the server should typically be implemented in a handshaker.  Once the connection is allowed, the client is responsible for ending the connection when it is no longer necessary.\nTechnically, you could pass data from the handshaker in AuthInfo that the service handler could access via the peer info, and this could include a function to close the connection.  I would think hard before heading down this path, however, as this is not something I've ever heard of anyone wanting to do before.\nLet us know if you have any further questions.. Even if you close the connection, the client will reconnect eagerly.  In the future, we will detect idleness and have the client not reconnect, but that is not yet implemented.\nAnother option for you would be to use MaxConnectionIdle enforcement on the server.  (This would not stop the reconnecting behavior, except I believe Java -- and possibly C -- clients will not eagerly reconnect after the resulting GOAWAY until the client attempts another RPC.)\nhttps://godoc.org/google.golang.org/grpc/keepalive#ServerParameters\nhttps://godoc.org/google.golang.org/grpc#KeepaliveParams\n. Looks like you're missing a build tag based on travis.  Please reassign after fixing.. @wangjingpei can you provide the code you were running, or a minimal reproduction case?  Without seeing how you're using the library, we can't do anything to help.. @wangjingpei any updates?. @wangjingpei why do you believe grpc is causing these problems?  Can you give an example that shows how you are using grpc when the goroutine leak happens?  A goroutine dump could also be helpful in pinpointing the problem.\n. @wangjingpei if you can provide a reproduction case for this, we can reopen the issue and investigate.. For unary RPCs, if the RPC status is not OK, the response message is invalid.\nPlease see https://github.com/grpc/grpc/issues/12824 for explanation / justification.\n. > Did you miss a \"not\"?\nYes.  I edited this text like 5 times, so it doesn't surprise me that I killed it at some point.\n. Thanks for sending this cleanup.  This is actually done intentionally, because we have another proto implementation that splits services and messages into separate packages, and this makes it clear which references to use for one or the other.  We would like to eliminate this and then clean this up, but it might be awhile.  Closing for now.. @mauriciogg - The connection establishment timeout will actually be given more than 20s after the backoff has increased beyond that amount.  (20s is the lower bound.)  WithBackoffMaxDelay or WithBackoffConfig can be used to configure this, but by default the maximum backoff is 2 minutes.\nOtherwise, no, there is no way to configure it currently.  What kind of scenario are you encountering where 20s is insufficient, and what kind of problems are you encountering?\n. Are you seeing grpc.Dial blocking forever?  The ClientConn will reconnect forever, but Dial shouldn't block at all if WithBlock is not passed in.  If it is blocking, can you get a goroutine dump so we can see where it's at?. This is an interesting idea, but I don't believe it is feasible, unfortunately.  A service definition should have only one canonical form, and generating different forms based on command-line flags or options in the proto file could be problematic.  Also, a change to your request/response messages would result in new function signatures being generated, which would cause backward compatibility problems when there otherwise would be none.  I'm sorry we won't be able to do this, but thank you for submitting your idea.\n. > unrecognized import path \"google.com/grpc/test/bufconn\" \nThis should be imported as \"google.golang.org/grpc/test/bufconn\", and then everything should work as expected.. @jadekler I think I'll send a minimal PR to disable this feature for the release tomorrow to reduce risk, and we can hold this PR until after.  SG?. And per #2406  we aren't supposed to remove this support until the 1.19 release anyway, so I actually just re-stated the official plan.\n. There an unused import making travis fail; can you fix it please?. gRPC 1.0.4 is very old and unsupported, but it seems like this may still be an issue at HEAD.  Essentially, any place where we exit operateHeaders before the handle(s) without calling s.cancel should be considered a context leak (the contexts will be cancelled when the transport is closed, but accumulating them until then is a problem).\nhttps://github.com/grpc/grpc-go/blob/30155c0ba1897b0168a32e3d4a587eebb33524ef/internal/transport/http2_server.go#L353\nhttps://github.com/grpc/grpc-go/blob/30155c0ba1897b0168a32e3d4a587eebb33524ef/internal/transport/http2_server.go#L359\nhttps://github.com/grpc/grpc-go/blob/30155c0ba1897b0168a32e3d4a587eebb33524ef/internal/transport/http2_server.go#L369\nhttps://github.com/grpc/grpc-go/blob/30155c0ba1897b0168a32e3d4a587eebb33524ef/internal/transport/http2_server.go#L375\nSome of these checks may be able to be reordered to be above the creation of the context, but some will still need to come later -- for instance, the inTapHandle needs the context with the timeout applied.. That doesn't sound right to me -- fields should not be discarded.  Can you provide a more complete example that demonstrates what you are seeing?\n. cc @ravenblackx. @krhubert do you have any updates here?. Hi @prannayk, sorry for the delay.  Now that #2512 is in, we can measure the effectiveness of this proposal.  (Note that it only impacts the sending side and not the receiving side, so I'm not actually sure how much it will help.)  If you are interested in further pursuing this, could you please update the benchmarks added in #2512 to use this facility when sending, and see if you can achieve a measurable improvement in the throughput for a single stream?  You can run the benchmarks via this tool.  To give it the best chance of success, you'll also want to turn on compression for the stream.  Note that currently the \"compression\" implemented in the benchmarks today is actually a dummy compressor that does nothing, and is there just to exercise the grpc code paths for compressors.  You'd want to use something like gzip instead.  Please let me know if you have any questions or need some help with this.  Thanks!. Is this a check the TLS library should implement, because it is a hard requirement?  Or is it an optional check that we're supposed to be performing that others might not?  (Seems like the former to me, but I'm not completely sure.). @spl0i7 can you take a look at this please and confirm whether this might be answered by the above?  Also, can you let us know exactly how you created the certificates so we can try to reproduce it?  Thanks!. @spl0i7 - do you happen to know the flags/config you used to generate them?. cc @ejona86 @markdroth FYI. > So \"get state from the old picker\" is probably not a solution like it is in C.\nCorrect.  If maintaining the position in the list is a requirement, probably what I'd do is make it so there actually is only one picker permanently, and update its state atomically/with a lock when a change needs to be made.\nEither way, I think this is a real improvement and the full fix is going to be lower priority for us.  I will file an issue for the follow-up changes.  In that, it would be nice if we could define the required/desired/acceptable behaviors for a RR balancer in gRPC in case we can find a user that would like to contribute a fix.\nAlso note that in case 2 we aren't even starting at the top today - as @ejona86 observed, we are shuffling our list of addresses due to the map from addresses to subchannels.. > Also note that in case 2 we aren't even starting at the top today - as @ejona86 observed, we are shuffling our list of addresses due to the map from addresses to subchannels.\nAnd per my observation in #2580, starting at the top doesn't even seem like a reliable thing that can be tested or relied upon by users, given that the first RPC will go to the first-connected backend and not unconditionally to the first address in the list.. One more:\n\nMaintain the same order provided by the name resolver.\n\nHow important is this, given that the first RPC will go to the first ready subchannel, then eventually we will alternate between the first+second ready subchannels, etc. ?\nFor pick-first, the order here is obviously very important, but for RR it actually doesn't seem to matter to me at all.. This is implemented according to the grpc spec here:\n\nSome data transmitted (e.g., request metadata written to TCP connection) before connection breaks | UNAVAILABLE | Client\n\nIt's unlikely this can be changed.\n\nI'm making a request to a gRPC server that is starting to stop in the middle of request handler execution.\n\nAs a general suggestion related to this, you may want to look into graceful shutdown for your servers:\nhttps://godoc.org/google.golang.org/grpc#Server.GracefulStop\nIt's common to call this, then -- in another goroutine -- wait for a timeout before calling Stop to allow existing RPCs extra time to finish before forcefully closing the server.\nAdditionally, you should try to make sure your API is designed such that all RPCs are safe to retry.. > \"Some data transmitted\" which clearly is not the same as \"All data transmitted\".\n\"All data transmitted\" is a special case of \"Some data transmitted\".  It's also unclear how this condition would apply to streaming RPCs.\nThis would be a change to the grpc spec, and any proposals to change it should be filed against the grpc/grpc repo or brought up on the grpc-io mailing list.\n\nI think that returning Unavailable for fully transmitted requests breaks guarantees for retryability mentioned in status codes docs.\n\nWhat docs are you referring to?  The status codes doc I linked effectively says there are no guarantees:\n\nAs shown in the table above, the gRPC library can generate the same status code for different cases.  ...  Therefore, there is no fixed list of status codes on which it is appropriate to retry in all applications.\n\nI agree this is unfortunate, but I'm not sure it will be feasible to change it.  We would also need to change the \"Server shutting down\" case for it to have any benefit.   Additionally, finding a more appropriate status code is tricky.  E.g. Aborted seems best, but we've already declared that grpc will never generate it.. > Furthermore, the current state of client status codes is incompatible with the spec.\ncode.proto is not a spec.  It provides guidelines on when to use different codes, which the grpc spec is unfortunately not following in this case.  grpc-go and grpc-java are following the grpc spec -- what you are asking for would require a change to the spec.  But be aware that it may not be possible to change given other constraints (backward compatibility, choosing a more appropriate code, consideration of streaming RPCs, etc).\nIn terms of retry, best practice is to make all RPCs idempotent or safe to retry, so retrying on Unavailable in all cases where grpc generates it would be OK.. This is due to #2565 as part of #2406.\nIf you want to consider the connection \"ready\" without actually being connected to a real gRPC server, you can use the environment variable GRPC_GO_REQUIRE_HANDSHAKE=off from now through our next release.  After that, we will be removing support for this entirely.  You should start a real grpc server in your tests if you need clients to believe they are successfully connected to a server.\nLet us know if you have any questions or concerns.\n. @euroelessar to help us prioritize this, can you let us know if this is causing any problems for you or is just something you happened to notice?  Thanks.. @ZhiqinYang have you configured keepalive enforcement on the server accordingly to match the client's pings?\nhttps://godoc.org/google.golang.org/grpc/keepalive#EnforcementPolicy\n. I'm not able to reproduce this with a simple test case.  The test case will send a ping every 10ms, and the server has a MinTime of 5ms.  Even with no activity and an active stream sleeping for 1 minute, the server correctly allows the pings.\nCan you provide a reproduction case I can run that shows the problem you are seeing?\nNote that pings are only sent every Time + Timeout period.  So in your parameters above, that's every 50s.  This is actually a bug and I will fix it soon.. Ping strikes should always be zero if the client is respecting the server's setting (both MinTime and PermitWithoutStream).  In addition, they are reset to zero any time the server sends data.  If you can provide example code that I can run to see the problem you're seeing, that would be helpful.. If you push new changes, travis will re-run.  Re-running the same change won't help in this case -- from the output:\n+gofmt -s -d -l .\n+fail_on_output\n+tee /dev/stderr\n+read\nbenchmark/client/main.go\nThat means benchmark/client/main.go isn't gofmt'd correctly.  Please reformat and try again.  FYI, you can run vet.sh locally so you don't have to iterate on travis.  (The output can be admittedly hard to parse sometimes, though.). Those are reasonable concerns, but none of the current team were around when the original API was designed, and it isn't something we're capable of changing at this point.  I don't think there's anything that can be done here.\n. Good idea.  Opened #2664 to track.. Lowering priority, since the main issue is resolved by eliminating the backoff after a successful connection is established.  Ideally the error message would be populated with \"server handshake not received\" (or similar).  When it's due to the handshake, this will happen naturally when we make transport creation function blocking.  When it's due to a subsequent connection attempt, we need to plumb the error from the transport to the addrConn.\n. @mklencke that doesn't seem to have worked.  Is the email address on your commit your @google.com email address?. That looks like a better error than the first one.  I'm not sure it's required for Google employees to sign the CLA directly, but that is what we did on the gRPC team and it made everything work.  So if you can't find some other way to take care of it, it's probably best to go through the process directly (it's pretty quick: click the link above, sign up as an employee, sign in with google using your corp account, link your github account, agree to the CLA, then drop a comment here to wake the bot up).. @JelteF The 10s client-side limit is a fix to bring us in line with the gRPC spec for keepalives here:\nhttps://github.com/grpc/proposal/blob/master/A8-client-side-keepalive.md#extending-for-basic-health-checking\nServer-side, there is no minimum defined in the spec.  However, we intend to implement second-level precision as part of a fix for #2638, which would mean sub-second times could not be honored.\n. @JelteF We could potentially allow a lower limit to be configured, given that we had unintentional support for this in the past.  If this is a valid use case, however, we should think about how we would support similar scenarios in Java and C, which currently also restrict clients to 10s keepalive ping intervals.  We have another client-side health checking mechanism (gRFC), but that will only quickly detect a problem where the server is still alive but reports itself as unhealthy, not in the case where the server becomes unavailable.\ncc @ejona86 who designed client-side keepalives.  He had concerns about this being used to detect network outages (where 100ms is probably too low to be reliable), but it sounds like you are mostly concerned about the server itself becoming wedged.. A target string intended for Dial is not a meaningful thing to use in a Listen API.  E.g. what do we do for \"dns://example.com\"?  This would be a problematic API.\nWe also would prefer not to expose an API for parsing target schemes at this time - #2403.\nI would recommend either using url.Parse to parse target URIs or use a different format for your flags and construct the target strings instead of trying to parse them.\n. @liangzhiyang have you had a chance to run again with logging enabled on your client and server?. @datuanmac could you please accept the CLAs so we can merge this?  Thank you.. There are a number of differences; too many to document in the godoc comment.  Here are the ones I can think of right now:\n\nKeepalive pings and server enforcement of client keepalive pings\nMax idleness detection\nConnection-level stats handlers\nInTapHandle (called before handling a new stream to potentially kill it before allocating any resources for it)\n\nIn addition, it is not a feature actively maintained by the grpc-go team; I would avoid using it.. Closing.  Please comment if you are still experiencing problems.. A separate CloseSend on the server would not be possible to implement without a gRPC wire protocol change.  The way the client implements CloseSend is by setting the end stream flag on the stream.  If the server were to do this, it would not be able to subsequently send the trailers for the RPC.\n\nThe issue is that in this setup the receiving goroutine can block on the call to stream.Recv() as long as the connection is healthy.\n\nstream.Recv() will return io.EOF after the server handler returns - signaling the end of the RPC.  I think your workaround is entirely fine and is the expected way to handle this kind of scenario.\nIf you need the stream to stay alive, but let the client know the server is done sending messages, that would need to be communicated via your API protocol (e.g. a field in your streaming message to indicate this).\n. @kazegusuri \nIf the connection fails to be created, this code will be skipped.\nDid you apply this patch to master@HEAD or another branch/commit?\nAre you setting GRPC_GO_REQUIRE_HANDSHAKE=off in your environment?  If so, please try not setting it.\n. Thanks for the update!. You also will want to use grpc.FailOnNonTempDialError(true) so you don't wait until the context expires to determine there was a persistent connection error.\nSimilar to Java, Go also have a connectivity state API:\nhttps://godoc.org/google.golang.org/grpc#ClientConn.GetState\nhttps://godoc.org/google.golang.org/grpc#ClientConn.WaitForStateChange. @carl-mastrangelo Go doesn't currently have the concept of idleness - it starts in Connecting, so these functions don't have side-effects.\n. The balancer doesn't matter - all ClientConns always start in Connecting.  The resolver returns addresses, the balancer gets them and then it immediately tries to connect to the ones that will be used.  Nothing in this process waits for an RPC.  We also eagerly reconnect, even after a GOAWAY, which is a known, meaningful difference between Go & other languages.  #1786\n. @carl-mastrangelo WithBlock blocks until the ClientConn is Ready, which is whenever the LB policy decides, but that's typically when a single connection itself becomes Ready.  It blocks until the deadline is reached, or until there is a non-termporary error if you also use FailOnNonTempDialError.\nIf you don't want to block, you can use the connectivity state API instead.\n\nIt would be great if this could be prioritized.\n\nI agree, but we have a few things in front of it right now.  It will probably be a couple more quarters before it happens at this rate, assuming no new things come in.\n. We always eagerly create the connection.  When we support NewClient, we will update the connectivity state API to allow for this (#1786).. Can the return value be an http.Header instead of a map?\n. Will this be used by default?  If not, would this be confusing for users, since basically everything respects these environment variables?\n. I would prefer \"Proxy\" (even though it stutters), or \"Interface\".  There are examples of both in the go standard libraries (hash.Hash; sort.Interface and heap.Interface).. ... and an implementation of a proxy that uses environment variables for configuration.\n(e.g.). Optional: ErrDisabled?  Indicates the proxy is not enabled?\n. Please add a comment explaining why this type exists.. It looks like it should be possible to share additional code (header formulation, bufConn creation) and limit the version-specific code to just sending the request.  For maintenance, in general, it would be better to minimize version-specific code.. Return nil, ?\nDisregard if this becomes shared code per earlier comment.. complele -> complete. What if the proxyer was responsible for connecting?  In that case, the proxyer would be mostly a wrapper, and the logic around here could be completely offloaded to it.  I.e.:\n```\npackage proxy\ntype Dialer interface {\n  Dial(ctx context.Context, addr string, dialFunc func(context.Context, string) (net.Conn, error)) (net.Conn, error)\n}\n```\nA proxy's Dial method would dial, determine the headers and new address if necessary, and perform whatever handshaking is required.  If the proxy was disabled/ineffective, it would be responsible for doing simply: \"return dialFunc(ctx, addr)\".  This avoids the need to pass around the headers / new address / disabled error, and should allow more flexibility in what Proxies could implement.. Actually, FYI, return values are assigned via a return statement, so return io.ReadFull(s.sr, p) is fine.  Example: https://play.golang.org/p/V1qMks7o9s.\n. Space after //. Please doc the jitter feature here.. Doc: the time the connection went idle?. data is allocated automatically.  p := &ping{} is equivalent.. Optional, random idea if you prefer: func jitter(v time.Duration) time.Duration and do time.Add() to also apply the jitter here.. It would be useful to have negative tests for this one and the third test -- i.e. make a call from the client every second for several seconds to ensure that the connection is not closed prematurely, and make sure the connection is not closed if the ping is responded to.\n. Why not just StoreUint32?  You're not using the result, and conditionally setting it is not important.. Same. Do we need to check c or peer also?. Please remove this local and use opts.UserAgent below directly.. This comment no longer appears to be true.  Did you mean to remove this check?. Should this be inside the header != nil above, or should that if be removed?. This 2*time.Hour should probably be a constant (next to maxPingStrikes?).. Should this be 101ms (or more) to avoid a race between the timers?\n. Same concern.. AsserT. AsserT. Should we have some notes in the ClientParameters to the effect of \"make sure you set these parameters in coordination with the service owners, as incompatible settings can result in RPC failures\"?\nEspecially since there's a default enforcement policy of 5m and no pings without streams, even if the service doesn't declare a policy.. Would you mind documenting the default values for all of the settings in this file?\n. ... returns an error if user-agent is ...\n(and below)\n. No, this is vestigial from a previous iteration.  Removed.. It has no effect.  It never reaches the end, continues, or breaks.. Done. To make it impossible to mutate the statusError and change the Code into OK.\nWithout this, but the rest of the code as-is, one could do:\nstatus := status.New(codes.Internal, \"blah\")\np := status.Proto()\nerr := status.Err()\np.Code = int32(codes.OK)\nMaybe status.Proto() and FromProto() should do the clone instead, making Status immutable, which is how I had it initially.\n. Done. The rationale for the interface is:\n\nso the package can return multiple different things implementing this type (an okStatus as well as a statusError).\nso statusError can be unexported (essential to avoid typed-nil-error problems), implement both error and Status accessors (convenient but not essential), have documentation for its methods, and not be visible externally.\n\nI agree there are other options here, and PR #1171 is essentially what you're suggesting.  I'm not convinced this version is \"wrong\", per se, but I do generally agree with the desire to avoid defining interfaces when they aren't strictly necessary.\n. Done. Combine these:\n```\nif s, ok := status.FromError(err); ok {\n  switch s.Code() {\n  ...\n  }\n} // TODO: handle non-status errors?\n. Remove this TODO based on new TODO added.. Done: remove?. Should we just always write any status-type error returned by sendResponse?  Seems like a reasonable policy, at least, but would need some inspection into sendResponse to see what kinds of errors it can return.. Cool, so if we decide we can replace transport.StreamErrors with status errors, then this behavior will be the same.\n. Nit: LoggerV2, not Loggerv2. How about a usage comment for this package?  \"Import this package anonymously to activate it as the enabled grpc logger\" or something?\n. Change the date?. Info logs args informationally?  Or something?. infoW, warningW, errorW io.Writer. Nit: date.. const?. Date?. Please fix this comment to refer to FATAL log.. Remove this type and use int directly in V.. Consider passing the method config instead so that we don't need to continue extending the parameter list here.. How about something like this:\ngo\nfunc getMaxSize(mcMax *int, doptMax, default int) int {\n  if mcMax == nil && doptMax < 0 {\n    return default\n  }\n  if mcMax != nil && doptMax >= 0 {\n    return min(*mcMax, doptMax)\n  }\n  if mcMax != nil {\n    return *mcMax\n  }\n  return doptMax\n}\nthen call that to compute both send and receive limits?\n. Should these be call options instead?\nI think the abstraction of restricting configuration that applies to calls to CallOptions and then providing a \"default call option\" as a dial option is nice.  I.e.\ngo\nWithDefaultCallOptions(cos ...CallOption) DialOption {\n  return func (o *dialOptions) {\n    o.callOptions = append(o.callOptions, cos...)\n  }\n}\nIt makes it slightly more cumbersome to set it when dialing, but it also provides more flexibility.\nMy recommendation is to add the above and then make WithMax{Send,Receive}MessageSize return CallOption instead of DialOption.\nThe legacy WithMaxMsgSize could still be provided to avoid an API breaking change, but it could be implemented by o.callOptions = append(o.callOptions, WithMaxReceiveMessageSize(s)).. If cc.GetMethodConfig returned an empty MethodConfig (instead of ok=false), then this wouldn't need to be special-cased.  There would be nils for these fields and the logic above would apply.  Is that feasible?. I agree; nil pointers are a better signal of not-set than magic values.. You changed the capitalization here but the field is still unexported.  Maybe just remove the \"PendingData is\" part?. nit: lack of flow control. Nit: extra space between still & outstanding.. Minor improvement:\ngo\nwu := f.pendingUpdate + f.loanedWindowSpace\nif wu >= f.limit/4 {\n  f.pendingUpdate = 0\n  return wu\n}\nreturn 0. What is the significance of the 25% here?  Maybe make that a const if it's arbitrary?\n. Do you know if the compiler optimizes this?\nIf not, this would be better:\ngo\nn, err = io.ReadFull()\ns.readFullErr = err\nreturn n, err. Similar comment regarding defer vs. straight-line.. Why nil out put?  Unless I'm reading it wrong, it's local to this loop, and we either return or continue after this.\n(Same below.). Maybe combine the ifs here:\ngo\nif put != nil {\n  if _, ok := err.(transport.ConnectionError); ok {\n    updateRPCStats()\n  }\n  put()\n}\n?. How about:\ngo\nif _, ok := err.(transport.ConnectionError); (ok || err == transport.ErrStreamDrain) && !c.failFast {\n  continue\n}\nreturn toRPCErr(err). Why is this update here?  Presumably nobody will care about the stats until put is called. (It's also not guarded by a put != nil)\n. go\nif ss, ok := rpcStatsFromContext(ctx); ok {\n  *ss = s\n}\n. FYI: there's a helper in ptypes for setting Timestamp messages.  It would probably be best to use that.. This makes an unnecessary copy.  You could instead use b.clientStats directly and clear it after calling Send.. Preferred go-ism:\ngo\nif err != nil {\n  return\n}\nAnd outdent the following code.. Optional: in the defer, if err != nil, do this b.clientStats.NumCallsFinished++.. How about rpcInfo instead?  Stats makes me think there are metrics inside.  Combine \"stats\" with bytesSent/Received, and I expect those to be ints instead of bools.. Good idea.  Done.. Nits: lower-case \"D\" in doesn't and a space before the open paren.  Maybe \"missing\" instead of \"doesn't contain\"?. used by the grpclb balancer. Actually, \"md\" is correct here (that's the variable name; MD is the type name).. Godocs often refers to parameters or receivers by their name; I think this is fine.. Here MD is the correct usage.  It creates something of the type MD.\n\"md\" should be used where it's referring to a parameter's name.. MD. This was probably fine the way it was before.  Or:\nJoin combines mds into a single MD.. s/md/metadata/ here.  It's referring to the concept and not a variable or type.. Same.. should be immutable -> should not be modified.\nIt is NOT immutable, and this might lead someone to believe that what is returned is a copy of the data and that modifying it is fine but doesn't change the underlying metadata (neither of which is true).\n(Also in FromOutgoingContext.). To avoid the duplication in cleanups, how about:\ncleanUp = func() {\n  if lis != nil { lis.Close() }\n  // etc\n}\ndefer func() {\n  if err != nil { cleanUp() }\n}()\n. In general, it's better practice to return an error vs. passing t around.. Similar: consider returning an error instead.  It's not really important here, but it's a good thing to get in the habit of doing IMO.  (Souce: I once wrote a gigantic test package that passed testing.T around a lot and ran into problems -- e.g.s: the error reporting was unhelpful, and I had to log the stack trace to work around it; it became hard to ignore certain errors for certain tests that expected them; etc.). Nit: If it is a drop request. Is b.countConnected > 0 sufficient?. This should be impossible, because b.countConnected > 0, right?  Maybe comment \"Should never happen\"?. bes is empty, so append is redundant.  This should work:\ngo\nbes := []*lbpb.Server{{\n  IpAddress: ...\n}, {\n  IpAddress: ...\n}}. if c.maxSendMessageSize != nil && ... ?  Or a getter that handles nil and returns the default?\nOtherwise this could be a nil pointer dereference.\n. This struct already has a callInfo which contains these fields.  Do we need this?. Perhaps we should have s.opts in serverStream instead of copying most of its fields here?\nOr if those options are not appropriate in the context of a stream, maybe a sub-structure (e.g. \"streamOptions\") for the options that apply per-stream?  This would make it easier to plumb anything else new through.\n. Could we use the pattern from newClientStream here?  I.e.:\n```go\ndefaultServerOptions = options{maxReceiveMessageSize: 1024 * 1024 * 4, ...}\nfunc NewServer() {\n  opts := defaultServerOptions\n  ...\n```. Can maxMsgSize be removed now?. Please keep the original documentation and then add \"Deprecated: use ... instead\".\nOr something like: \"WithMaxMsgSize is implemented as WithDefaultCallOptions(WithMaxReceiveMessageSize(s)).  Deprecated.\"\n. Nit: please wrap comment lines to ~80 cols.. I think you can safely remove the \", ok\" and document that this will return an empty MethodConfig if one cannot be found for the method OR the service.. ...returns a CallOption ...\nand below. I disagree somewhat with this comment.  The logic itself doesn't need to be tested, but this is an end-to-end test, and we should make sure we are performing the right calls to context.WithTimeout() by including a test for it.. This can also be implemented as return &b.\n. I think it would be more appropriate to document this on the field than on the struct.\nHow far do we think we will be extending this struct?  Would it make sense to have sub-structures for client- and server- only fields?. This doesn't appear to be used by anything.  If there's a longer term plan for this, please add a TODO, or just remove it.. Good catch, thanks.  I changed it to cache the token internally and fetch a new one when necessary.. Same question... Should we also log the request?\n. I think a te.cancel() here is necessary to avoid a hang in the main goroutine.  You could test this by changing it to if err == nil above.. This is where I meant to leave that comment.\nMaybe defer stream.CloseSend() at the top of the function?. Sorry, comment in the wrong place... SGTM.  A broken invariant is an internal problem.   (Please remove comment.)\n. Pass \"c\" instead?. Remove TODO; this is fine.. > Why would we do that, given that this work is already done, and is correct?\n\"Done and correct\" is not the bar for submitting code, though.  This approach causes a couple maintenance problems for us:\n\ngrpclb.proto is now no longer a copy of the canonical version of the proto.  This means updating it requires extra steps.\nload_balancer.pb.go is modified by sed, which is not ideal for several reasons (e.g. it's not based on the AST like gofmt would be).\n\nI can think of two low-effort options that would allow us to remove the sed commands until we can switch to using go generate in a later PR:\n\nLeave the rest of the changes as-is, but skip the grpclb.proto file in make proto and delete load_balancer.proto.\nRemove the service from grpclb.proto and re-add grpclb/grpclb_server_generated.go.\n\nEither of those would be acceptable as a short/medium-term solution to our concerns.. > How does go generate solve anything?\ngo generate will allow us to skip the flag to use the grpc plugin for this file.  Generating the grpc services is what introduces the circular dependency.\n\nDoesn't that contradict your other point about keeping a canonical copy of grpclb.proto?\n\nYes, but I am looking for practical, short-term options that don't involve sed.  Longer-term, if we generate this file with go generate, and without the grpc plugin, then it can be a verbatim copy again.  I.e. this option represents a step in the direction of the longer-term goal.\n. Should this be unsigned since it can't be negative?  (Also, you cast it to uint32 when reading and cast to int32 when assigning.). I think this would be easier to read without the casts and taking the negative of a negative number:\ngo\nif n > f.delta {\n  n -= f.delta\n  f.delta = 0\n} else {\n  f.delta -= n\n  n = 0\n}\n(Also would allow f.delta to be uint32 like it seems like it wants to be.). Nit: lowercase \"e\" in \"error\", and add a colon before the error being appended.. should the if be checking w?\nIf not, check n first.. Same as above re: w and n.\nCan this function be shared somehow?  Should it be a method on the Stream instead of server/client?. nit: \"there was\". if !timer.Stop() {\n  <-timer.C\n}\n(and below). encode() should return a status error instead, and then we can just return it.. I think you're right.  I thought the draining of the channel was necessary to unblock a goroutine that was pushing to the timer's channel.  But, it looks like no such goroutine exists, and the channel is buffered anyway.  So this should be fine  as-is.. This ideally should be pushed into encode() instead.  Otherwise, we will still be allocating more memory than desired.  encode() gets the size before it allocates the buffer, so we can safe the work of encoding the object if we check it there after we get the size.. > 0?  A timeout of zero seems like a problem.. +1, assuming perf is not worse.. Nit: 30s. Please use consts instead of string literals:\n```go\nconst (\n  runModeUnary = \"Unary\"\n  runModeStreaming = \"Stream\"\n)\nswitch runMode {\n  case runModeUnary:\n    ...\n}\n```. Please use a boolean instead.  For printing you can do:\ngo\ntracing := \"Tracing\"\nif !enableTrace {\n  tracing = \"noTrace\"\n}. It seems arbitrary to group together the tracing and mode settings, but not the other things.\nI'd prefer either:\n\nKeep them separate:\n\ngo\nfor _, mode := range []string{runModeUnary, runModeStreaming} {\n  for _, enableTracing := range []bool{false, true} {\n    ...\n\nOr, put everything together in a single testCase struct.  Then this pattern instead:\n\n```go\nfunc testCases() []testCase {\n  for , mode := range []string{runModeUnary, runModeStreaming} {\n    for , enableTracing := range []bool{false, true} {\n      ...\n              tcs = append(tcs, testCase{mode: mode, enableTracing: enableTracing, ....})\n....\nfunc BenchmarkClient() {\n  for _, tc := range testCases() {\n    b.Run(...)\n  }\n}\n```. Nit: reqSizeBytes and respSizeBytes\nIt took digging through like 5 calls to figure out whether this was bytes or KB.  Since it's bytes, 1MB (1024*1024) is going to be more interesting than 1KB.\nFWIW, this is also just a field in a proto message, so \"0\" actually means there is still some data being transmitted.\n. +1.  Different workloads will have different parameters that are interesting.\nOTOH, there are some parameters that will apply to all workloads (the network configuration, tracing vs. no-tracing, etc).  We should separate the two different types of parameters and group the combined ones but split out the different workloads with their parameters.\nIn this case, streaming and unary are separate workloads, so their parameters should be separated.  And grpc.EnableTracing should be set by the testing infrastructure separately from executing the workloads.\n. 1024*1024. missingAddrErr should be a var that is this errors.New().. this and the next 3 fields are unused.. Is this the only way to do this?  String matching errors should be avoided as much as possible.  Do we need to support a default port in our resolver?\nWe could do our own regex on the target.  This would mean there is already a port (or the address is invalid): /(^[^\\[]*:)|(\\]:)/. Nit: updateChan?\n(And unexport?). Does IPWatcher need to be exported?  If so, why?. target appears to be unused.. Could this be simplified by using a map instead of walking through the slices?  I.e.:\ngo\nupdate := map[string]*Update\nfor _, a := range newAddrs {\n  update[a.Addr] = &Update{Addr: a.Addr, Op: Add}\n}\nfor _, a := range oldAddrs {\n  if _, ok := update[a.Addr]; ok {\n    delete(update[a.Addr])\n    continue\n  }\n  update[a.Addr] = &Update{Addr: a.Addr, Op: Delete}\n}\n// convert update into a slice and return it\nYou can probably simplify further by making oldAddrs and newAddrs sets (map[string]bool) instead of a slice of Updates.\nIn either case, you also wouldn't need to sort the address lists, either.. As discussed in person, let's try this strategy and see if it works:\ngo\nif ip := net.ParseIP(target); ip != nil {\n  // target is an IPv4 or IPv6 (without brackets) address.\n  return target, defaultPort, nil\n}\nif host, port, err := net.SplitHostPort(target); err == nil {\n  return host, port, nil\n}\nif host, port, err := net.SplitHostPort(target + \":\" + defaultPort); err == nil {\n  return host, port, nil\n}\nreturn \"\", \"\", fmt.Errorf(\"invalid target address %v\", target). Half-serious comment:\nHow about:\n2,4,16,16,9,14,7,7 ?\n98,100,112,80,105,110,103,33 ?\n103,82,80,67,45,103,111,0 ?. \"Almost completely opaque\"?  :)\nPlease document these constants.. Is this only for debugging?  Maybe remove it to avoid the extra memory...\n. Doc the units, please.  Maybe:\n// Bytes received during this sample.  ?\n. Units?. // in seconds. its. \"It returns true only if a ping must be sent.\" (?)\n. Is this racy otherwise?. Should this* be setting isSent instead of add()?. If no ping has been sent yet, why count this sample?. Nit: +=. Observation: it may be possible to use sentAt = \\<zero> to represent this instead.. Not sure if the compiler is smart enough to optimize all these casts of b.sample to float64 -- maybe sampleFloat := float64(b.sample)?. I don't think the float64 cast is necessary for \"2\".  Also -- const?. Why is 4/3 ideal?\nWhy do we want to \"round things out\"?  Isn't the sample going to be a very large number, usually?. It doesn't seem like this 1.5x actually does anything, since we only use this computation to know when it grows relative to itself.. If the bandwidth goes down slightly, but the delay is increased by a larger factor, we won't ever adjust our BDP estimate to compensate?  That seems like a flaw in this logic.\nShould this actually be something like:\ngo\nbwCurrent := float(b.sample) / b.rtt\nif bwCurrent*1.5 > b.bwMax && float64(b.sample) >= float64(0.66)*float64(b.bdp) {\n  b.bwMax = bwCurrent\n  ...\n?\n(Also, please use consts for things that we may want to tune.). Hmm, the above \"if\" maybe should have a \"&& b.bdp != limit\" term to prevent us from updating the BDP when it's already maxed?\n. Can this not be false?\nIf I see an option called \"Initial __\", I don't assume that will disable dynamic updates to __.  Ideally there should be a different option to set to prevent BDP calculation from affecting the window sizes.. Nit: prefix consts with \"bdp\" to avoid name collisions.. Can you add a method to http2Client to do this instead of inlining it, please?  So this would be:\ngo\n    updateFlowControl: t.updateFlowControl. Is it not a problem to skip the onData/onRead calls into the flow control?\nFor a later discussion, not this PR: should the flow control be aware of the BDP estimator?. No longer \"do nothing\".... Apparently \"localhost\" may not be proper.\nhttps://github.com/grpc/grpc-go/pull/1237#issuecomment-301159371\nSince this is production code instead of a test (as in that PR), we should probably do this now.. beta shouldn't need a cast.. Move this comment to where gamma is defined.  Maybe add something like:\n\"If gamma were 4/3, we would set our new BDP estimate to double the measured BDP\".\nAlternatively, express gamma as that scaling factor and have another const of 1.5 for the 1.5 round trips covered by the sample.  Or even gamma = 3/1.5?. Yes.  Fixed.. IMO \"Delay\" here should be interpreted by sendResponse as applying to the response as a whole -- header, payload, and trailer -- not each piece independently.\nSo, I believe this actually should still be \"false\", and it shouldn't be factored into the logic in sendResponse until the final step (where looking for other writers before flushing).\n. My comment was on the WithDetails method.  I can't think of any good reason to have that return a not-fully-populated status.\nI agree it is possible that Details may have useful partial results.  But if we do that, I wouldn't want to stop when the first error is encountered -- the order would be important when it shouldn't be.  Instead I'd want to return all of the messages and all of the errors encountered.  A few ideas:\n\nfunc (s *Status) Details() []struct{proto.Message, error}\n\nI.e. return a slice of a struct containing either the message or the error encountered while decoding it.\n\nfunc (s *Status) Details() []proto.Message\n\nReturn nil for entries in the details that encountered an error decoding (or omit them).\n\nfunc (s *Status) Details() []interface{}\n\nThe values returned in the slice would be documented as either a proto.Message or an error. \n Type assertion is going to be used already by 99% of users of Details, so maybe this is pretty reasonable.\n. That basically matches my thinking.  I do think 2b (2 with omitting messages with errors) is fine for nearly everyone -- and the raw proto is available for those few users that want errors -- which is \"good enough\" IMO.  But given that #3 is just as easy to implement and use, I prefer it slightly more.\nLet's go with that unless anyone else on this PR has something insightful to add.\n. Refactor?\ngo\nvar drain bool\nselect {\n  case <-t.Error():\n  default:\n    drain = true\n}\nif err := ac.resetTransport(drain); err != nil {\n.... Nit: delete this blank line. // create watchers that poll...\n. (similar comment change). Does the DNS resolver need to support a target that is not a DNS name?\nThat would simplify this parsing considerably, if not.\n. parseTarget SGTM.. lookup?. Why not \"s\" instead of \"r\"?. Why not \"s\" instead of \"r\"?. lookup?. lookup?. lookup?. lookup?. lookup?. lookup?. This seems unusual.  It means every call to Next() in which there are changes detected will result in an immediate subsequent call to Next().\nHow about starting the timer for when the next poll should happen (stored in dnsWatcher) right before performing the lookup, and blocking on that at the beginning of Next?. By the existing convention, we would name this file \"go17.go\" (and \"go18.go\" for the other).\n. defer replaceNetFunc()(). This form doesn't seem very helpful.  I'd just use &Update{} inline.\n. Similar for this one.... It's more customary to declare this inside the test that uses it, since it's used only once.. convertToMap.  Or updatesToMap (or just toMap if this is the only thing like this).. No, fake only seems fine to me.  That would require integration testing to bring up a DNS server, presumably.. please define this func next to hostLookupTbl and share with the other version-specific file.. Ditto. Nit: %q instead of %s. Wouldn't a second call to Next() return an empty updates list, meaning this test would fail?\nWhy not call Next() in the main goroutine to make synchronization simper?\n. This should go inside the Test function (and not be named with caps at the start).. %q. \"a buggy Balancer that reports duplicate Addresses.\"\n. Capital \"T\" at the start of the sentence.. FWIW, I wrote most of this yesterday before @ghasemloo's reply, but did not finish before leaving the office.  tl;dr: let's go with 3!\n\nI kind of like 4b because preserves type-safety,\n\nNot to be pedantic, but all of the solutions proposed are type-safe; this would declare more-specific static types than 3, but the actual utility of that extra type specificity is questionable (proto.Message will need to be type asserted to a concrete message type to be used meaningfully anyway).\n\nhandles the common case (people won't care about the error usually), but makes it easy to get the errors if needed (without duplicating the body of Details).\n\nOne downside with 4a vs. 1 is that you have to iterate over the results by index instead of entry if you care about the errors.  I'm also somewhat opposed to needing to line up parallel data structures to make sense of things where it can be avoided.\n4b is nice in that the errors are hidden until requested, but has some moderate wasted effort to re-parse everything in order to find only the errors.  I would rather have two functions, one that returns only the messages that could be successfully unmarshalled and one that returns the messages and the errors, and different users would call different ones.\nGiven that there are other errors that come from unmarshalling besides the type being unknown, I'd like to go with only 3 for now, still.  We could later add a func (s *Status) ValidDetails() []proto.Message (open to better name suggestions) if there turns out to be a reasonable case for having an easier-to-use version.\nA bit of bikeshedding: it's too bad Go doesn't have a native/standardized Either type that could be used instead of a custom struct or interface{} -- which have poorer semantics -- or else that would be the obvious choice.\n. Should this not be addrs[0]?. Do we need to clear this ever?. return nil?. This could be removed to simplify a little, since it's always an empty slice.. Would you mind adding a valid Details after this invalid one?  Even if it's an empty message (so no need to use MarshalAny/WithDetails), it would be helpful to make sure the function doesn't stop when it encounters an error.. No, I think this way is reasonable.  If someone somehow attaches Details to an OK status, then we might as well read it out for them.\n. states. eventually. csEvltr -> c?  cse?. How about this:\ngo\nupdateState := func(new ConnectivityState) {\n  ac.csEvltr.recordTransition(ac.state, new)\n  ac.state = new\n}\n....\nupdateState(Connecting)\n...\nupdateState(TransientFailure)\n...\n. *assertState. Doesn't matter here, but it's nice to do:\ngo\nreturn NewDNSResolverWithFreq(defaultFreq)\n(In case you have more complicated things in the constructor.. Right, and we can't do a trial-and-error approach here, either, so just trying localhost and hoping for the best SGTM until we have better technology in The Future.. A panic here (as an assertion) would be preferable.\n. Put the for loop around the code above and delete from here down -- it's duplicated (and the style above is preferred).\n. Maybe a nit, but logically this seems to belong more closely with the rest of the timer code in Next and not as part of the lookup operation itself.  (I was having trouble finding this.). That's reasonable, but by the same logic, maybe also put the timer blocking inside lookup as well?  I'm OK either way, but it seems like the timer code should be together.. This should go on pickFirst instead, otherwise it will be visible in our godoc, but it's intended for readers of the code (not the API).. If you embed the rr balancer in pickFirst, you won't need to rewrite functions like this:\ngo\ntype pickFirst struct {\n  *roundRobin\n}. Function comments must start with the name of the function.\nIn this case, you should just leave them out because the type (and, consequently this function) are not exported.. pickFirst -> pf for receiver name?. Also, I think this comment is stale.  watchAddrUpdates() wasn't changed and Start just calls the RR balancer's Start.\nI think you can comment pickFirst as:\npickFirst is the same as roundRobin, but with a different type to control conditional behavior in grpc internals.. go\n_, isPickFirst := cc.dopts.balancer.(*pickFirst)\n(same elsewhere). This change is gratuitous and go style prefers shorter variable names for small loops like this (as it was before); please revert.. This and the next 4 lines can be currentAc := cc.conns[0], no?. Consider moving the rest of this logic to a method in addrConn (UpdateAddresses).. for _, a := range ac.addrs {\n.... Well, I was going to suggest this:\ngo\n// No ac.mu.Unlock()\nfor _, addr := range addrsIter {\n  // No ac.mu.Lock()\n  if state == shutdown\n  ...\n  ac.mu.Unlock() // as before\n  ......\n  ac.mu.Lock()\n}\nBut the logic is so complicated (continues and returns) that that is hard.\nCan the body of this loop be factored out into one or a few functions?  It's quite long.. go\nif t != nil && !drain {\n}. s/an//. I would omit this comment, personally, or at least the backward compatible bit..  The spec requires this behavior.. pass goAwayID as a parameter (lastValidStreamID or something)?  Then you can delete the field in http2client IIUC.. This is really a special case of a general problem.\nIf a server says GOAWAY(4), which is valid, we need to kill streams 5->max.  This code does not handle that.\nHow about:\ngo\n// Round to a multiple of two, then add one.\nstart := ((lastValidStreamID + 1) % 2) + 1\n. Can you rename n -> end or something with meaning, please?. t.prevGoAwayID probably should be set to end - 2 here.  Otherwise if we get\nGOAWAY(INT32_MAX)\nGOAWAY(0)\nthe second goaway will go through 1B iterations.\nOR, we can do something like this, above:\n```go\nif t.nextID == 1 {  // or len(t.activeStreams) == 0\n  return // there are no streams to stop\n}\nend := t.prevGoAwayID\nif end == 0 || end > t.nextID {\n  end = t.nextID - 2\n}\n```. How about doing the same thing with t.Close() ?  t.closeLocked() which assumes t.mu is already taken.\nIf this ends up causing a bunch of work or ugliness, then don't.  But there are other places that follow the unlock->Close() pattern.  It's not a performance problem, since this is not a critical path, but I worry a bit about races if you give up the lock after you determine you should shut down but before doing so.. Can you do this in t.goAwayActiveStreams() to avoid the duplication below?. Nit:\ngo\nfor id, s := range t.activeStreams {\n. If there are no active streams, can we close and return above the select, even?  Or do we care about the GOAWAY even though there are no streams?\n. As discussed, don't bother with this.. Revert but add closeConn:true to the goAway.. && t.drainChan != nil\n. Remove closeConn. defer timer.Stop(). Remove local var. isSecond -> isFirst and reorganize?. Unnecessary Reset.. Move version into quota.. No, don't.  There should only be one version per connection, not one per stream-quota.. 2>&1. 2>&1 with grep. set -o pipefail\n...\ngit ls-files ..... | (! read)\njust to be paranoid. stream_s_. These two should be const instead.. This is unlikely to even trigger if there is a 4GB message.  length should be a uint64 if we're serious about this.. Be consistent about your use of constants -- use \"payloadLen+sizeLen\" where you have 5.. 1->payloadLen. How about inverting things to share the length-writing and wirelength code?\ngo\nvar buf []byte\nif cp == nil {\n  buf = make([]byte, payloadLen+sizeLen+len(b))\n  buf[0] = byte(compressionNone)\n} else {\n  buf = b\n  buf[0] = byte(compressionMade)\n}\nbinary.BigEndian.PutUint32(buf[1:], uint32(length))\nif outPayload != nil {\n  outPayload.WireLength = len(buf)\n}\nreturn buf, nil. m = make(map[string]Builder) to save the line in init(). is \"ok\" necessary?  Or can the user check b == nil?\nor even:\ngo\nvar defaultBuilder Builder = &pickfirst.Builder{}\n-- now \"!ok\" is not possible (once implemented).. \"// remainder once one connection is successful\". \"All SubConnections start\". Should we document when this could happen (that a connection goes to IDLE)?. \"checks if the currently-connected address is still in the new list\" ?. How do you feel about shortening to SubConn / ClientConn here?  They're easier to say and will match grpc's existing nomenclature.. it's -> its  (x2). by ClientConnection.UpdateBalancerState(). Shall we start using \"wait for ready\" instead of \"failfast\" in our docs?. How do we know that?. Balancer takes input from gRPC, manages SubConnections, and collects and aggregates .... It also generates and updates the Picker used by gRPC to pick SubConnections for RPCs.. If we have to tear down the old AC, then does this need to return a new AddrConn?\nWill that new AC auto-Connect()?  (What if the old AC is IDLE?). what's this about?\nShould this go on 767 as an \"else\" instead?. Remove?. Eventually we should always have a balancer, right?\n\"TODO: remove condition once we always have a balancer\"?. ClientConn or SubConnection (?). grpclog.Error()\nreturn \"UNKNOWN\"\n?\n. Can these be removed now?\nWe import all of these elsewhere.. Many of the comments in the balancer package apply here as well.. Re: balancer comments, how about setting the default scheme to a Builder instead of a string?. I wonder if we even need this sentence.  Like, if the resolver refreshes every 30 minutes and calls this, it's not a problem.  The idea was to avoid a busy-loop situation, right?  I'm not sure who would implement a resolver to continually call NewAddress, or what line of thinking might even lead to that.. For further discussion: can we avoid the need for this by having grpc start up a new Resolver for the same target?\n. These changes will break anyone using the connectivity state API (it's new, but, still).\nMaybe we can push a PR with only that change in it ASAP while we iterate on this PR?\n. That's not an option -- we must follow the protocol, which is 4 bytes to encode the length.\nMy point is about the type of the variables.  Checking whether a uint is > MaxUint32 will only work on platforms where a uint is 64 bits.\ntl;dr: please make length a uint64.\n. Should we use toRPCErr for this conversion instead?  It does the same things but more, except for calling convertCode.  But that function says it's intended for use on errors returned by the server application, which is not the case here.\nSo something like this?\ngo\nst, _ := status.FromError(toRPCErr(err))\nss.t.WriteStatus(ss.s, st)\n. Same as above.. Update date, please. Remove the parens: var registerCompressor = make.... These need comments.\ngo\n// Compress writes the data written to wc to w after compressing it.  If an error\n// occurs while initializing the compressor, that error is returned instead.\nCompress(w io.Writer) (wc io.WriteCloser, err error)\n// Decompress reads data from r, decompresses it, and provides the uncompressed data\n// via the returned io.Reader.  If an error occurs while initializing the decompressor, that error\n// is returned instead.\nDecompress(r io.Reader) (io.Reader, error)\n// Name is the name of the compression codec and is used to set the content coding header.\nName() string. We want the users to define this on their own instead.  Please delete all of this.\n. Please create a gzip package under encoding that, when imported by the user, registers a gzip codec.\nAlso, we should use the same strategy of re-using the Reader/Writer via a sync.Pool like we do with the existing gzip codec in rpc_util.go\n. Compressor is used for compressing and decompressing when sending or receiving messages.\n. // RegisterCompressor registers the compressor with gRPC by its name.  It can be activated when\n// sending an RPC via grpc.UseCompressor().  It will be automatically accessed when receiving a\n// message based on the content coding header.  Servers also use it to send a response with the\n// same encoding as the request.\n//\n// NOTE: this function must only be called during initialization time (i.e. in an init() function).  If\n// multiple Compressors are registered with the same name, the one registered last will take effect.. To avoid the duplication here, we should find a way to wrap a grpc.Compressor and grpc.Decompressor into an encoding.Compressor.  Or, better, wrap a grpc.Compressor.Do into the function signature of encoding.Compressor.Compressor and pass just that function.\nBut probably OK in a follow-up PR.. Definitely not MaxUint64. Pull this out of the if & else, please.. Why not just do buf = cbuf.Bytes() above instead of involving b?. Yeesh, we didn't write the \"experimental\" disclaimer here.  Can you add it?. Same.. I think this also needs to add a PerRPCStatsHandler() in the default call options, or else the per-call stats handler (ref'd by call.go above) will not use this handler.  Unless I'm missing something.\n. This is the application error, which is not what toRPCErr is supposed to handle.  This is where convertCode is appropriate to use.. Ditto. Please create a gzip subdirectory for a new gzip package and put this implementation in there.\n(Also, it should register itself via an init().). Not done -- this would involve declaring buf a bit earlier in the function.. We shouldn't need a slice of conn handlers.  Let's only do this if we must.. Leaving these to default to \"nil\" is fine here.  Only maps need to be 'made'.. Travis is failing because there's no comment here.  You can keep the old comment.. Remove comment, please.. \"WithCompressor\" (capital W). ...which sets the compressor used when sending the request.. rename \"outBuf\" to \"outHdr\" please, to represent what it has.  Or just \"hdr\" and \"data\" would be fine, too -- it might look a bit nicer that way.\nPlease make a similar change everywhere encode is called.. I don't think it's well-specified, but I would say actually this check should not include the header.  So len(outData) alone is better.\nPlease make a similar change everywhere encode is called.. This should be saved before setting it to false (on line 115) and restored to its previous value here instead of set to true.\nThere is the same problem in another spot, too, please fix all of them.. This creates another allocation.  This would be better:\ngo\no := &transport.Options{}\nh.t.Write(..., o)\nh.t.Write(..., o)\nThis occurs below, too.. This comment needs updating.. Let's just remove \"length\" now.  len(b) is fine.. This hardly tests anything!. Nit: an RPC error. Nit: resources. Should we add a note about how that means it also won't be called multiple times at once, and so doesn't need to be thread-safe?. ```go\nfunc init() {\n  encoding.RegisterCompressor(new())\n}\nfunc new() {\n  return &compressor{pool: sync.Pool{New: ...}}  // etc\n}\n```\nAnd compressor should actually implement everything directly -- not inline here.. gzip.gzipCompressor stutters.  Just \"compressor\". defer c.pool.Put(z)\n. // TODO: return an error instead. please, to match http2_server.go.\nWe should never panic unless it's unrecoverable.\n. go\nfunc init() {\n  encoding.RegisterCompressor(&compressor{.....})\n}\nThe idea is users import this package and it automatically registers gzip support for them.. return \"gzip\"; delete name field.. We never Put() the compressor.\nI think this needs to be a little more complicated.  We need to override the Close() method on the WriteCloser returned by gzip.NewWriter so that it calls poolCompressor.Put() on itself.  LMK if you need help with this.. This will re-add the decompressor to the pool before it is even returned to the caller.\nWe need to override Read() instead, so that when the underlying reader returns io.EOF, we do poolDecompressor.Put() at that time.. With this change, if the user does UseCompressor(\"something that does not exist\"), no error will ever be returned, we will just silently ignore that CallOption.  We should make it so when this happens an error (probably Internal) is returned from the RPC (early).. Sorry, I put this in the wrong place.  I meant this for where encode() is called.  I think what you have here is fine.. You should change the test instead.  This codec should be called \"gzip\".. I would make this just:\ngo\nif c.compressorType != \"\" && encoding.GetCompressor(c.compressorType) == nil {\n  return Errorf...\n}\n. Remove this line. s/gzipWriter/writer/ as this is the gzip package.. Optional: if you embed the gzipWriter instead of giving it a field name, you won't need this.. s/gzip//. In theory, this field should already be set, unless Get() needed to return a new gzipWriter.  How about this, to fix New() so that it sets the pool field for you:\ngo\nfunc init() {\n  c := &compressor{}\n  c.poolCompressor.New = func() interface{} { return &writer{Writer: gzip.NewWriter(ioutil.Discard), pool: &c.poolCompressor} }\n  encoding.RegisterCompressor(c)\n}. return &gzipReader{...}, nil. Remove z; rename maybeZ -> z.  Return z inside this case.. With both paths above returning, this is no longer possible.  A panic should be fine here instead of a return.\nAlternatively, it may be possible to restructure so the nil case happens outside the switch (or use an if).. ... in which all addresses share .... \"It wraps around\" or \"It is a wrapper around\". You should pool.Put(z) here and if there is an error doing Reset(), below.. Optional: z.Reset(r) instead (Reader is embedded).. Could you add a TODO here, please: TODO: eliminate extra Compressor parameter.. I think we should re-add the cbuf parameter to avoid the allocation.\n. Similar TODO here, please. Capitalization: \"decompressor\" and \"WithDecompressor or RPCDecompressor\".\nAdd to this comment: \"To match legacy behavior, if ......\"\nWe should also document this behavior on the comments to WithDecompressor and RPCDecompressor and also clearly mark those functions as deprecated.. dcReader -> compressor.\ndcReader is a better name for the variable you're calling ioRead now.. cpWriter -> compressor.. If all your parameters are unnamed, you can do this instead:\ngo\nreturn func(int) {\n...\n}. I think we said we wouldn't include the header as the message for this check.  (Also the error message would be out of sync.). Oh, I see what happened here.\nPlease keep outputting this the way it was before: only the header + all of the message data.\nThen pass them into the transport.Write function as two slices (instead of making two separate calls), then combine within there.  This way, the knowledge that a frame is 16kb is contained within the transport, which is desirable as it's really a function of the transport not gRPC.\n. Should this be a SubConn type instead?  We only need it so we can notify the balancer that the ac's state changed, IIUC.. Should this be errConnDrain?. What SubConn state changes could result from this call if the current address is not in the new list?  It should depend on the current state of the subconn, presumably:\nIdle->Idle  (Would this trigger a callback?)\nConnecting->Connecting (to new address) ?  (Would this trigger a callback?)\nReady->TransientFailure->Connecting->Ready ?\nTransientFailure->Connecting->Ready\nShutdown->Shutdown. s/put/done/g?\nOr maybe rpcDone or rpcEnd?\n(I would rather this not seem like the opposite operation as Pick().). I think this should be http2MaxFrameLen - len(hdr), to avoid presuming the length of the header.. r.Reset(data). How about moving this after the new if r.Len() == 0 that you added so you don't need to check isLastSlice?. ....soooo much duplication :/\nAll of the comments in http2Client.Write apply here as well.\n(Don't try to fix the duplication in this PR.)\n. I'm fine with reverting this, but then we should change make proto in vet.sh to run go generate.  Otherwise you end up with the latency of go get every time you run vet.sh, which is not really desirable.\nAlternatively, we can check $GOPATH/..... for these two things and fail with a friendlier error if they are not found.\nThoughts?\n. I could find no way.\nSince this requires coordination with .travis.yml, how about let's make the script run unconditionally in travis but only launch it conditionally on the version?. Typo: \"features\". Typo: \"Latency\". How about a formatting function you can call that is simply:\ngo\nfmt.Printf(\"%-80s%12s%12s%12s%12s\\n\", args...)\nAnd then:\ngo\n// print the header\nprintline(\"Name\", \"latency-50\", \"latency-90\", \"Alloc (B)\", \"Alloc (#)\")\n...\n// print this benchmark's data\nprintline(<bm name>, <val>, <val>, <val>, <val>)\n?\n(Note that I shortened the names a bit to help them fit better). If these hard-to-read strings aren't important to the program, we should output them in a more readable format.\nE.g.\nNetwork: LAN, Callers: 1, ReqSz: 1B, RespSz: 1000000B, Compress\nNetwork: WAN, Callers: 10, ReqSz: 1000000B, RespSz: 1B, !Compress\n. Related to the other comment: we should simplify the latency simulation options down to a single \"type\" enum, instead of individual numbers, and provide 3-5 sets of parameters that may be interesting.\ngo\ntype network struct {\n  name string\n  latency, kbps, mtu int\n}\nvar networks = []network{{\n  \"LAN\", 2, 100*1024, 1500,\n}, {\n  \"WAN\", 30, 20*1024, 1500,\n }, {\n  \"Longhaul\" /* ? name ? */, 200, 1000*1024, 9000,\n...\n}}. Please update.. Please update.. We don't need this extra slice now, right?\nHow about this as a simplification:\ngo\nvar leaked []string\nfor time.Now().Before(deadline) {\n  if leaked = interestingGoroutines(); len(leaked) == 0 {\n    return\n  }\n  time.Sleep(50 * time.Millisecond)\n}\nt.Errorf(\"Leaked goroutines:\\n%v\", strings.Join(leaked, \"\\n\")) // (Or leave the for loop)\n. Should we add a leakcheck_test.go to make sure it works?\ngo\nfunc TestCheck(t *testing.T) {\n  go func() { time.Sleep(5) }()\n  if ig := interestingGoroutines(); len(ig) == 0 {\n    t.Error()\n  }\n  Check()\n}. // Wait up to \"timeout\", but..... \"the\". You should actually export errorfer in this case, so that it's documented (godoc) what a user needs to pass in as a parameter.. Nit: const. Call check() again with 3 seconds and expect zero new errors?. \"Local\"?. if _, ok := networks[networkMode]; !ok {\n  // error\n}\n. This is error prone and hard to keep in sync with something defined in a different package.\nHow about adding a field to Features for the network name and print that instead of these three metrics if it's present?  Otherwise fall back to showing them.\n. float?. Is this \"Avg latency\"?  (If so, label it please). Print directly:\nres += fmt.Sprintf(\"%v ns \\t\", s.NsPerOp). Just print the value directly:\nres += fmt.Sprintf(\"Count: %v \\t\", s.Operations)  // or %d works too. Remove this line please. Use a latency.Network directly?. Also, how about defining the common network types in the latency package, too?\ngo\nvar LAN = Network{Latency: ..., Kbps: ..., MTU: ...}\n// etc. Typo. Why start at 1?. log.Fatalf(). log.Fatal(). log.Fatal. Only defer close if no error. Derp.\nAre you saying to set check_proto temporarily to test this PR?. Can you update this for networkMode (and the trace change as well)?. This should not be necessary; it is unreachable.. changes += .... Why isn't ((val2-val1)*100)/(val1) sufficient?  If these are true time.Durations (int64 nanos), this should \"just work\" without anything special.. Is the \"k\" in the middle a typo?. This isn't fatal?. Please also document how to run twice and compare between the two.\n. Error from where?  I thought Go was smart enough to recognize Fatalf did not return.. This is no longer a valid command.\nLet's make sure you can copy/paste this into a terminal and it does something useful.\nJust delete the latency/kbps/mtu flags here; we can keep those in the binary, but only for unusual use-cases.. Can you show the command to run to compare the two runs?. make printline() take interface{} instead of string and you can remove the strconv.FormatInts and the .String() calls too.  (Print using %v instead of %s.). Revert in this PR?. I don't think we want this change in this PR (?).  This buffer is still relevant and I think this is beneficial for streaming RPCs.. that->than. Atomic instead of lock?. go\nif time.Since(t1) > 2 * time.Second {\n?. Maybe an atomic instead of a lock?  You still need the lock for incrementing the version at the same time as the add(), and for compareAndExecute, but that is presumably a more-rare operation than just getting the channel.. These are still being done without a deadline?. Is it possible to combine these conditions and still have conditions understandable?. How about \"hasData\" or something to explain what distinguishes this from the above?. This is true, but is that a problem?. For huge amounts of data and msgs that are natively Stringers, it's probably best to call truncate(stringerMsg.String(), truncateSize) inside fmt.Sprintf() to avoid an allocation and copy of the data that will be removed anyway.  But this is probably a pretty trivial savings.. Interesting.  Did you run the benchmark for streaming large messages with only one concurrent call/connection?  (Does that benchmark do repeated sends on the stream or just one send and one receive?)  That is the only place where this change should show much impact, so it would be interesting to see how much, if at all, it was impacted.. Typo: resulT. Nit: \"memProfileRate\" to match the case of the flag.. What is this about?\nI'd think not setting --memProfile would turn off profiling?\nHow does setting memProfileRate to zero differ from its default of zero?. Logically, this would be more intuitive:\ngo\nchanges := k2 + \"\\n\"\nchanges += fmt.Sprintf(\"%10s......\", \"Title\", .......)\nor even put the 'k2\\n' in the fmt.Sprintf call.. Needs package-level comment.. Why a trailing space?\nAnd why the trailing space in the sprintf below?\nYou shouldn't need this anyway.. You should be able to do:\ngo\nfmt.Sprintf(\".... %8.2f\", ..., (100*(val2-val1))/val1)\nSame for the FormatInt.  Use %v or %d to format it via the Sprintf.. Trailing space?. Trailing space?. Do you want the leading space?\nRemove the trailing space please.. \"sequentially\" means \"in order\".  I think we want \"synchronously from the same goroutine\" instead.. Change: \"The balancer is not required to call ClientConn.RemoveSubConn for its existing SubConns.\"?. Document how to use this or when/by whom it should be used?\nShould this package register itself with grpc when imported, instead of exporting NewBuilder()?\n. Nit: return directly instead of creating b.. \"to quickly lookup an address\" or \"used for quick lookup of an address\".. Go-ism:\ngo\nif err != nil {\n  grpclog.Warningf()\n  continue\n}\nb.subConns[a] = sc\n.... Log an error?  Should we be getting state changes for subconns we're not tracking?. Nit: generateS. Nit: returns. , or. \"does round robin selection of all Ready subConns otherwise\"?. Not necessary. Optional:\ngo\nif (s == connectivity.Ready) != (oldS == connectivity.Ready) ||\n  (b.state == connectivity.TransientFailure) != (oldAggrState == connectivity.TransientFailure) {. Maybe move this above (where you check Idle) and do:\ngo\nswitch s {\n  case connectivity.Idle:\n    sc.Connect()\n  case connectivity.Shutdown\n    delete(b.scStates, sc)\n}. Make this a nop as the Balancer has no internal state to clean up?. Add to comment: \"immutable\"?\nActually, only next is mutable, so maybe reorganize this / add comments accordingly?\n. Avoid defer of Unlock() here.. Optional: avoid the defer and reduce to a single return statement.\nOr delete the mutex and assert that cse can only be called synchronously (because HandleStateChange is called synchronously).. Let's fail after a few hundred/thousand attempts so this doesn't hang for 10 minutes if there is a problem.. 2s deadline?. Is 1ms long enough?  Maybe 50ms?. 2s deadline?. Should we have some fail-fast, some non-fail-fast?  They both should block.. Abort eventually.. Is this right?  In theory if the backend you stopped is still up, this will break before attempting the one that would reach it.  Instead you should loop, doing RPCs until you see [0] twice without seeing [backendCount].  (Then you can remove the 100ms sleep.). You should fail if you exit this loop with i == 1000 (or didn't set a flag or something).. Abort eventually.. How about just \"scState\" and \"scStateBuffer\"?  It's the usage of the struct/buffer that determine what the state means.. Should we make one of these that uses interface{} and put it in its own package?  It's slightly less performant than one that has a concrete struct, but this isn't a performance-sensitive path.. Naming nit: resolverUpdate?. Should this logically be owned by the cc, with the balancerwrapper pulling from there?\nIMO we should try to keep ccBalancerWrapper as small as possible to avoid potential future confusion (\"was that thing in the wrapper or clientconn?\").. Is this strictly necessary?\nIf so, how about doing nothing in the other select in the ccb.done case except breaking, and letting this close the balancer?  That removes some duplication.  (For clarity, you may want to put this select after the other, then.). Why would this happen?. if you drop the , ok, s will be Idle, so the following check will still work:\nhttps://godoc.org/google.golang.org/grpc/connectivity#State\n(We should probably add an Invalid or Unknown state as the zero value.). updatePicker _____. Avoid the defer, please.. *acBalancerWrapper?. Combine with previous line?  if t, ok := ...; ok {. Do we not need to call put()?. put->done?. put->done. put->done. put->done. put->done. put->done. put->done. Is 1ms enough time?  (And below.). Can you use a const instead?. newBlockINGPicker?. Drop the \"besides\".. Unnecessary.\n. UnregisterForTesting?. hBuf: &bytes.Buffer{}. connection. This will result in reallocations if there is any user-provided metadata.\nIt probably would be a win to count it first.. Delete. except after \"c\". beginning. hBuf: &bytes.Buffer{},. \"at least\". Delete. I think it makes sense to do the math to avoid the reallocation/copy.. Do the math?. Delete. respective. Can this logic be shared with the client's?\ntransport.go or control.go: headerFrameHandler(hEnc, hBuf, framer). Done.  The reason the conn is passed in the defer below, BTW, is because if ClientHandshake encounters an error, conn would probably be nil.  I tried to clean this up, but it's a slippery slope and the diffs start looking much bigger.  I'll wait until later and only do what you asked for now.. Someone once told me not to put TODOs in docstrings...  Seems like a reasonable policy since it's for code maintainers and not users?  Maybe move this inside the function or above the docstring comment.. if len(cc.conns) == 0 instead?  Then you don't need the \"if ac == nil\" below.\nWhy does this return toRPCErr(ErrClientConnClosing) but we return errConnClosing below?  We need to clean up this error stuff.. Disclaimer about \"for grpc internal use only\"?. *implemented. Removed the if, and unreachable entirely.\nYes, errorChan is essentially t.ctx.Done(), with only subtly different semantics that were irrelevant.  Removed.  Good catch.. Could you add some benchmarks to cover this so we can decide?  We don't have anything that includes metadata.  If the number of entries and size of each entry can be tweaked through settings, that would be ideal.  Ideally we could independently configure client headers, server headers, and server trailers.  It would be interesting to compare the performance of both approaches in the following scenarios:\n\nno metadata\nwith a small amount of metadata (~2)\nwith a large amount of metadata (~20)\n. Yes, that should have been obvious, sorry!. SGTM, let's try not to put it off for too long.  Maybe add a TODO in this PR.... typo. I'm fine with this mostly because the zero value isn't ideal (i.e. an Invalid or Unknown).  Otherwise, the code ends up simpler because there are fewer conditions involved.. \"is\"?. s/roundrobin// for package variables/functions/types where possible.. I see what you did there.. Why 100ms?  Sleeps make me nervous.. defaultCodec?. I think we should remove this block and add a function RegisterDefaultCodec to overwrite defaultCodec.  What do you think?. Sorry for the change, but can you move this and copy the Codec interface to grpc/encoding/codec.go (package encoding)?  We can leave the Codec interface in the grpc package for a release or two and remove it after then.\n\nAnd then move the proto codec to grpc/encoding/proto/proto.go?\nThe plan is to use the encoding package for both this and the compression API (PR #1428).. \"errEmptyContentSubtype\"?. \"errBadCodecString\"?. I'd mention here that the content subtype is a case-insensitive lookup, instead of above.\nAnd above, mention that the subtype is converted to lower-case before being included in the content-subtype field.. s/this/that/?. I would prefer to ignore it (just send \"application/grpc\") when this happens.. Similar to above.  It's probably better to continue without setting the content subtype than error out.  If there was some way to error CallCustomCodec() itself, that would make sense, but given that checking inside beforeCall delays the check, I think we should try to continue if possible.. How about this:\ngo\nif c.codec == nil {\n  c.codec = encoding.GetDefaultCodec()\n}\nc.contentSubtype = strings.ToLower(c.codec.String())\n. Go style nit: s/getC/c/. Go style nit: s/getC/c/\n. some->come. Maybe I don't understand...  Do you mean \"strip off application/grpc\"?  If so, I think I like this more (propagate the whole value verbatim).. Nit: \"or else\" or \", otherwise\". :disappointed: OK.  Maybe a comment here that says \"give grpc a chance to see the error and potentially close the connection\"?. > I need more direction here - will google.golang.org/grpc depend on google.golang.org/grpc/encoding?\nYes\n\nShould I add another public method GetRegisteredCodec(contentSubtype string) Codec to encoding? Also maybe GetRegisteredProtoCodec() Codec which must return nil?\n\nYes.  (I prefer GetCodec(string) Codec, though, to be less verbose.)\n\nIf Codec is still in google.golang.org/grpc, this will result in a cyclical dependency, as I need access to the registered codecs there, but encoding will rely on the base package.\n\nThe cyclical dependency is avoided by copying the Codec definition to encoding and referencing that one from both the grpc and encoding packages.  The old one only exists temporarily in case external users are referencing it.  Maybe:\n```go\npackage grpc\nvar _ Codec = encoding.Codec(nil)\n```\nTo paranoidly guarantee they are identical?\n. To get the proto codec, you can just use GetCodec(\"proto\") instead of a special function, no?\n. Yeah, that makes sense.  We don't need a \"default\" codec.. Undefined just means we haven't defined it.  :)  I'd rather explicitly define it that way.  Then users can do:\ngo\ncc := grpc.Dial(..., grpc.WithDefaultCallOptions(grpc.CallContentSubtype(\"blah\")))\n...\ncc.Call(..., grpc.CallContentSubtype(\"\"))\nTo restore the default behavior for that one call.. I think we need to avoid an error here to prevent a backward-compatibility breakage.  I.e. we currently ignore the String() of the codec provided entirely.  Now that I think about it more, we also really shouldn't set the content sub-type at all when this setting is used, either.  (Note that this is how WithCodec will be implemented.)  There's a reasonable possibility of breaking anyone using custom codecs and non-Go peers if we change it.\nWe need to provide this for backward compatibility and also for the use case you're interested in, but we should discourage users from using it, and direct them strongly toward the CallContentSubtype() + encoding.RegisterCodec() APIs instead.\nIf you disagree with the above and want to debate further, I'm happy to, but yes, let's take it offline.\n. Per other comments, this should not be done to preserve backward compatibility.. OK.  I think the hatred is more toward accessors and this is actively doing something.. This would remove a little duplication (less work done in the special case):\ngo\nscheme := \"\"\nremainder := target\nif len(ts) == 2 {\n  scheme = ts[0]\n  remainder = ts[1]\n}\nauthority, endpoint := parseEndpoint(remainder)\nreturn resolver.Target{<scheme, authority, endpoint>}\nOr here's something maybe a little too code-golfy, but I think it's OK:\n```go\n// split2 returns the values from strings.SplitN(s, sep, 2).\n// If sep is not found, it returns \"\", s instead.\nfunc split2(s, sep string) (string, string) {\n  spl := strings.SplitN(s, sep, 2)\n  if len(spl) < 2 {\n    return \"\", s\n  }\n  return spl[0], spl[1]\n}\nfunc parseTarget(target string) resolver.Target {\n  scheme, remainder := split2(target, \"://\")\n  authority, endpoint := split2(remainder, \"/\")\n  return resolver.Target{Scheme: scheme, Authority: authority, Endpoint: endpoint}\n}\n```. Could you add a short unit test for this function please?. >  I think this should be left to another PR as if this causes problems, it should be independently roll-backable.\nThat's fine, I agree with that premise.\nBut, I'm not sure I understand where you're seeing the risk of breakage.  I think the only possible breakage is if someone is doing something like this:\ngo\nvar codecDialOption = grpc.WithCodec\nand then in a test:\ngo\ncodecDialOption = func(grpc.Codec) grpc.DialOption {\n  // ???\n}\nOtherwise, these are identical interfaces, so anything that satisfies one satisfies the other.  Let me know if you see something I'm overlooking.\n\nNote it's your repository so this is up to you :-) but I am very against not following SemVer\n\nI'm pretty sure my hands are tied on our release process and versioning, but I can put out some feelers.. This was fixed by #1553.. t=transport.  I'll add a comment.  I wasn't a big fan of this function in the first place, but I decided to keep it (for now anyway).. The comment was removed when Go1.6 support was removed.. ```go\npackage balancer\nconst RoundRobin = \"roundrobin\"\n// or\nvar RoundRobinBuilder = ....\n```\n?\nIf not, I think a const in this test would be better than repeatedly using the same string.. I don't understand this comment...  Why doesn't the first RPC succeed with the first server?  Why try 1000 times if the second one should succeed?\nShould we do several attempts that all return the same server?  I.e. do calls until you get the one you want, then do ~3 more and make sure they all match the same one?  (So we're sure we aren't round-robining.). So, this does 1k RPCs until it eventually sees the server it's looking for, then it repeats through each server..  Why do 1k attempts?  Maybe the second run-through should only do a single attempt for each, since we know we have a connection to every server by then?. If you move this above 626, then you don't need to do it inside the if. Suggestion: I'd like to avoid \"failed\" here.  Maybe something like \"ignoring service config balancer configuration: WithBalancer DialOption used instead\"\n. Move this defer to the end and make it not a defer?  Or move it to the top and remove the direct call on 659?. s/will be/is/. Same. I don't know why I said anything about in this error case, but in the one below (line 71), if there's an error resetting z, we should still call .Put(z) I think.. s/will be/is/. Same. How is this different from clientCompression?  And clientNopCompression?  Please add a comment to all 4 of these fields.. FWIW, defers are fine (and help with readability) as long as they're not on critical paths / high-frequency operations.. LGTM.. For clarity, how about this:\ngo\nmaxInt := int(^uint(0) >> 1) // the maximum signed integer value on this system\nres = min(res, &maxInt)\n(Note that your version has an extra pair of parentheses which is hard to see because of all the nesting.)\nWhy does min() take pointers?  That just complicates things further.  maxInt could and should be a const instead.\nHow about make min() and get*MaxSize() take and return ints, and deal with pointers only where you call them:\ngo\nc.maxSendMessageSize = newInt(getMaxSize(*mc.MaxReqSize, *c.maxSendMessageSize, defaultClientMaxSendMessageSize)). How about this:\n\nChange callInfo.max*MessageSize to non-pointer types, since they must always be set.  This removes some paranoid checking\nChange min() to accept and return int instead of *int\nChange get[Raw]MaxSize to return ints instead of *ints\n\nIs that possible? I think this would make things simpler and keep getMaxSize able to handle *int inputs.. *\"bytes have\". This is actually not true (any longer?).\nNewStream only returns errors before it attempts to write to the network.  I.e. this block should be deleted (and is by #1597).. Similarly, if there is no error from NewStream, we always attempted to write bytes to the wire.  So this should be \"true\".  (And below.). *clearS. What is nextSL and nextSC?  Please document.. Should this say \"on all connected/ready backends?\". Do we want to keep this log permanently?. FWIW, optional, I prefer short constructor statements on one line to make the code shorter:\ngo\n  return &lbPicker{err: err}. Would this be useful?:\n```go\ntype errPicker error\nfunc (e errPicker) Pick(...... {\n  return nil, nil, error(e)\n}\n``. Should we move this to where the picker is constructed and set p.err instead?. OK.. well let's just makemindeal withints then?  We can deal with the pointers where it's called.  Then we can have a const formaxInt` and use it without creating memory.\nLet's make sure we add some comments on these CallOptions/ServerOptions that specify the maximum effective value is MaxUint32 due to protocol limitations.. Is this safe?  If lb.fullServerList is changed after the picker is created, the picker may be impacted.. Move this defer after we return errors?  Then you can remove the if err != nil bit.  It looks like we only return non-errors after returning every error.. This can't happen unless p.serverList > 0, because of the check at the top.  Move this inside the if for clarity?. This isn't really an \"else\".  This is more of a \"now\".. What does it mean if p.serverList has nothing but p.readySCs does?  Is that valid?. *sendS. Please update comments now that RR is not used.. wat?. Eep. (How) does this happen?  Do we actually handle this correctly?\nIt looks like it only happens if the ClientConn is closing.  Maybe we should just eat that error in grpc and give back some kind of dummy disabled subconn to make the balancer think it's happy?  Ideally it seems like the balancer shouldn't have to deal with this situation.. I think either ctx needs a deadline or we need to do this in a way that will unblock if lb.doneCh is closed:\ngo\ngo func() { lbCh<-lbClient.BalanceLoad() }()\nselect {\n  case s,err:=<-lbCh: ...\n  case <-lb.doneCh:\n    return\n}. Fatal? :cry: \nMaybe we should just ignore it instead?  Or continue / return?. I'd like to propose an alternative to avoid passing around this channel:\n\nMake this a context instead.\nCancel it here after readServerList returns.\nPass it to sendLoadReport.\n. I think the wg can be eliminated.  One of these tasks can be done in the background (probably sendLoadReport) and then readServerList can be done in the foreground.  If it closes the channel / cancels the context, it's guaranteed to kill the sendLoadReport quickly, and it's not critical that it finish before proceeding and starting up a new stream AFAICT.. Remove one-var block please.. how about func withSomeOtherKindOfDialer(...) DialOption {}?. I'd like to see if there's some other way to do this.  Registering a random string as our resolver seems a bit hacky.  Maybe we can make a new, internal dial implementation that doesn't need a scheme but has a resolver builder passed to it directly?\n\nOr can we use NewSubConn maybe (but never return it from the picker)?. See above comments in call.go. Make sure you write \"for testing only\" in the godoc then!!. I assume the \"testing only\" applies here too?. generateS. returnS\nand doES below.. IIUC, timer is a local, so you don't even have to stop it if you don't want.  You definitely don't have to check its return value and drain its channel.  That's only if you want to reuse it.. var remoteBalancerAddrs, backendAddrs []resolver.Address\n?. Can we return an error instead anyway?. This is still racy - is that OK?. It turns out this is not true.  I made some benchmarks, and found the following results:\nBenchmarkCancelContextErrNoErr-12               100000000           52.2 ns/op\nBenchmarkCancelContextErrGotErr-12              100000000           51.9 ns/op\nBenchmarkCancelContextChannelNoErr-12           200000000           19.9 ns/op\nBenchmarkCancelContextChannelGotErr-12          100000000           36.1 ns/op\nUsing the channel is faster as of today.  I believe the problem is the defer in context.Err():\nhttps://github.com/golang/net/blob/cd69bc3fc700721b709c3a59e16e24c67b58f6ff/context/pre_go17.go#L162\nThese numbers line up well with our benchmarks for mutexes vs. channels and defer vs no-defer:\nBenchmarkMutexWithDefer-12                  20000000            51.2 ns/op\nBenchmarkMutex-12                           100000000           14.5 ns/op\nBenchmarkSelectClosed-12                    50000000            20.3 ns/op\nBenchmarkSelectOpen-12                      300000000            5.74 ns/op\nNote how the select is faster than the mutex with a defer, but slower than the mutex with an inline Unlock().. > Quite surprising, thanks for the details. Your comment points to pre_go17.go but I\n\nassume these benchmarks were run on 1.9?\n\nYes -- presumably the std context package is implemented identically (based on the results that makes sense).\n\nThe former does strictly more work.\n\nGood catch.  Actually, it does not do strictly more work, because I forgot to call Err() in this case.  Adding it results in:\nBenchmarkCancelContextErrGotErr-12          100000000           52.4 ns/op\nBenchmarkCancelContextChannelGotErr-12      50000000            94.8 ns/op\nBenchmarkTimerContextErrGotErr-12           100000000           52.5 ns/op\nBenchmarkTimerContextChannelGotErr-12       50000000            86.7 ns/op\nI would still lean toward the existing approach, as it optimizes the non-error case.  But maybe the Go team should think about removing that defer.... FWIW I filed https://github.com/golang/go/issues/22401 for this.. return \"\", \"\", false?  We don't need s anyway.\n. \":///\" ?\n\"://\" ?. I think the code would be clearer as:\ngo\nif tq < size {\n  size = tq\n}\nstreamQuota -= size\nlocalSendQuota -= size. Can you add a comment to say what this test is supposed to catch that our other tests don't?. You can leave this bit of trivia out...\nMaybe say: \"This is a stress-test of flow control logic\"?. This seems fishy to me.  But not so much this as our settings handling in general.\nIt seems the way it works right now, we aren't applying any settings until right as we send the ack.  This will result in a race if I'm understanding it correctly:\n\nServer and client are in steady state with maximum streams = 5\nClient receives a settings frame with a limit of 1.  It appends the ack to the control buffer in response.\nClient initiates 5 new streams and pushes their headers onto the control buffer, which it thinks is fine because the settings are not applied yet.\nClient applies settings and sends settings ack, promising not to initiate more than one stream.\nClient sends headers for 5 new streams.\n\nI think we need to make sure settings are applied before queuing the ack message if the settings are more restrictive than they were before.  On the other hand, we should queue the ack message and then apply the settings if they are more permissive.  And...since it's possible for one setting to become more permissive while another becomes less permissive, we probably need to hold a lock, apply the settings, queue the ack, and then release the lock.\n. Nit: missing space before open paren in string.. Why not apply settings before enqueuing the ack the first time?\nThis results in more delay in processing the ack (it has to make it through the control buffer twice), and it splits up the code that handles settings, which makes it harder to reason about.\nIf there's some reason we have to do it this way, that's fine but we should put comments in to explain it.. Shouldn't we be comparing this to t.initialWindowSize?\nAlso, do we have to take a lock for this field or t.maxStreams?  I was thinking the only thing that should be changing these fields is the handling of a settings frame (so, this code here)...\n. Optional (but recommended): return int(s.Val) < t.maxStreams\n. Optional, maybe for a later refactor:\nThe code might be cleaner with two implementations: handleSettings and handleInitialSettings.  handleInitialSettings doesn't need to check restrictive/permissive -- just apply all the settings and write the ack.  Then the special-case code checking for max concurrent streams is removed from the ongoing handleSettings function.. Please add:\ngo\n//\n// This API is EXPERIMENTAL.\nHere and to the other functions you created.  We may want to make changes if it doesn't work out right for users.. (EXPERIMENTAL on this package comment is good for this whole file.). This doesn't seem valid.  You're adding the compressor that you're about to be using back to the pool.\n. I'll leave both the finish and the closing of s.done for now; we will clean things up in the future.\nUnexported statusGoAway.. Unexported, added TODO.  We have a lot more cleaning up we can do.. Deleted entirely (it was unused).. Let's remove frequent defers like this while we're here.. Same: remove defer please.  Go doesn't perform as well with them.  :disappointed: . Would it help to create the waiters{} once per stream and re-use it?\n. Document why this exists: what purpose does this quotaPool serve?. Fixed. Done. Deferred.. Ping?  Bytes not sent.. I still call cancel on the next line..  I don't expect that deadline exceeded vs. cancellation results in different paths through .Err() or .Done(), but TimerContext is a different implementation from CancelContext which is why I have two benchmarks.. How about:\nWith...Settings blocks until the initial settings frame is received from the\nserver before assigning RPCs to the connection.\nAlso, how about WithWaitForHandshake?  I think I'll be renaming the ServerOption to HandshakeTimeout to match Java/C.. Please give this a more specific name; I'm implementing a feature called \"retry\" too.  \ud83d\ude04 . and also the server preface, right?\nHow about \"connectDeadline is the time by which all connection negotiation must complete\"?. *transport. is->was. How about: var backoffDeadline, connectDeadline time.Time for brevity?. \"start\"?. Why?\nCan we simplify by removing the locals?. This loop could be refactored into another function perhaps?. Error seems too strong for this.  Warning?. Done, and added the same thing above in the case that UseCompressor specifies an unregistered compressor.  (I left out the details about \"identity\", because I don't think it's worth mentioning, and it's covered in the bottom section.). Done (x3). This is generated code; we aren't supposed to even look at it.  :). It may be faster (and is simpler, code-wise) to just clear everything unconditionally.. The timer fired; stopping it is unnecessary.. Warning?. \"InitialAddrs\"?. What are you filtering out with this?. Not done?. What's a fallback timeout?  OK, I think I know what it means, but can you explain in the comment?. What does this guard?. Not done?  Maybe your latest updates aren't pushed?. I have heard just importing the reflect package is a bad idea (in the form of performance or binary size or both?).  We should look into that claim and remove it if so.  Looks like we're already importing it, so LGTM for this PR.. Go style:\ngo\nbuilder := cc.dopts.balancerBuilder\nif builder == nil {\n  builder = newPickfirstBuilder()\n}. Can this result in multiple simultaneous outgoing calls to the resolver's ResolveNow method?  Is that OK?. go\ncc.mu.Lock()\nr := cc.resolverWrapper\ncc.mu.Unlock()\nif r == nil {\n  return\n}\ngo r.resolveNow(o)\n. True..  So what do you think about implementing it that way?\nIt seems like you essentially have 2 kinds of pickers, but they're implemented together and IMO it's preferable to have two simple things vs. one more complicated thing.\n. Can you move that if len(p.subConns) <= 0 above this one?\nI wasn't suggesting to move the defer outside the if, just below where you return errors.. Go style:\ngo\nif err != nil {\n  grpclog.Warning...\n  continue\n}\nlb.subConns[addrWithoutMD] = sc\n.... In that case, can we suppress the error on line 187 if doneCh is closed?. Can we make the scheme \"grpclb_internal_<random>\" or something like that instead so it's not just a garble of random characters that will confuse users?. remoteBalancerErr = nil?. Maybe we should add a comment to ResolveNow() to let users know that it could be called multiple times concurrently.. Should we try to make NewSubConn([]) do the same thing that sc := NewSubConn([foo]); sc.UpdateAddresses([]) would do, for consistency?. Space after //. Maybe put this inside a if prevAddr != resolver.Address{} for clarity?\nCould prevAddr be an int instead to avoid the need for this search?  If the address list is changed during a connection attempt, which is pretty unlikely, we could just restart from the top of the list and reset the backoff - what do you think?. Should this be TransientFailure?. Nit: Unwrap please.. I believe this comment is fine if you do:\ngo\nconst (\n  MaxRPCVersion...\n  ...\n)\nOtherwise, this must be formatted like // MaxRPCVersionMajor contains the maximum ............. for golint.. How about: grpc: addrConn.createTransport failed to receive server preface before deadline ?. Not return false?. Delete these lines; ac.transport must be nil.. Isn't this still a little racy?  We write the settings immediately after closing sent.  I think we should sleep for a little bit after closing sent, too.. Could you please update this to:\ngo\n// Version is the current grpc-go release version.\nThis should help avoid confusion with the new consts.. Actually, after more deliberation on our side, could you name these:\ngo\nProtocolVersionMaxMajor\nProtocolVersionMaxMinor\nProtocolVersionMinMajor\nProtocolVersionMaxMajor\nAnd also please update the comment above to change \"RPC version\" to \"protocol version\"?\n\nI prefer to name consts/vars/etc in a hierarchical fashion, putting the most significant parts first, which in this case would be ProtocolVersion, Major/Minor, Min/Max.  (This makes them sort better, and effectively namespaces them.)\nWe thought \"RPC version\" as a name was potentially confusing, and gets worse when it's grpc.RPCVersion.\n\nThanks!. *MinMinor\n. We used to do this only for the file it was in, IIRC.  Can you make this grep for 'clientconn.go:.*cancel ........ ?. WriteStatus is guaranteed to only execute once, but if this line of code is reached and THEN WriteStatus is called and gets all the way to closing ht.writes before the ht.do() here, then we would end up with the same panic.\nShould WriteStatus call ht.Close before closing ht.writes?  I think that would also fix the problem.  Then the code inside do will avoid writing to the closed channel.\n. Makes sense.  If we are here the only possibilities are Connecting or Shutdown.. OK, I see.. So it is still possible it returned before the settings were sent in this case, because we close sent before sending, but it's unlikely because of the sleep before sending.\n. Nit: \"addrs\". go\nisConnected := ac.backoffDeadline.IsZero() // Client received server preface.\n. Nit: add a blank comment line above this one so it's its own paragraph.. Nit: \"cannot\" or \"will not\".\nAlso, maybe something like: \"Balancers are registered by name using the balancer package\" before this sentence?. Per proto.Unmarshaler: \"The method should reset the receiver before decoding starts.\"\nSo this should not be necessary above the call to Unmarshal.  Also, buffer.Unmarshal says \"Unlike proto.Unmarshal, this does not reset pb before starting to unmarshal.\". > See ... golang/protobuf#424.\nThat's...amazing...\nOK, this LGTM then.  Thanks!. Please delete, then LGTM.. algothrim -> algorithms. creates. algorithms. Even though it's trivial, we may want to export this for our users, too.. EXPERIMENTAL for now?. *namE. I think we don't want \"newBalancerName != grpclbName\".  We should break if any Type is resolver.GRPCLB, and re-assigning newBalancerName to grpclbName is not worth optimizing away (actually it's more optimal than the extra branch).\n. I thought we wanted to go with gRPCLB if any address is a gRPCLB address?  This will switch away if both kinds are returned and we were previously using gRPCLB.\nMaybe we should have a test where the gRPCLB backend changes (but both before and after are gRPCLB).. > It will NOT send the current address list to the new balancer.\nWhy?  How do we bootstrap the new balancer then?. Should this go above the log about switching?. Is 1s long enough for these tests on travis?  I'd rather give them 5-10 to avoid potential flakiness -- they stop early when the condition occurs, so it's not harmful to give them longer unless the timing is important.. resolverBuilderUserOptions?\nI'm also OK with this as-is.\n. *DialOption. This should return strconv.FormatInt.... Needs copyright.. Do you need x?  It should be the same as i.. Nit: these ResetTimer/StopTimers are not necessary since there's no setup/verification code before/after the benchmark.. Done.. Done.  Good call, too: it turns out the quote characters are passed to UnmarshalJSON, which I was not expecting.. Right, but we don't need to log that we aren't switching if the name doesn't change, right?. Optional, but it's a go-ism:\ngo\nnewBalancerName := grpclbName\nif !isGRPCLB {\n  ...\n}\nbut it's a little awkward in this case, so either is probably fine.. I would rather move this above the info log on 720 also, but if you feel strongly then OK.. Shouldn't we do the preface checks before we even create the http2Server?\nHow about this:\n\ncheck preface\ncreate http2Server\ndefer func () { if err != nil { t.Close() } }()\n\nso we protect ourselves from this type of problem in the future.. Yes, thanks - done.. Accidental leftover from debugging.  Also removed lots of fmt.Prints. Moved to the appropriate place.. This is basically identical to BenchmarkMutex.  Sure, this is an int64, but the performance will be dominated by the mutex in either case.. We have some existing benchmarks of atomics above.  Can you name this BenchmarkAtomicValueStore (and rename BenchmarkAtomicValue to BenchmarkAtomicValueLoad) to match?  Also, the value type used should be the same -- if a time.Time isn't important, I'd rather use an int instead.\n. How about BenchmarkAtomicAddInt64 to line up with the existing benchmark names?. Now I'm curious how atomic.Value scales.  Can you use struct { a, b int64 } (16B) and struct { a, b, c, d int64 } (32B) in 2 benchmarks instead of time.Time?  Unless we're specifically trying to decide whether to store Times as an atomic or behind a lock, it's more useful to know exactly what we're testing.\nI believe what this means is that for the best performance, you should always use a pointer type for your atomic.Values, and make sure the memory behind them is immutable.. Do we need v at all?. This doesn't seem right to me.  I think you want to push b.N into atomicStore/mutexStore and put that loop inside the goroutine.  Otherwise I'm not sure the contention will be as high because a lot more time would be spent spooling up and tearing down goroutines and in waitgroup accouting.. This can be simplified by removing some duplication:\ngo\nfor _, n := range []int{1, 10, 100, 1000, 10000, 100000} {\n  b.Run(fmt.Sprintf(\"Atomic/%v\", n), func(...........\n...\nor\ngo\nfor n := 1; n <= 100000; n *= 10 {\n.... I was going to say that's surprising, but I am honestly never surprised by things like this anymore.  Comment in the code?. Also, knowing more now, it seems like we should just fall back to the more primitive atomic.*Pointer operations always for the best performance.  atomic.Value adds a bunch of runtime type checks that don't seem useful if you know what you're doing.\nIt seems we don't have any benchmarks for that, though -- could you add some like these?\nMaybe AtomicPointerStore, AtomicTimePointerStore, and another b.Run in ValueStoreWithContention?. Did the scaling look different with this change?  Can you include all the new numbers in the PR?. Oh, I meant atomic.StorePointer, not atomic.Store using a pointer type.  :smile:. This makes sense: Store does a LoadPointer, a type check, and then a StorePointer.  So a ~2x difference is exactly what I was expecting to see.. This should be checking whether the error was produced by the status package instead:\ngo\nif _, ok := status.FromError(err); ok {\n  return nil, err\n}\nIt's fine to keep returning streamErrorf(codes.Unauthenticated) below to avoid changing too much at once.\nWe also should update the comments in credentials/credentials.go for GetRequestMetadata to explain that if a status error is returned, it will be used as the status for the RPC.. How about \"are errors compatible with the status package\"?. After looking at this again, I think [][]string would actually be better here.  This would avoid the need to construct a map entirely.\n. Hmm, actually, what do you think about: func CallMetadata(kv ...string) CallOption (whose beforeCall does c.callMetadata = append(c.callMetadata, kv))?\nThis would have the same signature as metadata.Pairs. Like Pairs, the function should panic if len(kv) % 2 != 0.\nThis would allow users that want to attach several pieces of metadata to reuse their own slice specifying them.\n. The CallOption returned by this function should be magical and allow introspection, so interceptors can remove unwanted metadata if necessary.  It should be able to return k and v (or kv []string).. Could we add the metadata into the CallHdr instead to avoiding proliferating new parameters?. I believe you can do: for y := 0; y < j; y++ {} and the compiler won't remove it.. Can we remove one or two of these?  Maybe 10k (or 100k) and 10?. How about if we called it CallMetadataPairs?  That would more closely mirror the metadata.Pairs name, and also help to document its behavior.. Actually, let's leave this functionality out for this PR.  We are discussing options for making this work for all our CallOptions, not just this new one.  We have some ideas floating around, but haven't decided on anything yet...\n. Nit: Let's keep a single !headerDone block, and maybe also a !endStream block as well:\ngo\nif !s.headerDone {\n  if !endStream {\n    // Headers frame is not actually a trailers-only frame.\n    s.recvCompress = state.encoding\n    if len(state.mdata) > 0 {\n      s.header = state.mdata\n...\n...And this led me to find another bug, isHeader should only be set in the if !endStream block...  We can fix that separately.. This one is very subtle, but appending to another slice that may be shared by others can result in big problems.  Consider:\ngo\nctx := metadata.NewOutgoingContext(context.Background(), md)\nctx2  := metadata.AppendToOutgoingContext(ctx, md2)\nctx3  := metadata.AppendToOutgoingContext(ctx, md3)\nThe operation to append to ctx3 could potentially destroy ctx2's md2 entry.  You can see that behavior in this example: https://play.golang.org/p/-nTBJQBzyt__g\nThe fix is to not append to the existing slice.\n. This looks like Join().. This will panic if no MD has been attached yet, I believe.. ~~Hmm, I'm seeing something different; I wonder why?~~\n~~https://play.golang.org/p/F7q5wgSB-Q0~~\nEDIT: Oh, nevermind, I didn't have the , _, so it panicked instead of giving the zero value (and an unchecked ok=false).. > Ah shoot you already got it, sorry haha. I should have refreshed page. Yeah really strange behavior\nIt's a bit odd, but this is a feature of type assertions.  An untyped nil doesn't type-assert to anything, and a type assertion without a \", ok\" will panic if the type assertion fails.. This would be more natural in metadata_test.go since it only benchmarks functionality in that package.\n. This is benchmarking the adding of the initial metadata also, but I don't think that's what you meant to cover?\nI would expect this to be:\ngo\nctx := metadata.NewOutgoingContext(context.Background(), metadata.MD{})\nfor n := 0; n < b.N; n++ {\n  metadata.AppendToOutgoingContext(ctx, \"k1\", \"v1\", ...)\n}\nI also expect this should be no different from the \"WithoutPriorMetadata\" since AppendToOutgoingContext has to do the same things in both cases.. This would be more efficiently written as: newMD := metadata.Pairs() to avoid allocating an extra map.\n. I could go either way.  This is the \"old way\" of doing it, so it's a good idea to compare it to the \"new way\", but I don't think we need to keep it once the PR is submitted, either.. Sorry, I meant in the existing grpc/metadata/metadata_test.go :). This is only referenced once -- just inline it.. You can actually send a smaller payload in a loop and get the same effect.  That way no matter how big the flow control window is, you're guaranteed to get stuck eventually. You don't need a select/case here, just err = <-recvErr. t.Fatalf doesn't actually terminate the test if it's done in a separate goroutine.\nIf you return the error instead, your test will fail anyway because of the other checks.  Just make sure you do a \"defer close(recvErr)\" at the top of this function.. 5s or 10s would be better here to help avoid the possibility of a flake caused by a hiccup in travis.  It should only ever be reached in an error case, so we can be pretty generous.. Nit - locate this with the other rst fields (which are also guarded by the mutex(!))\nAlso, please add a comment.. A little nitty, but to be safe, this and the return below should return a specific new status error that is not ResourceExhausted.. var cancel context.CancelFunc; remove from above.. \"or other resources\"?\nOr something else that implies that the stats handler may not be invoked?. We need a cancel ~line 491 where we call cs.finish() as well.. This mutates kv (example of why this isn't safe even though it seems like it should be - be extra careful with append), which means we need to be clear about this in the documentation.\nEven if we just store it away by appending it directly and dealing with it in the transport after FromOutgoingContextRaw, we should document this.. Let's call this intended for internal gRPC use only.. These diffs can be reverted, I believe... I'd prefer to avoid bare return statements for functions that have return values;  return nil, false makes it more readable.. go\nmds := make([]MD, 0, len(raw.added)+1)\nmds = append(mds, raw.md)\nfor .....\nThis will avoid any reallocations of mds since we know its final size.. How about:\nAlternatively, metadata may be attached to the context using `NewOutgoingContext`. However, this\nreplaces any existing metadata in the context, so care must be taken to preserve the existing\nmetadata if desired.  This is slower than using `AppendToOutgoingContext`. An example of this\nis below:\n?. How about \"Please refer to the documentation of Pairs for a description of kv\"?\n. These test cases don't cover the actual behavior of appending; you could replace this with NewOutgoingContext and the tests would still pass; please append to a context that was created with NewOutgoingContext, and also do more than one append call to a single context to make sure all of that is working as expected.. Code being set to an integer directly should be avoided.  0 is OK, which is not what we want here.\nHowever, can you make the other change recommended in #1378?  I.e. change FromError's final return to:\ngo\nreturn New(codes.Unknown, err.Error()), false\nand then unconditionally return the first value from FromError here.\n. Checking s.Err() isn't necessary here; if there is a non-OK code, other tests should ensure that a status converts to a non-nil error.. Likewise regarding using an integer for a code: because this is Code(0), we can't see that this is OK instead of Unknown, and Unknown is the appropriate result.. No need to check Err() here for the same reasons.. How about:  Otherwise, ok is false and a Status is returned with codes.Unknown and ..... Please remove the Err=nil and similar below. This should be \"FromError(%v) ...\", err, .... This is not right -- incoming metadata and outgoing metadata are separate.  This comment should be on NewOutgoingContext.. I liked the comment, it was just in the wrong place.  Can you put a similar note on the NewOutgoingContext doc string?  I.e. \"Any existing outgoing metadata in the context is overwritten\".. Use maxServerReceiveMsgSize instead?  WithMaxMsgSize is deprecated.. go\ncase a, b:\n  return 1\nor\ngo\ncase a,\n  b:\n  return 1\n(if you like the way that looks better.). As discussed offline, this buffer is not shared between the two directions, there are two separate buffers of 256K each.. This is the only one that needs to change, I believe.  Please revert the other diffs, which are all outside the grpc core library (tests/benchmarks).  Thanks!. Delete this line and the one above; the case statements can be in one switch.. Is it worth testing other partial matches like \"[a]:[b]\" and \"[a]:/[b]\"?. This deserves a comment.  Why does what we were doing before not work (i.e. defer conn1.Close() in the goroutine)?. Can you rename \"server\" to \"lis\", please?  It's confusing since it's a listener and not a grpc server.. Is this necessary?  Why?  It's in the defer.. Comment in the code please?  If I saw this, I'd try to move it into the goroutine as a defer conn1.Close() like it used to be.\nBased on your explanation, though...can we still do this for conn1?\n. Could you make minConnectTimeout a const, and move the var to be in the test only?\n. This is something users implement (their service handler).  So:\n\"are compatible with\" -> \"should be produced by\"\nAlso, please add something like: \"Otherwise, the status code returned by the RPC will be codes.Unknown.\"\n(same for StreamHandler)\nEDIT: added \"by the RPC\". This may clarify (add to godoc string too?):\nThe error returned by this handler determines the status code and message of the RPC.  If the error was produced by the status package, the code and message encoded in the error will be used directly.  Otherwise, the status code will be UNKNOWN and the status message will be the string returned by error's Error method.\n. > All errors returned by UnaryHandler will either be produced by the status package,\n\nor wrapped in a status.Status with codes.Unknown as the status code and err.Error()\nas the status message.\n\nMaybe I wasn't clear on what this interface is for.  A UnaryHandler is something the user implements (it's their server's RPC handler), and we call.  This doc string should tell them what they need to do.  If they return another error type, that's fine, but we'll turn their error into an UNKNOWN status code for the RPC when we send the trailers back to the client.  This does not describe types of things we will return from the function.  (Your version seems to be describing something we are in control of.)  How about this slight modification:\nAll errors returned by a UnaryHandler implementation should be produced by the status package, or else gRPC will use codes.Unknown as the status code and err.Error() as the status message of the RPC.\nMaybe an additional sentence like \"a nil error indicates success\", even though it's obvious, since the above may lead someone to think they always have to return some error, even on successes, or else they'll get an UNKNOWN error code.  Or reword the first sentence to \"If a UnaryHandler returns an error, it should be produced by ....\". (?) . Is there a spec for the interop binaries and their flags?  Or are the flags language-specific?\nIf the latter, it may be preferable to have a single \"security\" option that could be \"none\", \"tls\", or \"alts\", to avoid the illegal combinations of flags messiness.\nIf this needs to work this way, then LGTM.. After talking offline this version LGTM -- it's not worth it due to needing to change the interop framework, which is in another repo.. +@cesarghali\nThis should be a separate PR.. > If WithCompressor is also set, UseCompressor has higher priority.\nThis raises an interesting point... we have some DialOptions that also can alter per-RPC behavior.  Especially WithDefaultCallOptions.\nTo do this right, we should make sure there are CallOption equivalents for every appropriate DialOption, express them in terms of default CallOptions instead, and then pass those defaults along with the per-call CallOptions to the interceptors.. We have some CallOptions that are structs and some that are not.  We should unify these since they are now exported.  The struct gives us a little better extensibility, so unless there's a compelling reason to use non-structs, I would prefer structs.. I'd prefer to export the field vs. add an accessor.  It would be simpler to use, and these CallOption types are passed around by value anyway, so we don't have to worry about the interceptor modifying something inside the application (except the header/trailer/etc data directly).  Would you have any concerns with that?\n. We are holding a.mu here, so looking at only a.trInfo.tr is good.  The only reason to check EnableTracing in the other cases is to avoid taking the lock if tracing is disabled globally.. \"ResourceInfo\"?  I think the code has been changed since this was generated.\n. Better practice would be to check \", ok\" of the type assertion, or do a type switch:\ngo\nswitch info := d.(type) {\n  case *epb.QuotaFailure:\n    log.Printf(\"Error details: %s\", info)\n  }\n  // case ....\n}\n(Interestingly, though, the type switch isn't necessary if you're just logging it.). Perhaps a comment to indicate this should only be consumed after the RPC has completed would be good.  (And on Trailer and Peer). This should not be necessary; your zero-value defaults should all be sane (and I see you do end up with the zero value, so this is true).. Is this always (or nearly always) supposed to be non-empty?  Or is it common to omit it?\nIf it's commonly used, it probably should be left as a parameter.. These also exist in the gzip package; I don't think we want to copy them here.\n. We should use gzip.DefaultCompression here to avoid affecting users already expecting it.. This should never fail, but let's not just blindly ignore err.  Check it and panic, please (since there is no capability to return an error, and we expect that it is impossible).  Same in rpc_util.go.\n. This makes it sound like gRPC has status.Status only because of the details.  All servers must return errors implemented by the status package or else clients will only see unknown status codes.\nWould you mind renaming this file to rpc-status.md or rpc-errors.md and either:\n\nAdd something to the effect of: All service method handlers should return nil or status errors (or be coerced to UNKNOWN); clients will see those ~directly.  Maybe an example of how to use it (status.Error(codes.Whatever, \"description\")) as well.\nLeave a placeholder for us to fill in later.\n\nEither way, \"Error Details\" should ideally be just a sub-section of a bigger \"Errors\" doc.\n. Could you change the docstring to say \"if it was produced from this package or has a method Status() *Status\". No, they will look like literal backticks. I would be fine with single or double quotes here as well, but I would prefer something that visually groups the method name with its return value for clarity.. > Do these options really need to be combined?\nThey all should get passed to the interceptor, which means they all need to be together unless we change the interceptor API.\n. Does this append method have the same performance as\ngo\nret := make([]CallOption, len(o1)+len(o2))\ncopy(ret, o1)\ncopy(ret[len(o1):], o2)\nreturn ret\n?. To optimize further, how about this instead:\ngo\nif len(o1) == 0 {\n  return o2\n} else if len(o2) == 0 {\n  return o1\n}\n. Good to know, thanks for testing!. Is it possible to do this with -o benchserver and avoid renaming the files?. This looks racy to me.  cancel might be called immediately after the timer fires, meaning the subconn would be removed from the ClientConn, and is no longer valid here.\nInstead of cancel, you could use the timer's Stop method directly.  If it returns false, then we need to wait until the function is done somehow (we will need to give up the lock, too), then re-add the connection.. Logging seems fine.  There's essentially nothing we can do here to \"recover\".  The only other option would be Fatal(), but Error() seems more appropriate since it's non-fatal to fail to close a single Conn, and we want to avoid crashing the program as much as possible.  I believe the only normal way Conn.Close would error is if you tried to close the same connection multiple times, anyway, so we should be fine.\n. How about ac.reconnect?  Note that this doesn't access any locals, so it doesn't need to be a lambda.. We no longer need this extra select, because the transport will call onGoAway.. These callbacks need some synchronization.  E.g. if a GOAWAY happens, we want to create a new transport.  After that, a subsequent error on the old transport (which will eventually happen) should not reset the transport again, since that transport is no longer the \"active\" one anyway.\n. @jadekler note this new requirement for the grpc layer to manually close the transport.  This will need to be done by the onError callback in your PR.. Optional, for clarity: addr := addrs[0] and use addr throughout.\nOr even change the function signature to not be a slice since we don't actually need this to be a drop-in balancer.ClientConn replacement.. This should be the global, not the source, to avoid creating a new rand.Rand every time.. I don't think this is necessary.  The source has already been seeded, and you don't want to re-seed the RNG every time it's used.. return bp.stickinessMDKey.Load().(string). drop the \", _\"; it's cleaner.. Optional: maybe \"_test.go\"?. Since the exit status is being checked, I think you can remove the | tee /dev/stderr | (! read).\nEDIT: Same with above?. Please use the \"standard\" deprecated syntax, so static analysis tools will notice it correctly:\n// DecodeKeyValue returns k, v, nil.\n//\n// Deprecated: use k and v directly instead.\n. Please add a comment.. What if we made this WithResolverBuilderOptions(opts resolver.BuilderOption) instead, to be more extensible?\n@menghanl . Remove this TODO and the one below, please.. Could you please revert this for now, too?. This one, too... ...then we will not need these, either.. Nit: could you please change this to \"DisableServiceConfig\" (throughout)?. This should be testpb, not test.. This should be checking len(b) > 0 also, before compressing, in case the marshaller outputs an empty slice or nil.\n. Delete this line and the declaration of b, above, and use := below.. Same re: := usage here.. This fails vet because you don't capture cancel and call it.. Nit: this should be named channelz.proto.. How about TestWithTransportCredentialsTLS, and this one can use DialContext() and a context with a timeout?. Even though err is a named return variable, please don't use bare returns to avoid potential problems.  You can then remove these explicit variable declarations entirely by using := where it and isEmpty are assigned below, which is usually worth doing.\n. Are the \"steps of this test\" still valid after these changes?\nDoes the intent of this test change at all?  (Should this be a new test case instead of a modification to this one?). Are there good reasons to accept a bool here?  If not, I'd prefer to leave it out and make it always disable it (like how WithBlock always makes it a blocking operation).\n. Rewording suggestion:\ngo\n// WithDisableServiceConfig returns a DialOption that causes grpc to ignore any\n// service config provided by the resolver and provides a hint to the resolver\n// to not fetch service configs.\n. The order in Go is typically \"got, want\" -- can you reverse these please?. These guys are global variables, and as such, should have long names.  Rationale:\n\nUsed far away from where defined.\nShadowed by commonly-used locals.. Same comment about names -- for readability, these should be descriptive.. I don't get it...what is idx?. indent please. I prefer:\n\nfor _____; do\nwhich you use below.  (Let's be consistent at the very least.). local return\nreturn=1. return=0?. Should this just be if [ $v != 0 ]; then return=0; fi. return is a bash keyword; let's not use it for a variable name, please.. exit terminates the script; did you want return?. needs better names.  Also should be local (since in a function), as above.. Use [ $# -gt 0 ] instead for consistency with code above or [[ $# > 0 ]] for efficiency/clarity, but non-sh-compliance.\n. prefer:\nif [ $# -eq 0 ]; then   # or [[ $# == 0 ]]\n  echo \"error stuff\"\n  exit 1\nfi\nrs=....\n.... do we need to vet the vals here, or can we pass them to the script and wait for it to fail?. echo \"\" == echo. Consider a refactor:\nparam() {\n  argname=$1\n  shift\n  if [ $# -eq 0 ]; then\n    echo \"$argname not specified\"\n    exit 1\n  fi\n  echo $1 | sed 's/,/ /g'\n}\n...\n  -c)\n    shift\n    cs=$(param \"number of connections\" \"$@\")\n    shift. why a subshell?\nThis is preferred: rs=\"$(echo \"$1\" | sed 's/,/ /g')\". High level suggestion for organization of this (albeit small --but it may grow) doc:\n## Client\nA [ClientConn]...\n## Streams\nWhen using streams, one must take care to avoid calling either...\n## Servers\nEach RPC handler attached to a registered server.... Isn't this that code path?. It mirrors the Java API.  That one returns a status directly, but I thought \"error\" would be more idiomatic for us.. If you're making changes here, would you mind summarizing what I wrote in #1854 regarding how to ensure a stream doesn't leak a goroutine?  Something like:\ngo\n// To ensure resources are not leaked due to the stream returned, one of the following\n// actions must be performed:\n//\n// 1. call Close on the ClientConn,\n// 2. cancel the context provided,\n// 3. call RecvMsg until a non-nil error is returned, or\n// 4. receive a non-nil error from Header or SendMsg besides io.EOF.\n//\n// If none of the above happen, a goroutine and a context will be leaked, and grpc\n// will not call the optionally-configured stats handler with a stats.End message.\n. How about\ngo\n// NewClientStream is a wrapper for ClientConn.NewStream.\n//\n// DEPRECATED: ....\nto avoid the need to duplicate documentation.. Oh yes, I see...parens are overloaded.. Maybe so?\n$ x() { echo \"hi\"; }\n$ y=$(x)\n$ echo $y\nhi\n. Let's delete this from the delayed listener, if it's no longer used.. Why defer the allowClose call?. As discussed, it shouldn't be a problem to include the whole serialized message -- if the stats handler wants the whole thing, it will have it, if not, it can truncate it.  We already are holding it in memory.. You can (re)move these shifts:\nwhile ... do\n  case \"$1\" in\n    -r)\n      set_param \"number of rpcs\" 0 $2\n      rpcs=...\n      ;;\n...\n  esac\n  shift; shift\ndone. Is it possible to put this into a .sh file and run it instead?\nAlso, you can put the go:generate into an existing file, that should be fine.\n. I think this should also rm the proto files and remove the directory, or else you'll leave things behind in the user's git workspace.\nMaybe download the files to mktemp directory instead?. Please add a blank line above this to set it off from the above fields.. \"headerSent\"?. \"Unused on server-side\"?. Nit: maybe \"appendHeaderFieldsFromMD\"?  This modifies the slice and returns it just like append does.. That sounds like a warning to me, what do you think?. \"Unidirectional\" is ambiguous.  I think you want something like \"For client streaming RPCs, clients should call ...\". \"Bidirectional or server-streaming clients\"?. I was asking, and now requesting you to add a comment like that since you confirmed that it's true.. Why not \"For client-streaming RPCs...\"?  We have historically called the two unidirectional streaming RPCs \"client-streaming\" and \"server-streaming\".. Another idea, to avoid the need for this boolean:\nRemove lb.readServerList() from this function and call it where this function is called from.  Then do:\nif err := callRemoteBalancer(); err != nil {\n  // Do not backoff\n  continue\n}\nif err := lb.readServerList(); err != nil {\n  // Backoff\n  ...\n}. Needs package comment. Does this belong in the package comment instead?. Instead of reusing an existing field and special-casing the absence of that field for this purpose, it would be less error-prone to have an explicit signal that this is a header that came from the client.  Either a new type and handler for it, or another bool/enum to signify this would be preferable IMO.\n. The following fields are exported.  :)\nShould the TODO go back to where it was before?. This still doubles the amount of time it takes to run the tests, because it does them in series for 1.9.\n. ....instead, let's add this to the \"matrix\" section above so we get a new instance for running only the appengine tests.. mktemp -d is the preferred way to create a temporary directory.. It could be a bad idea to put this into your environment.  Since you are sourcing this file, it will affect everything from here on.\nYou could do things in a single chained command instead:\nmkdir \\\n&& curl \\\n&& unzip \\\n&& export. There are no pipes in this file :)  Same comment as above, though.. ... make testappengine || exit 1; exit 0; fi\nto avoid running all the regular tests again.. Zero seems like a pretty good \"disable\" value to me..  It seems the problem is that we don't have a \"default{Dial,Server}Options\", but we do for \"defaultCallOptions\", right?  Could you look into adding some non-zero defaults that way if it's not too hard?. Should we also do the same thing with the write buffers while we're at it?. Is it possible to use defaultWriteBufSize instead (and move the const into the grpc package instead of transport)?\n. Updated error and added comment. PTAL. This or the stream := stream is needed to avoid the iterator reassignment problem.  I'm OK either way, but please remove one.. Remove msg please.. Delete if not needed.. Please document the default value here (and also for read).. Interestingly, SendMsg can return an rpc status error as well.  This happens if SendMsg is called inappropriately.  This also kills the stream and RecvMsg will eventually have the same status, unless the server had already sent trailers with a different status.\nI wish this was simpler.. How about this:\n```\nIn the event of a connection or stream error:\n- Client: io.EOF is returned and the RPC status can be retrieved by repeatedly\n  calling RecvMsg.\n- Server: the error is returned directly\nIn the event of an error caused by the application (e.g. the message size is over\nthe configured limit), an RPC status error is returned to the user.\nOn any error, the stream is aborted.  On the server-side, the service handler\nshould cancel any outstanding work and return as soon as possible.\n```\nAnother idea, since the semantics are unfortunately different between client and server, despite having the same interface: let's document all of Stream's behavior in ClientStream and ServerStream and leave comments in Stream indicating that the user should find the documentation there instead.  Stream probably isn't directly used anyway, so maybe this would be best?\n. This uses and encourages an anti-pattern for Go testing, which is passing testing.Ts around (which is particularly discouraged when crossing package boundaries and storing in structs AIUI).\nFurther, I'm not convinced this package is worth the extra API surface it introduces.  It is easy to start and stop a server/client with grpc.  The main complexity is the Listen-on-localhost problem (which really should be solved in the net package itself, but that's irrelevant here).  In the not-too-distant future (~1Q or so), we should be introducing an in-process transport that will be even easier to serve/dial.\n. Nit: \"Deprecated\" (case) - it actually matters to tools like staticcheck.\n. Nit: \"from ClientStream methods\"\n(and ServerStream). \"ClientStream defines the client-side behavior of a streaming RPC.\"?. \"On error, SendMsg aborts the stream.  If the error was generated by the client, the status is returned directly; otherwise, io.EOF is returned and the status of the stream may be discovered using RecvMsg.\"\n. How about:  \"- There is sufficient flow control to schedule m with the transport, or\"\n...or something to indicate that we aren't just letting people queue up a billion messages.. There are no message-acks in grpc.  I'd rather not even say this.  How about:\n\"SendMsg does not wait until the message is received by the server.\"\n. This is painfully explicit IMO.  How about:\n\"SendMsg does not wait until the message is received by the server.  An untimely stream closure may result in lost messages.  To ensure delivery, users should ensure the RPC completed successfully using RecvMsg.\"\nThe idea would be to refer people to RecvMsg and make sure we document it well enough there.. \"to call SendMsg or CloseSend\". Let's remove the client/server language:\nRecvMsg blocks until it receives a message into m or the stream is done.  It returns io.EOF when the stream completes successfully. On any other error, it the stream is aborted and the error contains the RPC status.\nIt is safe to have a goroutine calling SendMsg and another goroutine calling RecvMsg on the same stream at the same time, but it is not safe to call RecvMsg on the same stream in different goroutines.. Same comment as above.. Please remove client/server language here.. You deleted SendMsg by mistake.. \"Maximum\"?. The \"if\" here is unnecessary and is worse for performance (CPUs hate conditional branches far more than stores).. Let's not change this just yet...generated code still uses this (so vet fails).. Sorry...can you add this, too (only for ClientStream):\nIt should not be called until after Header or RecvMsg has returned.  Once called, subsequent client-side retries are disabled.\n. s/server/client/. The server can never know whether the client received the message without something in the user's protocol.\n. On the server, io.EOF should not be returned.. The \"or CloseSend\" only applies to the client, not the server.  The server doesn't have a CloseSend.  Sorry if I put my comment in the wrong place earlier.. SGTM. wait...did I write that?\nRecvMsg does return io.EOF, but it's when the client does CloseSend, not when the stream completes.  Sorry.. Will you update this?  The bit about \"ensure the RPC completed successfully using RecvMsg\" is misleading/wrong on the server-side.. \"It returns io.EOF when the client has performed a CloseSend.\"?\n. Ping on this comment... > I'm not sure it's worth saying, but is it correct to say that once the server returns in the handler, all messages will have been sent successfully? \nNo, the server never knows whether a message is received by the client unless the user builds their own acks into their protocol.  I think what you were proposing before was fine: \"SendMsg does not wait until the message is received by the client.\". Let's leave it this way for now, since the RST_STREAM and code are still correct and potentially meaningful.. I have some nits about this comment.  How about:\n// If WithFallbackServiceConfig is used in combination with this DialOption, the fallback\n// service config will be used.\n. s/starts/starting/. s/resolver/the resolver/. Can we make this pass fallbackServiceConfig, and do the \"disable\" functionality in resolver_conn_wrapper.go instead?  That would avoid the need for the magic value behavior for \nthe empty string.\nNote the subtle bug that can happen due to the magic value:\n- Fallback service config is specified\n- Resolver service config is NOT disabled\n- Resolver returns empty string for service config\n- Instead of applying an empty service config, we re-apply the fallback\nAlso note there is a behavior change: an empty service config will no longer clear the service config; it will be ignored instead.. Can we do a blocking dial and know that it has been applied before Dial returns?. Same. This will lead to flaky tests.\nInstead, we should use a polling loop that sleeps for a few ms between iterations, and times out after something like 10s.\nAlso, we should confirm that WaitForReady was true before applying the new service config.. s/true/false/. This is hard... how can you confirm that an asynchronous process will never occur?  1s may not be enough, but I don't want to sleep forever, either.\nCan we inject the service config synchronously?  This would help all our other tests that have to do polling loops to wait until the service config is installed.\nI think this would work:\n\ninternal/internal.go: define var TestingServiceConfigHandled = func() {}\nclientconn.go: handleServiceConfig does defer internal.TestingServiceConfigHandled()\nTests reassign internal.TestingServiceConfig to something like func() { close(done) } and then we know the config is applied after we do <-done.  Let's wrap that into a helper function since we have so many tests that update the service config and then poll until it's applied.. Yes, you're right.  The variable is not actually the frequency, it's the interval, hence my confusion.\n\nI also think we should rename the variable to something to indicate it's a limit now, e.g. \"maxDelay\" or \"minFreq\".. Please pull this up with the other stdlib imports. Just a note: things related to address selection and possibly backoff might be nice to pull out into their own type.  That could be done in a future change.. This is now unused, right?. Is it possible to get an onPrefaceReceipt call from a transport we have orphaned?  Or must it always be the active transport?  Note there's a similar concern with the onGoAway/onClosed callbacks as well.  I think we could fix this pretty easily with a check like if ac.transport == transport { reset } else { /* someone else did */ return }. will close itself?  or will be closed?. nit: \"that the resolver returned\".  Or more accurately: \"that the balancer assigned to the addrConn\".. i think you mean to say: upon the next error, the list is re-resolved and .... Why re-resolve on success?  We should only re-resolve when the connection dies.. We don't otherwise use this error, so minimize its scope.  Also, it should not be \"error\" severity per our defined guidelines, and the message should be annotated.\ngo\nif err := ac.nextAddr(); err != nil {\n  grpclog.Warningf(\"resetTransport: error from nextAddr: %v\", err)\n}\nOr even, just log it in nextAddr and don't return an error.  Why aren't we doing something here, though, like waiting for the re-resolve above to finish?. Maybe do this at the top before doing the resolveNow?. Why do we need the backoffDeadline passed here?  Why doesn't the transport use the connectCtx's deadline instead?  I think it's not correct to use the backoff deadline except when failures occur and we need to wait before the next iteration.\nI think the transport should be using the connect deadline only, and it can read that out of the connectCtx when necessary.. This is only used inside this one function, and can be removed.. Does this need to be passed into the transport?  It seems like you could avoid it by doing (below):\nif ac.dopts.waitForHandshake {\n        timer := time.NewTimer(connectDeadline.Sub(time.Now()))\n        select {\n        case <-timer.C: // instead of <-timedOutWaitingForPreface:\n            return errReadTimedOut\n        case <-prefaceReceived:\n            timer.Stop()\n        case <-ac.ctx.Done():\n            timer.Stop()\n            return errConnClosing\n        }\n    }. I was suggesting to move the block (so, instead?).  Is there a reason it also needs to be down here?. const?. Set some kind of deadline here?. Not strictly true, since it could return false if it's called asynchronously immediately after Fire has been called.  Documentation is hard.  :/. Nit for clarity: \"not move up addrIdx by 1\" -> \"reset addrIdx\"?  (Is that correct?). We should typically only re-resolve on a failure, so I would expect to see \"false\" here.. I'm not sure I'm following.  If the handshake fails, it is not an \"error\" per our defined guidelines of an error (https://github.com/grpc/grpc-go/blob/master/Documentation/log_levels.md).  Also, please keep err within the if statement's scope.. To avoid this mutex, you could use a different channel for when onClose was called, and block on prefaceRecevied or closed when waiting on the preface timer.\nAlso, I think the way it is now, it's possible to get a double-close.  If onClose and onPrefaceReceipt are both called at ~the same time and onClose wins the lock, then you'll double-close prefaceReceived.  Probably best to just have the second channel.. Optional: I think you could share 1 channel for both timedOutWaitingForHandshake and shutdownOccurred.. Why do we release the lock around GracefulClose now?  (Comment?). if t.Close() is called before we assign t.onClose, this will panic.  Initialize onClose above to func(){} or check for nil here.\nAside: I'm surprised travis is green. (!). Done. Done. It doesn't really matter in such a small package, but in general: I don't like global variables that are a single letter.  Also it's only an intermediate involved in setting another global.  We could do something like this instead to hide it:\n```go\nfunc caller() string { , b, , _ = runtime.Caller(0); return b }\nvar basepath = filepath.Dir(caller())\n```\nor\ngo\nvar basepath string\nfunc init() { // set basepath }. Yes, I could do that here instead of line 1051 above, which would mean we will reset the backoff deadlines even if we aren't currently backing off.  Done.. This fix is helpful for anyone using the 1.11 RC.  But not part of this PR.  Removed.. Both!. Please make this timeout eventually so the test won't hang forever if it breaks.  E.g.\ngo\nctx := context.WithTimeout(context.Background(), 10*time.Second). :man_shrugging: :disappointed: \nI'm fixing the ones I see when I run into them.. Technically, yes it's not necessary.  But it looks weird to a C programmer without it.  Removed anyway, this is Go.. How about fatal?. Please delete this line.  It's impossible for users to reference this now.. This is no longer necessary since it's in internal.. This will need to be done before handshaking ~line 170, otherwise you won't have a TCPConn anymore.\nAlso, you need to only do this for TCP connections.  The dialer could return a net.Conn backed by a UDS instead.. This is only ever used on line 1020 as a check -- is something broken or do we not need this at all?. Please don't shadow \"t\" in a test; this confused me.. This should go up with std lib imports.. This should stay \", 1\" and @menghanl's change to make this test more permissive still LGTM.  The new test you added will help us catch the race for regression purposes.. What was the error rate before fixing the bug?  I was expecting close to 100%.. Ah right -- otherwise it would be 100%.. This is unnecessary.  It's automatically initialized to its zero value.. You can remove this line now.. Nits: \"shouldn't\" -> \"may not\" (more precise/formal) and s/together// (it's cleaner).. Can this be moved to internal/?. It is stateless -- or at least, there is state, but it is immutable.\nMaybe a different name would be better.  BundleWithMode or BundleFor or NewBundle or NewWithMode?. Why not move the contents of new here?. IIUC, this should be fine instead: var vmOnGCP = alts.IsRunningOnGCP() (no init function). I don't think this defensive programming is necessary.  I don't believe it is possible to get a nil *creds from this package. (?). I didn't expect GRPCLB to need to be aware of ALTS at all.  Can this be named something else, like CredsBundleModeGRPCLBBackend?. I believe this could be moved inside credentials/google.go, since it isn't referenced elsewhere?\nIn general, I want to keep grpc and grpclb talking only in terms of concepts and not specific technologies (i.e. \"balancer\" or \"backend\" not \"tls\" or \"alts\").  (This is related to my other comment about CredsBundleModeALTS.). WithCreds->WithCredentialsBundle. Please file an issue and reference it here.. Done. \"If set to false\" sounds to me like the default might be \"true\" (even though this is Go and we know that's not possible).  What was wrong with the old wording?. expects -> allows. Nit: there's -> there are (\"no active streams\" is plural).\n(There's another instance of this above in the client-side PermitWithoutStream.)\n. \"runs keepalive checks\" -> \"sends keepalive pings\"?. This doesn't seem like the right way to do this..  We consult the Backoff() at the start of an iteration, and it's just a coincidence that we're setting the state to Connecting after calling Backoff().  If those were flipped, then you would probably miss the TransientFailure state, or at least it would be a race.\nI was picturing a backoff time of , and the signal to unblock would be a call to cc.ResetConnectBackoff(). Please use cc.WaitForStateChange or else this will be racy and/or sleep unnecessarily.. Nit: no need to name the return value.. return ss.SendMsg(reply). Can be simplified:\nreturn sd.Handler(server, ss)\ndelete local var err, remove \"else\", etc. Nit: this is a listener not a server, so s/server/lis/.  (It's helpful to follow a convention things like this.). How about something longer, like 10s, to avoid flakes in case Travis is feeling sleepy that day?. This is the pattern that avoids errors like this in the future:\ngo\nif got := ......; got != \"Subchannel Deleted\" {\n  return false, fmt.Errorf(..... , got)\n}\n. \"connect starts creating a transport\"?. The problem with sed...  ;). again, over-aggressive search&replace.. got?. I don't think this line is necessary / may be racy.\nWe should be in Connecting - or already in TransientFailure caused by the server closing the connection.  If we hit the latter case, this will block until ctx expires.. As discussed, I believe this should only be done in the allowedToReset case below.  Otherwise the state will go:\nConnecting -> Transient Failure -> Connecting -> Transient Failure -> ...\nfor any addrConn with multiple addresses as each dial attempt within the list fails.  We should stay in Connecting until we get to the very end of the list or succeeds.\nI think we also should only be doing it if we received the preface - unless we are currently trying the last address in the list.\n. This comment is backwards -- we are at the end here.. Is this the only time we'll set TransientFailure?  We want to make sure we set it whenever the current state is Ready and a connection error occurs, which I believe this will not do.\n. We should probably be checking the return value of all of these calls, unfortunately.  Maybe a helper function that you also pass t to to avoid making the code full of if !WFSC { Fatal }s?  It would be a good chance to try out T.Helper() (which I only learned about recently, at least).. > Is there a definitive flow diagram somewhere I can consult? I've looked around in grpc/grpc-go/Documentation and grpc/grpc/doc and didn't see anything at first glance.\nThere is a table of valid transitions for the channel (ClientConn) (but not a sub-channel (addrConn)) in grpc/doc, but not a full flow control diagram.  In fact, this particular situation doesn't happen in the other languages.  I have a meeting set up for later this week to discuss it with the other language leads.\nWhat you're suggesting behavior-wise seems to make sense.\nThat said, if what you're doing now is the same as what we were doing before this change, then I think we should leave it so this can be purely a bug fix.\n. Can this be an unbuffered struct that is closed instead?. \\\"%s\\\" --> %q. Please make \"state_recoding_balancer\" a const to re-use below, and you can eliminate the name field in the struct since you never set it to anything else.. In the end2end tests, we have a fake balancer that accomplishes the same thing by making the Balancer also implement Name() and Build().  I.e. this type isn't necessary.  Build() would just return itself (after setting underlyingBalancer).. Optional: if you embed underlyingBalancer instead, this will happen automagically.. Please name the test cases (add a name field to the struct) and print the name instead of the #.  It can be very helpful when debugging failures.. Comment here why we need this check, please.  And please explain why we don't check cc.balancerWrapper instead?. How often will this be invoked?  We should wrap it in a sync.Once if it's frequent enough.. \"Currently, proxy support is implemented in the default dialer.\". \"HTTP CONNECT proxies are supported by default...\". \"by the environment variables\". Should we list \"server shutdown\" specifically here?. Nit: there are any transport errors.. Why not 5s like the others?  2s does seem a bit short in case Travis is having a bad day.  (Another 2s is below.). \"are supportED.\". FWIW if s.updates[service] == nil would also be fine here.  (I don't think either is strongly preferred.). I don't think you need this select / default.  You can be sure the channel is empty now.. I guess this can happen if a client is not reading from its end and the server gets flow-control limited?  Otherwise it shouldn't be necessary IIUC.  Maybe add a comment?\n. Note that a single context should be fine to share across all the streams.. Note that a single client should be fine -- having many clients wrapped around the same ClientConn is equivalent to just having one client.. Could you split this into 2-3 different test functions for the various different things that are being tested by this one?  E.g. TestHealthWatch_DefaultServiceStartsServing, TestHealthWatch_UnknownServiceStartsUnknown, TestHealthWatch_MultipleWatchers?. There's no need to create a second context here; it's fine to use the first one twice.. const service1 = \"grpc.health.v1.Health1\"\n(and in other tests). Could you please name the parameters for documentation purposes?  E.g.\ngo\nfunc(ctx context.Context, newStream func() (interface{}, error), reportHealth func(bool), serviceName string) error. Please keep a blank line between functions.. \"should never happen\" sounds like an internal assertion? I think error is fine in that case.. Code golf:\ngo\nfor err := nil; err == nil; _, err = conn.Read(buf) {}. You should only need to make the listener once, right (i.e. outside the loop)?\nYou also never call lis.Accept(), which seems like it should be necessary.  You would also need to read from the conn on the server side, because the client writes its 24-byte preface before returning (we think this is the cause of a separate test flake we're having).. That works.  Or err := error(nil) would work to avoid the repeated Read invocation.. Instead of the extra synchronization and two new locals, could we split ac.successfulHandshake in two, one for client and one for server, and where it's checked in nextAddr, we check for the two &&ed together instead?  Would that be simpler possibly?. \"transportMonitor goroutine\"?  What's that?. Does this need to be exported?. pl.Addr().String() -> \"\".  The address is irrelevant when using this dialer.. \"... It should only be created using NewPipeListener\". Why was this chan added?  There are some problems (below) about determinism now that a second channel is involved.\nIf the issue is that the dialer panics once the listener is closed...I think that's OK - we just have to make sure we close the client before closing the listener, and without any dials in flight.  If we need this to work we should maybe talk about a different design.  We may be able to just add in a lock.. This is not guaranteed to happen first, making this non-deterministic once Close has been called.\n. I don't think we should need to worry about failing after we've gotten a signal that a dialer wants a connection.  It's a quick cycle in and out of Accept.. How about: lock, close p.c, unlock.  In Dial: lock, check that p.c is open, make a new chan and push onto p.c, unlock?  I think that avoids any potential races.. Please unexport C.. Would you mind adding some basic sanity tests that this works?  Accept, dial, write bytes on each side, read bytes from each side, close & observe errors?\nThanks!!. const want = \"hello world\". Maybe a short timer (100ms?) here instead?  Similar for the other tests.. I see a pattern in a bunch of these tests.\nHow about something like this:\n```go\nfunc testBlocksUntil(t *testing.T, blockingFunc, unblockFunc func()) {\n  unblocked := make(chan struct{})\n  go func() {\n    blockingFunc()\n    close(unblocked)\n  }\n  select {\n    case <-unblocked: //fail\n    case shortTimer: //pass\n  }\n  unblockFunc()\n  select {\n    case <-unblocked: //pass\n    case longerTimer: //fail\n  }\n}\nfunc TestCloseUnblocksAccept(t *testing.T) {\n  pl := NewPipeListener()\n  testBlocksUntil(t, func() { pl.Accept() }, func() { pl.Close() })\n}\n```\n(Or the same pattern in a table-driven test?)\nMay or may not be worth it.. Would this work instead? <-stream.Context().Done() ?. s2.Stop() will take care of this.  No need to do it separately.. s1.Stop() will take care of this.  No need to do it separately.. Avoid infinite delays:\ncontext.WithTimeout(context.Background(), 20*time.Second)?. *FullDuplexCall. You can omit the type here. Nit: maybe omit \"new\"?  This isn't a constructor.. Can this boolean be removed and just check retryCnt > 0?. This can be changed to:\ngo\nif ctx.Err() != nil { return nil }\nThe library has been changed to remove the defer that made Err slower than the select on Done.  (This isn't performance critical anyway.). What will happen with these errors when they're sent back?  Is it logged by grpc?  (This one should be logged somewhere as grpclog.Error.). Nit: it's more common to make this a pointer type itself: &healthpb.HealthCheckRequext{....  You could elide the local entirely, too, since it's only used in the call to SendMsg.. You can move this outside the for to avoid re-allocating.. status.Code(err) == codes.Unimplemented is totally fine instead since the default code is Unknown.. Simplify: reportHealth(resp.Status == healthpb.HealthCheckResponse_SERVING). With some minor restructuring, you can test multiple backoffs without incurring the sleep times or worrying about the imprecision of time.\nI.e.:\ngo\nvar backoffFunc = func(ctx context.Context(), d time.Duration) bool {\n    timer := time.NewTimer(d)\n    select {\n    case <-timer.C:\n        return true\n    case <-ctx.Done():\n        timer.Stop()\n        return false\n    }\n}\nNow your test can override this variable with a function that captures the delay without sleeping.. How about:\ngo\nvar backoffTimes []time.Duration\nbackoffFunc = func(...) bool {\n  backoffTimes = append(backoffTimes, d)\n}\nnewStream := func() ... {\n  if len(backoffTimes) < 5 /* ? e.g. */ {\n     return nil, errors.New(\"backoff\")\n  }\n  return nil, nil\n}\n. All errors should begin with a lowercase letter.  (This is just a test so not that important, but best to get into the habit anyway.). To restore this:\ngo\ndefer func() { backoffFunc = oldBackoffFunc }()\nThis makes it impossible to forget or have a code path that skips the restoration.. We need to check that len(backoffDurations) is what we expect first.  Otherwise if it's less than expected, the test still passes.\nAnother option would be to just build the slice and compare:\n```go\nwant := []time.Duration{bo.Backoff(0).NanoSeconds(), ...}\nif !reflect.DeepEqual(backoffDurations, want) ...\n```\nAlso consider hard-coding the backoff values here if we don't anticipate changing them.. In Go, the standardized terms used for this are \"got\" and \"want\".. We shouldn't need this fudge factor anymore, right?. Is this test still useful?. It shouldn't be an \"error\" level log when the server has health checks disabled.  Probably just \"info\" or maybe \"warning\".\nThis one, on the other hand, should be logged as an error, since it's an internal invariant that should never be broken.  Let's log it separately here.\nAs for what we log, please include the type that we did see (even though this is literally impossible so it will never happen):\ngo\ngrpclog.Errorf(\"newStream returned %v (type %T); want grpc.ClientStream\", rawS, rawS)\n. Actually after discussing offline, I agree with the other logs being \"error\": since the user has gone through great troubles to turn on health checking (importing this package, setting up service config, and using a balancer that supports it), then if it doesn't work, that could indicate \"an error in their usage of grpc\" (one criterion for the error log level).\nPlease set the channel as healthy when this unexpected event happens, too, though.. Add a space after \"//\", please.. I was expecting to see this in balancer/roundrobin.  Doing this here will affect our users' balancers that are built on top of this package, which I think we should avoid.. This appears redundant with scopts - can we remove it?. Go style: remove the \"get\" for simple accessors; healthCheckConfig is cleaner.. Nit: \"not longer\" -> \"no longer\".  Also, please put spaces before parenthesis.. This comment is no longer relevant because you aren't calling newTr.Close().  It should be moved up above where you are calling newTr.Close().\n. Do you mean to be holding the mutex through ac.newClientStream?. An addrConnStream won't have a balancer, so this can be eliminated.. As discussed, I believe we can close allowedToReset here and not pass either it or skipReset to startHealthCheck.  Please investigate.\n. Did you see problems calling context concurrent with a Send?  I wouldn't expect any.  Did you mean to put this on CloseSend?. There are two parts to this package.  A service implementation and the client-side health checks.  The service can be implemented with a different proto library (e.g. gogoproto) but not a different IDL.  The client can't be implemented by the user because it needs internal.  I'd leave that stuff out entirely, personally.\nBut please mention that there's a service here that users can use to expose server health.  And mention that this package must be imported to enable support for client-side health checks, if configured in the service config.\n. I think just \"Config\" should be fine for this.  This package shouldn't have anything else we want to configure.. This whole package is marked as experimental, so this should not be necessary.. Please change NewBalancerBuilder to return NewBalancerBuilderWithConfig(name, pb, Config{}) to avoid duplication and the need for future changes in both places.. Go prefers to avoid functions that spawn goroutines and return.  Instead, the function should block and we should call it in a goroutine.  I.e.  go ac.watchHealth(hcCtx, ...) or something.. Space after \"//\" and start with a capital letter please.. Can this block of code be moved into ac.newClientStream to minimize what's in the closure here?. The health check function is not allowed to call reportHealth(true) multiple times in a row?. You can use status.Code(err) == codes.Unimplemented to simplify.. As discussed offline, please add an error log if internal.HealthCheckFunc == nil and the other things would enable it.. Don't we need to go \"ready\" at this point?  Or are we relying upon HealthCheckFunc to do that?. Why do this in done instead of in addrConnStream.finish directly?. This is a pattern we follow in the test server in end2end test, but I'd rather avoid.  Could we make Watch be implemented by a func field in testHealthServer and put the actual implementation details in the tests that rely upon them instead?\nWe could initialize with the \"foo\" behavior as the default and overwrite when needed, since most tests seem to want the \"foo\" behavior.. Formatting got messed up here.. Include err please.. Do you need to cancel and create new contexts?  It would be simpler to use the same one for the whole test.. Please defer the cancel up where it's created instead since the timing of the cancellation isn't important.. tron->torn. Maybe I'm lost, but this comment you just added is on the Context method, not CloseSend or SendMsg.. I think it makes sense to keep it, but put it on CloseSend instead.  I'm pretty sure that was what you meant to do and just missed it by a couple lines.. Go style thing:\nPrefer:\ngo\nif x {\n   return\n}\nblah\nOver:\ngo\nif !x {\n  blah\n}\n<end function>\nThis makes it more clear that what happened was a nop and nothing else in the function needs to happen.  It also lets you outdent your code which looks nicer.\nYou can defer s.mu.Unlock() to make this style a little simpler.. Please document this field.. It might be a good idea to refactor to remove the duplicate terms:\ngo\nif !ac.cc.dopts.disableHealthCheck && healthCheckConfig != nil && ac.scopts.HealthCheckEnabled {\n  if internal.HealthCheckFunc != nil {\n    go ac.startHealthCheck(...)\n  } else {\n    grpclog.Error(...)\n  }\n}. Use Error instead of Errorf unless you're using args (you can run into problems if you have a literal \"%\" in your output).. Remove this \"go\" since this now should block.. s/special//\nOr delete the non-parameterized form and just let the initializers explicitly set \"defaultWatchFunc\".. Similar concept as in the balancer builder builder: delete this and call the other constructor with defaultWatchFunc as a parameter.. %v, err ;). Ping?. This a long error message.   We should try to keep them as close to 80 columns as possible.  Use multiple Error calls if necessary.. Formatting here as well. Ping on this and the defer cancel() comment.. This is still on Context().  Move to CloseSend() please.. Nit: s/server/lis/g in this whole test.. I think in the original idea for the test, this should not be done in a goroutine.  Otherwise you just immediately close the conn from the defer above, and the connection will probably die before the client even considers it READY.\nIf the real goal of the test is to make sure the client dials the second address in the list after the first one failed pre-preface, then I don't see any reason we need to mess with the framer.\n. firstResolveEvent?. \"Basic \" + basicAuth(u, p) ?. We still do the go generate check based on that, so we do still need it.. Yes, I'll revert it.  We should really have these hooked up with go generate too.. Remove these vestigial parens?. Token package comment?. wrapper->wrap. can this be an embedded syscall.Conn instead, and then we don't even need to implement the method manually?. \"OR Go1.11\"?  With the other being \"AND !Go1.11\"?. Actually, let's save this for a later PR.. If you embed the syscall.Conn instead of naming the field, you won't need to define the method at all.. Does this mean we'll be doing a re-resolve upon the first connection attempt?  That is not correct - we only want to re-resolve when a connection failure happens.. I think this is something we should compute at the top of the loop.  You are computing the backoff at the bottom instead, but that means you're not factoring it in when updating dialDuration @989 - that's using the previous iteration's backoff.\nAdditionally, the backoff is not the amount of time we should sleep between connection loop iterations.  It is the minimum delay between the start of this iteration and the start of the next iteration.  Meaning if we spent 10 seconds dialing and we had a 1 second backoff, we should start dialing again immediately (1 second has already elapsed).\nhttps://github.com/grpc/grpc/blob/master/doc/connection-backoff.md. We're immediately taking the write lock after this.  Let's just hoist that up and not take the read lock at all.. Does this need to be done inside the lock?  Maybe @menghanl can comment?. go\nif tryNextAddrFromStart.HasFired() {\n  ...\n}. Let's do away with the need for this entirely!  I believe if we make sure we update ac.curAddr to its zero value whenever we are not READY, then we can't update the list of addresses while the reconnect loop is running.  This would mean we potentially recreate a new addrConn when we don't absolutely have to, but it simplifies things a lot, and it would be a pretty rare condition anyway.\nI think it's time to fix updateConnectivityState so that it:\n\nchecks if the current state is already the one being requested,\ncalls handleSubConnStateChange,\nlogs the state change, and\nsets/clears ac.transport and ac.curAddr as appropriate.\n. Pretty sure this is always true - it can't go negative, right?. I don't think you need this local if you increment backoffIdx when the timer fires, above.. Might not need to be if we check ac.resetBackoff in the retry loop - but we can look at doing that in a different PR.. This delay is to allow time for the client to mess up and expose a bug.  It's not necessary for correctness, but it is necessary for effectiveness.. I made the connect timeout 250ms and dropped it to 1s instead.  In most cases, if there some kind of bug that considers the connection ready in error, this should be enough to catch it.\n. A pattern you can use for this is:\ngo\ndefer close(hcExitChan)\nreturn testHealthCheckFunc(ctx, newStream, update, service). This already defers leakcheck, above.. My context is long gone, but I remember feeling that this covered up the problem and didn't actually fix it.  Let's discuss offline.. Double-leakcheck.\n\nThese tests all look very similar in setup at least.  Can we consolidate somehow?  (Table-driven or common setup/check functions.). minConnectTimeout probably should have been a field in ClientConn with a non-exported DialOption to set it.  That would avoid these global-state-change things which also caused races.  I'll try to do this as a cleanup change later.  (This is just a note to myself.). Yes, KeepaliveParams is shared between acs, so you do need cc's RLock to read it.\n. Ping. Ping. Ping. I think that's right...but here, where we're updating the addrConn's addresses, curAddr would be nil and so we would never find it, causing us to tear down the ac and create a new one.  So we don't need to worry about this tryNextAddrFromStart at all (it can't happen).\nI think the health check is the only special case that doesn't need to set transport/curAddr.  But it could still pass them in as their existing values to make sure they don't change.  The idea behind this would be to make sure we always set these things together that we know are so closely related.. I think this line can be deleted now, and use := @979?. \"...can share a Server.\"\nOr grpc.ClientConn and grpc.Server?. Add something like: \"This example illustrates how to perform both types of sharing.\". Something like:\n\"This example shows how to set and read metadata in RPC headers and trailers.  Please see Documentation/grpc-metadata.md for more information.\". \"grpc.examples.features.metadata.helloworld\"?. rename directory to \"client\"?. Comment: \"// Read metadata from server's header.\"?. Do we need examples for all the various stream types?  Setting/reading metadata should be the same for all of them, right?. Please put everything into the main package to keep it as simple as possible for someone reading the code.. Comment please - e.g.: \"// Create a context with a one second deadline for this RPC.\". Please add an example with a streaming RPC.. Clients should not be connecting in the RPC path.  The server should create a client (to itself) when it starts up.. Add this newline?. Needs copyright.. grpc.examples.features.proto.echo ?\ngrpc.examples.echo ?. servicers->services. To be consistent this should accept a string parameter or the other function should accept the request message.. No \"helloworld_metadata_\" anymore.. Ditto. @menghanl this is a bug, right?  We should return an error regardless of failfast, shouldn't we?  Otherwise, we'll be trying to use sc (which will be nil) in the lookup below, and then eventually returning it from the function with no error.. But we will also get a re-pick if we return the transient failure error immediately.  Technically this works as-is, but if an error happens we should be returning an error, not a nil sc with no error.\nReturning a nil sc will also result in the info log (which should probably be a warning) \"subconn returned from pick is not *acBalancerWrapper\", which shouldn't normally be happening (and it puts our implementation details into the user's log messages).\nOK, let's go ahead with this PR but I think we should change this separately.  I'll send a PR.. Can we move this into internal, please?  (I.e. store the map itself there.)  I want to avoid cluttering our external API with implementation details or testing functions at all costs.. const  (or just delete and use the 3 directly on the following line). I think this could go above the lock.  (?). There is nothing asynchronous happening, so I don't think this needs a lock.  Remove the status local entirely maybe?. Either:\n\n\nelse { log.Fatal() }, or\n\n\nremove the if and just assume it's present.  Actually this will work and not output anything if it's missing, since we range over t, and ranging over a nil slice is fine.\n\n\nIn the second case...nothing bad happens if things are broken and the metadata is not present.  Is that OK?. Why accept interface{} and then type assert below, vs. accepting that type directly?\nAlso, it may be a good idea to name this type: type HealthChecker func(context.Context, ..... (in internal probably?). Does this not work: return s, lis, ts, s.Stop, nil ?. You are both deferring rcleanup and returning it in deferFunc.. For error cases you should not return a cleanup function - the caller should see an error and assume nothing needs to be done to clean up.. Is the order of cleanup right?  I think we should be closing the ClientConn before killing the resolver.. log.Error() (unless io.EOF)?. In general, avoid package-level globals.  Also, avoid setting other packages' globals.  Let's find a way to configure the service handler itself.. The request messages have a ResponseSize field in them (and a Type).  We could use the first request's size/type, or we could delete the field entirely and configure the server explicitly rather than using the request.. Unconstrained streaming benchmarks should ideally be their own runMode, separate from the ping-pong stream.. \"// Binary client is an example client\" (godoc compliance). Please remove the blank lines (and re-sort your imports), but keep the blank line between the stdlib and non-stdlib imports.. \"// Binary server __\". Please make this a field of server instead.\nAlso, give it a meaningful name - short names are only appropriate for things with very limited scope.. Moving c into server - if server has a constructor that does the dialing, that would be ideal, and more representative of what we believe our users should be doing.\n. A bit of a nit, but let's be precise in our examples: this is a service not a server.  echoService?. Ideally the port should be configurable in case the user already has something running on that port.  Can you add a --port flag here and in the client?. return status.Error(codes.InvalidArgument, \"request message not received\")?. Since your example doesn't use this, just return unimplemented here.. Since your example doesn't use this, just return unimplemented here.. Let's make this a flag instead of hard-coding.. In case port is zero, print lis.Addr()?. Only because we don't expect the client to end the stream, so if it stops prematurely, then something went wrong in this test.\nMaybe print an error instead, since it would be normal to see code like this in a server.. s/defer//. func (HealthChecker) grpc.DialOption. A nit, but it's usually preferred to save the whole ClientConn in the server in case anything else is needed from it in the future.. s/defer//. Please see https://github.com/grpc/grpc-go/blob/master/examples/features/metadata/client/main.go#L35 -- we usually declare all flags as package globals since they apply to the whole binary.. s/defer//. We are not closing this conn; should we?. log.Error() unless Cancelled then?. please update comment. Can we delete this now that warmup happens as part of the normal benchmark runs?. s/go// and on following line.. Closing a listener does not close any conns that came from it.  Closing the client will close its connections, though, and that will cause the conn to be closed on the server side as well.  So there is no leak.\nWhat is the problem if the client shuts down too quickly?  (And why does that happen?)\nI think a comment in the code is needed here to explain whatever is going on.  :)\n. You can use Fatalf here and remove the else.. PTAL?. Nit: please remove \"method\" as well.\n. I'm pretty sure a server could inadvertently trick this code into suppressing the trailers from the client:\ngo\nfunc ServiceHandler(...) error {\n   if err := doAThingThatMonitorsContext(ctx); err != nil {\n      stream.SetTrailer(md)\n      return err\n   }\n}\nThe proper fix is most likely in the transport layer.. Happy New Year! ;). It might be, but that's not related to this file.. Remove/update please.. Can you do this in a loop ~100 times and make sure we get one or a few instances of both results?. This is to make sure the call returns, I assume..  Is the wait channel necessary, then, given there is also a sleep?\n. We shouldn't still need a prefaceTimer, right?  We should be able to just use the normal connect deadline here I think.  And we can eliminate prefaceReceived, too, if we make NewClientTransport itself block until the preface is received.  That should simplify things even more.. Delete this line too please. \"select/case\" here can be deleted.. Just <-stream.Context().Done(). This is good.. but let's declare i outside this loop and then check it after to make sure it's != 500.  If so, this test isn't doing its job or something else weird is happening.. FYI you can use backticks to avoid escaping quotes:\ngo\nt.Fatalf(`status err: %v; got trl[\"a\"] == nil, wanted trl[\"a\"] != nil`, err). since this isn't used elsewhere, how about making it a lambda in the current test:\ngo\nvar i, cntCanceled uint\npermDenied := func() uint { return i - cntCanceled }\nfor ....\n. It's easy enough to do, anyway.  Done.. Effectively, yes.  They are ordered by how they happen to be in the readySCs map, keyed by the address (as a string, plus a \"type\" and a pointer to optional user metadata).  This is also not right, but I do think randomizing here is better than not randomizing, so we should merge this PR for now and file an issue to improve further.. Delete?. Shouldn't this fail unconditionally now?. As discussed offline, this should have a deadline and the sleep/cancel should be removed.. I think this is the same as cntPermDenied == 0.  Seems like that would be clearer.. Nit: it's a little nicer to use switch code here instead.  Or delete code and use switch status.Code(err) directly.. Please remove. These ifs could be simplified a bit:\ngo\nif cntPermDenied > 0 && cntCanceled > 0 {\n  return\n}\nif cntCanceled > 0 {\n  canceledOK = true\n  lower += (upper-lower)/10 + 1\n} else {\n  permDeniedOk = true\n  upper -= (upper-lower)/10 + 1\n}. This (gRPC trailer = second HTTP/2 HEADERS frame received) is not right, or the name isTrailer is misleading if this is how you have intended it to behave.  Just below we use the end stream bit in the frame to determine whether this is trailers.  That will result in true for trailers-only responses, whereas the check added here would be false.. I think it is actual preferable to not have a constructor here.  Consider:\nstate := newDecodeState(true, false)\nand\nstate := &decodeState{serverSide: true, ignoreContentType: false}\nThe latter is more readable.  Unless there's state that needs to always be initialized for correctness, a constructor is not necessary.. This comment applies more for parsedHeaderData than decodeState, doesn't it?. Is this \"reset\" comment correct?  Do we ever reset it? I thought we just make new ones every time.. Trailers (after headers) shouldn't even have a content-type - or if it does, we should always ignore it, no?. It may be cleaner to put \"isGRPC\" into parsedHeaderData to avoid setting it here based on knowledge of how processHeaderField works (i.e. no error and field name = \"content-type\" implies GRPC).\nMaybe the above logic can all be moved there too?  (The switch and the knowledge of skipping certain fields.). I don't think our users will know what \"HTTP fallback mode\" means.  Just \"malformed header: missing HTTP status\" seems OK.. if contentTypeErr is moved to parsedHeaderData, then it can be stored as a string instead, removing the need for this conversion.. How about \"; \"?  A tab might not be sufficient to separate the items.. Export from transport instead of copying?. These consts can be moved inside the test, to avoid polluting the (already huge) global namespace of the test package.. Please add package comment. Comment plz. Parser?  balancerload.Parser seems sufficient.. Delete. Done. Good idea.  Done. Considered and decided against since the client shows all the frames already.  No reason to show it twice.. Done. That's how the spec is.  The server has no minimum, but I'm thinking of using 1s granularity on the ping timer, so 1s would be the practical minimum.. Fixed.  (I was logging this earlier while debugging.). Done. Done. Done. Do we still need this?  Is it not safe to parse everything?  (If things must be skipped, can we move this logic to processHeaderField?)\nIf we're just skipping parsing some header fields in the name of efficiency, I don't think extra code complexity is worth optimizing error cases (unless it's a substantial difference).\n. Is it possible to eliminate httpStatus from data and set httpErr here instead?. Actually, this should also reset ac.backoffIdx = 0.\nEDIT: Oh we already do this below; maybe we should move it up here though for consistency and to avoid taking the lock an extra time.. Hmm, this doesn't seem right to me - we shouldn't have more than one subconn when we are in pick first mode, right?  We should send the full list of addresses to that subconn and update it as necessary.  Am I misunderstanding the requirements, was that hard/impossible for some reason, or did you just not consider that approach?. I think you could avoid some duplicate code by moving the handling of transient failure / connecting above this if.. These are all the defaults - maybe delete and a comment \"default DoneInfo indicates RPC did not occur\"?. The loneliest waitgroup.  Maybe use a channel instead?. Nit: you can probably leave out \"get\" and \"Func\".. Similar: consider s/Get//. ",
    "domesticmouse": "Hey GoogleBot, my real email account is brettmorgan@google.com.\n. ",
    "codahale": "I did some initial benchmarking of a gRPC service here, including profiling: https://gist.github.com/codahale/b3db28bfa3f7dd59d048.\nIt\u2019s slower than net/http, and @kenkeiter and I think it\u2019s probably due to IO contention. net/http uses bufio to buffer writes; grpc-go, on the other hand, uses a bare net.Conn.\n. It\u2019s not much, but the code\u2019s here: https://github.com/codahale/grpc-example.\nI just spun that up on two m3.2xls.\n. The profiler graphs I created were created with GOMAXPROCS=8.\n. I strongly recommend using two separate machines, as your loopback interface shares very little in common with your network interface.\n. ",
    "hsaliak": "@iamqizhao can we close this issue, given that we have such a framework in place\n. Closing this issue - docs are published here: https://godoc.org/google.golang.org/grpc \nWe will add a reference to it from grpc.io.\n. I do not believe we have plans to do structured logging at the moment. @menghanl @dfawley could you take a look and see what to do with this proposed enhancement. . Sounds good, this may make a good gRFC proposal. I'll defer to @dfawley though. We do not yet have a binary logging feature and are not prioritizing it actively, so closing the issue. As the reporter has already addressed this as via an interceptor, closing the issue.. @ppq I am closing this issue as there exists document and is stale. If you are still waiting for enhancements to this doc, please reopen.. #75 and #514 have landed, if this is still an issue, @nodirt please reopen.\n. @dfawley assigning to you to triage and make a decision on exposing the Flush knob to users.. Some of the levelled logging that is being implemented by @menghanl in #922 should help here.. @peter-edge @enisoc are you still seeing this issue?. @tamird sorry for not picking this one up, are you folks still seeing this error from time to time?\n. @menghanl would you be able to take a look?. @tamird we want to stress test the go client and report metrics from it.\n. @philips I am not sure if this still affects the project.. I am now closing the issue, please re-open if you still want some of changes made.. @MakMukhi would you be able to take a look and see if this is still an issue? This should be straightforward to reproduce.. We are not considering vendoring at the moment in grpc-go at the moment, so I am going to close this issue. The issues debated here are widely known and acknowledged - the general problem of dependency management for the library does need solving. \nWe will likely end up adopting any official guideline in this regard, but there are not any yet. . Closing this issue, since the API has evolved since. if the dial option to mark Dial as blocking is set, then a timeout can be set on it. If not, Dial returns immediately and the RPCs will either fail fast or after a specified timeout. \n@sjpotter please re-open if this API is still problematic for you. . #1095 should solve it. It implements gRFC A1: grpc/proposal#4 which is the feature @ejona86 mentioned.. @sarietta are you still seeing this issue?. @VenkateshSub is this still an issue, if so, could you give us some more details?. @dfawley with keepalive support and #536 closed, I am closing this as well. \n@AmandaCameron please re-open if you feel that this is still an issue.. #1210  should solve this. We also have #1073 in flight which should help with large messages throughput.\n/cc @MakMukhi @apolcyn \nCan you try the API introduced in #1210 and let us know if it helps? \nIf the issue still exists, it would be super useful for us to get pointers to benchmarks that can help us identify the problem.. @dfawley that works for me, closing this issue.\n. If there is still interest, this might be a good item to write a short proposal for and discuss with the community. . @peter-edge sounds good, took a look at #1203, as it looks like we are converging on a solution, lets just stay the course of iterating on the PR and close this issue out.\n. Providing this support depends on the App Engine project and there has been discussions with that team in that regard. \nI am not sure what the gRPC project can do here short of making changes to work around the limitations of the app engine sandbox, which is beyond the scope of the core project. . sync.Pool is a potential performance optimization, it is one tool that can be used when object pools are a good solution. However, we can only determine whether it's a good solution to a problem at hand after analyzing the problem,benchmarking and profiling.\ndefer is a language construct that lets you schedule a function to be run on the exit of the current scope. I am not sure how they relate. \nDoes that help?. @xh3b4sd do you still feel that this is something that should be solved in grpc-go or are you leaning toward having this resolved in the core net package ? If it's the latter, I'd like to close this issue.\n . I am going to close this issue out for now, if you hit this again and you feel this is something grpc-go should resolve, please reopen. Thanks for filing the issue! \n. @mwitkow Server reflection https://github.com/grpc/grpc-go/tree/master/reflection \nallows you to query for a list of services and obtain the FileDescriptorProto ( a .proto file represented as a protocol buffer) for a given service.\nI do not know if a dynamic protobuf serialization library is planned for in golang though. \n. @apolcyn assigning this to you to take a look. /cc @dfawley . @MakMukhi @menghanl can you reference the PR here and close the issue if it's already resolved?\n. Closing this. We are not officially planning to support flatbuffers as an IDL at the moment. There is some support upstream as per google/flatbuffers#4082 and it works as a message interchange format with grpc already. \nIf we do need to support flatbuffers in grpc-go natively, it should be proposed as a gRFC. Is this still an issue? @vrecan . Added an enhancement label to this, but the work is not prioritized at the moment. \nTo flesh this out further, it may be best to submit a proposal that discusses a few implementation options as a language specific  gRFC . @buckhx this seems to not be a gRPC issue but a protobuf issue, as filed in  golang/protobuf#237  so I am closing this. I'll follow up on the protobuf issue on that bug.. @panamafrancis we do not have any plans to implement this at the moment.  Strong arguments against doing something like this would be how this would work in an actual scenario where clients are connected to a service. As far as the client is concerned, the service availability is impacted, the same as a restart. . @menghanl can you reference related pull requests that might help here?\nWill #922 address this?. @MakMukhi what's remaining to commit this change in? Is it just waiting for a reviewer?. @jdoliner are you still seeing this issue?. @rogeralsing I am closing this issue since the performance seems to be caused and resolved by underlying GC changes in the language. If you think that there are still gRPC specific performance issues we should investigate, please reopen.. @menghanl assigning this back to you for the fix of #922 . I am going to close the issue because it refers to #297 which is closed.  @joseAndresGomezTovar please re-open with an example of how we can reproduce what you are seeing. . @menghanl another ServeHTTP issue for you to look at.. @MakMukhi please take a look. The same 2 lines are removed from control.pb.go and control.proto, perhaps they are good test cases to run the script against and identify why there are false positives?\n. The gRFC has been approved so we are committed to changing the license.\nWhile it's ideal to switch everything over at the same time, I am not aware\nof any requirement to actually do this in lockstep. If it's more\nconvenient, this can be done whenever a repository wants to.\nOn Fri, May 12, 2017 at 2:14 PM, dfawley notifications@github.com wrote:\n\n@nicolasnoble https://github.com/nicolasnoble, what is the status of\nthis? We noticed the counterpart changes in c and java are also pending. If\nit's OK to move forward with this change, can you please update it to\ninclude all the new/moved files? We will prioritize the approval then. If\nit's not appropriate, I would like to close it until it's ready. Thanks!\n\u2014\nYou are receiving this because your review was requested.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/1090#issuecomment-301187593, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAJKXIZgvNrKxrQWgZXjJxQ89kIQkMtkks5r5MuygaJpZM4MMPxs\n.\n. @pkieltyka We do a few different things but a good place to look at the long term features being planned for the project is at the grpc/proposal repo. Big changes in behaviour or new features do need an accepted gRFC. The grpc-go and grpc project are also using projects to track items for the upcoming release. We release on a fixed release cycle. The grpc-java project uses milestones, since that's worked well for them in the past. \n\nIs there any specific feature or functionality you are looking for?\n. /cc @LisaFC @rakyll fyi. This is an issue with the grpc_cli tool and is tracked here:  grpc/grpc#10304 \nClosing the issue here.. Given that Canceled is the spelling of the exported constant, the comment should be updated. \n@dfawley just a heads up to perhaps fix this the next time changes go into this file..\nAlternatively, @ghasemloo if you want to send us a PR that should work too.. @dfawley could you please take a look and comment. @MOZGIII -- trying to understand this request better, and I have some questions: What is the motivation for wanting something like this? Is this something that would be 'cool to have' or will it enable something critical for you?\n. Instead of the hypothetical example below, do you want to state that this example walks through how server reflection can be enabled on the hello world example?\nWe can then use real code such as \npb.RegisterGreeterServer(s, &server{}) instead of the functions below. \n. We can move this statement up.\n. ",
    "tsuna": "It's kinda ridiculous that go vet won't get fixed and you guys don't wanna change your usage that fools go vet either...  It's a big pain in the ass for those of us who use go vet, which should be considered best practice.\nIf anyone else runs into this, here's a workaround:\ngo\n    errf := grpc.Errorf // Confuse `go vet' to not check this `Errorf' call. :(\n    // See https://github.com/grpc/grpc-go/issues/90\n    return errf(codes.Unimplemented, \"RPC not yet implemented\")\n. This causes runtime problems in existing code.  The code still compiles fine, but existing tests for instance now fail with foo_test.go:88: Failed to connect to [::]:58898: grpc: no transport security set (use grpc.WithInsecure() explicitly or set credentials).\nNot cool :(\n. If the API changes break compilation, that's fine, it's easier to catch.  Runtime breaks are trickier ;o\n. Somewhat related to the issue above but I just noticed that gRPC sends a lot of packets.  I haven't yet dug into the details to understand why, but it's visible in the trace posted above by @rsc's test.  In my case I'm sending a small request and receiving a small response (~100 bytes each) but I see 18 packets go on the wire, most of which are 9-15 bytes (11 if we discount empty packets that are just acks).\n[after priming the connection by sending a first RPC]\n// this packet actually contains the PB of the request\n01:04:32.687525 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 204: (tos 0x0, ttl 61, id 5674, offset 0, flags [DF], proto TCP (6), length 190)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [P.], seq 421:559, ack 315, win 4096, options [nop,nop,TS val 557400299 ecr 4525713], length 138\n01:04:32.688483 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 81: (tos 0x0, ttl 61, id 22052, offset 0, flags [DF], proto TCP (6), length 67)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [P.], seq 559:574, ack 315, win 4096, options [nop,nop,TS val 557400299 ecr 4525713], length 15\n01:04:32.690579 00:1c:73:74:35:d8 > 00:1c:73:00:00:99, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 64, id 59436, offset 0, flags [DF], proto TCP (6), length 52)\n    172.24.12.82.6042 > 10.95.4.132.52612: Flags [.], ack 574, win 243, options [nop,nop,TS val 4526466 ecr 557400299], length 0\n01:04:32.690647 00:1c:73:74:35:d8 > 00:1c:73:00:00:99, ethertype IPv4 (0x0800), length 75: (tos 0x0, ttl 64, id 59437, offset 0, flags [DF], proto TCP (6), length 61)\n    172.24.12.82.6042 > 10.95.4.132.52612: Flags [P.], seq 315:324, ack 574, win 243, options [nop,nop,TS val 4526466 ecr 557400299], length 9\n01:04:32.696567 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 61, id 3587, offset 0, flags [DF], proto TCP (6), length 52)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [.], ack 324, win 4096, options [nop,nop,TS val 557400311 ecr 4526466], length 0\n01:04:32.697151 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 81: (tos 0x0, ttl 61, id 30455, offset 0, flags [DF], proto TCP (6), length 67)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [P.], seq 574:589, ack 324, win 4096, options [nop,nop,TS val 557400311 ecr 4526466], length 15\n01:04:32.697257 00:1c:73:74:35:d8 > 00:1c:73:00:00:99, ethertype IPv4 (0x0800), length 75: (tos 0x0, ttl 64, id 59438, offset 0, flags [DF], proto TCP (6), length 61)\n    172.24.12.82.6042 > 10.95.4.132.52612: Flags [P.], seq 324:333, ack 589, win 243, options [nop,nop,TS val 4526467 ecr 557400311], length 9\n01:04:32.702782 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 61, id 12564, offset 0, flags [DF], proto TCP (6), length 52)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [.], ack 333, win 4095, options [nop,nop,TS val 557400317 ecr 4526467], length 0\n01:04:32.703466 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 81: (tos 0x0, ttl 61, id 719, offset 0, flags [DF], proto TCP (6), length 67)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [P.], seq 589:604, ack 333, win 4096, options [nop,nop,TS val 557400317 ecr 4526467], length 15\n01:04:32.703564 00:1c:73:74:35:d8 > 00:1c:73:00:00:99, ethertype IPv4 (0x0800), length 75: (tos 0x0, ttl 64, id 59439, offset 0, flags [DF], proto TCP (6), length 61)\n    172.24.12.82.6042 > 10.95.4.132.52612: Flags [P.], seq 333:342, ack 604, win 243, options [nop,nop,TS val 4526469 ecr 557400317], length 9\n01:04:32.708048 00:1c:73:74:35:d8 > 00:1c:73:00:00:99, ethertype IPv4 (0x0800), length 77: (tos 0x0, ttl 64, id 59440, offset 0, flags [DF], proto TCP (6), length 63)\n    172.24.12.82.6042 > 10.95.4.132.52612: Flags [P.], seq 342:353, ack 604, win 243, options [nop,nop,TS val 4526470 ecr 557400317], length 11\n// this packet actually contains the PB of the response\n01:04:32.708159 00:1c:73:74:35:d8 > 00:1c:73:00:00:99, ethertype IPv4 (0x0800), length 143: (tos 0x0, ttl 64, id 59441, offset 0, flags [DF], proto TCP (6), length 129)\n    172.24.12.82.6042 > 10.95.4.132.52612: Flags [P.], seq 353:430, ack 604, win 243, options [nop,nop,TS val 4526470 ecr 557400317], length 77\n01:04:32.709612 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 61, id 31736, offset 0, flags [DF], proto TCP (6), length 52)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [.], ack 342, win 4095, options [nop,nop,TS val 557400323 ecr 4526469], length 0\n01:04:32.709915 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 81: (tos 0x0, ttl 61, id 15498, offset 0, flags [DF], proto TCP (6), length 67)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [P.], seq 604:619, ack 342, win 4096, options [nop,nop,TS val 557400323 ecr 4526469], length 15\n01:04:32.710010 00:1c:73:74:35:d8 > 00:1c:73:00:00:99, ethertype IPv4 (0x0800), length 75: (tos 0x0, ttl 64, id 59442, offset 0, flags [DF], proto TCP (6), length 61)\n    172.24.12.82.6042 > 10.95.4.132.52612: Flags [P.], seq 430:439, ack 619, win 243, options [nop,nop,TS val 4526470 ecr 557400323], length 9\n01:04:32.714292 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 61, id 34032, offset 0, flags [DF], proto TCP (6), length 52)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [.], ack 353, win 4095, options [nop,nop,TS val 557400327 ecr 4526470], length 0\n01:04:32.714810 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 61, id 58116, offset 0, flags [DF], proto TCP (6), length 52)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [.], ack 430, win 4093, options [nop,nop,TS val 557400327 ecr 4526470], length 0\n01:04:32.715478 00:1c:73:3d:9a:c1 > 00:1c:73:74:35:d8, ethertype IPv4 (0x0800), length 66: (tos 0x0, ttl 61, id 56796, offset 0, flags [DF], proto TCP (6), length 52)\n    10.95.4.132.52612 > 172.24.12.82.6042: Flags [.], ack 439, win 4095, options [nop,nop,TS val 557400328 ecr 4526470], length 0\nWhat are all these small 9-15 byte packets?. This change broke backward compatibility, I filed #1178 to track the issue.. Sounds good, thanks for clarifying.. BTW, I read the original issue (#1148) and understand the problem, but the solution is somewhat annoying to use in other cases where a library function could be used to fill in the context of either a client or a server.  Now the library functions have to either be duplicated or receive a flag indicating whether or not we're filling in incoming or outgoing metadata.. Any update on this bug?  We regularly run into crazy busy loop situations and I have to manually patch transportMonitor() in our vendored code to add a time.Sleep(1 * time.Second) at the end of the endless for loop in there as a poor man's solution to get gRPC to chill out instead of going crazy.. Any update please?  This is a problem with the client busy-looping when connecting to a TCP reverse-proxy like haproxy that accepts the connection and has no other choice than closing it if no backend is healthy.. Just to be clear (and for the casual pedestrian stumbling on this issue and seeing it closed) the issue isn't actually fixed unless we use the new DialOption called WithWaitForHandshake(), right?. Just to be sure I understand your suggestion, you're saying that right after this code:\ngo\n        if len(t.activeStreams) == 1 {\n                select {\n                case t.awakenKeepalive <- struct{}{}:\n                        t.framer.writePing(false, false, [8]byte{})\nwe need to add:\ngo\n                        t.awakenKeepalive <- struct{}{}\nif yes, then this appears to works, although it looks weird.  Definitely something that's going to deserve a comment in the code.\nI'm not sure why you want to keep holing t.mu when waiting on awakenKeepalive.  Holding locks while blocking on channels is a common recipe for deadlocks... unless you force strict lock+channel ordering (basically consider channels like locks and enforce strict ordering).  In this particular case, there is no benefit in keeping the lock while waiting on the channel.\nEither way, I'm happy as long as this hole is plugged.. You're right, good catch.. I amended my commit, PTAL.  Already have a CLA on file with Google.. @MakMukhi @jimjibone I just read #1516, this is a duplicate, sorry.  The confusion stemmed from the fact that I tried to update my vendored dependencies just a few minutes before the fix merged in #1517, so I fetched the latest upstream code and try to change it and it worked, but it worked not because of my useless no-op change, but because it had already been fixed upstream by #1517 (I was confused as well because when I did the calculation upon opening up the file, it seemed like version was correctly aligned already, but because I had just seen the failure in my other repo with the slightly stale vendored code, I brushed that away and ignored the evidence that was contradictory with my stale belief).\nSorry for the noise \ud83d\ude05. Probably a dup of #954. or of #1444.... Any update on this bug?. Cool.  I sent a PR (#1660) with a quick fix to always add a simple identity compression codec but maybe you had something else in mind.  At least this simple code change fixed the problem for us.. I signed the Linux Foundation CLA.  Also working to get a Corporate CLA in place between Arista and CNCF.. Just to be sure: did you verify that a Python client can now talk to a Go server with this change?. I can try it but it's not the first time we run into this sort of interop issue, it would be nice if upstream did better testing of interop between the various implementations of gRPC.. ",
    "mikeatlas": "Thanks for the workaround @tsuna, we'll use that as part of the existing sed calls we run on the grpc output to clean up other things as well.\n. @AdamSroka sure but grpc.Errorf is not Printf; I personally think a custom package's Errorf implementation can require the first argument(s) to be package-specific, which, in this case, they make reasonable sense: \nhttps://godoc.org/google.golang.org/grpc#Errorf\nfunc Errorf(c codes.Code, format string, a ...interface{}) error\n\nErrorf returns an error containing an error code and a description; Errorf returns nil if c is OK.\n\nThe only real issue here is that go vet was na\u00efvely matching the method name \"Errorf\" regardless of whether it was the core fmt.Errorf function or not. go vet had no business warning other packages that they can't make methods named Errorf with a different signature than the fmt package.\n. Bleh. grpc.Code(codes.Code).Errorf(...) is not an intuitive way to build a package-level error in my opinion. Any Error-builder/emitter should look like a factory method/function, and take in all required parameters in one shot. Why would I instantiate a Code first so that I can create an Error? What if the code were for something like \"Code.UnknownCode\"? Now I'm making an Error from an unknown code? It's just more seems more like natural logic to create an Error and give it an ErrorCode. The semantics just seem far more obvious. From an OO mindset, Code is an attribute of an Error, not something from which Error inherits (loosely speaking here). I'd raise a fuss in a code review if you designed it like that!\n. @tamird, @shurcooL posted above. OP can close ;)\n. @iamqizhao is there a reason for removing a public facing method State() on the master branch? This change broke our builds - is there any chance you plan to version grpc-go in the future so that we can rely on the package being stable across pulls? \n. Err, I suppose it is our fault for not seeing \"THIS IS AN EXPERIMENTAL API\" for State(), but it still begs the question further upstream why an experimental public method was added to a master branch of this package.\n. Thanks for the background on State() @iamqizhao. We had been using it after calling Dial() in order to check if it was in a Ready state, but for now it is good enough that we just check the Error returned from Dial() and proceed.\n. Got it, thanks!\n. ",
    "dmitshur": "For posterity, this issue has been resolved in go vet in https://github.com/golang/go/issues/12294 and will be available in Go 1.7.\n. @AdamSroka, what about fmt.Fprintf?\n. > Please use metadata API to transmit your key-value pair. The example is\n\nfor unary rpc:\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L355\nfor streaming rpc:\nhttps://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L504\n\nFor anyone looking for that example in the future, the above links won't work because master has since changed and the line numbers don't match up. Here are the correct links:\nFor unary rpc:\nhttps://github.com/grpc/grpc-go/blob/51496073b86104a21479f1b0c09337ab76f0f883/test/end2end_test.go#L355\nFor streaming rpc:\nhttps://github.com/grpc/grpc-go/blob/51496073b86104a21479f1b0c09337ab76f0f883/test/end2end_test.go#L504\nTip: You can press y when looking at a file to get a permalink:\n\n. > Both spelling are correct, but we should pick one and be consistent.\nRelated: https://dmitri.shuralyov.com/idiomatic-go#use-consistent-spelling-of-certain-words.\nThe Go project chose to go with the single-l spelling.. ",
    "AdamSroka": "The format string should be the first argument. This has a been a convention since the beginning of time.  https://en.wikipedia.org/wiki/Printf_format_string\n. @mikeatlas yeah, fair enough. I think making go vet less brittle is a good thing, but I would also prefer if familiar sounding methods had the signature that their names imply. I would do something like grpc.Code(codes.Code).Errorf(fmt string, a ...interface{}) and have Code return an interface that knows what to do with that. \n. ",
    "puellanivis": "I would like to support the idea of making the Errorf a receiver method of\ncodes.Code. It does make sense since the Errorf is semantically domained by\nthe codes.Code you're passing in.\nAnd, codes.InvalidArgument.Errof(\"gave a bad XY: %v\", thing) looks better\nin a syntax parallels semantics way.\nAdam Sroka notifications@github.com schrieb am Fr., 24. Juni 2016 um\n23:17 Uhr:\n\n@mikeatlas https://github.com/mikeatlas yeah, fair enough. I think\nmaking go vet less brittle is a good thing, but I would also prefer if\nfamiliar sounding methods had the signature that their names imply. I would\ndo something like grpc.Code(codes.Code).Errorf(fmt string, a\n...interface{}) and have Code return an interface that knows what to do\nwith that.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/90#issuecomment-228464260, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AKVkVuqmhWdKMjX75wBFgMpas9ymb9Tqks5qPElvgaJpZM4Do-gl\n.\n. > Why would I instantiate a Code first so that I can create an Error?\n\nYou shouldn't be instantiating codes at all.\nimport \"github.com/grpc/grpc-go/codes\"\nfunc fn() error {\n  return codes.PermissionDenied.Errorf(\"policy determined that user %s is restricted\", \"myUsername\")\n}\nAgain, why would you ever \u201cinstantiate\u201d your own code? It is semantically an enum, not an object.\nIn fact, the only difference between codes.PermissionDenied.Errorf(\"policy determined that user %s is restricted\", \"myUsername\") and grpc.Errorf(codes.PermissionDenied, \"policy determined that user %s is restricted\", \"myUsername\") is syntax: one is a receiver and the other is not.\nThey both have the same call-stack regardless.\nCreating a new error with a Errorf function for a specific code, is a domain being encapsulated semantically by that code itself. Every Errorf(code, format, args...) is bound semantically with the code.Code in the first place. So, why not express that as a receiver function?\n\u201cOne things you can do with a codes.Code is generate a new error with arbitrary text using its Errorf function, which works exactly as one would expect such an Errorf function to work\u201d\n. https://github.com/puellanivis/grpc-go/blob/master/codes/codes.go shows how the Errorf would work.\nThen StreamError in transport/transport.go can be just:\ntype StreamError struct { codes.RPCError }\nWith a custom Error to wrap the codes.RCPError.Error by adding in a \"stream error : \" in front. Accessing err.Code and err.Desc however, still work just the same as before.\nOh, and rpc_util.go kind of has a redundant rpcError type after this. Well, unless it's specifically desired that the error type and fields be unexported.\nNow, in server.go, we can do: codes.Internal.Errorf(\"io.ErrUnexpectedEOF\")\nAnd later in transport/handler_server.go we can use: code.Errorf(\"%s\", se.Error()) (where code of type transport.StreamError)\n. \u201cSeems odd that we would just let an RPC method call hang forever.\u201d\nWhy? This is Go. If you need it to be asynchronous, you can just go func() { }() it.\nA basic design principle in Go is that one should write ones API to be synchronous. Because, if the programmer needs asynchronous, then they can generally trivially make it asynchronous.\n. Seems like a reasonable argument. I'm just noting that it should not sound \u201codd\u201d to do that. (omg, stupid typos. Sorry for the spam in correcting this tiny little comment.)\n. So, I've had to work around haproxy not supporting HTTP/2.0 and requiring that all traffic therefore need be HTTP/1.1 with a Hostname field (which is a fair assertion about HTTP/1.1 traffic, because it's defined to require that header.)\nThe solution I came up with (which is sickening to admit) is using HTTP/1.1 requests and polling with cookies to hold a connection open, and return a net.Conn implementation on top of that for the accept method of the grpc server.\nI mean, this is insane, and dumb, and no one should do it but I needed things to work /now/.\nSo, yeah, please, no one should have to implement HTTP/2.0 over HTTP/1.1 over TCP/IP. It would be nice if there were a good solution, instead of things like my hack.\n. They have however released a beta test already. The whole idea behind pre-release versions is so that people can ensure compatibility for when it is released.\nIt's not like pulling context into the standard library is going to change. And whatever the eventual solution put into grpc is, it will still have to accommodate both Go 1.6.x and Go 1.7\u2026\nSo, why not have the bug open but lower priority, and maybe someone could work on it occasionally ahead of Go 1.7 is released at which point, this bug becomes a significantly bigger priority, and would just have to be reopened/refiled anyways. I mean, release is scheduled for August, two months-ish away.\nWhy would you close this bug, when it's just going to be reopened two months from now?\n. Maybe it's just the SRE in me, but I think projects should be prepared for change, rather than scramble once things are on fire.\nWe know this change is going to happen, and it has basically been on the horizon for a long time already. So, why not do something about it now?\nFix a bug before the vast majority of people even have a chance to file a ton of duplicate bugs? Priceless.\n. Yeah, I'm thinking of how one could go about making it work for both golang.org/x/net/context (pre-1.7) and context (1.7) at the same time\u2026\nKind of drawing blanks right now.\n. The problem is not the problem with being able to pass a context.Context into a function that takes golang.org/x/net/context.Context, it's that you can't use a Server with a receiver method func (s *Server) gRPCCall(ctx context.Context, in *pb.RequestProto) (out *pb.ReplyProto, error) into a function that wants an interface with a receiver method of gRPCCall(ctx golang.org/x/net/context.Context, in *pb.RequestProto) (out *pb.ReplyProto, error)\nThey don't match, because of Go's strong typing. Therefore, you need a mishmash of \"golang.org/x/net/context\" and \"context\" in different situations, where you use the x/net/context when you need to have a server match the method signature for the interface in gRPC, while you can use context elsewhere.\nYes, a work around is still \u201cjust use golang.org/x/net/context forever\", but the whole reason for putting context into the standard library is that it's no longer experimental anymore and isn't confined semantically into the \"net/\" packages anymore.\n. > What it means is go 1.7 users using GRPC will still need golang.org/x/net but this isn't really a huge deal.\nExcept now, because I have things that expect a context.Context receiver signature in the interface, and the gRPC one that expects a golang.org/x/net/context receiver signature in the interface.\nSo, now, I have to include two context packages, rename one of them, and one of them is obsoleted by the other. And then I have to remember which of the context packages I renamed, and remember if and when I didn't need to rename it because I'm only using one context type.\nI made that stepping stone the same day this bug was filed, but it is not good programming.\nWhen 1.7 releases, use of golang.org/x/net/context should be considered deprecated, and likely to disappear in 1.8, because that's how taking things out of the experimental tree typically works.\n. > Conversely, a package which breaks version compatibility in weird ways for the sake of reducing package count isn't good package maintenance.\nWhich is why I don't. My package github.com/puellanivis/protobuf/proto for instance doesn't break version compatibility. It uses \"context\" when go1.7 build tag is defined, and uses \"golang.org/x/net/context\" when go1.6 is defined.\nI would expect my eventual solution for gRPC will do similar.\n\nAt the current release rate of one every 6 months, this means that by February 2017, the only Go versions getting security updates will be 1.7 and 1.8. Anyone still on <=1.6 at that point who wants to run a server will move up.\n\nAnd if we fix it now, when people move from 1.6 to 1.7, they'll just replace \"golang.org/x/net/context\" with \"context\" and everything will continue to work. This would best be complete while 1.7 is the most recent release. Then at 1.8, drop support for 1.6.\n\nI think it's completely sensible to keep using x/net/context until approx. February 2017, to facilitate the transition\n\nBut why wait until the last day? Why procrastinate? This is like the Y2K bug. We know it's going to happen, so why not fix it NOW, rather than in a rush before it's required?\nI understand that you believe it to be sensible to keep using golang.org/x/net/context. However, not everyone need agree with you. I can work on this while other people work on other things, and eventually, integrate my patches (in a way that will not break version compatibility), and then people can switch over to using just \"context\".\nThis is a snowdrift, and I'm plowing it. One shouldn't be looking to stop me from plowing it by saying that we don't have to get to work until 17:02, and it's only 16:06, so like, \u201cWe have like an hour dude, we don't need to do it now.\u201d I don't care, I'm clearing the snowdrift now. I can take my time and make sure it's done right the first time, rather than adhoc because last second.\n. BUT\u2026 I don't actually have to change any of the grpc-go files that are not defining types\u2026 which saves a lot of effort and/or duplication of code.\n. Hm\u2026 could be useful\u2026 but it should work without changing anyone's code\u2026\n. Turns out, it looks like just interceptor.go needs to be pulled out by build tag, and the type definition for methodHandler in server.go needs to be extracted into its own file to allow for different build tags.\nWith that, it works with go1.7\u2026\nNow, to test with go1.6\u2026\n. And it works. The only changes need be made are extracting out a few types to versioned build tag files, and a synchronous change to protoc-gen-go (which just changes one variable saying which context to use when generating code). Then code will compile with 1.6 using \"golang.org/x/net/context\" and it also compiles with 1.7 with the only change being s(golang\\.org/x/net/context)(context)\ngRPC keeps using golang.org/x/net/context where it doesn't cause a type conflict, but can be switched the same way as user's code (the s-regex) when supporting anything earlier than 1.7 stops making sense.\nI'll work on gathering it up into something that can actually be a pull request. Plenty of time to get that done though. :)\n. Ok, I see what you're trying to say. The problem though is that fundamentally, we cannot make a \u201ccontext\u201d vs \u201cx/net/context\u201d agnostic system. People who more quickly move over to \u201ccontext\u201d will be broken unless we move some of the gRPC code over to \u201ccontext\u201d, but then others would have code break because they need the same source to be able to compile cross-versions.\nThere just isn't a way around this. Unless we use an interface{}, then type cast. But fundamentally they're still the same interface.\n:( Really, the problem here is that we're having the system care about if it's context.Context or golang.org/x/net/context.Context, when both of the interface definitions are identical (baring comments and whitespace).\nWith protoc-gen-go, it's easy enough to switch between the two as the target context library is a variable that can be swapped by option flags/whatever.\nUnfortunately, in gRPC, this isn't feasible\u2026 unless we use interface{} in the function definitions and just assert type. Which, I DO suppose is a possible option\u2026\nAlternatively, we could just use:\ntype Context interface {\n  context.Context\n}\nAnd then the function signatures use grpc.Context, but everything else can go ahead and just use context.Context or golang.org/x/net/context because they'll all assert type without any possible panic. (Due to the identical interfaces)\nBut even with interface{}, we're screwed, because people would have to change their code to whichever context they're using over to interface{}\u2026\nUgh\u2026 this problem\u2026\nLast idea, we just use interface{} on the function types, and then type assert it to whichever context it's actually defined as using\u2026 then the same code would work for both !go1.7 and go.17\u2026 and then, no code change on the external-to-gRPC side.\n. a) I've given up on throwing out x/net/context entirely. It's going to need to stay there until 1.6 is no longer a reasonable option.\nIt seems on its face to be good, but something just nags me. Oh, grr\u2026 I still can't make a server with RPCs that point to the other context. Sure, people are unlikely to mix-and-match in the same source, but at some point, someone will hit the \u201ccannot use func(context.Context) as func(context.Context)\u201d error, and too many people would probably not be able to understand what's broken there (because what do you mean you can't use that signature as the apparently same signature?)\n:/ Lots of possible solutions, but they all kind of suck one way or the other.\n. > Would it be possible to handle this with a flag to the codegen?\nI have worked some of that into my forks already, but unfortunately, you cannot write code that will compile for both 1.6 and 1.7 without using x/net/context. So, really, gRPC is stuck here at x/net/context until possibly 1.8, or 1.9, when all continue-to-be-supported versions have a context library. Then, everyone merges over at the same time.\nThere is literally no way to solve this problem such that it works for both 1.6, and 1.7 with the exact same code, and as such would break far too many people's projects to be a worthwhile change. :(\n. Yes, that is possible. It's also what my forks of grpc and protoc are currently. (They also lag behind the main branches significantly.)\nThey do not however completely give up x/net/context, they just isolate the things that absolutely must be context rather than x/net/context, and package those into _go1.7 and _legacy files. go1.6 and earlier then works transparently with x/net/context and go.17 and later works transparently with context.\nThat so little actually need change is a pretty good indicator that identically defined interfaces are already quite unique among types.\n. Dropping support for <1.7 was always eventually going to happen. But it\u2019s wise to wait until 1.6 has gone out of official support\u2026. Indeed. Which was an expected solution from the very beginning. It was noted that type aliases would solve this issue entirely, and with the least chance of breaking Go1 compat.. From x/net/http2: (link)\n\n21 more days...\nhttps://groups.google.com/forum/#!topic/google-appengine-go/wHsYtxvEbXI\n\nWe\u2019re writing to let you know that we are deprecating Go 1.8 support on App Engine, and new deployments using this language version will no longer be available on November 1, 2018.\n\n\nAnd golang/protobuf: (link)\n\nHere's a convenient countdown clock.. I'm highly confused about the code you present.\n\ngrpc-go$ grep -r DialServer .\nError Code 1 returned\nAre you wrapping around something in the library? Because if you are, we can't really help you out without knowing what the DialServer method is doing\u2026\n. https://github.com/golang/protobuf/blob/master/proto/lib.go#L892\nCurrent ProtoPackageIsVersionN is 2.\nProtoPackageIsVersion3 does not exist.\nAs such, there is no reason to up any of the \u201cProtoPackageIsVersion*\u201d anywhere in grpc to three.\n. Indeed, as I understand the use case, grpc will reference ProtoPackageIsVersion3 when there is a breaking-change between the protoc generation and grpc. As such, this will ensure that grpc will not compile correctly with an outdated {proto,protoc-gen-go}\nIt's ALMOST basically like a shared token to ensure that a change that must be done synchronously between {grpc-go,proto,protoc-gen-go} MUST happen synchronously\u2026 after all, otherwise the code would refuse to compile.\nSo if someone runs a go get -u grpc-go, but also need a {proto,protoc-gen-go} the code won't silently continue to compile.\n. Correct, (using mathematical notation and words for more clarity of meaning) we want not (err = nil or err = io.EOF) which is logically equivalent to err \u2260 nil and err \u2260 io.EOF.\n. I just have to ask\u2026 why was this ever not just a defer s.mu.Unlock()?\nAll paths need to unlock the mutex, so defer it. You should almost always be defering the unlock. The only time you shouldn't be is when a function needs to actually hold the lock once the function returns, or needs to unlock and relock with a different mutex first, or such like that\u2026\nClarification: I'm not trying to be mean here, or suggest that the typo should make anyone feel bad. The number of times I've done a defer mu.Lock() is pretty amazing. I'm just wondering why the defer mu.Unlock() pattern wasn't being applied in either the original, nor in this bugfix.\n. The question need be asked, is this an error in the encoders, decoders, or gRPC not taking into account something beyond their control?\nIn house at Google, generally something like this wasn't viewed to be a big issue, because everyone is sending good traffic (or they should prepare to write a postmortem), so \u201ccrash the server when you can't report an error\u201d is entirely valid behavior.\nBut outside of Google, yeah, that assumption doesn't necessarily hold. But I'm curious as to what the server could do other than ignore the request\u2026 or send an error response of \u201cbad input\u201d.\n. This article seems relevant: http://www.joelonsoftware.com/articles/Unicode.html\nNot that I'm a pillar of never-making-a-mistake\u2026\n. I don\u2019t find this documentation actually any more clear. I think more harnessing of the example might be important. Where is this code snippet supposed to go? How am I supposed to get yourMux? etc.\nIt still seems like the code is written towards people who already know how the functionality works, and doesn\u2019t need all the \u201cmagic\u201d spelled out for them.. I don\u2019t think putting said explanation here or in a FAQ would be documenting the code clearly.. I like the documentation a lot more now. :+1: . https://golang.org/ref/spec#Making_slices_maps_and_channels\nmake(T, n)       slice      slice of type T with length n and capacity n\nmake(T, n, m)    slice      slice of type T with length n and capacity m\nYou should never be using make([]type, arraylen, arraylen)\n. https://talks.golang.org/2014/names.slide#4\nA good name is:\nConsistent (easy to guess),\nShort (easy to type),\nAccurate (easy to understand).\n. Metadata is a receiver function of the Server type. Adding \u201cServer\u201d to the beginning of the name is likely to only cause studdering:\ne.g.:\nserver := GetServer()\nmeta := server.ServerMetadata() // bad form\n. \u201cmeta\u201d is fine. You know it's the metadata. What other kind of \u201cmeta\u201d would it be?\nIt's also an unexported value, which means brevity should trump descriptiveness. (see the vars immediately above, where md is a map of MethodDesc and sd is a map of StreamDesc.)\n. Yes, meta is a prefix in English. \u201cMetadata\u201d is also really the only kind of \u201cmeta\u201d that is ever used in computer programming.\nAs such the name is clear and unambiguous. Note \u201cmd\u201d not \u201cmdesc\u201d or \u201cmethods\u201d or anything else more descriptive.\n\u201cm\u201d is too short, because it's not descriptive enough. \u201cmeta\u201d is a perfect choice here.\n. Oops, missed the \u201cService\u201d vs \u201cServer\u201d distinction. Disregard my comments then.\n. From: https://github.com/golang/go/search?utf8=%E2%9C%93&q=meta\n// hasMeta reports whether path contains any of the magic characters\n// recognized by Match.\nfunc hasMeta(path string) bool {\n    // TODO(niemeyer): Should other magic characters be added here?\n    return strings.ContainsAny(path, \"*?[\")\n}\n232 func TestMatchGoImport(t *testing.T) {\n233     tests := []struct {\n234         imports []metaImport\n235         path    string\n236         mi      metaImport\n237         err     error\n238     }{...\nhttps://golang.org/pkg/regexp/#QuoteMeta\nfunc QuoteMeta(s string) string\nQuoteMeta returns a string that quotes all regular expression metacharacters inside the argument text; the returned string is a regular expression matching the literal text. For example, QuoteMeta([foo]) returns \\[foo\\].\nNow, while you all are looking up what \"meta\" means in a dictionary, I am a native speaker of English, and a former Google SRE who fixed some code in Google's internal Go RPC library.\nSince \"md\" is already taken up by a map of MethodDescs, \"meta\" is the appropriate name for the unexported variable here.\n. Ok, the link you posted has exported values with the name Metadata.\nPertaining directly to this question is this:\nhttps://github.com/golang/tools/blob/5a2fc32f4475381b7c41823909c5ec098f07c040/godoc/meta.go\n40 // If no metadata is present the original byte slice is returned.\n41  //\n42  func extractMetadata(b []byte) (meta Metadata, tail []byte, err error) {\n43      tail = b\n44      if !bytes.HasPrefix(b, jsonStart) {\n\u2026 \n52      if err = json.Unmarshal(b, &meta); err != nil {\n53          return\n54      }\n55      tail = tail[end+len(jsonEnd):]\n56      return\n57  }\nExported variable? Yes, Metadata. Unexported value? concise weighs out pedantic spelling things out: meta.\nAddendum: hah, the critically important line is line 42. :)\n. The way you're describing this function, I'm curious why it is an exported function?\nDo we ever expect importers to call this function? Should they ever call it? Is the only real thing an importer should be caring about is if return value == nil?\nIf the later, wouldn't it be better to use an exported method like IsValidService, or IsSupportedService, etc?\n. srv, ok := s.m[service]\nif !ok {\n  return nil\n}\nGo tends to prefer the \u201cerror out if you possibly can early\u201d in order to keep the number of scope levels as small as reasonably possible.\n. Ah yes, that would make sense then. At the very least the function's documentation should likely say something about what the intended audience is. Off the top of my head, \u201cfunction is used in other gRPC modules, and thus must be exported, it is however not intended for general use.\u201d\nThis is just an example though! You certainly want to use better/clearer/considered language. ;)\n. ",
    "tamird": "This is already fixed in 1.7 https://github.com/golang/go/issues/12294\n. > If it does not fit your requirements, it is trivial to provide your own custom authenticator impl to grpc.\nHow would one provide a custom authenticator impl?\n. This still seems unusable for the TLS case. TLSInfo.state is unexported, how can any authentication be done based on information contained in it?\n. That seems wrong. What is the point of TLSInfo.state? It's not used internally and not exported.\nEDIT: TransportAuthenticator is also the wrong level, because it precludes the possibility of per-method authentication unless you stash random state into your Context.\n. Right, but this is a problem for folks who have e.g. forked glog. This change promotes StdLogger to the default logger. Users should have no problem using glog if they so choose.\n. It is a problem for some users and is impossible to work around.\n. @dsymonds I disagree that this makes it harder to redesign the package - inevitably there will be a package which defines the necessary interfaces, and an auxiliary package which implements them for oauth. Getting this separation in now ensures that the eventual refactor does not neglect this concern.\n. Ping\n. We would like to experiment with grpc in https://github.com/cockroachdb/cockroach, and we will be using mutual-TLS rather than oauth.\n. To my knowledge, the oauth dependency is not actively harmful; this is stated in the original description. That said, I'd like to keep unused dependencies in my application to a minimum; this is just a personal choice, and there will be others who will feel similarly as grpc gains adoption. The status quo allows no workaround.\nThis proposal is pretty light - those who need the oauth support get it with a single import statement.\n. @adg from #220:\n\nTo avoid breaking bigtable, we can make this change in 3 stages:\n- add a new empty package credentials/oauth/oauth\n- import that package in bigtable\n- merge this PR\n\nWould you be open to my contributing these changes?\n. @dsymonds would you sign off on https://github.com/grpc/grpc-go/pull/220?\n. @dsymonds https://github.com/grpc/grpc-go/pull/237\n. grpc/credentials/oauth is an optional dependency, and interop is used only for testing I think. If you're just greping to find dependencies, you're doing it wrong.\n. @awalterschulze optional dependency means that if you import \"google.golang.org/grpc\" in your application, the optional dependency will not be linked in. If you're trying to prevent go get from downloading it, well, that's a lost cause AFAIK.\nAs far as running your dependencies' tests: not sure what that has to do with anything. Run them if you like.\n. Er, I meant to ping here.\n. To avoid breaking bigtable, we can make this change in 3 stages:\n- add a new empty package credentials/oauth/oauth\n- import that package in bigtable\n- merge this PR\n@dsymonds @iamqizhao what do you think?\n. @dsymonds does the above sound reasonable to you?\n. @dsymonds this should be ready whenever you are.\n. @dsymonds ping\n. @iamqizhao this should be safe to merge now that https://github.com/GoogleCloudPlatform/gcloud-golang/commit/95c332f94e6766fa122231c0a95ddf15ac26bf70 is in.\n. @dsymonds this is the non-breaking addition we discussed in #219.\n. @dsymonds should be ready for another look.\n. @iamqizhao should be good to go.\n. @iamqizhao it doesn't seem to be possible to interact with ClientConn from the server side, which is necessary; is that correct?\n. Without server side connection tracking it's not possible to implement e.g. rate limiting. Is it possible to work around this in some way that I'm not aware of?\n. Are you suggesting a specific rate limiting API? What about graceful shutdown? Will you implement a separate API for each feature that requires a callback when connections change state?\n. Great! That's exactly what ConnState is. Are you planning to add this hook?\n. @iamqizhao PTAL\n. @iamqizhao addressed your comments.\n. @bradfitz where do you pass the context?\nEDIT: to clarify: outgoing RPCs take a context argument, but the server side doesn't have an API for setting/mutating it, as far as I can tell.\n. Ah, yes. Interrupting Recv on either client or server is not possible without tearing down the stream.\nStill, how do you even cancel the stream's context? How do you get the cancel function?\n. Right, that's on the client. How about on the server?\n. Cancelling from the client can be done by cancelling the RPC context you\npassed in.\nCancelling from the server can be done by returning from the handler\nmethod. That means you'll need to call Recv() from a different goroutine.\nOn Nov 11, 2016 03:44, \"Erik Frey\" notifications@github.com wrote:\n\nSimilarly, I'd love to be able to cancel a ServerStream SendMsg from a\nclient that's hung, but it's not clear how.\nI was excited to find that I could grab the underlying transport stream by\ncalling StreamFromContext but it seems you've purposefully chosen not to\nexpose the cancel func.\nCanceling a misbehaving client seems like a common need - what's the right\nway to do this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/465#issuecomment-259909212, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPB5zqwq_ompWtlaruOuzyY9AaN2Bks5q9CragaJpZM4G1fLa\n.\n. @bradfitz, @iamqizhao: https://github.com/cockroachdb/cockroach/pull/4080 is currently blocked on this. What can I do to help get this over the finish line?\n. Understood, thanks. Cockroachdb doesn't currently run in \"production\", so\nwe're good with taking some risks :)\nOn Feb 8, 2016 16:04, \"Qi Zhao\" notifications@github.com wrote:\n@tamird https://github.com/tamird\nThanks for early adopting. We will check this in ASAP. It may take a bit\nmore time because it is a big change. Notice that this change directs your\ntraffic onto a new transport which has not been exercised much in grpc\nworld (yes, http2 package is heavily tested and it passes all end2end tests\nin grpc). So take your own risk here if you want to use it in prod.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/514#issuecomment-181539926.\n. You're probably right, but that's to be expected; I have created the ClientConn with a context.WithCancel, then cancelled the context while another goroutine tried to Send() on the ClientConn. This use is race-free in the race detector sense, but it will produce this error, which is just needlessly hard to compare to ErrConnClosing.\n. My code is attempting not to log these expected errors:\n\nerr := stream.Send(reply)\nif err != nil {\n    if !grpcutil.IsClosedConnection(err) {\n        log.Error(err)\n    }\n    return\n}\nwhere\n// IsClosedConnection returns true if err is an error produced by gRPC on closed connections.\nfunc IsClosedConnection(err error) bool {\n    if err == transport.ErrConnClosing {\n        return true\n    }\n    if grpc.Code(err) == codes.Canceled {\n        return true\n    }\n    if streamErr, ok := err.(transport.StreamError); ok && streamErr.Code == codes.Canceled {\n        return true\n    }\n    return false\n}\nSo the wrapping in rpcError is causing me to log an error where I do not expect to log one.\n. It's hard to give a small self-contained snippet, but the usage is here: https://github.com/cockroachdb/cockroach/pull/4080 - the last commit adds this logging suppression.\n. As far as I can tell, there are two categories:\n- RPC method returns before a call to stream.{Send,Recv} - this can happen in a bidirectional stream when the two directions are being processed asynchronously. we want to ignore this error.\n- context is cancelled before a call to stream.{Send,Recv}. we want to ignore this error.\n. These errors only occur during shutdown; our bidirectional streaming RPCs never otherwise terminate.\n. This was fixed by the combination of b5071124 and a3592bda22af2816dac989ab6ccab42f45223455.. You'll still get the reason, it just won't be wrapped in an rpcError.\n. For the same reason that ErrConnClosing is exported and io.EOF is not wrapped; I closed the conn - I meant to do that, and a subsequent Send() failed - that's expected. I'm using context cancellation to signal to the sender goroutine that it's time to stop; that's not an RPC error.\n. b5071124 + a3592bda22af2816dac989ab6ccab42f45223455 essentially amount to this behaviour already, so this can just be closed.. LGTM\n. :ship: \n. @iamqizhao you should probably cancel this build: https://travis-ci.org/grpc/grpc-go/builds/107889107 since it's for the first commit in this PR\n. LGTM\nOn Feb 9, 2016 14:19, \"Qi Zhao\" notifications@github.com wrote:\n\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/grpc/grpc-go/pull/538\nCommit Summary\n- Improve an error message\nFile Changes\n- M transport/http_util.go\n  https://github.com/grpc/grpc-go/pull/538/files#diff-0 (2)\nPatch Links:\n- https://github.com/grpc/grpc-go/pull/538.patch\n- https://github.com/grpc/grpc-go/pull/538.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/538.\n. LGTM.\n\nLeveled logging would be pretty helpful here.\n. Huh? rpc-bench doesn't benchmark grpc.(*Server).ServeHTTP at all.\n. Doesn't seem relevant.\n. Drawing any conclusions about the performance of ServeHTTP from that is not sound. Yes, perhaps the transport is shared, but the remaining 80% of the code is not, since it uses our (cockroachDB's) home-grown protobuf-based RPC codec.\nPlease do not use this as a reference point.\n. No, they are not the same. ProtoHTTP2 is using http://godoc.org/github.com/cockroachdb/cockroach/rpc/codec.\n. I've updated https://github.com/cockroachdb/rpc-bench to include grpc.(*Server).ServeHTTP and indeed the numbers look pretty bad compared to grpc.(*Server).Serve. See https://github.com/cockroachdb/rpc-bench/pull/9\n. @iamqizhao yes I did. However, I added locking to attempt to mitigate this, and still saw data races. Here's the race detector output:\n```\nWARNING: DATA RACE\nRead by goroutine 180:\n  runtime.slicecopy()\n      /Users/tamird/src/go1.5/src/runtime/slice.go:110 +0x0\n  golang.org/x/net/http2.(Framer).WriteData()\n      /Users/tamird/src/go/src/golang.org/x/net/http2/frame.go:576 +0x520\n  golang.org/x/net/http2.(writeData).writeFrame()\n      /Users/tamird/src/go/src/golang.org/x/net/http2/write.go:97 +0xc3\n  golang.org/x/net/http2.(*serverConn).writeFrameAsync()\n      /Users/tamird/src/go/src/golang.org/x/net/http2/server.go:736 +0x8b\nPrevious write by goroutine 176:\n  runtime.slicecopy()\n      /Users/tamird/src/go1.5/src/runtime/slice.go:110 +0x0\n  bufio.(Writer).flush()\n      /Users/tamird/src/go1.5/src/bufio/bufio.go:568 +0x399\n  bufio.(Writer).Flush()\n      /Users/tamird/src/go1.5/src/bufio/bufio.go:551 +0x3a\n  golang.org/x/net/http2.(responseWriter).Flush()\n      /Users/tamird/src/go/src/golang.org/x/net/http2/server.go:2172 +0xf8\n  google.golang.org/grpc/transport.(serverHandlerTransport).Write()\n      /Users/tamird/src/go/src/google.golang.org/grpc/transport/handler_server.go:225 +0x12d\n  google.golang.org/grpc.(serverStream).SendMsg()\n      /Users/tamird/src/go/src/google.golang.org/grpc/stream.go:383 +0x454\n  github.com/cockroachdb/cockroach/gossip.(gossipGossipServer).Send()\n      /Users/tamird/src/go/src/github.com/cockroachdb/cockroach/gossip/gossip.pb.go:210 +0x91\n  github.com/cockroachdb/cockroach/gossip.(server).Gossip()\n      /Users/tamird/src/go/src/github.com/cockroachdb/cockroach/gossip/server.go:189 +0xea5\n  github.com/cockroachdb/cockroach/gossip._Gossip_Gossip_Handler()\n      /Users/tamird/src/go/src/github.com/cockroachdb/cockroach/gossip/gossip.pb.go:196 +0xfe\n  google.golang.org/grpc.(Server).processStreamingRPC()\n      /Users/tamird/src/go/src/google.golang.org/grpc/server.go:583 +0x612\n  google.golang.org/grpc.(Server).handleStream()\n      /Users/tamird/src/go/src/google.golang.org/grpc/server.go:658 +0x150a\n  google.golang.org/grpc.(Server).serveStreams.func1.1()\n      /Users/tamird/src/go/src/google.golang.org/grpc/server.go:323 +0xad\n``\n. I never pushed that version (which doesn't do concurrent)Send`s.\nIt's pushed now https://github.com/cockroachdb/cockroach/commit/9df62b804ea7ed8697307029a2a3b5f19d53f7ad\n. Seems reasonable.\n. Confirmed this change fixes the issues seen in https://github.com/cockroachdb/cockroach/pull/4080. Thanks @bradfitz!\n. At the very least, this is a documentation issue.\nOn Feb 16, 2016 03:55, \"Qi Zhao\" notifications@github.com wrote:\n\nNon-TLS case needs some cmux type of connection dispatching solution we\ndesigned but not implemented.\nBut in general, if your system allows insecure connections, I do not see\nthe reason you need to stick to a single port shared by http2 and grpc. You\ncan have a grpc server and http server listening on 2 different ports in\nthis case.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/555#issuecomment-184582109.\n. @iamqizhao can you reopen this please? It is actually possible to make this work with some hacks https://github.com/cockroachdb/cockroach/commit/c036c1503cdb3756da4f5c911b77253e62706a45#diff-4bf1ae5b9eb22814a15582886403053f, however Go 1.6 contains a change that rejects insecure gRPC connections before they even get to the handler https://github.com/golang/go/commit/6e11f45ebdbc7b0ee1367c80ea0a0c0ec52d6db5, which breaks this again.\n\ncc @bradfitz \n. OK, but is closing the issue really appropriate? You don't have to fix it\nright now, but it's not fair to say that the use case is illegitimate.\nOn Wed, Feb 17, 2016 at 8:37 PM, Qi Zhao notifications@github.com wrote:\n\nPlease provide your use case to justify why your system cannot use 2 ports\nin the insecure case if you think we should pursue this further. It is not\nthat hard to implement cmux in grpc but we want to focus on the real\ndemands given the limited resource from our side. I think insecure port\nsharing is a really rare case and not needed by >99% grpc users right now.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/555#issuecomment-185499151.\n. Was this issue addressed somewhere?\n. > (*Conn).Close is asynchronous, so even if I try to force-close these connections during shutdown, they still end up leaking and failing my tests.\n\nYou replied to this with \"I plan to improve it\". Has the improvement been implemented?\n. LGTM\n. Yeah, there is no circular dep, but it's odd for a project to import both gogo/protobuf and golang/protobuf.\nThis is low priority, but deserves to be tracked.\n. What's the issue with MetaHeadersFrame? No, grpc does not depend on 1.7.\nHave you tried go get -u google.golang.org/grpc/...?\n. grpc.(*Server).ServeHTTP\n. I don't think we've seen it in some time.. LGTM\n. @bradfitz any progress on this?\n. @iamqizhao this is a serious issue - can we include it for GA?\n. This is still a major inconvenience for us. I've updated the performance numbers at https://github.com/cockroachdb/rpc-bench; the relevant ones are:\n```\nname                         time/op\nGRPCServe_1K-24                40.5\u00b5s \u00b1 1%\nGRPCServe_64K-24                365\u00b5s \u00b1 1%\nGRPCServe_Stream_1K-24         22.7\u00b5s \u00b1 1%\nGRPCServe_Stream_64k-24         298\u00b5s \u00b1 1%\nGRPCServeHTTP_1K-24             137\u00b5s \u00b1 1%\nGRPCServeHTTP_64K-24            585\u00b5s \u00b1 1%\nGRPCServeHTTP_Stream_1K-24     38.7\u00b5s \u00b1 1%\nGRPCServeHTTP_Stream_64k-24     455\u00b5s \u00b1 1%\nname                         speed\nGRPCServe_1K-24              50.5MB/s \u00b1 1%\nGRPCServe_64K-24              359MB/s \u00b1 1%\nGRPCServe_Stream_1K-24       90.3MB/s \u00b1 1%\nGRPCServe_Stream_64k-24       439MB/s \u00b1 1%\nGRPCServeHTTP_1K-24          14.9MB/s \u00b1 1%\nGRPCServeHTTP_64K-24          224MB/s \u00b1 1%\nGRPCServeHTTP_Stream_1K-24   52.9MB/s \u00b1 1%\nGRPCServeHTTP_Stream_64k-24   288MB/s \u00b1 1%\nname                         alloc/op\nGRPCServe_1K-24                16.3kB \u00b1 0%\nGRPCServe_64K-24                711kB \u00b1 0%\nGRPCServe_Stream_1K-24         11.6kB \u00b1 0%\nGRPCServe_Stream_64k-24         707kB \u00b1 0%\nGRPCServeHTTP_1K-24            34.2kB \u00b1 0%\nGRPCServeHTTP_64K-24            763kB \u00b1 0%\nGRPCServeHTTP_Stream_1K-24     12.4kB \u00b1 0%\nGRPCServeHTTP_Stream_64k-24     739kB \u00b1 0%\nname                         allocs/op\nGRPCServe_1K-24                  97.0 \u00b1 0%\nGRPCServe_64K-24                  147 \u00b1 0%\nGRPCServe_Stream_1K-24           22.0 \u00b1 0%\nGRPCServe_Stream_64k-24          72.0 \u00b1 0%\nGRPCServeHTTP_1K-24               169 \u00b1 0%\nGRPCServeHTTP_64K-24              275 \u00b1 0%\nGRPCServeHTTP_Stream_1K-24       29.0 \u00b1 0%\nGRPCServeHTTP_Stream_64k-24       138 \u00b1 0%\n```\nSo ServeHTTP's throughput is 50%-70% lower than Serve's, and its latency is 60%-350% higher.. Updates #586.\n. Some of this PR was extracted into #588, which should be more digestible.\nI'll rebase this once that PR is decided, and it will contain just one\ncommit.\nOn Mar 28, 2016 19:20, \"Brad Fitzpatrick\" notifications@github.com wrote:\n\nMy comments were deleted when this pull request was updated. Because\nGithub totally sucks for code review.\nI think we should switch gRPC 100% to Gerrit at some point.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/587#issuecomment-202622790\n. Closing this for now.\n. Updated; just one commit here now.\n. @iamqizhao ping\n. Huh? Looks up-to-date to me.\n. @iamqizhao ping.\n. @iamqizhao ping\n. Closed #587. Thoughts on this PR?\n. OK, this is just one commit now.\n. Why can't the c++ client be used?\n. Sure, it matters. It makes it impossible to make a certain class of mistake since it will now be caught by the compiler. It's just tidying up, but that's worthwhile.\n. @iamqizhao anything more I can answer here?\n. +1 on this change. misleading comments are misleading.\n. There are some cases (like ours) where there is no load balancer in the\npicture. There should be some way of treating this error as non\ntransient, even if it's not the default behaviour.\n\nOn Thu, Mar 31, 2016 at 10:03 PM, Qi Zhao notifications@github.com wrote:\n\nI can imagine that in some cases the name resolver or load balancer may\ndirect you to another server which can accepts the cert. I am still\ndebating with myself whether I should introduce some more tweaks into the\nconnection errors.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/622#issuecomment-204207188\n. @iamqizhao ping\n. @iamqizhao what's up with this?\n. ii) is bogus. On a bad certificate, the chance that the next RPC succeeds\nis approximately zero.\n\nOn Jul 18, 2016 14:05, \"Qi Zhao\" notifications@github.com wrote:\n\n@stevvooe https://github.com/stevvooe The thing becomes a bit tricky\nwhen there may be multiple underlying network connections. @menghanl\nhttps://github.com/menghanl will revise the PR (or in another separate\nPR) so that if WithBlock is not set:\ni) If the rpc is fail-fast, it fails immediately with the fatal\ncertificate error;\nii) if the rpc is not fail-fast, the rpcs keeps waiting since the next\naddress(i.e., a new server) from the name resolver/balancer might save the\nworld.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/622#issuecomment-233408934, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPINuMImE5G5ZJzZYUYt3TPeuuwk5ks5qW8BRgaJpZM4H8-3F\n.\n. It's not true that the problem is typically on the server. If the server\nrejects the client's certificate, it is almost certainly true that the\nclient is at fault, and the RPC will never succeed.\n\nOn Jul 18, 2016 14:36, \"Qi Zhao\" notifications@github.com wrote:\n\n@tamird https://github.com/tamird I think typically this kind of error\nis on the server side and there is some chance a newly started server\nfixing the problem. Technically, a non-fail-fast rpc never fails with a\nsingle connection error regardless it is fatal or not.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/622#issuecomment-233418002, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPOSgAK1wPahOWsMKLGcJHsLwpZukks5qW8eSgaJpZM4H8-3F\n.\n. Confirmed that this is fixed by buffering the writes on c1:\n\n``` diff\ndiff --git a/main.go b/main.go\nindex 99c0fdf..7012afa 100644\n--- a/main.go\n+++ b/main.go\n@@ -1,6 +1,7 @@\n package main\nimport (\n+    \"bufio\"\n     \"fmt\"\n     \"log\"\n     \"net\"\n@@ -35,6 +36,15 @@ func (ln *memListener) Close() error {\n     return nil\n }\n+type bufferedConn struct {\n+    net.Conn\n+    *bufio.Writer\n+}\n+\n+func (bc bufferedConn) Write(b []byte) (int, error) {\n+    return bc.Writer.Write(b)\n+}\n+\n func main() {\n     grpc.EnableTracing = true\n@@ -54,7 +64,7 @@ func main() {\n         grpc.WithDialer(func(_ string, _ time.Duration) (net.Conn, error) {\n             c1, c2 := net.Pipe()\n             log.Printf(\"Pipe created: %v %v\", c1, c2)\n-            ln.c <- c1\n+            ln.c <- bufferedConn{c1, bufio.NewWriter(c1)}\n             log.Printf(\"Pipe accepted: %v %v\", c1, c2)\n             return c2, nil\n         }))\n```\n. your commit message is wrong; you have changed it to a pointer receiver.\nLGTM\n. Include the test from #639?\n. Yeah, the client is the grpc-go client. Honestly, I have no idea how to repro.\nWe could potentially discuss in the Cockroach issue.\n. cc @mberhault\nOn Fri, May 6, 2016 at 2:24 PM, Qi Zhao notifications@github.com wrote:\n\nAre there any rpc cancellation/timeout/failure along with this symptom?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/670#issuecomment-217521211\n. Yikes, no test?\n. \n\n\nPreviously, bdarnell (Ben Darnell) wrote\u2026\n\n\n\n> #### Fix a listener leak when a server is stopped before it starts\n> \n> If server.Stop() and server.Serve() race, Serve() can return without\n> \n> closing the listener. This in turn can lead to clients timing out trying\n> \n> to connect to a server that is neither accepting nor rejecting\n> \n> connections.\n\nReviewed 2 of 2 files at r1.\nReview status: all files reviewed at latest revision, 2 unresolved discussions, some commit checks failed.\n\nserver_test.go, line 51 [r1] (raw file):\n\nGo\n  server := NewServer()\n  server.Stop()\n  err = server.Serve(lis)\n\nscope this more narrowly?\n\nserver_test.go, line 70 [r1] (raw file):\n\nGo\n      }\n  case <-time.After(time.Second):\n      t.Errorf(\"timed out waiting for Accept() to fail\")\n\nnit: timed out waiting for accept to return\n\nComments from Reviewable\n Sent from Reviewable.io \n. @menghanl FYI @JohanSJA will not be able to reopen this issue.\n. Consider a simple test.\n. Why'd you abandon this?\n. I think we should not regenerate all the .pb.go files before testing.\n\nThe .pb.go files are checked into the repo. If they are regenerated and overwritten, we are not doing test on the checked-in code.\n\nThat's true, but the build will fail if the regenerated protos do not match the checked-in protos.\n\nAnd it also introduces latency to the test.\n\nTrue, but isn't it worth it?\n. No, they will fail, because '! git status --porcelain | read || (git status; git diff; exit 1)' will exit non-zero.\n. Good point, I forgot to think about dependency pinning. I'll see about making this a separate build which Travis allows to fail - should give us the best of both worlds.\n. OK, this is ready for another look!\n. @iamqizhao @menghanl ping?\n. @iamqizhao thanks, green now.\n. Oh! It found bugs.\n. Still green :)\n. @iamqizhao any interest in this?\n. @iamqizhao ping.\n. @menghanl rebased, could you have a look?. @dfawley done.. I spoke too soon; regenerating the protos causes an import cycle. I suppose I'll employ some sed hacks to get around this.. @dfawley OK, I found the proper sed hacks, this should be good to go now.. Sorry, haven't had time to circle back on this. I'll try to get to it in\nthe next week or so.\nOn Fri, Jun 9, 2017 at 1:47 PM, dfawley notifications@github.com wrote:\n\n@tamird https://github.com/tamird, any thoughts on my comments?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/721#issuecomment-307495689, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPCUdaQU3YdwizTZdVPhmH-9D9Yfeks5sCa9YgaJpZM4IydgJ\n.\n. #779 would have been prevented by this PR.\n. Should be closed now that #840 is merged.\n. Can you add go vet to CI?\n. Here's a version of this change that saves a goroutine:\n\n``` diff\ndiff --git a/clientconn.go b/clientconn.go\nindex 9b9c78d..455dbb3 100644\n--- a/clientconn.go\n+++ b/clientconn.go\n@@ -248,30 +248,36 @@ func Dial(target string, opts ...DialOption) (*ClientConn, error) {\n            return nil, errNoAddr\n        }\n    }\n-   waitC := make(chan error, 1)\n-   go func() {\n-       for , a := range addrs {\n-           if err := cc.newAddrConn(a, false); err != nil {\n-               waitC <- err\n-               return\n-           }\n-       }\n-       close(waitC)\n-   }()\n+   errCh := make(chan error, len(addrs))\n+   for , a := range addrs {\n+       go func(a Address) {\n+           errCh <- cc.newAddrConn(a, false)\n+       }(a)\n+   }\n    var timeoutCh <-chan time.Time\n    if cc.dopts.timeout > 0 {\n        timeoutCh = time.After(cc.dopts.timeout)\n    }\n-   select {\n-   case err := <-waitC:\n-       if err != nil {\n+   errs := make([]error, 0, len(addrs))\n+retry:\n+   for range addrs {\n+       select {\n+       case err := <-errCh:\n+           if err != nil {\n+               errs = append(errs, err)\n+           } else {\n+               break retry\n+           }\n+       case <-timeoutCh:\n            cc.Close()\n-           return nil, err\n+           return nil, ErrClientConnTimeout\n        }\n-   case <-timeoutCh:\n+   }\n+   if len(errs) == len(addrs) {\n        cc.Close()\n-       return nil, ErrClientConnTimeout\n+       return nil, fmt.Errorf(\"%v\", errs)\n    }\n+\n    if ok {\n        go cc.lbWatcher()\n    }\n```\n. lgtm. needs a test\n. lgtm\n. I've added a commit that plumb network through all the dialing methods which allows the end2end tests to be re-enabled. PTAL.\nThis is unfortunately a breaking change.\nAlso, tests are failing in Go 1.5 because Dialer.Cancel doesn't exist.\n. PTAL. Updated to do this with a smaller API break and also made it backwards compatible with Go1.5.\n. @iamqizhao this is green. Any additional thoughts?\n. @iamqizhao ping.\n. Sigh. transport tests failed because the default dialer is now set in grpc, not transport.\n. Well, I had to extract a dialer package :(\nPTAL.\n. I think you're being a bit too religious about not introducing a new API here. What's wrong with a helper method? It's simple sugar, but almost certainly will be useful to many users.\n. OK. I've just left the end-to-end test unchanged in this regard (it doesn't use the cancellation).\n. @iamqizhao I believe I've addressed all your comments. No new APIs.\n. Something strange is going on in travis on 1.6. I'm investigating.\n. @iamqizhao huh, this is green now. PTAL\n. @iamqizhao can you please take a look? This is blocking some work in CRDB.\n. This PR was opened 23 days ago and received 51 comments before yours. Some relevant discussion is also ongoing in https://github.com/grpc/grpc-go/issues/758.\nTo my knowledge there was no way to do this without breakage, though I'm happy to hear your suggestions.\nReverting this is heavy handed.\n. Oh, you just want WithDialer to be unchanged? OK, easy enough.\nThe effect will be that external users don't get to use cancellation.\n. Yeah, I'm just going to leave the external interfaces alone for now (i.e. revert back to pre-this PR).\nhttps://github.com/grpc/grpc-go/pull/789\n. > There are some use cases which requires some special dialing ways (e..g, dialing from a GAE) that cannot be achieved by net.Dialer.\nOK, let's discuss that in #751. In particular, what special dialing requirements are you thinking of?\n\nAs a rpc lib, users usually typically only has a service DNS name and do not know what network they should use. This makes it different from net.Dial case. This also leaves the possibility grpc internals can choose the optimal network type for the users in the future (e.g., for intra-cluster traffic, use RDMA, for inter-cluster traffic, use TCP, etc.). Maybe we can add a grpc.DialNet API for you if you really want to specify network.\n\nSeems like putting the cart before the horse. Take a look at some of the tests which already have to jump through hoops to get unix sockets working.\n\ncontext is request scoped and I am not convinced it applies to connection. In addition, I think it introduces the confusion to the users.\n\nIs it? Go 1.7 introduced net.DialContext, so it seems that upstream disagrees with you. Perhaps @bradfitz could provide some guidance.\n. Sounds good. I've updated #751 and I'll let you know when I've had a chance to explore a context-based enhancement.\n. One more thing: after spending some time in this code with #796, I'm realizing that grpc is a soup of channels and extremely hard to reason-about cancellation propagation.\nWe should refactor this pretty extensively to use context-based cancellation so that it is propagated properly instead of the current soup of parents having to signal closure to their children (e.g. ClientConn -> addrConn -> transport, Server -> transport).\n. No, this is not the correct fix. This copies a mutex.\n. LGTM\n. This is green; let's get it merged plz.\n. Done (at least I tried). Let's see what it does.\n. Green.\n. heh, merge conflict\n. You're welcome! Sorry for the breakage. Package management grumble grumble.\n. cc @broady in case my assumptions about breaking users of TransportCredentials are incorrect.\n. @iamqizhao this is green - could you take a look?\n. @iamqizhao now that we are on latest grpc in CockroachDB, we are seeing a ton of leaked goroutines related to this problem: https://github.com/cockroachdb/cockroach/issues?utf8=%E2%9C%93&q=is%3Aissue%20is%3Aopen%20leaked%20goroutine%20google.golang.org%2Fgrpc%2Fcredentials.(*tlsCreds).ClientHandshake\n. @iamqizhao can you please take a look at this?\n. https://github.com/cockroachdb/cockroach/issues/8136#issuecomment-236372997\n. By the way, even without the particular bug here, it is a good idea to pass a context around as this PR does. There's a ton of reinventing-the-wheel in grpc-go - we can do a lot of cleanup using contexts for cancellation rather than passing around timeout parameters or homegrown cancellation channels.\n. Yeah, I think we can hold off on engaging the go team for now. Once 1.7 is out we'll update our CI stuff and see if the problem still exists.\n. So this is good to go?\n. LGTM\n. Possible related to #802.\n. No, we need to reopen this (and I can't). The failures we were seeing were on 5a547ed72c09faf4050739d54caea1d51f4d276d which did not include #799 at all.\n. I haven't been able to :(\nI wrote up my attempts https://github.com/cockroachdb/cockroach/issues/8133#issuecomment-236373835.\n. Sure, https://github.com/cockroachdb/cockroach/pull/8150.\nOur stress tests run nightly - I'll let you know what happens tomorrow night.\n. I am able to reproduce this using grpc's tests using https://github.com/cockroachdb/stress!\ngo test google.golang.org/grpc/test -c && stress -maxfails 1 -stderr ./test.test -test.run TestServerGoAway\nReproduces in under 30 seconds both in go1.6.3 and go1.7rc3.\n. No problem. And all I ask in return is that you review\nhttps://github.com/grpc/grpc-go/pull/796 :)\nOn Sat, Jul 30, 2016 at 7:35 PM, Qi Zhao notifications@github.com wrote:\n\nYeah, I found the root cause and have a fix now. will give it another\ncareful check and send out on Monday. Thanks again for your input.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/804#issuecomment-236396247, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPK5Er8-aBBqesLdQbB7Ng06yD16gks5qa9_MgaJpZM4JY2Hs\n.\n. You seem to have checked in a binary file.\n. LGTM modulo the superfluous nil check in the test.\n. Might be good to squash this down when merging - this is a lot of commits.\n. go get -u google.golang.org/grpc/...\n. Yes, welcome to Go's terrible package management.\n. +1 that retries should happen in case of failure before transmission of the RPC. please do not violate at-most-once.\n. Does this need a test?\n\nOn Fri, Aug 5, 2016 at 1:06 PM, Menghan Li notifications@github.com wrote:\n\n@xiang90 https://github.com/xiang90 @heyitsanthony\nhttps://github.com/heyitsanthony\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/811#issuecomment-237906314, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPMW1lOtNyDodOHTBlxtQX6gaROr4ks5qc22xgaJpZM4Jd4JC\n.\n. LGTM\n. @iamqizhao ping. @alexmullins is correct that the raw connection is leaked if ctx.Done() is closed during tlsCreds.ClientHandshake, and we are now seeing this problem in CockroachDB.\n\ncc @spencerkimball\n. It was discovered in https://github.com/cockroachdb/cockroach/pull/8529.\n. Closing in favour of #826 which includes a repro case.\n. LGTM\n. @menghanl @iamqizhao can you get this in please? It's currently preventing us from debugging an issue in CockroachDB.\n. @menghanl @iamqizhao can you take a look at this, please? It's currently preventing us from debugging an issue in CockroachDB.\n. Hm, my original test was broken - now that I've fixed it, it passes. Still investigating.\n. Ah, the issue was a mistake on our side. I'll leave this PR open in case this test is useful to you.\n. Does this need a test?\n. LGTM\n. @iamqizhao ping\n. @iamqizhao ping.\n. @iamqizhao this is green\n. @iamqizhao this is an easy change and yet it may suffer merge conflicts. Can it be merged, please?\n. @iamqizhao this is green.\n. @iamqizhao ping.\n. @iamqizhao added a test. Beware, the test is slow, because of the TLS handshake timeout being hardcoded to minConnectTimeout (20 seconds), so I've made the test skipped when -short is passed to go test (which is currently never). PTAL.\n. @iamqizhao done.\n. Green again.\n. Just merge #859.\nOn Wed, Aug 24, 2016 at 6:46 PM, Menghan Li notifications@github.com\nwrote:\n\nI created #859 https://github.com/grpc/grpc-go/pull/859 with my\nproposed test. Please take a look. You can cherry-pick the test from there\nif you want.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/846#issuecomment-242232090, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPM9nrSE43YavmPDKD7fYWdE-Cqw_ks5qjMnSgaJpZM4Jo-ij\n.\n. As I mention on the referenced PR, grpc.DialContext is not net.DialContext. The proximate reason for adding grpc.DialContext was to be able to effect the complete shutdown of the grpc.ClientConn via context cancellation. In particular, it is important that cancelling the context passed to grpc.DialContext terminates the goroutine which attempts to reconnect the grpc.ClientConn to its target. Note that net.DialContext does not spawn such a goroutine, and thus the semantics can be different.\n. Is this actually desired? Why?\n. This new contract is also problematic - if the user's passed-in context is\ncancelled, and the transport continues to retry connections, we will see\nthe same goroutine leaks that motivated this addition in the first place.\n\nThis is not net.Dial. net.Dial doesn't have a goroutine servicing the\nresulting connection, while grpc.ClientConn does. If the user wants to\nsupply a connection timeout, that is what grpc.WithTimeout is for.\nOn Tue, Aug 23, 2016 at 10:33 PM, Qi Zhao notifications@github.com wrote:\n\nThe previous one is problematic. For example, if a user pass a context\nwith 1 min timeout, the entire ClientConn becomes not usable after 1 min\nwith the previous impl.\nFollowing the similar concept in net.DialContext, the user-passed context\nshould be valid only during this dialing operation instead of the entire\nlifecyle of ClientConn.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/858#issuecomment-241940990, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPO_wK7SdabVArHPLZOEMlVnmtyNUks5qi62DgaJpZM4Jrkzr\n.\n. Sounds reasonable.\n. What's up with travis?\n. Green.\n. This was recently separately observed in https://github.com/cockroachdb/cockroach/issues/8939. @neilgarb which version of grpc are you on? Does it include https://github.com/grpc/grpc-go/pull/864?\n. What you're describing is definitely a bug. The current error handling is\nstill too heavy handed in its treatment of errors as permanent. @bdarnell\nwrites in\nhttps://github.com/cockroachdb/cockroach/issues/8939#issuecomment-243393233:\n\nThe error handling in GRPC needs to be reworked: instead of treating errors\nfrom ClientHandshake as permanent by default with a whitelist of transient\nerrors (currently io.EOF and context.DeadlineExceeded), it should assume\nthat all errors are retryable (as it does for every operation except\nClientHandshake) and use a blacklist of certificate-related errors to treat\nas permanent failures (or maybe even those should be transient - a\nmisconfigured server could transiently reject valid certificates and it\nshouldn't take a restart of all clients to recover from that).\nOn Aug 30, 2016 07:32, \"Neil Garb\" notifications@github.com wrote:\n\nThanks for your response. I pulled yesterday, so I'm running 79b7c34\nhttps://github.com/grpc/grpc-go/commit/79b7c349179cdd6efd8bac4a1ce7f01b98c16e9b\non both client and server. Still seeing this behaviour.\nMore info: In production (which is using an older version of grpc) I have\na balancer with two endpoints. There I also get connection reset by peer\nwhen I restart a server, but the client seems to sort itself out after a\nshort while.\nMore more info: I'm going to run a test with multiple connections per\nendpoint (something like https://github.com/google/\ngoogle-api-go-client/blob/master/internal/pool.go#L38), so even if one\nendpoint is provided the client establishes multiple connections. Will\nprovide feedback once I'm one.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/870#issuecomment-243411209, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPJGyV12v7b-b1mmiI6ZF2I-ju0AJks5qlBTkgaJpZM4JwQ0_\n.\n. @neilgarb be sure to log with both %T and %+v so you can see the error's type.\n. Hurray paranoid preemptive fixes!\n\nOn Sep 1, 2016 7:44 AM, \"Neil Garb\" notifications@github.com wrote:\n\nSeems resolved - I must just have muddled up my deployment.\nThanks for all the help/info.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/870#issuecomment-244054356, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPPUzCNwrxk5wicJiSBnBkD2kuEaVks5qlrqIgaJpZM4JwQ0_\n.\n. Dupe of #711.\n. does transport.StreamErrorf want to be unexported?\n. What behaviour does this change?\n. test?\n. lgtm\n. Can you add a test for this? Should be relatively easy.\n. https://github.com/grpc/grpc-go/commit/0f1aeede#diff-3081f1aaaf958d7d5e00ccec7a62ea6fL422\n. The correct solution is to implement String() on the structure of interest.\n. I'm not sure I see your point. In this concrete case, you could implement String() on grpc.Address with a type switch that checks if the metadata is a fmt.Stringer before delegating to the default %q behaviour.\n. instead of merging master, can you rebase this? Also, it should all be one commit.\n\nIn general, it is a good idea to avoid \"useless\" commits as they make reading the git log unnecessarily hard, and we often do.\n. LGTM\n. This approach adds an allocation to every logged line \ud83d\ude4d . This seems like a Bad Thing. Paging @bradfitz because disabling tests is not a solution.\n. Note: it is possible to do this with a smaller diff using https://godoc.org/bytes#Buffer.Reset\n. The issue here is not the copy of the slice, but the allocation of the bytes.Reader - Reset allows you to reuse the reader by replacing its backing slice.\n. Ah, yeah, that's the one :)\nOn Fri, Oct 21, 2016 at 9:41 PM, apolcyn notifications@github.com wrote:\n\nah I think I see, it was https://godoc.org/bytes#Reader.Reset rather than\nhttps://godoc.org/bytes#Buffer.Reset :), thanks!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/940#issuecomment-255499637, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPMsByYYWRrhpUssWWByKnSXB4igIks5q2WmsgaJpZM4KdqMy\n.\n. Why is it another object to allocate? You can make recvBufferReader.last a value rather than a pointer.. Ack, thanks for updating.\n\nOn Thu, Dec 22, 2016 at 7:11 PM, apolcyn notifications@github.com wrote:\n\n@tamird https://github.com/tamird for the extra allocation, I was just\nthinking about the per-stream overhead, where the new recvBufferReader is\ncreated per stream. But as noted this can probably be avoided some other\nway if it's necessary, and I don't have any evidence of it being a\npotential issue in that case anyways...\nlatest updates use a Reader.Reset instead of copy functions, small detail\nbut used an extra null check instead of changing constructors.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/940#issuecomment-268918526, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPJ7CqDELkO1gDQC99o_Atu-rSg2nks5rKxGmgaJpZM4KdqMy\n.\n. Why the random removals of cancel() in the tests?\n. LGTM\n\nOn Oct 25, 2016 19:55, \"Qi Zhao\" notifications@github.com wrote:\n\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/grpc/grpc-go/pull/949\nCommit Summary\n- remove context.WithCancel which is not needed\nFile Changes\n- M test/end2end_test.go\n  https://github.com/grpc/grpc-go/pull/949/files#diff-0 (3)\nPatch Links:\n- https://github.com/grpc/grpc-go/pull/949.patch\n- https://github.com/grpc/grpc-go/pull/949.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/949, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPMrT-GT0ZBrB4sTD_OYZ4J6chqBGks5q3pbmgaJpZM4KgnPn\n.\n. LGTM\n. ...so this will silently change behaviour. cool.\n. LGTM\n. Was there a resolution here?. OK, this is green.. @iamqizhao could you take a look?. @menghanl this is rebased and green again.. @menghanl ping.. @dfawley Done. I can't reassign to you because my privileges are insufficient.. @dfawley ping.. Yay! \ud83c\udf89 . Sorry - this was observed with Go 1.7.3 and not associated with a change in\nthe Go version used.\n\nOn Mon, Dec 19, 2016 at 3:59 PM, Brad Fitzpatrick notifications@github.com\nwrote:\n\nI don't know what version of Go you're talking about here. If you find a\nproblem with the net package in Go 1.8 compared to Go 1.7, please file a\nbug at https://github.com/golang/go/issues/new . Or if you find something\nunderdocumented, likewise file a bug.\nI don't know what's changed in gRPC lately to know what's biting you. I\ndon't regularly work on gRPC.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1026#issuecomment-268076622, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPN9WbSeya6Bo_7fnZZlpZ2R2oxHYks5rJvAVgaJpZM4LRHVF\n.\n. @bradfitz note that your issue is not the one I reported here; perhaps it deserves its own issue.. To clarify, the issue I reported here produces returned errors of the form: \"transport: write tcp 10.142.0.38:37617->10.142.0.44:26257: write: connection refused\" (emphasis mine).\n\nThe issue you're reporting here is different, and results in \"ambient\" error logging of the form: \"transport: dial tcp [::]:60765: connect: network is unreachable\" (emphasis again mine).. I believe https://github.com/golang/go/commit/bf0f69220255941196c684f235727fd6dc747b5c fixed this for Go1.9.. This is green. Who's the maintainer of this project now? Please have a look.. @menghanl?. Yeah, it's a minor behaviour change, but the current errors are quite difficult to deal with. Do you think someone is depending on the string representation of this error?. @menghanl so, is this good to go?. Thanks!. Without intending to sound pedantic, the source of improvement here is the removal of the item interface, more than the change from pointer to value. If you had kept the item interface and changed recvMsg to implement it by value, you would not have observed a change in memory allocations.\nThe commit message and PR description should both be amended to reflect the true source of improvement here.\nEDIT: Apologies, the item interface was not removed - but recvMsg no longer implements it. With that addendum, the above is still relevant.. Yes, that's correct, and that's my point - simply changing &recvMsg{} to recvMsg{} without also specializing the recvBuffer to use the concrete type rather than the interface does not improve memory allocation for the reason you mention.\nThe bottom line: this improvement is due to the specialization of recvBuffer from item (an interface) to recvMsg (a concrete type).. Cool, thanks. Please update the commit message as well.. This seems quite error prone. What if a future change forgets to set r.err? Perhaps you could extract most of Read into an inner read method, and have Read call it and set r.err in just one place.. LGTM with a slight preference for squashing these commits down.. Could also use build tags to do the right thing in 1.8.\n1 \u0444\u0435\u0432\u0440. 2017 \u0433. 5:58 \u041f\u041f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \"Russell Bewley\" \nnotifications@github.com \u043d\u0430\u043f\u0438\u0441\u0430\u043b:\n\nRelated to #711 https://github.com/grpc/grpc-go/issues/711\nIf passing a Go 1.7 context to gprc, and it is canceled or it times-out,\ngprc panics with the following message:\npanic: Unexpected error from context packet: context canceled\nI have traced the issue to this function in https://github.com/grpc/grpc-\ngo/blob/master/transport/transport.go#L572:\n// ContextErr converts the error from context package into a StreamError.func ContextErr(err error) StreamError {\n  switch err {\n  case context.DeadlineExceeded:\n      return streamErrorf(codes.DeadlineExceeded, \"%v\", err)\n  case context.Canceled:\n      return streamErrorf(codes.Canceled, \"%v\", err)\n  }\n  panic(fmt.Sprintf(\"Unexpected error from context packet: %v\", err))\n}\nI think a workaround could be to compare error strings, although it is\nerror-prone and frowned upon:\n// ContextErr converts the error from context package into a StreamError.func ContextErr(err error) StreamError {\n  switch err.Error() {\n  case context.DeadlineExceeded.Error():\n      return streamErrorf(codes.DeadlineExceeded, \"%v\", err)\n  case context.Canceled.Error():\n      return streamErrorf(codes.Canceled, \"%v\", err)\n  }\n  panic(fmt.Sprintf(\"Unexpected error from context packet: %v\", err))\n}\nAlternatively, you could remove the panic altogether, since the use of\npanic itself is discouraged, and simply return an error.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1061, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPHzAVHkO3SHRQrzx3AFr1vv27dDdks5rYQ4kgaJpZM4L0gFS\n.\n. Probably related to https://go-review.googlesource.com/c/39490/.. There is something wrong with your Go installation; your standard library is missing methods from 1.8.\n\nI'm not sure how you managed to create this situation, but I'm guessing you're sharing GOROOT with an earlier version of Go.. Use a steaming RPC?\nOn Apr 4, 2017 17:37, \"Martin Taillefer\" notifications@github.com wrote:\n\nFor performance reasons, we'd like to have a stateful protocol between\nclient and server. At the moment, gRPC server methods are involved in a\ncompletely stateless way, making it not possible to implement a reliable\nstateful protocol.\nTo support stateful protocols, what's needed is the ability for the server\nto track the lifetime of the state, and to identity which state to use\nwithin a gRPC method. For example:\n-\nJust like there is currently support for StreamInterceptors, introduce\n   ConnectionInterceptors which make it possible to track the lifetime of\n   connections.\n   -\nMake it possible for the connection interceptors to inject some\n   information in the context, such that individual server handler methods can\n   find this state for every method it implements.\nWith this, it becomes possible to allocate some state when a connection is\nestablished, figure out the state to use for every server method, and\ncleanup the state when a connection is closed.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1168, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPIJnYLQEORMGoO6IuFnaeEp9BC0-ks5rsrgbgaJpZM4Mzd0R\n.\n. @dfawley it looks like grpclb/grpc_lb_v1/grpclb.pb.go was edited by hand here, which is cause make proto to produce import cycles.. This causes a regression in CockroachDB - we're seeing a number of flaky tests. I haven't yet determined the exact bug, but git bisect points to this merge as the first bad commit.\n\nThe symptom appears to be a hanging streaming RPC, though I continue to investigate.\nMy understanding is that this new implementation was meant to be opt-in, but there's definitely a behaviour change here.\nEDIT: to reproduce in CockroachDB run make stressrace TESTS=TestDistSQLRangeCachesIntegrationTest PKG=./pkg/sql; the test fails reliably within 2 minutes.. Help me understand the change that was made here. Previously every call to resetTransport would (incorrectly) set the connection state to connectivity.Connecting, which would in effect cause RPCs to behave as non-failfast, since RPCs will wait for transports in that state. After this change, resetTransport sets the state to connectivity.TransientFailure, which has the correct behaviour of making failfast RPCs not wait.\nNow, help me understand what would happen in the following (racy) situation:\n- process 1 starts, calls Dial (or DialContext)\n- process 1's transport gets a \"connection refused\" because process 2 hasn't started yet\n- process 1 attempts to send an RPC on that connection\n- process 2 starts\n- process 1's RPC fails because the initial \"connection refused\" kicked it into connectivity.TransientFailure\nIs that right? I think that's probably the issue we're seeing in cockroach - multiple nodes are started in non-deterministic order by the test harness and all attempt to talk to each other, but now that GRPC tanks failfast RPCs even before the connection is ever established (after this change) we're seeing those races trigger start-up failures.\nWould it be possible for the transport to remain in connectivity.Connecting until a connection is established at all? That seems to be the design intent.\nPlease correct me if I've misunderstood the behaviour.. @menghanl thanks for making this change. Do you think it deserves a test?. Can you guys cut 1.7.1 with this?. Please pardon my tone in advance: what is the point of a commit message (and release description) that reads \"Check ac state shutdown before setting it to TransientFailure\"? I can already read the code, thanks.\nHow can anyone reason about this change without intimately knowing the code? This is a pattern I've noticed routinely in this project - release notes are for the public, please be mindful of that when writing them.\ncc @dfawley . Done\n. done\n. done\n. removed\n. done\n. ok, I pulled that commit out.\n. Could you guys follow up on this in another PR?\n. log.Fatal makes it pretty hard to debug test failures. I did put this in a separate commit, so if you really want a separate PR, I can do that.\n. I think a getter makes more sense here; you don't want to allow mutation of this state.\n. I'm pretty sure it does prohibit mutation; it returns a copy.\n. > Making it exported also makes a copy when users work on it (notice\n\ntls.ConnectionState is a object not a pointer).\n\nDoesn't matter that it's not a pointer. If it's exported I can overwrite it entirely: tlsInfo.State = myOtherTlsState.\n\nPlus, making a copy does\nNOT prohibit mutation because tls.ConnectionState contains a number of\npointer fields.\n\nYes, you could mess with some of the slices in tls.ConnectionState but it's still better than exporting this.\n\nActually adding a getter might make the 2nd copy (depends on how the\ncompiler optimizes it) which is not needed.\n\nThis doesn't matter - those copies are on the stack and the Go compiler does RVO.\n. Gotcha. I'll open a separate PR for this.\n. Alright, I've exported it.\n. errors.New doesn't interpolate arguments, while fmt.Errorf does, so they are semantically different. I could use fmt.Errof everywhere if you feel strongly, but it's somewhat less correct.\n. Done.\nOn Fri, Sep 11, 2015 at 2:23 PM, Qi Zhao notifications@github.com wrote:\n\nIn test/end2end_test.go\nhttps://github.com/grpc/grpc-go/pull/333#discussion_r39301088:\n\n}\nbody := make([]byte, size)\nswitch t {\ncase testpb.PayloadType_COMPRESSABLE:\ncase testpb.PayloadType_UNCOMPRESSABLE:\n-       grpclog.Fatalf(\"PayloadType UNCOMPRESSABLE is not supported\")\n-       return nil, errors.New(\"PayloadType UNCOMPRESSABLE is not supported\")\n\nPlease make them consistent. I think the fundamental reason to use\nerrors.New instead of fmt.Errorf is when your code and/or tests need to\nreuse this error in multiple places.\nWhether it can Interpolate arguments is just a artifact in my mind. You\ncan always use fmt.Sprintf as input param of errors.New.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/333/files#r39301088.\n. add grpc:\n. could you have one of the examples listen on a path that isn't \"/\"? It's not clear to me what client changes would be required.\n. is it necessary to spin a goroutine for each connection? in fact, shouldn't this be a blocking call?\n. Understood. In CockroachDB we're incrementally migrating from net/rpc to grpc - looks like net/rpc already gets out of the way by using a DefaultRPCPath and grpc sets a useful content-type. Seems like that might be enough. Thanks!\n. Doesn't doing this asynchronously violate least surprise for this function? It may return before all connections are closed.\n. Fair enough.\n. Having some trouble with this - it doesn't seem that the content type is coming through on grpc-initiated requests.\n\nHaving applied this diff to grpc (on top of this PR):\n``` diff\ndiff --git a/server.go b/server.go\nindex 577c031..276b98e 100644\n--- a/server.go\n+++ b/server.go\n@@ -438,6 +438,7 @@ func (hl handlerListener) authenticateConn(rawConn net.Conn) {\n func (s Server) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n    st, err := transport.NewServerHandlerTransport(w, r)\n    if err != nil {\n+       grpclog.Printf(\"error: %s\", err)\n        http.Error(w, err.Error(), http.StatusInternalServerError)\n        return\n    }\n```\nI see many instances of error: transport: invalid request content-type. Having inspected the request headers myself, I can tell you they're empty. Any ideas what might be going on here?\nEDIT: go 1.5, using http2.ConfigureServer(&httpServer, nil) for http2 support.\n. I believe so - note that NewServerHandlerTransport first checks the r.ProtoMajor and then the content-type, so r.ProtoMajor must be 2.\n. My code looks roughly the same:\n```\n    rpcServer := rpc.NewServer(nodeRPCContext)\n    grpcServer := grpc.NewServer()\n    handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        log.Printf(\"Got request: %#v\", r)\n        if r.ProtoMajor == 2 && strings.Contains(r.Header.Get(\"Content-Type\"), \"application/grpc\") {\n            grpcServer.ServeHTTP(w, r)\n        } else {\n            rpcServer.ServeHTTP(w, r)\n        }\n    })\nln, err := listen(addr, nodeRPCContext.GetServerTLSConfig()) // ends up calling tls.NewListener\nif err != nil {\n    return nil, err\n}\n\nvar mu sync.Mutex\nactiveConns := make(map[net.Conn]struct{})\n\nhttpServer := http.Server{\n    Handler: handler,\n    ConnState: func(conn net.Conn, state http.ConnState) {\n        mu.Lock()\n        switch state {\n        case http.StateNew:\n            activeConns[conn] = struct{}{}\n        case http.StateClosed:\n            delete(activeConns, conn)\n        }\n        mu.Unlock()\n    },\n}\nif err := http2.ConfigureServer(&httpServer, nil); err != nil {\n    return nil, err\n}\n\nlog.Fatal(httpServer.Serve(ln))\n\n```\nHere's what that Printf line logs:\nGot request: &http.Request{Method:\"PRI\", URL:(*url.URL)(0xc8204b8100), Proto:\"HTTP/2.0\", ProtoMajor:2, ProtoMinor:0, Header:http.Header{}, Body:(*struct { http.eofReaderWithWriteTo; io.Closer })(0x59dcf10), ContentLength:0, TransferEncoding:[]string(nil), Close:false, Host:\"\", Form:url.Values(nil), PostForm:url.Values(nil), MultipartForm:(*multipart.Form)(nil), Trailer:http.Header(nil), RemoteAddr:\"127.0.0.1:51108\", RequestURI:\"*\", TLS:(*tls.ConnectionState)(0xc8205b02c0), Cancel:(<-chan struct {})(nil)}\nNote that this is a bidirectional streaming RPC, not sure if it matters.\nThe code is here: https://github.com/cockroachdb/cockroach/blob/gossip-grpc/server/node_test.go#L68 but it may be quite hard to understand how everything works :(\n. ~~Found it. We are using a custom net.Listener implementation, and hitting this: https://github.com/golang/go/blob/release-branch.go1.5/src/net/http/server.go#L1296~~\n~~See also https://github.com/golang/go/issues/12737.~~\nEDIT: nope, that was obviously wrong. Still stumped. I've edited my standard library to add some logging:\n``` diff\ndiff --git a/src/net/http/server.go b/src/net/http/server.go\nindex a3e4355..834a1b6 100644\n--- a/src/net/http/server.go\n+++ b/src/net/http/server.go\n@@ -1293,7 +1293,9 @@ func (c *conn) serve() {\n        }\n    }()\n\nlog.Printf(\"type assertion\")\n    if tlsConn, ok := c.rwc.(*tls.Conn); ok {\nlog.Printf(\"type assertion ok\")\n        if d := c.server.ReadTimeout; d != 0 {\n            c.rwc.SetReadDeadline(time.Now().Add(d))\n        }\n@@ -1306,6 +1308,8 @@ func (c conn) serve() {\n        }\n        c.tlsState = new(tls.ConnectionState)\n        c.tlsState = tlsConn.ConnectionState()\nlog.Printf(\"proto: %s\", c.tlsState.NegotiatedProtocol)\nlog.Printf(\"tlsnNextProto map: %s\", c.server.TLSNextProto)\n        if proto := c.tlsState.NegotiatedProtocol; validNPN(proto) {\n            if fn := c.server.TLSNextProto[proto]; fn != nil {\n                h := initNPNRequest{tlsConn, serverHandler{c.server}}\n\n```\nThis produces:\nI0202 06:40:42.176913 82232 server.go:1296  type assertion\nI0202 06:40:42.176923 82232 server.go:1298  type assertion ok\nI0202 06:40:42.186413 82232 server.go:1311  proto:\nI0202 06:40:42.186444 82232 server.go:1312  tlsnNextProto map: map[h2:%!!(MISSING)s(func(*http.Server, *tls.Conn, http.Handler)=0x4662ad0) h2-14:%!!(MISSING)s(func(*http.Server, *tls.Conn, http.Handler)=0x4662ad0)]\nSomehow NegotiatedProtocol is not being set. Will continue investingating.\n. Ah, turns out this is because http2.ConfigureServer mutates server.TLSConfig, which is not the config we're using to create our listener. Initializing our server.TLSConfig to our config before the call to ConfigureServer makes this work!\n. Maybe just remove this flag entirely? I can't imagine when you'd want to run without it.\n. While you're here, would you mind moving this file up to the repo root and leaving it in package grpc_test? The current makeup of package test is confusing, where's it's half test utilities and half real tests.\n. t is unused\n. I guess this wasn't in an earlier version of this change, because I'm only noticing the problem now. The presence of this Flush() call makes it unsafe to call Send() on a stream from multiple goroutines. Here's an excerpt from a data race we encounter in our tests https://gist.github.com/tamird/f1bb77e6b5f865c888ed\n. Heh, Send() even races with writeFrameAsync. I added mutexes around my own usage and still saw data races https://gist.github.com/91bb4577bced458d01d2\n. purposes\n. broken comment\n. wow, there is no TLS on that website? that's surprising.\n. this also makes the listenerAddr argument unused, so you can simplify a bit more.\n. See the commit message https://github.com/tamird/grpc-go/commit/7e7145d85838627ba8531dc2f77623ef1a5b573a\n. Sorry, what?\nAs explained in the commit message, value receiver interface\nimplementations implement the interface for both value and pointer. In this\ncase, all the existing instances already use pointers, which is why there\nis no fallout.\nOn Mar 7, 2016 16:36, \"Brad Fitzpatrick\" notifications@github.com wrote:\n\nIn transport/control.go\nhttps://github.com/grpc/grpc-go/pull/588#discussion_r55276401:\n\n@@ -56,43 +56,33 @@ type windowUpdate struct {\n    increment uint32\n }\n-func (windowUpdate) isItem() bool {\n-   return true\n  -}\n  +func (*windowUpdate) isItem() {}\n\nBut you would have to adjust the call sites too, which you don't. So this\nchange is does not even seem correct.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/588/files#r55276401.\n. can this be a singleton? var streamKey = struct{}{}?\n. ah, I see you're matching a pattern used elsewhere. Feel free to ignore this.\n. I think this is better as-is. Unfortunately you lose too much type safety when you implement interfaces with values in Go.\n. http://play.golang.org/p/a6aF-a_9QC\n. I don't understand this comment. This is an unexported interface - what wrapper types are you envisioning?\n. I did mention it - \"isItem is a marker method\". Maybe that didn't come across.\n. it has an advantage if you actually use it as a value - all current uses are using pointers, and that's the problem with value receivers in general; they permit both.\n. it is just a simplification.\n. no reason for this to be a pointer receiver\n. er, nm. all of this is because of the interface, so may as well leave it a pointer.\n. Yeah, pointer is type-safer though.\n\nOn Wed, Mar 30, 2016 at 8:08 PM, Stephen Day notifications@github.com\nwrote:\n\nIn backoff.go\nhttps://github.com/grpc/grpc-go/pull/601#discussion_r57984147:\n\n\nMaxDelay time.Duration\n  +\n// TODO(stevvooe): The following fields are not exported, as allowing changes\n  +\n// baseDelay is the amount of time to wait before retrying after the first\n// failure.\nbaseDelay time.Duration\n  +\n// Factor is applied to the backoff after each retry.\nfactor float64\n  +\n// jitter provides a range to randomize backoff delays.\njitter float64\n  +}\n  +\n  +func (bc *BackoffConfig) backoff(retries int) (t time.Duration) {\n\n\nWe can change to use a value here. The interface will work correctly\neither way.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/601/files/cfa10b2c554ed1f23df44716d535188b499a1490#r57984147\n. That section of the FAQ is completely irrelevant. This method is never\ncalled, this is about interface-induced allocation and type safety.\nOn Apr 1, 2016 17:01, \"Qi Zhao\" notifications@github.com wrote:\nIn transport/control.go\nhttps://github.com/grpc/grpc-go/pull/615#discussion_r58266999:\n\ntype resetStream struct {\n    streamID uint32\n    code     http2.ErrCode\n }\n-func (resetStream) isItem() bool {\n-   return true\n  -}\n  +func (*resetStream) isItem() {}\ntype flushIO struct {\n  }\n\nI am still not convinced we should do this. According to golang faq,\n\"For types such as basic types, slices, and small structs, a value\nreceiver is very cheap so unless the semantics of the method requires a\npointer, a value receiver is efficient and clear.\"\nActually I am thinking we should change the caller's code instead.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/615/files/27c1c446d89d2e3f1e86424b9b02a79e7d8899f6#r58266999\n. @xiang90 do you think it should just be item()?\n. done\n\nOn Sat, Apr 2, 2016 at 12:06 AM, Xiang Li notifications@github.com wrote:\n\nIn transport/transport.go\nhttps://github.com/grpc/grpc-go/pull/615#discussion_r58288152:\n\n// All items in an out of a recvBuffer should be the same type.\n type item interface {\n-   isItem() bool\n\n@tamird https://github.com/tamird If this is a marker method, I feel\nso. There are some code in stdlib using similar approach (\nhttps://golang.org/src/go/ast/ast.go: type Expr interface is an example)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/615/files/27c1c446d89d2e3f1e86424b9b02a79e7d8899f6#r58288152\n. done\n\nOn Sat, Apr 2, 2016 at 2:15 AM, Qi Zhao notifications@github.com wrote:\n\nIn transport/transport.go\nhttps://github.com/grpc/grpc-go/pull/615#discussion_r58289418:\n\n// All items in an out of a recvBuffer should be the same type.\n type item interface {\n-   isItem() bool\n-   Item()\n\nunexported\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/615/files/55b5447f28b48ddb0fd04e7ea605fce7d4a425ac#r58289418\n. It is used further below where it is propagated in a header.\nOn Apr 13, 2016 12:49, \"Xiang Li\" notifications@github.com wrote:\nIn transport/http2_client.go\nhttps://github.com/grpc/grpc-go/pull/634#discussion_r59583363:\n\n@@ -236,9 +236,9 @@ func (t http2Client) NewStream(ctx context.Context, callHdr CallHdr) (_ *Strea\n    var timeout time.Duration\n\nit seems like the whole timeout var can be removed.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/634/files/025674fec58a059fa5bdb94d3cd64c2b1c3242e4#r59583363\n. @isn't !isReservedHeader(\":authority\") already true?\n\nEDIT: nope, disregard\n. rpcError does not implement error, only *rpcError does.\n. capitalization here is weird\n. surely this can be written as the less surprising\nif err := <-errC; err != nil {\n  errs = append(errs, err)\n} else {\n  return\n}\nor \nerr := <-errC\nif err == nil {\n  return\n}\nerrs = append(errs, err)\n. this change seems like overkill. why can't it be\ngo\nwaitC := make(chan error, len(addrs)\nfor _, addr := range addrs {\n    go func(a Address) {\n        errC <- cc.newAddrConn(a, false)\n    }(addr)\n}\nvar timeoutCh <-chan time.Time\n...\n. Refactored this to just use a net.Dialer.\n. > You can use \"f\" to create a net.Dialer and assign to o.copts.\nHow?\n. It's not a real leak - it's that the networking stack is not being\ncancelled and blocking, looking like a leak and causing our tests to flake.\nOn Jul 6, 2016 22:11, \"Qi Zhao\" notifications@github.com wrote:\n\nIn clientconn.go\nhttps://github.com/grpc/grpc-go/pull/751#discussion_r69842457:\n\n@@ -196,9 +196,9 @@ func WithTimeout(d time.Duration) DialOption {\n }\n// WithDialer returns a DialOption that specifies a function to use for dialing network addresses.\n-func WithDialer(f func(addr string, timeout time.Duration) (net.Conn, error)) DialOption {\n+func WithDialer(d net.Dialer) DialOption {\n\nI might have thought about something else you did not intend to tackle\nhere. To be clear, what goroutine is leaked? Is it a real leak or just the\nstress is too intensive to get that goroutine eliminated in time?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/751/files/497877d020a44cfed1f3b09ceb7e5e4f61ae32a2#r69842457,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ABdsPNG1iuCcqlTxrdzD71UK2pvDY2tPks5qTGA3gaJpZM4JFbEQ\n.\n. this should check for a specific error rather than just non-nil.\n. Understood. I believe I've addressed your concerns.\n. Yeah, that's fine. This was only done to reduce code duplication inside the grpc package. In other words, if you move transport to internal, all my changes will still work.\n. Ah, I see. Done.\n. That sounds like a reasonable philosophy, but having more copies of random channels certainly does not make this code easier to read.\n\nPerhaps you could refactor this as you see fit in a follow-up?\n. These are both exported for use in the tests. If you'd like to avoid external users of this, why not move the transport package to internal?\n. DefaultDialerFn is used outside the tests and depends on MakeDefaultDialerFn, so moving these functions to the tests is not possible. Copying them is possible, but is that better?\nAs I said, if you want to avoid external users depending on transport, you can move it to internal/transport. See https://golang.org/s/go14internal.\n. Once again:\n\n... moving these functions to the tests is not possible. Copying them is possible, but is that better?\n. This doesn't make an iota of sense; the important part of these functions is that they abstract over the Go version in use. If you ask that I re-write these functions in the tests, that's fine, but it means that these tests will also require build tags to achieve compatibility with Go 1.5 and Go 1.6. Do you also expect users to write Go-version-specific code to make use of this feature?\n. I've moved these functions out of transport and into grpc. Is that a reasonable compromise?\n. Renamed.\n. OK, thanks.\n. It's left unexported to encourage users to use the Dial method rather\nthan accessing this directly. That method encapsulates usage of the default.\n\nOn Jul 26, 2016 15:50, \"Qi Zhao\" notifications@github.com wrote:\n\nIn transport/transport.go\nhttps://github.com/grpc/grpc-go/pull/751#discussion_r72323639:\n\n@@ -341,8 +341,10 @@ func NewServerTransport(protocol string, conn net.Conn, maxStreams uint32, authI\n type ConnectOptions struct {\n    // UserAgent is the application user agent.\n    UserAgent string\n-   // Dialer specifies how to dial a network address.\n-   Dialer func(string, time.Duration) (net.Conn, error)\n-   // Cancel is closed to indicate that dialing should be cancelled.\n-   Cancel chan struct{}\n-   // dialer specifies how to dial a network address.\n-   dialer func(string, time.Duration, <-chan struct{}) (net.Conn, error)\n\nAny reason this is not exported? All the other fields are exported. If it\nis exported, it seems we do not need ConnectOptions.WithDialer(...).\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/751/files/e80127070134096b644d5877643696a67efebf3f#r72323639,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPIK_NVK188sAbESZnw_tyMCWArPGks5qZmUbgaJpZM4JFbEQ\n.\n. these last two fields don't exist in Go 1.6 or 1.5\n. Not true - the other user is the end2end test. That's a real user, isn't it? :trollface: \n. seems like all of this can be inside of ac.resetTransport when called with false.\n. this change is nonsense.\n\nif err == io.EOF then err != nil is already true. Perhaps you meant err != nil && err != io.EOF?\n. the invocation is different; -race is passed in testrace, but not here.\n. This timeout was not actually being used anywhere. If you look at the timeout passed to transport.NewClientTransport, you'll see that it's only non-zero if it is passed in ConnectOptions, which is never the case.\nIf you want to use max(backoff,minConnectTimeout), that would be a change of behaviour, but I can do it.\n. I'm guessing something in your GOPATH is owned by root. What happens if you sudo chown -R $(whoami) $GOPATH and try again?\n. What I'm saying is that this something specifically incorrect about your setup, and it will not affect most users because it only interacts with GOPATH and GOPATH should not be owned by root.\nI'm happy to help you debug your setup - come join our gitter channel https://gitter.im/cockroachdb/cockroach.\n. You're right! Fixed.\n. Yikes, that's really odd. It means that you can't use Go's incremental compilation for your tests. OK, I backed out this commit.\n. I'm always concerned about checks like this. Can you check for a specific error (or errors) rather than just any non-nil?\n. go\nswitch t.state {\ncase unreachable:\n  t.Close()\n  fallthrough\ncase closing:\n  t.mu.Unlock()\n  return nil\n}\n. uh, this won't unlock the mutex.\n. no need for this nil check.\nfrom grpc.Code:\nfunc Code(err error) codes.Code {\n    if err == nil {\n        return codes.OK\n    }\n...\n}\n. sure went through a lot of iterations just to avoid this suggestion.\n. Thanks for clarifying.\n. done\n. I'd prefer to keep it. The next step here is to add a DialContext method, which will accept a user-provided context; then Dial becomes a thin wrapper; leaving line 219 will reduce the overall diff.\n. why is this one different?\n. oh, derp. I misread, carry on.\n. Yep, that's right.\n. Sure. I was going for the publicly-visible description, since ClientHandshake is the only public interface changed. What would you like it to say?\n. Heh, in fact you can even edit the PR description yourself :)\n. cs is not even used until way down below after the for loop. Why does it need to be allocated way up here?\n. the context should be the first argument.\n. what's the point of this goroutine and done channel construction? except for a very unlikely race, it always just boils down to calling cancel.\n. Perhaps the name should be DialContext, following the standard library's example https://golang.org/pkg/net/#Dialer.DialContext\n. a \"gentler\" way to achieve this is to append | grep -vF 'pb.go:' to the end of this line.\n. Because go vet prior to 1.7 has the Errorf false positive that was\nremoved in this change.\nOn Thu, Aug 18, 2016 at 1:04 PM, Chris Broadfoot notifications@github.com\nwrote:\n\nIn .travis.yml\nhttps://github.com/grpc/grpc-go/pull/840#discussion_r75347380:\n\nscript:\n-  - '! gofmt -s -d -l . 2>&1 | read'\n-  - '! goimports -l . | read'\n-  - '! golint ./... | grep -vE \"(_string|.pb).go:\"'\n-  - '! go tool vet -all . 2>&1 | grep -vE \"constant [0-9]+ not a string in call to Errorf\"'\n-  - 'if [ \"$TRAVIS_GO_VERSION\" == 1.7 ]; then ! gofmt -s -d -l . 2>&1 | read; fi'\n-  - 'if [ \"$TRAVIS_GO_VERSION\" == 1.7 ]; then ! goimports -l . | read; fi'\n-  - 'if [ \"$TRAVIS_GO_VERSION\" == 1.7 ]; then ! golint ./... | grep -vE \"(_string|.pb).go:\"; fi'\n-  - 'if [ \"$TRAVIS_GO_VERSION\" == 1.7 ]; then ! go tool vet -all . 2>&1 | grep -vF .pb.go:; fi' # https://github.com/golang/protobuf/issues/214\n\nWhy only for 1.7?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/840/files/61ab86bcb3b2b5987b58c9662f8976985ce4a212#r75347380,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPCFui2-6itExDzNBE0Xy5W0wWdN7ks5qhJCegaJpZM4JnrCF\n.\n. Changed this to ignore the superset of oddities.\n. Done. Turns out golint Didn't complain about this.\n. Assign to a local?\n. Done.\n\nOn Mon, Aug 22, 2016 at 2:25 PM, Qi Zhao notifications@github.com wrote:\n\nIn transport/http2_client.go\nhttps://github.com/grpc/grpc-go/pull/846#discussion_r75731294:\n\n@@ -135,9 +135,14 @@ func newHTTP2Client(ctx context.Context, addr string, opts ConnectOptions) (_ Cl\n        conn, authInfo, connErr = creds.ClientHandshake(ctx, addr, conn)\n    }\n    if connErr != nil {\n-       temp := false\n-       switch connErr {\n-       case io.EOF, context.DeadlineExceeded:\n\nPlease add some comments.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/846/files/dba1af305f643090367ac595c5eb41819b092c98#r75731294,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABdsPOSZvO419HS6_WZ6BcFvWJrEUKVWks5qiemagaJpZM4Jo-ij\n.\n. This stuff is cached in travis, so there is.\n. How? What would the code look like?\n. The test needs to sleep long enough to allow the transport to reconnect - the backoff is controlled by this value, so the tests needs to be able to inspect (and modify for speed) this value.\n. Not sure what to tell you, but the proof is in the pudding. Prior to this PR, the various golint fixes included here were not needed, and now they are.\n. Done.\n. What's the downside to leaving this as it is? It more closely mirrors the true scenario we are interested in testing here. At this point, this issue is preventing CockroachDB from moving forward and producing a beta release, and the generally slow response times of the grpc-go team is not giving me confidence that such a rework of this PR will result in timely progress.\n. If that's really a concern, minConnectTimeout should move to a field on the options struct.\n. Good point. Changed this to be more robust.\n. @iamqizhao the leak happens here.\n. Done.\n. done.\n. not too important, but you can use .Error() instead of fmt.Sprintf(\"%v\", ...) here and on the next line\n. move this after the error check and defer it to prevent leaking goroutines when this test fails\n. s/Close/Clone/\n. seems like you can remove this extra argument now that there's a public method that does this.\n. ditto\n. s/Tls/TLS/g\n. FYI, this can be [...]*checkFuncWithCount\n. You'd need to check for nil anyway, since the map keys give you no guarantee whatsoever.\n. you should never ignore the cancel function this way - defer cancel() on the next line instead.. this still needs a deferred cancel. this still needs a deferred cancel. all the changes in this file are unnecessary (and the new code is worse, since it increases the scope of err needlessly).. can this just return the error directly instead of also logging it?. s/handling/handler/. s/Unimplemented/unimplemented/. inconsistent wrapping. what's this construction about? it seems equivalent to using srv.server below. s/fot/for/. missing space. would be nice to wrap this.. this is setting defaults, not validating.. why is this int instead of struct{}? the value is never used.. this should use infinity instead of repeating time.Duration(math.MaxInt64). defer timer.Stop() below this line.. why does this happen under lock? seems clearer to write:\nt.mu.Lock()\ndormant := len(t.activeStreams) < 1 && !t.kp.PermitWithoutStream\nt.mu.Unlock()\nif dormant {\n   ...\n} else {\n  t.controlBuf.put(p)\n}. why is there so much duplicated code here?. time.Duration is not necessary here.. perhaps this should be a function of the settings above, rather than hardcoded.\n\nthere are 4 instances of this.. This means that ctx will continue to be used even after it has been cancelled by the caller. Doesn't seem bueno.. Why would the main goroutine hang? It only waits on the waitgroup.. Why would we do that, given that this work is already done, and is correct?\nFeel free to add the go:generate lines, but that doesn't seem to be within the scope of this PR, to me.. OK, done.. > grpclb.proto is now no longer a copy of the canonical version of the proto. This means updating it requires extra steps.\nI didn't realize grpclb.proto was an upstream thing. That's a pickle. We can probably work around this by running the code generator twice; once with grpc and once without.\n\nload_balancer.pb.go is modified by sed, which is not ideal for several reasons (e.g. it's not based on the AST like gofmt would be).\n\nLet's be practical here; doing these edits based on the AST is a lofty goal, but this sed-based automation is certainly better than the existing hand edits which are prone to rot and also are obviously not based on the AST.\n\nI can think of two low-effort options that would allow us to remove the sed commands until we can switch to using go generate in a later PR:\n\nHow does go generate solve anything?\n\nLeave the rest of the changes as-is, but skip the grpclb.proto file in make proto and delete load_balancer.proto.\n\nIs that better? I think it's worse for reasons mentioned above.\n\nRemove the service from grpclb.proto and re-add grpclb/grpclb_server_generated.go.\n\nDoesn't that contradict your other point about keeping a canonical copy of grpclb.proto?. looks like you addressed this TODO.. what is tctx? the name leaves something to be desired and the doc comment doesn't explain it.. I believe it's conventional to omit trailing newlines in error strings.. this test is called TestInvokeCancelClosedNonFailFast - did you mean to make this change?. Is this accurate? It seems that ac.state will be set to connectivity.Connecting unconditionally and then to connectivity. TransientFailure on every attempt; I can't tell which part of this is sensitive to \"if it's not first time doing resetTransport.\". there's no reason to use a select here, and doing so can result in worse performance than the simpler alternative:\nif err := ctx.Err(); err != nil {\n  return toRPCErr(err)\n}. Ditto:\nif err := ctx.Err(); err != nil {\n  return toRPCErr(err)\n}. Quite surprising, thanks for the details. Your comment points to pre_go17.go but I assume these benchmarks were run on 1.9?\nAlso interesting that selecting on the open channel is faster than the mutex with inline Unlock.. By the way, there's something fishy here - how can BenchmarkCancelContextChannelGotErr ever be faster than BenchmarkCancelContextErrGotErr? The former does strictly more work.. cc @Sajmani -- looks like that defer has been there since inception; perhaps it's worth replacing with the an inline Unlock as @dfawley has shown.. ",
    "adg": "Travis reports this failure:\n../../../google.golang.org/grpc/credentials/credentials.go:206: undefined: oauth2.Context\nBut this CL specifically removes oauth2.Context from that line in that file.\nLooking at the build log, it looks like the script that fetches and tests the code does not apply the pull request patch between running go get and go test.\n. Now I can't find the build failure log from travis?! It just disappeared.\n. Maybe auth is a better package name.\nThe interface could be called CredentialsFetcher and the method FetchCredentials. Thoughts?\n. @okdave @rakyll thoughts?\n. Looking at the import graph, the oauth2 packages do seem extraneous:\nhttp://godoc.org/google.golang.org/grpc?import-graph&hide=2\nI think it does seem reasonable to trim the dependencies, and I think it's on our roadmap to make some changes to the cloud and oauth2 packages that will make this possible.\n. If you want to re-use tokens then you should cache the token and fetch a new one when it has expired. There's no need to re-use the token source; the setup for that is just allocating a couple of structs.\n. OK fair enough. I left some comments on how the code could be structured a little more idiomatically.\n. cc @iamqizhao (and @rsc @robpike @bradfitz who may be interested). @iamqizhao note the relationship between the simulated latency and the request time.\nFor HTTP/2, with 0 latency a request takes ~1ms. With 1ms of latency (both up and down) you get requests around 3ms (after the first request, when the connection is established). With 2ms of latency, requests are ~5ms. For 4ms latency, requests take ~9ms. See the pattern? So the actual time spent making HTTP/2 requests, minus the introduced latency, is constant (around 1ms).\nContrast this to the GRPC behavior, where the introduced latency multiplies the overall request time. \n\nWe need to figure out what is the expected value there (this counters my previous observation with http2.0...).\n\nStarting with zero latency, the GRPC requests take 4-6ms. What we should see for GRPC requests is a similar pattern, where 1ms of latency yields 8ms requests (6ms + 1ms + 1ms), 2ms latency = 10ms requests, and so on.. Why would\u00a0latency have any affect on proto marshaling overhead?. 12.423725ms 1ms HTTP/2.0\n3.02918ms   1ms HTTP/2.0\n2.775928ms  1ms HTTP/2.0\n4.161895ms  1ms HTTP/2.0\n2.951534ms  1ms HTTP/2.0\nThe additional 8-10ms in the first request is certainly setting up the HTTP/TLS connection, which increases proportionally with latency as it involves several round trips.. This change enables compression for GRPC and uses a randomly-generated payload to defeat the effect of the compression:\n```\ndiff --git a/grpcbench/main.go b/grpcbench/main.go\nindex 101677d..c03ac0f 100644\n--- a/grpcbench/main.go\n+++ b/grpcbench/main.go\n@@ -1,6 +1,7 @@\n package main\nimport (\n+       \"crypto/rand\"\n        \"crypto/tls\"\n        \"flag\"\n        \"fmt\"\n@@ -41,6 +42,7 @@ func main() {\n                                grpc.WithBlock(),\n                                grpc.WithTimeout(3 * time.Second),\n                                grpc.WithInsecure(),\n+                               grpc.WithCompressor(grpc.NewGZIPCompressor()),\n                        }\n                        conn, err := grpc.Dial(*addr, opts...)\n                        if err != nil {\n@@ -59,7 +61,12 @@ func main() {\n            ctx := context.Background()\n\n\nmsg := strings.Repeat(\" \", *msgSize)\nrandomBytes := make([]byte, *msgSize)\n_, err := rand.Read(randomBytes)\nif err != nil {\nlog.Fatal(err)\n}\nmsg := string(randomBytes)\n                for i := 0; i < *numRuns; i++ {\n                        t1 := time.Now()\n                        var err error\n@@ -86,7 +93,7 @@ func main() {var server *grpc.Server\nif *useGRPC {\n\n\nserver = grpc.NewServer()\nserver = grpc.NewServer(grpc.RPCDecompressor(grpc.NewGZIPDecompressor()))\n            helloworld.RegisterGreeterServer(server, greeter{})\n    }\n    l, err := net.Listen(\"tcp\", *addr)\n\n\n\n```\nThe numbers appear to be more or less the same:\n```\n2.477774562s    32ms    GRPC\n2.462765459s    32ms    GRPC\n176.592079ms    32ms    HTTP/2.0\n69.485653ms 32ms    HTTP/2.0\n```. How would that change things?. On investigating this further, I found an issue with the grpcbench program. It was not sending complete HTTP/2 requests, which explains the dramatic difference between GRPC and HTTP/2. GRPC still has performance issues, but it is only about 2x slower than x/net/http2 (not orders of magnitude as first reported).\nI updated grpcbench to correctly exercise HTTP/2, and also threw HTTP/1.1 into the mix.  See this pull request.\nInterestingly, while HTTP/2 is 2x faster than GRPC, HTTP/1.1 is more than 10x faster than HTTP/2. I talked to @bradfitz about this and it is because there's a fixed initial flow control window which never changes, with TODOs to improve this. It would make sense for GRPC to adopt x/net/http2 so that when those TODOs are done, GRPC users should benefit.\nHere's the data from a new run of grpcbench:\n```\nDuration    Latency Proto\n9.491853ms  0s  GRPC\n4.495137ms  0s  GRPC\n5.391501ms  0s  GRPC\n4.726007ms  0s  GRPC\n5.12824ms   0s  GRPC\n14.558358ms 0s  HTTP/2.0\n6.382529ms  0s  HTTP/2.0\n6.178314ms  0s  HTTP/2.0\n5.51633ms   0s  HTTP/2.0\n6.029431ms  0s  HTTP/2.0\n11.587129ms 0s  HTTP/1.1\n5.397509ms  0s  HTTP/1.1\n4.237196ms  0s  HTTP/1.1\n5.960211ms  0s  HTTP/1.1\n108.997722ms    0s  HTTP/1.1\n93.566806ms 1ms GRPC\n99.017055ms 1ms GRPC\n93.487803ms 1ms GRPC\n98.34198ms  1ms GRPC\n99.387167ms 1ms GRPC\n59.484852ms 1ms HTTP/2.0\n51.734641ms 1ms HTTP/2.0\n54.935499ms 1ms HTTP/2.0\n156.577757ms    1ms HTTP/2.0\n51.694602ms 1ms HTTP/2.0\n15.841433ms 1ms HTTP/1.1\n5.704361ms  1ms HTTP/1.1\n107.550483ms    1ms HTTP/1.1\n5.40012ms   1ms HTTP/1.1\n5.800495ms  1ms HTTP/1.1\n192.232563ms    2ms GRPC\n188.990258ms    2ms GRPC\n180.028459ms    2ms GRPC\n189.284675ms    2ms GRPC\n192.21624ms 2ms GRPC\n107.48685ms 2ms HTTP/2.0\n92.618437ms 2ms HTTP/2.0\n103.681046ms    2ms HTTP/2.0\n94.787457ms 2ms HTTP/2.0\n96.490496ms 2ms HTTP/2.0\n25.557564ms 2ms HTTP/1.1\n7.750258ms  2ms HTTP/1.1\n6.587036ms  2ms HTTP/1.1\n107.893995ms    2ms HTTP/1.1\n6.236492ms  2ms HTTP/1.1\n355.213791ms    4ms GRPC\n349.819232ms    4ms GRPC\n333.278482ms    4ms GRPC\n338.551819ms    4ms GRPC\n338.591875ms    4ms GRPC\n184.903654ms    4ms HTTP/2.0\n179.907175ms    4ms HTTP/2.0\n180.843634ms    4ms HTTP/2.0\n169.588748ms    4ms HTTP/2.0\n167.390974ms    4ms HTTP/2.0\n32.01951ms  4ms HTTP/1.1\n12.554588ms 4ms HTTP/1.1\n9.637975ms  4ms HTTP/1.1\n112.462356ms    4ms HTTP/1.1\n8.400449ms  4ms HTTP/1.1\n669.925997ms    8ms GRPC\n682.715267ms    8ms GRPC\n669.499247ms    8ms GRPC\n653.192967ms    8ms GRPC\n686.181873ms    8ms GRPC\n365.49212ms 8ms HTTP/2.0\n330.625694ms    8ms HTTP/2.0\n340.397932ms    8ms HTTP/2.0\n329.523659ms    8ms HTTP/2.0\n336.895354ms    8ms HTTP/2.0\n50.715689ms 8ms HTTP/1.1\n15.353749ms 8ms HTTP/1.1\n17.007966ms 8ms HTTP/1.1\n114.972155ms    8ms HTTP/1.1\n18.499317ms 8ms HTTP/1.1\n1.272841685s    16ms    GRPC\n1.285137518s    16ms    GRPC\n1.278880998s    16ms    GRPC\n1.286303382s    16ms    GRPC\n1.280546303s    16ms    GRPC\n688.093256ms    16ms    HTTP/2.0\n637.301763ms    16ms    HTTP/2.0\n623.836108ms    16ms    HTTP/2.0\n658.244228ms    16ms    HTTP/2.0\n633.323672ms    16ms    HTTP/2.0\n87.381827ms 16ms    HTTP/1.1\n24.798529ms 16ms    HTTP/1.1\n122.792087ms    16ms    HTTP/1.1\n24.228707ms 16ms    HTTP/1.1\n22.021362ms 16ms    HTTP/1.1\n2.500501533s    32ms    GRPC\n2.41936603s 32ms    GRPC\n2.458976246s    32ms    GRPC\n2.45729285s 32ms    GRPC\n2.475112293s    32ms    GRPC\n1.325008844s    32ms    HTTP/2.0\n1.185842114s    32ms    HTTP/2.0\n1.22962957s 32ms    HTTP/2.0\n1.195622848s    32ms    HTTP/2.0\n1.188314041s    32ms    HTTP/2.0\n150.393105ms    32ms    HTTP/1.1\n36.553453ms 32ms    HTTP/1.1\n40.572305ms 32ms    HTTP/1.1\n40.395134ms 32ms    HTTP/1.1\n146.914462ms    32ms    HTTP/1.1\n```\n. Interesting. The 5-6x slow down still makes GRPC (and x/net/http2) unusable\nfor us, unfortunately.\nI intend to work with @bradfitz on improving HTTP/2, because it's a\ncodebase I understand.\n. @MakMukhi I think there is benefit in creating a tracking bug specifically to address throughput issues on high-latency links.Issue #1280 does not mention latency at all (I know this is what \"BDP estimation and flow control\" refers to).  In my experience, issues focused on actual user problems yield better solutions.. ts is a fine variable name for a tokensource\n. if err == nil {\n  return map[string]string{\"authorization\": token.TokenType + \" \" + token.AccessToken}\n}\n. drop this if block\n. if err != nil {\n  return nil, err\n}\ns.tokenSource = ts\n. delete\n. ",
    "ebfio": "+1\n. @dsymonds Thanks David. Even though this comment is valid and widely accepted to support a request from another user (especially one without any reply from the maintainers), I just subscribed. Thanks for your time and helpful answer :+1: \n. +1\n. ",
    "pires": "Will it be enabled by default? I vote for not doing it by default.\n. @harlow you rock! Thank you.\n. According to this\nAll segments preceding and including FIN\nwill be retransmitted until acknowledged.  When the other TCP has\nboth acknowledged the FIN and sent a FIN of its own, the first TCP\ncan ACK this FIN.\nCan you try and turn the network only after the socket on the client side no longer exists? I believe the timeout on a standard Linux system is 2 minutes.\n. etcd and DNS SRV seems awesome!\n. > but we do allow ppl implement their own load balancer\nAwesome!\n. Same here, \ud83d\ude15 \n. This is awesome! Looking for it to get merged so I can implement a sink to InfluxDB.\n. @iamqizhao I'm using gb for vendor dependencies and build management. I'm OK with this since I just removed those from my dependencies and code compiles & runs. I'm also OK if this issue is closed. Thank you all.\n. Any update on this?\n. go get -u github.com/golang/protobuf/{proto,protoc-gen-go} did the trick for me, thanks for the tip.\n. This is beautiful!\n. ",
    "wencan": "+1\n. +Snappy\n. you can use grpc.Codec\n. such as: Close, Cancel, setDeadLine\n. ",
    "stevvooe": "@wencan Is there a way to use grpc.Codec such that it changes based on the request headers or would this be static at instantiation time?\n. @dsymonds The blog post outlines other uses as \"cancelation signals, and deadlines across API boundaries to all the goroutines involved in handling a request\". Not all of these boundaries may be exclusive to the request scope.\nImagine an API that could be used as part of a request-scoped server or client-side:\nfunc GetStuff(ctx context.Context) (Stuff, error) { ... }\nThis method may use something in context but it doesn't care whether or not the context is request-scoped. Subsequently, we may configure the default resources used by GetStuff in a background context that then may be replaced at request time with something more specific to the context.\nThere may also be information setup in the background context that is service-wide but doesn't make sense to pass explicitly, such as loggers and tracing facilities.\n. @dsymonds That is a blog post I have read several times. I understand that the values are request scoped. There are cases where one wants to control the instantiation of the background context. Here is an example, written by you.\n. @dsymonds How would you accomplish the same task now?\n. @iamqizhao @dsymonds Any insight implementing similar functionality to internal.BackgroundContext? A little more advice than \"don't do that\" would be helpful.\nAllowing one to set the server context provides a mechanism to control the background context used in requests. The main reasoning behind requiring a server-scoped context is to avoid having different ways of transmitting contextual values depending on the use case.\n. @iamqizhao Will there be any changes here? The current ClientConn, Conn and Picker approach is extremely confusing. A few questions specific to this:\n1. What is the role of Conn versus ClientConn?\n2. How would I make a ClientConn but delay actual dialing to when a method is actually called?\n3. Will we be able to make atomic changes for connection routing? Imagine being able to do an atomic database failover that can route all new requests to another connection.\n. @iamqizhao Thanks for your response!\n\nConn is an abstraction connecting to a single destination (i.e., at most 1 underlying transport at any time); a ClientConn may be consistent of 1 or more Conn;\n\nThe current API doesn't really suggest this model. For example, I would expect ClientConn to work like a pool, providing Conn instances, or something to that effect. Perhaps, the dialer would produce Conn instances that would be part of the pool controlled by ClientConn. The most confusing aspect of this setup is that Dial doesn't actually perform a Dial. The dial action is actually taken when creating Conn, but the functionality seems to hidden away in the interaction between ClientConn and Conn private fields.\nWill there be any efforts to clear up this naming and behavior? The current situation makes adoption of GRPC very hard, since there is overhead to get over the poor naming.\n\nThis is not the default behavior and you need to make your own picker impl by deferring NewConn call;\n\nI got this working. However, it still requires an initial address provided as the target argument to Dial, which is basically a throwaway. Effectively, it requires one to query the target set before creating the connection or incur a timeout cycle, rather than deferring immediately to the picker.\nIt was also confusing to implement since we don't have the ability to create the very first connection. One must instantiate the picker and ClientConn with the same arguments, then join them up in Init to ensure it is associated with the correct address. We then lose the ability to create Conn instances, since the code around the PickAddr call just uses it. At this point, we've lost control over the lifecycle of the Conn instance that is created as a result.\nMy immediate feeling is that Picker is way over built but the problems come down to the organization around ClientConn and Conn, not the design of Picker itself.\n\nWithout the concrete requirements I cannot give the perfect answer. But I think this also can be achieved using a custom picker impl.\n\nThe canonical example would be having a leader/follower set. Let's say that we have a ClientConn that is must always be homed to the current leader. Any in-flight connections could be cancelled and all new RPCs would be sent to the current leader. Obviously, there would need to be some support in the target service for this to working correctly.\n\nNote that Picker is still in experimental stage and may be subject to some revisions.\n\nWhat are the plans here?\n. @iamqizhao @dim @therc While I see the benefit of the load balancing proposals, this particular issue seems to be more about problems with the design of ClientConn and Picker. With a sane Go API model there, the rest of the discussion around proper load balancing implementations become academic. The current Go API design makes simple behavior challenging to implement.\n@dim I think everything in the lb proposal above lines up with the use cases we have in mind, except that one may want to \"push\" server lists, rather than poll for them. Other than that, it looks like a very sensible API.\nTo be clear, here are the following use cases for resolving a client connection (if I missed any, pleas chime in):\n1. Load balancing to an arbitrary host for every request. These are services that are either stateless of have shared state between all instances.\n2. Load balancing to an individual host after resolution. These are services that may have a session state associated with a single instance.\n3. Directing requests to a particular host based on request content. An example might be a queuing service where the Picker (or something) resolves a location for a particular queue. It could also be a key value store where the connection is directed to a particular server where the data is located.\n4. Atomic switchover for a particular \"class\" of service. Imagine a leader/follower service instructed by a watch in an etcd. When there is a switch between leader/follower, the current requests must be cancelled and a new connection must be made to the right service. It is acceptable to rely on service behavior to support this. For example, a leader that is no longer a leader may throw errors that could be intercepted by the picker.\nFor the most part 1, and even 2, are very straightforward. They can be implemented with very little code but impose a lot of requirements on the backend service (especially use case 1). Use cases 3 and 4 are more interesting and can make grpc useful for very complex services, such as databases or queueing servers. Solving these is hard and it would be great to have GRPC do some of the heavy lifting. The behavior doesn't needed to be provided by GRPC, but it should provide solid hooks, such as connection picking and interruption, to implement such behavior in the Go API.\n. > Anyway: @stevvooe, why push instead of pull? The latter can happen on the client's own schedule, not the load balancer's. The push model does have the advantage of being able to forward right away updates about servers that just joined or left the pool. I suspect pull is going to stay, though, at least for historical reasons (and those might have had a rationale like mine or additional ones, such as efficiency).\nThis may be one of those \"academic\" discussions. The chief reason I bring up pull is to have synchronized switchover. There is a resource cost to this, such as an active connection and server-side session.\nWhatever the advantages of push versus pull actually are, it should be a decision up to the application developer. The actual requirements for the GPRC Go API are similar in either case and should be able to support both.\nI'll try to come up with a proposal if this issue is still stalling.\n. > I treat \"Dial\" as a function call to dial a communication channel (i.e., ClientConn) for users so that users can put rpcs onto it. I think it is not necessary for Dial to create a real TCP connection or something similar, which is implementation details and should be transparent to users.\nThe term Dial, in every other Go project, means to create some sort of connection. When this is not the case, other terms, such as New or Open (see database/sql), are used. It is confusing to it to do anything else, and I've seen several developers stumble on this distinction. Furthermore, most of the \"dial options\" actually affect stream behavior and have nothing to do with dialing.\nIt seems like the model that is followed here is that of database/sql or net/http.Transport. Both of these pool connections behind the scenes but can be composed to provide more sophisticated behavior. For example, one can replace the entire transport implementation for an HTTP client to get interesting behavior. Yes, there are problems with this model, but it should be used to inform the API of this package.\nTo resolve this, I would start with the following (breaking) changes:\n1. Remove Conn and ClientConn. They expose a lot of internal detail and have confusing interfaces.\n2. Define an interface called Invoker. What does it do? It invokes!\ngo\n   type Invoker {\n     Invoke(ctx context.Context, args, reply interface{}, opts...CallOption) error\n   }\n3. Define a new type Client to take care of the connection pooling and dispatch, similar to what Conn and ClientConn manage today. The Client implements Invoker. It also manages connection selection and lifecycle. It's behavior can be augmented by a set of ClientOption, allowing one to influence dialing, picking, stream instantiation and pooling.\ngo\n   func NewClient(endpoint string, opts...ClientOption) (*Client, error)\nClient may expose a few methods to monitor or bounce connections.\n4. Generated code wouldn't have to change much other than having the type Invoker, rather than *ClientConn. Generated code is no longer tightly coupled to the GRPC implementation. We get functions like the following if we have service Foo:\ngo\n   func NewFooClient(invoker grpc.Invoker) FooClient\nOne can now write tests of my client without having to mock a service, bind a port and respond over http2.\n5. I would replace the Server type with just a Handler type. Most of the time, we already have a full net/http server running and just want to and GRPC as a handler.\nWhile there are details that need to be worked out, the above would provide a starting point to provide a more idiomatic package. A better package structure would lead to less bugs and more productivity when using GRPC.\n. > In grpc-go, I would say \"Dial\" creates ClientConn to perform rpcs. I want to emphasize that the connection could be an abstraction instead of a real tcp connection. And I think Dial still fits well here.\n@iamqizhao Respectfully, I think you're completely missing the point here. There is a well-established convention that Dial does a certain thing. The behavior here has caught out every experienced Go programmer I've worked with on GRPC related code.\n\nWe are talking about different abstractions here.\n\nWe aren't at all. I implore you to go back and thoroughly read my commentary. There are a few tweaks here that can expose the right abstractions. Many of these problems can be alleviated by leveraging the well-thought patterns from the standard library and extending them where needed.\n. > It seems github issue is not a good place to discuss the design issues like this. I will try to draft my ideas into a google doc and have the discussion there early next week.\nI agree with your sentiment, but let's make sure these issues are discussed in a transparent manner. I do apologize for slightly hijacking this issue, but I believe the problems with load balancing are rooted in problems with the Go API design.\n\nFor example, how can a user add his custom load balancing scheme in your proposal (e.g., he wants to do weighted round-robin on the list of addresses returned by the name resolver. )? In my understanding, he needs to create his own invoker implementation (in his own package) but lacks the building blocks because there are no ClientConn and Conn or something similar.\n\nThe overall approach is to define a clear role between Client, Transport and net.Conn while allowing one to augment dialing (net.Dialer), invocation (grpc.Invoker) and transport instantiation (grpc.Picker or something similar). The augmentation can be achieved by configuring the client, which has a long lifecycle in an application.\nMy proposal is really the first steps that need to be taken to make this package usable for the average Go user. The role of ClientConn falls to Client in the above proposal. A Picker in this proposal ends up sitting between the Client and the Dialer to setup and manage transport state. A Picker ends up looking a lot like some sort of transport allocator, which is kind of what it is doing now, but we make this role explicit.\n\ni) we must NOT break any existing user-facing API except the experimental ones; \n\nI would really hope you would reconsider this or someone may fork. The package, in its current state, has a number of problems and doesn't warrant this level of backwards compatibility guarantee. I'm sorry but the quality just isn't there. I'd be more than willing to update existing code. The cost is much less than having to shim non-idiomatic API usage and instruct others in using unfamiliar APIs. I've wasted days and a thinning reputation for choosing GRPC after running into these problems.\nThat said, there is most definitely a way to maintain compatibility and bring these changes. ClientConn can implement Invoker and still be an argument for grpc.Invoke. They will just be deprecated. If that is insufficient, a simple rewriter can be implemented that can automatically update code. The touch points are really just the NewXXXClient functions and their generated method implementations. For most code bases I'm working with, the updates would be mostly around connection instantiation and use of the Picker, which is experimental anyways. For most users, I'd expect the impact of API breakages to be minimal if careful changes are made.\n\nii) ClientConn or its counterpart cannot be an interface because we do not want to mislead users to let them feel they can make their own custom impl.\n\nWhat is your goal in limiting users ability to make a custom implementation? Right now, the generated code is only calling grpc.Invoke, which is tightly integrated with ClientConn and Conn implementations. This should just be an interface so that complexity can be hidden in the actual client. The current behavior makes adding cross-cutting behavior, such as custom logging and metrics a chore. We either have to add it everywhere, write a new plugin that adds them or fork GRPC.\n\nok, I would buy this. I probably can tune the code so that once Dial is returned at least 1 network connection is started (or setup if grpc.WithBlock() is set.).\n\nUltimately, it's probably better not to touch it if you're adamant about not changing it. Dial means to dial a single connection that can be used as such and ClientConn simply can't be used that way. More documentation about the role of ClientConn would probably be the right approach. Right now, it just says it makes a connection, which it doesn't actually do. This should probably be augmented with information about the ClientConn lifecycle and best practices.\n. > I do not like the idea to fuse rpc invoking and connection management together and do not think it is necessary.\nI apologize for not making a very clear point, as my proposal is to decouple them. By declaring an interface, invocation and connection become separate components.\nIn general, the current design is very tightly fused. A *ClientConn must be passed to a NewXXXClient function. Then, grpc.Invoke access private fields on grpc.ClientConn and grpc.Conn.\n\nSince the very similar thing works very well inside Google, I do believe this is addressable without ruining the existing user-facing API.\n\nGoogle has a lot of internal infrastructure, such as machined-local load balancing, that can help to make this particular abstraction work very well. In the outside world, the environments are not nearly as homogeneous. The interfaces and abstractions must be much more flexible to work in the myriad environments in which GRPC may now find itself in. For example, in Google may find it acceptable to deploy a separate load balancing process that can manipulate IP tables to route RPC requests, but this may be impossible in another environment.\n. @iamqizhao Here the main load balancing models I enumerated above:\n1. Load balancing to an arbitrary host for every request. These are services that are either stateless of have shared state between all instances.\n2. Load balancing to an individual host after resolution. These are services that may have a session state associated with a single instance.\n3. Directing requests to a particular host based on request content. An example might be a queuing service where the Picker (or something) resolves a location for a particular queue. It could also be a key value store where the connection is directed to a particular server where the data is located.\n4. Atomic switchover for a particular \"class\" of service. Imagine a leader/follower service instructed by a watch in an etcd. When there is a switch between leader/follower, the current requests must be cancelled and a new connection must be made to the right service. It is acceptable to rely on service behavior to support this. For example, a leader that is no longer a leader may throw errors that could be intercepted by the picker.\nI assume these are all represented within Google, by your declaration.\nI would also think that, in addition to remote load balancing, we'd want to support in-process load balancing, which is the issue at hand. Note that in each one of those classes of load balancing, there may be services that implement it at the application-level, which is currently problematic in the current Go API.\n\nI meant you put both of them into a single \"Client\" struct.\n\nI don't think I ever proposed struct contents. Client acts like an integration plane in this model. The implementation may immediately dispatch to the Picker or Transport but the user mostly only has to interact with a configured *Client or Invoker.\n. @proteneer While per-request load-balancing is of interest for client-side load balancing, the more compelling feature is security and scalability. From a security perspective, we'd prefer not to have certificates on the load balancers in addition to the endpoints. There is operational overhead and key distribution issues that arise in this model. Client-side load balancing, while more complex and expensive for each client, also doesn't incur the cost of transiting a load balancer process.\nDigressing, the debate on client-side versus out-of-process load balancing is not one I'm looking to have. Both have their merits. I'm hoping that GRPC's Go API can support this position and work in both scenarios based on the requirements of the application.\n. @iamqizhao Sent.\n. Rather than considering each use case, it seems more prudent to create a flexible abstraction that can handle cross-cutting concerns. Couldn't a lot of these use cases be covered by an Invoker interface, proposed in https://github.com/grpc/grpc-go/issues/239#issuecomment-204570585 and https://docs.google.com/document/d/1weUMpVfXO2isThsbHU8_AWTjUetHdoFe6ziW0n5ukVg (thank, @zellyn)?\n. @iamqizhao Would this be a proposal to change this document https://github.com/grpc/grpc/blob/e1d8d4dc1bdf54273b7ed9d4a495789430cbdf6c/doc/connection-backoff.md? What is the goal of having common backoff strategies across languages?\nThe current behavior is problematic, so I'd like to avoid a lengthy multi-language review. 120 seconds is just too long.\n. @makdharma Thanks for the response! That should get us off the ground. \nAlthough, it would be ideal to remove the lower bound on selecting the next deadline. This allows reconnection to be more evenly spread across the wait period. Other strategies might be better in certain conditions. For example, Exponential Backoff has the nice property that one can set the expected wait time for reconnection based on the number of retries. In practice, it means that after fault recovery, the spread of reconnections is uniformly spread across the next retry period. The current approach tends to have clients all recover in the same jitter window, meaning at the worst case, clients that fail right before recovery have to wait the entire window before trying again.\nI can submit a PR similar to the approach above but with backoffStrategy un-exported, if that will be accepted. It would be good to understand the intent on having all language clients implement similar strategies before submitting a proprosal in grpc/grpc.\n. @zellyn We could define a BackoffFunc, which implements BackoffStrategy:\ngo\ntype BackoffFunc func(int) time.Duration\nfunc (fn BackoffFunc) Backoff(retries int) time.Duration { return fn(retries) }\nThat would provide a shortcut for the common case and avoid having to pass method values when more structure is required.\nThis is probably academic until I get a response to my comment above.\n. @iamqizhao Will do. I'll probably be able to do this next week.\n(Just found this message un-posted, sorry about delay).\n. @iamqizhao A WIP PR has been submitted as #601.\n. @iamqizhao Thanks for your help getting this through!\n. @iamqizhao Maybe it would be good to add to this issue so that we can get back errors from rpc calls to detect timeout rather than just insta-closing the issue. With the way things are now, we cannot tell the difference between a timeout and a remote error.\n. I signed it!\nI'm from Docker, Inc.\n. @iamqizhao Is there any further direction on the CLA bot? I don't think I've done this correctly. We've filled out the CLA and signed and added me to the group. Do I need to fill out the form with the same information and submit that, as well? \n@googlebot ?\n. @iamqizhao I'll give that a try.\nPR is ready for review. I am not sure if we need better unit testing, hence the \"WIP\" label. I'll remove for now and take a pass at unit testing if we feel it is needed. Most code paths are covered by existing tests.\n. I signed it!\nDocker, Inc.\n@googlebot \n. I signed it!\n. @iamqizhao What's the current status of this PR?\n. @iamqizhao All the commentary should be addressed now!\n. @iamqizhao Are there plans to handle this for use cases outside of WithBlock? Seems odd that we would just let an RPC method call hang forever. In general, it seems best to avoid WithBlock but there is no way to boil up errors on synchronous operations.\n. > The thing becomes a bit tricky when there may be multiple underlying network connections.\nI get that.\nTypically, this is handled by separating the connection-side from the RPC-side, which isn't really done in grpc-go. One can differentiate between a failure or series of failures on Dial and a single failure on a single RPC.\nLet me ask this another way: how should we surface these kinds of errors to a user? How should we detect this particular state and report it?\n. @iamqizhao \n\n@tamird  In most of use case I have seen the client simply uses the machine CA (credentials.NewClientTLSFromCert(nil, \"\")) which has slim chance to get wrong compared to the server side.\n\nWhat about mtls? What about the distribution of self-signed certificates or certificates outside a machine's root CA?\nIn practice, I see people getting this wrong all the time.\nMost troubleshooting I have seen focuses on getting the right certificates client-side to operate against a fixed server configuration. While this won't be a GRPC problem initially, it will become one as the adoption of GRPC and http/2 servers increase.\nThere needs to be good error messaging so that one can observe and address the problem.\n. @puellanivis In that basic model, two bad things happen:\n1. Errors are obscured from the \"users\" (maybe the caller).\n2. Resources are leaked when callers go away.\nIn short, that analogy doesn't really apply here. There are lots of resources that may be locked under an RPC call. Simply putting the call into a goroutine then going away if it doesn't return in time will leak those resources (not to mention, the goroutine doing the call). We encapsulate this problem using Context and WithTimeout, deferring that functionality to grpc-go.\nWe can see this break down when there are multiple causal failures. Let's say we have a method that we'd like to attempt to call and return to a user in 1s. In the course of that 1s call, tens of TLS connection attempts may have failed. All the user will see in the current model is the timeout error, obscuring the root of the problem.\nEffectively, we have Context to deal with problem 2 but the solution to problem 1 is not addressed.\n. @iamqizhao Sounds good.\nI would suggest adopting the Temporary convention used throughout the Go community.\n. cc @iamqizhao \n. I was led astray by the TODO in https://github.com/grpc/grpc-go/blob/eb7b130505d4b7787791b1c3a8b4dc5949b5c4e0/transport/handler_server.go#L228 and wasn't sure whether this is fully supported or not.\nI actually went a different direction and ended up just mapping our internal errors to grpc codes, as the error code set is very good.\nPerhaps, you need some documentation updates to make it clear that grpc.ErrorDesc et al are deprecated in favor of status, as well some supported paths for including error details.\nIs there any way I can help here?. @dfawley We are generally on board with moving to the status package, but most new developers end up discovering the grpc.ErrorXXX functions and just use those. The changes in #1233 look good (WithDetail, specifically).\nI think the only real thing missing is the ability to add service-specific error codes, complete with string display (stringer doesn't lend itself well to extendable code sets). The case we needed was to represent a locked resource. We've decided to use Unavailable, as that represents the correct user action upon encountering that condition.\nIf you're curious about the approach, the PR is https://github.com/containerd/containerd/pull/1115. I am pretty happy with the result but would like to put stack traces from pkg/error into the details. We've basically mapped all of the error values to GRPC equivalents and throw those from deep application code. It avoids the coupling with grpc error codes, while avoiding ambiguity.\nFor the most part, the struggle is around having application errors become grpc errors, then having those become application errors on the client side, maintaining context and semantics. I think as long as the design keeps that in mind, we'll have what we need.. @dfawley I think these are sufficient. If I find anything that is \"impossible\", I'll raise an issue and see if we can't find a solution.. @MakMukhi Let me know if there is further troubleshooting required to help you isolate the issue.. @dfawley I gave that fix a try and am still seeing the client hang and timeout. Is it possible other changes are needed for this to be a full fix, as well?. @dfawley It looks to be blocking on the balancer picker:\nconsole\n~/g/s/g/c/containerd \u276f\u276f\u276f sudo GRPC_GO_LOG_SEVERITY_LEVEL=INFO  ctr --debug images pull docker.io/library/wordpress:latest                                                                                                                                                                               update-grpc-110 \u272d \u2731 \u25fc\nINFO[0000] connecting to containerd                     \nINFO[0000] dialing containerd                            address=\"/run/containerd/containerd.sock\"\n2018/03/07 13:38:54 dialing to target with scheme: \"unix\"\n2018/03/07 13:38:54 ccResolverWrapper: sending new addresses to cc: [{run/containerd/containerd.sock 0  <nil>}]\n2018/03/07 13:38:54 ClientConn switching balancer to \"pick_first\"\n2018/03/07 13:38:54 pickfirstBalancer: HandleSubConnStateChange: 0xc420077f40, CONNECTING\nThe first two log lines I added to ctr. \"dialing containerd\" is at the point that grpc.Dial is called. I get the same problem with or without the patch applied.. >  In this case, you are confusing our target parsing logic.\nWill all due respect, I am confusing nothing. We updated the package and now the dialer code doesn't work. Whatever changes were put in have created an incompatibility that is breaking our code.\nI get the same output, whether or not #1889 is applied. It doesn't seem to have any effect. The above output is with the patch applied.\nI modified the above example to show the address passed to dial:\nconsole\n~/g/s/g/c/containerd \u276f\u276f\u276f sudo ctr plugins -d type~=snapshotter                                                                                                                                                                                                                                          update-grpc-110 \u272d \u2731 \u25fc\nINFO[0000] connecting to containerd                     \nINFO[0000] dialing containerd                            address=\"/run/containerd/containerd.sock\"\nINFO[0000] dialing containerd                            address_passed_to_dial=\"unix:///run/containerd/containerd.sock\"\n2018/03/08 12:13:02 dialing to target with scheme: \"unix\"\n2018/03/08 12:13:02 ccResolverWrapper: sending new addresses to cc: [{run/containerd/containerd.sock 0  <nil>}]\n2018/03/08 12:13:02 ClientConn switching balancer to \"pick_first\"\n2018/03/08 12:13:02 pickfirstBalancer: HandleSubConnStateChange: 0xc4200b3d70, CONNECTING\nINFO[0000] address in Dialer wrapper                     address=\"run/containerd/containerd.sock\"\nINFO[0000] address in dialer                             address=\"run/containerd/containerd.sock\"\nINFO[0000] address trimmed for net.DialTimeout           address=\"run/containerd/containerd.sock\"\nERRO[0000] dial error                                    error=\"dial unix run/containerd/containerd.sock: connect: no such file or directory\"\nAs you can see, we are passing in the unix:///run/containerd/containerd.sock and when that gets passed to the custom dialer, the address has been stripped to run/containerd/containerd.sock.\nHere is the patch to containerd used to debug this:\n```diff\ndiff --git a/client.go b/client.go\nindex 2ac256dd..b9360247 100644\n--- a/client.go\n+++ b/client.go\n@@ -40,6 +40,7 @@ import (\n    \"github.com/containerd/containerd/dialer\"\n    \"github.com/containerd/containerd/errdefs\"\n    \"github.com/containerd/containerd/images\"\n+   \"github.com/containerd/containerd/log\"\n    \"github.com/containerd/containerd/namespaces\"\n    \"github.com/containerd/containerd/platforms\"\n    \"github.com/containerd/containerd/plugin\"\n@@ -94,16 +95,23 @@ func New(address string, opts ...ClientOpt) (Client, error) {\n        )\n    }\n    connector := func() (grpc.ClientConn, error) {\n-       conn, err := grpc.Dial(dialer.DialAddress(address), gopts...)\n+\n+       log.L.WithField(\"address\", address).Infoln(\"dialing containerd\")\n+       addressPassedToDial := dialer.DialAddress(address)\n+       log.L.WithField(\"address_passed_to_dial\", addressPassedToDial).Infoln(\"dialing containerd\")\n+       conn, err := grpc.Dial(addressPassedToDial, gopts...)\n        if err != nil {\n            return nil, errors.Wrapf(err, \"failed to dial %q\", address)\n        }\n        return conn, nil\n    }\n+   log.L.Infoln(\"connecting to containerd\")\n    conn, err := connector()\n    if err != nil {\n        return nil, err\n    }\n+\n+   log.L.Infoln(\"connected to containerd\")\n    return &Client{\n        conn:      conn,\n        connector: connector,\ndiff --git a/cmd/ctr/main.go b/cmd/ctr/main.go\nindex ec41c59a..a72c8fba 100644\n--- a/cmd/ctr/main.go\n+++ b/cmd/ctr/main.go\n@@ -18,7 +18,6 @@ package main\nimport (\n    \"fmt\"\n-   \"io/ioutil\"\n    \"log\"\n    \"os\"\n@@ -45,7 +44,7 @@ var extraCmds = []cli.Command{}\nfunc init() {\n    // Discard grpc logs so that they don't mess with our stdio\n-   grpclog.SetLogger(log.New(ioutil.Discard, \"\", log.LstdFlags))\n+   grpclog.SetLogger(log.New(os.Stderr, \"\", log.LstdFlags))\ncli.VersionPrinter = func(c *cli.Context) {\n    fmt.Println(c.App.Name, version.Package, c.App.Version)\n\ndiff --git a/dialer/dialer.go b/dialer/dialer.go\nindex 766d3449..35dec9ec 100644\n--- a/dialer/dialer.go\n+++ b/dialer/dialer.go\n@@ -20,6 +20,7 @@ import (\n    \"net\"\n    \"time\"\n\n\"github.com/containerd/containerd/log\"\n    \"github.com/pkg/errors\"\n )\n\n@@ -30,6 +31,7 @@ type dialResult struct {\n// Dialer returns a GRPC net.Conn connected to the provided address\n func Dialer(address string, timeout time.Duration) (net.Conn, error) {\n+   log.L.WithField(\"address\", address).Infof(\"address in Dialer wrapper\")\n    var (\n        stopC = make(chan struct{})\n        synC  = make(chan *dialResult)\n@@ -43,6 +45,7 @@ func Dialer(address string, timeout time.Duration) (net.Conn, error) {\n            default:\n                c, err := dialer(address, timeout)\n                if isNoent(err) {\n+                   log.L.WithError(err).Error(\"dial error\")\n                    <-time.After(10 * time.Millisecond)\n                    continue\n                }\ndiff --git a/dialer/dialer_unix.go b/dialer/dialer_unix.go\nindex e7d19583..8a3fddc5 100644\n--- a/dialer/dialer_unix.go\n+++ b/dialer/dialer_unix.go\n@@ -25,6 +25,8 @@ import (\n    \"strings\"\n    \"syscall\"\n    \"time\"\n+\n+   \"github.com/containerd/containerd/log\"\n )\n// DialAddress returns the address with unix:// prepended to the\n@@ -47,6 +49,8 @@ func isNoent(err error) bool {\n }\nfunc dialer(address string, timeout time.Duration) (net.Conn, error) {\n+   log.L.WithField(\"address\", address).Infof(\"address in dialer\")\n    address = strings.TrimPrefix(address, \"unix://\")\n+   log.L.WithField(\"address\", address).Infof(\"address trimmed for net.DialTimeout\")\n    return net.DialTimeout(\"unix\", address, timeout)\n }\n```. @dfawley How is this possibly correct behavior? Is the unix dialing scheme even supported? We still have to inject our own dialer for this to work at all. If the existing dialing stack doesn't understand the scheme, why is it parsing and modifying it? This seems completely broken.\nFor example, if you parse off the scheme for unix:///foo, the correct answer for the path is /foo, not foo. That doesn't seemed to be clear in the spec, but that is the correct way to handle URIs in most other systems.. @dfawley With this scheme, how would one continue using a custom dialer if the package incorrectly returns the path for the filesystem?\nI appreciate the RFC, but it seems like important details still need to be figured out.. > unix:////a/b/c\nNo system in the world implements it this way.. @dfawley Can we reopen this issue for tracking?\nPlease us know if you need help with building the use case.. > In the meantime, please use passthrough:/// to make sure your dialer gets \"\".\nWe probably will avoid upgrading, for now. I hope a solution can be found that doesn't break everyone.\nI'll also try to attend the community meeting to get a better understanding of the proposal. I know I reviewed earlier versions of this but the current proposal doesn't look very familiar.. > I would rather not make assumptions about the format of the target to determine whether to fallback to a default; that just makes things more complicated.\nThis might be the main problem with the logic. The correct parsing of a URI will pass everything after the scheme:// as completely own by the scheme, which would include that trailing slash in unix:///. There may also need to be more clarity around how a given scheme converts a URI into a (network, address) tuple that is passed into the net.Dialer. If we look at https://godoc.org/google.golang.org/grpc/naming#Update for example, it doesn't really have all the necessary inputs for dialer. Also, if we look at https://godoc.org/google.golang.org/grpc#BalancerConfig, the dialer there doesn't match the dialer from the stdlib, at all.. > We will keep the behavior of Dial to avoid possible breakages.\nAren't people already broken? Everyone is having to fix this in the leaf projects to make it work for users.. @menghanl Thanks for addressing this one!. BTW, I just confirmed the fix against 1.7 servers with 1.10.1 clients and everything looks good. Tests are ran as part of https://github.com/containerd/containerd/pull/2186 and we are looking good.. Is there a better, more permanent link?\n. We can change to use a value here. The interface will work correctly either way.\n. Cheers. I'll leave as is.\n. @iamqizhao I had backoff originally but it stutters. Most uses would be cc.dopts.backoff.backoff(n). I'll resubmit with this as bs but I'm open to other suggestions.\n. I suppose that works. Don't we want to apply defaults anyways if a partial backoffconfig is provided?\n. WithBackoffConfig(&BackoffConfig{MaxDelay: time.Second}) is invalid without this fix. It is only a partial config. Your proposal is fine, but I don't see why fixing this requires creating another exported function.\n. Will do.\n. Updated, with tests.\n. You cannot have constant structs in Go. We can make this a concrete value to get the same effect. In fact, making this a value was discussed in the original PR and we went with the pointer type.\nI'll make this change in a separate commit in case this isn't what you want.\n. ",
    "oxtoacart": "The problem with using grpc.Codec is that it operates one message at a time.  When streaming results, compression would work better when done at the stream level rather than the message level, since repetition of data may occur across messages rather than within them.\n. Ah, looks like there are some options now to use compression, for example https://godoc.org/google.golang.org/grpc#RPCCompressor\n. ",
    "pboyer": "@iamqizhao As noted by @oxtoacart, there are already compression API's. Are these API's supported? Is this issue to expand the set of supported compressors?. ",
    "c4milo": "This is already covered with the Compressor and Decompressor types, right?. It would be very nice if the underlined backoff mechanism covers this case as well.\n. I was going to mention @jellevandenhooff's observation too, I thought there was some sort of exponential backoff involved when reconnecting. \n. @menghanl in my case the server did not have properly setup the certificate.\n. @dsymonds is this gone from master? I can't seem to find it. . @menghanl, interesting! I have not but I will try it. Thank you!. So the suggestion is not to use gRPC for uploading arbitrary blobs of data? is it better to use HTTP multipart requests?\n-- Apologies for commenting on closed issues. \n. @vitalyisaev2 do you have any suggestion about how to do this? I came up with an approach I'm not feeling very proud of (using unsafe.Sizeof on unserialized messages and wild guesses).\nSerializing beforehand in order to get the real size seems too wasteful since gRPC would encode the message again before sending.  \nPerhaps having a custom codec with a timer and flushing whenever it reaches a size limit or a timeout? \n. @vitalyisaev2 nice, I had something very similar. I'll keep using this approach then. Thank you!. @dfawley, shouldn't this be closed since it seems addressed by https://github.com/grpc/grpc-go/pull/506? . FWIW, while testing https://github.com/philips/grpc-gateway-example as-is, and also testing my own usage of grpc.(*Server).ServeHTTP using Go 1.7rc6, I ran into the issue I just linked above. \n. > The http.Handler-based server only works with TLS right now.\n@bradfitz I haven't been able to make it work, even with TLS. This is the error I get https://github.com/philips/grpc-gateway-example/issues/11\n. The only way I was able to make it work was using explicitly  srv.ListenAndServeTLS\n. Is there an issue open to follow up on the migration to the standard HTTP2 package? I couldn't find it. . Once, when the first request is received, I would like to validate the token's scope in the specific handler, before letting the user to consume the stream.  But, I don't have any way to attach the token's scope info to the context of the request. From there on, I'm relying on TLS.. yes, I can totally try that. Thanks! . This change also broke my code. Although, I understand it was done because of security concerns. . @ghasemloo any update? . @dfawley, i'm trying to use the new Details/WithDetails added in #1233 with grpc-gateway, using its custom httpError handler. But, I haven't been able to make it work. The grpc client call always returns Details:[]*any.Any(nil). Any help or code sample is very much appreciated.. @dfawley, @ghasemloo,  that's exactly what I'm returning but I get nil details from the client. I might be missing something important or this just does not work using GRPC's  HTTP handler.  \nServer: https://github.com/c4milo/hello/blob/master/hello_service.go#L47-L53\nClient: https://github.com/c4milo/hello-test/blob/master/main.go\nResults: \n\n. @dfawley, I can give it a try if you are kind to fast-track my way into the code base, please :). @dfawley https://github.com/grpc/grpc-go/pull/1438. @ghasemloo thanks for this work! I'll be testing this PR today along with grpc-gateway to see if I can finally send structured app errors. . \ud83d\udc6f . @dfawley np, thanks for taking the time to review! . shouldn't this validation be done before decompressing the message? . additionally, it seems the message size validation is also being done in the parser: https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L253. Sure, I missed that one. I'll fix it as soon as I can get my hands on my laptop.. @dfawley done. ",
    "bboreham": "Since this is the top hit when I search for info on compression, I'll add a link here to\nhttps://github.com/grpc/grpc-go/blob/master/Documentation/compression.md. We have a very similar symptom - lots goroutines stuck in transport.(*quotaPool).get.  \nHere is the top of a goroutine dump:\n```\ngoroutine profile: total 84191\n83999 @ 0x42e1cc 0x43e438 0x7c8e2a 0x7debfa 0x812267 0x813463 0x816dc8 0x81e86f 0x45d411\n0x7c8e29    github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport.(*quotaPool).get+0x279 /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport/control.go:191\n0x7debf9    github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport.(*http2Server).Write+0x429 /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/transport/http2_server.go:885\n0x812266    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.(*Server).sendResponse+0x276     /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/server.go:770\n0x813462    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.(*Server).processUnaryRPC+0xfc2  /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/server.go:946\n0x816dc7    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.(*Server).handleStream+0x1527    /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/server.go:1143\n0x81e86e    github.com/weaveworks/cortex/vendor/google.golang.org/grpc.(*Server).serveStreams.func1.1+0x9e  /go/src/github.com/weaveworks/cortex/vendor/google.golang.org/grpc/server.go:638\n```\nAs far as I know we only create one connection per client-server pair.  [EDIT: I mis-described the situation]  We have multiple programs making the same gRPC calls, but only one type of caller has the symptom.\nIt will probably repeat in the next 1-2 days if there is more info I can collect.. Looking at the code, I would suspect fairness issues.  You have both a queue of waiters (waiting on a chan) and a lock to claim the quota, so any active goroutine can grab the lock and the quota before the waiter gets a chance to execute.. Thanks.  I had not seen that doc. \nI would still find it useful to see an example of how to use the calls described, perhaps as an extension to https://github.com/grpc/grpc-go/blob/master/examples/. To get the gzip compressor registered, is the idea that you do:\nimport _ \"google.golang.org/grpc/encoding/gzip\"\n\n?\nAlso it feels like there should be a constant for \"gzip\" rather than having me type it out and possibly misspell it.\n. I think it\u2019s trying to do too much - it gives examples of many different things, but not a copy-pasteable one for the most common \u201cturn on gzip\u201d. \nMaybe separate the doc into advice for users and advice for implementers of compression?. ",
    "therc": "I assume this means a Go port of Census, right? The C-based gRPC is getting the resource API implementation: https://github.com/grpc/grpc/pull/6774\n. Any progress? On Jan 19 you mentioned on the mailing list that there will be a load balanced picker. For us that need the functionality, though, that doesn't help much.\nThe code tells us \"not implement your own Picker for now\". There's no Go spec for the next steps and, absent that, I doubt any PRs from users would be accepted, either. So we can't use a built-in solution, we have little guidance on how to help build one and are also discouraged from writing our own. And with vague/changing timelines, it's hard to make a decision on whether to go for an alternative. This issue has been open for 2/3 of a year now. It looks like grpc-go is understaffed.\nAnyway, upstream has posted this a month ago: https://github.com/grpc/grpc/blob/master/doc/load-balancing.md\nwhich smells a lot like GSLB. Is that the direction for grpc-go, too?\n. Thanks for the update! We'll just use https://github.com/benschw/srv-lb and a custom Picker for now.\n. I agree that the API as implemented today is half baked and unintentionally broken for several purposes. It would have been superseded already if the new one hadn't taken so long.\nAnyway: @stevvooe, why push instead of pull? The latter can happen on the client's own schedule, not the load balancer's. The push model does have the advantage of being able to forward right away updates about servers that just joined or left the pool. I suspect pull is going to stay, though, at least for historical reasons (and those might have had a rationale like mine or additional ones, such as efficiency).\nI also agree about the need for flexible hooks for building more complex machinery on top, in an efficient manner.\n. I think the central part of @stevvooe's comment https://github.com/grpc/grpc-go/issues/239#issuecomment-190875540 sums up the main issues (puzzling dummy argument passed to Dial, loss of control over Conn objects).\n. > For example, in Google may find it acceptable to deploy a separate load balancing process that can manipulate IP tables to route RPC requests, but this may be impossible in another environment.\nKubernetes has iptables-based balancing, but Borg never did. You'd use organization-wide balancers, managed balancers, private balancers or a client-level balancer.\n. For the record, the C++ tree has early code for a Census implementation: https://github.com/grpc/grpc/tree/master/src/core/census\n. I am going to implement client support for this in Kubernetes: https://github.com/kubernetes/kubernetes/issues/21493\nMy main concern is about the \"v1alpha\". Is it going to be supported when e.g. gRPC hits 1.0? What do you recommend doing, @iamqizhao?\n. @ppg this is an example of the haproxy creativity you were asking about: http://www.juhonkoti.net/2015/11/26/using-haproxy-to-do-health-checks-to-grpc-services\n. Awesome, I was not in a hurry. :-) We use mainly Go and Python, so, yeah, I understand your need for coordination.\n. There's a similar issue on the C side: https://github.com/grpc/grpc/issues/8187\n. cc @rw @gwvo \n. This should be fixed now in HEAD, only waiting for release. Closing.. @menghanl We just saw that, thanks! . Retries are the kind of scenario I had in mind when I said in the original issue that maybe the peer could be set twice. And yeah, hedging is even trickier. Personally, I'd be OK if initially gRPC just tracked the last peer that succeeded/failed.\nOur use case: we're trying to debug an issue with canceled contexts that does not manifest itself in a more controlled environment. . Same here (consistent hashing with bounded load).. Any documentation?. @jiangtaoli2016 I see, thanks. Also, another suggestion for the whitepaper: a section on \"Why not Kerberos?\". I think I know some of the reasons, but others are bound to ask the same question.. ",
    "GeertJohan": "How would this work to do connection based authentication? So that you don't have to check password for each rpc call?\n. I've seen https://godoc.org/google.golang.org/grpc/credentials#TransportAuthenticator, but it would be nice to have an example of how this would work in practice.\n. This sounds pretty handy.\nThere is some overlap with the DialOptions and Credentials way of handling auth, I wonder how this will fit between those.\n. Wouldn't it be better to add the db connection as a field to the server structure that implements the service interface? I don't think context is supposed to be used for values like these..\n. Yes. When adding it to the context you'll have to get the value from the context and type-assert it to be your db connection...\nI don't think adding a line of code to each service implementing struct is a large problem.. If you have multiple services you could add the connection as a global instance or create a global getter.\n. I agree with @dsymonds that \"Server\" could be a good name for something that implements the server side of a service. But the name is indeed confusing with grpc.Server.\nThe grpc.Server can serve multiple server's that implement the service's server interface.. :stuck_out_tongue_winking_eye: \nSo how about adopting a new word? For instance Handler?\nA Handler interface describes a Service and is implemented by the user.\ne.g.:\n``` go\n// Generated from the proto service definition\ntype FoobarHandler interface {\n    Foobar(int) (int, error) // rpc method\n    OtherThing(int\n}\n// Registration linker\nRegisterFoobarHandler(*grpc.Server, FoobarHandler)\n```\nNow the grpc.Server can serve multiple Handlers which implement the service's Handler interface.\nThe name Handler is already being used by grpc, but afaik mostly internally.\nFor instance, this is generated code:\ngo\nvar _Foobar_serviceDesc = grpc.ServiceDesc{\n    ServiceName: \"foobar.Foobar\",\n    HandlerType: (*FoobarServer)(nil),\n    Methods: []grpc.MethodDesc{\n        {\n            MethodName: \"Info\",\n            Handler:    _Foobar_Info_Handler,\n        },\n    },\n    Streams: []grpc.StreamDesc{\n        {\n            StreamName:    \"Subscribe\",\n            Handler:       _Foobar_Subscribe_Handler,\n            ServerStreams: true,\n        },\n    },\n}\nWe can see Handler is already being used as a field to link service methods, but it also refers to the Server itself (HandlerType).\nI don't think that the ServiceDesc, MethodDesc and StreamDesc's are to be modified by the user, so the Handler word might be a good option here.\n. :+1: \n. Retriever is a better name; following the interface naming convention.\nAt first I found Retriever a bit too non-descriptive, but because this is in the package credentials most usage will of course be credentials.Retriever. :+1:\n. I think Metadata should be Retrieve\n. nil is a value too..\nIn the same way, is GetRequestMetadata a getter that does not get anything?\n. ",
    "ishbir": "I can't figure out how to use simple password based authentication. If I use TransportAuthenticator, https://github.com/grpc/grpc-go/blob/master/transport/http2_client.go#L127 won't let me have TLS and password based auth at the same time. I can't figure out how to use per request based authentication either.\nA small tutorial or a snippet of code would be highly appreciated.\nEDIT: I figured out how to use per request RPC auth. It's all about sharing metadata. For my fellow grpc-go users (and noobs, like me), I'm leaving it here. On your server, have something like:\ngo\nmd, _ := metadata.FromContext(ctx)\nif !authenticate(md[\"username\"], md[\"password\"]) {\n    return nil, ErrAuthenticationFailed\n}\nOn the client side, you need to have:\n``` go\ntype passCredential int\nfunc (passCredential) GetRequestMetadata(ctx context.Context) (map[string]string, error) {\n    return map[string]string{\n        \"username\": \"admin\",\n        \"password\": \"admin123\", // Stupid password.\n    }, nil\n}\n```\nThen, while dialling to the server:\n``` go\nvar cred passCredential\n// Set up a connection to the server.\nconn, err := grpc.Dial(address, grpc.WithPerRPCCredentials(cred))\n```\nStill couldn't figure out how to do auth once and store it in the context.\n. ",
    "Gurpartap": "@ishbir Make a call to AuthenticationService{}.Authenticate(user, pass) on the server. Make this function on the server to create a session token, store it in memory (memcache or redis) and return it to the client. Then have the client set this session token in metadata; and have the server verify it against the session store.\nOr use JWT (with exp) instead of session tokens to avoid the DB trip.\n. ",
    "amenzhinsky": "I would recommend to use interceptors:\n``` go\n// client\ngrpc.Dial(target,\n    grpc.WithInsecure(),\n    grpc.WithPerRPCCredentials(&loginCreds{\n    Username: \"admin\",\n    Password: \"admin123\",\n}))\ntype loginCreds struct {\n    Username, Password string\n}\nfunc (c *loginCreds) GetRequestMetadata(context.Context, ...string) (map[string]string, error) {\n    return map[string]string{\n        \"username\": c.Username,\n        \"password\": c.Password,\n    }, nil\n}\nfunc (c *loginCreds) RequireTransportSecurity() bool {\n    return true\n}\n// server\ngrpc.NewServer(\n    grpc.StreamInterceptor(streamInterceptor), \n    grpc.UnaryInterceptor(unaryInterceptor)\n)\nfunc streamInterceptor(srv interface{}, stream grpc.ServerStream, info *grpc.StreamServerInfo, handler grpc.StreamHandler) error {\n    if err := authorize(stream.Context()); err != nil {\n        return err\n    }\nreturn handler(srv, stream)\n\n}\nfunc unaryInterceptor(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {\n    if err := authorize(ctx); err != nil {\n        return err\n    }\nreturn handler(ctx, req)\n\n}\nfunc authorize(ctx context.Context) error {\n    if md, ok := metadata.FromContext(ctx); ok {\n        if len(md[\"username\"]) > 0 && md[\"username\"][0] == \"admin\" &&\n            len(md[\"password\"]) > 0 && md[\"password\"][0] == \"admin123\" {\n            return nil\n        }\n    return AccessDeniedErr\n}\n\nreturn EmptyMetadataErr\n\n}\n``\n. @milewski, seems like now you have to passgrpc.WithInsecure()togrpc.Dialexplicitly unless you set transport credentials. . I'm still do not understand why you need this, since you can set any tls option you want using theNewTLS` function:\ngo\ncredentials.NewTLS(&tls.Config{ServerName: \"example.com\"})\n. ",
    "milewski": "@amenzhinsky your code keeps yelling me with \ngrpc: no transport security set (use grpc.WithInsecure() explicitly or set credentials)\nsomething has changed?. ",
    "ghost": "So this is basically certificate pinning right ?\nHow is the expected usage here ? does clientCert == serverCert ?\n. I see another issue got confused also by TLS client certificate ..\nI propose the following signatures:\n```\n// NewClient constructs secure connection for client with optional rootCA\nfunc NewClient(server string, rootCA *x509.CertPool) TransportAuthenticator {}\n// NewClientFile constructs secure connection by loading rootCA from local file\nfunc NewClientFile(server, rootCAFile string) TransportAuthenticator {}\n// NewServer constructs a new server\nfunc NewServer(cert *tls.Certificate) TransportAuthenticator {}\n// NewServerFile constructs a new server by loading cert and key from local file\nfunc NewServerFile(certFile, keyFile string) (TransportAuthenticator, error) {}\n```\nThis way it's shorter (we know it's always TLS anyway), and people don't confuse for \"TLS client authentication\"\n. I see. Hope it doesn't get too bloated in the future. Closing this.\n. Ahh okay, just enjoy your holiday man .. :)\n\n\nThis should not happen if the interval between step 2 and 4 is long enough.\n\n\nHow long is long enough ? Where is this number defined ?\nI only tested for short interval between 2-4 (around 2 minutes), will try to test for longer duration. \n. I added a gist to help debugging. In my test, after turning of network about 2.5 minutes, message won't be retransmitted.\nIs this expected ? how can I control the period ?\n. Yes, I'm sure. I just reinstalled all dependencies (grpc, protobuf) just in case.\n- go version go1.4 linux/386\n- two machines, internal network\nSteps:\n- Run client.go on machine1, server.go on machine2\n- Turn off network connection in machine1, it should exit automatically. Output\n2015/04/16 06:04:52 Received 0 from server\n    2015/04/16 06:04:55 Received 1 from server\n    2015/04/16 06:04:58 Received 2 from server\n    2015/04/16 06:05:01 Received 3 from server\n    2015/04/16 06:05:09 stream error rpc error: code = 4 desc = \"context deadline exceeded\"\n    2015/04/16 06:05:09 client shutting down\n- Machine2 stays on:\n2015/04/16 06:04:52 Received 0 from client\n        2015/04/16 06:04:55 Received 1 from client\n        2015/04/16 06:04:58 Received 2 from client\n        2015/04/16 06:05:01 Received 3 from client\n- Turns on network on machine1\n- After some period, the fourth message appear on machine2\n2015/04/16 06:05:58 Received 4 from client\n    2015/04/16 06:05:58 grpc: Server.processUnaryRPC failed to write status: connection error: desc = \"transport: use of closed network connection\"\n. If it helps, netstat says FIN_WAIT1 on machine1 while connection's off.\n. It's still not clear what's the use of context. Graceful shutdown ?\n. No, I understand what context is, but in this case, server implementation could just create it's own context if needed. \nIs there any variable bound to the passed context, or is it just to listen to <-context.Done() (graceful shutdown) ?\n. As I understand from previous issue, client context doesn't get passed to server. So I don't see any use of it here.\nAn application can have its own context tree, and then only listen to that (if they wanna use context). Imposing the use of context here seems superfluous.. \n. To make myself clear, in blog.golang.org, the interface doesn't impose the use of context. It stays func (w http.ResponseWriter, req *http.Request){} instead of func (ctx context.Context, w http.ResponseWriter, req *http.Request){}\nApplication that wants to use context can just create it inside the implementation.\n. I don't have any problem with it, since it seems it's already decided early. My question is clear from the beginning: What is the use of context in grpc implementation as of now?  Graceful shutdown ? Any values bound to the passed context before ?\n. I'm asking this because I need to know in my long running grpc handler, what is the use case that might arise on <-ctx.Done() passed from grpc. Please don't be so condescending.\n. Sorry, I should be more clear. Thanks for answering...\nI ended up browsing the code. It seems that the context hierarchy might still change. Currently:\n1. It's new context derived from context.TODO() for every new transport.Stream\n2. Stream scoped, closed when the stream is closed \nClosing this issue since it's probably changed in the future.\n. @iamqizhao, do you have any docs that explains the new design and how to use it? Thanks.\n. Thank you @chancez and @iamqizhao for the pointers. We can work with that. \ud83d\udc4d \n. After digging into conn.Wait(ctx), I assume that I've somehow messed up the ctx in my code. I close this issue now and investigate down the context path.  \nsee below\n. Sorry for the open/close/open noise, I've confused myself with my usage of context, but that was in fact not related. \nAlso this is not related to \"reconnect\" as my client will re-start NewClientStream in case of any error (needed for my use case). So actually the problem manifests whenever a client tries to connect to a server, which does not listen (connection refused) and is easily reproducable with the stream example code:\n$ # start a route_guide client w/o starting a server first:\n$ go run $GOPATH/src/google.golang.org/grpc/examples/route_guide/client/client.go\nResult: 100% consumed CPU cycles\nThe busy wait problem does appear to be in above quoted code. conn.wait(ctx) returns ErrTransientFailue \"so that the caller gets chance to pick another transport to perform rpc instead of sticking to this transport,\" but apparently cc.dopts.picker.Pick() does not return \"another transport,\" because obviously there is none at all. \n. Hey MakMukhi,\n\nAFAIK, No, I do not have a custom balancer.\nNo idea, don't think so.\nNo idea.\n\nAs I mentioned, we saw this issue while attempting to use the Google Cloud Datastore client libraries. Reverting the last few commits from grpc-go seems to have fixed the issue for us.\nThe smallest possible repro is at: https://gist.github.com/dhavalcue/8fc6aba28aad59603fc01351a3dca6d6\nThe backtrace where it deadlocks is at: \nhttps://gist.github.com/dhavalcue/95558572a4d9fd827ce0a029d52789e1. @menghanl: #1606 seems to have fixed the problem. It's working as expected now. Will close this issue.. ",
    "kenkeiter": "I'll see if I can find some time to dig into it in the next couple of days.\nAs an aside: this is an excellent project. Really looking forward to using it when we get these perf issues sorted. Thank you for your hard work :+1:\n. ",
    "maxhawkins": "+1\nExposing these fields would also give errors a better JSON serialization, which could be useful when mapping GRPC to a REST API.\n. Could you make rpcErr similar enough to the internal status type that it will be compatible when/if you open source it? In the meantime a decent JSON serialization would be tremendously helpful.\n. Ah, I see. I assumed this only applied to the initial connection. I suppose this makes sense.\nI'll close this issue. Thanks!\n. Just signed it. Not sure how long it takes to go through.\n. ",
    "yinhm": "+1\nI want  to return something like \"404 page not found\" to the client, It would be easier if export RPCError.\n. I was thinking can we reconnect a little bit of more aggressive, I mean, I need to restart client mostly of the time when I restart server doesn't feels right. It killing my productivity.\nMaybe reconnect if there is new rpc calls in client? Maybe just a little of aggressive, my server has no complain of that. What I want is thought of like zeromq or nanomsg, you don't need to care if server was going down, you just send messages.\n. I encountered another stream dead lock, \n- server stream handler return error\n- client waits indefinitely(I could use a timeout contenxt, but it wasn't what I want)\nI want to produce it by tests then report it, just not have time yet.\n. @dsymonds Sounds logic. \nThere is no way client send that much data. Also, the counted bytes are random, eg:\n2015/04/10 14:39:05 transport: http2Server connection error: desc = \"recieved 4294959553-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:08 transport: http2Server connection error: desc = \"recieved 4294955938-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:09 transport: http2Server connection error: desc = \"recieved 4294963741-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:10 transport: http2Server connection error: desc = \"recieved 4294964600-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:15 transport: http2Server connection error: desc = \"recieved 4294962664-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:25 transport: http2Server connection error: desc = \"recieved 4294957419-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:28 transport: http2Server connection error: desc = \"recieved 4294959790-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:28 transport: http2Server connection error: desc = \"recieved 4294960371-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:31 transport: http2Server connection error: desc = \"recieved 4294958567-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:33 transport: http2Server connection error: desc = \"recieved 4294965801-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:35 transport: http2Server connection error: desc = \"recieved 4294966930-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:38 transport: http2Server connection error: desc = \"recieved 4294959610-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:44 transport: http2Server connection error: desc = \"recieved 4294962618-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:52 transport: http2Server connection error: desc = \"recieved 4294960581-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:54 transport: http2Server connection error: desc = \"recieved 4294962832-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:39:59 transport: http2Server connection error: desc = \"recieved 4294966675-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:40:04 transport: http2Server connection error: desc = \"recieved 4294966801-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:40:08 transport: http2Server connection error: desc = \"recieved 4294965389-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:40:20 transport: http2Server connection error: desc = \"recieved 4294952903-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:40:29 transport: http2Server connection error: desc = \"recieved 4294956834-bytes data exceeding the limit 1048560 bytes\"\n2015/04/10 14:40:32 transport: http2Server connection error: desc = \"recieved 4294960782-bytes data exceeding the limit 1048560 bytes\"\n. @iamqizhao Sorry, I may not be able to reproduce this. I encountered this issue when archiving our feed at friendfeed, https://github.com/yinhm/friendfeed \nSince friendfeed was closed, I have no longer be able to reproduce. So sad.\n. ",
    "hezhit": "+1\nrpcErr is very useful for error handling.\n. ",
    "joeatwork": "+1\nIt'd be great to be able to pass and read error descriptions\n. ",
    "sym3tri": "This is something I am very much in need of. Otherwise we'll have to resort to the gross last resort of parsing the error string.\n. @iamqizhao that would suite my needs and seems like a very simple solution.\n. ",
    "psanford": "Any update on this?\n. There is still no easy way to get certificate info inside a handler. What do you think of @tv42's patch for this: https://github.com/bazil/grpc-go/commit/3ff306a8149915033913e9ac6651695c9dc36e94?\n. Awesome. Thanks for the update.\n. A custom authenticator only solves the problem if it can pass information about the certificate on to the rpc endpoints. I don't know how you would do that with the current implementation.\n. It seems like the information is still not available to the rpc endpoints. From my brief investigation, it looks like the code in transport.newHPACKDecoder() that copies the metadata from d.mdata to d.state.mdata never gets called.\n. This looks great! Thanks for getting this merged.\n. You still have to implement your own TransportAuthenticator to get access to the certificates. Its probably easiest to make a new struct type that embeds a TransportAuthenticator and just overrides the ServerHandshake method.\n. You can use client certs without much difficulty. credentials.NewTLS(c *tls.Config) just takes a plain old crypto/tls.Config so its just like any other go tls program that supports client certs. On the tls.Config  you set the client Certificates on the client and the client CAs and ClientAuth: tls.RequireAndVerifyClientCert on the server.\n. I just tried to use tls.Config.KeyLogWriter in go 1.8 but it didn't work because cloneTLSConfig hasn't been updated for 1.8 yet. I assume 1.8+ can just use the new tls.Config.Clone() method. . I signed it!. This fixes #1042. ",
    "andres-erbsen": "Are there any updates on this?\n. I'm glad to hear that you are working on it. Which fields of tls.ConnectionState should be passed through depends on the limitations you are willing to impose on the users of this library -- tls.ConnectionState is already a chosen subset of the connection parameters, selected to be presented to the application developers.\n- I think HandshakeComplete is always true when a RPC has been received, so it can be dropped.\n- I would also guess that NegotiatedProtocol is used to negotiate http2, and thus redundant as well.\n- If http2 handles ServerName already, it is probably safe to drop it from here. If not, it is required for virtual hosting.\n- VerifiedChains and PeerCertificates -- required for doing mandatory and optional authentication (respectively) using plaintext certificates.\n- TLSUnique is in a significant sense the correct way to externally authenticate TLS connections; not providing it will result in the users who need external authentication implementing ugly hacks and likely messing up security-wise.\n- OCSPResponse and SignedCertificateTimestamps -- for checking extended validation (the \"green bar\" in browsers). Short-lived certificates (OCSP)  with transparency (SCT) are the future of TLS, and it would be a pity to keep from the users of this library.\n- I am not attached to the Version, DidResume, and CipherSuite fields; the only use case I can think of is requiring different TLS configurations for different RPCs...\n. ",
    "smithbk": "So if I want access to the client certificate, what is the conclusion here?  Do I have to write my own TransportAuthenticator and override the ServerHandshake method?  If yes, is there an example?\nIt would be very useful to have a simple example of how to access the client certificate.\nThanks\n. ",
    "jcramb": "This is also something I require in my project -> after reading through this and some of the Credentials source code I've come up with the following. Would appreciate some feedback on whether this is the intended way to solve this problem or if there is something more elegant...\ntype custom struct {\n    credentials.TransportCredentials\n}\n\nfunc (c *custom) ServerHandshake(conn net.Conn) (net.Conn, credentials.AuthInfo, error) { \n    conn, authInfo, err := c.TransportCredentials.ServerHandshake(conn)\n    tlsInfo := authInfo.(credentials.TLSInfo)\n    name := tlsInfo.State.PeerCertificates[0].Subject.CommonName\n    fmt.Printf(\"%s\\n\", name)\n    return conn, authInfo, err\n}. @smithbk Actually for posterity, I've just discovered that the client certificate can also be accessed using the grpc Peer package to extract the Addr/AuthInfo for a client from within an RPC call.\n\nfunc (s *server) HandleCommand(ctx context.Context, in *pb.CommandRequest) (*pb.CommandReply, error) {\n    peer, ok := peer.FromContext(ctx)\n    if ok {\n        tlsInfo := peer.AuthInfo.(credentials.TLSInfo)\n        v := tlsInfo.State.VerifiedChains[0][0].Subject.CommonName\n        fmt.Printf(\"%v - %v\\n\", peer.Addr.String(), v)\n    }\n    return &pb.CommandReply{Out: \"Cmd: \" + in.Cmd}, nil\n} . @iamqizhao This is an issue for me. What is the intended method of determining when a client has disconnected? In my project I can determine when specific clients connect by implementing a custom TransportCredentials and looking at the certificate details but I've been unable to find a reliable way to be notified when these clients disconnect.\n\nMy use case is long-term client connections using TLS and I need to track when each client connects/disconnects to form an accurate picture of all currently active clients. Is it intended that I implement my own client heartbeat mechanism... this seems a little silly considering that the information is already known internally to gRPC and it's removeConn() server method.\nIs there something I'm missing?. ",
    "MakMukhi": "Currently, to access the connection state you have to assert that type of\nAuthInfo object returned from handshake is TLSInfo and access the field\nnamed State.\nOn Fri, Jan 27, 2017 at 2:47 PM, John Cramb notifications@github.com\nwrote:\n\nThis is also something I require in my project -> after reading through\nthis and some of the Credentials source code I've come up with the\nfollowing. Would appreciate some feedback on whether this is the intended\nway to solve this problem or if there is something more elegant...\n` type custom struct {\ncreds credentials.TransportCredentials\n}\nfunc (c *custom) ClientHandshake(ctx context.Context, s string, conn\nnet.Conn) (net.Conn, credentials. AuthInfo, error) {\nreturn c.creds.ClientHandshake(ctx, s, conn)\n}\nfunc (c *custom) ServerHandshake(conn net.Conn) (net.Conn,\ncredentials.AuthInfo, error) {\nconn, authInfo, err := c.creds.ServerHandshake(conn)\ntlsInfo := authInfo.(credentials.TLSInfo)\nname := tlsInfo.State.PeerCertificates[0].Subject.CommonName\nfmt.Printf(\"%v\\n\", name)\nreturn conn, authInfo, err\n}\nfunc (c *custom) Info() credentials.ProtocolInfo {\nreturn c.creds.Info()\n}\nfunc (c *custom) Clone() credentials.TransportCredentials {\nreturn c.creds.Clone()\n}\nfunc (c *custom) OverrideServerName(s string) error {\nreturn c.creds.OverrideServerName(s)\n}`\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/111#issuecomment-275795933, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRw0pT9BiYCDW4Q1MZll8jnOzejveks5rWnQagaJpZM4DshP5\n.\n. @jan4984  You can cancel a stream by canceling the context used to create it.. Check your protobuf installation. Are there any errors? . We changed out write mechanism to use a dedicated loop to write which essentially solves the batching problem.. @tatsuhiro-t  @vlad-alexandru-ionescu  Agreed that empty Data frames are allowed but here it is more than that; the proxy modifies the header frame (making End Stream = False as mentioned here) and adds a data frame(albeit empty) when there shouldn't be any. This violates the gRPC protocol that the end-points rely on. . Merging the fork master branch after having reverted changes on it. Doing this because the revert on the master of grpc/grpc-go is not being reflected when trying to open another pull request with the changes. It says that the master already has those changes even though they were reverted.\n. Don't worry about polishing it, point us to the rough draft and we can discuss how to move forward from there. \n\nA few things that we care about are:\n1. Benchmark numbers comparing a sync.Pool to a leaky buffer cache.\n2. Benchmark numbers for not just memory profile but latency and corresponding CPU profile. Please take a look at our existing benchmark code.\nA few thoughts:\nGiven the messages are going to be of different sizes how are you planning to deal with that? A cache with different buffer sizes or a cache in which buffer sizes eventually grow with the message sizes seen. \nIf you go with sync.Pool all the cached memory will be released every GC cycle, it'll be interesting to see if that affects performance. Therefore latency benchmarks with CPU profiling are interesting.\nIf you go with a leakyBuffer, how are you planning to reclaim memory?. This is now tracked by #1455. Hey,\nWe are working on this; There's a PR out. However, the engineer working on\nit is ooo and we'll be able to give a status once he's back.\nBest,\nMak\nOn Tue, Jan 17, 2017 at 9:16 AM, enm10k notifications@github.com wrote:\n\nI had the same issue.\nAnd I want this to be fixed for our production use.\nwe are working on improving grpc logging (this should only appear in\ndebugging mode).\nIs there any work-in-progress branch as public?\nIf no one is working on this now, I'll do.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/875#issuecomment-273234537, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR5iRDt5jARYYvooF2Hnk554GphSbks5rTPdWgaJpZM4Jz8oI\n.\n. @xuss Can you throw some more light on what exactly are you seeing?\n\n\n\nIs your log spammed by errors originating in transport? Perhaps, consider setting your log verbosity level  to 1. grpclog\n\n\nAre you unable to connect because of a specific transport error? Please share more information, possible a reproduction.\n. Re-running the tests since it's an old change.. The tests are failing because the latest travis.yml changes aren't pulled in. Can you rebase, please?. This PR is obsolete. Moreover, this should be a subset of the retry support we plan to work on in the near future.. Closed by #1259 . We will be adding verbosity levels to logging soon. Error like that won't\nspam logs after that.\nNo plans yet to support cases where the server or client misbehaves.\n\n\nIn this particular case, seems like the underlying transport breaks due to\nsomething (may be the environment configuration doesn't allow for\nlong-living ideal connections or something). However, the aforementioned\nmonitor routine reconnects.\nOn Wed, Feb 22, 2017 at 10:04 AM, Ahmet Alp Balkan <notifications@github.com\n\nwrote:\n+1 this is still an issue. I have a very simple use case of gRPC and the\nlog output is flooded with this message every 4 minutes while it is sitting\nidle. This looks confusing for someone who just started using gRPC and\ntherefore warrants a Google search which eventually lands people here. I am\nstill wondering if I\u2019m doing something wrong.\n2017/02/22 09:37:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:41:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:45:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:49:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:53:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:57:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/954#issuecomment-281751094, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR6xwfuoJ8xHMLzi5a5vENRZ3BRC7ks5rfHjCgaJpZM4Kjh0O\n.\n. Opening anotther PR.. Thanks for the comments, Eric. On it!. go get -u github.com/golang/protobuf/{proto,protoc-gen-go}\ngo get -u google.golang.org/grpc\n\nOn Wed, Feb 1, 2017 at 12:59 PM, jab notifications@github.com wrote:\n\nI just hit this while going through Google's \"Building a gRPC service with\nNode.js\" codelab, see https://codelabs.developers.\ngoogle.com/codelabs/cloud-grpc/index.html?index=..%2F..%2Findex#3\nFiled an issue in googlecodelabs/cloud-grpc#5\nhttps://github.com/googlecodelabs/cloud-grpc/issues/5 but posting here\ntoo in case anyone here is interested or can help. Thanks!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1002#issuecomment-276780408, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR5sysAwcT8T37YjrhVu-RGzsi5xhks5rYPIogaJpZM4K-XVG\n.\n. Make sure $GOPATH/bin/protoc-gen-go is the updated binary. If not, delete\nit and let it regenerate.\nIt basically boils down to the fact that your generator and grpc code are\nnot in sync. You'll need to figure out what needs to be update.\n\nOn Wed, Feb 1, 2017 at 2:10 PM, jab notifications@github.com wrote:\n\n@MakMukhi https://github.com/MakMukhi I did that already and still get\nthe same error. Is there anything else you recommend I try? Thanks for\ntrying to help!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1002#issuecomment-276799224, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRxtJn_AnoBADBlR4eHI9Gj_HFjZKks5rYQLUgaJpZM4K-XVG\n.\n. books.pb.go is the code that needs to be regenerated. Quick fix: Inside\nbooks dir, run protoc -I . books.pb.go --go_out=plugins=grpc:.\nI'll find out how to get that repo updated.\n\nOn Wed, Feb 1, 2017 at 3:03 PM, jab notifications@github.com wrote:\n\n@MakMukhi https://github.com/MakMukhi I just moved aside my entire\n$GOPATH, reinstalled from scratch, and am getting the same error. Can you\nnot reproduce? Does this show that the issue is not on my end, but rather\nupstream? Anything else I should try?\n~ $ echo $GOPATH\n/Users//golang\n ~ $ mv -v $GOPATH $GOPATH.bak\n/Users//golang -> /Users//golang.bak\n ~ $ which -a protoc-gen-go\n ! ~ $ go get -u -v github.com/golang/protobuf/{proto,protoc-gen-go}17:56:20 http://github.com/golang/protobuf/%7Bproto,protoc-gen-go%7D17:56:20 2017github.com/golang/protobuf (download)github.com/golang/protobuf/protogithub.com/golang/protobuf/protoc-gen-go/descriptorgithub.com/golang/protobuf/protoc-gen-go/plugingithub.com/golang/protobuf/protoc-gen-go/generatorgithub.com/golang/protobuf/protoc-gen-go/grpcgithub.com/golang/protobuf/protoc-gen-go\n ~ $ which -a protoc-gen-go\n/Users//golang/bin/protoc-gen-go\n ~ $ go get -u -v google.golang.org/grpc\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nget \"google.golang.org/grpc\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc?go-get=1google.golang.org/grpc (download)github.com/golang/protobuf (download)\nFetching https://golang.org/x/net/context?go-get=1\nParsing meta tags from https://golang.org/x/net/context?go-get=1 (status code 200)\nget \"golang.org/x/net/context\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/context?go-get=1\nget \"golang.org/x/net/context\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net?go-get=1\nParsing meta tags from https://golang.org/x/net?go-get=1 (status code 200)golang.org/x/net (download)\nFetching https://golang.org/x/net/http2?go-get=1\nParsing meta tags from https://golang.org/x/net/http2?go-get=1 (status code 200)\nget \"golang.org/x/net/http2\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/http2?go-get=1\nget \"golang.org/x/net/http2\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/http2/hpack?go-get=1\nParsing meta tags from https://golang.org/x/net/http2/hpack?go-get=1 (status code 200)\nget \"golang.org/x/net/http2/hpack\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/http2/hpack?go-get=1\nget \"golang.org/x/net/http2/hpack\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/idna?go-get=1\nParsing meta tags from https://golang.org/x/net/idna?go-get=1 (status code 200)\nget \"golang.org/x/net/idna\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/idna?go-get=1\nget \"golang.org/x/net/idna\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/lex/httplex?go-get=1\nParsing meta tags from https://golang.org/x/net/lex/httplex?go-get=1 (status code 200)\nget \"golang.org/x/net/lex/httplex\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/lex/httplex?go-get=1\nget \"golang.org/x/net/lex/httplex\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/trace?go-get=1\nParsing meta tags from https://golang.org/x/net/trace?go-get=1 (status code 200)\nget \"golang.org/x/net/trace\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/trace?go-get=1\nget \"golang.org/x/net/trace\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/internal/timeseries?go-get=1\nParsing meta tags from https://golang.org/x/net/internal/timeseries?go-get=1 (status code 200)\nget \"golang.org/x/net/internal/timeseries\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/internal/timeseries?go-get=1\nget \"golang.org/x/net/internal/timeseries\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/codes?go-get=1\nParsing meta tags from https://google.golang.org/grpc/codes?go-get=1 (status code 200)\nget \"google.golang.org/grpc/codes\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/codes?go-get=1\nget \"google.golang.org/grpc/codes\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nFetching https://google.golang.org/grpc/credentials?go-get=1\nParsing meta tags from https://google.golang.org/grpc/credentials?go-get=1 (status code 200)\nget \"google.golang.org/grpc/credentials\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/credentials?go-get=1\nget \"google.golang.org/grpc/credentials\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/grpclog?go-get=1\nParsing meta tags from https://google.golang.org/grpc/grpclog?go-get=1 (status code 200)\nget \"google.golang.org/grpc/grpclog\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/grpclog?go-get=1\nget \"google.golang.org/grpc/grpclog\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/internal?go-get=1\nParsing meta tags from https://google.golang.org/grpc/internal?go-get=1 (status code 200)\nget \"google.golang.org/grpc/internal\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/internal?go-get=1\nget \"google.golang.org/grpc/internal\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/metadata?go-get=1\nParsing meta tags from https://google.golang.org/grpc/metadata?go-get=1 (status code 200)\nget \"google.golang.org/grpc/metadata\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/metadata?go-get=1\nget \"google.golang.org/grpc/metadata\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/naming?go-get=1\nParsing meta tags from https://google.golang.org/grpc/naming?go-get=1 (status code 200)\nget \"google.golang.org/grpc/naming\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/naming?go-get=1\nget \"google.golang.org/grpc/naming\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/stats?go-get=1\nParsing meta tags from https://google.golang.org/grpc/stats?go-get=1 (status code 200)\nget \"google.golang.org/grpc/stats\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/stats?go-get=1\nget \"google.golang.org/grpc/stats\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/tap?go-get=1\nParsing meta tags from https://google.golang.org/grpc/tap?go-get=1 (status code 200)\nget \"google.golang.org/grpc/tap\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/tap?go-get=1\nget \"google.golang.org/grpc/tap\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/transport?go-get=1\nParsing meta tags from https://google.golang.org/grpc/transport?go-get=1 (status code 200)\nget \"google.golang.org/grpc/transport\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/transport?go-get=1\nget \"google.golang.org/grpc/transport\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/peer?go-get=1\nParsing meta tags from https://google.golang.org/grpc/peer?go-get=1 (status code 200)\nget \"google.golang.org/grpc/peer\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/peer?go-get=1\nget \"google.golang.org/grpc/peer\": verifying non-authoritative meta taggolang.org/x/net/contextgolang.org/x/net/http2/hpackgolang.org/x/net/idnagolang.org/x/net/internal/timeseriesgoogle.golang.org/grpc/codesgolang.org/x/net/lex/httplexgoogle.golang.org/grpc/credentialsgoogle.golang.org/grpc/grpcloggolang.org/x/net/tracegoogle.golang.org/grpc/internalgolang.org/x/net/http2google.golang.org/grpc/metadatagoogle.golang.org/grpc/naminggoogle.golang.org/grpc/statsgoogle.golang.org/grpc/tapgoogle.golang.org/grpc/peergoogle.golang.org/grpc/transportgoogle.golang.org/grpc\n ~ $ cd src/cloud-grpc/start/\n ~/s/cloud-grpc  master  start $ go run client.go\n_/Users//src/cloud-grpc/start/books\nbooks/books.pb.go:107: undefined: grpc.SupportPackageIsVersion3\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1002#issuecomment-276811874, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR1bcpoUfexxZ9uhuBLvL2TTfhLN1ks5rYQ9XgaJpZM4K-XVG\n.\n. Also, this could serve as a nice quickstart for grpc-go\nhttps://github.com/grpc/grpc.github.io/blob/master/docs/quickstart/go.md\n\nOn Wed, Feb 1, 2017 at 3:14 PM, Mahak Mukhi mmukhi@google.com wrote:\n\nbooks.pb.go is the code that needs to be regenerated. Quick fix: Inside\nbooks dir, run protoc -I . books.pb.go --go_out=plugins=grpc:.\nI'll find out how to get that repo updated.\nOn Wed, Feb 1, 2017 at 3:03 PM, jab notifications@github.com wrote:\n\n@MakMukhi https://github.com/MakMukhi I just moved aside my entire\n$GOPATH, reinstalled from scratch, and am getting the same error. Can you\nnot reproduce? Does this show that the issue is not on my end, but rather\nupstream? Anything else I should try?\n~ $ echo $GOPATH\n/Users//golang\n ~ $ mv -v $GOPATH $GOPATH.bak\n/Users//golang -> /Users//golang.bak\n ~ $ which -a protoc-gen-go\n ! ~ $ go get -u -v github.com/golang/protobuf/{proto,protoc-gen-go}17:56:20 http://github.com/golang/protobuf/%7Bproto,protoc-gen-go%7D17:56:20 2017github.com/golang/protobuf (download)github.com/golang/protobuf/protogithub.com/golang/protobuf/protoc-gen-go/descriptorgithub.com/golang/protobuf/protoc-gen-go/plugingithub.com/golang/protobuf/protoc-gen-go/generatorgithub.com/golang/protobuf/protoc-gen-go/grpcgithub.com/golang/protobuf/protoc-gen-go\n ~ $ which -a protoc-gen-go\n/Users//golang/bin/protoc-gen-go\n ~ $ go get -u -v google.golang.org/grpc\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nget \"google.golang.org/grpc\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc?go-get=1google.golang.org/grpc (download)github.com/golang/protobuf (download)\nFetching https://golang.org/x/net/context?go-get=1\nParsing meta tags from https://golang.org/x/net/context?go-get=1 (status code 200)\nget \"golang.org/x/net/context\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/context?go-get=1\nget \"golang.org/x/net/context\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net?go-get=1\nParsing meta tags from https://golang.org/x/net?go-get=1 (status code 200)golang.org/x/net (download)\nFetching https://golang.org/x/net/http2?go-get=1\nParsing meta tags from https://golang.org/x/net/http2?go-get=1 (status code 200)\nget \"golang.org/x/net/http2\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/http2?go-get=1\nget \"golang.org/x/net/http2\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/http2/hpack?go-get=1\nParsing meta tags from https://golang.org/x/net/http2/hpack?go-get=1 (status code 200)\nget \"golang.org/x/net/http2/hpack\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/http2/hpack?go-get=1\nget \"golang.org/x/net/http2/hpack\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/idna?go-get=1\nParsing meta tags from https://golang.org/x/net/idna?go-get=1 (status code 200)\nget \"golang.org/x/net/idna\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/idna?go-get=1\nget \"golang.org/x/net/idna\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/lex/httplex?go-get=1\nParsing meta tags from https://golang.org/x/net/lex/httplex?go-get=1 (status code 200)\nget \"golang.org/x/net/lex/httplex\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/lex/httplex?go-get=1\nget \"golang.org/x/net/lex/httplex\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/trace?go-get=1\nParsing meta tags from https://golang.org/x/net/trace?go-get=1 (status code 200)\nget \"golang.org/x/net/trace\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/trace?go-get=1\nget \"golang.org/x/net/trace\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/internal/timeseries?go-get=1\nParsing meta tags from https://golang.org/x/net/internal/timeseries?go-get=1 (status code 200)\nget \"golang.org/x/net/internal/timeseries\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/internal/timeseries?go-get=1\nget \"golang.org/x/net/internal/timeseries\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/codes?go-get=1\nParsing meta tags from https://google.golang.org/grpc/codes?go-get=1 (status code 200)\nget \"google.golang.org/grpc/codes\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/codes?go-get=1\nget \"google.golang.org/grpc/codes\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nFetching https://google.golang.org/grpc/credentials?go-get=1\nParsing meta tags from https://google.golang.org/grpc/credentials?go-get=1 (status code 200)\nget \"google.golang.org/grpc/credentials\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/credentials?go-get=1\nget \"google.golang.org/grpc/credentials\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/grpclog?go-get=1\nParsing meta tags from https://google.golang.org/grpc/grpclog?go-get=1 (status code 200)\nget \"google.golang.org/grpc/grpclog\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/grpclog?go-get=1\nget \"google.golang.org/grpc/grpclog\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/internal?go-get=1\nParsing meta tags from https://google.golang.org/grpc/internal?go-get=1 (status code 200)\nget \"google.golang.org/grpc/internal\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/internal?go-get=1\nget \"google.golang.org/grpc/internal\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/metadata?go-get=1\nParsing meta tags from https://google.golang.org/grpc/metadata?go-get=1 (status code 200)\nget \"google.golang.org/grpc/metadata\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/metadata?go-get=1\nget \"google.golang.org/grpc/metadata\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/naming?go-get=1\nParsing meta tags from https://google.golang.org/grpc/naming?go-get=1 (status code 200)\nget \"google.golang.org/grpc/naming\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/naming?go-get=1\nget \"google.golang.org/grpc/naming\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/stats?go-get=1\nParsing meta tags from https://google.golang.org/grpc/stats?go-get=1 (status code 200)\nget \"google.golang.org/grpc/stats\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/stats?go-get=1\nget \"google.golang.org/grpc/stats\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/tap?go-get=1\nParsing meta tags from https://google.golang.org/grpc/tap?go-get=1 (status code 200)\nget \"google.golang.org/grpc/tap\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/tap?go-get=1\nget \"google.golang.org/grpc/tap\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/transport?go-get=1\nParsing meta tags from https://google.golang.org/grpc/transport?go-get=1 (status code 200)\nget \"google.golang.org/grpc/transport\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/transport?go-get=1\nget \"google.golang.org/grpc/transport\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/peer?go-get=1\nParsing meta tags from https://google.golang.org/grpc/peer?go-get=1 (status code 200)\nget \"google.golang.org/grpc/peer\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/peer?go-get=1\nget \"google.golang.org/grpc/peer\": verifying non-authoritative meta taggolang.org/x/net/contextgolang.org/x/net/http2/hpackgolang.org/x/net/idnagolang.org/x/net/internal/timeseriesgoogle.golang.org/grpc/codesgolang.org/x/net/lex/httplexgoogle.golang.org/grpc/credentialsgoogle.golang.org/grpc/grpcloggolang.org/x/net/tracegoogle.golang.org/grpc/internalgolang.org/x/net/http2google.golang.org/grpc/metadatagoogle.golang.org/grpc/naminggoogle.golang.org/grpc/statsgoogle.golang.org/grpc/tapgoogle.golang.org/grpc/peergoogle.golang.org/grpc/transportgoogle.golang.org/grpc\n ~ $ cd src/cloud-grpc/start/\n ~/s/cloud-grpc  master  start $ go run client.go\n_/Users//src/cloud-grpc/start/books\nbooks/books.pb.go:107: undefined: grpc.SupportPackageIsVersion3\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1002#issuecomment-276811874, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR1bcpoUfexxZ9uhuBLvL2TTfhLN1ks5rYQ9XgaJpZM4K-XVG\n.\n\n\n. Wait, my bad; That needs to books.proto  protoc -I . books.proto\n--go_out=plugins=grpc:.\nBut I see that they haven't provided books proto file.\nAnother quick fix: update the books.pb.go file yourself:\n1. Update all occurrences of grpc.SupportPackageIsVersion3 to\ngrpc.SupportPackageIsVersion4\n2. Replace Metadata: fileDescriptor0 with Metadata: \"books.proto\"\n\nHope they update repo soon, meanwhile this should work.\nOn Wed, Feb 1, 2017 at 3:20 PM, jab notifications@github.com wrote:\n\n@MakMukhi https://github.com/MakMukhi Thanks for continuing to debug\nwith me. I tried your quick fix and here is what happened:\n$ pwd\n/Users/bronsonj/src/cloud-grpc\n$ head .git/config\n[core]\n  repositoryformatversion = 0\n  filemode = true\n  bare = false\n  logallrefupdates = true\n  ignorecase = true\n  precomposeunicode = true\n[remote \"origin\"]\n  url = https://github.com/googlecodelabs/cloud-grpc\n  fetch = +refs/heads/:refs/remotes/origin/\n$ git pull\nAlready up-to-date.\n$ cd start/books\n$ protoc -I . books.pb.go --go_out=plugins=grpc:.\n[libprotobuf WARNING google/protobuf/compiler/parser.cc:546] No syntax specified for the proto file: books.pb.go. Please use 'syntax = \"proto2\";' or 'syntax = \"proto3\";' to specify a syntax version. (Defaulted to proto2 syntax.)\nbooks.pb.go:30:1: Expected \";\".\nbooks.pb.go:53:1: Expected top-level statement (e.g. \"message\").\nAnything else you suggest so I can get past this? Thanks again!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1002#issuecomment-276815483, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRzVmYKgAGelo-1Fk5foamaw3bZ6Lks5rYRMsgaJpZM4K-XVG\n.\n. @tamird Can you send some more logs around the error? For instance, if any other errors were seen (http2client.notifyError etc.) \nAlso, some context about the connecting client would be useful too; \nFor instance, what backoff strategy was used since I see here https://github.com/cockroachdb/cockroach/issues/12713 that the reconnect happened every second which seems odd. \nAlso, if I understand it right: You have a client that is connected to a server but then the server goes away(abruptly ?), and the client here keeps trying to reconnect to it. The server later comes up but the client still won't connect? \nAn arrow in the dark: when the said server comes up, does the ip address change and if so does the balancer's name resolver get updated to map to the right address? \n. I haven't had the chance to look at this today but I'm going to statically\nanalyze the code more to see what code path might be causing this issue\nMonday. Sorry about the delay.\nIt will be quite helpful though, if you can reproduce this error and\nprovide the scenario.\n\nBest,\nMak\nOn Thu, Jan 26, 2017 at 6:55 PM, Ben Darnell notifications@github.com\nwrote:\n\nThis is happening right now on one of our test clusters. The IPs have not\nchanged; one server crashed, was down for a time, and was restarted. The\nlogs are full of this:\nW170127 02:50:04.374599 96914562 storage/intent_resolver.go:352  [n1,s1,r64/7:/Table/51/1/14{12136\u2026-29906\u2026}]: failed to resolve intents: failed to send RPC: sending to all\n 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refused\n\"\nW170127 02:50:04.798731 96914733 storage/intent_resolver.go:352  [n1,s1,r422/1:/Table/51/1/40{39115\u2026-57292\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refuse\nd\"\nW170127 02:50:04.848520 96914767 storage/intent_resolver.go:352  [n1,s1,r423/3:/Table/51/1/86{26570\u2026-44684\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refuse\nd\"\nW170127 02:50:04.908616 96914878 storage/intent_resolver.go:352  [n1,s1,r428/3:/Table/51/1/83{72888\u2026-90988\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refuse\nd\"\nW170127 02:50:04.996322 96915001 storage/intent_resolver.go:352  [n1,s1,r428/3:/Table/51/1/83{72888\u2026-90988\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 4 desc = context deadline exceeded\nW170127 02:50:05.004075 96914928 storage/intent_resolver.go:352  [n1,s1,r423/3:/Table/51/1/86{26570\u2026-44684\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refuse\nd\"\nW170127 02:50:05.042729 96915124 storage/intent_resolver.go:352  [n1,s1,r45/5:/Table/51/1/13{41272\u2026-58971\u2026}]: failed to resolve intents: failed to send RPC: sending to all\n 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refused\n\"\nNote that the error is write: connection refused, so we're getting\nECONNREFUSED from a Write call (which isn't really supposed to happen,\nbut I can't tell whether it's explicitly forbidden). And the local port\nnumber (33844) is constant, so it's not trying to reconnect over and over.\nInstead, what appears to be happening is that because the \"connection\nrefused\" error is happening in an unexpected context, grpc continues to\nretry on the failed connection instead of closing it so it can reopen a new\none.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1026#issuecomment-275577915, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR2nHpeEjWo7JaxWpJk8h880OCfs-ks5rWVyMgaJpZM4LRHVF\n.\n. Other than the nitpicks, lgtm.  . lgtm. I'll have to wait for @menghanl  to come back from vacation to have a discussion over this.. LGTM. We have PR under-work that shows significant performance improvement. https://github.com/grpc/grpc-go/pull/1073\n\nMeanwhile, I'll have another PR out to provide DialOptions to be able to set window size on connection and stream.. Created a PR to make window size configurable: #1210. Merged the PR. Making severity of this bug P2.. #1248 . #1248 is merged now. Let us know if this helps for your use-case.\nSince this issue is about large messages on high latency networks we're closing this. However, we are continuing our work on flow control optimization.\nThis specific issue was tested against our benchmark:\n```json\n{\n   \"scenarios\":[\n      {\n         \"name\":\"go_protobuf_sync_unary_ping_pong_secure_2097152db\",\n         \"warmup_seconds\":5,\n         \"benchmark_seconds\":30,\n         \"num_servers\":1,\n         \"server_config\":{\n            \"async_server_threads\":0,\n            \"security_params\":{\n               \"use_test_ca\":true,\n               \"server_host_override\":\"foo.test.google.fr\"\n            },\n            \"payload_config\":{\n               \"simple_params\":{\n                  \"resp_size\":2097152,\n                  \"req_size\":2097152\n               }\n            },\n            \"server_type\":\"SYNC_SERVER\"\n         },\n         \"client_config\":{\n            \"client_type\":\"SYNC_CLIENT\",\n            \"security_params\":{\n               \"use_test_ca\":true,\n               \"server_host_override\":\"foo.test.google.fr\"\n            },\n            \"payload_config\":{\n               \"simple_params\":{\n                  \"resp_size\":2097152,\n                  \"req_size\":2097152\n               }\n            },\n            \"client_channels\":1,\n            \"async_client_threads\":1,\n            \"outstanding_rpcs_per_channel\":1,\n            \"rpc_type\":\"UNARY\",\n            \"load_params\":{\n               \"closed_loop\":{  \n           }\n        },\n        \"histogram_params\":{  \n           \"max_possible\":60000000000.0,\n           \"resolution\":0.01\n        }\n     },\n     \"num_clients\":1\n  }\n\n]\n}\n```\nThe QPS went up from 0.1 to 0.9.\n. The numbers show that we are not there yet and we plan on iterating over it. Reopening the issue to track it here.. Tried several failed attempts to fix the benchmarks but none has worked yet. At this point we're inclined towards running the client and server code on VMs in different continents and compare the numbers. I'll post those numbers by tomorrow.. Results from running the same benchmark on VMs in different continents (server in Asia and client in North America).\nLatency(RTT): 152ms\n```\nGRPC:\nDuration: 1.154495933s for GRPC\nDuration: 462.041517ms for GRPC\nDuration: 460.35183ms for GRPC\nDuration: 460.48016ms for GRPC\nDuration: 460.286916ms for GRPC\nHTTP 1:\nDuration: 1.572714757s for HTTP 1\nDuration: 305.216448ms for HTTP 1\nDuration: 156.844879ms for HTTP 1\nDuration: 157.044419ms for HTTP 1\nDuration: 157.040266ms for HTTP 1\nHTTP2:\nDuration: 3.251843565s for HTTP 2\nDuration: 2.596755423s for HTTP 2\nDuration: 2.595688722s for HTTP 2\nDuration: 2.596232422s for HTTP 2\nDuration: 2.594843281s for HTTP 2\n```. Following is our near-future plan to increase performance further:\n1. Further optimize Unary RPCs to expect a large response from the server: This is a fairly straight-forward change however it won't affect this benchmark.\n2. Batching: There's a bug in the transport that prevents batching from taking place. The fix for this is quite simple too. Again this won't have a major impact on this benchmark.\n3. BDP estimation and dynamic flow control: This is the next thing we're taking up now and hope to bring these benchmark numbers further down. . 1. We are not claiming that we'll stop working on gRPC performance after this; we're actively looking into strategies to dynamically update flow control.\n2. We acknowledge appropriate documentation should be written.\n3. However, we'd rather open a separate issue for both of them. It is important to close this issue since it contains misinformation (broken benchmarks that show gRPC's performance much worse than it is.) \n. @adg That's a good point. I've updated #1280 to mention that. Feel free to update it further. . If an error occurs on the transport (the tcp connection), this error\nmessage is logged. If the error were transient the monitor routine\nreconnects.\nOn Fri, Jan 27, 2017 at 9:31 AM, Anthony Seure notifications@github.com\nwrote:\n\nI'm experiencing the exact same error message as @rsc\nhttps://github.com/rsc by using cloud.google.com/go/storage for writing\nfiles to Google Cloud Storage. It only happens from time to time and\ndoesn't seem to impact the write operations in any way.\nWhat is the exact meaning of this error?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1044#issuecomment-275723554, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRxoFfKCdmOqeX2uSrB6_A-u7ODtjks5rWinngaJpZM4Le_Wu\n.\n. Hey,\n\nWe'll need more information to debug. Do you see anything peculiar on the\nserver or client other than the error? Have you been able to reproduce the\nissue?\nBest,\nMak\nOn Mon, Jan 16, 2017 at 4:49 AM, Steeve Morin notifications@github.com\nwrote:\n\nHi,\nWe are using grpc-go on iOS and noticed weird behaviours related to the\nnetwork connection. When ENOPIPEd, the network connection successfully\nreconnects (we are silencing SIGPIPE), however sometimes, after long\nsleeps, the gRPC calls fail with this error:\nerror=\"rpc error: code = 13 desc = connection error: desc = \\\"transport: read tcp x.x.x.x:y->z.z.z.z:443: read: socket is not connected\\\"\"\nInfo:\ngo version: go version devel +d9a0579 Tue Jan 10 17:29:46 2017 +0000 darwin/arm64\nos: darwin/arm64\nWe will try to reset the client, but we are not sure how to properly catch\nthis error in particular. I also think catching this on the client side\nthis is too high level.\nThanks!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1049, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR32gJxY0PcrTweqKyGly2Iup7blyks5rS2dogaJpZM4LkhU7\n.\n. @menghanl  is already working on adding different levels to logging. This should be taken care of there. Menghan can confirm further when he's back.. Hey,\n\nIf I understand this correct, you have a server written in Golang and a\nclient written in Python?\nSome from the top of my head debugging:\nIt seems like the server expects to read something from the connection but\nit's being closed prematurely. When you say \"client connects\", do you mean\nthat you dial every time you're sending an rpc request? If so, I'd make\nsure that the client closes the previous connections.\nRegards,\nMak\nOn Fri, Jan 20, 2017 at 7:58 AM, Evgeniy Polyakov notifications@github.com\nwrote:\n\ntransport: http2Server.HandleStreams failed to read frame: read tcp 127.0.0.1:8080->127.0.0.1:56140: use of closed network connection\nI get this error very frequently during usual operations\n(request-response, no streaming), server stops responding just after few\nrequests after it has been shown in the logs. Client connects but hangs.\nClient is written in python, if that matters, below is the tail of the\ntrace\n(peer=ipv4:127.0.0.1:8080): 67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding'\nD0120 18:53:35.079893197    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 15 '.'\nD0120 18:53:35.079900652    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip'\nD0120 18:53:35.079907722    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 40 02 '@.'\nD0120 18:53:35.079914199    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 74 65 'te'\nD0120 18:53:35.079920624    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 08 '.'\nD0120 18:53:35.079927281    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 74 72 61 69 6c 65 72 73 'trailers'\nD0120 18:53:35.079933853    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 40 0c '@.'\nD0120 18:53:35.079940642    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type'\nD0120 18:53:35.079947042    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 10 '.'\nD0120 18:53:35.079954235    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc'\nD0120 18:53:35.079961223    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 40 0a '@.'\nD0120 18:53:35.079967669    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 75 73 65 72 2d 61 67 65 6e 74 'user-agent'\nD0120 18:53:35.079974061    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 32 '2'\nD0120 18:53:35.079981812    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 50 79 74 68 6f 6e 2d 67 52 50 43 2d 31 2e 30 2e 34 20 67 72 70 63 2d 63 2f 31 2e 30 2e 31 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 29 'Python-gRPC-1.0.4 grpc-c/1.0.1 (manylinux; chttp2)'\nD0120 18:53:35.079990118    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 00 00 04 08 00 00 00 00 01 00 00 ff ff 00 00 '...............'\nD0120 18:53:35.079997916    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 0d 00 01 00 00 00 01 00 00 00 00 08 0a 06 61 '..............a'\nD0120 18:53:35.080005051    9288 tcp_posix.c:405]            WRITE 0x55f85b44d9f0 (peer=ipv4:127.0.0.1:8080): 6d 61 7a 6f 6e 'mazon'\nD0120 18:53:35.080034782    9288 chttp2_transport.c:675]     W:0x55f85b606d70 WRITING -> INACTIVE because terminate_writing\nI0120 18:53:35.279304104    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:35.279351411    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927615, tv_nsec: 479346764, clock_type: 1 }, reserved=(nil))\nI0120 18:53:35.479600966    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:35.479637071    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927615, tv_nsec: 679632041, clock_type: 1 }, reserved=(nil))\nI0120 18:53:35.679888501    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:35.679923695    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927615, tv_nsec: 879918449, clock_type: 1 }, reserved=(nil))\nI0120 18:53:35.880187881    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:35.880224235    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927616, tv_nsec: 80219363, clock_type: 1 }, reserved=(nil))\nI0120 18:53:36.080563629    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:36.080604930    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927616, tv_nsec: 280599705, clock_type: 1 }, reserved=(nil))\nI0120 18:53:36.280876915    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:36.280925668    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927616, tv_nsec: 480920166, clock_type: 1 }, reserved=(nil))\nI0120 18:53:36.481174706    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:36.481211857    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927616, tv_nsec: 681206979, clock_type: 1 }, reserved=(nil))\nI0120 18:53:36.681416250    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:36.681450708    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927616, tv_nsec: 881446184, clock_type: 1 }, reserved=(nil))\nI0120 18:53:36.881702944    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:36.881744110    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927617, tv_nsec: 81738749, clock_type: 1 }, reserved=(nil))\nI0120 18:53:37.082095945    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\nI0120 18:53:37.082136020    9288 completion_queue.c:333]     grpc_completion_queue_next(cc=0x55f85b5c7db0, deadline=gpr_timespec { tv_sec: 1484927617, tv_nsec: 282131502, clock_type: 1 }, reserved=(nil))\n^CI0120 18:53:37.282403669    9288 completion_queue.c:394]     RETURN_EVENT[0x55f85b5c7db0]: QUEUE_TIMEOUT\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1053, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR5XMpOv7LGqGaTyFINzUvWE7rvP3ks5rUNmggaJpZM4LpdMr\n.\n. Hey Martin,\n\nWe're working on adding levels to logging. This will reduce the verbosity\nfor more restrictive levels. The PR is under its way. However, I don't have\na time line for you at this moment since the guy working on this is out on\nvacation.\nRegards,\nMak\nOn Tue, Jan 31, 2017 at 5:49 PM, Martin Taillefer notifications@github.com\nwrote:\n\nI have a client/server setup using bidirectional streaming. When the\nclient calls stream.CloseSend and then immediately closes the underlying\nTCP connection, the server produces the following spurious log entry:\nhttp2_server.go:320] transport: http2Server.HandleStreams failed to read\nframe: read tcp [::1]:9091->[::1]:38241: read: connection reset by peer\nAfter this, everything works fine. If I insert a 1 second delay on the\nclient between the call to CloseSend and the call to close the socket, then\nthe log message doesn't get emitted.\nThis message is polluting our logs, that's why I'd like to get rid of it.\nThis is with Go 1.7.4\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1059, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR2aPvnyGNBwF3AXg_qZbnIgyoOWkks5rX-SMgaJpZM4LzZ2S\n.\n. Hey,\n\nI couldn't reproduce the issue on a machine running go 1.7.3. I tried the following code, let me know if you can reproduce it.\ngo\npackage main\nimport (\n        \"context\"\n        \"fmt\"\n        xcontext \"golang.org/x/net/context\"\n)\nfunc main() {\n        ctx, cancel := context.WithCancel(context.Background())\n        cancel()\n        err := ctx.Err()\n        switch err {\n        case xcontext.Canceled:\n                fmt.Println(\"comparision worked\")\n                return\n        }\n        fmt.Println(\"comparision did NOT work\")\n}. This is not a bug but a log spam: there's a goroutine on the server that reads on the connection synchronously, when the connection breaks(con.Close()), this gorutine logs an error out. Now the connection breaking might be completely valid (not an error, like in this case), the goroutine logs it out anyway.\nWe have a PR that'll take care of this by adding verbosity levels to log.\nI'm going to go ahead and close this issue since there're already several issues tracking the same thing. Please track the aforementioned PR for updates.. This should've been fixed by #1498\nOn Sep 25, 2017 5:43 PM, \"Ashwanth Kumar\" notifications@github.com wrote:\n\nHello, is there any other update or workaround to this issue?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1078#issuecomment-332052892, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR0pBdl2jd4gDaRRCSSwZgfNp2h_Pks5smEiwgaJpZM4MF4Ys\n.\n. Are you saying  that your rpc is stuck on a recv call even after the context has cancelled? This should never happen. Can you provide more details?\nThe read call that you're seeing in your stack trace is the reader go routine of a client's transport. It sits there waiting for data. But if an error occurred elsewhere and the transport was closed the underlying net.Conn will be closed and this goroutine will see that and return.\nIf this doesn't answer all your questions, please file a new issue and provide more information: What exactly are you doing? What behavior did you see? What did you expect?\nThanks.. @23doors  Thanks for the update.\n@vrecan  To pursue this bug we'll need more information from your side. Please respond to Menghan's comment, when you can. However, if this is not an issue anymore feel free to close it.. Closing with #1307. lgtm. It is added to the metadata which you can access from the context.\nhttps://github.com/grpc/grpc-go/blob/8b2e129857480cb0f07ef7d9d10b8b252c7ac984/transport/http_util.go#L208\n\nOn Thu, Mar 2, 2017 at 10:58 AM, Sam Nguyen notifications@github.com\nwrote:\n\nHow would I access the UserAgent of the client stream in my GRPC server? I\ncan't seem to find any examples or interfaces that would allow this.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1100, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRzde2G9VrhB_p5zjy5k-TceIsuJKks5rhxFmgaJpZM4MRY1p\n.\n. lgtm. ping. Thanks for the reproduction repository, made it really easy to pin-point the issue.\nWe have identified the code path that leads to this behavior and are trying to reason about what's the right thing to do:\n1. Close all streams in flight when ClientConn is closed.\n2. Have the users explicitly close all streams despite having called ClientConn.Close().\n\nAs a user do you have a preference or a use case that favors either of the behaviors?. Thanks again for filing the issue with a repro.. - Does the grpc python server log anything out about sending RST_STREAMS?\n- Is there any proxy between grpc-go client and grpc-python server?\nYou are getting a RST_STREAM with error code 8. According to the spec you should get this when the call is canceled: https://github.com/grpc/grpc/blob/c2bc59b2b0de079c64a2e544354b699685ea2853/doc/PROTOCOL-HTTP2.md\nor deadline set on RPC exceeded: \nhttps://github.com/grpc/grpc/blob/46357c882df1afc28f7a5228c40fde522093fa32/src/core/lib/transport/status_conversion.c#L40\nIf there's a deadline set on the RPC it's unlikely but possible that the server sees this deadline and sends RST_STREAM(with error code 8) before the client does.\nCan you think of anything about the tests that might be leading to these situations?\nAlso, it'd be worthwhile to include gRPC-python team to get their input; If the python server sends RST_STREAM in any other case too.\n- Are these unary or streaming RPCs? . @zllak  Can you provide more information please?\n Is the error code you get with RST_STREAM on client side same as above (i.e. 8)?\n Are both you server and client on golang?\n What exactly is the scenario: client makes a unary RPC and the server instead of responding back send a RST_STREAM with error code 8? Is the RST_STREAM sent immediately or the RPC calls hangs for a while and eventually gets the RST_STREAM?\n Since you could reproduce this in a test, would you be able to share some code (ideally a repro)?. ping. Thanks for opening the issue. :). Hey,\nFirst of all apologies for a late response, things have been crazy busy lately. \nSo we do appreciate the time and effort you put into this PR, however after a series of discussions within the team we have decided to not include this change, for following reasons:\n1. Marshaling protos is not too cumbersome in golang and therefore doesn't necessitate utilities added to the library.\n2. Not all of us can get behind the idea of using proto name as key. We are aware that Java does this but it's not a standard practice among grpc languages.\nHaving said that, you could still add it to your own repository and link it from grpc-contrib (https://github.com/grpc/grpc-contrib).\nAlso, it's always great to keep us in loop if you're planning to make changes or add features. We might aid with ideas/design and so the following reviews would be quicker too.\nBest,\nMak. Implemented by #1210. LGTM\n. You're right in correction of your misunderstanding; Send blocks until the\nmessage is sent out from the server and not until the client actually\nreceives it.\nOn Tue, Apr 11, 2017 at 3:53 PM, Ling Yuan notifications@github.com wrote:\n\nHi, I'm using server-side streaming, and I noticed that after the stream\nis established, Send() call on server-side does not seem to block, even\nif client side never calls Recv(). Does this mean the messages get\nbuffered in client's TCP (or transport, in general) layer? I just wanna\nconfirm that unread messages are not staged on the server side, causing\nmemory issue etc.\nAlso, the documentation\nhttps://github.com/grpc/grpc-go/blob/master/stream.go#L76 says Send()\nblocks until the message is sent. That conflict with what I described\nabove. May be I misunderstood and the doc simply means Send() blocks\nuntil the transport layer system call returns?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1179, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR80ghl1TvNmQYe7mlDsrwtjkHLW5ks5rvAR_gaJpZM4M6v0S\n.\n. When an error occurs in processUnaryRPC a status is written on the user stream. The returning of error from that function is only so that stats handling in defer can take place. This could have been avoided by creating a local err variable. We might make such a refactoring in the future but aren't actively working on it.\nThanks for bringing this to attention.. Making a revert.. I signed it!. Closing because of unclean commits. Will open another one.. On the client-side there's a call options to access the Peer object. \nNote: that a ClientConn doesn't represent that actual transport connected to  a server. Each clientconn can have more that one transports depending on the load-balancer. A transport is selected at the time when an RPC is made. Therefore, the peer object is accessible via CallOptions. . As you mentioned your example is on the server-side. On the client-side you'd use a call option to see what your peer was:\n\npeer := new(Peer)\nres, err := client.RPCCall(ctx, req, grpc.Peer(peer)) // Unary call.\nThis gives you peer information(about the server on the client-side) once the call is completed. \nIf you want to do some sort of check on server's creds on the client-side, right after the handshake, then, you could wrap around TLS creds in your own struct and override the client and server handshake methods. Not sure though if I understand your use-case completely.  . Glad that helped. As a reference here's the default TLS creds implementation of gRPC-go (client-side): https://github.com/grpc/grpc-go/blob/master/credentials/credentials.go#L202. Closing this, feel free to comment if there's more information needed. We can re-open too, if need be.. You're right and this should be an easy implementation too, given the hooks are available. However, we'd like to give a little more thought to it. \nWe are looking into batching writes together as performance optimization and would like to think about how this would interact with that, given the API says this is only a hint to the transport.\nAs a rough timeline on this I'd say somewhere around a month or more. \nAlso, we think what you're seeing is actually a flow-control push-back. Moreover, the hack( setting Delay ) shouldn't do anything in the current state of transport. Write function doesn't look at it:\nhttps://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L838\n. Both http2Server and http2Client now have a dedicated goroutine that flushes when there's nothing else to be sent or the underlying I/O buffer is full. I'm not sure how effective would it be to force flush a message; it would be helpful only in case where there are other streams but in such a case there's no way to prioritize one stream's schedule over the other.\nAre you still interested in this?\n. Update: Got around reproducing this (once every couple 1000 runs); The time out set on RPCs is 1 second. Changed that to 5 seconds and ran for 100000 times, all pass.\nNot sure though why 1 second wasn't enough.. Ah! My bad. I'll have a fix in later this morning.\nOn May 24, 2017 7:24 AM, \"Jeff Mitchell\" notifications@github.com wrote:\n\nThis was caused by #1248 https://github.com/grpc/grpc-go/pull/1248 (\n@MakMukhi https://github.com/makmukhi)\nIn the PR, in handler_server line 335 (6dff7c5#diff-\n95e9a25b738459a2d3030e1e6fa2a718R335\nhttps://github.com/grpc/grpc-go/commit/6dff7c5f333b5331089ccc6451fc842664f3be6a#diff-95e9a25b738459a2d3030e1e6fa2a718R335),\ns.trReader is set as a recvBufferReader. However, in the same PR,\neverywhere else it is changed to use transportReader. As a result the\ntype assertion at line 329 of transport.go (6dff7c5#diff-\nfa3fe0c81c67082780f26a237879b514R329\nhttps://github.com/grpc/grpc-go/commit/6dff7c5f333b5331089ccc6451fc842664f3be6a#diff-fa3fe0c81c67082780f26a237879b514R329)\npanics.\nThis error was not caught by existing tests.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1257, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR3Z0rkY-Ta8ah7UfgtE4qdhrMmwIks5r9D24gaJpZM4NlL2Q\n.\n. #1257 is fixed you can revert the first commit now.. Will wait for tests you're re-enabling (https://github.com/grpc/grpc-go/pull/1259/commits/1468d49922e5492fe1465b25a32126d5eda390f4) to pass locally before I merge.. Local tests pass too. Merging now.. 'course, and sorry about the breakage.. I tried to reproduce your issue but couldn't. How big are your messages? Here, I tried with 1 MB messages.\n\nBy the way, this example and subsequently my repro attempt both are leaking resources; conn should be closed once it's not useful anymore.. It'd be great if you can create a standalone repro of the issue. . Couldn't get the test to fail in more than 100,000 runs. Let's assume this is failure to be a travis aberration. \nPS the rerun is passing now.. Each connection has associated go routines: controller, keepalive and from the looks of it a writer routine that comes from the net/http library. If the connection is closed (shutdown channel closed) properly, these goroutines end. I'd start with making sure the connection was terminated properly.\nThe goroutine relating to reading(HandleStream) is associated with every RPC request the server receives and persists only as long as the RPC itself.\nDoes that help? If not, how exactly do you see these leaks? What's the use-case? Perhaps, some related code that you can share?. If there's nothing that needs to be done, can we close this issue? . So assuming this is your use-case: \nOn the client-side there are unused connections that aren't closed by the user (ClientConn.Close() wasn't called). These connections are therefore healthy but idle. In such a case you could set max age parameters on your server to close idle connections after a set time (and more functionalities).\nHowever, if your use-case is such that the connections are unhealthy (broken), however the client never closed them so the server never noticed them going down, keepalive probes will help you there. . In that case, keepalive probes will help the server realize that and close the connection.. Hey @lhecker, thanks for your PR. However, despite being a small change, it is tightly coupled to a specific use-case. A more general solution by @menghanl  ( to add a different verbosity level to transport layer errors) is already underway. That should solve the log spamming problems once and for all.. So I understand that this issue comes up when the code exits from this path: https://github.com/grpc/grpc-go/blob/master/stream.go#L253\nDoes this happen when the user application calls Close() on client conn and doesn't cancel the the stream's context? . I see. Thanks for link to the code. I want to discuss this with the team a bit (possible other ways to tackle the problem). Let's try and wrap this is up by tomorrow.. So this is in someway related to code changes made in the past. Looking at which your change does make sense. However, this also requires some more thought.\nWe all are in meetings all day today. This will have to go to Monday. Hope that's alright.. Hey,\nSo we agreed on the change that you're proposing. Can you also please add a test for it?. @menghanl  What do you think ?. Finding the failed test on travis is tough in general. I usually search for \"FAIL:\". In your case this test failed:\n FAIL: TestKeepaliveServerEnforcementWithAbusiveClientWithRPC (2.00s)\n    transport_test.go:694: Test failed: Expected a GoAway from server.\n. The context provided to a call is different (it's on the client side\nanyways) than the context used in the server's internal implementation.\nOn Jun 7, 2017 1:42 AM, \"Julio Guerra\" notifications@github.com wrote:\n\n@dfawley https://github.com/dfawley Still, the API is inconsistent\nsince we can provide a context to a client call...\nI have another relevant example where current implementation fails when\nsimply following common Go principles to further extend another type:\ntype MyContext {\n    context.Context\n    // plus whatever...\n}\nIf we could provide the context ourselves, we would be able to type-assert\nit to our own types. Current implementation forces using a global variable\ninstead...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1282#issuecomment-306730152, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRzG5NJR5xgpDYt7z6J9sAWhlSN8bks5sBmJ1gaJpZM4NxoM_\n.\n. TestKeepaliveServerEnforcementWithAbusiveClientNoRPC is flaky as well.. In fact, I couldn't get TestKeepaliveServerEnforcementWithAbusiveClientWithRPC to fail in 10000 runs!. These tests are flaky because for some reason frames(ping, goaway etc.) written on wire by one side don't get to the other side for as long as 50ms (possibly more that's the number I saw in my runs). Both side are on localhost, yet once in a while something as bizarre this happens.\nSince the flakes are quite rare, we'll let it be for now.. #1952 Solves all of this except for options to control network conditions. . This should be fixed by #1275.. Couldn't reproduce in 100000 runs in race mode.. Thanks for reporting, we'll have a fix out soon.\n\nOn Fri, Jun 16, 2017 at 12:34 PM, Anthony Romano notifications@github.com\nwrote:\n\nWhat version of gRPC are you using?\n1.4.1\nWhat version of Go are you using (go version)?\n1.8.3\nWhat operating system (Linux, Windows, \u2026) and version?\nUbuntu Linux 16.10\nWhat did you do?\nRan the etcd proxy tests:\nPASSES=grpcproxy ./test\nWhat did you expect to see?\nNo data races from grpc.\nWhat did you see instead?\nWARNING: DATA RACE\nWrite at 0x00c425b38ff1 by goroutine 2016:\n  google.golang.org/grpc/transport.(http2Client).operateHeaders()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/transport/http2_client.go:1027 +0x93\n  google.golang.org/grpc/transport.(http2Client).reader()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/transport/http2_client.go:1142 +0x8f8\nPrevious read at 0x00c425b38ff1 by goroutine 795:\n  google.golang.org/grpc/transport.(Stream).BytesReceived()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/transport/transport.go:377 +0x91\n  google.golang.org/grpc.(clientStream).finish()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/stream.go:492 +0x8d1\n  google.golang.org/grpc.newClientStream.func3()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/stream.go:254 +0x209\nGoroutine 2016 (running) created at:\n  google.golang.org/grpc/transport.newHTTP2Client()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/transport/http2_client.go:267 +0x12ec\n  google.golang.org/grpc/transport.NewClientTransport()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/transport/transport.go:463 +0xbe\n  google.golang.org/grpc.(addrConn).resetTransport()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/clientconn.go:876 +0x414\n  google.golang.org/grpc.(ClientConn).resetAddrConn.func1()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/clientconn.go:648 +0x41\nGoroutine 795 (running) created at:\n  google.golang.org/grpc.newClientStream()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/stream.go:269 +0x1703\n  google.golang.org/grpc.NewClientStream()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/stream.go:105 +0x1fc\n  github.com/coreos/etcd/etcdserver/etcdserverpb.(watchClient).Watch()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/github.com/coreos/etcd/etcdserver/etcdserverpb/rpc.pb.go:3191 +0xe9\n  github.com/coreos/etcd/clientv3.(watchGrpcStream).openWatchClient()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/github.com/coreos/etcd/clientv3/watch.go:774 +0x18e\n  github.com/coreos/etcd/clientv3.(watchGrpcStream).newWatchClient()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/github.com/coreos/etcd/clientv3/watch.go:691 +0x5c3\n  github.com/coreos/etcd/clientv3.(watchGrpcStream).run()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/github.com/coreos/etcd/clientv3/watch.go:422 +0x12b\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1316, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRw_Y6mQMKt4Hjv_X2K-Ql6pN6PRRks5sEtizgaJpZM4N82Z0\n.\n. Hey, thanks for reporting the issue. This is actually the expected behavior, the error returned by send is not indicative of the final status of the stream (it only tells the error incurred during that send). The final status of the stream can be retrieved by Recv().\nNote: If there's data before the error Recv will first return the data and finally the error.\n\nI've opened a pull request to document this: #1320. There might be data stored in the buffer before the error, so it's\nimportant that the user application code calls Recv and consume the data\nand eventually see the error.\nOn Mon, Jun 19, 2017 at 9:08 PM, stub42 notifications@github.com wrote:\n\nIf Send gets EOF or other unrecoverable stream error, should it\nautomatically call Recv() and return that error instead?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1319#issuecomment-309639373, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR_L0h-SIya8yHfmcBSJ3-ewszdRwks5sF0XKgaJpZM4N-hLw\n.\n. Hey,\nThanks for the detailed representation of the problem. However, the data seems very hard to believe. The number of go routines seems to be decreasing proportionally to the increase in number of RPCs. This can't be the case. Would you mind vetting the graphs further? \nAlso, it'd really helpful if you can provide a reproduction of this issue. . That doesn't answer why would the goroutines increase during inactivity? Broken connections don't spawn new goroutines on Go server.\n\nCan you provide a reproduction of it?. Since it's really hard to pursue this without a reliable reproduction and the fact that we haven't noticed anything similar. I'm going to go ahead and close this issue. Feel free to get back to us if you find something more or perhaps a reproduction. Thanks for your time and effort.. Looks like the error changed from \"too_many_pings\" to client seeing a flow control violation after patching the bdp PR. \nTo rule things out further can you patch with just this change instead of the whole bdp PR.\nAlso, is it possible to create a reproduction of this?\nFrom what I understand: you have a Golang server and a Python client making streaming RPCs. \nHow many RPCs are made on a single connection? What are the data sizes like?  Is the connections long lived?\nAgain, sharing a minimum reproduction will greatly speed-up the process to resolve this.\nThanks.\n. Your test script? Is there a link you provided earlier that I missed? If so, can you share it again here please?\nThanks for running it with just the patch, helps narrowing things down.. I believe I know what the issue is. I'm gonna create a PR and let me know if patching that solves it.. Try this patch on top of the last patch. I'm hoping this should solve the problem.\n. @kahuang  You're right, applying both the patches is important: without the ping patch you'll still see the \"too_many_pings\" error.\n. Can you share the logs for the new issue that you're seeing?\nAlso, note that bdp_probing is important for performance. Ideally, we'd like to resolve the issue with bdp_probing enabled.. 1. What are the differences between the local and production environment besides the scale? Maybe, that'll give us some insight into how we can reproduce the issue.\n\nCan you please paste the exact error(code, description, trace everything) that you get with both patches in and bdp enabled on production? Admittedly, http2 parsing error is quite misleading since both issues so far have been in the transport.\n\nAnd yeah as @ncteisen  mentioned we really would like to resolve this issue with bdp_probing enabled. So thanks for all your effort and input.. Seems like there's still a flow control violation happening somewhere. Need to think over it a bit more. Thanks for the logs.. I realized there was a mistake in the patch. I've update the PR\nCan you patch this and let us know how this affects the local environment?. Hey @kahuang, I was wondering if the issue was resolved after the fix to that patch.. These logs don't show the root cause. Was there any error like we were seeing before; \"overflows incoming window\"? Could you also send client-side logs?. @ncteisen  Can you recommend what tracing keywords should be used?. I'm suspecting this is no more a transport error. Perhaps, gRPC server is sending the c client something that it can't parse. Can you maybe share the go server logs as well?. Are you using go from head? If so set the verbosity level to 2.\nBut meanwhile, can you share the current go logs that you have?\n@ncteisen  Do you find anything unusual in the latest tmp.txt that Andrew shared? Also, can you think situations when c-core would throw this \"Failed parsing HTTP/2\" error?. By the way you can set the verbosity level using this environment variable: GRPC_GO_LOG_VERBOSITY_LEVEL. Sounds good, thanks for all the input. . @menghanl  Can you help with logging?\n@kahuang  Make sure you do apply the patch https://github.com/grpc/grpc-go/pull/1367 on top of master since it's not merged yet.. That seems unlikely to cause the kind of error you're seeing. Are you sending any metadata back to the client?. Ah I see, not sure about that either. But are you sending any metadata back or perhaps sending Status errors?. > It seems that applying #1367 (at least the ones I could) cause immediate failure of parsing the HTTP/2 stream.\nI don't suppose partially patching #1367 will help. Let me clean it up and get back to you.\n. @kahuang  Ok I have resolved the conflicts can you please try again?. These changes were merged along with PR #1310.. This is bizarre; t.outQuotaVersion should've been initialized with the transport struct.  Does this help?\nWait, is it a 32bit system? If so, try this instead.. The change that you're pointing to only moves the field to the beginning of the struct. Am I reading it right? Why does it work?. The issue that you're pointing too is about making 64-bit variables work on 32-bit machines. In that case the second fix that I provided should work.\nDid you try both of them?. Cool, I've created a pull request to do just that. We'd rather use a 32-bit number than going the alignment route.. Thanks for notifying.. Hey,\nThanks for the idea, I'm already working on something similar (as you noticed in the issue you linked earlier). Would it be ok if I took over this effort along with the other thing?\nThanks.. Cool, closing this PR for now then. Will have something out possibly by end of next week.. Here's why I think you're getting the 503s (Service Unavailable):\nWhen the client gets a GoAway for a transport, it GracefullyCloses that transport(i.e. waits to close it until all the active RPCs have finished) but at the same time it tries to create another transport to the server (relevant code). However, in your case the server has stopped serving on that port and presumably the new one hasn't started serving on that port yet. Thus this new connection attempt will result in a 503 Service Unavailable.\nFor all the transports on all the ClientConns that got a GoAway one 503 will be received.\nDoes this hypothesis align with what you're seeing? . @mattklein123  Just to be clear I don't suspect that the problem here is in-flight RPCs being cancelled.\nI would expect to see a service unavailable when trying to establish a connection instead of making an RPC call. Also, even if two GoAways were sent the client behavior would remain the same; close the current connection and try to establish a new one. Since the server would stop listening after the 2nd GoAway the client would still see a 503 when it attempts to connect agan.\nAlso going through the server code quickly it looks like the in-flight RPCs would just be accepted by the server even after having sent a GoAway. This might be a bug that we'd have to look into; I believe we should be more restrictive and not accept those RPCs(this pertains to the race you mentioned above). . Do you think there might be a situation when the old server has stopped listening but the new server hasn't started listening yet and the client is creating a new connection(perhaps a new ClientConnbeing created or an existing ClientConn saw a connection error and is trying to reconnect).\nAbout the server accepting in-flight RPCs even after having sent a GoAway: I'll dig deeper into that and perhaps write a simple reproduction of it to make sure this is the case.. After further investigation it turns out that although the sever doesn't reject the new stream request outright it silently ignores it. Moreover, when the client gets a GoAway it closes all streams originated after the goaway ID sent by the server, causing Unavailable errors on all those in-flight RPCs.\nThe proposal of sending two GoAways makes sense and is in fact mentioned in the HTTP2 spec itself. It was also proposed in #147 but wasn't implemented. \nWe'll prioritize this and should have something out by end of this week.. The implementation is not complete yet. I'm working on the server-side part of it which I hope to send the PR out today. By next week it should be merged and if you guys require a release for that, we can go ahead and create 1.5.1.\n. This slipped through the cracks this week. We'll get this is in early next week.\nSorry about and thanks for you patience.. The issue here is about incompatibility between different golang versions.\nA golang server between 1.3 and 1.4 (inclusive) is incompatible with any client that tries to do BDP estimation. Now c-core included BDP estimation a while back (unsure which version) and we introduced it in v 1.5. I suppose the bug went unnoticed for this long  because there weren't many cases of golang server talking to a c-core client, yet.\nYou are right that the safest bet is to update the golang server.. Going this route instead.. This was never seen again. Closing it now.. Yes go ahead; the peer is set in a stream's context when it's created so setting it somewhere here should do the trick.. There's a server option to limit the number of streams that a client can\nopen on a connection MaxConcurrentStreams.\nOn Aug 15, 2017 7:47 PM, \"Suhas\" notifications@github.com wrote:\n\nI am new to go-gRPC. So please pardon my ignorance if this is not the\nright place to ask questions and let me know where I could ask questions in\nfuture regarding go-grpc.\nI am looking for pointers to any existing mechanisms to prevent rouge\nclient abuse in go-grpc.\nTo keep it simple, For example, if I have a gRPC server which has a single\nrpc called SendData() which streams Data.\nNow I want to restrict invocation of SendData() to once per client only.\nI.e. only one active stream per client. Is there way to enforce this ?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1442, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR-dyracdkBK2mlqygaiK2mYjCnLEks5sYlg6gaJpZM4O4Wqu\n.\n. This should be in this week. Sorry for the delay I got distracted by something else.. A timeout on an RPC may occur even before an underlying transport is selected to send the RPC on. In other words the operation timed-out before we could get to know about a peer and therefore there's no peer info to be returned.  . The lowest hanging fruit, IMHO, is to reuse the memory buffer allocated to copy bytes from data frame, as @dfawley suggested. \nhttps://github.com/grpc/grpc-go/blob/cd563b81ec334abe62cda34a33b8640ea67ef5ba/transport/http2_client.go#L869\n\nThis buffer can be recycled when the application has read it and copied to its preallocated chunck to be used for unmarshling.\nhttps://github.com/grpc/grpc-go/blob/cd563b81ec334abe62cda34a33b8640ea67ef5ba/transport/transport.go#L143\nWe can have a few sync.Pool with different sizes increasing in say, powers of 4, and capped to the max size of http2MaxFrameLen. Also it'd be great if the API is such that the base(4 in this example)  is configurable so we can try different values with our benchmarks.\nOne problem with sync.Pool is that it drops all the memory during every GC cycle. In following efforts we can work with the Go team to resolve that. Using a leaky buffer comes with the onus of doing our own GC cycles for that memory. I don't think that's a good idea.\nThings that C++ and Java do don't necessarily apply to Go. \nC-core takes a user configurable parameter to cap the max amount of memory to be used by the binary. They start releasing memory as pressure builds up. So, it's essentially a library specific GC.\nJava, IIRC, allocates memory from the OS space and keeps reusing that without ever freeing it again. This memory is not freed by the GC either because it's outside of JVM's jurisdiction.\nI personally think, in gRPC-Go, we shouldn't go about allocating a big a chunk and doing our memory management on that. We should at least give sync.Pool a chance and see if that can be optimized to fit our use case.. @Zeymo  Thanks for the suggestions. We are aware of sync.Pool's behavior to not return the most fitting buffer. In the suggestion above I mention using buckets of sync.Pool with different sizes. \nHowever, as an exercise it may be interesting to know how would a single sync.Pool perform for the transport layer since we know no single buffer can be greater than http2MaxFrameLen which is 16K for now.. @coocood This actually seems like a good idea to reuse buffer on the gRPC layer (perhaps a variation of this might work on the transport layer as well). It's simple, doesn't require global shared pools. However, the only thing that worries me is the case when we have long lived streams with intermittent reading/writing activity of large messages (say >=1MB). In such a case, this stream will carry a large buffer for its life. Now if there were thousands of such streams we're in serious trouble. Perhaps, this problem can be solved by freeing this buffer every once in a while from the stream monitor loop here:\nhttps://github.com/grpc/grpc-go/blob/7f2472bbc6ac269af51a9c47ae8ef8c625383e1f/stream.go#L283. @coocood I like that idea too. You're right accessing the buffer from two different goroutines would require use of locks which is an additional performance overhead. CallOption seems like a good way to go about it. \nI'm not actively working on memory reuse currently, though it's high priority on my list of things to do. If you want to take a stab at it, go ahead. Otherwise, hopefully, I'll get to it Q1 2018.\nAlso, thanks for these great ideas. :). Hey, @scotthew1\nSorry about the late response. I have written a design doc for this optimization but I haven't started its implementation yet because we are working on a major refactoring of our transport. \nThis change will follow that refactor and I anticipate about 4 more weeks for this change to be merged.. Following the aforementioned design, I have created #1987. Once this is merged, we will experiment with reusing the buffer.. @havoc-io  Yes soon after we finished the implementation we identified potential security issues with that approach.\nCurrently, we have following ideas to get rid of extraneous memory and our efforts are around identifying which one is most feasible in short term.\n\n\nUpdate the framer library to not reuse a frame's buffer until told to.\n\n\nUpdate proto deserialization to be able to work on multiple buffers.\n\n\nUse our own framing implementation to be able to use scatter/gather on underlying net.conn. I've opened an issue on golang protobuf repo to provide an API to deserialize from multiple buffers proto.UnmarshalV, which is point 2 in the previous post.\nI'll be working on it further to hopefully provide an implementation for it.. Hey @tsuna  Thanks for looking into the deadlock and coming up with a solution.\nMay I suggest an alternative approach:\nThe purpose of awakenKeepalive channel is tell operateHeader method if keepalive routine is dormant or not. If the channel is writable (i.e has no data on it) the keepalive routine must be dormant. Thus, \n\nIt needs to be woken up by writing on it, and\na keepalive ping must be sent out.\n\nWhen we initialize the transport we make this channel non-writable by writing some data on it so that later when keepalive routine realizes it must go dormant it makes the channel writable by reading from it. Notice at this time a lock must be acquired since the condition to go dormant depends on number of active streams.\nThis works fine for the first time; the 1st stream comes in, operateHeader kicks off keepalive routine out of dormancy by writing on it and sends a ping. However, at this time after keepalive routine has read from the channel and woken up the channel is writable! This is erroneous since awakenKeepalive must be writable only when keepalive routine is dormant.\nSo, the next time around when keepalive routine comes to realize that it must go dormant it tries to read from the awakenKeepalive channel. But since awakenKeepalive is writable it doesn't have any data on it and can't be read from. At this point the keepalive routine waits having held the lock causing this deadlock.\nI suggest we make awakenKeepalive non-writable by writing data on it again in operateHeader.\nIf this code is executed that must mean that keepalive routine is waiting here. And now since the channel is writable we can write on it again after this line so that awakenKeepalive becomes non-writable when keepalive routine is non-dormant. . @tsuna Yes you understood that right and glad that it solves the problem.\nYour concern is valid that this code is quite complicated with the locks and channels. However, holding that lock while making awakenKeepalive writable again is necessary. If we released the lock before reading off the channel, it might so happen that NewStream code might get executed which tries to write on the channel but doesn't since the channel isn't writable yet. Later when keepalive routine starts executing again and expects to read from awakenKeepalive it won't be able to since it missed the opportunity. Holding the lock ensures that if keepalive sees number of streams to be 0 it can safely go dormant by making awakenKeepalive writable.\n. @tsuna  Thanks for taking care of this. Looks good.. @irfansharif  Thanks for reading through the code, understanding it and finding optimization hot-spots. However, we do wish you had talked to us a little, before writing out all the code that you have in all your 3 PRs. We're working on optimization quite actively and are aware of these optimizations. Some of your changes albeit good, introduce issues. For instance, in your buffer re-use PR you're making a copy of data to add it to headers. Also, some of these changes will be rendered obsolete once we're done with the code restructuring that's underway currently.\nUnfortunately we won't be able to review the code either for a couple of weeks. I apologize about that. \nIt's always a good idea to drop in a quick note to discuss design before making changes.\nThanks for your time,. @irfansharif This PR looks good. Can you run our benchmarks on this to see that it doesn't have any detrimental effects on the normal case(proto object doesn't implement Marshal or Unmarshal).. Ping.. @irfansharif  Seems like this PR is trying to optimize the error path. Am I understanding this right? If so, we'd rather not get into this. What do you think?. From my understanding the increased latency is because the sheer load 2000 concurrent streams put on the CPU and memory. I ran our benchmark for 2000 concurrent streams, 1kb request and response, Unary rpcs and found CPU utilization at 450%.\nNote that batching is actually good when there are multiple streams since it saves expensive syscalls.\nI also ran the same benchmark on this PR. This improves the performance notably. Perhaps, run your benchmarks on this and see if it helps. We plan to merge it in soon.\nA small remark on your benchmark code:\nvar dur is accessed unsafely by multiple go-routines. You could perhaps change it to int64 and update it atomically.\nAlso, I recommend looking at our benchmark code\n. On my machine the master branch shows latency(for 2000 concurrent streams) to be 107ms \n```\n  go run benchmark/benchmain/main.go -benchtime=10s -workloads=unary   -compression=off -maxConcurrentCalls=2000 -traceMode=false   -reqSizeBytes=1024 -respSizeBytes=1024   -latency=0s -kbps=0 -mtu=0   -cpuProfile=cpuProf -memProfile=memProf -memProfileRate=10000\nUnary-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_2000-reqSize_1024B-respSize_1024B-Compressor_false:\n  187049         53730 ns/op    23579 B/op       143 allocs/op\nHistogram (unit: \u00b5s)\nCount: 187049  Min: 656.6  Max: 265936.6  Avg: 107151.64\n\n[      656.573000,       656.574000)       1    0.0%    0.0%\n[      656.574000,       656.581629)       0    0.0%    0.0%\n[      656.581629,       656.647462)       0    0.0%    0.0%\n[      656.647462,       657.215542)       0    0.0%    0.0%\n[      657.215542,       662.117581)       0    0.0%    0.0%\n[      662.117581,       704.417928)       2    0.0%    0.0%\n[      704.417928,      1069.433260)       0    0.0%    0.0%\n[     1069.433260,      4219.199233)       0    0.0%    0.0%\n[     4219.199233,     31398.949825)      61    0.0%    0.0%\n[    31398.949825,    265936.642000)  186984  100.0%  100.0%  ##########\n[   265936.642000,              inf)       1    0.0%  100.0%\n```\nWhile the loopy_writer branch show latency of 64ms for the same:\n```\n go run benchmark/benchmain/main.go -benchtime=10s -workloads=unary   -compression=off -maxConcurrentCalls=2000 -traceMode=false   -reqSizeBytes=1024 -respSizeBytes=1024   -latency=0s -kbps=0 -mtu=0   -cpuProfile=cpuProf -memProfile=memProf -memProfileRate=10000\nUnary-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_2000-reqSize_1024B-respSize_1024B-Compressor_false:\n  308610         32460 ns/op    27690 B/op       190 allocs/op\nHistogram (unit: ms)\nCount: 308610  Min:   3.7  Max: 208.1  Avg: 64.81\n\n[        3.675335,         3.675336)       1    0.0%    0.0%\n[        3.675336,         3.675343)       0    0.0%    0.0%\n[        3.675343,         3.675405)       0    0.0%    0.0%\n[        3.675405,         3.675924)       0    0.0%    0.0%\n[        3.675924,         3.680273)       0    0.0%    0.0%\n[        3.680273,         3.716730)       0    0.0%    0.0%\n[        3.716730,         4.022339)       0    0.0%    0.0%\n[        4.022339,         6.584198)       7    0.0%    0.0%\n[        6.584198,        28.059752)     601    0.2%    0.2%\n[       28.059752,       208.085003)  308000   99.8%  100.0%  ##########\n[      208.085003,              inf)       1    0.0%  100.0%\n```\nBut I understand that you're looking for better latency values. \nSince you do not care about throughput, how about limiting number of concurrent streams to 100, the latency in such a scenario is about 5ms:\n```\ngo run benchmark/benchmain/main.go -benchtime=10s -workloads=unary   -compression=off -maxConcurrentCalls=100 -traceMode=false   -reqSizeBytes=1024 -respSizeBytes=1024   -latency=0s -kbps=0 -mtu=0   -cpuProfile=cpuProf -memProfile=memProf -memProfileRate=10000\nUnary-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_100-reqSize_1024B-respSize_1024B-Compressor_false:\n  186687         53590 ns/op    23374 B/op       145 allocs/op\nHistogram (unit: \u00b5s)\nCount: 186687  Min: 672.2  Max: 12310.5  Avg: 5357.82\n\n[     672.227000,      672.228000)       1    0.0%    0.0%\n[     672.228000,      672.233097)       0    0.0%    0.0%\n[     672.233097,      672.264170)       0    0.0%    0.0%\n[     672.264170,      672.453619)       0    0.0%    0.0%\n[     672.453619,      673.608640)       0    0.0%    0.0%\n[     673.608640,      680.650517)       2    0.0%    0.0%\n[     680.650517,      723.583115)       3    0.0%    0.0%\n[     723.583115,      985.332614)      12    0.0%    0.0%\n[     985.332614,     2581.155000)    1286    0.7%    0.7%\n[    2581.155000,    12310.492000)  185382   99.3%  100.0%  ##########\n[   12310.492000,             inf)       1    0.0%  100.0%\n```\nYou can do this by setting this server parameter:  grpc.MaxConcurrentStreams. Why are more than one concurrent RPCs needed if it's only a heartbeat?\nFor a single call on on a connection, the latency is actaully 193 us\n```\n go run benchmark/benchmain/main.go -benchtime=10s -workloads=unary   -compression=off -maxConcurrentCalls=1 -traceMode=false   -reqSizeBytes=1024 -respSizeBytes=1024   -latency=0s -kbps=0 -mtu=0   -cpuProfile=cpuProf -memProfile=memProf -memProfileRate=10000\nUnary-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1024B-respSize_1024B-Compressor_false:\n   51275        195028 ns/op    23465 B/op       214 allocs/op\nHistogram (unit: \u00b5s)\nCount: 51275  Min: 131.3  Max: 2684.4  Avg: 194.82\n\n[    131.280000,     131.281000)      1    0.0%    0.0%\n[    131.281000,     131.285151)      0    0.0%    0.0%\n[    131.285151,     131.306533)      0    0.0%    0.0%\n[    131.306533,     131.416676)      0    0.0%    0.0%\n[    131.416676,     131.984026)      0    0.0%    0.0%\n[    131.984026,     134.906488)      5    0.0%    0.0%\n[    134.906488,     149.960282)    666    1.3%    1.3%\n[    149.960282,     227.503392)  45713   89.2%   90.5%  #########\n[    227.503392,     626.933186)   4880    9.5%  100.0%  #\n[    626.933186,    2684.423000)      9    0.0%  100.0%\n[   2684.423000,            inf)      1    0.0%  100.0%  \n```\nWe do realize that the critical path has a lot of contention and all the synchronization code makes latency worse as the number of concurrent calls increase on a connection. We are working on reducing this so these numbers should get better in the near future.\nAbout having multiple connections vs 1: What's the use case that you are optimizing? Is it a heartbeat mechanism? Or a service that sends a query to a server and gets response back? A mixture of both?\nIn case it's 1: I would go with one connection and limit the number of concurrent heartbeats to 1. However I do recommend looking into our keepalive feature. This essentially makes sure that a connection is active(i.e. the other side is alive)\nIn case it's 2: If it's the same backend server, I wouldn't really suggest opening another connection to it, rather limit the number of concurrent calls.\nIn case it's 3( The client queries the server and also runs a heartbeat mechanism): I would recommend the same as case 2 with keepalive turned on.\n. I see. I suppose I was wrong in expecting that a server throttling of number of concurrent streams would show better results. It shouldn't. The client still creates all 1000 streams but sends out only 100 at a time, thus the client side contention isn't really reduced.\nMy opinion is that the high latency numbers are because of client-side contention and not batching. Consider a case where no batching happens, if a lot of big messages get to write before a small one, even then the small message RPC will see high latency. Furthermore, loopy_writer branch that improves batching show performance improvement.\nCreating multiple connections might help with the contention but I can't say that confidently since we don't have benchmarks for such a scenario. I can work on putting something together this week to test it out. \nAbout keepalive, if it finds that a connection is no longer active it closes the connection itself. At such a point all FailFast RPCs should fail.\n. Wanted to update this issue with the current state of master:\n```\nmmukhi@mmukhi:~/go_code/src/google.golang.org/grpc$ go run benchmark/benchmain/main.go -benchtime=60s -workloads=unary   -compression=off -maxConcurrentCalls=1000 -trace=off   -reqSizeBytes=1024 -respSizeBytes=1024 -networkMode=Local -cpuProfile=master.cpu\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1024B-respSize_1024B-Compressor_false: \n50_Latency: 19.3949 ms  90_Latency: 22.7358 ms  99_Latency: 27.6315 ms  Avg latency: 16.8282 ms     Count: 3565323  24009 Bytes/op  134 Allocs/op \nHistogram (unit: ms)\nCount: 3565323  Min:   1.5  Max:  86.4  Avg: 16.83\n\n[       1.530619,        1.530620)        1    0.0%    0.0%\n[       1.530620,        1.530627)        0    0.0%    0.0%\n[       1.530627,        1.530677)        0    0.0%    0.0%\n[       1.530677,        1.531058)        0    0.0%    0.0%\n[       1.531058,        1.533959)        0    0.0%    0.0%\n[       1.533959,        1.556014)        0    0.0%    0.0%\n[       1.556014,        1.723683)        4    0.0%    0.0%\n[       1.723683,        2.998366)     3651    0.1%    0.1%\n[       2.998366,       12.689008)   911234   25.6%   25.7%  ###\n[      12.689008,       86.361053)  2650432   74.3%  100.0%  #######\n[      86.361053,             inf)        1    0.0%  100.0%\n```\nWe're working on further improving concurrency and will keep updating this issue as we move along.. fixed by #1624. ugh! I can't believe I missed it. Making version 32 bit now. Sorry for the breakage..  Fix this issue with #1517. Doesn't work! :( . From what I understand the framer reuses it's internal buffer.\ncode link 1\ncode link 2\ncode link 3\nSo the writer(a buffer in our case) that the framer writes to must copy the contents of this buffer and not keep a reference to it since it will be invalidated in the next call.. >  ~50% of the performance delta is due to allocations of the MetaHeadersFrame and MetaHeadersFrame.Fields\nHow are you getting that number? I ran a cpu profile on your benchmarks( btw cpu profiling there is a little buggy at least for y, if you have fixed your local branch, can you push the updates?) and it seems to me that most of readMetaFrame's time is going into growing slices(presumably for MetaHeadersFrame.Fields). Perhaps, reusing MetaHeaderFrames might came in handy. But we've gotta keep in mind that hpack encoding and decoding are inherently expensive. If we don't pass along a decoder then we'll have to do this decoding ourselves.\n\nUnfortunately, those changes don't move the performance needle at all for gRPC.\n\nDid you make a prototype change already?\nimplementation of readMetaFrame may be sub-optimal but I haven't taken a closer look at net/http internals. \nOur current focus is on reducing contention for high-concurrency use-cases. Vetting net/http's implementation for performance seems like long shot right now. @dfawley what do you think?\n. Hey can you share more information to help us reproduce it.\nWhat exactly did you do?\n1. Do you have a custom balancer?\n2. Is your code a mix of both v1 v2 balancer/resolver code?\n3. Is the dial blocking or non blocking?\nPlease feel free to share any other relevant information that you think might help us dig deeper.\nIt'd be great if you can share some code? A small reproduction code will be even better.\nThanks. I signed it!. So did TestServerStatsClientStreamRPCError\n- FAIL: TestServerStatsClientStreamRPC (0.00s)\n    stats_test.go:710:  - 0, *stats.ConnBegin = &{Client:false}, ctx: context.Background.WithCancel.WithValue(stats_test.connCtxKey{}, &stats.ConnTagInfo{RemoteAddr:(*net.TCPAddr)(0xc82015f050), LocalAddr:(*net.TCPAddr)(0xc82015f020)})\n    stats_test.go:710:  - 1, *stats.ConnBegin = &{Client:false}, ctx: context.Background.WithCancel.WithValue(stats_test.connCtxKey{}, &stats.ConnTagInfo{RemoteAddr:(*net.TCPAddr)(0xc82015e180), LocalAddr:(*net.TCPAddr)(0xc82015e0c0)})\n    stats_test.go:710:  - 2, *stats.ConnEnd = &{Client:false}, ctx: context.Background.WithCancel.WithValue(stats_test.connCtxKey{}, &stats.ConnTagInfo{RemoteAddr:(*net.TCPAddr)(0xc82015f050), LocalAddr:(*net.TCPAddr)(0xc82015f020)})\n    stats_test.go:712: got 3 stats, want even positive number. The commit was reverted by #1619. Sorry about the breakage.. Thanks for the change.. Travis runs on revert sees same problem. Closing this.. You need to update your net/http2 package. Try go get -u.\nSimilar issues: https://github.com/grpc/grpc-go/search?q=+f.fr.SetReuseFrames&type=Issues&utf8=%E2%9C%93. Thanks for your repro. This is an issue in the gRPC server's transport logic; having scheduled the status for last stream in graceful stop mode the server incorrectly assumes that it can close the transport. It must wait for the status to be written out before doing that.\nI'll have a fix out today. . If that's the commit that's causing problems, then the server may set\nMaxConcurrentStreams limit and the client will not create more streams than\nthat limit.\nhttps://github.com/grpc/grpc-go/blob/a62701e4aa1d276bec70311251d62a478404d63f/server.go#L242\nOn Wed, Nov 29, 2017 at 6:36 AM, Derek Buitenhuis notifications@github.com\nwrote:\n\nI think we're experiencing the same issue (huge memory growth). I bisected\nthe first bad commit to 0d399e6\nhttps://github.com/grpc/grpc-go/commit/0d399e6307b06024368bb2624572e1271c6eda3d\n.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1685#issuecomment-347878892, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR3bpJmdfqOWSMTwfuTE-F9a2ibCKks5s7WwHgaJpZM4QoUu7\n.\n. When the client gets too_many_pings go away from the server it updates it's keepalive settings.https://github.com/grpc/grpc-go/blob/2ef021f78d5bf8030e14f780caa7f7c9d9488390/clientconn.go#L1064. Can't reproduce in 100,000 runs with race mode on.. So the default keepalive enforcement policy on the server is 5 minutes:\nhttps://github.com/grpc/grpc-go/blob/cce0e436e5c428f2094edd926779788f1024938d/keepalive/keepalive.go#L62\n\nAlso, by default client doesn't run keepalive unless there are active streams:\nhttps://github.com/grpc/grpc-go/blob/cce0e436e5c428f2094edd926779788f1024938d/keepalive/keepalive.go#L38\nThis explains why the connection is closed by the server soon after the client makes an RPC.\nIf you set the client to send keepalive pings every 120 seconds, make sure you set the server's enforcement policy accordingly.\nAlso turning on transport level logs by setting verbosity level to 2 will help seeing these messages:\nhttps://github.com/grpc/grpc-go/blob/cce0e436e5c428f2094edd926779788f1024938d/grpclog/grpclog.go#L26\nHope this helps.. Close() cancels the clientConn context which results in closing of all underlying transports.\nhttps://github.com/grpc/grpc-go/blob/cce0e436e5c428f2094edd926779788f1024938d/clientconn.go#L889. Would you like the docs to say something like \"although clientconn.Close returns the underlying transports may have not finished closing yet\" ? Such behavior, by the way, is true for a lot of other things too; for instance, a stream.Send may return but the message might not have been written on the wire yet. \nI don't see how these subtle implementation details affect using of the API.\nDo you have a suggestion on how you want the documentation to be updated and why is it important for your use-case? . On a quick first look it seems like this might be a problem in gRPC code. I'll take a deeper look later today and keep you posted. Thanks for reporting this issue.. This indeed is a gRPC issue. I'll create a fix for it today.\n. @willhug Thanks for reporting, I'll try and take look at it today.. Lol! I was actually going to reply with the same thing. I added a second sleep and things seemed fine. To make sure all grpc related goroutines exit you can use leakcheck : https://github.com/grpc/grpc-go/blob/8fba5fc8fd4073dd1452d6b2c7335030848a41b1/test/leakcheck/leakcheck.go#L22. Opened another PR(#1748) to fix this.. Clientconn.Close() cancels context( which underlying transports detect) before putting addrConns in Shutdown state. Fixed by #1870 . lis.Close() must cause lis.Accept() to fail with a temporary error which causes the server to keep trying to accept.\nhttps://github.com/grpc/grpc-go/blob/2720857d978b9873027f6d739d74794428f49c03/server.go#L504. Oh! The timing, I was just responding with the same thing. Following this realization I tried to repro your issue but was unable to. Here's my attempt.\nCan you provide a working repro?. Previously, I was doing fatalf in the server causing the program to exit before that print. I have updated the code. Did you try after the second commit? I can see \"Server exited\" printed:\nmmukhi@mmukhi:~/go_code/src/google.golang.org/grpc$ go run examples/helloworld/greeter_server/main.go \nfailed to serve: accept tcp [::]:50051: use of closed network connection\nServer exited. My repro doesn't change any gRPC code. I've updated the helloworld example to look more like your example code:\n```go\npackage main\nimport (\n    \"fmt\"\n    \"log\"\n    \"net\"\n    \"sync\"\n\"golang.org/x/net/context\"\n\"google.golang.org/grpc\"\npb \"google.golang.org/grpc/examples/helloworld/helloworld\"\n\"google.golang.org/grpc/reflection\"\n\n)\nconst (\n    port = \":50051\"\n)\n// server is used to implement helloworld.GreeterServer.\ntype server struct{}\n// SayHello implements helloworld.GreeterServer\nfunc (s server) SayHello(ctx context.Context, in pb.HelloRequest) (*pb.HelloReply, error) {\n    return &pb.HelloReply{Message: \"Hello \" + in.Name}, nil\n}\nfunc main() {\n    lis, err := net.Listen(\"tcp\", port)\n    if err != nil {\n        log.Fatalf(\"failed to listen: %v\", err)\n    }\n    s := grpc.NewServer()\n    pb.RegisterGreeterServer(s, &server{})\n    // Register reflection service on gRPC server.\n    reflection.Register(s)\n    var wg sync.WaitGroup\n    wg.Add(1)\n    go func() {\n        defer wg.Done()\n        if err := s.Serve(lis); err != nil {\n            fmt.Printf(\"failed to serve: %v\\n\", err)\n        }\n    }()\n    lis.Close()\n    wg.Wait()\n    fmt.Println(\"Server exited\")\n}\n```\nAre you saying that this code on your machine doesn't print the following?\nfailed to serve: accept tcp [::]:50051: use of closed network connection\nServer exited. Nevermind, I my client wasn't synced to master thus #1745 wasn't taking effect. Let me retry.. @sg3des Thanks for intimating us of your findings.\nQPS numbers are subjective to the machine that you're using. Try and saturate your resources by varying number of connections and number of concurrent streams. May be start with 100 connections each with 100 concurrent streams. \nAlso, we are actively working on performance and tracking it in issues #1512 and #1280.\nSince this issue has misleading numbers and is a dupe I'm going to go ahead and close this one.. Ah man! I missed that possibility. I'll try and have a fix out by tomorrow\n( traveling today).\nAlso, thanks a lot for digging up the root cause.\nOn Fri, Dec 29, 2017, 2:25 AM Vlad Hanciuta notifications@github.com\nwrote:\n\nPlease answer these questions before submitting your issue.\nWhat version of gRPC are you using?\nHEAD as of today (65c901e\nhttps://github.com/grpc/grpc-go/commit/65c901e45820d9c7555afc14337b82cf53ea3e42\n)\nWhat version of Go are you using (go version)?\ngo version go1.9.2 darwin/amd64\nWhat operating system (Linux, Windows, \u2026) and version?\nLinux, macOS\nWhat did you do?\nConnecting to a server, passing grpc.WithWaitForHandshake() option.\nWhat did you expect to see?\nConnection succeeding and staying up.\nWhat did you see instead?\nConnection comes up, and data gets exchanged, but after 20 seconds the\nconnection gets terminated with:\nWARNING: 2017/12/29 00:23:01 grpc: addrConn.transportMonitor didn't get server preface after waiting. Closing the new transport now.\n(GRPC_GO_LOG_SEVERITY_LEVEL was set to info)\nThe connection seems to be closed in addrConn.transportMonitor after the\nconnection deadline passes, because ac.backoffDeadline is not zero. The\nreason why it isn't zero, although the preface has been received (and\nactually RPC calls were done over that connection) is because the code that\nsets/resets it is racy.\nWe seem to reset it in onPrefaceReceipt closure (in\naddrConn.createTransport) which gets passed to\ntransport.NewClientTransport, but that closure is called from a different\ngoroutine (see http2Client.reader). It might happen that this closure\ngets called before we set ac.backoffDeadline at the end of the loop in\naddrConn.createTransport, so ac.backoffDeadline will remain set even\nthough the connection has succeeded.\nThis race is made a lot more likely by setting WithWaitForHandshake\nbecause in that case we wait for done to be closed, which means that we\nwait for onPrefaceReceipt to be called before we proceed further and set\nac.backoffDeadline.\nThe issue was introduced in #1648\nhttps://github.com/grpc/grpc-go/pull/1648.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1772, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR8lGMEdBtGfbMjHR6SM5AXYBsqbLks5tFKHegaJpZM4RO8WX\n.\n. You should be able to do that and more using a stats.Handler. Look into\nthe stats package and see it helps your use case.\n\nOn Fri, Dec 29, 2017, 5:54 AM ivanjaros notifications@github.com wrote:\n\nI would like to expose an rpc call that would provide brief information\nabout the server and one thing I cannot figure out is how to get the number\nof active connections?\nI've looked at the code but the Server.conn is simply inaccessible from\nany callback I've looked at.\nIs this even possible at this time, to get this information?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1773, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR_-XImBb-ZygGyXEqsCfo1jUAJHJks5tFNL8gaJpZM4RPEFG\n.\n. Hey @coocood We recently came with an idea to get rid of this buffer entirely. Currently, the transport reads data into individual buffers and puts them on recvBuffer (which is sort of an infinite buffer). The GRPC layer looks at data header to see how big the message is and then goes about allocating this buffer to ask transport to fill it up.\nWe are thinking about making the transport do all that work; look at the header and figure out the message size and allocate memory for the whole message instead of small data chunks. Later, GRPC layer will accept a byte slice from the transport instead of asking the transport to fill one. \nThis will reduce multiple allocations and one copy for every message. However, this also requires updating the stream level flow control.\n\nIn lieu of this new plan, I'm going to go ahead and close this PR. We do greatly appreciate your time and effort that you've put in.. It shouldn't take too long once started. But, currently I'm working on a\nprecursor to it and will only be able to get to it in a few weeks down the\nline. Hopefully, by end of this quarter.\nOn Tue, Feb 6, 2018 at 8:34 PM, Ewan Chou notifications@github.com wrote:\n\n@MakMukhi https://github.com/makmukhi\nThe new design sounds good.\nHow long does it take to implement?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/1774#issuecomment-363654610, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRy0AWwQ6xNwaDtY3mp1uzYVMdtnBks5tSSfmgaJpZM4RP1rc\n.\n. @zhexuany friendly ping.. Unable to reproduce in 111,000 runs with race mode enabled.. @zhexuany Do you have any updates?. fixed by #1862. Thanks for reporting.. Hey @snowzach can you benchmark how long does it take to marshal/unmarshal both kinds of messages that you have? From gRPC's point of view, performance should same for a 4k byte message. If the bottleneck boils down to the cost of serialization and deserialization then it might be a more suitable question for the protobuf team.\n\nAs a workaround, you may try the following if it helps:\n Have multiple goroutines serialize these messages into bytes.\n If the order doesn't matter put them in a pool\n If the order does matter have them wait for their turn.\n Have another goroutine which only does stream.(grpc.ClientStream).SendMsg(&bytes)\n* This sending goroutine may either read serialized messages from the common pool(if order doesn't matter) or signal goroutines for their turns. (This synchronization will be costly!)\n. Turn on transport level logs by setting these two env variables:\nexport GRPC_GO_LOG_VERBOSITY_LEVEL=2\nexport GRPC_GO_LOG_SEVERITY_LEVEL=info\nMay be the transport on the server of client side sees some error and\ncloses.\nOn Sat, Feb 24, 2018 at 4:26 PM, Michael Andersen notifications@github.com\nwrote:\n\nSorry, I can reproduce this even without the GZIP compressor. I still\ndon't know how to debug it though. The only error I get on the client with\nloglevel set to info is:\npickfirstBalancer: HandleSubConnStateChange: 0xc421bc6160, TRANSIENT_FAILURE\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1882#issuecomment-368271626, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR6yLJFRNEv-Nn5h7ILZfqwbUZoMFks5tYKisgaJpZM4SSF8Z\n.\n. The client is pinging the server too many times. Perhaps the keepalive\nsettings on the client and the server are not the same.\n\nOn Sat, Feb 24, 2018, 7:18 PM Michael Andersen notifications@github.com\nwrote:\n\nI finally got a new piece of information. The server prints this shortly\nbefore the client disconnects:\nERROR: 2018/02/24 19:16:02 transport: Got too many pings from the client, closing the connection.\nERROR: 2018/02/24 19:16:02 transport: Error while handling item. Err: connection error: desc = \"transport is closing\"\nWhat does \"too many pings\" mean?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1882#issuecomment-368279607, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR11QehqgSkWt1PprRu28OaEXAN7Lks5tYND3gaJpZM4SSF8Z\n.\n. If it's not keepalive then the only thing inside gRPC that sends pings is bdp estimator which will be triggered on receiving data. However, every time the server sends data frames it resets its pingStrike counter. To quickly check if it's the bdp estimator or not, try turning it off by setting WithInitialWindowSize and WIthInitialConnectionWindowSize dial options on the client.. A bdp estimator ping is sent by the client when it receives data from the server. And the server must have reset its counter for that dataFrame. In an old version the server was buggy and was resetting its counter only once for a message but you don't seem to be using that.\nThanks for narrowing it down. I'll take a look at it Monday. I don't suppose it would be easy for you to create a repro of it but if you find a way can you please upload it.. Thanks for trying to reproduce it. We might need to collaborate to debug it\nfurther. I'll look at the code again tomorrow to see if I can find any\nobvious mistakes.\n\nOn Sun, Feb 25, 2018, 4:17 PM Michael Andersen notifications@github.com\nwrote:\n\nI made a simple reproducer where a program connects to itself and invokes\nan operation that has roughly the latency I observed with roughly the\nconcurrency I had, but it doesn't seem to trigger it, sorry. Let me know if\nthere is anything else I can do to help\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1882#issuecomment-368359055, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR2G2PhzC3C28Mkgu9vdg9P-T_2Zgks5tYfgigaJpZM4SSF8Z\n.\n. Hey, that's great that you can reliably reproduce it in your set up.\nFollowing is what we know:\n\nHere's where the client sends a bdp ping: https://github.com/grpc/grpc-go/blob/f0a1202acdc5c4702be05098d5ff8e9b3b444442/transport/http2_client.go#L840\nAs you see this is inside of the data handler which is called every time the client gets a dataFrame.\nMoreover,client sends ping only if it already got an ack back from a previous ping.\nOn the server-side whenever a dataFrame is sent the server resets the ping counter:\nhttps://github.com/grpc/grpc-go/blob/f0a1202acdc5c4702be05098d5ff8e9b3b444442/transport/http2_server.go#L901\n\nClient sees data and sends ping.\nClient sees more data doesn't send ping.\nClient sees more data doesn't send ping.\nServer sees ping, but the counter was reset.\nServer sends back ping-ack and now sets the ping strike counter again.\nClient sees ping-ack and records bdp.\nThe whole process is repeated.\n\nI would start with making sure the faulty pings are always bdp pings. (Assuming you can update your local code):\n1.  Make sure there's nothing else that's pinging the server:  In the server's handlePing, right after this line\nI'd add the following:\ngo\nif f.Data != bdpPing.data {\n  panic(\"Some other pinger?\")\n}\n2. Perhaps, print out time whenever server sends data and receives ping.\nI'm sorry I have not been able to find out anything obvious yet. But my best guess so far is that something else is pinging the server.\nLet's see what you find and we'll go from there. I'd like to stare at these print statements and logs once you have them.\nAlso, thanks for helping out. :)\n. They are reset while scheduling dataFrame here:\nhttps://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L901\nBut I think you're right if we moved this to just before writing dataFrame\non the wire it would work out. Can you try doing that?\nOn Thu, Mar 1, 2018 at 2:41 PM, Michael Andersen notifications@github.com\nwrote:\n\nTo save you reading the whole logs, here is the important sequence:\n1519942464739172903 BDPDEBUG[http2Server=0xc423778f00] writing data1\n1519942464739261537 BDPDEBUG[http2Server=0xc423778f00] handlePing\n1519942464739266862 BDPDEBUG[http2Server=0xc423778f00] addPingStrikes2\n1519942464739318578 BDPDEBUG[http2Server=0xc423778f00] writing data1\n1519942464739452781 BDPDEBUG[http2Server=0xc423778f00] handlePing\n1519942464739459106 BDPDEBUG[http2Server=0xc423778f00] addPingStrikes2\n1519942464739535670 BDPDEBUG[http2Server=0xc423778f00] writing data1\n1519942464739659435 BDPDEBUG[http2Server=0xc423778f00] handlePing\n1519942464739674955 BDPDEBUG[http2Server=0xc423778f00] addPingStrikes2\n1519942464739678469 BDPDEBUG[http2Server=0xc423778f00] >maxPingStrikes\nBasically because we are writing three data frames and no header frames,\nwe don't reset the ping strikes.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1882#issuecomment-369756425, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR9_TJ7joHxuljxqyp2kjDJH0pojtks5taHkAgaJpZM4SSF8Z\n.\n. I'd put it right before, but it shouldn't matter as much.\n\nOn Thu, Mar 1, 2018 at 2:52 PM, Michael Andersen notifications@github.com\nwrote:\n\nShould it be before writing it to the wire or just after (the header is\njust after) ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1882#issuecomment-369759047, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnRw5oyiJb42TnfzK7L8UjQXon4iEtks5taHuSgaJpZM4SSF8Z\n.\n. I suspect the problem was a ping ack being scheduled between the two steps\nwhere write method resets the counter and schedules data. This scheduling\nof ping-ack sets the counter back.\nIf this indeed was the case then moving the reset instruction in\nitemHandler will eliminate the race.\n\nMultiple data frames get queued up in gRPC itself and I'm sure in the OS\nkernel too.\nOn Thu, Mar 1, 2018 at 6:24 PM, Michael Andersen notifications@github.com\nwrote:\n\nStill didn't hit this case now after four hours. I stopped the test\nbecause I needed the machine for a different test. Is it always the case\nthat every data frame gets an interleaved reply 1:1 ? You can't queue a few\nframes in the TCP window, clearing the reset strikes after each and then\nreceive the three pings in response all together? I don't know the code,\njust wondering\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1882#issuecomment-369799471, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR81g0qy4vfeZaUvmQ3YinIBywbxGks5taK1mgaJpZM4SSF8Z\n.\n. @PraserX We'd ideally like BDP estimator to be turned on. Is it possible for you to turn it back on and patch #1905 to see if that fixes your problem as well.  . This looks good. Thanks for the contribution. . @wy-zhang I just want to reiterate what @dfawley said. Are you sure that the RPCs were successful. Not seeing an error logged out doesn't necessarily mean that the RPCs are being successful, perhaps the connection is never established.\nI say that because:\n\n\nThe error on the server suggests that the client never sent settings frame, if so the connection won't be established.\n20 minutes is the amount of time in which the client will try to complete connection establishment.\n\nMy theory is that the client tries to connect but somehow the settings frame is not sent or \"lost\", the client waits for 20 minutes before timing out.\nTo reduce variables can you try and run you code on same machine instead of different virtual machine?. What I mean by connection establishment is an HTTP2-level connection establishment that is completion of client and server handshake which ends with an exchange of preface from both sides. Merely opening a port and accepting an incoming tcp connection (shown by netstat) doesn't mean that HTTP2 connection was established.\nWhen you say the client receives data from server, do you mean an RPC gets a response back?\nThe warning and info logs will be printed if GRPC_GO_LOG_SEVERITY_LEVEL is set to INFO (by default it is set to ERROR).\nAgain, I would advise running this locally first and making sure you still get errors.. The error saying \"context deadline exceeded\" means that the RPC didn't finish successfully. And the error on the server suggests that the connection wasn't established.\nIf you're not seeing any successful RPCs at all then I suspect there's some communication problem between the VMs that you have.. @vitalyisaev2  That limit of 100 is imposed only until server settings are received(part of handshake). If those settings contain a MaxConcurrentSetting then the limit is set to that. However, if the settings don't contain that specific one then the client allows infinite streams.\nFor the second part: It's not uncommon to have long-lived bidi streams especially when communication is frequent. Stream creation doesn't have significant overhead but it does require extra steps(encoding headers, spawning goroutines, writing extra data etc.). So, it really depends on your use case; if communication is infrequent enough, perhaps unary RPCs will suffice.\n@minaevmike  Thanks for pointing this out. This seems like a bug, I'll put in a fix soon. Thanks.. Can you clarify what's the error that you see? In the log you posted I only see goroutine traces.. Based on your response on grpc.io, can we close this issue then?. Couldn't reproduce in about 11,000 runs. Will keep this issue open in case travis finds it again.. --- FAIL: TestFailFastRPCErrorOnBadCertificates (25.36s)\n    end2end_test.go:533: Running test in bad-cred environment...\n    end2end_test.go:6177: Dial(_) = context deadline exceeded, want <nil>. \npicker_wrapper.go, line 73 at r4 (raw file):\n\n```Go\nfunc (bp *pickerWrapper) updateStickinessMDKey(newKey string) {\n  if oldKey, _ := bp.stickinessMDKey.Load().(string); oldKey != newKey {\n```\n\nHow about atomic.Swap instead of atomic.Load ?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \npicker_wrapper.go, line 80 at r4 (raw file):\n\n```Go\nfunc (bp *pickerWrapper) clearStickinessState() {\n  if oldKey, _ := bp.stickinessMDKey.Load().(string); oldKey != \"\" {\n```\n\nSame as above\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nstickiness_test.go, line 172 at r4 (raw file):\n\n```Go\n  }\nif picked[0] != 0 && picked[1] != 0 {\n```\n\nHow about: (picked[0] != 0 && picked[1] == 0) || (picked[0] == 0 && picked[1] !=0) ?\n\nComments from Reviewable\n Sent from Reviewable.io \n. Hey @a-robinson, sorry about the regression. I've created a fix for this which is in review. I should've linked it here. I'll have our team prioritize its review.\nThanks for starting out #2090 , but unfortunately the problem is a bit more nuanced.\nSince, reader goroutine can WriteStatus in parallel to WriteHeader happening in writer goroutine it can lead to multiple problems:\n\nRace on headerOk and header metadata as you know.\nPotentially sending header after status.\n\nAdding the lock back doesn't have much performance impact(~500ns on 120us that it takes for a unary RPC) since it's a non contended lock.. My best guess is that there's a connection problem between the client and the second server.\nTurn on logging on both client and the faulty server and see if anything interesting is printed out.\nexport GRPC_GO_LOG_SEVERITY_LEVEL=info\nexport GRPC_GO_LOG_VERBOSITY_LEVEL=2. MaxConcurrentStreams is a limit that the server sets for its clients by sending out HTTP2 settings frame.\nI'm not quite sure what do you mean by \"when I hit the concurrent stream limit\". As soon as you create at least as many streams allowed by the server, you have hit the limit. \nAs a user one is allowed to spawn as many streams as one would like but the gRPC client needs to respect the limit set by the server to throttle the clients. A server would ideally set it based on the resources available, so if your server has ample resources and are willing to increase the limit, why set a limit in the first place?. I suppose, right before this line, we could add the following logging:\ngo\nif t.waitingStreams == 0 {\ninfo(\"Client is trying to create more streams that allowed by server.\")\n}\nWhat do you think about this? However, I'd like to get more input on this from the rest of the team. \nOut of curiosity, is gRPC-Go performance something that you guys care about. If so, we'd love to hear what scenario does your service run in?. What you're describing is part of one of our goals; to have a gRPC-specific tracing for an RPC. We plan to record the time it takes an RPC in various stages of its life cycle; from creation to being scheduled, from being scheduled to being written on the wire and from there to getting a response back. We might add more events to that list since we haven't gotten to creating a formal design yet.\nDoing this also requires that normal scenarios (when this tracing is turned-off) are not affected by these expensive time.Now() calls.\nWe can use this issue to track progress on this goal.\nOn the server-side, we could have similar info logs(as mentioned in the previous comment) printed out whenever settings for a connection are updated.\nThat's great to know. We are actively working on improving our performance and feedback is always welcome. \nAdditionally, looking at your code(part of it) looks like you are using gRPC's native server implementation which defaults to math.MaxUint32 for MaxConcurrentStreams. How does your client get throttled to go  HTTP2 server's default limit?. It's not new. If you look at the example code in our repo, you'll see that gRPC servers are launched by calling Serve() on them. That's using gRPC's native implementation for the underlying HTTP2 transport. \nLooks like your code does the same thing.\nWhy then, I wonder, do you see a smaller limit? When you say you increased the MaxConcurrentStreams limit, how did you do that?\nOne thing that I should mention: by default gRPC clients start with a limit set to 100, this limit is overridden when the server sends its first settings frame(part of the handshake process).. Do you have a proxy between your server and client? How did you increase this limit?. Ah man! Ok so this is what's happening:\n\n\nThe version of gRPC-Go that you are using is very old and had a bug such that the client would default to 100 streams and not update this value even after receiving the first settings frame from server unless the server explicitly set that setting.\n\n\nI'm sorry I got confused when you referred to code from head and thought that you were perhaps using a newer version. I do realize now you mentioned using v1.1 couple times actually.\n\n\nMoreover, that old version has a vastly different implementation of the underlying transport. We have made several major performance improvements on that since then. I'd recommend trying out our latest release. The most recent performance improvement, however, will go out in v1.12. You're more than welcome to try it out on head.\n\n\nAlso, feedback is always appreciated. :). Glad that worked.. Done. PTAL. \n\n\nReview status: all files reviewed at latest revision, 10 unresolved discussions.\n\nrpc_util.go, line 423 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAre those errors still true?\n\nYes they are.\n\ninternal/msgdecoder/msgdecoder.go, line 19 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis is internal to transport. I wonder if moving this to grpc/transport/internal/msgdecoder would still work..\n\nActually, we'd like it to not be a transport internal, since we don't want to make the transport aware of gRPC-level encoding.\n\ntransport/stream.go, line 64 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nUsing a field of an exported field from another package doesn't look like a good idea to me.\nIMO, the linked list related fields/structs should be kept in the same package.\nDefine a recvMagListNode struct with a next pointer as a container for *msgdecoder.RecvMsg?\nOr https://golang.org/pkg/container/list/ if you don't mind the extra type assertion.\n\nThat's a good point. How about I move the linked-list to msgdecoder package?\n\ntransport/stream.go, line 107 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhat does this comment mean?\nput and getNoBlock are already protected by b.mu.\n\nThat's right, the comment is just explicitly saying that. I'll remove it.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \ntransport/stream.go, line 3 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\n2018? Not sure.\n\nIt does contain code from 2014. So I don't really know what's the right thing either!?\n\nComments from Reviewable\n Sent from Reviewable.io \n. What error did you see? Can paste the log here?\nOn Wed, Apr 11, 2018, 9:40 PM AshfordN notifications@github.com wrote:\n\nPlease answer these questions before submitting your issue.\nWhat version of gRPC are you using?\ngRPC 1.11.3\nWhat version of Go are you using (go version)?\nGo 1.10\nWhat operating system (Linux, Windows, \u2026) and version?\nUbuntu 16.04\nWhat did you do?\nI tried to install the package using the recommended command go get -u\ngoogle.golang.org/grpc\nWhat did you expect to see?\nI expected the package to install without any problem\nWhat did you see instead?\nInstead I'm a massive amount of dependency errors\nI'm not sure why a simple RPC framework has to be this heavy. Personally I\nthink this is ridiculous.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1988, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR5e4PEI4qUj9EX0NqibcvNlDKli_ks5tntrTgaJpZM4TRIVY\n.\n. Thanks for the reproduction code. I'll try and run it Monday.. I tried to reproduce it using your code, but was unable to. Can you take a look at this and see if I'm missing anything?\n\nThis is what I get when I profile the server:\n```\nmmukhi@mmukhi:~/go_code/src/google.golang.org/grpc/examples/helloworld/greeter_server$ go tool pprof -lines http://localhost:50048/debug/pprof/heap\nFetching profile over HTTP from http://localhost:50048/debug/pprof/heap\nSaved profile in /usr/local/google/home/mmukhi/pprof/pprof.main.alloc_objects.alloc_space.inuse_objects.inuse_space.021.pb.gz\nFile: main\nType: inuse_space\nTime: Apr 16, 2018 at 6:32pm (PDT)\nEntering interactive mode (type \"help\" for commands, \"o\" for options)\n(pprof) top\nShowing nodes accounting for 512.19kB, 100% of 512.19kB total\n      flat  flat%   sum%        cum   cum%\n  512.19kB   100%   100%   512.19kB   100%  runtime.malg /usr/lib/google-golang/src/runtime/proc.go:3266\n         0     0%   100%   512.19kB   100%  runtime.mstart /usr/lib/google-golang/src/runtime/proc.go:1180\n         0     0%   100%   512.19kB   100%  runtime.newproc.func1 /usr/lib/google-golang/src/runtime/proc.go:3289\n         0     0%   100%   512.19kB   100%  runtime.newproc1 /usr/lib/google-golang/src/runtime/proc.go:3318\n         0     0%   100%   512.19kB   100%  runtime.systemstack /usr/lib/google-golang/src/runtime/asm_amd64.s:409\n``. Currently, in our code we aren't reusing memory so I'd be really surprised if there was a memory leak.. The gRPC server can return trailer only responses in error cases. Metadata in those trailers can be accessed by the [grpc.Trailer`](https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L213) call option.. Hey thanks for reporting the issue along with a repro. There seems to be a known issue in go 1.10 with plugins on MacOs. The most relevant issue that I found is this. Other related issues are: this and this.\nI'd recommend trying go at head and if there's still a problem perhaps provide this repro on one of the open issues on golang's repo. \nI'm closing this issue here but feel free to reopen/comment if there's something needed for gRPC-Go for this issue.. Created a fix.. We are actively trying to reduce the memory footprint for gRPC. #1455 tracks this issue.\nThe current effort is to be able to deserialize message spanned over multiple frames, hence multiple slices. This way we won't have to allocate this extra slice just to pass it to the deserializer.\nCan you clarify what you mean by using local function array?. To receive a message we create an empty buffer using make. This buffer based on its size and other details may or may not escape the current goroutine stack. We do not have any control on that.\nAs  for reusing memory we can use other constructs that #1455 talks about.\nI don't suppose there are any action items here and the aforementioned bug tracks memory reuse and efficiency. . Thanks for reporting. I'll take a look at it shortly.. Closing because it's a subset of #2074.. Can you provide a reproduction, please?. @abeeshks Can you clarify what's this about? Also seems like these changes were made on 1.4?\nAre you guys seeing data races in some scenario? If so, can you speak more on that. Possible provide a reproduction. We'd rather avoid using locks as much as possible.. #2144 fixes this. Ran 100000 successful runs.\nThe problem was that server didn't register an incoming stream with loopy until the stream had anything to send out. However, the client sends proactive stream-level window updates to the server which loopy processes. If this stream isn't registered with loopy yet, the window update gets lost.. 1M RPCs in parallel each of which is 1MB of data is about 1TB of memory. Unless it's a beefy server, I suspect your system isn't making progress very fast on those RPCs and the context deadline expires.. Hey @cstockton ,\nThanks for opening the issue. I apologize that the documentation is not clear enough to convey that the send doesn't lead to message being written on the wire.  We'll update that.\nAs for blocking until a message is written on wire, that would require augmenting the API surface. We'll have a discussion over that and get back to you.\nNote we can't make this our default behavior since it severely hampers performance. . The buffering is not indefinite, every time a message is sent it takes away from a local soft limit of 64KB which is then replenished as bytes are written on the wire. This also provides user back pressure and prevents them from accidentally OOMing their local machine. \nSafety on the send side it is about preventing users from accidentally OOMing themselves and not protecting them against a malicious DoS. \nAll of this is internal implementation and therefore not documented in the API. Also note that send and recv are not primitive calls they do not act on the underlying connection.\nBlocking on a write call until it has been been written on the wire has problems like the following:\n1. User initiates a write and blocks.\n2. The underlying connection is lost.\n3. Write takes say 20-30 mins to realize that and blocks until then.\n4. Note that net.Conn Write doesn't take a context that could timeout.\n5. Even if the stream's deadline expired, the write operation will be blocked for that long.\n. > Blocking on a write does not mean blocking until success, it means blocking until the most immediate definite outcome. In your example as soon as the connection is lost you have an outcome (broken pipe, io.EOF, whatever)- so return it. \nThat's the problem; the underlying net.Conn write call won't return for 20-30 mins until it realizes the connection is not reachable any more.\n\nt has deadlines, which are set by context.WithTimeout and context.WithDeadline. If the current API faces challenges to crossing context and net boundaries then I would be happy to help solve those. But I think Go developers are very accustomed to setting context deadlines as well as having some separate (often constant) values for maximum time to wait for network deadlines for background work.\n\nThe only way to set deadlines on net.Conn is through an API call which would mean that we set and reset the deadline for every single write call we make.\nAlso, the whole premise of this issue is about client canceling the context without reading status back from the server. Note this is not an expected usage of gRPC.\nSo if one were to follow the correct usage pattern, the safety check to prevent OOMing remains opaque to the user.\nI agree that the documentation needs to be updated but disagree with updating the design due to incorrect usage resulting from poor documentation.. > The streaming API made the unconventional design decision to bind ctx to the Stream object and not the calls that actually block, like Send. Which means you don't need to set a deadline on every single write call, because the context is only received once. If you multiplex onto a pool of streams you would just have a baseline timeout and retry when deadline was exceeded but ctx was not done.\nSo are you suggesting that the transport keeps a heap of deadlines and every time a new send message call comes in we add it to the heap and update the underlying net.Conn's deadline? I'd really appreciate it if you think over design ideas thoroughly before standing by them  vehemently. By the not buffering any thing means a syscall for every frame that we write!\n\nWhat you are telling me is that this is unexpected usage:\n\nNo, I don't see you canceling any contexts on gRPC stream before CloseAndRecv finishes.\n\nI can't think of a reason why you would do this other than specifically wanting an unbound queue.\n\nIf you're curious, feel free to look up the PRs that made these changes and the associated benchmark numbers. A channel and serveral other data structures were tried and the current implementation proved to be most optimal.\nNow I'm not saying this is the most optimal implementation that there can ever be. This is an active effort, after all. \n\nEssentially it is UNSAFE (if you care about messages getting delivered) to use the streaming API with a context that can be canceled, which.. is most any context in use by any go program I've ever seen. \n\nWow, that's definitely an exaggeration! Do you expect a \"sent\" message to be received by the other side instantly? If gRPC doesn't buffer then, the kernel will and so will all the network buffers along the way. Just don't cancel the context until you have got a status from the server!\n\nIt's like an os.File with Sync removed and now takes a context.Context and starts a goroutine running a f.loopy(ctx) writer!\n\nFunny you should mention that because the underlying net.Conn is very similar to that minus the context of course. You can read about it more here. The point here is that a network is not just an os.File, there are other considerations and performance is a big one. We can't just make syscalls for every message that goes out. \nAmong this we haven't even discussed flow control by the way. Each data frame that goes out needs to be checked against stream level and connection level flow control. The latter will require a global lock to be acquired by every stream for every data frame to go out. That's a very high contention lock!\nMoreover, if you want to wait for every send to actually go on the wire, you have a bunch of stream goroutines that are waiting for each other to write(connection level flow control requires cooperation). Also, don't forget round trip times for window acks that a send might have to wait for. These user goroutines can be instead doing meaningful work!\nI really don't want to justify our design choices further. If you're so inclined, I encourage you to perhaps come up with a prototype and run it against our benchmarks. If it makes things better there's no reason gRPC won't accept it.\n. I can relate to the frustration resulting from complex, sometimes unfortunate, engineering problems. I've been there several times myself.\nI do think that blocking a Send until the message is written on the wire is a reasonable feature to expect. Moreover, this can be accomplished fairly easily in the current implementation. I have strong apprehensions about making it the default behavior however. \nFollowing is how I envision this behavior can be added:\nEach dataFrame can be augmented with another closure onWrite, that signals the sending goroutine when the complete message is written on the wire by loopy. In fact, this was done in one of the iterations but decided against because of the performance impact it had for streams just wanting to send a lot of small messages. This is the reason we don't want it to be our default behavior. \nTo signal the transport  we can add a field to Options that tells it if the Write call needs to be blocking.\nThe API surface can also be augmented to support blocking with something like.\ngo\ntype Blocker interface {\nSendMsgBlocking()\n}\nclientStream struct will implement that method.\nThe generated code, can then, have a method like:\ngo\nfunc (x *routeGuideRecordRouteClient) SendBlocking(m *Point) error {\n x.ClientStream.(Blocker).SendMsgBlocking()\n}\nHere's a running example of such a pattern.\nIf you think gRPC should have this API, I encourage you to open another issue specifically for that.\n\nI wanted to make sure that there is no way in gRPC to send a data frame with no data or header?\n\nThe only way to send an empty data frame is by closeSend which can happen only once for a stream.. @yangliCypressTest Thanks for reporting.\nCan you clarify a few things please:\n1. gRPC version. libprotoc is not gRPC version.\n2. Is the endpoint a gRPC server as well?\n3. Is there a proxy in between.\n4. Can you turn on verbose logging and print the logs here. Set the following two env variables to turn on all logging.\nGRPC_GO_LOG_SEVERITY_LEVEL=info\nGRPC_GO_LOG_VERBOSITY_LEVEL=2. Did you get gRPC via go get or by vendoring a specific version?\nAlso, we'll need more information; a reproduction of the issue would be the most ideal case. \nMeanwhile, you could also share all the logs from the client and the server. \nNote that this is a use case that gRPC supports and has integration tests for it too. If there's a bug, we'll need help zeroing it down.. @jaytaylor thanks for your question. We'll need some more information however; Does your apache web server support HTTP2?\ngRPC currently works over a HTTP2 type transport. We do have provide mechanism for a secure authentication, You can find more information about that here.\nAlso, note if you want to talk to a gRPC server via a browser or a HTTP1.0 client, then you can find more information here: grpc-web. . LGTM\n. on it.. Couldn't reproduce in 100000 runs. Is it possible that the error was seen on an old branch that didn't have the fix in?\n. Reproducible only on head. Pushing a fix.. Trailer shouldn't be read unless an EOF or error is read from stream.\nhttps://github.com/grpc/grpc-go/blob/6a43dcc2acf906fdf92a918cbd6c468e60c3a4fc/transport/transport.go#L303\nBecause close stream updates status and trailers without taking locks:\nhttps://github.com/grpc/grpc-go/blob/6a43dcc2acf906fdf92a918cbd6c468e60c3a4fc/transport/http2_client.go#L643\nCulprit line:\nhttps://github.com/grpc/grpc-go/blob/6a43dcc2acf906fdf92a918cbd6c468e60c3a4fc/stream.go#L438\n. I suppose we could return with errConnClosing instead of break. Setting it to -1 would fire instantly.. transport package depends on this. grpc package depends on transport. Putting this in grpc will create a circular dependency.. We'll have to do the same thing inside the http server handle there, doesn't make it any easier to use that.. 1. In such a case the function Peer(peer peer.Peer) would now need to take a double pointer argument: Peer(peer peer.Peer).\n2. Providing a pointer to the original object doesn't have much use since there's no use case that requires the user to update the object.\n3. All these changes only enable one less copy of the object which seems like over-optimization.. I see your point. Yes, it makes sense to remove this copy from the codepath of the users that don't care about this value.. nit: Update the comments to look like complete sentences; Capital letter in the beginning and a period in the end. . Can we just assign payload to the response object here so that we won't have to do the assignment inside the loop.. can get away with protoMsg := v.(proto.Message). Same nitpick about comment; Should look like a complete sentence.. Could this be a global const? That's save allocation to a const every time the function is called.. Since sizeNeeded isn't used anywhere else, it might make more sense to just remove that variable:\nnewSlice := make([]byte, proto.Size(protoMsg) + protoSizeFieldLength). if err := buffer.Marshal(protoMsg); err != nil {\n    return nil, err\n}. Do we need to SetBuf(nil) here? We anyways do a SetBuf before using it. I believe SetBuf replaces the underlying slice.. Same thing about SetBuf here.. I imagine, if there were an error while Marshaling, we'd still want to put the buffer back in the pool? . nit: Make t *testing.T be the first argument of the function. We don't need a pointer to the protoCodec{} struct, since:\n1. There's nothing in the struct to update.\n2. The methods this struct implements are over the object itself, anyways.. We probably don't need to declare this variable in advance.. Same thing: don't need a pre-declared err variable.. original := &codec_perf.Buffer {\n    Body: expectedBody\n}\nAlso, can we reuse this variable instead of creating variable deserialized? That's just a nitpick again.. Since result is not used anywhere else maybe it's better to use deserialized.GetBody{} inline, here.. const (\n    numGoRoutines = ..\n    ...\n). This is inconsistent with how the previous function initializes protoCodec( via newProtoCodec). However, this looks cleaner. May be, remove the function newProtoCodec() altogether?. This function also could use styling comments mentioned above.. Since we exit the controller function right after, wouldn't the timer be eventually gc'd. Also, since nowhere outside the function is timer.C accessed, it should be safe even if timer.C later gets a value. In fact, maybe we can get rid of the case <- timer.c: altogether? . No all exit paths of this routine have a timer running. Trying to stop and empty timer.C of an already stopped timer leads to a deadlock.\nDoing this in defer would mean making sure that all exit paths of the routine reset the timer(if one is not running already). Future updates to the code might miss that and get into trouble.. Emptying of the awakenKeepalive channel needs to happen inside the lock so that some other thread creating a new stream sees the channel as writable if the number of streams go form 0 to 1.. Duplication here is by choice to keep the structure of the routine simple so that new users can understand the logic behind easily.\nTo reuse the outer select we'll need to have some additional book-keeping about if the ping was sent or not.. The comments need to be updated to explain the new behavior.. golang\nn, err = io.ReadFull(s.sr, p) \nreturn\ninstead of \ngolang\nreturn io.ReadFull(s.sr, p)\nSo that the return values n and err get updated and the defer function has a valid value of err to populate s.readFullErr.. The http2 framer.WriteWindowUpdate(streamID, incr uint32) doesn't support int64. It's better to let the sizes be in uint32.. Ah neat! . nit: Since last is a slice now, we can get away with checking only for len(r.last) > 0 \nhttps://play.golang.org/p/8_3pzaZZWI. In blocking mode, we need a mechanism for lbwatcher to send an error back(if there's one) to DialContext so that the users can be intimated about it.\nAlso, we need to discuss which error should lbwatcher return since there could be multiple errors while connecting to multiple servers. That's actualy a good idea and looked into it turns time.Add works on type time.Time while MaxConnectionAge is type time.Duration (type int64).. Done.. Done.. Do we need this assertion?. Why?. Following offline discussion leaving as is.. \"failed to called\" doesn't seem right. Perhaps, something like Error from InTapHandle ?. Should this be a fatalf? When can such a condition occur?. Again, Fatalf might be too harsh.. Seems like we will loan out window space every single time. There may be times when we don't even need the loan. \nDoes it make sense to have a few if conditions like these:\ngo\nif len(p) - fc.pendingData < fc.limit {\n    // no need to loan\n} else {\n    delta = (len(p) - fc.pendingData) - fc.limit\n    loan(delta)\n}. Again Fatalf seems too harsh.. Perhaps log the error. same thing. Is this needed on both sides? The server will sleep for 2 seconds before reading and sending response back. Could we just call s.ReadFull and that'll wait for server to send all p bytes?. s/reflect/type assert/. same thing here and below. The send quota on both client and server for both stream and connection takes the default value of defaultWindowSize. Therefore, that can be our lower cap. Is there an issue if it's smaller than initialWindowSize?. Leaving as is following offline discussion.. I still think there should be a time.Sleep(time.Second) here since there's really no good way of knowing that settings received were applied.\nCan't check if streamSendQuota on transport was updated since in case of the second test that remains at the default size.\nThis seems flakier than before.\n. You're right, if we introduce call back hooks in the code we can get this done. However, I don't feel very comfortable with introducing test-facilitating code in our codebase, especially for a small test like this. What do you think?. These test here rely on recvMsg's behavior of reading the full message, (which is true when it interacts with the transport stream). However, here the parser is given a \"fake\" buffer instead of the stream. Given that we changed recvMsg to go from io.ReadFull to p.r.Read, the \"fake\" buffer here needs to read the full message just like the transport stream does. . How about I add comments and leave the names as is? I fear long names take us away from Go-styling.. No. s.requestRead is set to an empty function above.\nwindowHandler closure is now part of transportReader and that should be an empty function too for handler_server. This is the reason I earlier set it directly to recvBufferReader (since we didn't need to do any windowUpdates), but forgot about the type assertion I added later for error checking.. s.requestRead is called inside stream.Read()\nhttps://github.com/grpc/grpc-go/blob/master/transport/transport.go#L332. This doesn't trigger the server to send GoAway. Moreover, what's the rationale behind triggering it? It is a connection level concept; closing of a stream with some error shouldn't trigger closing of a connection, right?\nWe liked your original change, any specific reasons for updating it?. If we rely on keepalive related goaways, we don't test the code you wrote s.write(recvMsg{err: err}).\nThe test should be a simple reproduction of your prod issue; a goroutine is blocked on stream.Read() while clientconn.Close() is called, with your change in place this goroutine should unblock.. For this test, we don't really need to set custom window sizes.. The test looks good. I was expecting an end2end test with clientconn.Close() being called(like your prod issue) but this is essentially the same thing. \nCan you make sure that this test fails without your changes in place.. Looks good. Thanks for your time and effort @gyuho . 'at' the same time. same thing.. We don't take a timestamp until the ping is actually written on the wire. However, we do want to calculate all the bytes received from the moment we decided to make a measurement (including the bytes that prompted this new measurement cycle).. Resolved after offline discussion.. Resolved after offline discussion.. Some follow-up work needed for this.. Resolved after offline discussion. Yes, InitialWindowSize settings frame can be sent multiple times during the  life of a connection. In fact, that's why we're seeing this error. C decreases the IWS during it's bdp calculations causing the issue to occur.. Yes, to keep things consistent. When the settings frame is received it adds to the current quota. Since we took something out of it earlier, we must add that back since we're not going to send anything.. Are these debug prints?. Although rare, but it's possible multiple goroutines can make a call to WaitForStateChange. Why is the sleep needed?. What do you think about something like?\ngo\nselect {\ncase <-ht.writes:\n  return nil\ndefault:\n}. How about we relied on something else to cause the RPC to fail, like may be trying to send a message size bigger than the max allowed message size?\n This call option can be used to set a max message size allowed for a call. And the test can then deliberately try and send a message bigger than what you set.. Why is pos needed?. Should we have only one variable traceMode with default value to be false?. Should we change the name of this function since it isn't really reading from an intSlice?. nit: This could have have been an unnamed variable since this is not used in the function and is there only to maintain function signature.\ngo\nreturn func(_ int) {\n...\n}. This is actually needed, since now a SendMsg call can be made again before the previous msg was written on the wire. In such a case if we have a common buffer, we'll run into a race; concurrent access of the underlying buffer by encode and the writer goroutine. . Offline discussion. Follow-up PR will take care of this case.. Offline discussion. Leaving it as is. Let's chat more on it if needed.. The streaming benchmark does sent multiple messages on a stream.\nRun on master:\n```\nStream-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1048576B-respSize_1048576B-Compressor_false\nlatency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1048576B-respSize_1048576B-Compressor_false\n    2116       4727531 ns/op 10686814 B/op      2390 allocs/op\nHistogram (unit: ms)\nCount: 2116  Min:   3.9  Max:   6.1  Avg: 4.73\n\n[      3.892775,       3.892776)     1    0.0%    0.0%\n[      3.892776,       3.892780)     0    0.0%    0.0%\n[      3.892780,       3.892801)     0    0.0%    0.0%\n[      3.892801,       3.892905)     0    0.0%    0.0%\n[      3.892905,       3.893434)     0    0.0%    0.0%\n[      3.893434,       3.896116)     0    0.0%    0.0%\n[      3.896116,       3.909704)     1    0.0%    0.1%\n[      3.909704,       3.978559)     7    0.3%    0.4%\n[      3.978559,       4.327462)   240   11.3%   11.8%  #\n[      4.327462,       6.095440)  1866   88.2%  100.0%  #########\n[      6.095440,            inf)     1    0.0%  100.0%\n```\nOn loopy writer:\n```\nStream-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1048576B-respSize_1048576B-Compressor_false\nlatency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1048576B-respSize_1048576B-Compressor_false\n    2615       3824685 ns/op 10669306 B/op      1798 allocs/op\nHistogram (unit: ms)\nCount: 2615  Min:   3.2  Max:   4.7  Avg: 3.82\n\n[      3.182852,       3.182853)     1    0.0%    0.0%\n[      3.182853,       3.182857)     0    0.0%    0.0%\n[      3.182857,       3.182876)     0    0.0%    0.0%\n[      3.182876,       3.182968)     0    0.0%    0.0%\n[      3.182968,       3.183415)     0    0.0%    0.0%\n[      3.183415,       3.185592)     0    0.0%    0.0%\n[      3.185592,       3.196198)     2    0.1%    0.1%\n[      3.196198,       3.247853)    15    0.6%    0.7%\n[      3.247853,       3.499428)   283   10.8%   11.5%  #\n[      3.499428,       4.724684)  2313   88.5%  100.0%  #########\n[      4.724684,            inf)     1    0.0%  100.0%\n``. Actually, most of these cases in item handler can be shared between the server and the client along with most of Write method's code and I'm guessing there are other such opportunities too scattered throughout the transport layer. I kept it this way to be consistent with the rest of the code and with hopes that one day we'll refactor the transport to take care of all this duplication.. Same argument as above.. Same argument as above.. I thought of that but we'll need to loop through the and see if there are any reserved headers in there or not twice (one to count and one to actually append in the slice). And that holds true for other headers too. So I ended up counting the ones that are easily countable. And since append allocates twice the current size. I'm guessing there won't be too many allocations happening.\nThoughts?. I know we talked about this but now I realize that hEnc needs the same buffer.. perhaps _was_ marked as ?. I was in fact thinking about that last night. The code sort of evolved to be this way and you're right it makes more sense to apply the settings in the settings handler itself. .t.initialWindowSizecontrols our inbound window size whilet.streamSendQuota` controls our outbound window size. So, when we get a settings frame to update our initial window size( the outbound one) we update this variable.\nYou're right about t.maxStreams; updating it never required a lock. I copied the original code and made the same mistake!\n. The same logic, acking after applying restrictive settings and before applying permissive settings, holds for handleInitialSettings. The client doesn't wait for the server to send initial settings before starting streams. \nWe'll still need to check if http2.SettingMaxConcurrentStreams is present or not in the initial settings. So we will only be able to save one conditional check by creating this other function.. Perhaps nanosecond? Or maybe sleep for a microsecond to be sure?. Yeah you're right; end the of the day it's only a cancelled context that we're checking for.. strconv.Itoa(i) ?. Oh! Didn't realize that.. This is needed since we pass a closure to the transport to update these variables upon receiving settings. Now an error can occur while writing and resetTransport might get invoked while these settings might get updated by that closure.\nBy doing this, we can prevent that closure from updating these variables.. After a successful dial we can't call cancel on the context used for it's dialing because of Go 1.6 issue. But Go 1.9 vet catches that and throws an error with a message that this grep captures. . If the state was Shutdown we don't want to put it to TransientFailure. Dial waits until the settings are sent and the channel is closed before they are.. s/resolveing/resolving/. Since target and opts are not used in the method, maybe we should not name those parameters?. Why do we sleep here?. Not in your PR but we can get rid of this time.Stop() now that we're here.. Shouldn't this be deleted too?. Golang tour suggests to use factored import statements. So we should change this statement back.\n. How about:\ngo\nallocLen := int(math.Pow(2, math.Ceil(math.Log2(length))+1)). I don't suppose we need two bools here, one would do.. Make this a little bit bigger just to be safe. Our I/O buffers for benchmark tests are 128K each side but the new changes may result in writing data a little over that limit.. defer will only be called after done channel is closed which might not happen if server.Accept() is blocked so this is to unblock that.. This is to make cleanup easier. Otherwise we need to make sure the server goroutine closes the second connection only after the test is successful which requires another signaling mechanism.. on it.. If the execution gets here, ok will always be true right?. What if we called getSymbols() from Register()? That way we won't need this lock since all subsequent accesses to serviceNames and symbols will be read only.. Could we change this method into a function as well, since it doesn't use any fields from the struct?. What if we called getSymbols() from Register(...)? That way we won't need this lock.. Ah! I see. I hadn't thought about that. Good point.\nYes, sync.Once makes sense.. Just brainstorming here; Do these options really need to be combined? Could we just apply the default first followed by non defaults or something? I say that because allocating and moving memory(albeit small) seems like an overhead in critical path.. You're right. How about we add another method append or addTo which adds vals to existing vals?. Seems like this will be called on the hot path. How about we don't use defer, if that's the case?. Can stickinessMDKey be updated on the fly? Currently, it seems like this method is called only during initialization.. If this can change after initialization, how about using atomic.Value instead of mutex?\nSince stickyStore has a mutex inside of it, we don't need another mutex to protect it.. Explain this a little more?. Can this key not be a string? This key doesn't depend on service config right?\nAlso, How about instead of using a map we use 10 slices each with it's own mutex and convert the string into an int (without using something as expensive as hashing), and mod the int with 10 and use that slice for it?\nOne possible way of string to int could be:\ngo\na := \"some_string\"\nnum := 0\nfor _, b := range []byte(a){\n    num += int(b)\n}. Instead of deleting the check, what if we replaced io.EOF with errStreamDone ?. I think you're right, the check in the first place wasn't serving any purpose. It was fine in both cases; success err == nil or failure err == io.EOF. The only purpose that it was serving then, was to make sure that write doesn't return any other error except io.EOF. Deleting this should fine then.. Actually, it was a mistake to print the message bytes in the first palce. The output is pages and pages of 0s. Let's only print the length of the expected and actual messages.. To avoid this condition so that user-agent can be added to metadata exposed to service handler.\nAlternatively, we can add a case specifically for user-agent in the enclosing select. \nOr maybe even rename this function to be isWhitelistedinstead.\nThoughts?. Yes the steps remain the same. The idea is to eliminate a race; previously we'd allow the client to read goaway after it sends something on the wire(not necessarily headers). Now, we allow it to read goaway only after the server has received the headers. . Updated the return statements but can't using := for vars since there's a logging call in defer that needs err.. Update documentation for this function?. When can this be zero? . This seems fine.. What does this lock protect?. if connectionDeadline is always non-zero we can get rid of timer != nil checks.. Instead of this closure, how about transport's reader goroutine wait on connectionDeadline as well while it waits on reading settings frame?. Should we also update ac.transport = nil. resetTransport() always resets connectionDeadline, but if haven't exhausted our list of backends we'd like to use the same deadline.. I'd rather not replace loop with recursion. . onDeadline() closure will do this for us now, so this can be removed.. All input parameters that are arrays, need an index to go over them while keeping other parameters' index unchanged. This way we can generate all combinations without using nested for loops. rs is an array. rs=(1 2 3). cs=$(param \"arg1\" \"arg2\") doesn't call param(). Maybe, I'm doing something wrong.. inc is called from an infinite loop. exit 0 makes sure that we stop the script when we are done with all combinations.. Add TODO to truncate msg.. This i is not same as the i above. := assigns a new local variable. Same thing i is local to the for loop above.. linkedMap itself has a clear method. Should we just call that instead? . Nit: proto imports should end in \"pb\".\n. Which PR are you referring to?. Yeah, it is unused. Not sure if you're suggesting that I add it or merely asking?. done.. done.. done.. How about if we just take address of s, since it's already a copy?. Same thing here.. if e.httpHandler {. Should we put all connection-level defaults together? They all live here right now https://github.com/grpc/grpc-go/blob/master/transport/flowcontrol.go#L50 but I don't think some of them belong in a file called flowcontrol.go. Should we update createHeaderFields to record the total size so that we don't have to loop over all fields again?. If execution gets here, hdrListSizeErr will always be nil, right?. decodeResponseHeader is supposed to be used on the client side. Perhaps, rename the function to something more appropriate since it's being used on the server as well.. Alternatively, add the if frame.Truncated check right at the beginning of this function itself.\n. Is it better to return an error or merely log it and still try and parse the header that we got?. Should we also do this check while writing trailer?. Same thing; perhaps the headerFrame should have a field to record size.. Only relevant to the server side: Should Framer.MaxHeaderListSize be set only after fist settings' ack is received?\nThe other side might start with an unlimited MaxHeaderListSize(https://tools.ietf.org/html/rfc7540#section-6.5.2) and start sending header frames before our settings frame is received by it.. Done.. I was actually thinking about it this morning. A documentation like this seems confusing given that a stream ends only with receiving either an EOF or an Error. From a user's point of view when or if a message is delivered should be opaque.\nPerhaps the documentation that we need is a stream's life cycle (if it doesn't already exist.);\n1. Client creates a stream(headers are sent to the server)\n2. Client sends data(potentially more than one message)\n3. Server sends headers.\n4. Server sends data(potentially more than one message)\n5. Client sends end of stream.\n6. Server sends trailers(status).\n7. Client receives EOF/error.\n. controlBuf is a data structure used to pass control frames to loopy by reader goroutine, stream goroutines and keepalive goroutine. \nactiveStreams is loopy internal data structure to keep track of streams wanting to send data.. Done.. executeAndPut has this check because put calls it with a nil argument. . loopy writer runs in a separate goroutine so other goroutines(read, streams, keepalive) need some way to tell it about:\n1. Headers to send.\n1. Data to send.\n1. Incoming window updates\n1. Incoming settings\n1. Incoming goaways\n1. Locally generated stream cleanup requests.\nWe use a data structure called controlBuf, which is made up of a lock and a list, to give this information to loopy. This information is passed as specific structs, we call them control frames, since they pass control information to loopy.\nFollowing are the structs: https://github.com/grpc/grpc-go/blob/f1ab7acf3f0b8e779bc3592e7e9728cd2fd20402/transport/controlbuf.go#L87\nLoopy can write window updates, acks, settings etc. in any order but data and headers of a stream need to be ordered, pass flow control and broken into data frames of max size 16KB(for now). To accomplish this loopy uses another data structure activeStreams(this is a list).. > I don't follow, could you elaborate? That detail is what this documentation attempts to address.\nWhat I mean is perhaps changing the documentation from \"it blocks until the message is sent\" to \"blocks until a message is scheduled\" makes more sense, rather than warning users of what unexpected behavior may occur in the current implementation if they don't use the API right.\nWe bind a stream's context to all operations happening on that stream, if the context is canceled then no further operations can happen on it. Further, for a client stream to only send messages and not read a status is incorrect usage. I see the motivation to inform users that a send call doesn't block until the whole message is written to the wire, but more importantly they shouldn't be cancelling streams' context without reading an EOF or error.\nDoes that make sense?. I added documentation to controlBuffer, other than that the suggested documentation is already there (spread across run, processData and activeStream declaration).\nThe part about describing loopy's scheduling as round-robin doesn't seem necessary. \n1. We don't want to make a guarantee that it will stay this way. \n1. I don't see a user doing anything differently being equipped with this information.\nWhat do you think?. Done. PTAL. Done.. Yes. Done.\n. done.\n. ",
    "spikebike": "@jcramb that works, although I was after the public key, so I did:\npeer, ok := peer.FromContext(ctx)\nif ok {\n      tlsInfo := peer.AuthInfo.(credentials.TLSInfo)\n      v := tlsInfo.State.PeerCertificates\n      for _, v := range tlsInfo.State.PeerCertificates {\n         fmt.Println(\"Client: Server public key is:\")\n         fmt.Println(x509.MarshalPKIXPublicKey(v.PublicKey))\n      }\n}\nUnfortunately I've not figured out how to access the server public key from the client.. Is it possible to allow mutual authentication based on the peers public key and not depending on a CA?  I've managed to read the client public key from the server, but not the opposite.. Right, that's in my example, \"peer, ok := peer.FromContext(ctx)\", but the problem is that the call fails when ctx is context.Background.  So how can get access to the peer from the ctx?  \nCan you explain a bit more exactly what you mean by \"the peer object is accessible via CallOptions\"?. Ah, thanks.  I do want the client to check the server cert, and the server to check the client cert.  Much like ssh.  I think the overriding the handshake methods is best.  Thanks, that's a very helpful suggestion.. I'm going to try to modify the grpc/examples/route_guide example to override the handshake like mentioned at https://github.com/grpc/grpc-go/issues/111#issuecomment-275795933\nSeems like that would allow me to do what I want.  . My goal is to associate some metadata with the peer on the other end of the connection.  By tying it to the public key you can prevent peers from impersonating other peers (at least without the private key).  Much like ssh.\nYou mentioned \"If you want to do some sort of check on server's creds on the client-side, right after the handshake, then, you could wrap around TLS creds in your own struct and override the client and server handshake methods\"\nI have this working and it looks good.  Especially the documentation of WithTransportCredentials saying \"WithTransportCredentials returns a DialOption which configures a connection level security credentials (e.g.,  TLS/SSL).\"  That's exactly what I want, connection level security with TLS/SSL.\nWith that working the question becomes where to store the metadata related to the peer on the otherwise of the connection.  On the client side, ClientHandshake is passed context.Context.  https://blog.golang.org/context was very helpful, and seems designed to do exactly what I need.\nProblem is the  ServerHandshake(net.Conn) (net.Conn, AuthInfo, error) is not passed context.Context.  \nAny suggestions on how to get some information from the per connection ServerHandshake to the gRPC calls running in the server on behalf of the client?\n. Where would you set it, during the handshake?  Or in the Interceptor?  Where would you check it?  Going from context -> peer is easy, but I don't always have the context.. ",
    "rakyll": "I don't like Credentials because it's already in the credentials package. You may like to move the interface definition to the grpc package and provide the implementations in the credentials package.\nAuthToken or simply Token sounds a better alternative. Credentials name already hints that this is about auth.\n. Security credentials traditionally has a very specific meaning, credentials used to access security related systems. I would prefer auth.\n. I am not sure what's wrong with the user code either. Filed a bug against the compiler at https://github.com/golang/go/issues/16741.\n. Not a regression against 1.7. 1.8 devel breaks it, we can close the issue here.\n. I re-ran rsc's grpcbench against the master. You can ignore the HTTP/2 results (due to the reason explained at https://github.com/grpc/grpc-go/issues/1043#issuecomment-272052075), and compare the GRPC results with the previous. I think the new results confirm the improvement. \n```\nDuration    Latency Proto\n5.330994ms  0s  GRPC\n4.009134ms  0s  GRPC\n3.763ms         0s  GRPC\n3.527573ms  0s  GRPC\n3.38252ms   0s  GRPC\n12.472857ms 0s  HTTP/2.0\n5.688769ms  0s  HTTP/2.0\n6.409501ms  0s  HTTP/2.0\n108.958873ms    0s  HTTP/2.0\n6.5151ms    0s  HTTP/2.0\n10.245881ms 0s  HTTP/1.1\n3.852141ms  0s  HTTP/1.1\n3.36458ms   0s  HTTP/1.1\n3.510561ms  0s  HTTP/1.1\n144.650239ms    0s  HTTP/1.1\n14.371479ms 1ms GRPC\n12.052484ms 1ms GRPC\n13.174659ms 1ms GRPC\n14.036349ms 1ms GRPC\n11.440052ms 1ms GRPC\n64.310362ms 1ms HTTP/2.0\n55.559025ms 1ms HTTP/2.0\n53.085947ms 1ms HTTP/2.0\n55.087295ms 1ms HTTP/2.0\n55.589108ms 1ms HTTP/2.0\n17.123458ms 1ms HTTP/1.1\n5.117912ms  1ms HTTP/1.1\n4.775117ms  1ms HTTP/1.1\n5.077654ms  1ms HTTP/1.1\n4.713239ms  1ms HTTP/1.1\n24.798243ms 2ms GRPC\n21.212411ms 2ms GRPC\n24.497939ms 2ms GRPC\n23.300286ms 2ms GRPC\n23.151755ms 2ms GRPC\n109.657809ms    2ms HTTP/2.0\n95.343032ms 2ms HTTP/2.0\n90.149519ms 2ms HTTP/2.0\n97.416299ms 2ms HTTP/2.0\n97.81356ms  2ms HTTP/2.0\n22.973429ms 2ms HTTP/1.1\n5.949754ms  2ms HTTP/1.1\n6.118488ms  2ms HTTP/1.1\n6.083907ms  2ms HTTP/1.1\n6.043363ms  2ms HTTP/1.1\n43.104757ms 4ms GRPC\n38.983007ms 4ms GRPC\n43.200068ms 4ms GRPC\n40.585982ms 4ms GRPC\n42.068339ms 4ms GRPC\n195.434971ms    4ms HTTP/2.0\n177.185796ms    4ms HTTP/2.0\n174.232438ms    4ms HTTP/2.0\n174.821399ms    4ms HTTP/2.0\n178.38124ms 4ms HTTP/2.0\n33.23372ms  4ms HTTP/1.1\n8.903388ms  4ms HTTP/1.1\n111.990624ms    4ms HTTP/1.1\n8.636087ms  4ms HTTP/1.1\n9.185476ms  4ms HTTP/1.1\n77.66905ms  8ms GRPC\n80.779861ms 8ms GRPC\n79.943702ms 8ms GRPC\n77.576871ms 8ms GRPC\n75.829066ms 8ms GRPC\n358.309164ms    8ms HTTP/2.0\n330.676264ms    8ms HTTP/2.0\n335.010852ms    8ms HTTP/2.0\n326.09494ms 8ms HTTP/2.0\n328.849144ms    8ms HTTP/2.0\n52.560471ms 8ms HTTP/1.1\n13.88925ms  8ms HTTP/1.1\n114.867623ms    8ms HTTP/1.1\n13.123327ms 8ms HTTP/1.1\n14.003239ms 8ms HTTP/1.1\n158.107222ms    16ms    GRPC\n154.017713ms    16ms    GRPC\n155.516585ms    16ms    GRPC\n153.444526ms    16ms    GRPC\n150.978996ms    16ms    GRPC\n702.038344ms    16ms    HTTP/2.0\n642.113313ms    16ms    HTTP/2.0\n635.178695ms    16ms    HTTP/2.0\n640.926487ms    16ms    HTTP/2.0\n636.483788ms    16ms    HTTP/2.0\n86.444409ms 16ms    HTTP/1.1\n22.05304ms  16ms    HTTP/1.1\n19.950935ms 16ms    HTTP/1.1\n125.511693ms    16ms    HTTP/1.1\n23.579457ms 16ms    HTTP/1.1\n345.991851ms    32ms    GRPC\n286.761732ms    32ms    GRPC\n290.791148ms    32ms    GRPC\n282.957969ms    32ms    GRPC\n280.404998ms    32ms    GRPC\n1.30835792s 32ms    HTTP/2.0\n1.183078984s    32ms    HTTP/2.0\n1.196037184s    32ms    HTTP/2.0\n1.187915833s    32ms    HTTP/2.0\n1.189489888s    32ms    HTTP/2.0\n150.990974ms    32ms    HTTP/1.1\n42.193952ms 32ms    HTTP/1.1\n38.220967ms 32ms    HTTP/1.1\n36.277676ms 32ms    HTTP/1.1\n142.413035ms    32ms    HTTP/1.1\n``. Is it sufficient to add some high-level godoc examples tocredentialsandcredentials/oauth`? If so, I can contribute the necessary reference examples.. OpenCensus require 1.8 right now. We can relax this requirement if it is necessary to support OpenCensus gRPC integrations for 1.6.. Happy to spend time to support 1.6 at OpenCensus if gRPC 1.6 support will not be discontinued in the near future.. gRPC doesn't depend on OpenCensus directly. You can plug it in optionally. I am not going to execute on https://github.com/census-instrumentation/opencensus-go/issues/61 for now given that OpenCensus Go libraries are still at the bootstrapping stage and gRPC may discontinue 1.6 support until we achieve maturity.\n/cc @bogdandrutu . > I am not convinced Retriever is a better name to go\nIt retrieves credentials, it explains the very basic single action the implementers of this interface does. If you're not convinced Retriever is a better name, please suggest one. Because Credentials is no way a good name for an interface in Go in this context.\n. ",
    "okdave": "At the moment, I'm looking at unifying credentials for all Go APIs, in particular those under google.golang.org/cloud. I'm trying to design the interfaces in such a way that it's useful for both those APIs as well as grpc. When that's ready and stable, it should be able to replace the credential package here altogether.\n. Are there any \"real world\" uses of gRPC which do not currently rely on oAuth? Removing the dependency seems to have very little practical implications.\n. This is somewhat related to #76.\nThat issue was resolved by adding the timeout option, but it didn't pursue the suggestion that dialling should fail immediately if the failure mode is permanent.\n. ",
    "jellevandenhooff": "Ah -- I guess the messages themselves might by only a symptom of the underlying problem. I don't know if instantaneous reconnecting is desired behavior, but if it is not, then adding some back-off would probably also make me happy :)\nThanks!\n. How do you feel about moving the exponential back-off into clientConn so it also backs-off if a server is crashing or misbehaving? I could try and see what it looks like if you like that idea.\n. While I appreciate #922, the many log messages are a symptom of many failed connection attempts. I think it'd be worthwhile also reducing connection frequency, because even if we're not spewing logs, we're still hammering some poor unsuspecting service.\n. The root cause of the error is exactly as you described, and a backoff as you propose is the solution. My PR from last year sketched out an approach for implementing that, but with all the changes to grpc, doesn't quite merge anymore.\n. closing in favor of #954 . I wish the Transport interface let implementors take care of service discovery and connection management. I suspect that both an in-memory transport and an HTTP/1.1 transport would want to do their own connection maintenance and service discovery and replace the default grpc.ClientConn implementation.\nOne way to do that would be to add a target string to the NewStream method. That would make the Transport interface much closer to http.RoundTripper (and make it easy to swap in eg. a http2.Transport-based implementation for eg #2112.) That would also make it possible to move the default connection maintenance and service discovery to a Transport out of the ClientConn, making the package quite a bit more flexible.\nI wonder if what this issue is really asking for is for grpc.ClientConn to be an interface as in #1287; the Transport.NewStream method above looks super similar to ClientConn.NewStream.. This issue is the same as in #120, but @cdesousa's here description is much better.\n. @iamqizhao, what do you think of adding logic to the transport monitor to back-off before attempting to reconnect again if the first read in the reader goroutine fails?. @menghanl any updates on the zero-delay reconnect logic?. Hi @menghanl, I don't think #1340 fixes this issue. Though it helps with the log messages, the file descriptors in @gm42's comment still get exhausted.. ",
    "mauaht": "Got the same high volume of error messages under the following scenario:\n- Set up generic TCP listener which accepts and closes connections\n- Send a request to server using the grpc hello world tutorial\nIt looks like there is an infinite loop in the Invoke function in /grpc/call.go as follows:\n- Invoke creates callInfo with callInfo.failFast=false at line 109 \n- If sendRequest() in line 169 fails with transport.ConnectionError a subsequent continue jumps to loop start at line 143\n- If recvResponse() in line 181 fails with transport.ConnectionError a subsequent continue jumps to loop start at line 143\n- Loop does not exit because callInfo.failFast=false blocks error handling at line 150  \nSetting callInfo.failFast=true aborts the loop at line 143 and prevents the infinite loop.\nIn this case where the server explicitely closes the connection, because it does not recognise the grpc protocol, the client should not attempt retry. \nThere may be other transport.ConnectionError cases for which limited retries are appropriate, but unbounded retries are not a good idea.\nThe following test code reproduces the above scenario\n```\nimport (\n    \"fmt\"\n    \"testing\"\n    \"net\"\n    \"golang.org/x/net/context\"\n    \"google.golang.org/grpc\"\n    demo \"google.golang.org/grpc/examples/helloworld/helloworld\"\n)\nconst hw_addr = \"127.0.0.0:50000\" \nfunc TestGrpcReject(t *testing.T) {\n    // Create a simple server that just closes connections\n    lis, err := net.Listen(\"tcp\", hw_port)\n    if err != nil { fmt.Printf(\"Listener not started: %v\\n\", err); return }\ngo func() {\n    for {\n        conn, _ := lis.Accept() \n        fmt.Printf(\"Server closing connection\\n\")\n        conn.Close()\n    }\n}()\n\n// Set up client and send request to server\nconn, err := grpc.Dial(hw_address, grpc.WithInsecure())\nclient := demo.NewGreeterClient(conn)\nclient.SayHello(context.Background(), &demo.HelloRequest{Name: \"World\"})\n\n}\n```\n. ",
    "artushin": "I also felt this was extremely annoying in a dev environment (I have multiple services connecting to a non-critical grpc service and they would cumulatively spit out over 10k log lines in 5 seconds if I turned that service off), so I just replace the grpclog Logger with grpclog.SetLogger(log.New(ioutil.Discard, \"\", 0)) when in dev.\n. ",
    "gm42": "I am experiencing same issue and using grpclog.SetLogger. However, as I see it the problem is that there is no context about the error: if I am using different gRPC services I will never know by reading this log message which one is specifically the failing service.\nWould it be possible to add to this log message more context e.g. the hostname or service name?\n. I signed it\n. @iamqizhao my test with:\nconn, err := grpc.Dial(\"localhost:81\", grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(time.Second * 5))\n        if err != nil {\n            log.WithError(err).Fatalln(\"failed to create connection to myservice\")\n        }\ngives the expected:\nFATA[0000] failed to create connection to myservice          app=myapp error=grpc: timeout options requires block option\nIf I use instead:\nconn, err := grpc.Dial(\"localhost:81\", grpc.WithInsecure(), grpc.WithBlock(), grpc.WithTimeout(time.Second * 5))\n        if err != nil {\n            log.WithError(err).Fatalln(\"failed to create connection to myservice\")\n        }\nThen it works without any error.\nEdit: I assume you would like a test added for it? Will provide.\n. @jellevandenhooff in #120 the main topic is how annoying the message are, however here I instead see that:\n- grpc-go has \"smart\" functionality to handle reconnection with goroutines\n- such functionality does not cover the case of a (unarguably misbehaving) server accepting and immediately closing connections\nSince grpc-go wants to automate reconnection, I think it should cover also the case of this type of misbehaving servers to prevent self-DoS scenarios. It should be possible to cover not only the dialing stage but the reconnection stage (think about the IRC \"reconnecting too fast\" error message).\n. I have dug a bit into this, it turns out that the tight-looping (DoS-ing) behaviour is to be found in the transportMonitor() goroutine, which does not take in account an EOF-serving service.\nSteps to reproduce the problem:\n1. start a gRPC example server on port 8081\n2. connect the client\n3. stop the server (e.g. Ctrl+C)\n4. start a dummy EOF-server in place of it: socat -v TCP-LISTEN:8081,fork SYSTEM:'echo hi'\nThe client will generate an avalanche of reconnections, quickly exhausting all file descriptors.\n. @thelinuxfoundation check again please. Nevermind, I realise there is no such log level (debug). ",
    "schmohlio": "is there any update on this? This problem really floods logs when running on GCE. setting the logger seems to provide an initial fix, but feels wrong\n. ",
    "raff": "Actually I did realize that I would need this also after the connection has\nbeen established. The problem of just sending requests when the connection\nis down is that I do have a ton of those and some of those are long\nrunning. I have no problem returning an error or losing requests (this is\nfor a sharded pubsub) but I can't keep clients pending if one of the nodes\nis not responding.\nI am going to do some experiments with timeouts and see where I get.\nOn Tue, Mar 17, 2015 at 6:45 PM, David Symonds notifications@github.com\nwrote:\n\nI don't think we should be adding APIs that are fundamentally racy. And a\nnetwork connection can fail at any time, so it's not just a startup problem.\nIf an RPC service needs a way to probe the liveness of a connection, it\ncan easily add a Ping method or similar.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/121#issuecomment-82676062.\n. So, what about having the option to define a callback triggered when the connection status changes ?\n. \n",
    "awalterschulze": "What do you plan to do?\nWith github.com/gogo/protobuf I generate a Size, Marshal and Marshalto method.\nThe Marshal method calls Size and then allocates a byte slice of that size.\nThen it pass that to MarshalTo which fills in the byte slice.\nThis way there is only one alloc per message.\nGiven a big enough pre allocated buffer it would be easy to reuse the buffer with Marshalto.\nAnd Marshalto could even be changed to a MarshalZeroTo which does the same job as Marshalto, but with less copying.\nMy experience is that golang/protobuf does not really want to generate code, but wants to do almost everything with reflect and unsafe pointer.  This is definitely slower, but I understand that it is less code to maintain.\nSo what I want to know is what you plan to do here and if you golang/protobuf is still not generating code (things might have changed), how can I still get the maximum speed out by leveraging gogoprotobuf's generated code?\nIf you just call Marshal then atleast I just do one alloc.\nBut there is so much speed to gain here, since writing the buffer does a copy anyway thus it is really easy to reuse a big buffer.\nFor example\nhttps://github.com/gogo/protobuf/blob/master/io/varint.go\nI could make this even faster by not having any copying instructions in MarshalTo\ngrpc looks great and I would really love to use it, but speed is very important.\nI would like to be prepared for any changes I can make to gogoprotobuf to still gain the maximum amount of speed.\nI really don't want to make another fork.\nSo maybe there is someway we could work together?\nI have grpc code generation merged into gogoprotobuf on the proto3 branch if you are at all interested.\n. That is great news about being IDL and codec independent.\nI could also use that in totally different project I am working on :)\nCould you tell me when you are ready so I can test your change?\nThank you very much.\n. awesome :)\nOn 19 March 2015 at 19:54, Qi Zhao notifications@github.com wrote:\n\nOn Thu, Mar 19, 2015 at 1:31 AM, Walter Schulze notifications@github.com\nwrote:\n\nThat is great news about being IDL and codec independent.\nI could also use that in totally different project I am working on :)\nCould you tell me when you are ready so I can test your change?\nIt should be within next week.\nThank you very much.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-83416600.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-83693718.\n. This looks great :)  I can't wait to start playing this.\nThank you very much.\n. That is fine my fork generates code with the correct import :)\nBut thank you for the heads up.\n\nOn 2 April 2015 at 21:36, Qi Zhao notifications@github.com wrote:\n\nI am not clear how you will proceed. Just be aware of that the grpc server\ndoes unmarshaling inside the generated code (e.g.,\nhttps://github.com/grpc/grpc-go/blob/master/test/grpc_testing/test.pb.go#L544\n)\nfor unary rpc. You probably need to treat it properly depending on your\napproach.\nOn Thu, Apr 2, 2015 at 1:21 AM, Walter Schulze notifications@github.com\nwrote:\n\nThis looks great :) I can't wait to start playing this.\nThank you very much.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-88820770.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-89020023.\n. For when you get back from holiday\n\nI have finally created a repo for the grpc benchmarks and some simple tests.\nhttps://github.com/gogo/grpctests\nI don't think I am doing it exactly right I get quite a few log messages,\nfor example:\n\"failed to write status: connection error: desc = \"transport: use of closed\nnetwork connection\"\"\n\"http2Client.notifyError got notified that the client transport was broken\nEOF\"\nThese are the important files, could you please take a look:\nhttps://github.com/gogo/grpctests/blob/master/bench/bench_test.go\nhttps://github.com/gogo/grpctests/blob/master/simple/grpc_test.go\nThank you again and enjoy your holiday\nOn 7 April 2015 at 11:57, Walter Schulze awalterschulze@gmail.com wrote:\n\nThat is fine my fork generates code with the correct import :)\nBut thank you for the heads up.\nOn 2 April 2015 at 21:36, Qi Zhao notifications@github.com wrote:\n\nI am not clear how you will proceed. Just be aware of that the grpc server\ndoes unmarshaling inside the generated code (e.g.,\nhttps://github.com/grpc/grpc-go/blob/master/test/grpc_testing/test.pb.go#L544\n)\nfor unary rpc. You probably need to treat it properly depending on your\napproach.\nOn Thu, Apr 2, 2015 at 1:21 AM, Walter Schulze notifications@github.com\nwrote:\n\nThis looks great :) I can't wait to start playing this.\nThank you very much.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-88820770.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/124#issuecomment-89020023.\n. I ran go get -u google.golang.org/grpc again\nand then ran my tests and benchmarks again.\nI still see the same problems.\nI would really appreciate it if you could take a look.\n. I was wondering if you have had any time to check those benchmarks out?\n. join the club :)\n. get well soon\n. Looks great.  Am I right in my interpretation that you can now create your own buffer optimisations inside a codec since the codec object is going to be reused for every encode?\n. Will do :)\n. Looks good :)\nNow I just need to be able to inject my own codec somewhere, or am I missing something?\n. No problem :)\nI was just checking and I am happy to hear that everything is still moving\nforward.\n\n\nOn 30 March 2015 at 09:43, Qi Zhao notifications@github.com wrote:\n\nHi Walter,\ndsymonds and I reached consensus on the design. But for now only half of\nthe change is in -- I still hard coded protobuf as the codec for\neverything. Please hold a little while for the other half, which should be\ndone in the next a couple of days. Sorry about the delay.\nOn Sun, Mar 29, 2015 at 11:56 PM, Walter Schulze <notifications@github.com\n\nwrote:\nLooks good :)\nNow I just need to be able to inject my own codec somewhere, or am I\nmissing something?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-87571958.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/126#issuecomment-87581057.\n. Networks without internet access running on generators are a thing (I have not worked with oauth, but I assume it requires access to the internet). (potential idiot right here)\nBut either way grpc-go has a lot of dependencies.\nI don't know if it has been fixed, but I think something even depends on google.golang.org/cloud.\nI want to use grpc and go without using a google api.\nI am not complaining I think grpc-go gives a lot, but I still think there could be fewer dependencies.\nI think its dependency on github.com/bradfitz/http2 is probably temporary.\nIt just adds up.\nMaybe with go having such a great standard library we have just become quite allergic to dependencies.\nAnyway I am just saying tamird is not alone, but I love grpc-go, I just use git-anchor to manage all of its dependencies.\n. Seems grpc still has some oauth2 dependencies\n\nsrc//google.golang.org/grpc/credentials/oauth/oauth.go: \"golang.org/x/oauth2\"\nsrc//google.golang.org/grpc/credentials/oauth/oauth.go: \"golang.org/x/oauth2/google\"\nsrc//google.golang.org/grpc/credentials/oauth/oauth.go: \"golang.org/x/oauth2/jwt\"\nsrc//google.golang.org/grpc/interop/client/client.go:   \"golang.org/x/oauth2\"\nsrc//google.golang.org/grpc/interop/client/client.go:   \"golang.org/x/oauth2/google\"\nThis results in depending on google.golang.org/cloud which results in depending on google.golang.org/api which is huge.\nI have seen there has been some progress on removing the dependency on oauth.\nAre there still plans on completely removing it?\n. Maybe I could just delete the folders (credentials/oauth and interop/client) from my git subtree?\n. Ok ... so explain an optional dependency please. :)\nAre you telling me that:\n1) if I go get and run go test -v ... everything will work?\n2) I shouldn't run my dependencies' tests?\n3) ?\n. Please don't think I don't appreciate the work you have do to move this dependency as far into a corner as possible.\nWhat do you do to manage this dependency?\n. Well I run them and try to build everything I'll get a compile error asking for google's cloud and api libraries.\nOk so this sucks, but I'll be pragmatic and delete those folders (credentials/oauth and interop/client) in my git subtree and then I can delete those google cloud and api dependencies.\nThank you for all the work of moving these dependencies to somewhere, where I can delete the code that uses it.\n. Thats really cool :)\nI'll have to look into that for letmegrpc.\n. Seems you have installed gogoprotobuf and not golang/protobuf you can simply type --gogo_out instead of --go_out that should also fix your problem\n. I am thinking it happens during a parallel build, where the two are trying to be built at the same time.\n. I signed it!\n. I don't see how coverage could have dropped?\nI also don't see where the server and client binaries were ever called.\n. I can revert the interop dir, but then those will probably conflict with some other binary called client or server in another project in future.\nThis is a problem with generic binary names.\nAre you sure you want me to revert the interop dir?\n. I guess the likelihood of a parallel build building a name conflicting binary at the same time goes down as it crosses package boundaries, since there is still some kind of order.\nBut if you do not have unique binary names these files will always overwrite themselves as part of a go install ./..., since the binaries are all installed into the same location.\nAnother conflict can happen even if the other binary is not a go binary.  For example if you name one of your binaries \"grep\" and it is go installed and your go bin folder is in your $PATH then the OS is going to have choose which grep you are referring to when you try to call grep from the command line.  This is actually a case study and has caused a small bit of frustration.\nSo I am still happy to revert the interp folder, since it should still solve my problem with parallel builds, but I just want you to reconsider given all the information. \n. Uhm I don't think its stale, we are still waiting on your reply?. I would prefer that the codec object itself is in a sync pool and is\nassumed to be unsafe to use concurrently. This way the codec itself can\noptimize its buffer use. This codec that I would like to use with grpc is\nthe one that I have implemented in github.com/gogo/protobuf/codec\nOn Sat, 2 Dec 2017, 01:18 Muir Manders, notifications@github.com wrote:\n\nIs someone actively working on this? I've been playing around with\ndifferent buffer pool strategies, but don't want to look too much more if\nI'm just duplicating work.\nI've been trying bucketed []byte buffer pool where there are tiers of\nbuffer sizes. For example, if you ask for a 700 byte buffer, it might pull\na buffer from the 1024 byte pool, slice it down to 700 bytes and return it.\nI'm trying a sync.Pool implementation (one sync.Pool per buffer size\nbucket) and a leaky buffer implementation (one buffered chan []byte per\nbuffer size bucket). They certainly help, but neither one has plucked all\nthe low-hanging garbage fruit on the first try.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1455#issuecomment-348649013, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABvsLVtUmZ3k_U6PNTHhMQgt9P0qZ57qks5s8JddgaJpZM4O_JA3\n.\n. Here is the correct link\nhttps://github.com/gogo/protobuf/blob/master/codec/codec.go\n\nOn Sat, 2 Dec 2017, 10:42 Walter Schulze, awalterschulze@gmail.com wrote:\n\nI would prefer that the codec object itself is in a sync pool and is\nassumed to be unsafe to use concurrently. This way the codec itself can\noptimize its buffer use. This codec that I would like to use with grpc is\nthe one that I have implemented in github.com/gogo/protobuf/codec\nOn Sat, 2 Dec 2017, 01:18 Muir Manders, notifications@github.com wrote:\n\nIs someone actively working on this? I've been playing around with\ndifferent buffer pool strategies, but don't want to look too much more if\nI'm just duplicating work.\nI've been trying bucketed []byte buffer pool where there are tiers of\nbuffer sizes. For example, if you ask for a 700 byte buffer, it might pull\na buffer from the 1024 byte pool, slice it down to 700 bytes and return it.\nI'm trying a sync.Pool implementation (one sync.Pool per buffer size\nbucket) and a leaky buffer implementation (one buffered chan []byte per\nbuffer size bucket). They certainly help, but neither one has plucked all\nthe low-hanging garbage fruit on the first try.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1455#issuecomment-348649013, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABvsLVtUmZ3k_U6PNTHhMQgt9P0qZ57qks5s8JddgaJpZM4O_JA3\n.\n\n\n. Thats interesting.  I didn't think that the http2 transport could also benefit from buffer pooling.  At the previous company I worked, we had a similar buffer pooling scheme for a different purpose.  The API asks for a buffer size and returns that size of byte buffer for you.  It does this by looking through its slice of buffers for a big enough buffer and then returning the resized to your smaller size.  When buffers are returned they are stretched back to their original capacity. \nThis buffer pool also had retention policies, which were a bit more complex, but implemented orthogonally with the buffer pool.. What are other languages doing to optimize this situation? Maybe we can\nlearn something from c++ or even Java.\n\nOn Sat, 2 Dec 2017, 23:26 Muir Manders, notifications@github.com wrote:\n\nMy thought was instead of returning the buffer to the codec, return the\nbuffer to the generic buffer pool instead. Then anyone can pull a buffer\nfrom the pool, and consumers of the buffers can put them back in the pool\nwhen they are done. It is safe to never return a buffer, or even to return\na buffer that you did not get from the pool originally. On the other hand\nit is very unsafe to return a buffer and then still use it, or a slice of\nit. We would need some approach to make ourselves confident that\nuse-after-release doesn't happen.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1455#issuecomment-348724659, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABvsLa5exKkGKa5OEtS5Rb-3p0UBGuf_ks5s8c6GgaJpZM4O_JA3\n.\n. Fair enough.  It would be great to compare sync.Pool with classic buffer reuse.. Totally, gogo/protobuf/proto would like to avoid importing golang/protobuf/proto, because currently gogo/protobuf has zero dependencies and would like to keep it that way.\nAlso some users, including (past) me would prefer to have one implementation of the proto library as a dependency.  This also becomes really hard with grpc-go.\n\nA few years ago, when I was a user, I used to take a git subtree of grpc-go and change all the import paths from github.com/golang/protobuf/proto to github.com/gogo/protobuf/proto.\nThat way goimports can automatically resolve to the correct import.\nSo maybe I would ask, is it possible to create an interface that avoids importing any serialization library, including flatbuffers?\nFor two reasons:\n1. The reason above\n2. golang interfaces don't really nest well, which means that you must import interface A if you want to implement interface B which has a method that uses interface A, which goes against the whole implicit interface goal in my opinion, but thats a separate issue. . Cool :)\nMay I ask what is the use case for EnsureNativeMessage?. ",
    "harlow": "\nThere will probably be a mechanism for attaching bits of data to an RPC via a context (e.g. trace IDs), but it's going to be tightly controlled, and almost definitely not what you want to use.\n\n@dsymonds any idea how far down the line this will be? It would be really nice to have Trace IDs as part of the context. It feels a bit clunky having to wrap all the Protos with Args/Req/Reply messages just to pass the Tracer around.\nUpdate\nExample of using gRPC metadata context: \nhttps://medium.com/@harlow/grpc-context-for-client-server-metadata-91cec8729424\n. Here is a commit w/ an example of using metadata for anyone interested. \nWorks nicely for things such as traceID:\nhttps://github.com/harlow/go-micro-services/commit/221a67bca4309aa12b69cd57ca6c1e9522c09cfc\n. After fumbling around a little I got sub-traces working on rpc calls. Hopefully this is helpful for anyone else trying to use Stackdriver traces:\nManually create the google trace header and store it in the metadata:\n```go\nfunc myHandler (w http.ResponseWriter, r *http.Request) {\n  span := e.Tracer.SpanFromRequest(r)\n  defer span.Finish()\nctx := metadata.NewContext(\n    r.Context(),\n    metadata.Pairs(\n      \"trace_header\", fmt.Sprintf(\"%s/0;o=1\", span.TraceID()),\n    ),\n  )\n```\nPull the trace header from metadata in downstream service, and create a new remote child span with SpanFromHeader:\ngo\nfunc (s *myServer) SayHello(ctx context.Context, req *hello.Request) (*hello.Result, error) {\n    md, _ := metadata.FromContext(ctx)\n    span := s.traceClient.SpanFromHeader(\n       \"/svc.Hello/SayHello\", strings.Join(md[\"trace_header\"], \"\"),\n     )\n    defer span.Finish() \nCode samples from: Tracing gRPC calls with Google Stackdriver. @tecbot thanks for the reply. I didn't realize there is opportunity to add trace interceptor to the grpc client. that's brilliant! Will see if i can get that approach working w/ google-cloud tracing library.. ",
    "dramdass": "Is there a way to set the metadata from the generated protobuf client code?\n. ",
    "WIZARD-CXY": "@harlow just what I need! Tnank you!. ",
    "hongchaodeng": "Hi @iamqizhao .\nThanks for the replying. I've been following this issue. \nLet me clarify our problem. We let user to define their grpc service and messages. It's beneficial because it makes serialization and network communication of messages more efficient (via zero copy, if supported by grpc).\nWhile user define grpc service and messages, our framework needs a layer between (user) grpc service methods and actual serialization/networking:\nGo\nfunc serviceCalled(methodName string, ctx context.Context, input proto.Message) (ctx context.Context, input proto.Message) {}\nfunc messageReturned(methodName string, ctx context.Context, output proto.Message, err error) (output proto.Message, err error) {}\nThis would be super awesome if grpc can support it. I believe there are many more use cases.\nAnd the same thing applies to client side to add dynamic wrapper, which we uses grpc.Invoke as a hack now. A official generic layer would be very nice to handle this problem.\n. Hi. I think it's a little out of track. Let me give an example.\nLet's say user provide a message definition:\nmessage userMsg {\n   ...\n}\nWe might want the rpc call to handle serialization:\nrpcCall (input *ourPb.Input, userMsg proto.Message)\nOtherwise, we have to add a bytes field:\nmessage ourMsg {\n...\nbytes userData\n}\nIn this way, we can save the computing time and mem space for serialization work. We can only do serialization once and send it over network. Otherwise we have to serialize twice and they are all in user space. This is particularly important when we handling a large size user data.\n. Just like netty SimpleChannelInboundHandler.\nIt hands over the serialization work to communication layer. As long as user defines Decode() method (in grpc case, as long as it's proto message), netty/grpc would handle it.\n. > Doesn't grpc do the serialization work for applications already?\nThat's right. There are actually two layers of \"application\". We use grpc to transfer messages over network. Message would look like this:\nmessage Entry {\n    required uint64     Type  = 1;\n    required uint64     Term  = 2;\n    required uint64     Index = 3;\n    optional bytes      UserData  = 4;\n}\nThe 4th field is actually carrying the user data. User is likely to define a proto message, and then serialize it in order to put in 4th field. This could be huge cost. Think that we are carrying 2GB user data. Serializing it TWICE would be huge cost. Not to mention that it could be improved more in zero copy in network communication.\nCan we put the entire grpc layer to user? No, we still need the other metadata (Type, Term, Index, etc.) and some control. One example would be:\nhttps://github.com/coreos/etcd/blob/9b4d52ee73e9b4d62e506f981db4887326a6b3e0/raft/raftpb/raft.proto#L20\nhttps://github.com/coreos/etcd/blob/d9b5b56c82a3f9a7a5f9bd9f5be5e2372ef33cd6/raft/multinode.go#L387\nIdeally, it's about huge data transfer and improving efficiency in serialization and network communication.\n. NP. Let me try to explain better.\nIn TCP socket, we send bytes:\nsend(b []byte)\nNow that we have protobuf and grpc, we want to send messages instead.\nsend(msg proto.Message)\nThe send(msg) API is provided by the framework, and message should be defined by user.\nI wonder how can we do it efficiently without letting user define msg/rpc on their own?\n. ",
    "xiaoyunwu": "@iamqizhao, sorry I am late to this thread, on airplane. \nOne of the reason we liked grpc is that it provides typed method directly to application, so that application can be written in an domain natural way without worry about serialization, and dispatching (if one have to use http directly). \nScanning through the threads, I think there is a miscommunication (or at least the wording suggested so). I agree that your callFoo is a good idiom, but only for application. Not for framework. The project that we worked (taskgraph) is actually at framework, by that I mean we expect many different machine learning applications can be build on top of this. In fact, we might have different framework implementations, each provide different semantics (BSP, the current implementation is just an example). \nWhen defined on host:port level, grpc is really a low level interface. Application might want to have the same typed method call at higher level. For example, assume that we have multiple replica of the same service, and we want to do automatic retries between them, or send request to all of them, and return when the first response come back. While it is easy to wrap each rpc service in each application, but that will requires many code duplication, which is bad. Also, some of these things might be hard to get it right, without leaving hooks so that it makes it easy to write framework level code will only make go programmer life harder. \nI did not really following your argument here, can you elaborate a bit more. \n \"Actually our prior experience indicates that pushing all these complexities\ninto a generic library is a bad idea. We do not have plan to to support\ngeneric interceptor (except java) in grpc.\"\nIf I go with my understanding of what it is, I think we are not ask grpc-go to provide a generic library, mere the ability to write such generic library. I do not think possibility of writing a bad generic library is a good reason to do not support the attempt to write one. \nGRPC is one of the things that I miss when I left google (among many other things), while I understand that applications might represented most of current grpc users, I think adding support to allow write framework level code will only make it more usable.   \nSo the question we have really is how do we write framework level code in grpc-go?\n. ok, right when I say grpc, I really meant stubby. I know they are\ndifferent, but they are all google rpc implementations. :-)\nI understand from your point of view, our framework indeed look like\napplication. But I want to make a distinction using the following text\nbased digram: <-text->, text inside <- and -> describe the nature of the\ninteraction between modules or layers.\nthe grpc/application split in your mind:\ngrpc  <- typed method call defined by application-> application\nwhich basically mean that application can talk use grpc for typed method\ncall that are defined by application themself.\nI do not think I made it clear in my last email, our framework want to act\nas man in the middle and adding certain framework specific checks, so we\nhave three way splits:\ngrpc <- -> taskgraph framework <-typed method call defined by application->\napplication\nHere between our framework and application, again we want to allow\napplication to define/implement typed method call, taskgraph can be just as\nlow level as grpc from application point of view.\nSo there is the difference between your view from grpc and our view from\napplication side.\nAmong other things, we try to provide a generic block synchronous parallel\nframework implementation, we might want to on the server side check whether\nthe request is in the same epoch or not, and we want to have the option not\nto burden application developer with framework logic. To do that, the only\neasy and elegant way is to be able to write service forwarding in generic\nfashion. If you have other solutions, we are happy to learn.\nI can understand that you do not want to expose too much. And load\nbalancing channel and name service will be definitely useful. Are these\nmainly a client side things, or will there be some server side construction\nas well. And is it possible to use the similar technique to add framework\nlogic in a generic fashion.\nOne last thing, I am just wondering whether you can go a bit into detail\nwhy java have generic interceptor and go does not?\nI am sorry it takes lot of your time, and I can imagine that this will be a\nsmaller request, which have many different hacks. On this front, do you\nknow effort that people hack the code generation?\nXiaoyun\nOn Wed, Mar 25, 2015 at 5:08 AM, Qi Zhao notifications@github.com wrote:\n\nOn Tue, Mar 24, 2015 at 6:02 AM, xiaoyunwu notifications@github.com\nwrote:\n\n@iamqizhao https://github.com/iamqizhao, sorry I am late to this\nthread, on airplane.\nOne of the reason we liked grpc is that it provides typed method directly\nto application, so that application can be written in an domain natural\nway\nwithout worry about serialization, and dispatching (if one have to use\nhttp\ndirectly).\nScanning through the threads, I think there is a miscommunication (or at\nleast the wording suggested so). I agree that your callFoo is a good\nidiom,\nbut only for application. Not for framework. The project that we worked\n(taskgraph) is actually at framework, by that I mean we expect many\ndifferent machine learning applications can be build on top of this. In\nfact, we might have different framework implementations, each provide\ndifferent semantics (BSP, the current implementation is just an example).\nYour framework is still an APPLICATION from the point of view of grpc. I am\nnot clear what your project does. But per my previous email, I think the\nright approach for your project is to provide your client library (probably\nand server library) to your users instead of exposing grpc directly. Recall\nthat how bigtable, GFS, chubby etc. do using google internal rpc library.\nWhen defined on host:port level, grpc is really a low level interface.\nApplication might want to have the same typed method call at higher\nlevel.\nFor example, assume that we have multiple replica of the same service,\nand\nwe want to do automatic retries between them, or send request to all of\nthem, and return when the first response come back. While it is easy to\nwrap each rpc service in each application, but that will requires many\ncode\nduplication, which is bad. Also, some of these things might be hard to\nget\nit right, without leaving hooks so that it makes it easy to write\nframework\nlevel code will only make go programmer life harder.\ngrpc is NOT defined on host:port level. load balancing channel and name\nservice is under design right now, which achieves most of what you\ndescribed above.\nI did not really following your argument here, can you elaborate a bit\nmore.\n\"Actually our prior experience indicates that pushing all these\ncomplexities\ninto a generic library is a bad idea. We do not have plan to to support\ngeneric interceptor (except java) in grpc.\"\ngRPC is a generic library -- it does not bind to a particular application.\nI think this also clears your following confusion.\nIf I go with my understanding of what it is, I think we are not ask\ngrpc-go to provide a generic library, mere the ability to write such\ngeneric library. I do not think possibility of writing a bad generic\nlibrary is a good reason to do not support the attempt to write one.\nGRPC is one of the things that I miss when I left google (among many\nother\nthings), while I understand that applications might represented most of\ncurrent grpc users, I think adding support to allow write framework level\ncode will only make it more usable.\nGRPC is not the thing you miss because it was born after you left Google.\nIt must be something else. :P\nSo the question we have really is how do we write framework level code\nin grpc-go?\nMy suggestion would be following how bigtable/gfs architects their\nframework.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85485847.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85690915.\n. Sorry, one more thing. The bigtable and gfs actually provide a low level\napi to application, and all serialization are controlled by application. We\ntry to piggyback on the good work by grpc so application can design and\nimplement their logic in an higher level interface, without worry about\nserialization.\n\nOn Wed, Mar 25, 2015 at 6:00 AM, Xiaoyun Wu xiaoyun.wu@gmail.com wrote:\n\nok, right when I say grpc, I really meant stubby. I know they are\ndifferent, but they are all google rpc implementations. :-)\nI understand from your point of view, our framework indeed look like\napplication. But I want to make a distinction using the following text\nbased digram: <-text->, text inside <- and -> describe the nature of the\ninteraction between modules or layers.\nthe grpc/application split in your mind:\ngrpc  <- typed method call defined by application-> application\nwhich basically mean that application can talk use grpc for typed method\ncall that are defined by application themself.\nI do not think I made it clear in my last email, our framework want to act\nas man in the middle and adding certain framework specific checks, so we\nhave three way splits:\ngrpc <- -> taskgraph framework <-typed method call defined by\napplication-> application\nHere between our framework and application, again we want to allow\napplication to define/implement typed method call, taskgraph can be just as\nlow level as grpc from application point of view.\nSo there is the difference between your view from grpc and our view from\napplication side.\nAmong other things, we try to provide a generic block synchronous parallel\nframework implementation, we might want to on the server side check whether\nthe request is in the same epoch or not, and we want to have the option not\nto burden application developer with framework logic. To do that, the only\neasy and elegant way is to be able to write service forwarding in generic\nfashion. If you have other solutions, we are happy to learn.\nI can understand that you do not want to expose too much. And load\nbalancing channel and name service will be definitely useful. Are these\nmainly a client side things, or will there be some server side construction\nas well. And is it possible to use the similar technique to add framework\nlogic in a generic fashion.\nOne last thing, I am just wondering whether you can go a bit into detail\nwhy java have generic interceptor and go does not?\nI am sorry it takes lot of your time, and I can imagine that this will be\na smaller request, which have many different hacks. On this front, do you\nknow effort that people hack the code generation?\nXiaoyun\nOn Wed, Mar 25, 2015 at 5:08 AM, Qi Zhao notifications@github.com wrote:\n\nOn Tue, Mar 24, 2015 at 6:02 AM, xiaoyunwu notifications@github.com\nwrote:\n\n@iamqizhao https://github.com/iamqizhao, sorry I am late to this\nthread, on airplane.\nOne of the reason we liked grpc is that it provides typed method\ndirectly\nto application, so that application can be written in an domain natural\nway\nwithout worry about serialization, and dispatching (if one have to use\nhttp\ndirectly).\nScanning through the threads, I think there is a miscommunication (or at\nleast the wording suggested so). I agree that your callFoo is a good\nidiom,\nbut only for application. Not for framework. The project that we worked\n(taskgraph) is actually at framework, by that I mean we expect many\ndifferent machine learning applications can be build on top of this. In\nfact, we might have different framework implementations, each provide\ndifferent semantics (BSP, the current implementation is just an\nexample).\nYour framework is still an APPLICATION from the point of view of grpc. I\nam\nnot clear what your project does. But per my previous email, I think the\nright approach for your project is to provide your client library\n(probably\nand server library) to your users instead of exposing grpc directly.\nRecall\nthat how bigtable, GFS, chubby etc. do using google internal rpc library.\nWhen defined on host:port level, grpc is really a low level interface.\nApplication might want to have the same typed method call at higher\nlevel.\nFor example, assume that we have multiple replica of the same service,\nand\nwe want to do automatic retries between them, or send request to all of\nthem, and return when the first response come back. While it is easy to\nwrap each rpc service in each application, but that will requires many\ncode\nduplication, which is bad. Also, some of these things might be hard to\nget\nit right, without leaving hooks so that it makes it easy to write\nframework\nlevel code will only make go programmer life harder.\ngrpc is NOT defined on host:port level. load balancing channel and name\nservice is under design right now, which achieves most of what you\ndescribed above.\nI did not really following your argument here, can you elaborate a bit\nmore.\n\"Actually our prior experience indicates that pushing all these\ncomplexities\ninto a generic library is a bad idea. We do not have plan to to support\ngeneric interceptor (except java) in grpc.\"\ngRPC is a generic library -- it does not bind to a particular application.\nI think this also clears your following confusion.\nIf I go with my understanding of what it is, I think we are not ask\ngrpc-go to provide a generic library, mere the ability to write such\ngeneric library. I do not think possibility of writing a bad generic\nlibrary is a good reason to do not support the attempt to write one.\nGRPC is one of the things that I miss when I left google (among many\nother\nthings), while I understand that applications might represented most of\ncurrent grpc users, I think adding support to allow write framework\nlevel\ncode will only make it more usable.\nGRPC is not the thing you miss because it was born after you left Google.\nIt must be something else. :P\nSo the question we have really is how do we write framework level code\nin grpc-go?\nMy suggestion would be following how bigtable/gfs architects their\nframework.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85485847.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85690915.\n. If there is server side interception, we can first introduce an new type:\n\n\ntype Message interface {\nproto.Message\n// These two functions allow framework to piggy back the meta information\n// it need to carry out its work.\nGetMeta() Meta\nSetMeta(m Meta)\n}\nwe then as application define/implement their grpc service as usual, but in\nyour generic service interceptor, we define interface based Message as\noppose to proto.Message, this way, framework can have a chance on server\nside to manipulate meta info needed by taskgraph, but still have grpc takes\ncare of serialization issues in one shot. This is still a work around, but\nat least it meets our goal as allowing application define their own message\npassing.\nOn Wed, Mar 25, 2015 at 8:16 AM, Qi Zhao notifications@github.com wrote:\n\nOn Tue, Mar 24, 2015 at 2:56 PM, Hongchao Deng notifications@github.com\nwrote:\n\nDoesn't grpc do the serialization work for applications already?\nThat's right. But there are two layers in \"application\". We use grpc to\ntransfer messages over network. Message would look like this:\nmessage Entry {\nrequired uint64 Type = 1;\nrequired uint64 Term = 2;\nrequired uint64 Index = 3;\noptional bytes UserData = 4;\nThe fourth field is actually carrying the user data. User has defined a\nproto message, and then serialize in order to put it in fourth field.\nThis\ncould be huge cost. Think that we are carrying 2GB user data. Serializing\nit TWICE would be huge cost. Not to mention that it could improved more\nin\nzero copy in network communication.\nokay, I see the problem here. This cannot be addressed now because the\nopen-sourced protobuf does not have this kind of zero-copy support (the\ninternal version does have). But I am wondering how this can be resolved\nwith a interceptor?\nWhat if we put the entire grpc layer to user? No, we still need the other\nmetadata and some control. One example would be:\nhttps://github.com/coreos/etcd/blob/master/raft/raftpb/raft.proto#L20\nhttps://github.com/coreos/etcd/blob/master/raft/multinode.go#L387\nIdeally, it's about huge data transfer and improving efficiency in\nserialization and network communication.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85709700.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85748953.\n. In the last round, we did not allow application to define typed rpc method.\nInstead, they are only allowed to implement a rather low level method:\n\nServe(taskID uint64, meta string, input []byte) []byte\nWhere user have to manually handle serialization and method dispatching.\nXiaoyun\nOn Wed, Mar 25, 2015 at 10:55 AM, Qi Zhao notifications@github.com wrote:\n\nOn Tue, Mar 24, 2015 at 5:16 PM, Qi Zhao toqizhao@gmail.com wrote:\n\nOn Tue, Mar 24, 2015 at 2:56 PM, Hongchao Deng <notifications@github.com\nwrote:\n\nDoesn't grpc do the serialization work for applications already?\nThat's right. But there are two layers in \"application\". We use grpc to\ntransfer messages over network. Message would look like this:\nmessage Entry {\nrequired uint64 Type = 1;\nrequired uint64 Term = 2;\nrequired uint64 Index = 3;\noptional bytes UserData = 4;\nThe fourth field is actually carrying the user data. User has defined a\nproto message, and then serialize in order to put it in fourth field.\nThis\ncould be huge cost. Think that we are carrying 2GB user data.\nSerializing\nit TWICE would be huge cost. Not to mention that it could improved more\nin\nzero copy in network communication.\nokay, I see the problem here. This cannot be addressed now because the\nopen-sourced protobuf does not have this kind of zero-copy support (the\ninternal version does have). But I am wondering how this can be resolved\nwith a interceptor?\n\nBTW, I do not see this is the problem for grpc only. How can you avoid it\nin your previous http based solution? If you pass all these uint64 fields\nas http headers, grpc can do the same thing already.\nWhat if we put the entire grpc layer to user? No, we still need the other\n\nmetadata and some control. One example would be:\nhttps://github.com/coreos/etcd/blob/master/raft/raftpb/raft.proto#L20\nhttps://github.com/coreos/etcd/blob/master/raft/multinode.go#L387\nIdeally, it's about huge data transfer and improving efficiency in\nserialization and network communication.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85709700.\n\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/131#issuecomment-85801078.\n. \n",
    "shafreeck": "Now I know grpc.WithTimeout(timeout) controls the timeout of grpc.Dail including the time cost of retries. Sometimes in fact I want to set the timeout of tcp connect which avoid blocking the goroutine and I'd like it to retry many times.\n. I don't think its a good way to rely on a separate goroutine. Why not build the timeout feature in like using context ? \n. OK , It works. Thanks  !\n. ",
    "bobbytables": "Is there anything I can do to help with this? I'm also looking for a way to gracefully shutdown a grpc server.\n. ",
    "frosenberg": "What is the status of this item?\n. ",
    "ctiller": "Or (depending on your need) - have the client initiate a bidirectional\nstream to the server that the server later sends data to.\nOn Mon, Apr 6, 2015 at 11:43 PM Qi Zhao notifications@github.com wrote:\n\nThere is no such thing (service, method impl) on client side. gRPC is a\nclient-server rpc framework instead of a peer-to-peer messaging library.\nYou can start both a grpc client and a grpc server on each peer to\naccommodate your requirements.\nOn Mon, Apr 6, 2015 at 9:04 PM, prazzt notifications@github.com wrote:\n\nDoes grpc support bidirection (e.g. server calls client's method\nanytime)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/152.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/152#issuecomment-90398783.\n. Part of fixing this should include extending the interop suites so that such bugs are impossible in the future.\n. Our standard for these benchmarks has always been that it's ok to tune any\nknob that a user could reasonably be expected to tune in order to\ndemonstrate the best that we can do.\n\nI'd argue GOGC and GOMAXPROCS certainly fit this.\nAs far as configuration goes, I'd place the standard as being someone\nfamiliar with the benchmarks could discover the configuration changes made\nper benchmark (ideally we'd call it out in the benchmarking logs).\nI agree we need to periodically reverify these things. For C/C++ we'll be\ndoing sweeps of parameter spaces automatically on a reduced schedule from\nour normal benchmarking efforts. As we notice better configurations we'll\ncertainly change to them. Alternatives would be to file bugs to revalidate\nsay... Once per quarter.\nThis parameter space searching is important work: I hope early in the new\nyear we can start writing performance optimization guides for all of our\nstacks, and data points like 'switching to one core for ping pong bound\nworkloads' are valuable for those efforts.\nOn Thu, Dec 8, 2016, 10:28 AM Xiang Li notifications@github.com wrote:\n\n@apolcyn https://github.com/apolcyn Sure. We should finish the\ndiscussion about tuning paras before setting this to a fixed value of 1.\nAlso we need to constantly evaluate these values.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1014#issuecomment-265815867, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJpudQKrdffc8BZQ9vcb7saUR9YI5i0dks5rGExZgaJpZM4LGLx8\n.\n. It's an intentionally loose definition: I usually take it as meaning would\na power user tuning an application that's expected to run in production\nturn to this knob.\n\nFor me, that draws a line between disabling gc - which is probably\ninteresting for benchmark analysis but not for real systems, and tuning\ncore and thread counts and gc frequency and thresholds - which are more\nlikely to be early tools for power users to reach for to find that last xx%\nfor their application.\nOn Thu, Dec 8, 2016, 11:24 AM Xiang Li notifications@github.com wrote:\n\n@ctiller https://github.com/ctiller\nOur standard for these benchmarks has always been that it's ok to tune any\nknob that a user could reasonably be expected to tune in order to\ndemonstrate the best that we can do.\nWhat do you mean by\na user could reasonably be expected to tune?\nDo you mean that users can also make the tune easily (in this case setting\ngo proc == simply adding a evn var)? Or they think the tuning itself is\nreasonable (setting go proc = 1 is reasonable to users)?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1014#issuecomment-265830267, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJpudcT81NMlwp6J4wbNrTqgvNuIBD1Jks5rGFlggaJpZM4LGLx8\n.\n. If you were working on an application that connected to a single peer and\nexchanged messages would you set max procs to 1? Such applications exist,\nand we need an idea of how grpc performs for them also.\n\nOn Thu, Dec 8, 2016, 11:37 AM Xiang Li notifications@github.com wrote:\n\n@apolcyn https://github.com/apolcyn\nI understand we can tune go GC. That is a reasonable thing to do for a lot\nof cases.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1014#issuecomment-265833704, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJpudf2ZahBMkMwbuhTAlAtZuqa3JBwLks5rGFx2gaJpZM4LGLx8\n.\n. I know of plenty of co-process applications (not all written in Go, but\nthey could conceivably be) that don't need a high thread count, but do\ndepend on low latency: proxys such as Lyft's Envoy spring to mind (and such\nuse cases are becoming more common so far as I can tell).\n\nOn Thu, Dec 8, 2016 at 12:17 PM Xiang Li notifications@github.com wrote:\n\nIf you were working on an application that connected to a single peer and\nexchanged messages would you set max procs to 1\nProbably not. Making procs to N can make gc run concurrently with\napplication. Also my application might want to process the data in async\nmanner.\nAlso do you have any real world example? Are these application the large\nmajority?\nIt seems to me that you are simply making it up to justify your decision.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1014#issuecomment-265843728, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJpuddB-MrTdh5fR6ZeFaeVqf_AdLM2kks5rGGXtgaJpZM4LGLx8\n.\n. \n",
    "kidoman": "Ah. I only saw the steaming examples. Thanks for this :+1: \n. Reopening this for a clarification.\nOn the client, if I do this:\nctx = ctx.WithValue(someKey, someValue)\nAnd on the server:\nvalue := ctx.Value(someKey)\nShould I not expect to get the value back?\n. I can see that a deadline set on the client gets propagated to the server. Any reason for excluding the values (apart from serialization/deserialization headaches) ?\n. I just saw https://godoc.org/google.golang.org/grpc/metadata\nThanks for the clarification though :)\n. ",
    "ilius": "I would say adding a context.SetBackground(ctx) feature would allow a process-scoped default context (not server-scoped or client-coped, since there is no boundary between server and client in microservice infra, most of our servers are also clients for other servers)\nThe same way that you can set http.DefaultClient or http.DefaultTransport in your own program (typically init func, or main), and that becomes the default for that process / program.. Thanks for the answer.\nBut UnaryClientInterceptor does not return a context, so how can i modify the context and add timeout?\nAlso DialOption is only for dial, right? Even if I modify the context for dial, I don't think it effects every request.. Looks like the only place that I may be able to add timeout on every grpc.Invoke, is this:\nhttps://godoc.org/google.golang.org/grpc#ClientConn.GetMethodConfig\nBut unfortunately ClientConn does not export the MethodConfig field (cc.sc.Methods) so I can't modify it. . I signed the CLA. @menghanl That worked, thank you very much. ",
    "inconshreveable": "I understand there's no need for it, but from a usability perspective, this was frustrating for me. It's inconsistent with the handler APIs generated for unary RPCs which is disorienting, but it's also inconsistent with the client API for streaming RPCs which take a context as their first parameter just like unary calls.\nMy initial thought was \"do streaming RPC's not support a context? And then I had to chase down documentation the definition of the generated Stream type and then it's embedded interfaces grpc.ServerStream and grpc.Stream before I found it.\n. Just an additional perspective here. grpc-go needs an interceptor/hook interface. The lack of an interceptor/hook interface makes grpc-go a complete non-starter for production applications.\nExposing metrics is not a reasonable compromise of a solution. The metrics important to me will be different from others and the implementation will undoubtedly be unable to satisfy everyone. There are other hooks that are unrelated to metrics that would be important to add like logging, authentication, panic reporting, etc.\nOf those I know using go-grpc in production, one of the following is true:\n1. They have been forced to fork GRPC to work around this limitation, ultimately fracturing development effort and making them less likely to upgrade.\n2. They have been forced into code generation, polluting their code base with a wrapper for every service interface that injects logic into every call. This adds an additional build step and complexity and sophistication in writing such a tool.\nWhile I sympathize with @dsymonds worry about misbehaving middleware, it is unfounded. It is akin to arguing that there should be no http.Handler interface in net/http because a misbehaving middleware could cause bad and surprising behavior. They certainly can, but most don't and they add a tremendous amount of value and important ecosystem.\n. Looking forward to client-side interceptors as well for two major use cases:\n1. Automatically unmarshalling rich error objects from trailer metadata and overriding the returned error to the caller.\n2. Tracing instrumentation and logging/error reporting\n. Would love to see whether this or an equivalent fix could be merged. I currently have to use a fork of grpc  which includes this PR.\n. @timbunce You're not missing anything. I mentioned during the talk that the code was not safe for concurrent usage and that any real implementation would need synchronization. Concurrency safety was omitted from the slides/talk in the interest of 1) focusing on the code related to grpc-specific issues 2) keeping the code on the slides big enough to be readable. ",
    "gxb5443": "I signed the thing\n. Hey!  Good catch.  I've added a flag on the server to know when goaway is received.  The flag prevents more active streams from being created.  Meanwhile, any streams that have made it through that have an id greater than the one specified in the GOAWAY frame get cleared out of the active streams.  The implementation is similar to the one in grpc-java.\n. I'm a little unsure of how to go about testing the goaway handler.  You don't seem to explicitly test the handlers, and the goaway doesn't really do anything (besides not throw an error) that the client can really test for?  Could you offer a little guidance?\n. I agree, the server shouldn't really do anything when it receives a GOAWAY (just don't throw an error).  I've made the handler a no-op.  If you accept it, this will close and then I'd like to implement it in the client then.  Although under what condition would the client ever practically send the GOAWAY frame?\n. Fixed\n. Fixed\n. Gets rid of the iteration, just sets flag to true (now with mutex!)\n. This part I'm a little unclear on still.  Can you point me to some documentation so I can understand this better?\n. I know, I just put it there for readability purposes.\n. Good catch.  Fixed.\n. Like log to terminal or writting a message out to the client?\n. Ok that's what I thought, haha.  I only thought of writing out to the client to help with testing.  I am working on testing now.\n. I've come this far, I'm going to get it!  Thanks :D\n. ",
    "zoutaiqi": "Sorry,  it should be caused by my shell script, it works well if the server started by manual, thank you for your quick reply. and I have another question: \nif client crashed before streaming rpc finished, will the server detect it and clean the related resource(goroutine)? I noticed that the rpc goroutine blocked at stream.Recv on server side after client crashed, did that expect? thanks!\n. Got it, thank you very much. and expect the health checking mechanism add in, before that I will set timeout or deadline in context on client side to handle this case.\n. HI, @iamqizhao , I got the same problem after I fixed the script issue, and I got a very little output when they quit, I don't know if there is any useful information for you debug.\nenv:\nI have 7 node as cluster in the test bed, every node just exchange three simple message each other as streaming Client and Server, they ran normally about 3 days, then one node quit with unknown reason(maybe the same issue), and the other nodes were trying to reconnect it, then I restart it and try to rejoin the cluster, then almost all of nodes quit here without core dump:\n.....\ngoroutine 621 [select, 9 minutes]:\ngoogle.golang.org/grpc/transport.(*http2Server).controller(0xc20827e380)\n        /home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:594 +0x551\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n        /home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:132 +0x88a\ngoroutine 784 [IO wait]:\nnet.(_pollDesc).Wait(0xc2082a2450, 0x72, 0x0, 0x0)\n        /usr/local/go/src/net/fd_poll_runtime.go:84 +0x47\nnet.(_pollDesc).WaitRead(0xc2082a2450, 0x0, 0x0)\n        /usr/local/go/src/net/fd_poll_runtime.go:89 +0x43\nnet.(_netFD).Read(0xc2082a23f0, 0xc2083a1cd4, 0x9, 0x9, 0x0, 0x7fd8d2f03bf0, 0xc208364100)\n        /usr/local/go/src/net/fd_unix.go:242 +0x40f\nnet.(_conn).Read(0xc2082a6020, 0xc2083a1cd4, 0x9, 0x9, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/net.go:121 +0xdc\nio.ReadAtLeast(0x7fd8d2f06730, 0xc2082a6020, 0xc2083a1cd4, 0x9, 0x9, 0x9, 0x0, 0x0, 0x0)\n        /usr/local/go/src/io/io.go:298 +0xf1\nio.ReadFull(0x7fd8d2f06730, 0xc2082a6020, 0xc2083a1cd4, 0x9, 0x9, 0x1000100000147, 0x0, 0x0)\n        /usr/local/go/src/io/io.go:316 +0x6d\ngithub.com/bradfitz/http2.readFrameHeader(0xc2083a1cd4, 0x9, 0x9, 0x7fd8d2f06730, 0xc2082a6020, 0x0, 0x0, 0x0, 0x0)\n        /home/elc/go/src/github.com/bradfitz/http2/frame.go:228 +0xa2\ngithub.com/bradfitz/http2.(_Framer).ReadFrame(0xc2083a1cb0, 0x0, 0x0, 0x0, 0x0)\n        /home/elc/go/src/github.com/bradfitz/http2/frame.go:373 +0xf2\ngoogle.golang.org/grpc/transport.(_framer).readFrame(0xc2082aa210, 0x0, 0x0, 0x0, 0x0)\n        /home/elc/go/src/google.golang.org/grpc/transport/http_util.go:436 +0x50\ngoogle.golang.org/grpc/transport.(_http2Server).HandleStreams(0xc20827e100, 0xc2082b9380)\n        /home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:244 +0x691\ngoogle.golang.org/grpc.func\u00c2\u00b7014()\n        /home/elc/go/src/google.golang.org/grpc/server.go:208 +0xc5\ncreated by google.golang.org/grpc.(_Server).Serve\n        /home/elc/go/src/google.golang.org/grpc/server.go:212 +0x5f5\ngoroutine 783045 [select, 9 minutes]:\ngoogle.golang.org/grpc/transport.(*http2Server).controller(0xc20827e100)\n        /home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:594 +0x551\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n        /home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:132 +0x88a\n. thanks\uff0cI will try it\n\u53d1\u81ea\u6211\u7684 iPhone\n\n\u5728 2015\u5e745\u670816\u65e5\uff0c2:01\uff0cQi Zhao notifications@github.com \u5199\u9053\uff1a\nThese logs are not useful. Can you add some logging to the exit points of\nthis loop:\nhttps://github.com/grpc/grpc-go/blob/master/server.go#L193\nand rerun your stuffs? Then if it happens again, we know where grpc exits.\nIf it happens without any of these logs, it means your program terminates\nthe process instead of grpc.\nOn Fri, May 15, 2015 at 1:15 AM, zoutaiqi notifications@github.com wrote:\n\nHI, @iamqizhao https://github.com/iamqizhao , I got the same problem\nafter I fixed the script issue, and I got a very little output when they\nquit, I don't know if there is any useful information for you debug.\nenv:\nI have 7 node as cluster in the test bed, every node just exchange three\nsimple message each other as streaming Client and Server, they ran normally\nabout 3 days, then one node quit with unknown reason(maybe the same issue),\nand the other nodes were trying to reconnect it, then I restart it and try\nto rejoin the cluster, then almost all of nodes quit here without core dump:\n.....\ngoroutine 621 [select, 9 minutes]:\ngoogle.golang.org/grpc/transport.(*http2Server).controller(0xc20827e380)\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:594\n+0x551\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:132\n+0x88a\ngoroutine 784 [IO wait]:\nnet.(\n_pollDesc).Wait(0xc2082a2450, 0x72, 0x0, 0x0)\n/usr/local/go/src/net/fd_poll_runtime.go:84 +0x47 net.(_pollDesc).WaitRead(0xc2082a2450,\n0x0, 0x0)\n/usr/local/go/src/net/fd_poll_runtime.go:89 +0x43\nnet.(\n_netFD).Read(0xc2082a23f0, 0xc2083a1cd4, 0x9, 0x9, 0x0, 0x7fd8d2f03bf0,\n0xc208364100) /usr/local/go/src/net/fd_unix.go:242 +0x40f net.(_conn).Read(0xc2082a6020,\n0xc2083a1cd4, 0x9, 0x9, 0x0, 0x0, 0x0)\n/usr/local/go/src/net/net.go:121 +0xdc\nio.ReadAtLeast(0x7fd8d2f06730, 0xc2082a6020, 0xc2083a1cd4, 0x9, 0x9, 0x9,\n0x0, 0x0, 0x0)\n/usr/local/go/src/io/io.go:298 +0xf1\nio.ReadFull(0x7fd8d2f06730, 0xc2082a6020, 0xc2083a1cd4, 0x9, 0x9,\n0x1000100000147, 0x0, 0x0)\n/usr/local/go/src/io/io.go:316 +0x6d\ngithub.com/bradfitz/http2.readFrameHeader(0xc2083a1cd4, 0x9, 0x9,\n0x7fd8d2f06730, 0xc2082a6020, 0x0, 0x0, 0x0, 0x0)\n/home/elc/go/src/github.com/bradfitz/http2/frame.go:228 +0xa2\ngithub.com/bradfitz/http2.(\n_Framer).ReadFrame(0xc2083a1cb0, 0x0, 0x0, 0x0, 0x0)\n/home/elc/go/src/github.com/bradfitz/http2/frame.go:373\nhttp://github.com/bradfitz/http2/frame.go:373 +0xf2\ngoogle.golang.org/grpc/transport.(\nhttp://google.golang.org/grpc/transport.(_framer).readFrame(0xc2082aa210,\n0x0, 0x0, 0x0, 0x0)\n/home/elc/go/src/google.golang.org/grpc/transport/http_util.go:436 +0x50\ngoogle.golang.org/grpc/transport.(\n_http2Server).HandleStreams(0xc20827e100, 0xc2082b9380)\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:244\nhttp://google.golang.org/grpc/transport/http2_server.go:244 +0x691\ngoogle.golang.org/grpc.func\u00c2\u00b7014()\nhttp://google.golang.org/grpc.func%C3%82%C2%B7014()\n/home/elc/go/src/google.golang.org/grpc/server.go:208\nhttp://google.golang.org/grpc/server.go:208 +0xc5 created by\ngoogle.golang.org/grpc.( http://google.golang.org/grpc.(_Server).Serve\n/home/elc/go/src/google.golang.org/grpc/server.go:212 +0x5f5\ngoroutine 783045 [select, 9 minutes]:\ngoogle.golang.org/grpc/transport.(*http2Server).controller(0xc20827e100)\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:594\n+0x551\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n/home/elc/go/src/google.golang.org/grpc/transport/http2_server.go:132\n+0x88a\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/193#issuecomment-102312737.\n\u2014\nReply to this email directly or view it on GitHub.\n. Hi, @iamqizhao, this is the last output that some of nodes panic almost at the same time, hope this is helpful for debug, thanks. \n\n\n2015/05/22 06:56:30 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2015/05/22 06:56:30 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2015/05/22 06:56:30 grpc: Server.processStreamingRPC failed to write status: connection error: desc = \"transport is closing\"\n2015/05/22 06:56:30 transport: http2Client.notifyError got notified that the client transport was broken EOF.\npanic: close of closed channel\n. Sorry, @iamqizhao , I found a bug might close a channel twice in my program, should be the root cause,I will close this  issue. Thanks for your help.\n. ",
    "derekperkins": "@iamqizhao @zoutaiqi I'm getting the same transport: http2Client.notifyError got notified that the client transport was broken EOF., while not triggering an error. I'm running on App Engine Managed VMs using BigQuery and Bigtable.\n. ",
    "sqs": "You are right, sorry. I was thinking about it in the wrong way (trying to use the gRPC client like an HTTP client).\n. ",
    "yangzhouhan": "I signed it.\nOn Mon, Jun 1, 2015 at 3:15 PM, googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project, in which case you'll need to\nsign a Contributor License Agreement (CLA).\n[image: :memo:] Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your\n  GitHub username or you're using a different email address. Check your\n  existing CLA data https://cla.developers.google.com/clas and verify\n  that your email is set on your git commits\n  https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n  company's name.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/207#issuecomment-107733585.\n. done. PTAL\n. fixed\n. fixed, PTAL. Thx.\n. fixed.\n. fixed. \n. I could find a way to remove a commit history from a PR, would you pls\nadvise me if you know how to do that? Thanks.\n\nOn Fri, Aug 7, 2015 at 4:00 PM, David Symonds notifications@github.com\nwrote:\n\nLooks like you've picked up a Vim swap file. (I think you'll want to\nrewrite the commits to get rid of that, not just stack another commit to\ndelete it, since it can hold sensitive information.)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/281#issuecomment-128853686.\n. s/could/could not/\n\nOn Fri, Aug 7, 2015 at 4:00 PM, David Symonds notifications@github.com\nwrote:\n\nLooks like you've picked up a Vim swap file. (I think you'll want to\nrewrite the commits to get rid of that, not just stack another commit to\ndelete it, since it can hold sensitive information.)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/281#issuecomment-128853686.\n. fixed.\n. All done. PTAL. Thx.\n\nOn Wed, Jun 17, 2015 at 7:58 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/229#discussion_r32695701:\n\ns, err := t.NewStream(ctx, callHdr)\nif err != nil {\n    return nil, toRPCErr(err)\n}\nreturn &clientStream{\n-       t:     t,\n-       s:     s,\n-       p:     &parser{s: s},\n-       desc:  desc,\n-       codec: cc.dopts.codec,\n-       t:         t,\n-       s:         s,\n-       p:         &parser{s: s},\n-       desc:      desc,\n-       codec:     cc.dopts.codec,\n-       traceInfo: trInfo,\n\nthis is incorrect. This is making a copy of the traceInfo struct, but the\ntrace has a pointer to the original, so any changes to firstLine after this\nwill not be reflected, for instance. It'll also pin both bits of memory.\nEither clientStream needs to be allocated higher up (probably the simplest\noption), or it needs to have a *traceInfo field instead.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/229/files#r32695701.\n. yes, that's correct.\n\nOn Thu, Jun 18, 2015 at 4:54 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/229#discussion_r32791029:\n\n@@ -161,6 +173,16 @@ func (cs *clientStream) SendMsg(m interface{}) (err error) {\nfunc (cs *clientStream) RecvMsg(m interface{}) (err error) {\n    err = recv(cs.p, cs.codec, m)\n-   defer func() {\n-       // err != nil indicates the termination of the stream.\n\nso it is certain that this method won't be called again after this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/229/files#r32791029.\n. Fixed. Thx\n\nOn Thu, Jun 18, 2015 at 4:55 PM, David Symonds notifications@github.com\nwrote:\n\nIn stream.go\nhttps://github.com/grpc/grpc-go/pull/229#discussion_r32791055:\n\n@@ -161,6 +173,16 @@ func (cs *clientStream) SendMsg(m interface{}) (err error) {\nfunc (cs *clientStream) RecvMsg(m interface{}) (err error) {\n    err = recv(cs.p, cs.codec, m)\n-   defer func() {\n-       // err != nil indicates the termination of the stream.\n-       if EnableTracing && err != nil {\n-           cs.traceInfo.tr.Finish()\n\nthis needs to be after the two calls below. cs.traceInfo.tr cannot be\nused after Finish is called.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/229/files#r32791055.\n. Yes, I think an extra arg is necessary here, Method Desc only contains method information, no service name. \n. \n",
    "zboralski": "Do servers work now?\n. ",
    "nodirt": "I assume this bug was closed because it was ambiguous (client or server).\nI've filed #805 specifically about server.\n. errors.WrapTransient is the wrapper function we use to mark errors as transient. \ngo\nerr := grpc.Errorf(codes.Internal, \"Internal server error\")\nerr = errors.WrapTransient(err)\nactualCode := grpc.Code(err)\nhere actualCode is codes.Unknown instead of codes.Internal. We have other wrappers as well.\nSee also Dave Cheney's tutorial on error inspection.\n. Package errors is a general purpose package. Adding a grpc dependency to it is a wrong thing to do.\n. My last comment contradicts the proposal. We worked around this by unwrapping an error.\n. oauth.go implements client-side oauth, while this bug is about server-side oauth. In other words, the code that reads \"Authorization\" HTTP header, communicates to an authority to verify the access token and puts a peer.Peer to the context.\n. Basically we need to be able to inject something like\ngo\ntype MetadataAuthenticator interface {\n    // AuthFromMetadata validates metadata and returns AuthInfo.\n    // May block.\n    AuthFromMetadata(md metadata.MD) (AuthInfo, error)  \n}\nto grpc.Server, so it calls AuthFromMetadata with the metadata parsed from request headers. On success, puts AuthInfo to the peer.Peer in context. Otherwise reply with HTTP 401.\nHowever, I am not sure\n- whether this should happen sometime in the beginning of a HTTP2 request on every stream. Probably once.\n- why credentials.TransportAuthenticator has to implement credentials.Credentials. GetRequestMetadata has nothing to do with server-side authentication. IMO this part of the design should be revisited, maybe client- and server- authentication should be split. To me grpc.Creds func is confusing (as a server owner, I want to check creds of clients, not provide creds) and should be renamed to something like Auth that accepts an interface for server-side authentication.\n. Why code generation had to be changed? The interceptor in the generated code seems to be unnecessary because handler func doesn't pass any data to the interceptor that the caller of the handler func doesn't already have. Note that in is passed to dec\n. Yeah, I see. I just tried to write a patch for (*Server).processUnaryRPC and got stuck on the fact that the handler that interceptor receives is a func that accepts a decoded messages whereas generated handler receives a func that does decoding, not a decoded message.\n. @mwitkow not at this time\n. ",
    "ernestoalejo": "It was about server only (the server can't be started on an instance.), client was already working last year when I filled the issue. Thank you for referencing the new tracking issue; looking forward to see it resolved.\n. ",
    "crast": "The only \"harm\" is that doing a \"go get\" gets a tree of dependencies which increases the time spent in go get, increases your checkout size, and the code is compiled into your binary even if not using oauth... because it's currently in the same package as other credentials you might be using.\nWe're using GRPC right now in a new internal microservice we're writing, and it's not using OAuth. It's not in full production yet, but it will be in the next month; again I know it's not harmful, it's just a bit silly that I need the oauth library which I won't use to compile into my binary.\n. @puellanivis The simple solution: Have people keep using golang.org/x/net/context  - it already has provisions where it will use all the underlying implementations from go 1.7 context.Context  if running in go 1.7.\nYou can pass a x/net/context.Context to a function expecting a context.Context because both of the interfaces have equivalent method sets. The only problem here is in satisfying the function signature from generated servers.  What it means is go 1.7 users using GRPC will still need golang.org/x/net but this isn't really a huge deal.\nYou can compose contexts using e.g. WithTimeout WithCancel from both packages and it will do the right thing.\n. ^ fwiw, I've already tested the above on go 1.7beta1, and it works, and the advantage is the same code works on go 1.5, 1.6. 1.7 \n. > I made that stepping stone the same day this bug was filed, but it is not good programming.\nConversely, a package which breaks version compatibility in weird ways for the sake of reducing package count isn't good package maintenance.\n\nYes, a work around is still \u201cjust use golang.org/x/net/context forever\", but the whole reason for putting context into the standard library is that it's no longer experimental anymore and isn't confined semantically into the \"net/\" packages anymore.\n\n\"forever\" is kind of over-blown, given how fast the Go ecosystem moves. If the trend shown in the last set of security fixes this spring hold, where the crypto vulnerability was only fixed for 1.5 and 1.6 (1.5.4 and 1.6.1) then the currently supported versions of go is the last 2 minor versions. At the current release rate of one every 6 months, this means that by February 2017, the only Go versions getting security updates will be 1.7 and 1.8.  Anyone still on <=1.6 at that point who wants to run a server will move up.\nI think it's completely sensible to keep using x/net/context until approx. February 2017, to facilitate the transition\n. > This is a snowdrift, and I'm plowing it. One shouldn't be looking to stop me from plowing it by saying that we don't have to get to work until 17:02, and it's only 16:06, so like, \u201cWe have like an hour dude, we don't need to do it now.\u201d I don't care, I'm clearing the snowdrift now. I can take my time and make sure it's done right the first time, rather than adhoc because last second.\nThat's not what I was implying by saying this.  I'm not saying don't plow the snowdrift. By no means should we not be forward looking... however, what I'm saying is: avoid knocking over telephone poles with your giant ice pusher, because some people really want their lights to work.\nI'll use a really real example. I work for a company and we have a number of Go projects. Currently, we're doing continuous integration, testing, and actual deployment on Go 1.6.2, but a number of our developers are using 1.7beta1 locally, to be forward looking and make sure our stuff is ready for Go 1.7 as we're excited about moving over. That said, our code still needs to use x/net/context because we need to deploy on go 1.6 for the forseeable future (even when 1.7 gets released, there will likely be a lag time of a month or two before we fully roll it out in production, during which time we might have some 1.7 processes as canaries comingled with 1.6.2 processes).\nIf GRPC starts conditionally using context vs x/net/context depending on where it's built, it could force us to use build flag based interfaces everywhere in order for us to build with both versions, which leads to a fair amount of duplication.  The only good thing is that this will be caught at the compiler level and by CI, so it won't make it to production, but it will significantly increase complexity for minor savings, given x/net/context still imports \"context\" in go 1.7 making it a fairly thin package from the binary level (yes I know, it still requires you checking out the considerably large x/net repo... but given that GRPC also needs x/net for other things it's mostly irrelevant)\n. @puellanivis You can actually handle it in the grpc codegen with some sort of option or flag to the codegen.\nif you wrote a method called Ping, you would get a method handler something like this:\ngo\nfunc _MyService_Ping_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n    in := new(PingRequest)\n    if err := dec(in); err != nil {\n        return nil, err\n    }\n    if interceptor == nil {\n        return srv.(MyServiceServer).Ping(ctx, in)\n    }\n    // [...]\n    // omitted because it would be solved similarly\n}\nIt's actually already generating an adaptor for every function anyway, to call each individual method as needed but provide a single function call interface to the GRPC server setup. it's this _MyService_Ping_Handler that is actually called before it calls your server implementation's Ping method.\nIf you were to make a tweak to the code-gen that when some flag is set, it would generate code like this:\n``` go\nimport (\n    context \"context\"\n    netcontext \"golang.org/x/net/context\"\n)\n// [ ... ] other code-gen stuff\ntype MyServiceServer interface {\n    Ping(context.Context, PingRequest) (PongResponse, error)\n}\nfunc _MyService_Ping_Handler(srv interface{}, ctx netcontext.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n    in := new(PingRequest)\n    if err := dec(in); err != nil {\n        return nil, err\n    }\n    if interceptor == nil {\n        return srv.(MyServiceServer).Ping(ctx, in)\n    }\n    // [...]\n    // omitted\n}\n```\nYou'll notice the function body is almost identical, except that the method uses netcontext.Context and the interface containing Ping uses context.Context. This would actually work and compile, because each Context's method set is a strict subset of the other (it's directly assignable without a type assert).\nAdvantages:\n1. Allows user to choose how they use context, without conditional compilation stuff\n2. Solves this in a way that allows go 1.7 users to keep using x/net/context if they need it (like compiling for multiple versions)\nDisadvantages:\n1. You still need to import x/net/context even on go 1.7, for use inside your .pb.go file and GRPC uses x/net/context internally everywhere (though your project code could be free of x/net/context)\n2. Maybe more complexity to the codegen to support both modes, as you can't do this with a simple regex.\n. ",
    "aaronbee": "@awalterschulze FYI, godep will do the right thing here and will only vendor the import paths (and the import paths of those import paths) that your code actually uses. ie. My project vendors grpc with godep but my godep workspace doesn't include google.golang.org/cloud, api, glog, etc.\n. It sounds to me that @tomq42 has an updated grpc, but an out-of-date compiled .pb.go file.\n@tomq42 You should upgrade your go-protoc plugin:\ngo get -u github.com/golang/protobuf/{proto,protoc-gen-go}\nAnd recompile your .proto files.\n. That doesn't appear to be true:\nfunc main() {\n    lis, err := net.Listen(\"tcp\", \"localhost:0\")\n    if err != nil {\n        panic(err)\n    }\n    if err := lis.Close(); err != nil {\n        panic(err)\n    }\n    _, err = lis.Accept()\n    fmt.Println(\"Error:\", err)\n    fmt.Println(\"Temporary?\", err.(interface {\n        Temporary() bool\n    }).Temporary())\n}\nThis prints:\nError: accept tcp 127.0.0.1:56102: use of closed network connection\nTemporary? false\n(I would have shared on Go playground, but that doesn't allow creating a tcp socket.). I am a user and the use case I had for the transport package was the error types, such as transport.ErrConnClosing. We log errors that are unexpected, and we don't consider errors due to a connection closing unexpected. So we try to determine if an error should be logged by comparing it against ErrConnClosing.. @dfawley Got it. Thanks. My code no longer directly depends on the transport package.. To get the IP address you can use the peer package. Use peer.FromContext, and pass in the context you got from the call.\nI'm less sure on how to get the user-agent string. Likely you can use the metadata to get it.. ",
    "kissgyorgy": "I don't:\n```\n$ go get google.golang.org/grpc                                                                                                                   [07:42:44]\ngoogle.golang.org/grpc/transport\n../../../../go/src/google.golang.org/grpc/transport/handler_server.go:220: undefined: http2.TrailerPrefix\n../../../../go/src/google.golang.org/grpc/transport/http2_client.go:1070: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http2_client.go:1191: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http2_server.go:232: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http2_server.go:404: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http_util.go:200: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http_util.go:465: f.fr.SetReuseFrames undefined (type http2.Framer has no field or method SetReuseFrames)\n../../../../go/src/google.golang.org/grpc/transport/http_util.go:466: f.fr.ReadMetaHeaders undefined (type http2.Framer has no field or method ReadMetaHeaders)\n../../../../go/src/google.golang.org/grpc/transport/http_util.go:596: f.fr.ErrorDetail undefined (type *http2.Framer has no field or method ErrorDetail)\n``. I get this when Igo get google.golang.org/grpc`:\n```\ngoogle.golang.org/grpc/transport\n../../../../go/src/google.golang.org/grpc/transport/handler_server.go:220: undefined: http2.TrailerPrefix\n../../../../go/src/google.golang.org/grpc/transport/http2_client.go:1070: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http2_client.go:1191: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http2_server.go:232: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http2_server.go:404: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http_util.go:200: undefined: http2.MetaHeadersFrame\n../../../../go/src/google.golang.org/grpc/transport/http_util.go:465: f.fr.SetReuseFrames undefined (type http2.Framer has no field or method SetReuseFrames)\n../../../../go/src/google.golang.org/grpc/transport/http_util.go:466: f.fr.ReadMetaHeaders undefined (type http2.Framer has no field or method ReadMetaHeaders)\n../../../../go/src/google.golang.org/grpc/transport/http_util.go:596: f.fr.ErrorDetail undefined (type *http2.Framer has no field or method ErrorDetail)\n```. That worked, thank you!. ",
    "zombiezen": "If you're trying to write functions that operate over any stream, then you can't write to the stream without having the ClientTransport.  You're forced to pass around the triple of (server transport, client transport, stream).  This would allow clients of Stream to only need the stream.\n. Fair enough. You're right that refactoring the Write method onto the Stream would be more ideal. I can wait for that.\n. ",
    "ggitau": "Creating a symbolic link to this file (protoc-gen-gogo) with the name \"protoc-gen-go\" has solved the issue.\n. Oh my bad.Thanks for pointing that out. Installed golang/protobuf and that sorted it out.\n. ",
    "ejona86": "Yes, we are actively working on client-side LB in gRPC as a whole and have been expecting to eventually support etcd and DNS SRV (among others). Much of how it will function cross-language has been defined at this point, but due to \"reasons,\" like \"time,\" hasn't been made public. I don't have an ETA on when the work will be started in Go, other than \"soon,\" in part because of the July 4th holidays several people are on vacation and I can't ask them :).\n. We can leave for tracking for now at least. The \"main\" tracking issue is grpc/grpc#1790, which is... quiet. It is the issue which would make the design public. Java is tracking it at grpc/grpc-java#428 and has more information regarding some plans, but not quite enough for you to go off of.\n. No, we need a reliable channel and UDP is not supported. There has been discussion with playing with things like QUIC, but we aren't currently trying it out and it would be even longer before we decided whether we would support it.\n. #147 is server-side, this is client-side.\n. It looks like this was completed in #610. Am I reading that right?\n. We are adding HTTP CONNECT support in C core (grpc/grpc#7611) and this is a cross-language feature that would benefit all languages. Thus, we believe we should do it in Java (grpc/grpc-java#2193) and Go as well. C will definitely get it first, but we will be trying to schedule this work in for the other languages.\nWe are not planning on SOCKS or similar. Those are much larger beasts than simple HTTP CONNECT.\nNote that I wouldn't have expected this to be a good solution for haproxy and other server-side proxies, although I guess maybe it could work as a hack. The supported use case is for environments that must connect through a HTTP proxy to access the external world, like many enterprise environments.\n. @iamqizhao, the Recv() will not error if there is no write. This issue effectively boils down to the same as #536.\nAnd note that health checking is not a full solution, because the health checking RPCs could go on a different TCP connection (let's say GOAWAY was received) and then a NAT drops the TCP connection with the long-lived RPC due to inactivity but keeps the TCP connection for the health checking.\nTCP keepalive actually solves the problem, although we're still avoiding it due to DoS, configuration, and waste concerns.\n. Is this actually fixed? #780 looks server-side only. This issue was not specific to server.\n. The library should guarantee that it doesn't generate empty messages (empty messages from the server are \"okay\"). I did a quick look and found multiple places where it appeared empty descriptions were used.\n. LGTM. That's the only one I'm seeing now as well.\n. There should be no need for a parameter for this. The default behavior should just be fixed.. > I don't know what the wider gRPC consensus is around content type negotiation, but the current implementation implicitly returns the same ContentSubtype as invoked.\nI think this is the most advanced negotiation considered up to this point.\n\nI do believe that since gRPC is HTTP2-based, we should leverage both the Accept (what the cleint can read as a response) and the Content-Type header (to sigify the encoding of the client content).\n\nI'm not sure what the use-case is for that. If the request is JSON, then having the reply in JSON seems good to me. Replies will still use grpc framing, so it's not like we will negotiate between HTML, TXT, and PDF.\nThere have been two proposed use-cases for a method supporting multiple encodings:\n 1. JS clients could possibly benefit from using JSON instead of binary protobuf. For performance or code-size reasons\n 2. Visibility during debugging\nBoth of those would use the same format for the request and response.. @bbeaudreault, the code sends two GOAWAYS. The first one is here. We send a PING after the first one and when the PING ACK arrives, we send the second. The second is when we would possibly close the connection.. > I mean it will handle cases when pod is destroyed/recreated but it will not handle cases when service is scaled up and new pods are ready to serve requests, isnt?\nThe solution for this is MAX_CONNECTION_AGE. This has the advantage that the service owner controls the configuration. (It also works with L4 proxies where the address doesn't change.). > DNS in k8s only returns the Virtual IP of the service and not all IP's of the replicas. This will not work when reusing a single client conn.\n@trusch, what do you mean by \"not work?\" It should \"work\" as in function, in a basic sense. It would also load balance multiple clients to multiple servers and rebalance the clients over time (via MAX_CONNECTION_AGE). But I would agree it does not help distributing load from a single client to multiple servers. Is this what you mean?\nOur (internal-discussion only?) proposed solution there is to create multiple ClientConns and round-robin over them. To make this easier, we could enhance grpc to create the multiple ClientConns behind a single ClientConn so it becomes mostly transparent. The extra connections would not be guaranteed to get different backends, but that is a normal property of L4 LBs.\nThe discussions/work to do this stalled. But this work can be re-opened if necessary. But either being okay with what you have (because, let's say, you don't a single client causing a substantial amount of load) or exposing IPs to your clients to avoid the L4 LB sounds superior in this case.. @HannesMvW, see https://github.com/grpc/grpc-go/issues/1663#issuecomment-343566633. The API is keepalive.ServerParameters.MaxConnectionAge.. @HannesMvW, use grpc.KeepaliveParams() to create a ServerOption that's passed to grpc.NewServer(). @HannesMvW, when the server shuts down the connection due to age, the client will re-resolve DNS. This is very natural with pick-first load balancing. With round-robin it is slightly awkward, but it doesn't seem severe enough to warrant determining yet-another-lb-solution. It's also only a problem when using DNS as other naming systems tend to provide update notifications.\nDNS polling is in general a bad idea. In the presence of round-robin DNS servers it would cause actual harm. High-frequency DNS polling can cause quite a bit of load to a critical system, which is dangerous.  It is also client-side configuration which generally cannot be updated quickly. I'm right now hoping to kill the 30 minute polling, as Go is the only implementation that does that and there should not be a need.\nThe k8s documentation itself talks about them avoiding the same problems.\n\nI'd like to keep server connections up throughout the lifetime of the server\n\nWhat harm are you seeing by the re-creation of connections? I know of several possible issues, but I'm interested in what you're experiencing.. Ah, I see. Your case is quite pathological. I'll try to have a quick conversation with some other devs today about this.\nWorst-case, you could write your own name resolver \"my-better-dns\" that does exactly what you want. Most of the complexity of the current resolver is to do all those things you don't like anyway, so this probably isn't that bad. (Not to say it should be the solution, just that it isn't a bad \"worst case.\")\n(1) shouldn't return any results for your case, so (2) will be a noop. Most of the time we wouldn't expect 100s of LB servers and instead you rely on a few virtual IPs or the like for large scale-out. Pick-first is used to the LB.\n(4) shouldn't return any results for your case as well.\nThe steps 1, 2, 4 aren't really intended to be configured and it's unfortunate they are always done even when unnecessary, but this is sort of the state of the world. There's not many other options other than \"use better service discovery.\" There's quite a lot in the pipeline to add to service config, so in the future I hope you'd be interested in it, although the \"not supported by k8s\" would need a resolution of some sort.. Discussed with @dfawley. @HannesMvW, we think writing your own name resolver is the way to go. Should be pretty easy and you can configure it exactly like you want.\nYou're actually in deep enough of a case where you may want \"serious client-side load balancing.\" That would mean \"gRPC LB.\" Unfortunately that still lacks a server and so has high barrier for entry, but it allows you to more seriously scale and adapt to load and changes. I expect you'd be happy enough with making your own resolver, but I thought I should mention that your case is getting into this other realm.. > Envoy responds to them instead of proxying them to the client.\nThis is working as expected. Keepalive is monitoring a particular TCP connection, which is the same hop-by-hop stuff Envoy is mentioning.\n\nThe solution we agreed on in my issue is that Envoy would implement its own Keep Alive heartbeat\n\nThis is exactly what I would suggest. The connection from your backend to Envoy can be quite different from the connection to Envoy from the client. The two will fail at different times (thus Envoy can't depend on backends triggering ping to the client), easily need different keepalive times, connection lifetimes, and more. So keepalive being hop-by-hop is really an intended feature and not a bug.. @cdelguercio, you need some level of keepalive support in Envoy. That could be via kernel-provided keepalive. But to do anything other than have keepalive in Envoy in a hack.\nDoing your own heartbeat can help. But it sounds like you want the heartbeat to be from the server to the client, since you want Envoy to notice when the TCP connection is broken. Triggering a write is a good way to do that.. @cdelguercio, Envoy is likely not noticing the client's connection is broken. It may take 20 seconds or more (1-2 minutes) for it to notice. It then needs to cancel all HTTP/2 streams that are impacted, which should notify gRPC of the RPC failure. It sounds like Envoy is taking longer than you'd like to notice the failure. That seems like an Envoy issue and little can be done from gRPC. I'll note that gRPC mainly solves that problem with keepalive, so the same solution would likely fix that problem as well.\nI'll note that adding a proxy generally increases the amount of buffering in the system. That can allow many more Send calls to succeed before slowing down or failing.. I'm arguing we need to remove the WithResolverUserOptions in #1812. @dzbarsky, I assume you'll want to speak up and maybe explain some of the difficulties of your use case.. I have the same concern of \"all languages or none.\" Balancer configuration is more up-in-the-air across languages, I think. Today, I think the Name Resolver is what's configuring the balancer (as with gRPC LB), but this unfortunately couples the resolver and the balancer at least partially.\nI'm not quite clear on how the resolver and balancer communicate today in Go, for things like gRPC LB configuration. Using this for hash function/seed was the example. If that information can be provided by the name resolver, that would be the strong preference. If not and it's also not global... then I think we'll need to figure something out. Similar to name resolver, I do think it may make more sense to pass in a pre-configured balancer object to gRPC instead of side-loaded configuration. But that's arguable.. CC @dzbarsky. > Due to internal optimizations\nThat's pretty vague. Is there a way to describe the use-case such that we could make design decisions?\nI do understand trying to figure out things on the resolver vs balancer.. Ah! We've been looking at that sort of use case as well. I can totally understand why it would impact you. It's not that different from like Bigtable architecture.\nSome follow-up questions:\n1. How do you determine which shard to use for an RPC? Can it be semi-automatic, e.g., by looking at a field of the request?\n2. I don't quite understand your (4), since I don't know why/how some shards would be \"special\" to receive custom configuration. Unless that was also received in your (2)?. CC @dgquintas, who's been messing with this sort of design cross-language.\nSo our rough design for this would be to move all the resolution into gRPC (you own name resolver, but it is \"within\" the ClientConn) and allow the LB to make the sharding decisions.\nBasic flow:\n1. Register resolver and LB globals\n2. Client creates ClientConn with target that chooses custom resolver, with basic necessary details like service name\n3. Custom resolver resolves at least part of the details. It selects the proper LB via client config (COMING SOON) and passes any necessary details to the LB via resolver.Address.Metadata\n4. Client issues a request, along with a custom CallOption (or Context attribute, or something else)\n5. LB sees custom CallOption (or similar) and routes the request to the proper shard.\nPotential problems:\n1. Resolver and LB may be service-specific in your world. It sounds like that isn't the case, but this design sort of imagines you'd have \"a few\" such resolvers/LBs. If you end up getting 10+ it starts being awkward\n2. Some of the plumbing might still be in-progress. The client config piece in particular may not have landed. Client passing something to LB may also benefit from a more direct API\n3. This sort of stuff is complex. Maybe we're missing something.. Oooh, nice. Thanks!\nIt does look like it turned into a bit more work than I had hoped, but y'all also get a nice improvement to wall-clock duration.. As discussed in-person, we could also initially issue a warning when a too-large metadata. Later the warning would turn into an actual enforcement.. I was going to write something up that just documents the three language's behavior. I had expected that to be internal, simply because it is sort of a \"cliff notes.\" Then I thought there would be an in-person meeting between the three languages and one or two others involved with the design and we'd figure out where to go from here. We could then make the design direction public and the public could comment.\n@akshayjshah, note that for your internal codebase I'd recommend just relying on grpc-status and grpc-message HTTP headers. The benefit of the details is really the repeated Any.. @akshayjshah, ah, I think I understand now. It sounds like you're coming at this from the perspective of API/implementation design because you have your own implementation; not just limited interop. Yes, the output of the conversation will be a definition of what the details means/how it is encoded when. Right now, between the three languages, we have every possible interpretation.. This is still not resolved. I would still avoid using grpc-status-details-bin with something other than google.rpc.Status. That doesn't sound that bad, as a degenerate case. Being the literal worst case, degenerating to pick_random... meh. Like, I have trouble caring.\nTo change that behavior would require a larger change in how subchannels are passed to the picker. So it sounds like this PR should still be merged, even if it is expected to be replaced later.. > Presumably, the picker stores the index into its list. So couldn't we have the client channel let the new picker get some state from the old picker before swapping the new picker in?\nThe problem is that the lists will be of different sizes. Go and Java pass a list of size equal to the number of ready subchannels.\nIn addition, because there is no locking around uses of the Picker the old and new picker are used simultaneously (or rather, old calls to the old picker may still be in progress when new calls to the new picker begin). So \"get state from the old picker\" is probably not a solution like it is in C. (Instead, we would probably share data between the two, but we have a GC so that doesn't cause lifetime issues.)\n\nIf this PR only adds randomization for the case 1, not case 2, then I agree that it can go forward, since it's a strict improvement over resetting to 0 every time a subchannel's state changes.\n\nIt would randomize on both cases. Case 2 is rare; while I can agree it'd be good to start at the beginning, I don't think it practically matters. The current implementation for case 1 is dire; it degrades to all RPCs being sent to one backend. It seems weird to me to block an obvious \"this could cause an outage\" fix because of an \"occasionally we will do things in a slightly different order, but nobody will probably even notice\".. For (1), I think the answer is \"why do we care?\" It literally only matters for the very first RPC of a channel and there will always be a \"first transport\" to become ready, so the initial index is guaranteed not to matter.\nFor (3), if we always start with 0 it seems it is baaaad for a flapping task causing the name resolver to add/remove the entry.. I do admit that subchannel state flapping is a more common event, because it happens when any subchannel is closed. And that may happen for normal reasons (although... the number of reasons may be low, because MAX_CONNECTION_AGE and MAX_CONNECTION_IDLE don't make sense in rr), independent of backends coming up and down. So \"not every subchannel state change causes a name resolver address list change\".\nSo a flapping task may impact both, but we may still want the \"don't randomize on subchannel state changes\" for other cases.\nThe most obvious case of \"subchannel state changes\" is during startup. If you have 100 subchannels to rr over, when the 50th connection is made... I still can't prove that random is wrong. Also, startup already has the problem that we'll bombard the first subchannel that comes up, so any nuanced discussions of \"fairness\" are probably not worth our time. (I'm also not sure how I feel about maintaining the index when we may be iterating over all 100 items in the list every single RPC since there is only one connection ready; that's not that big of a deal in some ways, the same as random...)\nI think I'm in the same position as before: during subchannel state changes maintaining the previous index is probably nice, but it's unclear whether it would actually matter to anyone.. > In fact, you're the one who has advocated for using this approach to allow round_robin clients to discover new backends\nYou're right :-). Not MAX_CONNECTION_IDLE though. But yes, it would be normal for MAX_CONNECTION_AGE.\n\nEach time we get a new connection, we should start distributing load evenly across those connections. Otherwise, round_robin devolves to pick_random.\n\nI don't know what \"distribute load evenly\" in that situation means. I feel like you have a definition in mind that is circular. We know the load won't be even initially. It takes time to even out.\n\nI think there's a difference between saying \"we won't try to be fair at all\" and \"we'll do the best we can based on the set of subchannels that are currently connected\". The latter seems like a much more reasonable stance.\n\nThat's an overly a black and white view of the situation. Always using '0' would be the \"we won't try to be fair at all\". There's a spectrum here, and your proposed solution isn't fair in an absolute sense either as the new backends would have fewer RPCs.\nI think there's a difference between \"the solution works reasonably well\" and \"there is a more 'correct' solution.\" The only reason to care about more 'correct' solutions is because they provide reasonable benefit to users for a reasonable cost.\n\nWhile it may be that many people will not notice, I would be very surprised if this never causes a problem for anyone.\n\nIf it causes one user a problem once for one second, then I don't have enough time to care. I think what's not being communicated is how severe the problem would be and for whom.. Our servers have a minimum time enforcement policy, but not all do. The worry is that people really like setting \"really low\" values without understanding the network/load implications. For example, if the server is overloaded, you don't want the client to disconnect and reconnect. It is a dangerous tool to wield at 100ms, as it can amplify problems.. Need to think about what happens when these are 0.. Consider moving to transport.. This seems unnecessary. I don't know a case where WithKeepaliveParams wouldn't work.. This should only be used for reading. Writing does not prevent the need for keepalive ping.. Update activity after handleSettings?. This doesn't seem to follow this part of the spec:\n\nSince keepalive is not occurring on HTTP/2 connections without any streams, there will be a higher chance of failure for new RPCs following a long period of inactivity. To reduce the tail latency for these RPCs, it is important to not reset the keepalive time when a connection becomes active; if a new stream is created and there has been greater than keepalive time since the last read byte, then a keepalive PING should be sent (ideally before the HEADERS frame). Doing so detects the broken connection with a latency of keepalive timeout instead of keepalive time + timeout.. You're kidding me... we can't do better than this?\n\nAt the very least you should link to https://github.com/golang/go/issues/9661 , but.... Should we be concerned about logging so frequently (every connection)? If it fails once, it's likely to fail every time. It feels like a code smell to get into this situation.. This mutates an object from the user. Should there be some copy involved?. This is a really poor constant to export. There is no way to change it to 30 seconds or similar in the future. One option is to export the defaults. A better option is to not export them.\nTwo ways to remove the export: 1) in the transport, create a zero-filled Params{} and then call Validate(), 2) move the Validate logic to transport. I have preference to 2 because it also removes the Validate export.. nit: s/chanel/channel/. Could you make a note that this is safe since the timer will always be Reset in case <-timer.C?\nOtherwise it could deadlock. https://play.golang.org/p/4CHuWZJFwu. This continue has no effect. Ditto in next case. Remove?. s/Time/Timeout/. s/Timeout/Time/. Hmm... Won't this cause the timer to continue firing on an idle channel? It also makes the various states hard to follow. Could this just be reset to Inifinity?. The separate package looks weird to me as well. We may want to figure out alternatives.. s/fot/for/. s/keepalive_timeout/Timeout/ ?. I don't think this documentation is saying what you hope. It actually says it will always close the connection after a keepalive check. You need something saying that it only matters when nothing is received by the timeout.. After a duration of this time after having receiving nothing. (I'm pointing out the issue; my text still needs rewording). How about explaining a bit more.\n\nKeepaliveParameters configures how the ClientConn will actively probe to notice when a connection is broken and to cause activity so intermediaries are aware the connection is still in use.. Maybe get rid of the pointer? It is probably better to inline it from a locality perspective, but it would also mean validate() wouldn't need as much thought. I think it is safe today, but you are mutating a structure that is used from multiple threads but without any synchronization; it would be trivial to accidentally break that in the future.\n\nNote this also would make defaultKeepaliveParams unnecessary.. So note this prevents this goroutine from blocking permanently, but it may not free the timer. The Stop() may not process immediately, in which case <-timer.C doesn't have a value yet, but it will in the future.\nIt's probably better to either get rid of the if t.kp.Time == infinity { optimization above or check a second time here (you could have a variable, say isTimerUsed).. It seems there should be some mention of the current defaults. Here and below. Try to use language that implies the defaults may change, like \"the current default is\" and similar.. You're forgetting there is a sender of that value. The sender could keep the timer alive and prevent GC.\nNow, I happen to know that the channel is buffered, and so in this specific case the sender will be able to queue an entry and then remove the timer from its data structures and then the timer could be GCed.. Is this missing the Host header, or will that be copied from the URL automatically by go?. I'd highly suggest including 1k or so of the response body (or have a way to log it). It helps a ton when debugging failures.. Update the comment :). Is this actually enough to increase the window persistently? I'd expect there'd be some value for the \"target\" window that needs to be increased as well.. 'estimate' seems a bit of a poor name, since it is 2x the actual estimated BDP.. We found it necessary to make sure the bandwidth also increased, in order to avoid buffer bloat in the network. Buffer bloat was most severe in testing setups, but lives in the wild quite a bit as well.\nhttps://github.com/grpc/grpc-java/blob/v1.2.0/netty/src/main/java/io/grpc/netty/AbstractNettyHandler.java#L180. There needs to be something to increase stream windows as well. Initially, I'd suggest just increasing both of them.. Did you test this at all? Because there is the resp.Body.Close() line right before the if.. Why this change? \"user-agent\" isn't a pseudo header.. Renaming the function to isWhitelistedHeader or similar seems good. Tweak the documentation for the function as well.. Java does not currently have a way to set CallCredentials in ManagedChannelBuilder. So there's naturally no way it could be used when communicating with the balancer.. So go is iterating through the channels in a shuffled order?. Yes, I wasn't wanting to block this PR.. ",
    "karimofthecrop": "If you can use them, we have at least one available gopher to help out. \n. ",
    "jrossi": "You can do this now just takes a some more code: \n``` go\nfunc (r *RPC) Dialer(addr string, timeout time.Duration) (net.Conn, error) {\n        // Just some custom code to populate SRV records from dns or for testing the environment vars\n        r.RegenerateSRV()\n        for _, s := range r.SRV.Addrs {\n                host := net.JoinHostPort(s.Target, strconv.Itoa(int(s.Port)))\n                glog.V(2).Infoln(\"Connection tcp://\", host)\n                conn, err := net.DialTimeout(\"tcp\", host, timeout)\n                if err == nil {\n                        return conn, nil\n                }\n        }\n        return nil, errors.New(\"dialing failed\") // do something better \n}\nfunc (r *RPC) Dial() error {\n        var opts []grpc.DialOption\n        if dictator.GetString(\"agent.server.cert\") == \"\" {\n                opts = append(opts, grpc.WithInsecure())\n        }\n        opts = append(opts, grpc.WithBlock())\n        opts = append(opts, grpc.WithDialer(r.Dialer))\n    // filler is just that a required string that never gets used. \n    conn, err := grpc.Dial(\"filler\", opts...)\n    if err != nil {\n            return err\n    }\n    r.Conn = conn\n    glog.V(2).Info(\"Connection success\")\n    return nil\n\n}\n```\n. ",
    "dim": "We have been working on a draft implementation of the load-balancing proposal - feedback is welcome: https://github.com/bsm/grpclb\n. ",
    "zellyn": "@iamqizhao just FYI, I'm going to be unfortunately on vacation for our fortnightly sync-up, but just a quick reminder that I pulled out an \"invoker\" interface without too much trouble here: https://docs.google.com/document/d/1weUMpVfXO2isThsbHU8_AWTjUetHdoFe6ziW0n5ukVg\n. Context: at Square, we are converting from our in-house Stubby-alike (\"Sake\") to GRPC. We're starting with just Server-side, since it takes little effort to also serve GRPC. In fact, Sake and GRPC method signatures are alike enough that I was able to refactor Sake into the same interface as GRPC in one big, ugly, but mostly mechanical set of commits. Our methods now take context.Context as the first arg, instead of a SakeContext, but they still expect a Sake context embedded in the context.Context.\nWe need the following abilities in server-side per-call interceptors:\n- the ability to add things to the context, so interceptors need to return a context.\n  - our existing code (for now) still expects a SakeContext embedded in the Context. We plan to remove this once we switch completely to GRPC.\n  - traceID etc. for our Dapper-alike\n- the ability to access the certs used for authentication: we do per-method ACLs, storing usernames in the OrganizationalUnit in our TLS certs.\n- the ability to retrieve ServiceName/MethodName of the current call (mostly used in logging code, but a few code paths depend on it for eg. having two versions of the same functionality, one of which can skip some permissions checks because it's only available internally)\n- the ability to abort the call and return\n  - we do validation of some proto fields based on custom proto properties, using ServiceName/MethodName to look up descriptors. But we need to be able to return an error message if validation fails.\nIn server-side per-connection interceptors, we probably just need to be able to perform extra validation, and have access to the certs.\nOur client side is where most complexity lives, and I haven't yet had time to figure out exactly how it maps to GRPC: we read custom properties from the proto service method descriptors to figure out whcih calls are idempotent, perform automatic retries, and interact with our Global Naming Service (a sort of gslb/gns combo) to do discovery and geographic failover on retries. At a minimum for now I know we intercept all calls and read custom proto properties to set default timeouts.\nApologies: this is neither as clear nor succinct as I would wish, but I'm pressed for time right now. Happy to elaborate.\n. Right now, we have no equivalent of streaming RPCs (although we have use cases where we'd love to have them), so we haven't thought as much about that side of things.\n@jhump can probably add more thoughts, but I'd say the most important stuff would be at the start of the call. The only things that spring to mind as being per-message are validation, setting dapper-ish span ids. I'm assuming the initial context modifications would be preserved.\n. Indeed, it would be lovely if the go and java (and ruby, and...) versions used similar terminology, although there's a tension with keeping the go code idiomatic\n. @maniksurtani Fatalf is okay (and normal) in the goroutine that runs the test function, just not in goroutines started by it.\n. @bradfitz Ah, good catch.\n. Moved to https://github.com/grpc/grpc-go/pull/517 due to CLA bot :-)\n. Hmmm. This didn't change any code: tests must be flaky, or broken\u2026\n. Example: https://github.com/grpc/grpc-go/blob/master/server.go#L57\nThis is mirrored in generated grpc code with declarations like func _ExampleService_SendData_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error) (interface{}, error) {\n. Note: I'm increasingly un-up-to-date on current style now that I can't lurk the internal Google mailing lists, so happy to defer to you folks on style opinions. :-)\n. Nope, actually, now that I do a quick grep :-)\nThat one just jumped out at me because \"handler\" is one of the natural places to interpose wrapping functions / filters, which we're going to need to add as we convert from our in-house RPC mechanism to GRPC (it's Stubby-ish, called Sake). We'll probably need to wrap handlers in things that can do (a) per-method ACL checks, (b) insertion of things into the context that all our code currently expects, etc.\nOn the other hand, that srv interface{} feels a little like self / currying, as you mentioned, so it seems okay. The other reason I ask is that I've seen this pattern in our own internal code, and erred on the side of moving ctx to first position, but it made me curious.\n. It doesn't seem confusing: of course it uses a custom protobuf RPC codec: there is no public protobuf-based RPC mechanism. That is what GRPC will be.\n. +1  Docker, CoreOS, Kubernetes and many other prominent Go projects are doing this.\n. This exact thing happened to me, and it was as @dsymonds describes.\nNot that our environments are likely to be exactly the same, but for reference: We're still using godep and path rewriting. I had to track down the non-Godep-folder import of x/net/trace that goimports had inserted into my code somewhere. The simplest way was for me to rm -rf $GOPATH/src/golang.org/x/net and go build ./... everything, find the build error, then fix that import to use the Godep'd import.\n. @mitake nope - we just vendor everything we use.\n. +1 although I'd pass a func(int) time.Duration rather than an interface.\n. Yeah, after posting that I noticed that the whole clientConn is passed to Invoke and NewClientStream :disappointed:\n(Our internal stubby-alike passes a \"Channel\" interface, which is much narrower, and I got the two mixed up.)\nI think we've figured out workarounds for our use case, so I'll close this. Sorry for the noise.\n. I created a separate doc instead.\n. Fair enough. I work on a team responsible for building common infrastructure in Java, Ruby, and Go, in a company where programmers usually use more than one language, so I guess I have a different perspective.\n. @spenczar there are version checks in both grpc-go and go-protobuf specifically to ensure that the versions match. If you upgrade one, upgrade the other too.\n. At Square, we typically open three connections to each \"cluster\" (which would correspond to the \"target\" passed to the Resolver). But our naming service might return eight hosts for that cluster: we open three connections, and if one of them goes unhealthy, we open a new one to a different host in the cluster, and close the unhealthy one. So our naming service reports the full set of hosts in a cluster, and we try to keep n (usually 3) healthy connections open.\nAs written in this PR, it would be difficult: it appears that the clientConn opens connections to every host reported by the resolver.\n@jhump for more comments\u2026\n. In GRPC Java, a LoadBalancer is given all the addresses by the Resolver, but can easily decide to open transports to only a subset of them.\n. This PR also makes naming.Resolver an implementation detail of Balancer. Or is the intention to support Resolvers aside from balancing too?\n. Yeah, it makes sense to have naming be part of balancer. Just not sure about naming having its own package then - it makes it look like a first-class entity, rather than an implementation detail of balancer. But it's not a big deal.\n. @iamqizhao the interface is small enough that you could just put it inside balancer.go\n. Hmmmm. Given that GRPC started as a sort of stubby-alike, I'd think a zippy-alike compression fits in just fine. In fact, it should probably be part of the spec\u2026\n. Sent an email referencing this issue: https://groups.google.com/forum/#!topic/golang-dev/lOqzH86yAM4\n. You need to re-get the latest Go protobuf generator. protoc itself changes rarely: go get -u github.com/golang/protobuf/{proto,protoc-gen-go}\n. LGTM\n. It's a little confusing, because that comment is trying to give you a hint, if you end up looking there because of breakage, that you need to upgrade the _other_thing - github.com/golang/protobuf/proto, not the file that the comment is actually in.\n. Hmmm. No - I'd actually thought about a design where the mutable metadata\nis stored in a map, with the Address objects used as keys\u2026 I'll convert to\nthat design.\nOn Fri, Sep 9, 2016 at 6:04 PM Qi Zhao notifications@github.com wrote:\n\nBTW, the metadata part of grpc.Address is intended be immutable once being\ninitiated. Is it necessary for you to change it in your codebase?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/886#issuecomment-246053369, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AACDWYVv7HYeJV4HZGRmf1IMCyOW-ea6ks5qodfQgaJpZM4J5Z5h\n.\n. The underlying fields of sync.Mutex are being printed in the error message, which is causing a race condition. Here's a short example: https://play.golang.org/p/xDC2dwol59\n\n@tamird @lukaszx0\n. @tamird did you read the play link? %q recurses through the struct's fields as it prints an error message, regardless of the fact that I implemented String().\n. According to the fmt package docs, %q is only defined for strings and\nintegers, not structs or interfaces.\n. There should be no pointers in grpc.Address unless you add one as Metadata. I think %v (\"default format\") is appropriate.\n. @menghanl so this looks good then? Anything else I need to do to it?\n. Is there no situation in which you'd want to replace the old metadata completely? This PR makes that impossible, whereas before you could merge manually if you needed to. metadata.MergeContext would be an appropriate addition?\n. Qi, just wanted to post a quick note thanking you for all your work, effort, and energy in implementing GRPC and engaging the wider community. It was a pleasure working with you!. Edit: nevermind. Gerrit was just confusing me with its terrible usability. The code does in fact do what I expected.\n~~I'm not sure I understand everything completely, but isn't it likely that the existing server will have already performed the TLS handshake? In that case, we want the GRPC Server not to have creds, and we should try to cast rawConn to a tls.Conn, and return an AuthInfo of TLSInfo{tlsConn.ConnectionState()}~~\n. put is a strange name\u2026 It's not like you're returning the connection for re-use (like with a pool). How about done or finish?\n. Add sentence: \"It will be called { insert description of when in the grpc client lifecycle it will be called }.\"\n. When is Notify() called? When can slices be sent across the channel? Can addresses be duplicated in a slice? (At Square, we often connect to load-balancer VIPs, so it's useful to have, say, three connections to a single VIP, which round-robins between, say, eight actual backends.\n. Is there a recommended way for a Balancer to know which addresses have recently had errors so it can avoid them?\n. Called repeatedly? So, on the first call, you return a chan that you'll write to whenever the Balancer wants to notify grpc internals of changes. On the second call, you return a different chan that\u2026? Or are you expected to write to the first chan once, then close it?\n. Why not this?\nfor addrs := range lb.Notify() {\n}\n. > You can implement ur custom logic in down() handler to count the errors to the addresses.\nDoes a connection get taken down if there is any error at all?\n. s/merged/merged by appending/ or something explaining how the merging works.\n. Until I read the code and found the slice append, I was worried that it would \"merge\" maps by overwriting existing keys.\n. ",
    "proteneer": "FWIW we're working with the nghttp2 developers @tatsuhiro-t to get per-request load balancing working (which is the main use-case of client-side load balancing as far as I can tell, since we're less concerned about security since everything is firewalled off internally). Having the proxying and load-balancing done on the load balancer seems more sane in that it keeps both client code and server code simple. \nhttps://github.com/nghttp2/nghttp2/issues/566\nhttps://github.com/nghttp2/nghttp2/issues/562\n. We've been working with tatsuhiro on nghttp2, to the point where it finally has good support for layer 7 round-robin load-balancing (https://github.com/nghttp2/nghttp2/issues/562) on a per-request level!\n. Should it cause an immediate reconnect? I assume the first time it tries to reconnect it will fail, (since it probably happens in between the server restart). I'm trying to figure out where the reconnect/retry code is.\n. I'm debugging this still - I stuck a bunch of fmt.Println's in backoff() and it's only ever being called once. Even when the server's connection breaks it's not automatically reconnecting.\n. It looks like resetTransport poll only works when the client initially tries to connect to the server upon startup.\nWhen a previously working server is disconnected, it looks like notifyError() is called, which closes the http2Client's errorChan (which doesn't seem to actually do anything), and sets its state to unreachable.\nBut I'm not seeing how this is triggering resetTransport() to resume the connection.\n. I stuck a print in the select statement:\n```\n    for {\n        fmt.Println(\"polling\")\n        select {\n        // shutdownChan is needed to detect the teardown when\n        // the ClientConn is idle (i.e., no RPC in flight).\n        case <-cc.shutdownChan:\n            fmt.Println(\"shutdown chan invoked!\")\n            return\n        case <-cc.transport.Error():\n        fmt.Println(\"transport error detected\")\n\n        cc.mu.Lock()\n        cc.state = TransientFailure\n        cc.stateCV.Broadcast()\n        cc.mu.Unlock()\n        if err := cc.resetTransport(true); err != nil {\n            // The ClientConn is closing.\n            grpclog.Printf(\"grpc: ClientConn.transportMonitor exits due to: %v\", err)\n            return\n        }\n        continue\n    }\n}\n\n```\nIt looks like shutdownChan is closed immediately, causing transportMonitor() code to return. Something is calling ClientConn.Close() immediately.\n. This was my fuck up.\nI deferred a conn.Close() prematurely. What was unexpected was that the connection was still semi-working even after I call ClientConn.Close() it, hence the confusion.\nClosing for now.\n. looks like I set the haproxy client timeout to 50seconds on haproxy, which is exactly the interval in which this error happens. closing.\n. I saw this comment:\nhttps://github.com/golang/protobuf/blob/master/jsonpb/jsonpb.go#L110\n// TODO: proto3 objects should have default values omitted.\nWhich seems to suggest that the behavior is intended to be similar, eg. a PB generated message that looks like\ntype Bar struct {\n    Foo   int32  `protobuf:\"varint,1,opt,name=foo\" json:\"foo,omitempty\"`\n}\nstill won't serialize out the \"foo\" field if Foo == 0 (the default value of int32)\n. Thanks - I'll wrap around it.\n. ",
    "flyingmutant": "@iamqizhao sorry, what is the status of this issue? Is there any code/documentation I can read?\n. ",
    "cstockton": "@iamqizhao I tried to dig through this thread and PR #690 but it's not clear what the status of this is? I read the points from @stevvooe and I very much agree with all of them. I was just digging through the godoc and I'm a bit surprised at the design of this, it doesn't seem to fit with the rest of GRPC.\nMy experience is as follows:\nI have two GRPC servers at an endpoint, I simply want to load balance roundrobin between the two of them. If one doesn't respond, remove it from rotation for a backoff period. I search the page for load balance, cool I find Balancer. I look at the interface and say, ugh, woah, this must be a low level primitive and has common factories with the Opts() style like the rest of the package for common configuration else where. I figure there must be a way to create a balancer using one or more already-configured clients, or a []string of targets that share a group of Opts. I look for things that return a balancer, and I see RoundRobin. NICE! Then I see it takes a naming.Resolver that is in a separate package.. I understand the purpose of it after looking at the source, seems a bit overkill for my use case, but okay. \nI head back over to take a deeper look at Balancer to see if I can short cut implementing the resolver, and I'm just met with more confusion. None of the API resembles the GRPC API client, I don't see any correlation to the client API I am familiar with Dial(target string, opts ...DialOption) (*ClientConn, error) other then the Start(target string, config BalancerConfig) error)  having a target string, which has a different signature than dial as it contains the BalancerConfig which seems like a stripped down version of the Options, providing credentials for the entire balancer? This design constraint prevents using various GRPC backends that may have slightly different connection options or even authentication means cross data center, different creds, maybe different client cert auth vs api key, etc, maybe you are migrating company wide authentication standard but some datacenters have less resources then others. These are real world scenarios that people end up with unfortunately despite arguing how \"correct\" it is, the last person to blame is the guy stuck trying to implement it (we have all been there).\nSo as someone who just wants to balance requests to 2-3 GRPC basic servers using DNS with some fault tolerance if one goes down.. I feel like I would be implementing all of the logic for load balancing, under the implementation constraints of another authors design in multiple areas. This tax is going to be paid for everyone who is just trying to experiment or use GRPC in a basic POC. My best option to move on is to create a small pool of clients and do my own round robin in a 20 line struct with client := Obtain() defer Recycle(client).\nDoing this though I miss all the value of your efforts for the nitty gritty and edge cases. Which let me take a moment to say it's fantastic the API is so robust and has been carefully engineered. Im not saying the design is wrong, I am not familiar enough with the code base to make that statement. I'm just saying it's too intimidating to users, people like me.\nAll I really want to do is:\n```Go\nprimaryConn, err := Dial(...)\nsecondaryConn, err := Dial(...)\nrrConn, err = Balance(WithConn(primaryConn), WithConn(secondaryConn))\n// WithDial for cohesion with API\nrrConn, err = Balance(WithDial(target, WithCredentials()...), ..)\n// POC is running, I want to have a separate DC as a lower weight\ndc1Conn, err = Balance(WithConn(primaryConnDC1), .. WithWeight(n))\ndc2Conn, err = Balance(WithConn(primaryConnDC2), .., WithWeight(y))\nrrConn, err = Balance(WithBalancer(dc1Conn), WithBalancer(dc2Conn))\n// POC is running, I want to know when endpoints are down for monitoring\nnotifyFunc := func(... notify info ...) \nrrConn, err = Balance(WithConns(conns), WithNotify(notifyFunc))\n```\nBasically that is what I expected given the fluidity of the rest of GRPC, it covers the basics for 99% of the small teams, quick POC's, etc. Good work over all on GRPC though, don't take this as negative feedback please, just trying to provide a perspective that is difficult for a maintainer, being a dumb user.. I just recently ran into this- I'm curious if any code today would break if CloseSend actually \"Closed\". I've only seen CloseSend in defer statements in all example code, so maybe having CloseSend cause this goroutine to exit would be safe?. @dfawley That is exactly what I was thinking, because CloseSend is on \"Recv\" only API's and it was also the only \"Close\" related function I could find. I posted my closing thoughts https://github.com/grpc/grpc-go/issues/2015 there. What it boils down to is as a client I want to be certain of one thing when I exit my call frame: request resources are released. I guess that just isn't possible without receiving all items, which I feel is unacceptable but I don't expect everyone will agree.. > In summary, CloseSend() does not end an rpc call, and thus will not release the resources related to the stream. To cleanly end a rpc call, one must use one of the four ways as described here.\nI think I understand how it works now, but feel it is unexpected for a goroutine to stay running after calling a function that has the name Close in it. Having so many conditions for a stream to be closed doesn't seem right already, but adding to this CloseSend not actually closing anything- but rather \"Asking politely\" to stop is probably the main issue here.\nI'm having trouble understanding the benefits of the current design. When does someone want to ask the server to stop sending messages, but not terminate the connection? I guess my point here is based on what I've read the purpose of CloseSend is to indicate the client no longer wishes to receive messages by asking the server to stop sending. Since it is the clients intent to no longer Recv, that intent should be immediately honored by closing the connection. . > You may have some misunderstanding here. CloseSend() means client will no longer send message to server, not asking the server to stop sending messages, and client will be willing to receive messages from server. It only stops local sending.\nYes I was misunderstanding, this is very surprising to me. The api I am calling https://godoc.org/github.com/osrg/gobgp/api#GobgpApi_GetPathClient doesn't even support sending messages. I assumed \"send direction\" would be the server since no methods are exposed for me to \"Send\" anything, I only call the \"Recv\" function so it seemed reasonable to think the server is calling a \"Send\" function since the documentation also says:\n\n // CloseSend closes the send direction of the stream. It closes the stream\n// when non-nil error is met.\nCloseSend() error\n\n\nI really want to avoid exiting a function without control of the running goroutines, instead explicitly free resources before returning to callers. Currently that is impossible without draining the entire channel but that is just unacceptable, there can be tens of thousands of items. It potentially opens up a denial of service vulnerability if a caller can find a code path that queues such a large stream and immediately disconnect. Currently my only work around is to increase the cost of a user request by at least 2 goroutines (depending on current ctx) and use the the stream.go with context.WithTimeout - but I still have to exit that function scope with the \"cleanup\" goroutine started by GRPC live and hope it exists timely.\nHaving to think so much to free a simple network resource is a good sign it just needs an io.Closer in my opinion. Then I can put in a defer after my connect statement and break the recv loop as needed. Since I assume the interface is frozen what about implementing io.Closer on stream? Then I can do the below and return to caller knowing all resources in that call frame are free:\nGo\nfunc streamClose(sr stream) (err error) {\n    switch T := sr.(type) {\n    case io.Closer:\n        err = T.Close() // compiled with grpc that supports io.Closer\n    case GobgpApi_GetPathClient:\n        for err == nil {\n            _, err = stream.Recv() // recv messages for no reason >_<\n        }\n  case ...:\n    ...\n    }\n    return\n}\nstream, err := pkgthat.ReturnsStream(ctx, PkgRequest{...})\nif err != nil {\n    return nil, err\n}\ndefer streamClose(stream)\n. @dfawley That will fully terminate the stream eventually but not when the function exits. Right now it is impossible to ensure that the resources created in the call frame have been freed before returning to the caller.  Even if you give the stream a context and cancel it before even exiting the function AND fully drain Recv until an error, the goroutine GRPC started is still active when you return and keeps the entire object live.\nI don't think this is acceptable and feel this API lacks the single idiomatic method that exists for this exact purpose: io.Closer. I've explained this adequately so if the GRPC authors want to agree to disagree that is okay, thanks for clarifying the implementation regardless and feel free to close this issue to indicate you won't fix.. @MakMukhi Thanks, though I am concerned that there is invisible buffering on something that has a primitive IO interface, i.e.: zero controls or documentation around how it buffers messages internally. How and when does it create back pressure? Since buffering indefinitely must not happen here as it would be a security issue (immediate potential for a denial of service vector) it has to block at some point. Performance is not being increased via the current API, the cost is being hidden in exchange for safety and reliability by removing. An exchange that I believe goes against a core tenet of high quality library design in Go: block callers by default.\nI understand that not everyone may share this opinion- but I believe their is only one path forward: API must be blocking. Maybe an example underneath with a few lines of code to get the identical (user facing) behavior that GRPC has today would be a better alternative:\nGo\nch := make(chan Thing, HowBigIsThis_NobodyKnows) \ngo func() {\n    stream, err := grpc.New...\n    for {\n         select { case <-ctx.Done(): ...; case thing := <-ch: stream.Send(thing); }\n    }\n}(). > The buffering is not indefinite, every time a message is sent it takes away from a local soft limit of 64KB which is then replenished as bytes are written on the wire. \nSo depending on the size of the message thousands of messages may be lost in a short burst? I changed my request counts to find the maximum message loss for my use case is 2800, so I can infer around 2800 messages fit in this 64k buffer.\n\n2048 delivered 35-43 consistently for a loss of 2000 messages.\n4096 delivered 1350-1390 consistently for a loss of 2700 messages.\n8192 delivered 5480-5490 consistently for a loss of 2800 messages.\n\n\nThis also provides user back pressure and prevents them from accidentally OOMing their local machine. Safety on the send side it is about preventing users from accidentally OOMing themselves and not protecting them against a malicious DoS.\n\nI am glad that there are defined bounds- but to be clear you are not naming this as a feature of the current design right? Because the same would be true if there was no buffering at all.\n\nAll of this is internal implementation and therefore not documented in the API. Also note that send and recv are not primitive calls they do not act on the underlying connection.\n\nThanks for helping me understand the original rationale for not documenting the fact the library could drop 64kb of user messages. I think at this point we can agree this is not an internal implementation detail- but user facing since it has been noticed by one.\n\nBlocking on a write call until it has been been written on the wire has problems like the following:\n\nBlocking on a write does not mean blocking until success, it means blocking until the most immediate definite outcome. In your example as soon as the connection is lost you have an outcome (broken pipe, io.EOF, whatever)- so return it. Leave the decision to retry 20-30 minutes in a tight loop to the very capable programmer utilizing the library. \n\nNote that net.Conn Write doesn't take a context that could timeout.\n\nIt has deadlines, which are set by context.WithTimeout and context.WithDeadline. If the current API faces challenges to crossing context and net boundaries then I would be happy to help solve those. But I think Go developers are very accustomed to setting context deadlines as well as having some separate (often constant) values for maximum time to wait for network deadlines for background work.\nI don't feel sufficient technical merit has emerged here to justify the current design, nor will it given how well traveled this area of design is. I believe it should be the number one priority of the rpc to provide strong guarantees around message delivery, but I understand if the grpc authors will not concede here. I'll keep an eye out on this issue in the future, for now I'm switching to the equivalent non-streaming API. Thanks!. >> Blocking on a write does not mean blocking until success, it means blocking until the most immediate definite outcome. In your example as soon as the connection is lost you have an outcome (broken pipe, io.EOF, whatever)- so return it.\n\nThat's the problem; the underlying net.Conn write call won't return for 20-30 mins until it realizes the connection is not reachable any more.\n\nIt will only block for for 20-30 minutes because there is no deadlines, the entire point of my post was to add them.\n\nThe only way to set deadlines on net.Conn is through an API call which would mean that we set and reset the deadline for every single write call we make.\n\nThe streaming API made the unconventional design decision to bind ctx to the Stream object and not the calls that actually block, like Send. Which means you don't need to set a deadline on every single write call, because the context is only received once. If you multiplex onto a pool of streams you would just have a baseline timeout and retry when deadline was exceeded but ctx was not done.\n\nAlso, the whole premise of this issue is about client canceling the context without reading status back from the server. Note this is not an expected usage of gRPC.\nSo if one were to follow the correct usage pattern, the safety check to prevent OOMing remains opaque to the user.\n\nWhat you are telling me is that this is unexpected usage:\n```Go\n    http.HandleFunc(\"/bar\", func(w http.ResponseWriter, r *http.Request) {\n        stream, err := client.GetStream(r.Context())\n        if err != nil {\n            http.Error(w, ...)\n            return\n        }\n        defer stream.CloseAndRecv()\n    for _, req := range reqs {\n        if err = stream.Send(req); err != nil {\n            http.Error(w, ...)\n            return\n        }\n    }\n})\n\n```\nI disagree, I believe your API has an unexpected design that is a risk to users data. It introduces a type of subtlety that is extremely difficult to track down. Send doesn't send anything. It queues an object to maybe be sent, maybe not. The SendMsg func obtains some buffer allowance in Write(concerning, please see [1]) before calling controlBuf.put which does little more than wrap a mutex around an unbound enqueue to a linked list and potentially wake a waiting receiver, essentially reproducing channel semantics. I can't think of a reason why you would do this other than specifically wanting an unbound queue.\nEssentially it is UNSAFE (if you care about messages getting delivered) to use the streaming API with a context that can be canceled, which.. is most any context in use by any go program I've ever seen. That is why context is used. But if you feel it is more reasonable is for me to give GRPC stream a background Context() and monitor my request context until it's done, at which point I stop sends and call \"CloseAndRecv()\" than I have no option but to do so. I've never had to solve this unusual problem before, but I guess you expect users to write something like below or do you have another suggestion?\n```Go\n    http.HandleFunc(\"/bar\", func(w http.ResponseWriter, r *http.Request) {\n        ctx, cancel := context.WithCancel(context.Background())\n        defer cancel()\n    stream, err := client.GetStream(ctx)\n    if err != nil {\n        http.Error(w, ...)\n        return\n    }\n    defer stream.CloseAndRecv()\n\n    for _, req := range reqs {\n        if err = r.Context().Err(); err != nil {\n            http.Error(w, ...)\n            return\n        }\n        if err = stream.Send(req); err != nil {\n            http.Error(w, ...)\n            return\n        }\n    }\n})\n\n```\nThough that does no good for any other type of failure, panics, hardware failure, and so on. The bottom line here is that the API provides no way to ensure a prior Send has completed without closing the stream all together. This contradicts the very reason why I want to stream items, to make incremental progress towards my goal. I don't think it's unreasonable to want to be able to measure or create checkpoints at any interval of my choice as if I was writing to a file. It's like an os.File with Sync removed and now takes a context.Context and starts a goroutine running a f.loopy(ctx) writer! To ensure your data is written you have to call f.Close() or write \"enough\" data for loopy to decide to Sync for you, for performance reasons. Not unix like, Go like, or anything like.\nAnyways, if you are concerned about backwards compatibility I would be happy to brainstorm ways to fix this API while keeping performance gains you claim would be lost. But if you really believe this design is sound, feel free to close the issue I've made as much of a case as I can the rest is up to the GRPC authors.. This was a very difficult to issue to track down because there are so many moving parts when the system is shutting down which is the only time this issue surfaced. I think some of my frustration began to leak into the issue once I saw it being dismissed, I'm sorry for that. It's also disappointing I've somehow failed to show you the difficult ergonomics users face with the current API, despite multiple snippets of code that I felt were clear examples. The latest of which you replied:\n\nNo, I don't see you canceling any contexts on gRPC stream before CloseAndRecv finishes.\n\nYou have demonstrated the subtlety of the API first hand by giving me a thumbs up on this. Maybe you can come back to it when you have a clear head and look at the below snippet again. Once you notice it I challenge you face the difficult ergonomics of the library first hand by writing a version that ensures all messages are sent before the function exits:\n\nWhat you are telling me is that this is unexpected usage:\n```Go\n  http.HandleFunc(\"/bar\", func(w http.ResponseWriter, r *http.Request) {\n      stream, err := client.GetStream(r.Context())\n      if err != nil {\n          http.Error(w, ...)\n          return\n      }\n      defer stream.CloseAndRecv()\n  for _, req := range reqs {\n      if err = stream.Send(req); err != nil {\n          http.Error(w, ...)\n          return\n      }\n  }\n\n})\n```\n\nFinally I forgot to include the footnote for my [1] on Write. I wanted to make sure that there is no way in gRPC to send a data frame with no data or header? It appears that in such a scenario the get() that acts as the write barrier to create back pressure when writes are slow could be skipped, causing unbound writes to the linked list.. @brettbuddin I came to the conclusion I was failing to convey the difficulties of using this API and took no further action. I would be more than happy to give feedback on any API proposals, but don't have any desire to drive the effort. Thanks for reaching out to me hope you find a work around.. @dfawley Developers are looking for the same reasonable guarantees they find in most any language or library they use, including the Go standard library: A call to Send*|Write* blocks until a corresponding system call has completed. Under these circumstances they may use their understanding of the source and destination systems network configuration details to create delivery assumptions to determine if further protocol support would be needed to meet their SLA.\nFor GRPC the guarantee is that your message will be placed into a buffer in memory. So to understand delivery guarantees one must dig through thousands of lines of code as I did, only to come to the conclusion under normal usage there are none. Since as you stated above the conditions which delivery can have reasonable guarantees require writing unusual Go code- that is you must be aware that messages may not be delivered to write such code. Though I disagree that you can even write software to work around this, since at some point you need to have a mechanism for cancellation. Even with a separate background context- how do you eventually give up and measure the progress you have made thus far, given you have no way to check how many of the existing buffered messages resulted in a system call?\nTo summarize I disagree with the counter argument that delivery guarantees at the media or destination host layer is impossible with additional protocol support. While you are not wrong I don't see how it's related to GRPC lacking the same reasonable guarantees (1 send -> 1 host system call) provided by most any other networking library developers have ever used. I also disagree with performance benefits given that I consider performance gains that sacrifice correctness or introduce bullet-item lists of nuance (i.e. grpc stream docs) to have no merit. Though stream send requests could carry synchronization back to the blocked callers once a corresponding system call has been made with little affect on throughput capability of concurrent senders, affecting only the synthetic benchmarks of single-senders.. ",
    "pvox": "Hi All,\nI'm trying to use name resolution and lb feature of gRPC for client side load balancing. However, it is not working properly.\nBelow is the construction of name resolution factory.\npublic Factory getNameResolverFactory() {\nfinal Attributes NAME_RESOLVER_PARAMS = Attributes.newBuilder()\n    .set(GrpcNameResolutionLBConstant.RESOLUTION_ATTR, \"yeah\")\n    .build();\nAttributes attrs = Attributes.newBuilder()\n    .set(GrpcNameResolutionLBConstant.ATTR_LB_ADDR_AUTHORITY, Constant.HOST + \":\" + Constant.PORT)\n    .build();\nfinal ArrayList EAG = new ArrayList();\nSocketAddress addr = new InetSocketAddress(Constant.HOST, Constant.PORT);\nEquivalentAddressGroup addrgrp = new EquivalentAddressGroup(addr, attrs);\nEAG.add(addrgrp);\nfinal NameResolver.Listener nrlistener = null;\n\nFactory nameResolverFactory = new NameResolver.Factory() {\n    @Override\n    public NameResolver newNameResolver(URI targetUri, Attributes params) {\n        try {\n            targetUri = URI.create(Constant.HOST + \":\" + Constant.PORT);\n            params = NAME_RESOLVER_PARAMS;\n        } catch (Exception e) {\n            logger.log(Level.SEVERE, \"Error: \" + e);\n        }\n        NameResolver nrslvr = new NameResolver() {\n            @Override\n            public String getServiceAuthority() {\n                return Constant.HOST + \":\" + Constant.PORT;\n            }\n\n            @Override\n            public void start(NameResolver.Listener listener) {\n                listener = new NameResolver.Listener() {\n                    public void onUpdate(List<ResolvedServerInfoGroup> servers, Attributes attributes) {\n                        throw new UnsupportedOperationException(\"Not supported yet.\");\n                    }\n\n                    public void onAddresses(List<EquivalentAddressGroup> servers, Attributes attributes) {\n                        servers = EAG;\n                        attributes = NAME_RESOLVER_PARAMS;\n                    }\n\n                    public void onError(Status error) {\n                        logger.log(Level.SEVERE, \"onError called: \" + error);\n                    }\n                };\n                listener.onAddresses(EAG, NAME_RESOLVER_PARAMS);\n            }\n\n            @Override\n            public void shutdown() {\n                throw new UnsupportedOperationException(\"Not supported yet.\"); //To change body of generated methods, choose Tools | Templates.\n            }\n        };\n        nrslvr.start(nrlistener);\n        return nrslvr;\n    }\n\n    @Override\n    public String getDefaultScheme() {\n        return \"defaultscheme\";\n    }\n};\nreturn nameResolverFactory;\n\n}\nAlongwith name resolution, using rrlb for load balancing.\nRoundRobinLoadBalancerFactory.getInstance()\nThings are working fine when I exclude nameResolverFactory.\nCan someone help me?\nP.S. Using NettyChannelBuilder.. Thanks @dfawley . ",
    "juliusv": "@dsymonds I'm at GopherCon and I'm going to be at the Prometheus hackday meetup at 14:00 in the CoreOS room along with a bunch of other Prometheans and friends (or happy to chat elsewhere). I haven't used grpc yet, but it could be interesting to talk about this more. I wonder if @peterbourgon from https://github.com/go-kit/kit has thoughts about this as well.\nI guess this is mostly a matter of judgement. I would also really like to see instrumentation options that are less cumbersome than option a). For hooks I think it's a fair expectation that users have to write well-behaved hooks or suffer the consequences. At least that seems like pretty straightforward, non-surprising behavior. But I also appreciate being conservative about what features to add, and being really careful about not giving users any rope to hang themselves with.\n. @dsymonds @peterbourgon Sounds good - best channel to reach you (I know Twitter works for Peter)? Want to stop spamming the issue here :)\n. @dsymonds ack, thanks!\n. So we talked about this in an illustrous round of people (@dsymonds, @peterbourgon, multiple Prometheus people). A brief summary:\n- @dsymonds' concerns about hooks still hold. His worry is that misbehaving hooks will cause bad and surprising behavior. Given a mix of wrappers and frameworks, one also cannot expect that a hook would always be supplied by the final grpc user, making the problem worse. Further, such a hook would require quite a broad interface (offering all kinds of metadata) so that it would cover for many use cases, not just metrics.\n- Another mentioned alternative was to use an interceptor pattern (https://en.wikipedia.org/wiki/Interceptor_pattern), but that would suffer some of the same downsides.\n- It seemed @dsymonds was most open to an approach where metrics would be counted in a very primitive fashion by grpc itself, and then asynchronously (every couple of milliseconds or so) sent to a hook, in contrast to calling a hook in a blocking fashion whenever a request is made. This would require grpc to at least track request counters and duration sums, split along relevant dimensions. The upside would be that this way grpc could guarantee that the critical path only does very simple calculations. The downside is that everyone agreed that implementing anything more sophisticated than counters (like histograms or summaries) in grpc itself would be too complex. Further, this approach is pretty specific to metrics, and not really usable as a more general hook. The question arose whether such an approach would be useful enough at all, or if the average grpc user would still want to wrap each call then to get e.g. histograms.\nIf someone desires the latter approach, they should write a design document detailing the exact counters and dimensions needed, as well as what the asynchronous hook interface and backoff behavior should look like.\nOverall, it seemed like nobody was really happy with any of the approaches, so it looks like this is going to be a situation where you can just focus on choosing the least bad solution...\n. @xiang90 As I understood it, the worry was that the end user of grpc might not even be the one supplying the hook, such as when using grpc as part of a framework or similar. While the approach where grpc itself maintains counters is more complex, it would ensure that only predictable, fast calculations would happen in the hot path.\nFWIW, personally I agree that a normal hook interface would be the best (least bad) solution. I can see the above-mentioned downsides of it, but also think the other approaches are even nastier.\n. ",
    "peterbourgon": "Please ping me if you sit down together. I'd like to at least listen :)\n. There is no way out of this situation without introducing a breaking change. That was the understood and unavoidable consequence of opting-in to x/net/context over a year ago. We're now two point releases past context being available in the standard library. It's time to switch.. @Merovius Will type aliases be included in 1.9? If so, will x/net/context be aliased to context in 1.9? If so, what about the users who for whatever reason can't or won't upgrade to 1.9?. The point of the questions was to demonstrate that waiting for 1.9 is not a panacea.\nThe impatience stems from the fact that use of context is all but ubiquitous, and that adapting gRPC's x/net/context requires frustrating hacks that impose real and continuous maintenance burdens.. I encountered this message when using the Stackdriver Logging client library. It was very surprising, because I gave no config or param to the logging.NewClient constructor that would indicate I've given it permission to write to my console. From my perspective, the problem is that package grpclog declares and uses a global log.Logger that writes to stderr.\nI worked around it by stubbing out a no-op grpclog.Logger and swapping it in with SetLogger, but I shouldn't have to do that: packages should not log, and especially not to stdout/stderr directly; only package main has the right to decide what gets routed there. Instead, packages should take loggers (or event callbacks) as parameters, and leave the decision on what to do with the events to the caller.\nIMO, the best possible resolution here would be for each exported type that wants to log (e.g. cloud.google.com/go/logging.Client) to have a functional option that sets a Logger callback. By default, logs should go to /dev/null.. ",
    "Sajmani": "Perhaps we should consider a lossy subscription interface; something like:\npackage grpc\n// Subscribe arranges for f to be called with RPC trace records.\n// Records are dropped when the total size of outstanding TraceRecords for this subscription exceeds bufSize.\n// The total number of dropped TraceRecords is reported in f's dropped parameter.\nfunc Subscribe(bufSize int, f func(*TraceRecord, dropped int))\ntype TraceRecord struct {\n  Service, Method string\n  ID string // unique RPC identifier\n  Bytes int  // size of this message\n  Request bool  // Request is client->server, Response is server->client\n  Index int  // zero for unary RPCs, sequence number for streaming RPCs\n  Elapsed time.Duration  // since start of RPC\n}\nThe implementation of grpc.Subscribe can avoid blocking on slow subscribers by buffering up to bufSize then dropping records.  How the queuing is implemented and is up to grpc, which keeps the critical path under the package's control (in particular, we will eventually want to be able to mitigate lock contention on multiprocessors under very high load).  This externalizes all the metric aggregation and reporting; we may event want to move the net/trace integration behind this API.\nThe main question I have is whether lossy reporting is OK for the kinds of metric aggregation and reporting that applications require.  This API is not suitable for critical accounting (e.g., for billing), but it should be sufficient for monitoring and debugging.\nA secondary question is what policy to use when dropping records: drop oldest, drop newest, drop random, something else.  We don't want to get into the business of pushing filters into Subscribe; all filtering and aggregation should happen outside grpc.\nAlso: the only piece of this that's grpc-specific is TraceRecord.  The rest could be provided by a separate Go package for building in-process publish-subscribe networks.\n. Strange, I thought I did create a separate PR for just the rename of the family.  I pushed the second commit after I created this PR.  I'm still new to this stuff.  I'm happy to split out the renames into their own PR, if I can figure out how to do it :-)\nAs for Recv, I didn't realize it was present in gRPC (I thought only Sent was supported so far).  Certainly they should both be changed together.  I'll update it in another commit (then somehow figure out how to create a PR with just the rename commits).\n. Hmm, I did the renamings in branch \"tracename\" and the event log change in branch \"events\".  But I guess when I did \"git push\" to my fork, they all got collapsed back into \"master\".\n. Ok, reverted the clientconn changes for now.  I'll figure out how to push them to a different branch tomorrow.\n. I suppose it's your call whether EnableTracing governs event logging, but it's much lower overhead than request tracing, since it's not on the RPC critical path and happens for infrequent events.\n. OK!  I've managed to restore this pull request and commit the change to do event logging only when EnableTracing is true.  I believe this addresses your last comment.\n. If we need to read cc.state to compose the trace message, then yes, we\nshould hold the mutex.  But otherwise it's just holding the mutex\nunnecessarily; it seems better to avoid holding it during operations that\ndon't need it.\nOn Sep 4, 2015 4:21 PM, \"Qi Zhao\" notifications@github.com wrote:\n\nMy biggest concern is as follows (copy and paste from the previous comment\nyou probably missed):\n\"I actually tend to put the operations on cc.events (Printf, errorf,\nfinish ) along with the change of cc.state. cc.state records the state\nchange of ClientConn (and here we just need to push it to tracing) and has\nbeen protected by a mutex.\"\nIt sounds a natural choice.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/287#issuecomment-137844088.\n. Ok, I've added a new commit that records a trace event for each client conn state change.\n. OK!  Still figuring out GitHub.  I think I've addressed your remaining comments; sorry about all the back-and-forth.\n. Qi Zhao, I believe I've addressed all your comments. I'd like to present this feature when I speak on gRPC at GothamGo Oct 2. Can we get this merged soon? Thanks.\n. OK, locking is all fixed.  PTAL.\n. Updated with new commit.  Note that I've added another bug fix regarding context cancelation.\n. Any security concerns exposing args or payload in traces by default?\nWill this keep large args and payloads pinned in memory longer than users expect?\n. Done.\n. Finish should only be called once.  The return at line 418 happens when Close has already been called; we don't want to call Finish in that case.\n. EventLog has its own mutex; it's safe for concurrent access.\n. Per chat, can we leave as is? Anything else needed for this PR?\nOn Aug 25, 2015 5:24 PM, \"Qi Zhao\" notifications@github.com wrote:\nIn clientconn.go\nhttps://github.com/grpc/grpc-go/pull/287#discussion_r37921644:\n\n@@ -167,6 +173,18 @@ func Dial(target string, opts ...DialOption) (*ClientConn, error) {\n    return cc, nil\n }\n+func (cc *ClientConn) printf(format string, a ...interface{}) {\n\nrename to eventPrintf and eventErrorf respectively?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/287/files#r37921644.\n. Sorry, I'm still getting used to this UI.  I've set cc.events = nil here and s.events = nil in server.go, after Finish.\n. Ah, I see. Now that we're setting it to nil under the mutex, we need to put all the accesses to events under the same mutex.  I'll make that change.\n. Done.\n. Done.\n. Moved to trace.go\n. Changed sensitive to false.  Not sure about switching to Printf, though; stringer avoids the printf and adds no struct overhead.  But really we should probably just have a way to add strings to a trace without requiring a stringer.\n. We need stringer for LazyLog(stringer(statusDesc), true) so that we can set its sensitivity and ensure statusDesc is not interpreted as a format string.  While we could write LazyPrintf(\"OK\"), I don't see why we should, given that LazyLog(stringer(\"OK\"), false) does what we want.\n\nBut personally I'd prefer to have a Trace.Log(string) method to sidestep this whole issue.  I can send that in a separate CL if you're open to it.\n. Interesting.  trasport.Stream has a cancel field to cancel its Context, but I cannot see when it's called.  Still hunting.\nI need this change for my gRPC demo, but I can pull it out into a separate PR in parallel with this one.\n. Stream.cancel is called from http2Server.closeStream, which is called from a few different places, notably http2Server.WriteStatus.  But WriteStatus returns early if the stream is already in state streamDone, which is set in 3 places.  I cannot tell whether those other code locations lead to calling Stream.cancel, but it seems like they should.\n. David: I'd like to leave this PR as is but add a comment on these two context additions indicating that they should be removed when transport.Stream.Context is canceled properly.  I will file this as an issue against gRPC once I can pull together a repro case and link to that issue in the comments.  But I'd prefer to keep this PR as is so that the Context as seen by handlers is correct (and the demo works properly).\n. Simple is a terrible name.  What does this picker actually do?\n. Go APIs should not return sentinel values like nil.  Return (conn, bool) or (conn, error) instead.\n. What is target?\n. This should return (Conn, error) as well.\n. Simple is a terrible name.  I think what you mean here is newSinglePicker.  You should also document what it does.\n. How would server.go:handleStream get the traceInfo if it's created in http2_server.go?  Would you expose it via the transport.Stream type?  I think this refactoring belongs in a separate PR.\n. So can this remain unchanged?\n. See discussion above; I think Qi Zhao has ideas on how to refactor.\n. ",
    "purpleKarrot": "Well written, @inconshreveable. I agree to every single point. I need an interceptor/hook interface for logging, authentication, and panic reporting. I already thought about writing a custom code generator...\n. ",
    "alecthomas": "Yes, we are currently going down the second path (code generation) and it's really not ideal.\n. Just wanted to say that the interceptor interface is great, works exactly as I'd hoped. Thanks @iamqizhao . I don't think this decision is consistent with other dialing behaviour of grpc-go:\n\nWhen using a custom resolver, the WithBlock() option does not do what it describes.\nIt is the opposite of the default DNS \"resolver\" behaviour (see below).\nIt makes the default DNS resolver behaviour unique and unreplicable.\n\nThe default \"resolver\" behaves in such a way that If you pass a host:port that is not resolvable, it will return successfully, but continually fail until the name starts to resolve, except if WithBlock() is provided. This seems ideal and is not uncommon in environments with dynamic service DNS or, as others in this issue have mentioned, dynamic resolvers..\nFor a custom resolver, if I pass grpc.Dial(\"FooService\", grpc.WithResolver(resolver)) and the hosts backing FooService are not ready yet, this will error. This is a) different to how the default works b) makes it impossible to replicate the builtin behaviour via a custom resolver.\nDefault builtin \"resolver\":\n```go\npackage main\nimport (\n    \"log\"\n    \"os\"\n\"google.golang.org/grpc\"\n\"google.golang.org/grpc/grpclog\"\n\n)\nfunc main() {\n    l := log.New(os.Stdout, \"grpc: \", 0)\n    grpclog.SetLogger(l)\n    _, err := grpc.Dial(\"foobar.asdfasdf:1234\", grpc.WithInsecure())\n    if err != nil {\n        panic(err)\n    }\n    l.Println(\"dialled\")\n    select {}\n}\n```\nCustom resolver:\n```go\npackage main\nimport (\n    \"net\"\n\"google.golang.org/grpc\"\n\"google.golang.org/grpc/naming\"\n\n)\ntype resolver struct{}\nfunc (r *resolver) Resolve(target string) (naming.Watcher, error) {\n    return &watcher{target}, nil\n}\ntype watcher struct {\n    address string\n}\nfunc (w watcher) Next() ([]naming.Update, error) {\n    host, err := net.ResolveTCPAddr(\"tcp\", w.address)\n    if err != nil {\n        return nil, nil\n    }\n    return []*naming.Update{{Op: naming.Add, Addr: host.String()}}, nil\n}\nfunc (w *watcher) Close() {\n}\nfunc main() {\n    r := &resolver{}\n    _, err := grpc.Dial(\"foobar.asdfasdf:1234\", grpc.WithBalancer(grpc.RoundRobin(r)), grpc.WithInsecure())\n    if err != nil {\n        panic(err)\n    }\n}\n```\nIt seems like it should be consistent one way or the other.\n. In fact, I would suggest that this behaviour is contrary to the connectivity semantics described for the CONNECTING state (emphasis mine):\n\nCONNECTING: The channel is trying to establish a connection and is waiting to make progress on one of the steps involved in name resolution, TCP connection establishment or TLS handshake. This may be used as the initial state for channels upon creation.\n\nAnd wait-for-ready:\n\nRPCs SHOULD NOT fail as a result of the channel being in other states (CONNECTING, READY, or IDLE).. I just opened #1860, then found this. We have the same issue as @dzbarsky. . I just found #1710 and #1812 which argue for passing this information via query parameters. In the example above that would mean that every Dial would need something like the following, is that correct?\n\ngo\nconn1, err := grpc.Dial(\"etcd://foo?server=etcdhost1&server=etcdhost2\")\nconn2, err := grpc.Dial(\"etcd://bar?server=etcdhost3&server=etcdhost4\")\nThen the ResolverBuilder would need to maintain a mapping from query parameters to etcd client connections...?. I guess now that I've read the other two issues, my question is: what is the best way to solve this conundrum?. Read the naming guide. I guess it would be:\n```go\nconn1, err := grpc.Dial(\"etcd://etcdhost1,etcdhost2/foo\")\nconn2, err := grpc.Dial(\"etcd://etcdhost3,etcdhost4/bar\")\n```. No, the hosts are the etcd servers. Foo and bar are service to resolve.. At heart my question is how to use different sets of resolver hosts in a reusable way. With the old API I would just pass two separate Balancer values around. With the new API it seems I have to pass the host strings around and concatenate then with the targets?. Okay, thanks!. Though I have to say, if the authority needs a large amount of configuration, like SSL certs, passphrases, etc., having to pass all that through a URL-like string is pretty tedious.. ",
    "jhump": "Agreed that the most important part is intercepting the start of a call. However, certain metrics (like total request and response message bytes, histograms) requires seeing each message in a stream. Also, validation of messages also would require seeing each piece of a stream.\nIt would be great to have something like the interceptor interfaces in the Java implementation. Albeit those interfaces are also lacking (as of this writing) in that, for authentication and quota enforcement, we also need access to the client IP address and authenticated identity (e.g. X509 principal).\n. @iamqizhao wrote:\n\nAs an alternative, I think if you can add a wrapper on the top of the grpc generated code to switch on in-process and normal case you should probably achieve this without any changes in grpc library\n\nUnfortunately, this is not necessarily the case. The main issue for an in-process implementation is that the CallOption stuff is totally opaque. So, even were there a way to communicate headers and trailers in-process, there is no way code can actually interact with these options -- at least not without forking grpc-go and adding said code to that package so it can interact with these un-exported types.\nI brought this up on the mailing list some time ago:\nhttps://groups.google.com/d/msg/grpc-io/NOfh5ESgnyc/RgDJe5g0EgAJ\nI'm now re-visisting this issue because I've hit it again with something else I'm trying to do: a client interceptor to do client-side retries that can support \"hedging\". With hedging, the interceptor may issue a retry before the prior attempt completes -- triggered by a timeout vs. a failure. But  header/trailer metadata for only the last/successful attempt should be used for the header and trailer call options. Since these types are totally opaque, it isn't possible for an interceptor to do what it needs to do. And if it just passes along the options, without any changes, to multiple concurrent attempts, it is both non-deterministic as well as unsafe/racy regarding how the client-provided metadata addresses will get set.. > Otherwise, the team still doesn't have the bandwidth to take on an in-process transport any time soon.\nIt would be nice if the API were at least amenable to a 3rd party library supplying this. Unfortunately, it is currently not due to #1495.. There are still a couple of issues that prevent a 3rd-party package from providing an in-process channel: #1495 and #1802, \nHowever, that hasn't stopped me! Take a look at these:\nhttps://godoc.org/github.com/fullstorydev/grpchan\nhttps://godoc.org/github.com/fullstorydev/grpchan/inprocgrpc\nThe above issues do represent shortcomings though. They basically mean that call options are ignored by custom channel implementations (which means you can't get response headers and trailers from a unary RPC). And there is a similar issue on the server side: you cannot set response headers or trailers from a unary RPC implementation.\n@Random-Liu, I don't really understand why you can't use bufconn though. It is similar enough in concept and construction that if you can't use it, you may not be able to use inprocgrpc either. The main advantage of inprocgrpc is that it doesn't incur serialization/de-serialization overhead or deal with HTTP/2 framing to wire the client up to the server. It uses a channel with a limited buffer size to achieve back-pressure, instead of using flow control windows in the HTTP/2 protocol.. > This means that if the server modifies the request message, then the client would see the result of that modification\nNo, this does not happen. The library avoids serialization/de-serialization, but it does copy.. > can I make the grpc server serve both remote requests and inproc requests at the same time?\n@Random-Liu, yes. Though it does not make the *grpc.Server serve both requests -- rather it makes the actual service implementation serve both -- one via *grpc.Server and its HTTP/2 transport; the other via in-process channel.\n```go\nhandlers := grpchan.HandlerMap{}\n// if no interceptors, then can just use handlers directly as the registry,\n// since it implements the right interface\nreg := grpchan.WithInterceptor(handlers, unaryInterceptor, streamInterceptor)\n// Now register services using reg, not a *grpc.Server\n// NOTE: must use RegisterHandler, which means using\n// --grpchan_out option w/ protoc\ngeneratedpkg.RegisterHandlerMyService(reg, &svcImpl{})\n// Now you can use the registered services for both in-process and real server\nreal := grpc.NewServer() // no need for interceptor options! (handled above)\ninproc := new(inprocgrpc.Channel)\nhandlers.ForEach(real.RegisterService)\nhandlers.ForEach(inproc.RegisterService)\n```. > it may be better (if possible) to serialize and deserialize using the configured codec in the fallback case\n@dfawley, that's an interesting idea. But currently, an inprocgrpc.Channel does not have a configured codec. (That's a grpc.ServerOption, for normal HTTP/2 servers.) That is something I will happily add if the library gets users asking for it (i.e. using grpchan but not using protobuf).. @veqryn, until this is provided in the core library, try out this:\nhttps://godoc.org/github.com/fullstorydev/grpchan\nhttps://godoc.org/github.com/fullstorydev/grpchan/inprocgrpc\n```go\n// collect handlers in this map\nhandlers := grpchan.HandlerMap{}\n// Pass --grpchan_out parameter to protoc after installing protoc-gen-grpchan\n// in order to get this extra registration function:\npb.RegisterHandlerPixel(handlers, myService)\n// Now we can wire up the handlers to both a gRPC server and an in-proc one\ngrpcServer := grpc.NewServer()\ninproc := &inprocgrpc.Channel{}\nhandlers.ForEach(grpcServer.RegisterService)\nhandlers.ForEach(inproc.RegisterService)\n// GRPC Gateway\nmux := runtime.NewServeMux()\n// protoc --grpchan_out will also generate this alternate client factory:\ninprocClient := pb.NewPixelChannelClient(inproc)\nerr := gw.RegisterPixelHandlerClient(ctx, mux, inprocClient)\n``. @jellevandenhooff, that is not the responsibility of a transport; that is the responsibility of a channel. In the layers of a gRPC client, the transport is the lowest layer and just implements a single connection. The channel (in the Go package,*grpc.ClientConn`) is basically a \"transport manager\", which handles connection life cycle, pooling, pluggable service discovery, and pluggable load balancing strategies).\nTo implement custom service discovery and connection management, implement your own channel. While the current protoc-gen-go plugin does not work with custom channel implementations, you can use https://github.com/fullstorydev/grpchan for that. The channel currently must provide a separate method for unary RPCs vs. streaming RPCs (since that mirrors the API of *grpc.ClientConn, and the code generated by protoc-gen-grpchan is intentionally identical to that produced by protoc-gen-go, accept for taking a grpchan.Channel instead of a *grpc.ClientConn).. @MakMukhi, @menghanl: any discussion about this?\nThe main interest is the same thing that motivated the existence of Java's in process transport -- efficiency for dispatching calls within the same process. Note that the Java implementation of GRPC already has more flexible abstractions with Channel and Transport, whereas in Go the channel (grpc.ClientConn) is a concrete struct (and thus inhibits flexibility).\nThe transport (transport.ClientTransport) is an interface, but ClientConn provides no way to provide a custom transport factory (which could be used to push an in-process implementation into the transport layer, which is how the Java implementation works). Even if there were such a mechanism, the existing interface would need to be changed to support transfer of messages from a channel -> in-process client transport -> in-process server transport -> server without incurring serialization/de-serialization costs. (The Java implementation does this with a custom marshaller and special sub-class of InputStream that allows the marshaller to avoid/defer serializing, and then to extract the wrapped message without having to de-serialize it.). Ping. This is a pretty small change. Anyone available to take a quick look?. @dfawley, any ETA for a solution? I'm trying to decide whether to write code to work-around the issue or if I can just wait on a fix.. It is a shame that the \"default\" behavior -- code that doesn't have such boiler-plate error-checking for every call or use a custom interceptor -- doesn't just do the intuitive thing. A well-designed and easy-to-use API is one where the \"right\" thing to do is also the easiest and most intuitive (\"pit of success\").. ping: any opinions on this?. @MakMukhi, no worries. Thanks for closing the loop.. > Long-lived streams such as we use them effectively defeat dynamic load balancing. Even though there might be a fleet of servers available to service requests, our client would be bound permanently to a single specific server.\nNote that your proposed solution, a ConnectionInterceptor, would not help here since the LB could route requests through multiple connections to multiple backend servers. How would the state a server holds be distributed to others in the fleet? This sounds like it may be a pretty niche problem. Maybe you need the client to just include all of that \"state\" with every request, via metadata, or enough of it for any server that gets a request to look up all the state from a shared store (like redis or something).. Thanks to #1902 and #1904, this is no longer an issue.\nServer interceptors can create a new context with their own transport stream implementation that intercepts calls related to headers and trailers (and can then delegate using original context).\nClient interceptors can examine the call options and potentially replace them in order to intercept headers and trailers.. @ppacher, in case it helps, we get around this using some boiler-plate in the server wiring code, an interceptor, and the protoreflect library:\n```go\nmethods := map[string]desc.MethodDescriptor{}\n// this example just shows how to do this with a unary interceptor, but\n// the same approach works for streaming\ninterceptor := func(ctx context.Context, req interface{}, info grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp interface{}, err error) {\n    md := methods[info.FullMethod]\n    // stash md in a context using context.WithValue, which now\n    // now provides access to it from interceptor and handlers\n    newCtx := ctxWithMethodDesc(ctx, md)\n    return handler(newCtx, req)\n}\n// if you already use an interceptor, you can have interceptor above invoke\n// it instead of handler OR use something like grpc-middleware to combine\n// them into one...\nsvr := grpc.NewServer(grpc.UnaryInterceptor(interceptor))\nsomepkg.RegisterSomeServiceServer(svr, someServiceImpl{})\nsomeotherpkg.RegisterSomeOtherServiceServer(svr, someOtherServiceImpl{})\nyetanotherpkg.RegisterYetAnotherServiceServer(svr, yetAnotherServiceImpl{})\n// when done registering services, you can get the descriptors from the\n// gRPC server and stash them into the map that the interceptor uses:\n// (error handling omitted for brevity)\nsds, _ := grpcreflect.LoadServiceDescriptors(svr)\nfor , sd := range sds {\n    for , md := range sd.GetMethods() {\n        methodName := fmt.Sprintf(\"/%s/%s\", sd.GetFullyQualifiedName(), md.GetName())\n        methods[methodName] = md\n    }\n}\n// Now you can start svr and your interceptors/handlers can get\n// the method descriptor from the context and interrogate it \n// (like examining custom options to determine policy)\n``. @dfawley, an alternate approach that does not require changes to protoc-gen-go would be to store theServiceDesc` in the context. It's a little smelly to rely so much on values in context, but it is not out of line with how a lot of other information is already being supplied to interceptors/handlers.. > Is there an easy way to convert the MethodDescriptor back to the original proto extension?\nI'm afraid you lost me. A method descriptor is not a proto extension, so there is no conversion between. If you are asking how to get the proto extensions from the method descriptor:\ngo\nvar md *desc.MethodDescriptor\nopts := md.GetMethodOptions()\nproto.HasExtension(opts, E_someGeneratedMethodExtension)\n(Where E_someGeneratedMethodExtension would be the extension description generated into Go code from your custom option definition.). @nate-meta, my example showed that (mostly). Extensions in a proto file result in exported *proto.ExtensionDesc values in the generated code named E_<name>. So your example should result in E_SomeValueOption. You can then query it like so:\ngo\nvar md *desc.MethodDescriptor // assume we populate this for current RPC\nopts := md.GetMethodOptions() // we get the descriptor's options\n// and then get the custom option from that message\nnestedVal, _ := proto.GetExtension(opts, E_SomeValueOption)\nnestedMsg, ok := nestedVal.(*NestedMessage)\nif ok {\n  // this will print \"asdf\"\n  fmt.Println(nestedMsg.Stuff)\n}. Relevant thread on grpc.io Google Group, along with a work-around that uses WithDialer (which is not always viable, unless one also forks more logic in the default dialer, like proxy handling):\nhttps://groups.google.com/d/topic/grpc-io/yCUwuHycNWk/discussion. FWIW, I have been implementing this sort of clean-up by wrapping the client-stream and then calling the hook from either of two places:\n\nFrom clientStream.RecvMsg, when the underlying stream returns non-nil error.\nFrom a finalizer (e.g. runtime.SetFinalizer) on the underlying client-stream. (Should be safe as long as the grpc library itself does not use a finalizer on the stream it returns. This stream never \"escapes\" since it is wrapped, so calling code cannot set a finalizer.)\n\nSome sort of synchronizer (sync.Once, an atomic int flag, or a mutex) can be used to ensure the hook is only called from one of these two places.\nThe second bullet handles the case where the client never exhausts the response stream. Yes, I know it is not pretty. But I haven't found any other way to know with certainty that a client stream is over.. Yes, please! This also allows users of the libraries to implement interceptors just once instead of 2x (since the current formulation requires two separate interceptors for any cross-cutting functionality: one for unary RPCs and one for streaming RPCs).\nAdmittedly, if users want to supply unary-only interceptors in the current simplified form, it would be nice if grpc-go still supported that mechanism. That would also provide backwards compatibility with the current APIs. There should also be a way to register stream interceptors that (unlike with current APIs), are triggered for unary RPCs, not just streaming RPCs. That would also address #1494.\nA similar bifurcation exists in the server-side, which requires server interceptors to be written twice, too. If the server side behaved similarly to the proposed client-side change, then server dispatch could register a grpc.ServerStream in the context instead of a *transport.Stream, which would also be a step towards addressing #1802.. > introduce a new type of interceptor\nI think that new type of interceptor would basically look exactly like the stream interceptor.\nFor backwards compatibility, instead of a new type, I think you'd just need a new dial option (new exported method). The new method would configure the client to use a stream interceptor for unary RPCs. Or perhaps a new method that registers a stream interceptor that is used for both streaming and unary RPCs.\nI'll file a new issue to describe this feature request.. The name of this issue does not indicate that it is client-only. Is that the intent? If so, I can add a new issue for the server-side analog. I think it's quite straight-forward in the server, too, with a stream handler that does so:\ngo\n// error handling elided for clarity\nm := new(RequestType)\nreq, err := stream.RecvMsg(m)\nresp, err := svr.CallMethod(stream.Context(), req)\nstream.SendMsg(resp). @dfawley:\n1) I have a protoc plugin that generates an alternate implementation of the client interface and a factory function that takes a Channel interface.\n    * The Channel interface exposes the Invoke and NewStream methods with the same signatures as those on *grpc.ClientConn.\n    * The alternate implementation is identical to the one that protoc-gen-go emits, except that there's a different struct whose field type is a Channel, not *grpc.ClientConn.\n2) There is a function to create an \"intercepted channel\", which takes a Channel and both unary and stream client interceptors.\n    * The resulting thing still implements the Channel interface, but invokes the interceptors instead.\n    * Since there is no *grpc.ClientConn, a nil value is passed for that argument of the interceptor. Of the various interceptors I've written, none have actually needed this anyway.\n    * The code provides a valid grpc.Streamer/grpc.UnaryInvoker to the interceptor, which will proceed to the transport implementation.\n3) The server-side is similar to gRPC internals: you register a service by supplying the server implementation and the grpc.ServiceDesc.\n    * The alternate transport handles marshalling/unmarshalling metadata and request & response bytes. It then invokes the handlers.\n    * There is an alternate implementation of grpc.ServerStream that is passed into the handlers for stream methods.\nWe haven't open-sourced this stuff yet, but I have considered it. The channel implementations we have are an in-process channel and a grpc-over-http channel (for http 1.1 -- does not support full-duplex bidi streams, but handles all other kinds of methods).\nDue to the various bugs I've filed (including this one), neither can support call options, which also means that it is not currently possible to use response metadata (the server can't set it per this bug; the client can't get it per #1495 ).. @dfawley, in case you are still curious how I am doing alternate transports, it is now open-sourced: https://github.com/fullstorydev/grpchan. Also, any idea how I kick the cla/linuxfoundation CLA check? I agreed to CLA but don't know how to re-trigger the check so that one goes green.\nI am pretty confident my change could not have induced the test failures. And I cannot reproduce any of them locally. Perhaps the tests are a little flaky and I got a bad run? Nothing in this branch would have introduced anything racy: it's very straight-forward replacing of a concrete type with an interface and one spot where I moved some code from one method to another.. > The Go1.9 run says rpc_util.go needs to be run through gofmt\nDoh! My bad. I must admit: I didn't look at the details because I saw two jobs that both used 1.9.4, and it passed in one and failed in the other. (So I assumed it must have been some sort of flake.)\n@dfawley, I addressed your comments, so I think it's ready for a review now.. Ping?. > What do you think?\nI dig it. I didn't do it that way to start because I was worried about adding too much API surface area to support. But, admittedly, if it's documented that the thing implements a particular method, it's not too much more to just give users exported types to use, too.\nAs far as marking it experimental, would it suffice to just document all of the new exported types like so?\ngo\n// This is an EXPERIMENTAL API.\nOr should I also make note of it in the call option factory function (since its return type is part of the experimental API)?. > Please add some test cases in test/end2end_test.go.\nSince I didn't actually change behavior, it's unclear what tests you wanted me to add. I just added one that ensures that all call options (including those setup as default call options during dial) are correctly observed (and thus inspectable via exported types) by interceptors.. @dfawley, PTAL. ping. @dfawley, might you have some time today or later this week to take a look at this (and at #1902)?. Ping?. > The transport package is supposed to be grpc-internal-only; this would presumably expose an API for users?\nIn it's current form, it does -- the mechanism for storing a transport stream into context. But I think I can move stuff around so that is not the case (so that new exported API is in the main grpc package, not in transport).\n\nIf grpc-go natively had an in-process transport, would this still be useful?\n\nI suppose it depends. If, in the process of creating an alternate transport, the gRPC library gained better abstractions that de-couple the interfaces from the existing singular implementations, then maybe! But if there were no additional abstractions provided for custom transports, then that would only help so much.\nWe also want this for an HTTP 1.1 transport implementation. And, generally, abstracting these details so that custom transports can be used is IMO very empowering to users.\nAnother argument for something like this is to resolve #1494, to make the interceptor model generally more coherent. As is, only streaming interceptors can actually intercept response metadata (both client and server sides). But this change makes it possible for a unary interceptor to do so -- by installing its own \"transport stream\" implementation in context that can intercept metadata and forward to the real stream. (Similarly, #1902 enables intercepting metadata for unary RPCs on the client side.). I pushed a change so that new API is in the main grpc package (not in transport). This results in a change to the exported API of the transport package: no more StreamFromContext method. (If that package were not meant to be used by anything other than gRPC internals, I expect that is okay.)\n@dfawley: Does this seem better?. ping. Thank you @dfawley! I understand that it is not ideal. I do appreciate you merging it anyway.. @dfawley, @cesarghali. @dfawley, can you take a look at this? Or if there is someone else better-suited to review server reflection stuff, maybe you could at-tag them for me?. @dfawley, curious what you think about this approach:\nhttps://github.com/grpc/grpc-go/compare/master...jhump:jh/supply-service-desc-to-interceptors\nIt provides a uniform way for interceptors (both server and client) as well as server handlers to access the metadata, using the existing grpc.ServerInfo type. Since it would be a breaking/incompatible change to modify the unary interceptor signature, the info must be supplied via context and also must come from generated code.\nThe above diff shows two commits -- the first is small API addition for querying the grpc.ServerInfo from context (as well as putting it into context for server calls), and the second API is re-generating all of the protos using a modified protoc-gen-go. The change to protoc-gen-go is what actually puts the grpc.ServerInfo into context for client-side stuff. You can see that diff here:\nhttps://github.com/golang/protobuf/compare/master...jhump:jh/supply-service-desc-to-interceptors\nWhat do you think? Should I put these out as pull requests, or can you think of better ways to do this?\nI think what I would prefer (but it would be incompatible) would be if ServiceInfo were an interface, that *ServiceDesc implemented. That way, that could be used to expose the information in various contexts without the risk of user code ending up with a mutable reference to a global/package variable. It would also consolidate a bit, since it feels like there are multiple ways to supply almost-the-same information to things like interceptors or consumers of grpc.Server, just with various slight differences.\nI'll freely admit that it feels gross to stuff more junk into context. I just don't see any other way to do it and still maintain API-compatibility.. @dfawley, ping (please see previous comment). @dfawley, I do like that idea: having a registry, keyed by fully-qualified service name, could be generally useful for reflective operations around RPCs. At Square, the custom proto-based RPC we built worked this way (via a fork of protoc-gen-go, written before grpc existed so the fork also generated the service stubs and interfaces).\nThinking about it more, I believe this does indeed satisfy this issue quite well. It does not need any changes to the existing stub/interceptor signatures and does not need any more cruft stashed away in the context.\nDo you know if there is already an issue in the protobuf project about this (registry of known services)? If not, I can add one. (Seems like a protobuf project thing, not grpc-go, since that is where protoc-gen-go lives; it seems appropriate to keep the registry in the same package as the existing message, enum, and extension registries.). @dfawley, that sounds good. Nothing really urgent here.\nWe very well may need something before the v2 protobuf API is done, but we can do something custom. We already use an alternate protoc plugin to generate stubs: protoc-gen-grpchan. So we could just do something in that generated code to make descriptors available to client interceptors (likely in an internal fork -- doesn't seem worth open-sourcing if a better solution from the official protobuf library isn't far off).. @nate-meta: see the code example in #1526.. FWIW, I didn't choose to export this function in the original PR because it was not really necessary.\nYou can access the methods of the ServerTransportStream just using the context in which it is stored, by passing the context into grpc.SetHeader, grpc.SendHeader, and grpc.SetTrailer. The only other method, for the stream name, is indirectly accessible via grpc.MethodNameFromStream (by creating a dummygrpc.ServerStream implementation whose Context method returns the context in question).\nWhile not ideal, it was a way to minimize the exported surface area since it's experimental.. This is the actual fix. The rest of this PR is updates to tests and test data.. I'm guessing this test doesn't run in CI? This resulted in a stack overflow because this fmt.Sprintf call effectively recursed back into this Error() method. (I discovered this when I tried to run all tests locally, and this was the only package that failed.). Done. done. The other dial options (WithMaxMsgSize and WithCodec) use WithDefaultCallOptions under the hood. So exposing these all to the interceptor turned out to be pretty simple.. Done. That totally makes sense. The methods only made sense when relying on caller to do an interface-type-assertion to call it. With exported types, this is much better.. I updated the comments. Hopefully the wording is to your liking. If not, I'm open to suggestions :). That seems awkward. It would require that the reflection service be registered last, so that it has access to previously registered services. Any services registered with the *grpc.Server after it will be unknown to the reflection service. And if it is instead registered first, it will serve no information (which is actually how our internal libraries work: they register it before returning a *grpc.Server for calling code to add other services so that all of our internal microservices always have reflection support.)\nWhat if I change it to use sync.Once instead, so the set of descriptors will be frozen after the first real method invocation? Now that I'm staring at this, I think that would make it a lot simpler and cleaner.. Doh, good point. Fixed. Changed to use sync.Once.. Yes. Done.. Good catch! See #1948. I went ahead and changed it.\nBut I was also curious about the performance, so I benchmarked it. With the following benchmark, they are indistinguishable from each other:\n```go\nfunc combineViaCopy(o1 []CallOption, o2 []CallOption) []CallOption {\n    // we don't use append because o1 could have extra capacity whose\n    // elements would be overwritten, which could cause inadvertent\n    // sharing (and race connditions) between concurrent calls\n    if len(o1) == 0 {\n        return o2\n    } else if len(o2) == 0 {\n        return o1\n    }\n    ret := make([]CallOption, len(o1)+len(o2))\n    copy(ret, o1)\n    copy(ret[len(o1):], o2)\n    return ret\n}\nfunc combineViaAppend(o1 []CallOption, o2 []CallOption) []CallOption {\n    // we don't use append because o1 could have extra capacity whose\n    // elements would be overwritten, which could cause inadvertent\n    // sharing (and race connditions) between concurrent calls\n    if len(o1) == 0 {\n        return o2\n    } else if len(o2) == 0 {\n        return o1\n    }\n    ret := make([]CallOption, 0, len(o1)+len(o2))\n    ret = append(ret, o1...)\n    ret = append(ret, o2...)\n    return ret\n}\nvar co5 = []CallOption{Header(nil), Trailer(nil), Peer(nil),  FailFast(false), MaxCallRecvMsgSize(100)}\nvar co10 = []CallOption{\n    Header(nil), Trailer(nil), Peer(nil),  FailFast(false), MaxCallRecvMsgSize(100),\n    Header(nil), Trailer(nil), Peer(nil),  FailFast(false), MaxCallRecvMsgSize(100),\n}\nfunc BenchmarkCopy5_5(b *testing.B){\n    for i := 0; i < b.N; i++ {\n        combineViaCopy(co5, co5)\n    }\n}\nfunc BenchmarkCopy5_10(b *testing.B){\n    for i := 0; i < b.N; i++ {\n        combineViaCopy(co5, co10)\n    }\n}\nfunc BenchmarkAppend5_5(b *testing.B){\n    for i := 0; i < b.N; i++ {\n        combineViaAppend(co5, co5)\n    }\n}\nfunc BenchmarkAppend5_10(b *testing.B){\n    for i := 0; i < b.N; i++ {\n        combineViaAppend(co5, co10)\n    }\n}\n```\nOn my 2016 model 15\" Macbook Pro (2.2Ghz Intel Core i7, 1.6Ghz DDR3 RAM):\nBenchmarkCopy5_5-8                  10000000           127 ns/op\nBenchmarkCopy5_10-8                 10000000           160 ns/op\nBenchmarkAppend5_5-8                10000000           129 ns/op\nBenchmarkAppend5_10-8               10000000           159 ns/op. Good idea. Done.. ",
    "sasha-s": "In a meanwhile we are using https://github.com/sasha-s/grpc-instrument for server-side latency instrumentation.\n. Can rr.Close() be called multiple times? \nif so, close here would probably panic.\nMaybe do \nclose(rr.addrCh) \n    rr.addrCh = nil\nhere. And\nrr.w.Close()\n    rr.w = nil\n``` above.\n. ",
    "willtrking": "I wrote a boilerplate code generator which leverages go generate to easily implement the interceptor/middleware pattern on gRPC service calls. It's been of great help to me in my current project, so I figured other people might find it useful.\nhttps://github.com/willtrking/grpcintercept\n. My method allows you do to things other then metrics/logging. Generally allows you to initialize new objects (database sessions, custom auth, generalized validation, etc.) on each incoming gRPC service call, and makes them available in your gRPC service logic through a single additional input param. \nUnless my sleep deprived brain is missing something and things beyond metrics will be supported?\n. ",
    "rlmcpherson": "Is there any update or ETA for client-side interceptors? We need this as well at Yik Yak, for the reasons @zellyn described above and in his proposal document. Being able to instrument client calls is especially helpful and would also solve the problem with instrumenting the bigtable client described in https://github.com/GoogleCloudPlatform/gcloud-golang/issues/270 if the client-side interceptor is a grpc.DialOption\n. ",
    "NeoCN": "Is there any update for client-side interceptors? Without client interceptors, we have to wrap the gRPC client to do client instrument, and copy & paste method name again and again. \n. when use c client code with async method to invoke golang grpc server, there will be a problem, because there is no  async server interface, unless to modify the generated code. @dsymonds \n. get it, thanks @murgatroid99. I will give it a try. \n. @iamqizhao any updates on this issue? In our code, we want to add one common header in server-side interceptor, but with the Once-only restriction, we can not do it!\n. looking forward to see this pull request being merged\n. @menghanl Thanks, I figured out the way as you mentioned! close this issue now.\n. ",
    "troylelandshields": "@iamqizhao, this is fantastic news. Are you will to give an ETA now that you've started working on it?\n. Sorry, I misread the source. The missing flag is SupportPackageIsVersion3, not ProtoPackageIsVersion3. So I guess my whole argument is just wrong. My question still stands though, is there a solution for compiling code that imports clients from older versions?\nSeems like there isn't and if that's the case feel free to close this. Any insight you can give is appreciated though. Thanks.\n. We are having a nightmare of a time dealing with this issue also.\nHow do you recommend working through it?\nHere's an example. We have a developer working on project A who generates their gRPC code with grpc.SupportPackageIsVersion3. \nA developer working on project B imports the package with the generated gRPC client from project A. Project B also needs to generate a gRPC server but they generate it with grpc.SupportPackageIsVersion4. \nNow the project cannot build. \nSo you suggest re-generating the protofile from Project A, but this feels weird to us. Now we're changing a vendored file (the generated file) that really should be untouchable since it belongs to Project A. \nAlso, a lot of our files are difficult to generate in the vendored folder since they rely on imports that are relative to Project's A repo. \nAnother alternative is to copy/paste the code in the .proto file from Project A to Project B and have Project B generate its own client code. We're duplicating that code and separating it though, so this seems less than ideal as well.\nWhat do you suggest? This really is a huge problem for us and I don't feel like there's a great solution. If you have a single gRPC dependency you are importing then it might be fairly straight-forward, but when you have several that each have several dependencies of their own it becomes a huge pain.\n. @iamqizhao thank you for reminding me that it's not rocket science.\nMy point is simply that other non-rocket-scientist people are going to run into this issue as gRPC and Go increase in popularity. They are going to be coming here to look for answers or ask for help as well.\nObviously a lot of thought was put into the decision to add this feature so I think it was fair to assume that we were doing something wrong and you guys must have had a best practice in mind for dealing with this. Apparently the best practice is \"deal with it,\" which is pretty unsatisfactory but we'll figure it out.\n. ",
    "ahundt": "I second this request, I have data that doesn't need guaranteed delivery and instead needs delivery with a deadline, at which point the message/request should simply be dropped because a new more up to date one will be sent.\nUpdate: https://github.com/grpc/grpc/issues/9493 is the open UDP issue\n(at the time of writing 2017-07). ",
    "gazialankus": "It would be great to have a UDP option for streams. . ",
    "bkeys": "A UDP option would be awesome to have. ",
    "varshanbp": "I too Support this request. UDP option would be good.. In C# context.Host works well. Return Host Name/IP and port. @menghanl I want to get Peer Info on C# and Java. In c# context.host works. How to get Peer IP in Java ?. In C# context.Host works well. Gets Host Address/IP and Port. ",
    "hainesc": "I'd like UDP as well.. ",
    "Swoboderz": "UDP would be absolutely wonderful in boosting TCP, according to this article https://gafferongames.com/post/deterministic_lockstep/\nit would make gRPC ideal for any lockstepping systems, and likely many other systems!. ",
    "nmittler": "@dsymonds Can you take a look?\n. @iamqizhao OK sounds good.  I'l just close this then ... #243 is already assigned to you.\n. done.\n. ",
    "murgatroid99": "\"async\" is not a property of the method. It is simply the way the method is invoked from the point of view of the invoker. Synchronous and asynchronous versions of the same method send functionally equivalent data on the wire, which is consistent with either form of the method on the server side. There is no problem with sending an RPC using the async interface with a C client and handling it with a synchronous go server.\n. ",
    "awpr": "I think I have to re-create this with my @google.com email address and go through a bunch of red tape to get this merged.  So, closing for now.\n. Updated to set codes.Internal and a description.  Confirmed that an RPC to my misbehaving server still completes immediately but now returns grpc.rpcError{code:0xd, desc:\"server closed the stream without sending trailers\"}.\nI think I've confused Googlebot by shuffling email addresses between GitHub accounts.\n. I moved my google.com email address from one GitHub account to the other between the first and second commits, and it was not verified at the time I pushed the second.  The primary email address of this account is my gmail.com address.  Googlebot may have been dissatisfied with one of those two facts.\n. The spec specifically says that there is no connection between frame\nboundaries and delimited message boundaries.  Empty data frames are unusual\nbut not invalid, and should be treated as if nothing had happened.  From a\nhigher level, the spec treats the concatenated contents of all data frames\nin a stream as if they were an undistinguished stream of bytes -- that is,\nit explicitly knows nothing about where frames begin and end.  Treating an\nempty HTTP2 data frame as illegal would be inconsistent with this, because\nit would behave differently depending on on how the bytes were divided into\nframes.\nOn Fri, Jul 24, 2015, 23:44 Qi Zhao notifications@github.com wrote:\n\nIn grpc wire protocol, there is no empty data frame since it always has\ncompression flag and a length field (refer to \"Delimited-Message\" part in\nthe protocol doc). But yes, we need to deal with misbehaved peers. For this\npurpose, the right operation to deal with empty data frame and data frame\nwith EOS flag (your previous pr) should be tearing down the connection. We\nshould not keep talking to the peer when we already know it is broken. I\nwill make a pr to fix this (including the case of data frame with EOS).\nYour contribution is welcome too.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/259#issuecomment-124814002.\n. It's true that there's no way for an optimal implementation to send an empty data frame because it would always be cheaper not to send it, but it's not disallowed.  An implementation could legally send each byte in its own data frame followed by an empty data frame, if it felt like being wasteful.  I don't understand how allowing the RPC to be silently corrupted in this case is an acceptable option.\n. Also, regarding tearing down the connection in response to a data frame with EOS, it's not necessarily the case that the whole connection is broken.  This could happen if the client erroneously tries to make an RPC call to a path on the same server that's not a gRPC path (e.g. /example_dashboard_url).  In this case the server is not broken (because normal HTTP/2 requests are not required to have trailers frames) and the connection is not invalid; only that particular RPC is affected, and other RPCs on the same connection to correct paths can still succeed.\n. SGTM.\n. There is no call to CloseStream that could wake this select.  The only two calls involved in unary RPCs are in sendRequest and in Invoke; the first has already not happened if we're blocking on stream.Header() in recvResponse, and the second does not happen until after recvResponse returns.\n\nI've seen this happen every time a server sent a RST_STREAM frame with REFUSED_STREAM.  I'm not sure how easy it would be to make this happen with a Go gRPC server since I haven't looked at that code much, but if it has a configurable concurrency limit, setting it to 0 should probably reproduce it consistently.\n. Regarding retrying, the client code has a quota pool that's supposed to block requests until there's an available concurrency slot on the server, so the only time we should get REFUSED_STREAM is when the settings are out of sync, and that should correct itself relatively quickly (~ network round-trip time).  If the quota pool is working correctly, retrying immediately at the top level would actually result in blocking on stream quota until we believe the server will not refuse the stream and then retrying.\n. ",
    "willnorris": "merging this was definitely fine.  It looks like the initial commits on this PR were fine.  For example, 6246152 has an explicit OK cla status.  It looks like googlebot didn't become unhappy until later commits were added, likely 34745e1.  I still don't see any reason why that would get flagged, so there's definitely something strange here.\n. @awpr: that's almost certainly what caused it... thanks for the extra context (saves me from going off on a wild goose chase!)\n. ",
    "carl-mastrangelo": "Reverting based on other feedback.\n. And the certs in test/testdata/ ?\n. Actually, are the ones here fine?\ntest/testdata/ca.pem\ntransport/testdata/ca.pem\nexamples/route_guide/testdata/ca.pem\ntest/testdata/server1.pem\nexamples/route_guide/testdata/server1.pem\n. @jboeuf LGTM\n. @apolcyn Perhaps I don't understand this change:  It appears that this change makes the marshaller / unmarshaller just reuese the proto.Buffer objects, rather than reuse the underlying slices.  Is that correct?  IIRC it's the slices that are actually expensive to allocate, not the protobuf wrappers around them.  \nI'm curious how this improves performance.  The only way I could imagine this helping at all is when encoding proto2, which allocates new ints on the heap, and caches them inside the proto.Buffer.  Even then, it only helps with encoding, not decoding, and they disappear after a GC.  That would hurt most of the gains from this PR.  \nAm I missing something?. Adding fuel to the fire:  Java could reasonably tune GC knobs, but we mostly don't for benchmarks.  The machines that these benchmarks run on have copious amount of ram, which would let us avoid doing GC for very long periods of time.  This would artificially inflate Java's throughput, but would be outside the parameter hull of reasonable settings.. @dfawley yes, it would.  . cc: @MakMukhi . @dfawley once this is merged we can continue on the internal CL.. + @dfawley \n+ @menghanl . Looks like this only fails with recent go lint.  A few options:\n\nMove gen.go to another directory, so it can have it's own package.   This is kind of annoying.  Thx golint.\nAdd a \"// Code Generated\" comment to let the file be ignored.  This would be patently wrong, but would allow the file to live in the correct directory.\nMake the package change to not include the underscore.  This would mean the package statement in the file would be \"grpcgpc\" and would not match the directory structure.  This can be done from the go_package statement in the proto.\n\nPersonally I'd go with 1 or 2, even though 3 is likely the better long term solution.  Leaning towards 1.\n@dfawley up to you which you would prefer.\n. I moved the gen script to the nearest ancestor.  Conveniently, the common.go sibling file says never to reference the package.  PTAL.. @MakMukhi & @menghanl since Doug is out, could you take a look?. This change is needed for the same reason as https://github.com/grpc/grpc-go/pull/2028. @lyuxuan friendly ping.  I can't assign a reviewer or assignee.. @lyuxuan could you also merge?. @MakMukhi Since I can't assign or request a reviewer, could you PTAL?. @menghanl whoops, I didn't realize.  Sorry!. Same idea as #2056. @menghanl done. @menghanl done. @menghanl PTAL\nThere is now a fake grpclb used in the top level test.  \nAlso, NewLBBuilderWithFallbackTimeout is no longer public.  It is not public in Java, with the expectation of adding it in if necessary.  The reason it is removed is because it is the only public method in the package which looks weird.  Since no one is using it internally, or from searching on Godoc, I think it should be removed.  \nThere is still some followup items (like docs),  but I can do those in a followup PR.  This one is getting too big.. Reviewable is asking for overly broad permissions so comments here:\n\nHow about creating a new ctx with 2 seconds timeout before entering the loop?\n\nI had at one point, but it needs a second cancel function, and the deadline is already encompassed by the prior one.  If the call hangs (as it did in my testing) this could take 1000 * 2 seconds to complete, which is too long.\n\nWhy is this check necessary?\n\nBecause if the RPC hangs, the context will be done, but the iterator will not be 1000.\nAll other comments are \"done\". @menghanl done!  PTAL. @menghanl  updated my codegen. everything green with the vet.sh changes.  Rolling them back and then submitting.. @dfawley SGTM.  I'll Snooze this for a week.. @dfawley  Good to Go?. @dfawley I looked at those, but neither say that they have any effects.   GetState doesn't say that the state will ever go towards READY.   WaitForStateChange doesn't imply it kick the ClientConn towards READY, only that if it does change, it will eventually run.  Neither move the ClientConn towards the state I want.\n@lyuxuan Thanks!  it would be very helpful if WithBlock had some synonyms in that doc.  I ctrl-F'd through the doc for \"Eager\" and \"ready\", but neither came up.  . @dfawley Then how does pick first work?  Doesn't that LB policy imply that connections are lazily created?   . Ok so what you are saying is Dial and DialContext are always eager.  WithBlock  still doesn't seem to be what I want because:\n\n... blocks until the underlying connection is up\n\nWhich connection is this?   In the load balanced case there may be multiple.   Does it wait until failure, or success, or either?   \n\n1786\n\nIt would be great if this could be prioritized.  It was confusing to me that Dial would return an error type, but not block.  I think NewClient could return an $CLIENT, without an error param, making it clear that no connection has been established.. @lyuxuan I'll give it a go tonight.  In the mean time, is there a public doc on debugging tools (like the env vars) for gRPC Go?   I wouldn't have known that they are listed on the grpclog page, or that they existed if you hadn't said so.. leave this nil?\n. FYI Size is a fairly slow function. \n. It would probably be better to keep track of the size after encoding rather than before.  It hurts the first time, but should be amortized after.\n. return the error?\n. if err := c.writeBuffer.Marshal(protoMsg); err != nil {\n  return err\n}\n. Since you are sharing buffers, doesn't this need to be synchronized?\n. This needs to also be updated in the doc, since these flags were standardized across all languages:\nhttps://github.com/grpc/grpc/blob/master/doc/interop-test-descriptions.md\n. You can confirm by looking at the highly readable assembly produced. You might consider rewriting this without the defers.  They have become faster recently, but they aren't free.\nAlso  benchmark it.. Size is really expensive to calculate.  It may be faster to not call it and just guess.  . This can be done using sub benchmarks.   See https://golang.org/pkg/testing/#B.Run. nit: Add is a synchronous call, which may make the benchmarks noiser than they could be.  Consider calling it just once.. Errorf.. Also, for that matter: b.SetParallelism. This is racy no?  p is modified across the loop . Errorf\nIf any conditional here doesn't depend on the previous one, it should be Errorf. I don't know if it is somehow possible, but Go will check the type every time.  You might consider benchmarking without this interface punning to see if it helps at all.  If it does, you could modify the code to somehow avoid the type pun.. This will be a contention point right?   If marshalling hundreds of thousands of RPCs per second this will contend heavily.. dumb question: are codecs expected to be thread safe?  will Codecs ever have user visible state?  I know the one below doesn't but this is becoming part of public API, so it may be worth documenting how to use this more clearly.. Any reason to not embed proto.Buffer?  It would make the marshal method below a little cleaner:  cb.SetBuf(newSlice) and cb.Reset(). If a message was marshalled larger than 2^31-1, won't this be left in a broken state?  The creation of newSlice will panic on a negative capacity right?. Yes, done.. done.. Done.  Also, yes it would have shown up :/. in keeping with alts pattern (PR linked in desc) I am using pb to indicate messages, and grpc to indicate grpc stubs . Oh sorry, I referred to others in my other PR.  This is the exemplar: https://github.com/grpc/grpc-go/pull/2028. optional: In English ~ is commonly spelled tilde. ",
    "aybabtme": "I have this insane grpclog implementation for now :P https://github.com/aybabtme/grpclogrus\n. @peter-edge I guess that could be 1 implementation among many, my idea here is to use a structured logging interface instead of printf-logging. The backing implementation isn't really something important to me =P.\n. I prepared this deck about why structured logging matters, could help motivate why we'd like to see GRPC use it somehow:\nhttps://docs.google.com/presentation/d/1SnjZpcfJq9r6OpgsjsX9ExCFTQgpXsSjD--Y3PPDiAQ/edit?usp=sharing\nIt's a bit annoying that a library be logging on it's own, with its own means. At least logging to key-values would leave the choice to users to printf the stuff together as needed (and a default grpclog), or keep the key-value mappings and do nice parseable logs.\n. hi @hsaliak, @menghanl and @dfawley ! I haven't contributed to the discussion for a while, but I think the need is still very present. If we can agree on an interface, I'm happy to do the work myself.. Alrighty then! No worries.\n@iamqizhao the stdlib logger has a mutex the same way: https://golang.org/src/log/log.go?s=2250:2542#L38\n. Having structured logging somehow would be really nice. Is there another way you folks would see the concept being used by this project?\n. @iamqizhao I'm not totally following what you mean with:\n\nso that it would be easy to convert it to the desirable format in post-processing\n\nMind to elaborate? Are there APIs for structured logging that you'd prefer? \nI could perhaps, in another PR, change the many uses of grpclog in tests to use t.Errorf and t.Fatalf, which would reduce the noise we see here. Unless there was a specific reason to use grpclog.Fatal in tests?\n. @iamqizhao alright, thanks!  It would be great if that interceptor allowed setting things in the context.Context that is later sent to the rpc method.\n. Go 1.8 has been released, can this now be done? It would be pretty useful, lots of codebases are stuck with odd mixes of x/net/context and context until protobuf and grpc do the switch.. @Merovius in my opinion, what has changed is that 1.6 is now >2 versions behind.\nI think it's reasonable to only target the 2 latest versions of Go, i.e support 1.7 and 1.8 and drop support for 1.6, which is what most projects aim for. Good rationales about why 2 versions back is a sweet spot have been provided in the comments above.. Meanwhile, I think the problem is that we're optimizing for:\n(1) people who want to use old Go versions but compile against recent gRPC versions.\nInstead of:\n(2) people who want to use new Go versions with recent gRPC versions.\nI think that by deciding to be \"safe\" and using old Go versions, people in the (1) group should bear the consequence of also being stuck with older versions of gRPC. We shouldn't keep everybody else  stuck with odd mixes of context to satisfy a group that chooses to not upgrade. That's at least, my point of view.. I see what you're saying, but breaking interface changes have happened before in gRPC. Arguably it was for features marked as experimental in the godocs, but still. The issue is only going to show up when people pull in a new version of the grpc-go package. There's been usage of tags before, maybe make a new release before dropping support for x/net/context. Otherwise, the status quo is to stick with an old package until some eventual proposal maybe get implemented in Go 1.X, which seems like not a great idea to me.. There's no guarantee that type aliases will make it in the language.. Cool sounds good to me!\n. This new log line means that all existing code has grpc emit logs when it dials, and along with line 56   all existing code that dials with a host or ip and no specified scheme gets a second log line about the lack of resolver for \"\" scheme.\nThis is a problem because grpc is used in CLI clients and having grpc log these internal, unimportant details makes the UX of these CLIs confusing; random logs are showing up, suggesting that there's connection errors when there aren't.\nIt would be good if these two log lines could be removed, or reduced to a debug level.",
    "zj8487": "ok, it will in my later post!\n. # Shortage of abilitys for grpc\n- Checking the connection on client and server end\n- Marking the connection on client and server end\nChecking the connection on client and server end\nWhy need the ability?\nBoth client and server should know the status of the connection to the other side, so that the each end can do some work tor it. for example, reconnect, destroy the resource.\nmaybe you will say we can monitor the service status by zookeeper, etcd. yes, you are right. but it will not  work on mobile client.\nWhat is the status of the connection?\n- heartbeat timeout\n  grpc should supply the ability to config the heartbeat. the options is as follow:\n  - enable the heartbeat detect ---if no, no heartbeat package will send to detect the connection.\n  - the interval for the heartbeat package----send the heartbeat package every interval\n  - heartbeat timeout time----if no package received in the time, heartbeat timeout hook will be triggerd\n  - handshake between client and server. the client should detect the connection like server. but should use the same config as above used in server side. so handshake is needed to sync the config.\n_if the handshake is possible to sync the custom config is better_\n- connection lost for reason, ETIMEDOUT, EHOSTDOWN, EHOSTUNREACH, heartbeat timeout(detect by above)etc.\n  if the event of connection tiggerd, the hook will trggered\nMarking the connection on client and server end\nWhy need the ability?\nIn general, we can not give every connection a hook. so we should know the owner of the connection in the hook.\nfor exapmle, if a client logon by a login rpc function, and then we record the userId or user object on the connection. and then, we will know who is lost in the hook function. \non the client side, it will be very useful when there are more than one connection. \n@iamqizhao \n. @iamqizhao \nWhy does i care connection on server side?\nScene: \n- some mobile client connect to a grpc server.\n- there are some different state for every client on the server.\n- the server should destroy the resource and clean the state of the user lost connection.\nHealth checks\n\nHealth checks are used to probe whether the server is able to handle rpcs, but it can not check if client is alive.\nit is not automatic, it need user layer to start a timer.\n. @iamqizhao \nHealth check will satisfy the need\n. +1 for peer info\n. +1 for access peer info and connection info in near future\n. \n",
    "joseAndresGomezTovar": "I have the same requirement, because:\n- I want to run a DLL remotely\nScenary:\n- I have a main process and tow dlls\n- one of these dll will be run using the gRPC server\n- By making a new dll \"gRPC client\" that is loaded in the main process, I remotely connect the original \"dll\" with the main process\nThe gRPC server loads the \"original dll\" when the connection is established, and it downloads it when it breaks\nthank you in advance. ",
    "beldpro-ci": "Is there work being done in this area? Is it possible to track it somehow if true? \n@joseAndresGomezTovar what was your workaround in this case?\nThanks!. Hey, what's missing here aside the .travis.yml rebase?. ",
    "miekg": "On the whole this looks good and something worth having IMHO.\n. Does this need to be exported?\n. closing dot.\n. There is a mismatch between the doc and the name 'Errored' vs Erred. Maybe just call those Error? Although that might create confusion with Error() of the error interface.\n. doc is wrong\n. do these interface serve a real purpose?\n. Ah, you use the type here. disregard my comment above about exporting this type.\n. How about just monitoring on the fullMethodName? You don't need to do this split and the server|clientRpcMonitor struct will be one member smaller.\n. ",
    "raliste": "+1 for updates here\nActively using grpc at preyproject.com\n. We are seeing the same error, but in our case is rather stable for messages larger than 5 MB even with:\ngrpcOpts := []grpc.DialOption{\n        grpc.WithInsecure(),\n        grpc.WithDefaultCallOptions(\n            grpc.MaxCallRecvMsgSize(64<<20),\n            grpc.MaxCallSendMsgSize(64<<20),\n        ),\n    }. Just out of curiosity, why is the go implementation different than the Java implementation? Scaling replicas up with grpc-go fails miserably as the subconn map is not properly updated. The Java implementation works flawlessly.\nOur main issues are:\n\nWhen all subconns are in transient failure, no resolution is requested, so we depend on the 30 minute interval. . Thank you for your answer @menghanl \n\nWe were actually testing using and old version. After upgrading we saw good resolving but it appears than when all subconn go into transient failure no re-resolve is done.. grpc-java did implement name resolution if all subconns go into transient failure.\nhttps://github.com/grpc/grpc-java/pull/1591. @menghanl thank you. The actual problem is that when all subconns are in transient failure no re-resolve is performed. I understand that a re-resolve is done whenever a subconn goes into transient failure, but in our case, whenever all subconns go into transient failure (at once), no re-resolve is performed. The subconn map is only updated after 30 minutes.\nrpc error: code = Unavailable desc = all SubConns are in TransientFailure\nI think the problem is that you do re-resolve (one time), but you don't keep re-resolving until a new subconn is retrieved.. ",
    "rolandshoemaker": "@iamqizhao has this been depreciated by the server side interceptor currently being reviewed internally + proposals for client side interceptors?\nThe Boulder team at Let's Encrypt is currently working on moving away from our current RPC implementation in favor of gRPC but the lack of exposed metrics hooks is slowing us down somewhat (one of the reasons for moving to gRPC was to get rid of a bunch of non-CA code we had to maintain, including various hacks to collect client and server side metrics which we'd rather not re-implement).\nWe'd prefer to use something native to grpc-go rather than writing another set of wrappers/using additional code generators and are wondering if there is a prospect of this being merged or if the other proposals have superseded it (and in either case if you have a ETA, even if it is likely to shift).\n. ",
    "neild": "LGTM\n. ",
    "michael-berlin": "For the time being, you could extend the error message and let people know how to fix it or point them to this issue.\nFor the record: Due to the breakage, tests will fail with \"grpc: no transport security set\".\nEven though this is a small hiccup, we're super happy with the Go support for gRPC. So keep up the good work :-)\n. I agree. There's always a limit somewhere.\nThe biggest problem here was that I didn't get any feedback at all from gRPC. One solution might be to ignore the original (too big) error and respond with an internal error instead which clearly states the problem? That internal error could also contain a truncated version of the original error.\nThe other alternative would be to send the original error, but truncate it. However, that may corrupt data when the error is more than just a simple string.\n. Thanks for the fix :-)\nWhich assumptions should we make about size limits in the Go gRPC implementation? Should we stick with the 8 KiB limit from the gRPC specification or is it safe to assume that there aren't limits on gRPC error messages anymore? (At least, when using the Go implementation.)\n. Travis probably already sets the GOPATH for you?\n. Alternatively, just symlink here?\nln -s $TRAVIS_BUILD_DIR $GOPATH/src/google.golang.org/grpc\n. ",
    "stanley-cheung": "Got it. Re-arranged the helloworld folder structure. Can you see if that makes sense now?\n. Error gone\n. I was just copying the files from the grpc-common repo. Didn't make any change.\n. ",
    "aaijazi": "Two points:\n1. I highlighted an example in the linked PR, where I'm seeing an rpcErr even when it's not returned from the server. It's returned by a client/connection issue, which is preventing the RPC from ever reaching the server. I would like to differentiate between these two classes of errors. For example, we have an application that would like to reconnect/retry for client errors, but not necessarily for server issues. \n2. It would be much easier to use if we could look at an arbitrary error, and determine \"was this error returned by the server?\". Is there a downside to provide such a function? It makes the client library implementation cleaner, because we wouldn't have to keep track of which errors could have come from the server vs other issues.\n. Note that this doesn't fully solve the issue. There are some client/connection problems that are still rpcErrors. For example, one thing I've noticed: if there's a closing connection, it can be returned as an rpcError (probably due to Invoke calling toRPCErr when returning an error).\nThis can lead to an error that has a string like: rpc error: code = 2 desc = \"grpc: the client connection is closing\"\n. I would advocate against truncating the Trailers field. I believe it should currently be possible to send other structured data through the Trailers field, not only errors. \n. ",
    "sigmonsays": "+1\n. no worries, I have no issue making a pickfirst load balancer.. ",
    "floridoo": "That fixed it. Thanks!\n. ",
    "gdm85": "I had problems with glog (namely the fact it requires its own CLI flags), so I worked it around by using a shim: https://github.com/bookerzzz/glog\n. (Tried with 1.5 and 1.7 here)\nI was trying to arrange a PR for this issue, adding an example similar to @ryszard's code and based on the greeter example, but I get:\n2016/08/27 13:31:00 transport: http2Client.notifyError got notified that the client transport was broken write tcp 127.0.0.1:45990->127.0.0.1:50051: write: connection reset by peer.\n2016/08/27 13:31:00 could not greet: rpc error: code = 13 desc = transport: write tcp 127.0.0.1:45990->127.0.0.1:50051: write: connection reset by peer\nSo I don't think ServeHTTP is usable at the moment (I tested only without TLS), the best pick would be @soheilhy's cmux for the time being.\nI guess next step would be to add proper servehttp_test.go tests to cover the malfunctions?\nFor anyone interested, the server-side is here: https://github.com/grpc/grpc-go/compare/79b7c34...gdm85:master\nThe client-side greeter example can be used to test this.\nMy end goal would be to have HTTP 1.x health checks support (e.g. /status replying with a 200 status code in case of healthy service and 5xx otherwise) and eventually human-readable HTML, which I achieved using cmux in this POC: https://github.com/gdm85/grpc-go-multiplex\n. > Can you run the server with Go 1.7 and env GODEBUG=http2debug=2 and report what it says when grpcServer.ServeHTTP fails?\nThe client-side shows:\n2016/08/27 23:31:57 http2: Framer 0xc420196180: wrote SETTINGS len=0\n2016/08/27 23:31:57 http2: Framer 0xc420196180: wrote WINDOW_UPDATE len=4 (conn) incr=983025\n2016/08/27 23:31:57 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: write tcp 127.0.0.1:45945->127.0.0.1:50051: write: broken pipe\"; Reconnecting to {\"localhost:50051\" <nil>}\n2016/08/27 23:31:57 could not greet: rpc error: code = 14 desc = grpc: the connection is unavailable\nexit status 1\nHowever, if you look at the server I'm using (https://github.com/grpc/grpc-go/compare/79b7c34...gdm85:master), I might say that this is not enabling HTTP2 at all before serving gRPC. I have already tried adding http2.ConfigureServer(srv, &http2.Server{}), same result (thus this line is not there right now). This is reinforced by the fact no extra debug output is generated by this server (thus http2 is probably not used), while I can see it with the vanilla greeter server.\n. > It did have tests. I'm curious what could've regressed.\n@bradfitz I had looked at your PR (https://github.com/grpc/grpc-go/pull/514) before commenting here, I understand it's a refactoring step, but I see no explicit test for .Serve() as used by me/others (or .ListenAndServe(), would be equivalent) - maybe that's the missing test?\nSorry if my mention of tests was confusing, I didn't mean any regression happened but just that the bug/missing feature could be added by first writing the (failing) test for it (as a work-in-progress PR), then adjusting the code until test passes.\n. @c4milo can you also check the example I mentioned above (basically @ryszard's adjusted code)? Does it seem correct? @bradfitz had mentioned he needs to make this work, so I think nothing new here to report.\n. @bradfitz tags make sense if you have a reported version, releases and a CHANGELOG, which I see you don't. So maybe one should first politely ask for those.\n. @bradfitz wow. That means versioning software is such an useless flick. How could I not see this before :wink: Thanks anyways.\n. Dup of #562.\n. ",
    "chancez": "Id love something like this. We have a lot of common logic between our current RPC methods, and a lot of it could be lifted out into something that gets called before the handler. I'd like to have an easier way to put things into the context object, and this could help.\n. @peter-edge Yeah, that's something I could do, but it's exactly what I don't want to do.\n. @codefx9 I would look at the tests. This is the diff where unary interceptor got added. The line I highlighted is a simple one that causes all rpcs to return an error: https://github.com/grpc/grpc-go/commit/c321387fd984d6082a3b5a1d603a18e51ed04212#diff-f49ccd532c52633a095dbddbde772107R1699\n. Why can't you use context for this?\n. It might be useful to provide a snippet of how this is accomplished without this PRs changes. Much like net/http chaining multiple handlers is implemented by the user, rather than the library. I think it might make sense to have it work that way here, but seeing a comparison of the two would be good.\n. ",
    "sr": "FYI I open-sourced a small package that includes a protoc compiler plugins for generating instrumented handlers:\nhttps://github.com/sr/grpcinstrument\n. ",
    "rschmukler": "Any updates on this? I would also love to see this feature.\n. ",
    "soltanmm": "From @johnnyluo586 on September 18, 2015 7:39\nI often encounter this problem, very upset, it is strongly recommended that stored in github.com\n. ",
    "luan-cestari": "I signed it!\nOn Wed, 23 Sep 2015 at 15:28 googlebot notifications@github.com wrote:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project, in which case you'll need to\nsign a Contributor License Agreement (CLA).\n[image: :memo:] Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\nIf you've already signed a CLA, it's possible we don't have your\n  GitHub username or you're using a different email address. Check your\n  existing CLA data https://cla.developers.google.com/clas and verify\n  that your email is set on your git commits\n  https://help.github.com/articles/setting-your-email-in-git/.\nIf you signed the CLA as a corporation, please let us know the\n  company's name.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/356#issuecomment-142689448.\n. Hi,\n\nI'd like to include you in my network to share updates and stay in touch.\n- Luan\nConfirm that you know Luan: https://www.linkedin.com/e/v2?e=1u6it1-ig9fmmul-0&t=ssuw&tracking=eml-guest-invite-cta&ek=invite_guest&sharedKey=xOSNpPdZ&invitationId=6064769999766114305\nYou received an invitation to connect. LinkedIn will use your email address to make suggestions to our members in features like People You May Know. Unsubscribe here: https://www.linkedin.com/e/v2?e=1u6it1-ig9fmmul-0&t=lun&midToken=AQFmrFVhGEd36Q&ek=invite_guest&loid=AQEcS6fBL9FkBAAAAVCpkjsTUk4-pj7bwN5GuvGgubr3eGQGeTRqYxvN5f9ZzVOjzpvTJvhlCN2PUSdRfgX2NIdzO3DIpQ9hx3rSUCNQIq1FIuvxEQZpa_hq8jWI6fwQwyp9gC8SW6oG7NsrROLpNOoyV4MohtvfUllok_zIkk-aTEYODwbdJ1BxgUO-E1G4ekhw&eid=1u6it1-ig9fmmul-0\nThis email was sent to reply@reply.github.com.\nIf you need assistance or have questions, please contact LinkedIn Customer Service: https://www.linkedin.com/e/v2?e=1u6it1-ig9fmmul-0&a=customerServiceUrl&ek=invite_guest\n\u00a9 2015 LinkedIn Corporation, 2029 Stierlin Court, Mountain View CA 94043. LinkedIn and the LinkedIn logo are registered trademarks of LinkedIn.\n. Luan Cestari would like to connect on LinkedIn. How would you like to respond?\nAccept: https://www.linkedin.com/e/v2?e=1u6it1-igjd5uy9-1b&a=preRegInvite&ek=first_guest_reminder_01&li=8&m=hero&ts=accept_text&sharedKey=xOSNpPdZ&invitationID=6064769999766114305\nView Luan Cestari's profile: https://www.linkedin.com/comm/profile/view?id=AAsAAAuDamABlzuQLwLOKYmSkblayibTmJ-BKJ8&trk=eml-first_guest_reminder_01-hero-9-profile_text&trkEmail=eml-first_guest_reminder_01-hero-9-profile_text-null-1u6it1%7Eigjd5uy9%7E1b\nYou received an invitation to connect. LinkedIn will use your email address to make suggestions to our members in features like People You May Know. Unsubscribe here: https://www.linkedin.com/e/v2?e=1u6it1-igjd5uy9-1b&t=lun&midToken=AQFmrFVhGEd36Q&ek=first_guest_reminder_01&li=12&m=unsub&ts=HTML&eid=1u6it1-igjd5uy9-1b&loid=AQGhmTO4A0JOegAAAVDOBSI9Rz-7ycjcz5jwXhnAFMzLNdxOPiLOvBH3-0Pl0z2d9zpDOA0FRJDlCoc3WYf3EMNF8ez3xm0vITAwpvQ2FCD4cxQK8ldGf03gqvEwkdJKBCusDdVwDFutm31-Gl62WadgQ92FDkN4FIYKaqeXe-L7t3rx75AeNmhTWwI__PDHpH8V\nThis email was sent to reply@reply.github.com.\nIf you need assistance or have questions, please contact LinkedIn Customer Service: https://www.linkedin.com/e/v2?e=1u6it1-igjd5uy9-1b&a=customerServiceUrl&ek=first_guest_reminder_01\n\u00a9 2015 LinkedIn Corporation, 2029 Stierlin Court, Mountain View CA 94043. LinkedIn and the LinkedIn logo are registered trademarks of LinkedIn.\n. Luan Cestari would like to connect on LinkedIn. How would you like to respond?\nAccept: https://www.linkedin.com/e/v2?e=1u6it1-igtljz5y-5w&a=preRegInvite&ek=second_guest_reminder_01&li=12&m=hero&ts=accept_text&sharedKey=xOSNpPdZ&invitationID=6064769999766114305\nView Luan Cestari's profile: https://www.linkedin.com/comm/profile/view?id=AAsAAAuDamABlzuQLwLOKYmSkblayibTmJ-BKJ8&trk=eml-second_guest_reminder_01-hero-13-profile_text&trkEmail=eml-second_guest_reminder_01-hero-13-profile_text-null-1u6it1%7Eigtljz5y%7E5w\nYou received an invitation to connect. LinkedIn will use your email address to make suggestions to our members in features like People You May Know. Unsubscribe here: https://www.linkedin.com/e/v2?e=1u6it1-igtljz5y-5w&t=lun&midToken=AQFmrFVhGEd36Q&ek=second_guest_reminder_01&li=15&m=unsub&ts=HTML&eid=1u6it1-igtljz5y-5w&loid=AQGghGO95hZ3_QAAAVDy-fSJlpSaP3O_XrlAttxYBSn0l8Kuiw-wWfgdjYkZuH1Nr_L8O0i4BTAMeyE6fXZpHCxuFIXTOhsmok1_F8mx5RC3cc4_1-9k9jECR-gUPERtKq2L8wMc81n50iUZaSrkRRMn9xHf4OU9i_L0aj3d36dxXjtzy73zTtuKbvTHRYKevpF5\nThis email was sent to reply@reply.github.com.\nIf you need assistance or have questions, please contact LinkedIn Customer Service: https://www.linkedin.com/e/v2?e=1u6it1-igtljz5y-5w&a=customerServiceUrl&ek=second_guest_reminder_01\n\u00a9 2015 LinkedIn Corporation, 2029 Stierlin Court, Mountain View CA 94043. LinkedIn and the LinkedIn logo are registered trademarks of LinkedIn.\n. ",
    "kujenga": "I'm seeing the same issue. The following line doesn't have a closing curly brace...\nhttps://github.com/grpc/grpc-go/blob/master/rpc_util.go#L189\n. Wonderful, thank you\n. ",
    "nussjustin": "Added constants and reran tests with go test -bench . -benchmem -benchtime 3s. Compared commits are 3e7b7e58f49 and 5165699f376. \nEdit: Raw data is available here\nNew results:\n```\nbenchmark                              old ns/op     new ns/op     delta\nBenchmarkClientStreamc1-8              121497        128525        +5.78%\nBenchmarkClientStreamc8-8              20963         20454         -2.43%\nBenchmarkClientStreamc64-8             12370         11980         -3.15%\nBenchmarkClientStreamc512-8            13542         13152         -2.88%\nBenchmarkClientUnaryc1-8               240343        237528        -1.17%\nBenchmarkClientUnaryc8-8               46075         44415         -3.60%\nBenchmarkClientUnaryc64-8              32634         33386         +2.30%\nBenchmarkClientUnaryc512-8             33065         32732         -1.01%\nBenchmarkClientStreamNoTracec1-8       115481        93490         -19.04%\nBenchmarkClientStreamNoTracec8-8       20309         20031         -1.37%\nBenchmarkClientStreamNoTracec64-8      11684         11413         -2.32%\nBenchmarkClientStreamNoTracec512-8     12805         12218         -4.58%\nBenchmarkClientUnaryNoTracec1-8        226422        221111        -2.35%\nBenchmarkClientUnaryNoTracec8-8        41646         41079         -1.36%\nBenchmarkClientUnaryNoTracec64-8       29446         28662         -2.66%\nBenchmarkClientUnaryNoTracec512-8      29964         29044         -3.07%\nbenchmark                              old allocs     new allocs     delta\nBenchmarkClientStreamc1-8              46             41             -10.87%\nBenchmarkClientStreamc8-8              47             41             -12.77%\nBenchmarkClientStreamc64-8             46             41             -10.87%\nBenchmarkClientStreamc512-8            46             40             -13.04%\nBenchmarkClientUnaryc1-8               100            91             -9.00%\nBenchmarkClientUnaryc8-8               101            92             -8.91%\nBenchmarkClientUnaryc64-8              100            91             -9.00%\nBenchmarkClientUnaryc512-8             100            91             -9.00%\nBenchmarkClientStreamNoTracec1-8       43             37             -13.95%\nBenchmarkClientStreamNoTracec8-8       43             37             -13.95%\nBenchmarkClientStreamNoTracec64-8      43             37             -13.95%\nBenchmarkClientStreamNoTracec512-8     42             36             -14.29%\nBenchmarkClientUnaryNoTracec1-8        87             78             -10.34%\nBenchmarkClientUnaryNoTracec8-8        88             78             -11.36%\nBenchmarkClientUnaryNoTracec64-8       87             78             -10.34%\nBenchmarkClientUnaryNoTracec512-8      87             78             -10.34%\nbenchmark                              old bytes     new bytes     delta\nBenchmarkClientStreamc1-8              2015          1920          -4.71%\nBenchmarkClientStreamc8-8              2046          1950          -4.69%\nBenchmarkClientStreamc64-8             2044          1948          -4.70%\nBenchmarkClientStreamc512-8            2053          1957          -4.68%\nBenchmarkClientUnaryc1-8               5983          5838          -2.42%\nBenchmarkClientUnaryc8-8               5985          5841          -2.41%\nBenchmarkClientUnaryc64-8              6010          5866          -2.40%\nBenchmarkClientUnaryc512-8             5949          5809          -2.35%\nBenchmarkClientStreamNoTracec1-8       1887          1791          -5.09%\nBenchmarkClientStreamNoTracec8-8       1917          1821          -5.01%\nBenchmarkClientStreamNoTracec64-8      1928          1832          -4.98%\nBenchmarkClientStreamNoTracec512-8     1923          1828          -4.94%\nBenchmarkClientUnaryNoTracec1-8        4303          4159          -3.35%\nBenchmarkClientUnaryNoTracec8-8        4304          4160          -3.35%\nBenchmarkClientUnaryNoTracec64-8       4330          4186          -3.33%\nBenchmarkClientUnaryNoTracec512-8      4280          4139          -3.29%\n```\nOutput of benchstat -delta-test=ttest (3 runs for each commit, also -benchtime 3s):\n```\nname                       old time/op    new time/op    delta\nClientStreamc1-8              122\u00b5s \u00b1 1%     127\u00b5s \u00b1 3%     ~     (p=0.174 n=3+3)\nClientStreamc8-8             21.0\u00b5s \u00b1 0%    20.5\u00b5s \u00b1 0%   -2.45%  (p=0.000 n=3+3)\nClientStreamc64-8            12.3\u00b5s \u00b1 0%    12.0\u00b5s \u00b1 0%   -2.64%  (p=0.001 n=3+3)\nClientStreamc512-8           13.5\u00b5s \u00b1 0%    13.1\u00b5s \u00b1 0%   -2.90%  (p=0.001 n=3+3)\nClientUnaryc1-8               242\u00b5s \u00b1 1%     237\u00b5s \u00b1 1%     ~     (p=0.087 n=3+3)\nClientUnaryc8-8              45.2\u00b5s \u00b1 2%    44.5\u00b5s \u00b1 0%     ~     (p=0.318 n=3+3)\nClientUnaryc64-8             32.6\u00b5s \u00b1 1%    32.6\u00b5s \u00b1 2%     ~     (p=0.955 n=3+3)\nClientUnaryc512-8            32.9\u00b5s \u00b1 0%    32.6\u00b5s \u00b1 1%     ~     (p=0.120 n=3+3)\nClientStreamNoTracec1-8       114\u00b5s \u00b1 1%     105\u00b5s \u00b111%     ~     (p=0.242 n=3+3)\nClientStreamNoTracec8-8      20.3\u00b5s \u00b1 0%    19.8\u00b5s \u00b1 1%   -2.34%  (p=0.045 n=3+3)\nClientStreamNoTracec64-8     11.6\u00b5s \u00b1 1%    11.4\u00b5s \u00b1 0%   -2.09%  (p=0.006 n=3+3)\nClientStreamNoTracec512-8    12.7\u00b5s \u00b1 1%    12.2\u00b5s \u00b1 0%   -4.42%  (p=0.001 n=3+3)\nClientUnaryNoTracec1-8        228\u00b5s \u00b1 1%     226\u00b5s \u00b1 3%     ~     (p=0.755 n=3+3)\nClientUnaryNoTracec8-8       41.5\u00b5s \u00b1 0%    41.1\u00b5s \u00b1 0%   -1.07%  (p=0.005 n=3+3)\nClientUnaryNoTracec64-8      29.4\u00b5s \u00b1 0%    28.7\u00b5s \u00b1 0%   -2.40%  (p=0.003 n=3+3)\nClientUnaryNoTracec512-8     29.8\u00b5s \u00b1 1%    29.0\u00b5s \u00b1 1%   -2.66%  (p=0.005 n=3+3)\nname                       old alloc/op   new alloc/op   delta\nClientStreamc1-8             2.02kB \u00b1 0%    1.92kB \u00b1 0%   -4.75%  (p=0.000 n=3+3)\nClientStreamc8-8             2.05kB \u00b1 0%    1.95kB \u00b1 0%     ~       zero variance\nClientStreamc64-8            2.04kB \u00b1 0%    1.95kB \u00b1 0%   -4.71%  (p=0.000 n=3+3)\nClientStreamc512-8           2.05kB \u00b1 0%    1.96kB \u00b1 0%   -4.65%  (p=0.000 n=3+3)\nClientUnaryc1-8              5.98kB \u00b1 0%    5.84kB \u00b1 0%   -2.41%  (p=0.000 n=3+3)\nClientUnaryc8-8              5.99kB \u00b1 0%    5.84kB \u00b1 0%     ~       zero variance\nClientUnaryc64-8             6.01kB \u00b1 0%    5.87kB \u00b1 0%   -2.39%  (p=0.000 n=3+3)\nClientUnaryc512-8            5.95kB \u00b1 0%    5.81kB \u00b1 0%   -2.39%  (p=0.000 n=3+3)\nClientStreamNoTracec1-8      1.89kB \u00b1 0%    1.79kB \u00b1 0%     ~       zero variance\nClientStreamNoTracec8-8      1.92kB \u00b1 0%    1.82kB \u00b1 0%     ~       zero variance\nClientStreamNoTracec64-8     1.93kB \u00b1 0%    1.83kB \u00b1 0%     ~       zero variance\nClientStreamNoTracec512-8    1.92kB \u00b1 0%    1.83kB \u00b1 0%   -4.96%  (p=0.000 n=3+3)\nClientUnaryNoTracec1-8       4.30kB \u00b1 0%    4.16kB \u00b1 0%     ~       zero variance\nClientUnaryNoTracec8-8       4.30kB \u00b1 0%    4.16kB \u00b1 0%     ~       zero variance\nClientUnaryNoTracec64-8      4.33kB \u00b1 0%    4.19kB \u00b1 0%     ~       zero variance\nClientUnaryNoTracec512-8     4.28kB \u00b1 0%    4.14kB \u00b1 0%   -3.32%  (p=0.000 n=3+3)\nname                       old allocs/op  new allocs/op  delta\nClientStreamc1-8               46.7 \u00b1 1%      41.0 \u00b1 0%  -12.14%  (p=0.003 n=3+3)\nClientStreamc8-8               47.0 \u00b1 0%      41.0 \u00b1 0%     ~       zero variance\nClientStreamc64-8              46.3 \u00b1 1%      41.0 \u00b1 0%  -11.51%  (p=0.004 n=3+3)\nClientStreamc512-8             46.0 \u00b1 0%      40.0 \u00b1 0%     ~       zero variance\nClientUnaryc1-8                 100 \u00b1 0%        91 \u00b1 0%     ~       zero variance\nClientUnaryc8-8                 101 \u00b1 0%        92 \u00b1 0%     ~       zero variance\nClientUnaryc64-8                100 \u00b1 0%        91 \u00b1 0%     ~       zero variance\nClientUnaryc512-8               100 \u00b1 0%        91 \u00b1 0%     ~       zero variance\nClientStreamNoTracec1-8        43.0 \u00b1 0%      37.0 \u00b1 0%     ~       zero variance\nClientStreamNoTracec8-8        43.0 \u00b1 0%      37.0 \u00b1 0%     ~       zero variance\nClientStreamNoTracec64-8       42.7 \u00b1 2%      37.0 \u00b1 0%  -13.28%  (p=0.003 n=3+3)\nClientStreamNoTracec512-8      42.0 \u00b1 0%      36.0 \u00b1 0%     ~       zero variance\nClientUnaryNoTracec1-8         87.0 \u00b1 0%      78.0 \u00b1 0%     ~       zero variance\nClientUnaryNoTracec8-8         88.0 \u00b1 0%      78.7 \u00b1 1%  -10.61%  (p=0.001 n=3+3)\nClientUnaryNoTracec64-8        87.0 \u00b1 0%      78.0 \u00b1 0%     ~       zero variance\nClientUnaryNoTracec512-8       87.0 \u00b1 0%      78.0 \u00b1 0%     ~       zero variance\n``\n. Done, rebased changes\n. Try to update your copy of golang.org/x/net/http2. SetReuseFrames was only added recently and you may have an older version. Just executego get -u golang.org/x/net/http2` and then everything should work again.\nThe commit that added SetReuseFrames: https://github.com/golang/net/commit/bb807669a61aca6092d8137da1fab2150bb96ad7. Your golang.org/x/text is not on a branch. Just do a \"git checkout master\" inside your $GOPATH/src/golang.org/x/text and then run go get again. Or just delete $GOPATH/src/golang.org/x/text and run go get again.. Thanks, I missed that. It's fixed in the new commit.\n. Done\n. Done\n. ",
    "jtattermusch": "Friendly ping. We are trying to make all interop client and servers uniform in terms of cmdline args and addressing this asap would help.\n. Ping?\n. I confirm that the test started passing on Jenkins https://grpc-testing.appspot.com/job/gRPC_interop_master\nThanks for fixing this.\nPASSED: cloud_to_cloud:java:go_server:empty_stream [time=1.3sec; retries=0;0].\n. LGTM, will merge once I do corresponding changes in the run_interop_tests.py script\n. CC @vjpai\n. Go QPS workers seems to be in good enough shape for our GA needs. Let's close this issue and track progress with per-feature issues if needed?\n. CC @vjpai \n. Sorry, wrong repo.\n. CC myself.\n. Will be fixed by https://github.com/grpc/grpc-go/pull/1290.. Seems this will be an issue with all google.protobuf.* well-known protos, not just timestamp.proto. CC @menghanl  @MakMukhi . Umbrella issue: grpc/grpc#11130. Can we merge?. @dsymonds  I'm going to take a guess: The commit logs for this PR show your committer email as *****@golang.org - perhaps you should change that to the e-mail address known to the CLA checker? AFAIK the CLA checkers usually go by the e-mail address to recognize users.. ",
    "karlkfi": "I see that Picker.Pick was replaced by ClientConn.getTransport. Would it be reasonable to make getTransport public now that it's been refactored?\nhttps://github.com/grpc/grpc-go/blob/master/clientconn.go#L424\n. ",
    "liudanking": "Many thanks to @psanford. Below is my sample code:\nclient side:\n```\n    // load peer cert/key, cacert\n    peerCert, err := tls.LoadX509KeyPair(config.PeerCert, config.PeerKey)\n    if err != nil {\n        log.Error(\"load peer cert/key error:%v\", err)\n        return nil, err\n    }\n    caCert, err := ioutil.ReadFile(config.CACert)\n    if err != nil {\n        log.Error(\"read ca cert file error:%v\", err)\n        return nil, err\n    }\n    caCertPool := x509.NewCertPool()\n    caCertPool.AppendCertsFromPEM(caCert)\nta := credentials.NewTLS(&tls.Config{\n    Certificates: []tls.Certificate{peerCert},\n    RootCAs:      caCertPool,\n})\nconn, err := grpc.Dial(config.ServerAddr, grpc.WithTransportCredentials(ta))\nif err != nil {\n    return nil, err\n}\n\n```\nServer side:\n```\n    // load peer cert/key, ca cert\n    peerCert, err := tls.LoadX509KeyPair(config.PeerCert, config.PeerKey)\n    if err != nil {\n        log.Error(\"load peer cert/key error:%v\", err)\n        return\n    }\n    caCert, err := ioutil.ReadFile(config.CACert)\n    if err != nil {\n        log.Error(\"read ca cert file error:%v\", err)\n        return\n    }\n    caCertPool := x509.NewCertPool()\n    caCertPool.AppendCertsFromPEM(caCert)\n    ta := credentials.NewTLS(&tls.Config{\n        Certificates: []tls.Certificate{peerCert},\n        ClientCAs:    caCertPool,\n        ClientAuth:   tls.RequireAndVerifyClientCert,\n    })\nlis, err := net.Listen(\"tcp\", fmt.Sprintf(\":%d\", cs.port))\nif err != nil {\n    log.Error(\"server listen on port %d error:%v\", cs.port, err)\n    return\n}\ns := grpc.NewServer(grpc.Creds(ta))\nproto.RegisterSomerServer(s, cs)\ngo s.Serve(lis)\n\n```\n. ",
    "ashishgandhi": "I signed it!\n. Apologies, that was very vague of me. Here's more details.\nProtoBuf\n```\nsyntax = \"proto3\";\npackage test;\nservice Nop {\n  rpc Do(Empty) returns (Empty) {}\n}\nmessage Empty {}\n```\nServer\n```\npackage main\nimport (\n    \"flag\"\n    \"log\"\n    \"net\"\n    \"test\"\n\"golang.org/x/net/context\"\n\"google.golang.org/grpc\"\n\n)\nvar addr = flag.String(\"addr\", \":1337\", \"listen addr\")\nfunc main() {\n    flag.Parse()\n    s := grpc.NewServer()\n    test.RegisterNopServer(s, &server{})\nl, err := net.Listen(\"tcp\", *addr)\nif err != nil {\n    log.Fatalf(\"Cannot listen on %s: %v\", *addr, err)\n}\nlog.Fatalf(\"Server stopped: %v\", s.Serve(l))\n\n}\ntype server struct{}\n// XXX: If you do this the server crashes. Instead, if you do a\n//\n//    return new(test.Empty), nil\n//\n// then life is okay. The error being nil or not does not make\n// a difference.\nfunc (server) Do(context.Context, test.Empty) (*test.Empty, error) { return nil, nil }\n```\nAny client is fine.\n. Well, nothing prevents me from doing that. But it'd be nice to not crash the server because of this? Same reason why the standard library http package does not crash even if you panic.\n. ",
    "wari": "A quick test shows that this is not possible Serve() requires a net.Listener instead of a net.Conn. Is it not possible to get this working other than setting up two servers?\n. I'm not sure if this is relavant, net/trace reports that this is still connected:\nActive Requests\n| When | Elapsed (s) | function |\n| --- | --- | --- |\n| 2015/10/29 17:54:06.673896 | 462.982815 | /routeguide.RouteGuide/RouteChat |\n| 2015/10/29 17:53:56.994103 | 472.662693 | /routeguide.RouteGuide/RecordRoute |\n| 2015/10/29 17:53:56.993537 | 472.663315 | /routeguide.RouteGuide/ListFeatures |\n| 2015/10/29 17:53:56.993120 | 472.663784 | /routeguide.RouteGuide/GetFeature |\n| 2015/10/29 17:53:56.992671 | 472.664285 | /routeguide.RouteGuide/GetFeature |\n. When using Java to connect to the grpc-go route_guide server, I got the following error:\n```\n[wari@nucky examples]$ ./build/install/grpc-examples/bin/route-guide-client\nOct 29, 2015 6:35:04 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: *** GetFeature: lat=409,146,138 lon=-746,188,906\nOct 29, 2015 6:35:05 PM io.grpc.internal.TransportSet$1 run\nINFO: Created transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980\nOct 29, 2015 6:35:05 PM io.grpc.internal.TransportSet$TransportListener transportReady\nINFO: Transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980 is ready\nOct 29, 2015 6:35:05 PM io.grpc.examples.routeguide.RouteGuideClient getFeature\nWARNING: RPC failed\nio.grpc.StatusRuntimeException: UNIMPLEMENTED: unknown service routeguide.RouteGuide\n        at io.grpc.Status.asRuntimeException(Status.java:430)\n        at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:156)\n        at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:106)\n        at io.grpc.examples.routeguide.RouteGuideGrpc$RouteGuideBlockingStub.getFeature(RouteGuideGrpc.java:169)\n        at io.grpc.examples.routeguide.RouteGuideClient.getFeature(RouteGuideClient.java:80)\n        at io.grpc.examples.routeguide.RouteGuideClient.main(RouteGuideClient.java:232)\nOct 29, 2015 6:35:05 PM io.grpc.internal.TransportSet$TransportListener transportShutdown\nINFO: Transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980 is being shutdown\nOct 29, 2015 6:35:05 PM io.grpc.internal.TransportSet$TransportListener transportTerminated\nINFO: Transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980 is terminated\nException in thread \"main\" io.grpc.StatusRuntimeException: UNIMPLEMENTED: unknown service routeguide.RouteGuide\n        at io.grpc.Status.asRuntimeException(Status.java:430)\n        at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:156)\n        at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:106)\n        at io.grpc.examples.routeguide.RouteGuideGrpc$RouteGuideBlockingStub.getFeature(RouteGuideGrpc.java:169)\n        at io.grpc.examples.routeguide.RouteGuideClient.getFeature(RouteGuideClient.java:80)\n        at io.grpc.examples.routeguide.RouteGuideClient.main(RouteGuideClient.java:232)\n```\nBut the same program worked when connected to the CPP route_guide server:\n[wari@nucky examples]$ ./build/install/grpc-examples/bin/route-guide-client\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: *** GetFeature: lat=409,146,138 lon=-746,188,906\nOct 29, 2015 6:36:42 PM io.grpc.internal.TransportSet$1 run\nINFO: Created transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980\nOct 29, 2015 6:36:42 PM io.grpc.internal.TransportSet$TransportListener transportReady\nINFO: Transport io.grpc.netty.NettyClientTransport@7085bdee for localhost/127.0.0.1:8980 is ready\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: Found feature called \"BerkshireValleyManagementAreaTrail,Jefferson,NJ,USA\" at 40.915, -74.619\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: *** GetFeature: lat=0 lon=0\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: Found no feature at 0, 0\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: *** ListFeatures: lowLat=400,000,000 lowLon=-750,000,000 hiLat=420,000,000 hiLon=-730,000,000\nOct 29, 2015 6:36:42 PM io.grpc.examples.routeguide.RouteGuideClient info\nINFO: Result: name: \"PatriotsPath,Mendham,NJ07945,USA\"\nlocation {\n  latitude: 407838351\n  longitude: -746143763\n}\n.......\nOf course, java server + cpp client works as well. Can't say the same for go and java.\n. OK, I'll cook up something in a few hours.\n. You can find the example server and client code @ https://github.com/wari/grpc-trace-test\n. ",
    "labkode": "@dsymonds Thanks for your quick response. We can close this issue.\n. @menghanl  Thanks for your comment.\nI thought about that approach but I need to pass configuration options to the NewFunc which has a  bit more complex signature \n// NewFunc is the function that gRPC services need to register at init time.\ntype NewFunc func(conf map[string]interface{}) (sd grpc.ServiceDesc, ss interface{})\nand the conf is loaded at main time so I cannot use that approach as conf is not ready at init time.\n. @menghanl I got it now!  It works like a charm. \nThanks a lot for your time.. ",
    "vitalyisaev2": "@c4milo I guess that everyone who need to transfer big objects via grpc will have to implement chuncking on the top of grpc streaming.. @c4milo I would suggest you to try the following approach: \n```proto\nservice BlobKeeper {\n    rpc Put (stream PutRequest) returns (PutResponse);\n}\nmessage PutRequest {\n    message Key {\n         string key = 1;\n    }\n    message Chunk {\n          bytes data = 1;\n          int64 position = 2;\n    }\n    oneof value {\n        Key key = 1;\n        Chunk chunk = 2;\n    }\n}\n```\nOn the client side you'll have to split your data to particles and send it sequentially within a stream. Probably in the first message you should provide some kind of metadata, like data key or something like that. On the server side you can do whatever you want: yoy may buffer the chunks, or you can put them on disk immidiately.\nThe one's is for sure: you'll have to do a lot of manual work here, but it's quite straightforward.. @dfawley thanks for a response, I just wanted to use your sync.Pool with *proto.Pool in a third-party project, and I didn't want to copy-paste code with tests from grpc-go :) \nSince you're planning to change it, please take into account, that it would be nice to have this structure (or maybe a method that initializes it) public. Thank you.. @dfawley thank you, yes, it's possible... But if I copied this code, I'd have to maintain it (including tests), whereas these efforts have been already made by GRPC authors.\nSo I just wanted to notice that this code seems to be separable from the main code base, and it would be very nice to see these structures public some day.. @dfawley Could you please clarify if the example of client-side streaming is following the proper stream protocol? Because context.Background() is used to create stream, and it's not cancelled explicitly.\nhttps://github.com/grpc/grpc-go/blob/master/examples/route_guide/client/client.go#L59. @menghanl thank you, now examples look much more idiomatic.. I would just like to mention that for streams simple cancellation (context.WithCancel instead of context.WithTimeout) could be a little bit more idiomatic. Steams may be potentially infinite, so timeouting on working stream may cause data inconsistency and so on.. @dfawley Unfortunately I'm not aware of 'wait-for-ready` RPC semantics... But I think we can consider simple and wide-spread examples of streaming.\nFor instance, suggest we've implemented file upload / download protocol on the top of the GRPC stream (so it would be client-side streaming for uploading and server-side streaming for downloading). We have to download a huge 10GB file, but we have weak connection with low throughput. If we set a timeout for this kind of stream, we'll never be able to download the file, because the stream will be canceled and next time download will start from the very beginning.\nAnother example is a gossip protocol utilizing GRPC bidirectional streaming. From my point of view, this stream should not be terminated by timeout either. Because data exchange between peers should take place during the whole lifetime of peer processes. I agree with you that reconnection and retrial logic should be implemented in any case, but I don't see a good reason to terminate the stream which is actually expected to live forever.. @MakMukhi thanks for explanation!\nRegarding to the second question, I actually meant the situation when GRPC client and server collaborate through bidirectional streaming, but every session is quite short (maybe a couple of messages from each side), however, unary RPC is not an option here. In fact the question is how to find out, what would be better: to reuse one long-living stream with the cost of extending messages, or just to create new bidirectional stream per every request.. ",
    "cirocosta": "Are there any infos about how people are doing this? Any libraries that already do this on top of grpc? Multipart, as suggested?\nThx!. ",
    "matiasinsaurralde": "@cirocosta I'm interested on this as well!. Thanks, this was useful!\n. ",
    "zhaohansprt": "i m  in this  too. ",
    "Sicos1977": "Same here. ",
    "tzutalin": "I try to implement chuncking in java and cpp, and write some sample code and protobuf. Hope it helps. Any feedback is appreciated.\n. ",
    "LakeCarrot": "@tzutalin \nNever mind the previous post.\nI have two more concerns,\n1) https://github.com/tzutalin/example-grpc/blob/0bde7f0ed4a9b0961429697307be88314734cbe5/java/src/main/java/UploadFileClient.java#L74\nFor this line, we need to change to \nByteString byteString = ByteString.copyFrom(buffer, 0, tmp);\nto make sure we didn't waste space and change the original file size.\n2) Another thing is for the file with multiple chunks to send, we need to wait for a while after sending each chunking, otherwise, the server will refuse to process the request.\n. ",
    "smreed": "This is using go 1.5.1 on linux/amd-64 fwiw.\n. You're right, and I also have filed this issue w/ the wrong project. Got the grpc and cloud/bigtable stuff mixed up. Closing this issue.\n. ",
    "pjvds": "@iamqizhao: the client process died, or the underlying connection got closed.\n. @iamqizhao Perfect! That totally fixed the problems I ran into. :+1:\n. ",
    "xiaoshuai": "does \"go get github.com/grpc/grpc-go\" work?. ",
    "irfansharif": "@kissgyorgy: try go get -u -v google.golang.org/grpc?. no problemo! for future context the -u switch updates the named packages and their dependencies. By default, go get only pulls in missing packages (from your local GOPATH) but does not use it to look for updates to existing packages, which seems was the case for the http2 dependency.. can I take this on? I have a draft I can polish up for submission if nobody else is already working on this/something orthogonal/supplanting it entirely.. @chy168: there are in fact no buildable Go source files in the /grpc/grpclb source tree, there's no func main() which is what go will look for when instructed to build (look for go build ... in the verbose output). I suspect what you really wanted to do is to simply get the grpc source tree, in which case go get -u -v google.golang.org/grpc is sufficient.. With pprof on the heap profile, sample_index=alloc_space for a short run of https://github.com/irfansharif/pinger, before:\n.          .     57:func (p protoCodec) marshal(v interface{}, cb *cachedProtoBuffer) ([]byte, error) {\n         .          .     58:   protoMsg := v.(proto.Message)\n  532.29MB   532.29MB     59:   newSlice := make([]byte, 0, cb.lastMarshaledSize)\n         .          .     60:\n         .          .     61:   cb.SetBuf(newSlice)\n         .          .     62:   cb.Reset()\n         .   556.81MB     63:   if err := cb.Marshal(protoMsg); err != nil {\n         .          .     64:           return nil, err\n         .          .     65:   }\nThe misuse of proto.Buffer is indicative here:\n.          .    261:func (p *Buffer) Marshal(pb Message) error {\n         .          .    262:   // Can the object marshal itself?\n         .          .    263:   if m, ok := pb.(Marshaler); ok {\n         .   551.80MB    264:           data, err := m.Marshal()\n       5MB        5MB    265:           p.buf = append(p.buf, data...)\n         .          .    266:           return err\n         .          .    267:   }\nWhere p.buf is the slice set in cachedProtoBuffer.SetBuf. Not only are we excessively allocating, we're also unnecessarily copying.. \nThis is the svg view of the same showing the dual allocation above.\nNB: The Unmarshal code path technically isn't affected given we don't allocate buffers for it but it saves us the unnecessary sync.Pool.{Get,Put} operations and the tiny overhead with that. The earlier revision was doing essentially what was already contained within proto.Unmarshal.. > What benchmark are these measurements from?\nIncluded above but it's running the simplest ping server https://github.com/irfansharif/pinger for a 30s duration on my local macbook-pro.\nIf you want to replicate run ./pinger -server -t grpc -s 512 and ./pinger -d 45s -t grpc -p 64 -c 512 -s 512 separately (second is the client) then curl -k -o heap 'http://localhost:6060/debug/pprof/heap'; pprof -alloc_space pinger heap where pinger is the binary.\nNote for this run proto.Buffer is not allocated given the incoming message satisfies proto.Marshaler, which presumably from the generated .pb.go files comprises of the majority of requests. (I'm actually not sure when it would not.)\nDitto for the satisfied proto.Unmarshaler interfaces, proto.Buffer isn't allocated because we do not hit those code paths.\n. @apolcyn: I wrote up https://github.com/golang/protobuf/pull/418, which I think is a more appropriate fix for the issue you mentioned above (~though I'm still unsure how to replicate it within the context of grpc-go~). \nAlso for the benchmarks you linked above, are those nightly master runs? I'm not sure how to navigate it and what/how to infer, are the benchmarking programs publicly accessible?\nEDIT: gah, I only just realized that generated proto messages don't always have {Unm,Marshal} methods, I was using gogo/protobuf and had these autogenerated (+cc https://github.com/golang/protobuf/issues/280). I still think https://github.com/golang/protobuf/pull/418 is the right way to solve this in conjunction with this changeset. Presumably this what you meant with \"if the protos generated from an updated generator would have an effect here\".. golang/protobuf#418 was unfortunately rejected due to what seems to be them exporting their internal version and wanting to avoid merge conflicts. This PR still benefits the group of users providing {Un,M}arshal methods (they're also not affected by the proto.Buffer allocations by not hitting those codepaths) at the cost proto.Buffer allocations for users not doing so. We can of course copy over small parts of golang/protobuf (effectively re-doing golang/protobuf#418 but here) to address both use cases but I'd like to know first if this is something that is acceptable here. \nIn any case it's worth noting that with the current state of things across here and golang/protobuf, for every marshaled byte through grpc we allocate twice as much as necessary (within these codepaths) and explicitly copy once more than necessary (within these codepaths).. @MakMukhi: it's no issue haha, like I mentioned in #1482 I was just chomping through some of our grpc bottlenecks within cockroachdb/cockroach and these seemed like easy enough pickings to upstream. Nothing ventured, nothing gained!\n\nWe're working on optimization quite actively and are aware of these optimizations\n\nI'm glad to hear, I should have consulted earlier but wanted to see how far I could get over the weekend. I have a host of other PRs on a similar vein that I'll hold off on in the interim.\n\nAlso, some of these changes will be rendered obsolete once we're done with the code restructuring that's underway currently.\n\nAre plans for this publicly visible? I did look for indication of this (mailing lists and here) but did not come across any. In all cases if there's anything here I can help with, do let me know. Looking to upstream as many of our results as possible. Feel free to close this PR if needed.. @MakMukhi, @apolcyn: PTAL here, reworked it in light of golang/protobuf#418 not getting in. We should should have the best of both worlds, no extra proto.Buffer allocs and no extra copying for users providing {Unm,M}arshal helpers. Commit message updated accordingly.. (pprof) list WriteStatus\nTotal: 7.97GB\nROUTINE ======================== google.golang.org/grpc/transport.(*http2Server).WriteStatus in /Users/irfansharif/Software/src/google.golang.org/grpc/transport/http2_server.go\n         0    97.50MB (flat, cum)  1.19% of Total\n         .          .    793:                   Name:  \"grpc-status\",\n         .          .    794:                   Value: strconv.Itoa(int(st.Code())),\n         .          .    795:           })\n         .          .    796:   t.hEnc.WriteField(hpack.HeaderField{Name: \"grpc-message\", Value: encodeGrpcMessage(st.Message())})\n         .          .    797:\n         .    97.50MB    798:   if p := st.Proto(); p != nil && len(p.Details) > 0 {\n         .          .    799:           stBytes, err := proto.Marshal(p)\n         .          .    800:           if err != nil {\n         .          .    801:                   // TODO: return error instead, when callers are able to handle it.\n         .          .    802:                   panic(err)\n         .          .    803:           }\nRunning pprof -alloc_space on a toy pinger program: https://github.com/irfansharif/pinger\n\n. Running pprof -alloc_space pinger for https://github.com/irfansharif/pinger, a simple 'ping' program. Before, describing the copying mentioned above (all of these are over a 30s run):\nROUTINE ======================== google.golang.org/grpc/transport.(*http2Server).Write in /Users/irfansharif/Software/src/google.golang.org/grpc/transport/http2_server.go\n  422.23MB   431.73MB (flat, cum)  9.05% of Total\n         .          .    837:   // TODO(zhaoq): Support multi-writers for a single stream.\n         .          .    838:   secondStart := http2MaxFrameLen - len(hdr)%http2MaxFrameLen\n         .          .    839:   if len(data) < secondStart {\n         .          .    840:           secondStart = len(data)\n         .          .    841:   }\n  422.23MB   422.23MB    842:   hdr = append(hdr, data[:secondStart]...)\n         .          .    843:   data = data[secondStart:]\n         .          .    844:   isLastSlice := (len(data) == 0)\n         .          .    845:   var writeHeaderFrame bool\n\nAfter (notably Write is now absent). \n\nSimilar CPU profiles for the tiny overhead in copying is not shown.\n. It should also be possible to fetch from the global free list in new{Client,Server}Stream when initializing the stream specific compression buffer (and returning it on stream.finish) but I haven't simulated workloads where the effect of this would be visible.. > maybe return an empty header slice from encode(), and include the header info in the data buffer when compression is enabled?\nFor what it's worth I thought of this too but the chunking into the header buffer for 16KB frames in http2Server.Write is what introduced the allocation. We could of course have this as a special case (if nil header, only chunk data buffer) but I wasn't too happy with that (can still augment this to this end if OK with you folk). The cleanest answer for both ends I think would be the presence of a msg.MarshalTo([]byte) helper (with a corresponding msg.Size() but unfortunately not currently present in golang/protobuf (is in gogo/protobuf however).\n@MakMukhi: anything public here that I can track? Most of the work across the three PRs are bottlenecks with usage patterns as it pertains to cockroachdb/cockroach. . If we have a large slice and then only hold on to a reference pointing to just the first x elements of the slice (slice[:x]), Go still holds onto the large slice (/backing array) in memory. (The same is true for strings). The following snippet for example doesn't allocate/copy per iteration, s is pinned in memory throughout:\nfunc main() {\n    s := \"abcdefghi\"\n    a := make([]string, 100)\n    for i := 0; ; i++ {\n        a[i%100] = s[:3]\n    }\n}\nPresumably what we want here is an explicit copy instead.. yup, that works.. ",
    "anzw": "It is the cause, thanks a lot.\n. ",
    "vladshub": "I signed it!\n. ",
    "dkwasnick": "This race condition manifests itself when you have multiple goroutines accessing the same ClientConn, and one of them attempts to manually close the connection. The second goroutine can set cc.state to TransientFailure, when the correct behavior would be for the value to remain as Shutdown.\nThis bug causes resource leaks that make using grpc-go in production unfeasible for us, currently.\n. ",
    "linuxerwang": "Thanks for your input. I agree you have your good point. But think about a mission critical service crashes because one request encountered a panic somehow and all the other current requests are lost (innocent users would be affected).\nI think \"quietly catching a panic\" is certainly unacceptable. The panic should be well logged, and the rpc caller should get an error.\n. Agreed. Thanks, dsymonds.\n. I wonder if this metadata change in grpc-go would be backward compatible with grpc-java?. Great. Thanks for clarifying.. ",
    "benhoyt": "I realize this issue is closed, but this behavior was jarring to me too. It seems to me that most servers (web and gRPC) shouldn't have state shared between requests, so \"leaving the server in an inconsistent or incorrect state\" doesn't apply for most web servers.\nIt seems much better to have zero downtime and keep the server running in case of an expected panic than take the whole thing down and rely on server management stuff to hopefully start it up again quickly.\nEither way, we'll probably add this to our wrapper package. It'd be nice if there was an option to enable this for grpc-go though.. ",
    "jboeuf": "You're right, I'll fix them all.\n. Thanks much for the PR! taking a look now.\n. I think that that it would also awesome to have a function like this:\nfunc WithGoogleDefaultCredentials() DialOption\nWhich would, for now, setup the DialOption with SSL and the Google Default Core credentials (scopeless). Please reach out directly to me for more clarifications.\n. @iamqizhao \nSGTM. Thanks!\n. And please CC me on the name PR for WithGoogleDefaultCredentials. Thanks!\n. If you are going to allow this for something TLS with HTTPS semantics (server host name authentication), the gRPC framework will have to make sure that the :authority is present in the server cert. Otherwise the client could be subject to an attack. Please see https://github.com/grpc/grpc-java/pull/2662#issuecomment-275562710 for an illustration of this.  . By the way, the gRPC stack does this for C core (and wrapped languages) and Java. I'm sure about the golang stack but if it does not, it should be fixed, @menghanl could you point me to this code in grpc-go?. > @jboeuf :authority is set to the server name specified by TransportCredentials: https://github.com/grpc/grpc-go/blob/master/clientconn.go#L323. If it's not specified by TransportCredentials, it's set to the DialTarget.\nWe should probably take this one offline but does the TransportCredentials make sure that the :authority set in this creds is correct (e.g. listed on the server cert for TLS)?\n\nI'm OK with making this work when WithInsecure.\n@jboeuf Are there any concerns with :authority in the case of insecure connections?\n\nI don't have an issue if used with WithInsecure.\n. it looks a bit weird to me have a function called withTransportCredentials that takes a TransportAuthenticator object.\nIn C/C++ we call these ChannelCredentials so maybe we could align here with:\nfunc WithChannelCredentials(auth credentials.ChannelCredentials) DialOption\nor just be consistent\nfunc WithTransportAuthenticator(auth credentials.TransportAuthenticator) DialOption\n. In C++ we call these CallCredentials. It could be a good idea to align as well. \n. ",
    "Julio-Guerra": "I'm wondering if this is that clear on the documentation. It really deserves a \"Client-Server Synchronization\" and \"Handling errors\" chapters.. @dfawley Still, the API is inconsistent since we can provide a context to a client call...\nI have another relevant example where current implementation fails when simply following common Go principles to further extend another type:\ntype MyContext {\n    context.Context\n    // plus whatever...\n}\nIf we could also provide the context ourselves to the server, we would be able to type-assert it to our own types.  Current implementation forces using a global variable instead.... @MakMukhi @dfawley I understood, by reading the code, that stopping the server should cancel contexts provided to handlers (as argument or through stream's Context().\nSo handlers and sub-goroutines have two ways of knowing they should abort: using context.Done() or when RecvMsg() SendMsg() return an error.\nThis really should be clearly documented answering the question when is a handler's context or stream's canceled.. ",
    "xtaci": "absolutly, doc is not clear enough. ",
    "jan4984": "@iamqizhao how to cancel a strem.recv() for bidi-streaming rpc at server side. I not find any stream mehtod.. @MakMukhi no, I mean the server side.  The stream is allocated by gRPC framewok.. @nhooyr but how can I return if I'm blocking at recv() call. ",
    "nhooyr": "@jan4984 You would return from the goroutine that received the stream.. @jan4984 recv in a different goroutine.. Given @snowzach's solution and h2c being implemented in net/http, this can be closed afaict.. Never mind, no it does not. I misread.. @MakMukhi shouldn't the docs on Close() be updated?. @MakMukhi ping.. It was important for my use case because I wanted to know if Close() would take a long time or not. I was writing a map for grpc connections to different hosts (each host is of the same service type) and after a single host was not used for a while, I wanted to close it.\nBut I wanted to make sure that Close() would not send a message and then wait or anything like that would could take a relatively long time when the map is locked. I could have closed all old connections in a loop after the all old connections were deleted from the map but I was just curious whether it would take a lot of time and if I would need to do that.\nYes, what you suggested would be fine. However, after reading your response, I'm not positive because you're right, it is true for many other things.. > Default to DNS resolver instead of passthrough.\nCould you elaborate on what passthrough would be? Wouldn't all Dial's use a DNS resolver?. Ah, its from https://github.com/grpc/grpc-go/pull/858\nI think this behaviour should be changed or clearly documented on grpc.DialContext. I can whip up the PR if this sounds good.. Or maybe we should just deprecate grpc.DialContext and recommend users who want to timeout the dial use the WithDialer option and use a timeout inside of that.. That all sounds great to me.. I'll keep this open to track the clarification of the docs.. Should mention that in the non blocking case, the ctx does not act against the dial but how long the non blocking dial takes to setup. As in, thats what could potentially make it useful in the non blocking case.. @dfawley would it be possible to add the features necessary into the Go http2 client and then use that instead?. is it possible we can at document the various fields available in the context.Context of endpoints somewhere?. ",
    "ppg": "This does help, thank you; I had missed the doc directory at first, that's a good reference point.\nI had a few follow on questions:\n1. A lot of existing things that care about health checks make it easy to query HTTP style services via a check URL. They mainly pay attention to status code, but can also look for response body matching. I think because this endpoint always returns 200 and the body is binary encoded they can't tie into that, but I was curious if there's some creativity I'm missing that would allow me to tie in say haproxy into checking this endpoint?\n2. Due to 1, it seems like it'd be prudent for most services to build a 'health checker' app that hits that RPC and returns an exit code that service monitors could use (i.e. ala nagios style). Does that sound correct, i.e. is that the anticipated usage pattern? Or is there a different way one would expect to integrate the health endpoint to nagios, consul, etc.?\n3. It seems like 2 would be a consistent enough pattern that it might be something worth investigating adding to either grpc-go or grpc in general, where grpc_health_checker (or whatever) calls the health check endpoint on a server (optional CLI arguments for service names) and then translates the response into a return code?\nThanks,\n\\Peter\n. @therc thanks for the follow up, I'll give it a read and try it out.\n. @MakMukhi Just to clarify the issue is with a c-core client >= 1.5 and a golang server >= 1.3 and < 1.5 right? Would it also affect a c-core server >= 1.5 and golang client >= 1.3 and < 1.5? Either way it sounds like the safest path is upgrade all golang servers/clients to >= 1.5 first; then upgrades to c-core based clients/servers should be safe?. ",
    "mahendrakariya": "Reinstalling protobuf solved the problem. Thanks a lot!\n. ",
    "MarcMagnin": "Hi,\nI have the same issue (Ubuntu 16, Go 1.7, protoc 3)\nAny ideas ?. ",
    "mangrep": "same issue\n\u25b6 uname -a\nDarwin MacBook 16.3.0 Darwin Kernel Version 16.3.0: Thu Nov 17 20:23:58 PST 2016; root:xnu-3789.31.2~1/RELEASE_X86_64 x86_64\n\u25b6 protoc --version\nlibprotoc 3.6.1\n. I did not get any error. It just created Java beans. Service class was not created. \nCommand:\nprotoc --java_out=src/main/java/  /proto/file/path/\n. ",
    "shuaichang": "+1, currently seems there's no way to close a clientconn gracefully, the transport is not accessible from outside of clientconn, so it's not possible to send Goaway to the transport. We implemented a client conn cache and sometime need to evict some conn out of cache and close it, without a graceful Close method, we will have implement something based on timeout, which is sub-optimal comparing to the grpc transport.GracefulClose. Seems the function is there, but not exported out of the package. Cool, I'll +1 in that issue. +1. ",
    "peanutgyz": "use grpc-go/benchmark/server/main.go  and  client/main.go\nwhen runtime.GOMAXPPROCS(runtime.NumCPU()), benchmark failed. \nand go 1.5 also fail\n. crash when use stream mode. \ntry set rpc_type =1 ?\n. \n. ",
    "erikfrey": "Similarly, I'd love to be able to cancel a ServerStream SendMsg to a client that's hung, but it's not clear how.\nI was excited to find that I could grab the underlying transport stream by calling StreamFromContext but it seems you've purposefully chosen not to expose the cancel func.\nCanceling a send to a misbehaving client seems like a common need - what's the right way to do this?\n. ",
    "ldelossa": "Does a cancel of the context on the client side evaluate to an io.EOF error?\nFor example\n...\nfunc (c *Client) Proxy() {\n    // begin proxy loop\n    for {\n        msg, err := c.stream.Recv()\n        if err != nil {\n            if err == io.EOF {\n                log.Printf(\"received EOF. stream closed. exiting proxy loop\")\n                return\n            }\n            log.Printf(\"failed to call Recv on stream: %s\", err)\n            continue\n        }\nIs valid to remove me from my loop if I was to call \"cancel\" in the main go routine? . @menghanl thanks a lot, appreciated. Will change my code around. . ",
    "kelseyhightower": "@iamqizhao Can I take a shot at this one?\nSeems like one or more backwards incompatible changes will be required to the metadata package to enforce gRPC metadata requirements. For example should the metadata.encodeKeyValue function return an error which would then need to propagate through the metadata.New function?\nIdeally we could add a new function to validate function to the metadata package:\nfunc Validate(md MD) error\n. Is this something I can take a shot at? I've been playing with grpc and wonder if the following example would work:\nCreating a grpc error:\nhttps://github.com/kelseyhightower/grpc-hello-service/blob/master/hello-server/main.go#L74\n. Fixed in https://github.com/grpc/grpc-go/pull/506. @jcanizales PTAL\n. @jcanizales I maybe missing something, but it seems I can only set the trailers once using grpc.SetTrailer. This function only takes a metadata type, which supports basic string key/value pairs.\nI'm not sure where to return the QuotaFailure message. Again, I maybe overlooking something.\n. See the gRPC upstream docs for more information regarding naming and discovery.\n. Fixes #511\n. @dsymonds Happy to push this to the Kubernetes org and I agree it makes sense for it to live there. I'll figure out the best place for that to live and create a new PR.\n. I think the best way to check that would be to test the error code:\ngrpc.Code(err) != codes.Unknown\nFrom the docs:\n\nCode returns the error code for err if it was produced by the rpc system. Otherwise, it returns codes.Unknown.\n. \n",
    "ossareh": "@dsymonds I'm very sure I regenerated the protobufs after the upgrade. I'm going to run another test now and I'll let you know soon after that.\n. OK, I can confirm that regenerating the protobufs worked. It required that I run go get -u github.com/golang/protobuf/{proto,protoc-gen-go} which while obvious in hindsight was not while debugging.\nI'd like to discuss this in a non-noisy-for-others forum, @dsymonds would the protobuf mailing list be the most apt place? Ultimately I'm trying to understand what we can do to ensure we don't get bitten by this again. I see this as an unexpected and accidentally incorporated API change; which means there's a relationship which we do not have well encoded in our build flow, I'd be interested in establishing how others' mitigate this sort of issue.\n. ",
    "jcanizales": "You're very very welcome to give it a shot!\nMore specifically, we'd want a sample that shows sending an error with metadata attached in the HTTP/2 trailers, as well as reading that metadata from the client. That being the recommended way to specify error details when the error code isn't enough to make the error actionable.\n. CC: @iamqizhao for review\n. The example and the tutorial text look great to me, and the way it's done in Go is refreshingly easy. I'm not fluent enough to review the Go code in detail, though.\nIf it's not too much to ask, it would be ideal if one of the standard error-detail protos were sent in the trailers (for example, QuotaFailure). They are what Google APIs use, so the situation of a client app having to read one of them is going to be common. And is one step more complicated than sending and receiving text trailers.\n. Oh, interesting! @iamqizhao, how does one set binary headers/trailers in the Go library?\n@kelseyhightower, this is exactly the kind of things I expect we'll find as we create these samples across all languages :)\n. What's the function to serialize a proto message into a string? (I've never used Go protos) Does string accept \\0 bytes in the middle?\n. > using status.FromProto instead of trailer metadata\nTo be clear: On the wire, they'll be sent as trailer metadata, correct? It's important that all languages interoperate in this respect.. What's the type of the proto sent there?\nAnd can we confirm that grpc-status-details-bin is the trailer key used by all languages?. Network errors should be Unavailable IIRC.\n. Re: metadata, see https://github.com/grpc/grpc/issues/4543, whose Go version this PR is trying to solve.\n. ",
    "enocom": "In #506, the APIs on display were grpc.SetTrailer on the server, and grpc.Trailer on the client.\nWith the helpers added in #1358, it seems the preferred way to transfer error details is status.WithDetails on the server and status.Details on the client.\nIs that the gist of the issue here?. @dfawley I would be happy to help close out this issue. Would adding something to examples and Documentation be your preference?. OK. Sounds good. \ud83d\udc4d . @dfawley Would an addition Documentation help close this issue? I would be happy to provide a summary of all the above discussion and that of linked issues.. @menghanl Sounds good. I'll put a document together.. @dfawley Thanks for the detailed response. I'll make the requested changes and update the PR shortly.. @dfawley I've updated the PR. Thanks again for all the feedback.. @dfawley Thanks for your patience. Looks like I pushed an old revision over a newer one. I've double checked the diff and updated the recent commit to include all the discussed changes.. This is an update of #506 using status.WithDetails and status.Details on the server and client respectively.. @dfawley Thank you for the review. I'll expand on the usage as suggested and move the addition to the Documentation directory.. @dfawley I've made the discussed updates. Let me know if you would like me to expand on the documentation.. @dfawley There are likely additional improvements to the wording. What do you think?. Note, this PR likely also needs either a new entry in the Documentation directory or an update to Documentation/grpc-auth-support. \nI propose updating the existing grpc-auth-support.md to include a discussion of the example in this PR.. @menghanl I've revised the wording still further, opting to use \"may\" in place of \"could,\" which seems to follow the spirit of your suggestions.. @menghanl Has the gRPC-go team considered moving the work of vet.sh into the Makefile with something like make vet?. @menghanl That's a very nice addition for contributors. Thanks!. That's much nicer. Will do.. \ud83d\udc4d . Yes, I understand now.. Sounds good.. \f\fWill do.. Nice catch! Done.. Good point. I'll update the example to include the ok on the type assertion.. Good catch. I'll fix this as well.. I think rpc-errors.md sounds good. I'll make the changes and update the PR, adding an initial, general discussion of errors.. Yep. I'll delete these lines.. Sounds good.. Yes! I didn't realize there were certs in the project. I'll use those.. I'll use credentials.NewTLS(&tls.Config{InsecureSkipVerify: true}) as an option on the client to avoid this complexity.. Will do.. Sounds good.. \ud83d\udc4d . Sounds good. I'll update the PR.. \ud83d\udc4d . ",
    "boz": "I can take a look.  What other peer information should be included?  AuthInfo?\n. Updated.\n. Doing so makes a circular dependency between the grpc and grpc/transport packages unless gprc.Peer is an interface (among other things.)  Happy to do this but it might be a bit kludgy.\n. no problem.  thanks!\n. ",
    "gosuri": "+1\nThis will help immensely!\n. ",
    "mgilbir": "This seems to be the same issue that was reported in https://github.com/golang/protobuf/issues/63\nIt is still unclear to me how to handle the situation\n. Thanks @dsymonds! I managed to get it working\n. ",
    "gwpp": "Hi, mgilbir. I get a problem like you. For example:\n- messag.proto:\n```\nsyntax = \"proto3\";\npackage pb;\nmessage Message {\n    string text = 1;  \n}\n```\n\n\n\nlogic.proto in same directory\n    ```\n    syntax = \"proto3\";\n    package pb;  \n    import \"message.proto\";\nservice LogicServer {\n    rpc PostMessage(Message) returns(Message){}\n}\n```\n\n\nAfter execute protoc --proto_path=. --go_out=plugins=grpc:. logic.proto, the logic.pb.go import message use import pb1 \".\". \nHow to fix it? Please help me. Thank you.. ",
    "bits01": "That could work on the client side, but what about the server side? I don't see any useful ServerOptions or another way to bypass the listen/accept.\n. Ah good point, just realized it's an interface. Thanks!\nAny other solutions for the problem I'm trying to solve? Both sides need to act as both server and client at the same time. Ideally I'd like to use a single connection (initiated from the side that can dial out) and do bi-directional gRPC on it but I'm not sure it's possible even with streams. Two connections (both started from the side that can dial out) is the next best thing if I can get it to work with the above hack-ish way.\n. Any good examples you could point to, please?\n. I see. But then there's only one message type for the stream so you'd have to encode multiple message types into a single type, you lose the RPC-ness, dispatching to multiple functions that can take various inputs and return various values.\n. Can the server initiate a new stream on an existing connection? From looking at the examples it seems the client makes a call potentially with multiple streams and then can't make another call until that call is completed. I'm not sure I understand the multiple streams part. In my scenario, the server needs to tell the client (behind the firewall) to do a various things, e.g. doX(x,y) retdata1, doY(z) retdata2, etc. all inputs and returns are of different types. Not sure how the server can make such calls or start streams with the correct data type. Are there any examples?\nThis is turning more into a discussion thread rather than a real issue. Is there a forum or mailing list specific to Go gRPC where we can discuss such things?\nThere is https://gophers.slack.com/messages/grpc/ but it doesn't seem to be very active.\n. @codeitcody My use case was exactly like yours. I ended up not using gRPC and instead I just did straight up net/rpc with https://github.com/hashicorp/yamux for the multiplexed streams. Worked out quite well for me, less overhead in not having to maintain the proto files, I could share the Go structs both on the client and server directly.\nBut I do agree, it would be nice for gRPC to support bidirectional RPC where either side can initiate the connection.. @nbari see my comment above yours. Using https://github.com/hashicorp/yamux you could probably do the same thing and use gRPC instead of net/rpc, but I have not tried it.. ",
    "codeitcody": "Is there any new information on this? @bits01 - did you find a satisfactory solution?\nThe Client/Server duality concept would be very useful  - especially in IOT applications.\nFor example, imagine a IOT Thermostat that we can control remotely through a cloud service. Its plugged in and connected to a home network, thus behind a NAT and Firewall.\n1: Thermostat (client) dials the cloud service (server) via gRPC to create a channel. This can't go the other way around as the cloud service has no ability to dial the thermostat behind the NAT+FIREWALL.\nGreat - so far so good. Thermostat=Client, Cloud Service=Server\n2: Now the cloud service needs to send the Thermostat a command to change its temperature. I would love to have some RPC services registered on the Thermostat such as \n- rpm ChangeTemperature (ChangeTemperatureAsk) returns (ChangeTemperatureResult) {}\n- rpm EnableAwayMode (EnableAwayModeAsk) returns (EnableAwayModeResult) {}\nand so on...\nbut now the Thermostat=Server and the Cloud Service=Client... gRPC does not handle this without some implementation burden on both client and server sides like @dsymonds suggested.\nThe only solution to avoid running a client and server on both ends and instead created a bidirectional stream:\n- rpc Link (stream UpLink) returns (stream DownLink) {}\nwhere the UpLink and DownLink message just basically have a string in which I parse on the client: {changeTemp: 55}. Pretty bad code smell in gRPC.. @nbari I ended up just creating my own channel using HTTP/2 based transport. I did a pub/sub thing instead of rpc. As @bits01 suggested I also think you could coerce gRPC into doing this with some work. :/. ",
    "nbari": "@codeitcody @bits01 I am facing the same issue, mainly with nodes behind a firewall that can connect to the server but I would like to use that same connection to send back data.\nI am thinking about using zeromq - what else have you found to solve this?\n. ",
    "dustin-decker": "I have gRPC servers behind firewalls that I'd like to subscribe to events on from a publicly routable gRPC client. I'd like to do this as simply and lightweight as I know possible by having the server make a TLS TCP connection to the publicly routable client listener, and then start gRPC communication on that established net.Conn.\nDialing from the client with a custom grpc.WithDialer() that returns the provided net.Conn seems pretty straight forward. I don't see an easy way to use a provided net.Conn for the server though.\nCould someone elaborate on how to get grpc.Server to use a provided net.Conn? Maybe provide an example? Sorry if I missed something obvious, and thanks.. Thanks for the tip @menghanl.\nI've uploaded some example client/server code that will do what I need here: https://github.com/dustin-decker/grpc-firewall-bypass\nclient initiates a TCP connection to server with client's gRPC server listening on the connection, and then then the server's gRPC client connects to the client's gRPC server over the incoming TCP connection.\nI wish it wasn't required to do like that, but there it is.\nAdditionally I found a different and more complicated route someone took of sending binary messages over a websocket: https://github.com/glerchundi/grpc-boomerang. ",
    "wizardsd": "Is it possible to reuse the same connection with C++ backend?. ",
    "xfxyjwf": "@iamqizhao Could you take a look at this? The old link is broken after I removed the INSTALL.txt file.\n. ",
    "svanharmelen": "Anyone any pointers?\nThx!\n. Thanks @iamqizhao! This was just the info and pointer we needed :grinning: \n. @iamqizhao it works like a charm! I now make a health check call from the client to the server every 30 seconds which prevents HAProxy to timeout after 50 seconds :grinning: \nThanks again!\n. @HuKeping I would keep any calls to the health endpoint separate from other calls (so not mix it with any of your own calls/endpoints).\nAs normally (IMO) you would like to use the health endpoint for periodically checking the health of your service and use that info in your client to make decisions based on the state of your service.\nOr (like in my case) abuse this to keep open a TCP session when you use a gRPC stream that sends updates with a too big interval for some proxies you might pass (which causes your connection to be dropped).\n. ",
    "HuKeping": "hi @svanharmelen what's the best practise to add the health checker service into an exist service? Create another structure to hold the old one and the health checker or sth else? \n. @svanharmelen  Thanks!\n. ",
    "liujiongxin": "Thanks, I think you are right, the call site should guarantee the options are valid, and do not trigger undefined behavior.\n. ",
    "dpetersen": "@iamqizhao thanks so much for looking at this. Here is the output I see, first from a successful run and then from an unsuccessful one:\nroot@ubuntu-2gb-sfo1-01:~# /root/gopath/bin/client localhost:4000\n2016/01/22 22:13:09 connected\n2016/01/22 22:13:09 rpc error: code = 12 desc = \"unknown service Foo\"\nroot@ubuntu-2gb-sfo1-01:~# /root/gopath/bin/client localhost:4000\n2016/01/22 22:13:09 connected\nIt outputs the \"connected\" message and then sits until I Ctrl-c it. I just tried this fresh on a couple of brand new Ubuntu DigitalOcean instances, versions 14.04 and 15.10, with the same results. I've documented every keystroke I typed in after SSHing in this gist: https://gist.github.com/dpetersen/2f2149aa4fb939cb63d8\nFor fun I just installed Go via homebrew on my OSX laptop, which I never use for development, and did a go get and was able to run it 500 times in a row successfully. So I have no idea what the difference is.\n. If there are any debug or verbosity options I can use on the server or client end, I would love to try them out. In an earlier version of this code that wasn't so stripped down, I was running the /debug/requests and /debug/events endpoints and never saw a request for any of the failed connections.\n. On the server you see nothing, on the client you see this, with that time counter just spinning along indefinitely while the request is hung.\n\nI took some time trying to figure out why it says to: <nil>, but as far as I can tell it doesn't ever set the destination address. Successful requests also say to: <nil>.\n. The server block endpoint is:\n--- contention:\ncycles/second=1999821374\nand the client is:\n--- contention:\ncycles/second=2000000938\nThe trace files are on S3:\n- server\n-  client\n. I've been looking at this further. If the client that I'm using to make the connection is not a streaming client, it always works as expected.\nMy real-life issue is with bidirectional streams. I've found that it works 100% of the time (in my small test case and in my real app) if I always send a message before I attempt to receive. I've updated my demo codebase. With this commit, it always works. If you remove the Send call, it continues to periodically fail.\n. Your changes in #505 fixes my demo app and my real application, they connect consistently whether I send first or not. Thanks so much for looking at this so quickly! I really appreciate it, I don't think I could have found the fix.\n. ",
    "daaku": "Would you happen to know where this is sent? I'm looking at https://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L507. I see the Framer writes the length in endWrite but that's just for the frame not the entire message right?\nDigging into it so far it seemed like protobuf the Codec needs to encode the length of the serialized struct before it when it's considered a Message.\n. I also see http://www.grpc.io/docs/guides/wire.html which mentions Message-Length but unclear to me if \"Delimited-Message\" is one entire Message or the data frames for a single message and if the length refers to the former or the latter.\n. I came across this which was super helpful to understand the wire protocol: https://github.com/bradfitz/grpc-go16-demo/blob/master/demo.go#L56\nIn that code it seems like the gRPC wire protocol requires the Message-Length prefix as part of the 5 byte header and then the payload itself has a message length prefix because that's what protobuf does. I guess that answers my question but I'm still left wondering why gRPC requires it.\n. ",
    "marshome": "I signed it\n. I signed it\n. thx for reply=.=\nI am using grpc like this:\nuser send http/json or http/protobuf request to my api gate,then gate will send the request to real rpc service.\nso I try to add two concepts:support json or protobuf codec per message & support send serialized data directly.\nnow I add an new InvokeWithCodec method to use temporary=.= @iamqizhao \n. ",
    "johansja": "Any follow up on this? I am about to start tinkering on how to use the client side load balancing inside a Kubernetes cluster.\n. Perhaps this function can help?\n. ",
    "LetsUseGerrit": "Gerrit code review: https://go-review.googlesource.com/19272 (at git rev 61d3357)\n. Gerrit code review: https://go-review.googlesource.com/19272 (at git rev eb0e517)\n. Gerrit code review: https://go-review.googlesource.com/19272 (at git rev 81d512c)\n. Gerrit code review: https://go-review.googlesource.com/19272 (at git rev d0f39dd)\n. Gerrit code review: https://go-review.googlesource.com/19272 (at git rev 7346c87)\n. Gerrit code review: https://go-review.googlesource.com/19426 (at git rev 854ad34)\n. Gerrit code review: https://go-review.googlesource.com/19427 (at git rev 07d3de8)\n. Gerrit code review: https://go-review.googlesource.com/19433 (at git rev 3d9421a)\n. Gerrit code review: https://go-review.googlesource.com/19434 (at git rev a62244e)\n. Gerrit code review: https://go-review.googlesource.com/19435 (at git rev 8aa7cbb)\n. Gerrit code review: https://go-review.googlesource.com/19436 (at git rev 60f2c4e)\n. Gerrit code review: https://go-review.googlesource.com/19451 (at git rev 4e9bdfe)\n. Gerrit code review: https://go-review.googlesource.com/19451 (at git rev fa64708)\n. Gerrit code review: https://go-review.googlesource.com/19451 (at git rev 35e8ba4)\n. Gerrit code review: https://go-review.googlesource.com/19451 (at git rev f8861d0)\n. Gerrit code review: https://go-review.googlesource.com/19451 (at git rev 303da9d)\n. Gerrit code review: https://go-review.googlesource.com/19451 (at git rev ffa8131)\n. Gerrit code review: https://go-review.googlesource.com/19480 (at git rev b4c526a)\n. Gerrit code review: https://go-review.googlesource.com/19480 (at git rev 9c3ba3a)\n. Gerrit code review: https://go-review.googlesource.com/19480 (at git rev 6a305b0)\n. Gerrit code review: https://go-review.googlesource.com/19480 (at git rev d2d9c0a)\n. Gerrit code review: https://go-review.googlesource.com/19521 (at git rev b109742)\n. Gerrit code review: https://go-review.googlesource.com/19866 (at git rev 2d2d0dc)\n. Gerrit code review: https://go-review.googlesource.com/19866 (at git rev 89754ea)\n. Gerrit code review: https://go-review.googlesource.com/19866 (at git rev 0c64cd8)\n. Gerrit code review: https://go-review.googlesource.com/19866 (at git rev beba3ca)\n. Gerrit code review: https://go-review.googlesource.com/19866 (at git rev deaabfc)\n. Gerrit code review: https://go-review.googlesource.com/19866 (at git rev 479781d)\n. Gerrit code review: https://go-review.googlesource.com/19866 (at git rev 110fd99)\n. Gerrit code review: https://go-review.googlesource.com/19903 (at git rev a1adcd4)\n. Gerrit code review: https://go-review.googlesource.com/19903 (at git rev 15de328)\n. Gerrit code review: https://go-review.googlesource.com/19903 (at git rev 8241265)\n. Gerrit code review: https://go-review.googlesource.com/19903 (at git rev ac5c8d7)\n. Gerrit code review: https://go-review.googlesource.com/20077 (at git rev fe861cb)\n. Head d614b8ab18d51bcba04f5e2dc6f5d564e2f296ec has 2 commits. Please squash your commits into one. @LetsUseGerrit only supports syncing a pull request with a single commit, as that is how Gerrit is typically used.\n. Gerrit code review: https://go-review.googlesource.com/20077 (at git rev f3c6dc5)\n. ",
    "maniksurtani": "This isn't the only test that uses Fatalf ... \n$ git grep t.Fatal | grep _test.go | sed -e 's/:.*//' | uniq\ncall_test.go\nclientconn_test.go\nmetadata/metadata_test.go\npicker_test.go\nrpc_util_test.go\ntest/end2end_test.go\ntransport/http_util_test.go\ntransport/transport_test.go\nMakes sense to fix them all?\n. Race condition with #516?\n. Never mind, just saw comment re: CLA bot.\n. Nice. +1 to providing this as a part of the library, as @kazegusuri pointed out, to prevent multiple implementations emerging (something I think is bound to happen if the library doesn't provide such widely useful capability).\n. +1 to a proper Stop() and not relying on a crash/exit. But maybe that should be a separate PR.\n. ",
    "hunnain": "Can you tell me how i request grpc server from browser whenever i called browser give me an error of cross origin i have seen 75 and 514 but didn't understand. ",
    "sailat": "We are working on batching requests this quarter to improve perf. Will update this thread with available date soon. . @tamird @bdarnell  are you still seeing this issue?. ",
    "coveralls": "\nCoverage decreased (-0.06%) to 68.894% when pulling accbf4c185541dad9085ad3e341d22684527ccb1 on tamird:regen-protos into 66b94b9f6b45652d6824be23517979d31f27cd15 on grpc:master.\n. \n\nCoverage increased (+0.5%) to 69.74% when pulling 854ad3492a0bc6703c6998c56f58a58504c7f1b8 on iamqizhao:master into e390a330991433b3433d7340d85995bb815027f1 on grpc:master.\n. ",
    "theraphim": "I've just wasted a few hours on this, because in my case crash was happening without any diagnostic info or anything (just exit 1, I suppose).\n. ",
    "songshine": "I got this error:\ngrpc: Server failed to encode response proto: Marshal called with nil\nRefer to https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L265,  the nil message case has been considered. So the msg is not a nil interface{} here but the real object of msg is nil.  Shouldn't this be fixed?. @menghanl please think about the example from @ashishgandhi \nfunc (*server) Do(context.Context, *test.Empty) (*test.Empty, error) { return nil, nil }\nso when the code run into the line https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L265,\nif msg != nil {\n...\n}\n the msg is (*test.Empty)(nil), but msg != nil will return true. This is an expected behavior?. ",
    "dvyukov": "\nDmitry, what are the limits on how many goroutines the race detector can track?\n\n8192 simultaneously alive goroutines is the limit.\n. ",
    "Thomasdezeeuw": "Do mean Log(context.Context, error) or Log(context.Context, string)?\nAlso with a API change could Fatal be removed completely? I realise that there are situations where it would be required, but not being to able to cleanup on exit (by using os.Exit(1)) can have some bad side effects.\n. But how would you log (debug) information? Or are you planning on adding log levels to the context?\n. You need to update grpc and protobuf, run go get -u google.golang.org/grpc and go get -u github.com/golang/protobuf, see https://github.com/grpc/grpc-go/issues/643.\n. This (https://github.com/golang/go/issues/17040) makes switching to the new context package a little easier, allowing go tool fix to rewrite the code for you.\n. Can't this be archieved using a custom codec? Using https://godoc.org/google.golang.org/grpc#WithCodec and https://godoc.org/google.golang.org/grpc#CustomCodec.\nOtherwise people not using flatbuffers still import the package.\n. And what about adding Cap 'n Proto? And after that Apache Avro, then Apache Thrift, etc. Then we'll need to import 5 more packages, without even using them. My point is that this can already be achieved without adding code to gRPC, so I don't think this needs to be added to gRPC. But that is just my opinion.\n. Shouldn't this be snappyCompressor?\n. I don't think meta alone would be correct here, according to wikipedia meta is \"a prefix used in English to indicate a concept which is an abstraction from another concept, used to complete or add to the latter\". [1] Meaning meta only really says nothing. Metadata would be correct here.\n[1] https://en.wikipedia.org/wiki/Meta\n. I don't think we'll agree here and since it's a unexported variable I don't care too much about it either. So even though I would prefer metadata, meta seems fine.\n. Type-o: iff -> if.\n. Ah never seen that before, good to know, thanks.\n. Why does this needs to be another package? Can't it ben inside the grpc package?\n. Why the K prefix for all the fields?\n. Why not -1 as infinite?\n. This line can be removed, false is the default.\n. Change this to 20 * time.Second, the is is much clearer and won't need the comment.\n. Add a comment about being atomic?\n. It's common to just use an uint64, rather then a reference. For example see http.Server code: https://github.com/golang/go/blob/7dc97d9e328edc800e2ce41d5b211ef4e0ef41d0/src/net/http/server.go#L2363-L2364.\n. Rather then exposing this lock, why not use functions for it. e.g.\ngo\nfunc Enabled() bool {}\nfunc Enable() {}\nfunc Disable() {}\n. ",
    "ryszard": "Hi, are there any updates here? I really hate having to pass to my binaries both -port and -debug_port, and it's not really trivial to get this to work just from looking at the code in #514...\n. @xiang90 Thanks a lot for the link! However, it doesn't really work for me, and it's not obvious to me if I am doing something wrong, or there's something about using grpc-gateway, swagger, etc. that makes it work in the example.\nMy code looks kinda like this:\n``` go\n// grpcHandlerFunc returns an http.Handler that delegates to\n// grpcServer on incoming gRPC connections or otherHandler\n// otherwise. Copied from cockroachdb.\nfunc grpcHandlerFunc(rpcServer rpc.Server, otherHandler http.Handler) http.Handler {\n    return http.HandlerFunc(func(w http.ResponseWriter, r http.Request) {\n        // TODO(tamird): point to merged gRPC code rather than a PR.\n        log.Info(\"here\")\n        log.Infof(\"r: %#v r.Header: %v\", r, r.Header)\n        if r.ProtoMajor == 2 && strings.Contains(r.Header.Get(\"Content-Type\"), \"application/grpc\") {\n            log.Infof(\"in grpc\")\n            rpcServer.ServeHTTP(w, r)\n        } else {\n            log.Infof(\"in normal server\")\n            otherHandler.ServeHTTP(w, r)\n        }\n    })\n}\nfunc main() {\n    // other, unrelated stuff\nserver := rpc.NewServer(opts...)\ndm := NewMyServer()\npb.RegisterCapacityServer(server, dm)\nsrv := &http.Server{\n    Addr:    fmt.Sprintf(\":%d\", *port),\n    Handler: grpcHandlerFunc(server, http.DefaultServeMux),\n}\n\nlog.Exit(srv.Serve(lis))\n\n}\n```\nThe HTTP part works fine, but as soon as I try to send an RPC using a Go client, then it goes in the wrong branch. The request looks like:\ngo\n&http.Request{Method:\"PRI\", URL:(*url.URL)(0xc82043e780), Proto:\"HTTP/2.0\", ProtoMajor:2, ProtoMinor:0, Header:http.Header{}, Body:(*struct { http.eofReaderWithWriteTo; io.Closer })(0xc97990), ContentLength:0, TransferEncoding:[]string(nil), Close:false, Host:\"\", Form:url.Values(nil), PostForm:url.Values(nil), MultipartForm:(*multipart.Form)(nil), Trailer:http.Header(nil), RemoteAddr:\"[::1]:57688\", RequestURI:\"*\", TLS:(*tls.ConnectionState)(nil), Cancel:(<-chan struct {})(nil)} r.Header: map[]\nSo, it goes down the wrong path because the content-type is not set the to application/grpc.\nAll this both in Go 1.5 and 1.6.\n@bradfitz Can you please shed some light on this?\nOh, and this is for experimental code. I am fine with performance regressions, bugs, etc.\n. ",
    "BrianHicks": "I'm seeing the same behavior in Go 1.7.\n. ",
    "atombender": "\nThe http.Handler-based server only works with TLS right now.\n\nWhat's needed to support non-TLS?\n. @Raffo No idea.. This is useful functionality. Why not provide it in a utils subpackage?\n. ",
    "devoid": "Ok digging through issue https://github.com/grpc/grpc-go/issues/75 and this one, I found that @ryszard's example in https://github.com/grpc/grpc-go/issues/549#issuecomment-191455642 worked for a TLS enabled http.ServeMux.\nCan we at least document this approach if it is workable for users of TLS?. Hi @iamqizhao, I may be able to explain the use case. I am already using the getter functions generated by protoc-gen-go to define interfaces that can access common data shared across multiple protobuf types:\n```\nmessage GetUserResponse{\n    bool valid = 1;\n    // ... etc.\n}\nmessage GetGroupResponse{\n    bool valid = 1;\n    // .... etc.\n}\nAnd then in a separate Go package: \ntype maybeValid interface {\n    GetValid() bool\n}\nfunc IsValid(response interface{}) bool {\n    if t, ok := response.(maybeValid); ok {\n        return t.GetValid()\n    }\n    return false\n}\n```\nAnd now I can read multiple different protobuf messages with the same function! But I can't create ways to write into multiple different protobuf messages. Thats where having setter functions would be helpful:\n```\ntype invalidator interface {\n    SetValid(bool)\n}\nfunc Invalidate(response interface{}) {\n    if t, ok := response.(invalidator); ok {\n       t.SetValid(false)\n    }\n}\n```\nI'll add that the easy approach is just to hand-write whatever methods you need in a separate file in the same package as the generated code. Not 'clean' but it works well.. ",
    "Raffo": "@atombender Do you have an idea if this got fixed or if there is any plan to fix that? It's really annoying and I would need HTTP for production as well, I don't need termination in the APP (I am using a proxy). . ",
    "seiflotfy": "updated with code references and new pprof output (simplified the go code)\n. I now tried to start the server and take a base.heap snapshot then run the client once and take a current.heap snapshot. https://www.dropbox.com/s/i8d6yhqz9m3mudf/skizze-mem.svg?dl=0\nwhite top is saying\n(pprof) top\n33.62MB of 33.62MB total (  100%)\nDropped 11 nodes (cum <= 0.17MB)\nShowing top 10 nodes out of 23 (cum >= 31.53MB)\n      flat  flat%   sum%        cum   cum%\n   23.53MB 69.99% 69.99%    31.53MB 93.78%  github.com/golang/protobuf/proto.(*Buffer).dec_slice_string\n       8MB 23.79% 93.78%        8MB 23.79%  github.com/golang/protobuf/proto.(*Buffer).DecodeStringBytes\n    1.09MB  3.24% 97.02%     1.09MB  3.24%  html.init\n       1MB  2.98%   100%        1MB  2.98%  internal/golang.org/x/net/http2/hpack.addDecoderNode\n         0     0%   100%    31.53MB 93.78%  datamodel/protobuf._Skizze_Add_Handler\n         0     0%   100%    31.53MB 93.78%  github.com/golang/protobuf/proto.(*Buffer).Unmarshal\n         0     0%   100%    31.53MB 93.78%  github.com/golang/protobuf/proto.(*Buffer).unmarshalType\n         0     0%   100%    31.53MB 93.78%  github.com/golang/protobuf/proto.Unmarshal\n         0     0%   100%    31.53MB 93.78%  github.com/golang/protobuf/proto.UnmarshalMerge\n         0     0%   100%    31.53MB 93.78%  google.golang.org/grpc.(*Server).Serve.func2.1.1\n. ",
    "vgough": "I just came by to say that don't believe insecure port sharing to be rare.  It is very convenient to setup grpc (& other http services) on insecure ports during development, and it is also convenient to reduce the number of ports in order to simplify firewalls and avoid port conflicts. I had been in the process of trying to consolidate ports for a project when I came across this issue. \n. ",
    "glerchundi": "\nI do not see the reason you need to stick to a single port shared by http2 and grpc. You can have a grpc server and http server listening on 2 different ports in this case.\n\nWhat about grpc-gateway and grpc server working on the same endpoint? For example like @philips did in his  philips/grpc-gateway-example repo.\n. > at least another quarter\nNow that one quarter has passed, any chance to make any progress on this?\n. @dfawley I would like to thank you for keeping this going forward. Much appreciated and keep up the good work!. ",
    "AlekSi": "I also do want to have gRPC and REST APIs on the same port. It is possible with TLS (see link in the previous comment), but not without it. And local developing with self-signed certificates is still cumbersome.. Cross-reference to implement a workaround with cmux:\nhttps://open.dgraph.io/post/cmux/\nhttps://github.com/soheilhy/cmux. ",
    "dhrp": "Sorry for the little self-promotion, but I've written a comprehensive article about this issue, in particular about the issues with ServeHTTP() for the purpose of multiplexing to the grpc-gateway. I also discuss some alternatives... I struggled with the details of this issue and think others may not need to.\nWhy choose between gRPC and REST? \nOptions for letting your service expose both REST and gRPC API\u2019s on a single port.. ",
    "snowzach": "I finally figured this one out. You can wrap your handler function in the h2c helper if you want to disable TLS\n```\nimport (\n         \"net/http\"\n    \"golang.org/x/net/http2\"\n    \"golang.org/x/net/http2/h2c\"\n)\ns.server = &http.Server{\n    Addr: \":8080\",\n    Handler: h2c.NewHandler(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n        if r.ProtoMajor == 2 && strings.Contains(r.Header.Get(\"Content-Type\"), \"application/grpc\") {\n            grpcServer.ServeHTTP(w, r)\n        } else {\n            mux.ServeHTTP(w, r)\n        }\n    }), &http2.Server{},\n}\n```. Thanks @menghanl is there any other people that have done this? Maybe this isn't really the problem. When I close the Stream from the other side, my benchmark code speeds up dramatically. \nIs there no notion of buffering on SendMsg or is it tying up the machine. I will try marshalling to bytes and see if this makes it faster in testing.. Okay, I implemented a custom codec that overrides the proto codec. Basically if it detects a []byte it will pass it through untouched, if it's anything else it sends it to the real proto codec. This did have some effect but not really as much as I was hoping. It just seems like there's a limit to the throughput of the stream. Is this normal? Should I only expect so much and open multiple streams?. I'm streaming lots of 4k messages. Do I need to pack more into the same message in order to make it faster? I was hoping using a stream would negate the need for larger messages. The encoding/compression also seems to cause performance issues as if I enable compression I get approximately a 20% decrease in message throughput. \nI looked at the code for a little bit.. I'm certainly not an expert and I'm sure it's not this simple (it never is) but would it be possible to make Send thread safe, handle message marshaling/compressing asynchronously and have maybe a small channel/buffer such that messages can get queued up as to send them more efficiently on the wire?\n. To illustrate my point, I did some more testing (I can provide code examples if you need) but basically I created a simple bidirectional stream and the message had one field which was a single google.protobuf.Value. I opened the stream and the server sent that single message as fast as it could.\nI used the jsonpb library to fill that value with a single string (from JSON) that when marshalled was about 4k. I was able to send this message at 36k times per second.\nI then used the jsonpb library to fill that value with a complex struct generated from https://www.json-generator.com/. When converted to protobuf(bytes) it was also about 4k. I was only able to send this message about 3.5k times per second. Same message size on the wire, 10 times slower. \nThis is having dramatic effect on my throughput. It even seems like if I open more than one connection it really doesn't get substantially faster. I have also opened multiple connections to the same server to try to increase the speed however beyond about 2 connections it doesn't make any difference in throughput. \nWith a complex protobuf message generating about a 4.3k message (after marshaling to []bytes) , I can only hit about 6k mps or 25 MB/s from a client/server on the same host over multiple streams.. @MakMukhi I have been playing around with exactly what you suggest and it makes a huge difference in the performance. I get that the protobuf I am using is complicated. I guess the point I am trying to make is that the Send function of GRPC shouldn't be single threaded (for a stream especially) if it's going to include serialization (or compression)\nI also have the same problem with compression. Since it takes a small amount of time to compress the message, my message per second throughput drops dramatically and I suspect it's because the send queue is getting starved for data.\nIf Send was multi threaded and handled everything it could (encoding/compression) simultaneously and then send the data, it could be much faster. \n. @steve-gray I ended up just implementing my own stream wrapper along with my own intelligent protobuf codec. Essentially, my protobuf codec, when handed []byte will pass it through making the assumption it has already been marshaled from a protobuf struct to []byte. Then, I implemented a buffered wrapper that wraps the send function and will handle marshaling and unmarshaling protobuf structs to/from []byte in parallel and then using SendMsg to move them on. Using this, I've massively increased the throughput. . @suyashkumar I had the same issue. When I was sending large messages, the compress and encoding actually took a lot longer than the writing. Check out my comment in https://github.com/grpc/grpc-go/issues/1879 for what I did. I was able to saturate my network link after that.. ",
    "rjammala": "/cc @bradfitz \n. Thanks @bradfitz \n. ",
    "yang-g": "LGTM\n. The c side PR is https://github.com/grpc/grpc/pull/5327\n. This cause grpc-go to reject valid metadata from C-based libraries.... Maybe you can check whether input_length % 4 == 0.... Thanks for the quick fix.\nAlso thanks to @menghanl for helping me debugging.. FWIW I sent out this PR a while back trying to get it mentioned in the\nspec. https://github.com/grpc/grpc/pull/12825\nThat being said, I think nothing stops you to send something custom if you\ncontrol both client and server.\nOn Fri, Dec 7, 2018 at 10:02 AM Akshay Shah notifications@github.com\nwrote:\n\nAny updates on this?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/2329#issuecomment-445315105, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJp0CvKVmcoVaWIMR_uylaHAoAURuMSyks5u2q1GgaJpZM4W3pHM\n.\n. \n",
    "onlyjob": "Tags/releases are useful for downstream package maintainers (in Debian and other distributions) to export source tarballs, automatically track new releases and to declare dependencies between packages. Read more in the Debian Upstream Guide.\nVersioning provides additional benefits to encourage vendoring of a particular (e.g. latest stable) release contrary to random unreleased snapshots.\nPlease consider tagging releases.\nThank you.\nSee also\n- http://semver.org/\n- https://en.wikipedia.org/wiki/Software_versioning\n. IMHO it should wait till protobuf v3 is released: https://github.com/google/protobuf/releases\n. ",
    "mitake": "@dsymonds @zellyn thanks a lot for your reply. I agree that this problem isn't gRPC specific one. So closing it.\n@zellyn so are you using your own modified (not upstream) version of x/net/trace?\n. @zellyn thanks, I'll try the building again.\n. Thanks for your reply, @iamqizhao . Do you mean gRPC provides APIs for passing per-rpc tokens to existing connections? Could you tell me the names of the APIs?\n. Thanks for your pointer. However, the metadata API would be a thing used from server side (I'm not fully sure this understanding is correct or not). Is there a API that can be used from client side?\n. @iamqizhao Thanks, the e2e test file seems to be a good source of example usages. I'm closing this issue because your answers satisfied my question :)\n. @dfawley thanks for the pointer and I'm glad to see the problem was fixed. Will the format of the parameter be changed in the future?. @dfawley thanks for the answer. And I don't have problems for now. Will #1433 be backported to 1.5.x?. I understand. Thanks a lot for your help!. ",
    "immesys": "Please reopen this, this is a problem. I have an application that exposes a grpc service, and the application happens to use etcd which happens to also use grpc. I can't change how etcd vendors grpc, so how do I solve this problem?. Perfect, thank you!. Sorry, I can reproduce this even without the GZIP compressor. I still don't know how to debug it though. The only error I get on the client with loglevel set to info is:\npickfirstBalancer: HandleSubConnStateChange: 0xc421bc6160, TRANSIENT_FAILURE\n. The client respects those variables, but the server does not, perhaps the etcd client in the server (which uses grpc for its own stuff) is overriding it? Is there a way to set the log level in the code?\nEDIT: I see,  etcd calls SetLoggerV2, pity that is global, but I can work around that.. I finally got a new piece of information. The server prints this shortly before the client disconnects:\nERROR: 2018/02/24 19:16:02 transport: Got too many pings from the client, closing the connection.\nERROR: 2018/02/24 19:16:02 transport: Error while handling item. Err: connection error: desc = \"transport is closing\"\nWhat does \"too many pings\" mean?\nThe client prints this with the new verbosity:\nINFO: 2018/02/24 19:16:02 Client received GoAway with http2.ErrCodeEnhanceYourCalm.\nINFO: 2018/02/24 19:16:02 pickfirstBalancer: HandleSubConnStateChange: 0xc420b7c4b0, TRANSIENT_FAILURE. Incidentally, neither the server nor the client passes any keep alive parameters, everything is default. But both sides are built with the same version of grpc, neither specify any keepalive parameters and it only happens under heavy load.  . Thanks for the suggestion. I have set both of those options (to 1MB) and will leave it running overnight. It typically takes somewhere between 2 minutes and 20 minutes under load to trigger, so a few hours should be conclusive.. It's been running without any problem for about 9 hours, so I think that confirms that the BDP estimator is at fault. I know nothing about how pings or BDP estimation works inside grpc, but surely if the pings are sent when data goes from client to the server and the pingStrike counter is only reset when data is sent the other way then if the RPC operations spike in latency (e.g mine go to hundreds of milliseconds) then you can exceed the counter simply because the server isn't yet responding to any RPC calls so isn't doing any resets?. I made a simple reproducer where a program connects to itself and invokes an operation that has roughly the latency I observed with roughly the concurrency I had, but it doesn't seem to trigger it, sorry. Let me know if there is anything else I can do to help. I didn't succeed in making a standalone reproducer but I can reliably reproduce it in-situ even on completely different hardware. More than willing to help track this down, just let me know how.. @dfawley no, there is no http2 proxy involved. I was initially running in kubernetes, but I have reproduced it on bare metal with no fancy networking at all.\nEDIT: actually I lie there is probably some overlay networking still going on, but that would be at layer 3, nothing should be injecting HTTP2 pings. Ok I reproduced it with the debug statements in place. \nHere are the changes I made https://gist.github.com/immesys/be704af11354ba943f35b801abc9caf8\nHere are the 2000 lines preceding the event. I included two files, one with all the log output and one filtered to only include log output from the specific http2Server that tripped (there are 10 concurrent connections): https://gist.github.com/immesys/aff2b454b9e0eab843e062e76bfb65c3\nLet me know if I can re-run this with different print statements. @dfawley yeah in the entire log the OTHER PING DATA print statement never triggers, so I think this is just BDP pings.. Is there a reason the ping strikes are only reset when sending headers in itemHandler? Why not when sending dataFrame?. To save you reading the whole logs, here is the important sequence:\n1519942464739172903 BDPDEBUG[http2Server=0xc423778f00] writing data1\n1519942464739261537 BDPDEBUG[http2Server=0xc423778f00] handlePing\n1519942464739266862 BDPDEBUG[http2Server=0xc423778f00] addPingStrikes2\n1519942464739318578 BDPDEBUG[http2Server=0xc423778f00] writing data1\n1519942464739452781 BDPDEBUG[http2Server=0xc423778f00] handlePing\n1519942464739459106 BDPDEBUG[http2Server=0xc423778f00] addPingStrikes2\n1519942464739535670 BDPDEBUG[http2Server=0xc423778f00] writing data1\n1519942464739659435 BDPDEBUG[http2Server=0xc423778f00] handlePing\n1519942464739674955 BDPDEBUG[http2Server=0xc423778f00] addPingStrikes2\n1519942464739678469 BDPDEBUG[http2Server=0xc423778f00] >maxPingStrikes\nBasically because we are writing three data frames and no header frames, we don't reset the ping strikes.. Should it be before writing it to the wire or just after (the header is just after) ?. It's been running for a while with no problems. I'll run it overnight and see. Does this completely remove the race condition or just make it harder to hit?. Still didn't hit this case now after four hours. I stopped the test because I needed the machine for a different test. Is it always the case that every data frame gets an interleaved reply 1:1 ? You can't queue a few frames in the TCP window, clearing the reset strikes after each and then receive the three pings in response all together? I don't know the code, just wondering. Well it only happens after a few minutes under very high load. I think it has to do with the order that goroutines get scheduled. We could probably make a reproducer by putting in some sleeps in the code in the right places. Having a hard time reproducing this with a minimal example. This might be my fault.... Sorry for the noise, this was indeed my fault. . ",
    "sdboyer-stripe": "@immesys using dep will fix the issue for you - it'll ensure there's only one copy in vendor.. ",
    "enisoc": "I never found out the status of the fix. I just stripped all the newlines from my error messages.. ",
    "tecbot": "I switched my server implementation to use ServeHTTP of the gRPC server which was merged some days ago, now it works. So the issue is in the http2 part of grpc.\n. The response from ServeHTTP looks different then the Serve implementation of the gRPC server.\nFollowing the framing logs with ServeHTTP.\n- serve_http_without_proxy.txt\n- serve_http_with_proxy.txt\nIf you compare it with the logs from above with Serve, you can see that the trailers are set now.\n. Yes, the setup is grpc-client<==>nghttp2 proxy<==> server.\nFlow with Serve:\n```\nRequest headers from client\n:method: POST\n:scheme: http\n:path: /test.Test/Hello\n:authority: localhost:3000\naccept: /\naccept-encoding: gzip, deflate\nuser-agent: nghttp2/1.9.0-DEV\ncontent-type: application/grpc\nx-forwarded-proto: http\nvia: 2 nghttpx\nte: trailers\nResponse headers from grpc server\n:status: 200\ncontent-type: application/grpc\ngrpc-status: 12\ngrpc-message: unknown service test.Test.Hello\nResponse headers from proxy\n:status: 200\ncontent-type: application/grpc\ngrpc-status: 12\ngrpc-message: unknown service test.Test.Hello\nserver: nghttpx nghttp2/1.9.0-DEV \nvia: 2 nghttpx\n```\nFlow with ServeHTTP:\n```\nRequest headers from client\n:method: POST\n:scheme: https\n:path: /test.Test/Hello\n:authority: localhost:3000\naccept: /\naccept-encoding: gzip, deflate\nuser-agent: nghttp2/1.9.0-DEV\ncontent-type: application/grpc\nx-forwarded-proto: https\nvia: 2 nghttpx\nte: trailers\nResponse headers from grpc server\n:status: 200\ncontent-type: application/grpc\ntrailer: Grpc-Status\ntrailer: Grpc-Message\nResponse headers from proxy\n:status: 200\ncontent-type: application/grpc\ntrailer: Grpc-Status\ntrailer: Grpc-Message\nserver: nghttpx nghttp2/1.9.0-DEV\nvia: 2 nghttpx\n```\nAs you can see, there is already a difference in the headers which are sent back from the server.\nIn the other logs above you see the data which is send, and there is also a difference, because the error message will be send separately with ServeHTTP. IMO the flow of ServeHTTP looks more correct to me.\n. These are the data sent from the gRPC server with Serve:\n[  0.005] recv (stream_id=13) :status: 200\n[  0.005] recv (stream_id=13) content-type: application/grpc\n[  0.005] recv (stream_id=13) grpc-status: 12\n[  0.005] recv (stream_id=13) grpc-message: unknown service test.Test\n[  0.005] recv HEADERS frame <length=57, flags=0x05, stream_id=13>\n          ; END_STREAM | END_HEADERS\n          (padlen=0)\n          ; First response header\nAnd with ServeHTTP:\n[  0.020] recv (stream_id=13) :status: 200\n[  0.020] recv (stream_id=13) content-type: application/grpc\n[  0.020] recv (stream_id=13) trailer: Grpc-Status\n[  0.020] recv (stream_id=13) trailer: Grpc-Message\n[  0.020] recv HEADERS frame <length=41, flags=0x04, stream_id=13>\n          ; END_HEADERS\n          (padlen=0)\n          ; First response header\n[  0.020] recv (stream_id=13) grpc-message: unknown service test.Test\n[  0.020] recv (stream_id=13) grpc-status: 12\n[  0.020] recv HEADERS frame <length=43, flags=0x05, stream_id=13>\n          ; END_STREAM | END_HEADERS\n          (padlen=0)\n. I like the separation but what I'm missing is the possibility to consider the available streams per connection in the balancer. There is currently no way to get this information which can result to hidden blocking calls to servers if all available streams are inflight.\nAlso I agree with @zellyn regarding the connection to all hosts, maybe a lazy connection pool would fit better like it is done in the http2 pkg?\nhttps://github.com/golang/net/blob/master/http2/client_conn_pool.go\n. @iamqizhao Yes I can, but it's impossible to know the real configuration of maximum streams per connection, because a server can overwrite the client configs in the SETTINGS frames. So I can only guess it or need to coordinate it with the server configs which is not good IMO.\n. @harlow You can use https://github.com/lovoo/gcloud-opentracing what we are using at Lovoo in combination with https://github.com/grpc-ecosystem/grpc-opentracing/tree/master/go/otgrpc and https://github.com/opentracing/opentracing-go. ",
    "tatsuhiro-t": "I think it would be helpful to see the request headers nghttpx sends to backend gRPC server.  You can see it with -LINFO option for nghttpx.  The log lines are after something like \"[DCONN:0x7f53a7040933] HTTP request headers\".\n. I think this is the fault of gRPC:\nSee https://github.com/nghttp2/nghttp2/issues/588#issuecomment-214941473\n. nghttpx probably avoid empty body if it correctly utilizes END_STREAM in HEADERS.  I still think that gRPC should accept empty body, because it is allowed in HTTP/2 spec.\n. ",
    "tcolgate": "FWIW, I've just been debugging this with traefik. I think the differennce is possibly that the standard serve function does not announce the trailers it is going to send.\nCaddy actually has a fix for this\nhttps://github.com/mholt/caddy/blob/master/caddyhttp/proxy/reverseproxy.go\nIf there's no good reason for gRPC-go to not announce the headers, then it shoud probably anounce them, ottherwise it may be that the proxy is at fault. The PR for traefik makes it easy to see the fix that was needed. https://github.com/containous/traefik/pull/1639. ",
    "danielmsong": "any updates on this @iamqizhao ?\n. ",
    "let4be": "same issue, go get -u google.golang.org/grpc doesn't help\nis there a stable version that supposed just to work?...\n. go 1.6 linux\n. interesting...\n```\ngo get -u golang.org/x/net/http2\ncd /home/dev/projects/go/src/golang.org/x/net; git pull --ff-only\nYou are not currently on a branch. Please specify which\nbranch you want to merge with. See git-pull(1) for details.\ngit pull <remote> <branch>\n\npackage golang.org/x/net/http2: exit status 1\n```\nafter I did \ncd /home/dev/projects/go/src/golang.org/x/net\ngit pull origin master\nit seems to fix the problem\n. ```\ngo get -u google.golang.org/grpc/...\ncd /home/dev/projects/go/src/github.com/golang/protobuf; git pull --ff-only\nYou are not currently on a branch. Please specify which\nbranch you want to merge with. See git-pull(1) for details.\ngit pull <remote> <branch>\n\npackage github.com/golang/protobuf/proto: exit status 1\ncd /home/dev/projects/go/src/golang.org/x/net; git pull --ff-only\nYou are not currently on a branch. Please specify which\nbranch you want to merge with. See git-pull(1) for details.\ngit pull <remote> <branch>\n\npackage golang.org/x/net/context: exit status 1\n```\n. ",
    "treeder": "Does this depend on Go 1.7 now or something? Updating doesn't help because it's still using MetaHeadersFrame: https://github.com/grpc/grpc-go/blob/35896af9ad39c1fb1b1cd925fe3621be361e3d81/transport/http2_client.go#L814\n. Turns out I had to update golang.org/x/net (I think), then it built. \n. ",
    "chamilad": "I came across this error on go1.6.2 linux/amd64 on Ubuntu 16.04. Here's how I got it fixed, if someone comes across this issue after googling. \nI tried go get -u google.golang.org/grpc. It didn't work because golang.org/x/net had uncommitted changes. git checkout -- . inside golang.org/x/net and repeated go get -u google.golang.org/grpc. This fixed it. \n. ",
    "sagikazarmark": "Don't know why, but when using Glide, I had to manually update the lock file to use the latest commit. Even when I removed the lock file, it resolved the wrong commit hash. Subsequent glide ups work though, it does not touch the (correct) commit hash.. The problem appers for me as well, even though probes are properly configured. I use ELB, could that be the problem?. ",
    "ypapax": "cd $GOPATH/src/golang.org/x/net/http2 && git checkout master && git pull\nhelped me.\nThe head was detached there for some reason:\n$ git status\nHEAD detached at f09c466\nnothing to commit, working directory clean. ",
    "timakin": "In my case, library cache remains on glide project directory.\nSo I had to clean cache with glide cc before I entered glide up .\n. ",
    "KeKe-Li": "[keke@localhost etcd]$ go build\ngoogle.golang.org/grpc/transport\n../google.golang.org/grpc/transport/handler_server.go:220: undefined: http2.TrailerPrefix\n../google.golang.org/grpc/transport/http2_client.go:1070: undefined: http2.MetaHeadersFrame\n../google.golang.org/grpc/transport/http2_client.go:1191: undefined: http2.MetaHeadersFrame\n../google.golang.org/grpc/transport/http2_server.go:232: undefined: http2.MetaHeadersFrame\n../google.golang.org/grpc/transport/http2_server.go:404: undefined: http2.MetaHeadersFrame\n../google.golang.org/grpc/transport/http_util.go:200: undefined: http2.MetaHeadersFrame\n../google.golang.org/grpc/transport/http_util.go:465: f.fr.SetReuseFrames undefined (type http2.Framer has no field or method SetReuseFrames)\n../google.golang.org/grpc/transport/http_util.go:466: f.fr.ReadMetaHeaders undefined (type http2.Framer has no field or method ReadMetaHeaders)\n../google.golang.org/grpc/transport/http_util.go:596: f.fr.ErrorDetail undefined (type *http2.Framer has no field or method ErrorDetail)\nways such questions\uff1f. okI http://cn.bing.com/dict/search?q=I&FORM=BDVSP6&mkt=zh-cn'm\nhttp://cn.bing.com/dict/search?q=%27m&FORM=BDVSP6&mkt=zh-cn try\nhttp://cn.bing.com/dict/search?q=try&FORM=BDVSP6&mkt=zh-cn again\nhttp://cn.bing.com/dict/search?q=again&FORM=BDVSP6&mkt=zh-cn ,thank you !\n2017-08-25 0:18 GMT+08:00 dfawley notifications@github.com:\n\n@KeKe-Li https://github.com/keke-li did you go get -u\ngoogle.golang.org/grpc?\nIf so, try to repro with a clean GOPATH. The build is green\nhttps://travis-ci.org/grpc/grpc-go, so there must be something wrong\nwith your environment.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/581#issuecomment-324684289, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AMdRq-MKy9kJIE1Q5zB0ICdbIRpaYuwsks5sbaJAgaJpZM4HmGio\n.\n. \n",
    "makdharma": "Hi Stephen, Please take a look at https://github.com/grpc/grpc/issues/5377,\nand associated pull request in the bug. Does this satisfy your requirements?\nOn Thu, Mar 3, 2016 at 2:12 PM, Stephen Day notifications@github.com\nwrote:\n\n@iamqizhao https://github.com/iamqizhao Would this be a proposal to\nchange this document\nhttps://github.com/grpc/grpc/blob/e1d8d4dc1bdf54273b7ed9d4a495789430cbdf6c/doc/connection-backoff.md?\nWhat is the goal of having common backoff strategies across languages?\nThe current behavior is problematic, so I'd like to avoid a lengthy\nmulti-language review. 120 seconds is just too long.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/585#issuecomment-191989753.\n. Hi Stephen - Would you be willing to submit a PR that just implements the\nfunctionality in https://github.com/grpc/grpc/issues/5377 in grpc-go,\nfollowed by a separate PR that implements a BackoffStrategy? That will\nminimize the divergence as we come close to GA.\n\nOn Fri, Mar 4, 2016 at 10:38 AM, Stephen Day notifications@github.com\nwrote:\n\n@zellyn https://github.com/zellyn We could define a BackoffFunc, which\nimplements BackoffStrategy:\ntype BackoffFunc func(int) time.Durationfunc (fn BackoffFunc) Backoff(retries int) time.Duration { return fn(retries) }\nThat would provide a shortcut for the common case and avoid having to pass\nmethod values when more structure is required.\nThis is probably academic until I get a response to my comment above.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/585#issuecomment-192400928.\n. \n",
    "Iulian7": "Is there any update on this? . ",
    "mofirouz": "Is this performance still an issue given the use of grpc.ServerHTTP?\nI'm weighing on whether we should wrap gRPC's server with net/http server as I need to define / as healthcheck so that it will pass GCLB?. @alexcarol I believe I'm in the same situation as you. Can you tell me how bad (in terms of Gigabyte) the memory leak was, and how quickly the application consumed the available memory once leak started? I understand this is quite situational but may be your experience could shed some light on mine?. ",
    "vieux": "I signed it!\n. ",
    "sadlil": "Lowering the map key resolves the problem. The peer did not close the connection now. \nit would be better if  WithPerRPCCredential returns the map keys  lowercase. \n. The issue is fixed. Thnx.\n. ",
    "jmuk": "@adg \nI'm not sure what's your point.\nTo me, \nserviceAccount, _ := oauth.NewServiceAccountFromFile(filename, scopes...)\nconn, _ := grpc.Dial(service, grpc.WithPerRPCCredentials(serviceAccount))\nit's not so obvious this code doesn't reuse the token.\nAnd this happens not because serviceAccount can't create reusable token source, but because serviceAccount creates a reusable token source and then discard it every time a token is required. And, I think it's not great to put the logic of reusing token (and refreshing it when necessary) here, it already exists in golang.org/x/oauth2 as reuseTokenSource; and this patch bring reuseTokenSource here.\n. ",
    "broady": "I agree, but the README says it \"requires Go 1.4 or above\", which implies Go 1.4 is supported.\n. Happy to change both to say 1.5 (or even add a section on only supporting the last two stable versions of Go).\nPerhaps this is something we (Google) should standardize on for google.golang.org/... libraries?\n. Updated.\n. @iamqizhao \n. Could you add a sentence saying that Go 1.5+ is required and only the last two major versions of Go are supported?\n. This looks like it could be a real failure in gRPC in Go 1.7\n--- FAIL: TestTimeoutOnDeadServer (5.31s)\n    end2end_test.go:420: Running test in tcp-clear environment...\n    end2end_test.go:420: Running test in tcp-tls environment...\n    end2end_test.go:420: Running test in unix-clear environment...\n    end2end_test.go:420: Running test in unix-tls environment...\n    end2end_test.go:420: Running test in handler-tls environment...\n    end2end_test.go:1956: Leaked goroutine: goroutine 1 [chan receive]:\n        testing.(*T).Run(0xc4200920c0, 0x86e7eb, 0x17, 0x89f6d0, 0xc420175d20)\n            /home/travis/.gimme/versions/go1.7beta2.linux.amd64/src/testing/testing.go:647 +0x316\n        testing.RunTests.func1(0xc4200920c0)\n            /home/travis/.gimme/versions/go1.7beta2.linux.amd64/src/testing/testing.go:793 +0x6d\n        testing.tRunner(0xc4200920c0, 0xc420041e30)\n            /home/travis/.gimme/versions/go1.7beta2.linux.amd64/src/testing/testing.go:610 +0x81\n        testing.RunTests(0x89f8a8, 0xa65b40, 0x20, 0x20, 0xc420175f20)\n            /home/travis/.gimme/versions/go1.7beta2.linux.amd64/src/testing/testing.go:799 +0x2f0\n        testing.(*M).Run(0xc420041ef8, 0x4)\n            /home/travis/.gimme/versions/go1.7beta2.linux.amd64/src/testing/testing.go:743 +0x85\n        main.main()\n            google.golang.org/grpc/test/_test/_testmain.go:116 +0xc2\n. @iamqizhao PTAL. Tests pass locally.\n. Updated with your patch, @menghanl and also to disable the composite literal vet check.\nIt fails on protoc generated code.\n$ go tool vet -all .\nreflection/grpc_testing/proto2.pb.go:28: github.com/golang/protobuf/proto.ExtensionRange composite literal uses unkeyed fields\n. It can. We turn the vet check off.\n. Is there no way to do this in a backward compatible way?\nIf you're going to break people, I'd suggest introducing an interface so that people can choose whether to implement cancelation (have a Dialer interface with a Dial method with the current signature, and a separate Canceler interface, check implementation of Canceler to perform cancelation).\nBut if a function signature must be changed, I'd prefer to see using Context for cancelation rather than a channel.\nIn either case, it might be worth reverting this until more discussion can be had.\n. I don't follow the gRPC project, I only see when it breaks people using my code.\nAnyone using the google.golang.org/cloud/... packages on App Engine are now broken by this change.\nThere's definitely a way to do this while maintaining backward compatibility: add a new option function. Something like grpc.WithCancelableDialer (there might be a better name, I haven't thought much about it).\nReversion is common when a change breaks many clients. I believe this does that.\n. After a few more seconds of thought, WithContextDialer might be nice.\n. Yes, I disagree that context.Context is request scoped. I know of several significant projects that use Contexts that span various parts of an application lifecycle.\nHere's App Engine's Dial function:\nfunc Dial(ctx context.Context, protocol, addr string) (*Conn, error)\nhttps://cloud.google.com/appengine/docs/go/sockets/reference#Dial\nUsing this function signature would be nice.\n. LGTM, thank you for the quick action, @tamird!\n. @iamqizhao is that planned for the future? If not, it would be good to see documentation to that effect.\n. FYI, 1.8 on GAE isn't GA yet.. /cc @jba @zombiezen @rakyll. I should have made it more clear: I work on the Cloud team - on App Engine and on Google and GCP client libraries.\nOur customers will need to pin back to an old version. And yes, we all agree they should be doing that already.\nHow about some nominal support for 1.6 - that is, still drop \"support\" for 1.6, but any bugs that arise are fixed with only a best-effort guarantee.. Yes, we reverted x/net/trace back to 1.6 because of App Engine.\n(note, you'll see I'm the reviewer on the rollback CL)\n\"can pretty easily vendor\" - as you know, the dependency management situation for Go isn't quite that straightforward at the moment.. No comment on open census support.\nOfficial GCP libraries don't use it (yet), so there's not a large impact to customers using Go 1.6 on GAE.\nOr, does gRPC depend on opencensus?. Sounds like a reasonable tradeoff to me!. This isn't equivalent.\nDoes that test actually pass?\n. You're right. Sorry.\n. Why only for 1.7?\n. No need for -u.\n. It's only cached in Travis if you have something like this:\nhttps://github.com/GoogleCloudPlatform/golang-samples/blob/8595f791e2b9d7de2088c9a6082328ad2d2c58a0/.travis.yml#L20-L23\n. ",
    "rjarmstrong": "I found I got this error if putting ':' inside metadata.Pairs when creating a new request context:\ne.g. \ngo\nmetadata.NewContext(\n            context.Background(),\n            metadata.Pairs(\n                \"prefs:private_view\",\n                                \"true\",\n            )). ",
    "ThomasHabets": "So the valid keys for GetRequestMetadata are whatever the underlying data transport allows in \"headers or other contexts\"?\nThis is not clear from:\nhttps://godoc.org/google.golang.org/grpc/credentials#Credentials\nAnd it implies a leaky abstraction, where key-values break when underlying transport changes, doesn't it?\n. Why shouldn't those errors be checked?\n. ",
    "aaronlehmann": "This is a pain point for me as well. There doesn't seem to be a way to surface a TLS-related error to the calling code. It only goes to the log. This means an application where the user is not monitoring the logs directly can't signal the error condition.\n. Thanks, the described behavior sounds good to me. I didn't know about FailFast (the version of grpc I'm working against appears to be too old to support it), but this sounds like a decent way to pass the errors through when not using WithBlock.\n. It would be nice to avoid collapsing the error to a string so that the caller can inspect it.\n. ",
    "HNoreick": "Thanks for that hint. I changed my workflow. If anything is in the same package, it works. But an open wrapper would be more comfortable.\nNo, I don't want to write my own implementation of grpc.Server, but i want access to the services from other servers. I have different use cases in mind and the services could be the one door any request has to pass. On the same maschine i would like to have a replacement for linked libraries an in web I want codecs for url encoding and jsonrpc. Even accesss to html pages could be validated with a service or rpc results could be transformed with an engine behind. That are different requirements the grpc.Server cannot deal with. The current design makes it nessesary to register any service, codec and compressor twice. I have to say that I am at a beginner state with just two weeks experience, unsure wether I have the right understanding for all concepts right now. I just can see the enormous potential in this solution.\n. May be I did not find the right words for what i mean. My native language is german. \n\n..Or iterate over the servers that are registered on a grpc.Server?\n\nYes, I mean the services registered on a grpc.Server. I can't find a reason why the service registration is part of the server. I think registration should be an independent class accessed from the server then it would be accessible from other servers too. The current implementation forces redundant work for that. \nI separated the registration from the server with the (default-)option to register the names lowercase, so it fits better to web apis. \nI also register codecs and compressors[TODO] independent from the server.\nWith this separation I can build different execution shells without bindings to a specific server implementation. At the moment it is still experimental and poor documented.\nWhat I originally asked for is unspectacular, I just register the service to a Provider instead of the grpc.Server. \n```\nfunc NewProvider() Provider {\n    return &Provider{\n        servicePool: make(map[string]service),\n        ServiceKeyConverter: strings.ToLower,\n        codecPool: make(map[string]encoding.Codec),\n    }\n}\n// The public methods\nfunc (m Provider) CodecByContentType( content_type string ) (encoding.Codec, error)...\nfunc (m Provider) DecoderByRequest( req http.Request ) (encoding.Codec, error)...\nfunc (m Provider) EncoderByRequest( req http.Request ) (encoding.Codec, error)...\nfunc (m Provider) Method(method_path string ) (*Method, error)..\nfunc (m Provider) RegisterCodecCreator(create_codec encoding.CodecCreator )..\nfunc (m Provider) RegisterService(sd *ServiceDesc, ss interface{})...\n```\n. > Why can't you register a service implementation with multiple servers?\nYes, I could , but i won't. The current grpc.Server leads me to reimlement the registration part and I implemented id reusable, because i think of multiple servers who can profit. I will separate the access and execution of rpcs from the server. On the web site i need the handeling of multiple codecs and internal I don't need (tcp/ip)/http at all.\n. ",
    "Ralphbupt": "Thank you for your code\uff0c it helps me understand how to user metadata. ",
    "rhysh": "Thanks for the quick turnaround on this. The situation is improved somewhat, but this issue is still present. The transport's window shrinks to zero more slowly than before, but it still shrinks and requests still stall. Is there more than one leak?\nWith #633, the reproducer processes can exchange around 2400\u20132600 RPCs on my laptop before stalling. Without it, they can exchange only 140\u2013160.\nWhen I adjust my reproducer to send fewer response messages (one per request) so the requests are not canceled, the client and server can exchange 1,000,000 RPCs with no hiccups.\n. I added logging to google.golang.org/grpc/transport.(*http2Client).handleData, which indicates that the amount of data ready for the application to read (t.fc.pendingData) grows as messages arrive, and does not shrink when streams are closed. It grows to around 3/4 of a MB when communication stalls. This is contrasted with t.fc.pendingUpdate which grows to around 1/4 of a MB and then shrinks, likely as a window update is sent.\nIt looks like the receiver's pendingData, the receiver's pendingUpdate, and the sender's transport window tq sum roughly to a constant number\u20141MB once communication is established.\nThe receiver seems to be collecting a pile of byte-tokens for data it will never read\u2014in its pendingData pool. Is that what's depriving the sender's tq window of the byte-tokens it needs to continue communicating?\n// pendingData is the overall data which have been received but not been\n    // consumed by applications.\n    pendingData uint32\n    // The amount of data the application has consumed but grpc has not sent\n    // window update for them. Used to reduce window update frequency.\n    pendingUpdate uint32\n. It looks like 963ee99c990767c31b3c467ec17045abd47d76c5 fixes the problem for me. Thanks!\n. @iamqizhao Where are breaking API changes like this announced? I didn't see it mentioned on the grpc-io mailing list, is there somewhere else I should subscribe? Thanks.\n. Thanks for including the information in the tag. Is there a high-signal list where grpc-go releases are announced?\n. Done. Done. \"TestFlowControlLogicalRace\", to emphasize that is wouldn't be caught with Go's (data) race detector.. Done. Done. Done. ",
    "spenczar": "This backwards-incompatible change broke builds for teammates and produced confusing errors. I can't find any explanation for the changes, nor can I see anything in the milestones which would have helped see that this was coming up. Is there anywhere I can subscribe to announcements of these kinds of breaking changes?\n. ",
    "riannucci": "Drive-by/Quick question: Why do the handler methods/types take interface{} instead of proto.Message?\n. ",
    "hilljgo": "Any reason this was closed? I am currently running into the same issue\n. Ah yeah, that was definitely the case! Either protoc (cli tool) protoc-gen-go or my grpc plugin was out of date.\n. ",
    "vlad-alexandru-ionescu": "I signed it!\n. Ah right, perhaps a RWMutex would be needed just for that map.\nThanks for having a look anyway.\n. We'll be maintaining a fork with these changes (with race condition fixed). If you ever plan on dynamic services I'd be happy to fix this PR and maybe add a test as well. Otherwise, feel free to close.\n. Not sure why you say that, I'm using a RWMutex.RLock() for the hot path (shared read).\n. ",
    "kazegusuri": "I have implemented my own version of NewMultiUnaryServerInterceptor which is almost same to this PR.\nI think just exposing NewMultiUnaryServerInterceptor is enough for simplicity, but direct handling of multi interceptor is better for users.\nIn either case, grpc-go should provide the functionality. Otherwise, everyone have own implementation of MultiUnaryServerInterceptor.\n. I signed it. Addressed a comment. Please take a look.. @menghanl I have experienced same issue here. Here is a reproduce procedure.\nExample program\nclient\n```\npackage main\nimport (\n    \"context\"\n    \"log\"\n    \"time\"\n\"github.com/golang/glog\"\n\"github.com/kazegusuri/grpc-test/pb\"\n\"google.golang.org/grpc\"\n_ \"google.golang.org/grpc/grpclog/glogger\"\n\n)\nfunc main() {\n    ctx := context.Background()\n    conn1, err := grpc.DialContext(ctx, \"127.0.0.1:9000\", grpc.WithInsecure())\n    if err != nil {\n        log.Fatalf(\"grpc.DialContext failed: %v\", err)\n    }\n    defer func() { _ = conn1.Close() }()\ncli1 := pb.NewEchoClient(conn1)\nfor {\n    time.Sleep(1 * time.Millisecond)\n    _, err := cli1.Echo(ctx, &pb.EchoMessage{Message: \"hello\"})\n    if err != nil {\n        glog.Infof(\"goroutine: echo failed: %v\", err)\n    } else {\n        glog.Infof(\"goroutine: echo succeeded\")\n    }\n}\n\n}\n```\nserver\n```\nfunc main() {\n    server := newServer()\n    s := grpc.NewServer(grpc.KeepaliveParams(keepalive.ServerParameters{\n        MaxConnectionAge: 10 * time.Second, // this kills connection 10s after connected\n    }))\n    pb.RegisterEchoServer(s, server)\nlis, err := net.Listen(\"tcp\", \":9000\")\nif err != nil {\n    log.Fatalf(\"failed to listen: %v\", err)\n}\n\nif err := s.Serve(lis); err != nil {\n    log.Fatalf(\"err %v\\n\", err)\n}\n\nselect {}\n\n}\n```\nResults\nWith above programs, I run the server in v1.19.0 and the client with v1.17, 1.18, 1.19.\nThe server kills the connection from the client 10 second after connected.\nThe client should reconnect to the server but its behavior is different in client versions.\nWith 1.18 and 1.19 the client takes over 1 second to recconect the server.\n```\nv1.17.0 (default)\nI0303 02:22:59.745529    6773 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00001e030, TRANSIENT_FAILURE\nI0303 02:22:59.745566    6773 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00001e030, CONNECTING\nI0303 02:22:59.745732    6773 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00001e030, READY\nv1.18.0 (default)\nI0303 02:20:58.820793    5169 clientconn.go:941] Subchannel Connectivity change to TRANSIENT_FAILURE\nI0303 02:20:58.820892    5169 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00016e010, TRANSIENT_FAILURE\nI0303 02:20:59.821028    5169 clientconn.go:941] Subchannel Connectivity change to CONNECTING\nI0303 02:20:59.821189    5169 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00016e010, CONNECTING\nI0303 02:20:59.822058    5169 clientconn.go:941] Subchannel Connectivity change to READY\nI0303 02:20:59.822155    5169 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00016e010, READY\nv1.18.0 (GRPC_GO_REQUIRE_HANDSHAKE=off)\nI0303 02:25:59.393033    8568 clientconn.go:941] Subchannel Connectivity change to TRANSIENT_FAILURE\nI0303 02:25:59.393096    8568 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00017c010, TRANSIENT_FAILURE\nI0303 02:26:00.393385    8568 clientconn.go:941] Subchannel Connectivity change to CONNECTING\nI0303 02:26:00.393510    8568 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00017c010, CONNECTING\nI0303 02:26:00.393937    8568 clientconn.go:941] Subchannel Connectivity change to READY\nI0303 02:26:00.393979    8568 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc00017c010, READY\nv1.19.0 (default)\nI0303 02:28:57.655932   11292 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc000160010, TRANSIENT_FAILURE\nI0303 02:28:58.656125   11292 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc000160010, CONNECTING\nI0303 02:28:58.656489   11292 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc000160010, READY\nv1.19.0 (GRPC_GO_REQUIRE_HANDSHAKE=off)\nI0303 02:30:30.642447   11438 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc000162010, TRANSIENT_FAILURE\nI0303 02:30:31.642680   11438 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc000162010, CONNECTING\nI0303 02:30:31.643004   11438 pickfirst.go:73] pickfirstBalancer: HandleSubConnStateChange: 0xc000162010, READY\n```. This causes performance regression on the server which provides single subconn like Google Spanner.\nSpanner seems to kill connections in the server-side in an hour, then the connection becomes transient failure. For a second in transient failure until becoming ready, the connection keeps unavaiable.. If it's okay to try to create transport immediately after transient failure for first retry, how about adding code like this here?\nif ac.backoffIdx == 0 {\n    backoffFor = 0\n}. > It will get the first server with id>= specified id.\nYes, that's right. So I checks returned server matches the specified id's server.\n\nI think you need to implement a new function db.get().GetServer(id) on the channelMap struct, which will find the server with the specified id.\n\nOK. it's easy to understand than the current implementation. I will add the function.. fixed!. It works fine for us to reconnect immediately after connection closed.\nBut I think this also resets backoff duration when reconnecting failed repeated in the server down, which resulted in trying to reconnect aggressively. \nSo the backoff duration should be reset only in first retry like this?\nif ac.backoffIdx == 0 {\n    backoffFor = 0\n}. sorry it's my bad. I tried again with this branch and it does retry to connect with backoff correctly.. ",
    "lukaszx0": "\nI think contrib will be out soon. It is outside the library repo -- we will have a new org and multiple repos for contrib.\n\n@iamqizhao was this ever released? I don't see it in https://github.com/grpc-ecosystem. Thanks, I'll have a look - this will probably work.\nRegardless of workaround though, shouldn't this be exposed in ClientConn as part of cross-language spec: https://github.com/grpc/grpc/blob/master/doc/connectivity-semantics-and-api.md#channel-state-api ?. ",
    "pkieltyka": "I secondly, love the idea of having middleware support in grpc. btw, aren't x/net/context and context compatible in latest versions of x/net/context ? as x/net/context imports context for 1.7+ with build tags\n. ",
    "meomap": "|I signed it!|\nOn 04/24/2016 04:02 PM, googlebot wrote:\n\n|I signed it!|\n. \n",
    "leonardyp": "the address google.golang.org/grpc is wrong that can not be go get\nand Direct access to the browser could not access\n. ",
    "chai2010": "@leonardyp Please DONOT use go get in China!\nUse git clone https://github.com/grpc/grpc-go.git $GOPATH/src/google.golang.org/grpc.\n. ",
    "shaxbee": "Still there are errors when testing grpc package due to test files not being regenerated.\n. Pulled, works at HEAD. Sorry for the ruckus.\n. @iamqizhao I think you misunderstood intention of this PR - the idea here is to modify context metadata in interceptor and make those changes visible to Handler by updating ServerStream context.\n. Sorry, I haven't seen your 2nd reply as we posted at roughly same time.\nContext injection for Stream methods is much harder right now compared to Unary methods.\nWould adding func ServerStreamWithContext(ss ServerStream, ctx Context) ServerStream be a better solution then?\n. I completely understand that adding new public API is not a lighthearted decision. It his highly likely though that users of library will be looking for a way to inject metadata into context which is easy in unary case, but quite tricky for stream, having library support for such operation will make it much less 'hacky' too. Can we at least leave this PR open for discussion in case someone else tries to solve same problem?\n. Created Gist for reference: sscontext.go\n. ",
    "jaytaylor": "Dead Links\nThe wire protocol link is dead, here are archived snapshots:\nhttp://web.archive.org/web/20170429155158/https://grpc.io/docs/guides/wire.html\nhttp://archive.is/JAngz\nand the updated version of the document:\nhttps://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md. Thanks for the info @MakMukhi, those are good questions.\nInvestigating revealed the Linux is too old to easily get mod_http2 installed.  However, as luck would have it I'll be dumping Linode and moving to a newer Linux machine (and distro) in the next day or two.\nSo assuming an apache server with mod_http2 is in the mix, is there a way to let apache do the SSL termination and still let gRPC clients in?\nI understand it may not be feasible, as the server's TCP port is being multiplexed between an HTTP goroutine and the gRPC goroutine based on a header, all behind an apache reverse-proxy.\nIt seems to me like a matter of getting the gRPC client to connect to example.com:443 using SSL, without using credentials (i.e. establishing HTTPS tunnel without authentication).\nDoes my understanding of what is needed seem correct?\nThanks for bearing with me.. @dfawley Right on, this is along the lines of what I've been aiming for (in the meantime, things are connected through unreliable SSH tunnels as a temporary workaround for the HTTPS termination issue).\nThe part I could really use some advice and / or guidance on is the TLS/Apache part:\nWhat is the technique to configure (or otherwise convince) the gRPC client to connect to the SSL Apache proxy server?\nIs it necessary to somehow bind the LetsEncrypt TLS certificate to the gRPC client?  This would be somewhat of an annoyance since the cert must be renewed on a quarterly basis.  That said, any solution which improves on the current strategy would be wonderful :). Sort of.  Part of the problem was a missing cmux matcher for HTTP2 on the web server side of things.\nFull HTTP2 direct mode must be available on the Go side, because apache2 does not support HTTP1.1 -> HTTP2 connection upgrades.\nI also incorporated support for h2c in the commit above, which was necessary for things to play nice with apache.  After applying the changes above, streaming gRPC requests started working!  However, now non-streaming gRPC requests still don't work.  I get 1 of the following 2 errors in the client:\nFATA[0001] rpc error: code = Internal desc = server closed the stream without sending trailers  source=\"andromeda/main.go:475\"\nFATA[0000] rpc error: code = Internal desc = transport: received the unexpected content-type \"text/plain; charset=utf-8\"  source=\"andromeda/main.go:475\"\nOther cases of gRPC trailers errors exist, and AFAICT the error is the result of a proxy making alternative decisions about HTTP2 structures that ultimately result in gRPC wire-protocol issues.\nHere is another similar gRPC error message when proxying by nginx.\nOverall I'm not feeling too good about this situation.  gRPC requires HTTP2, and the most widely-used load balancers are not able to completely work with gRPC.\nI've considered trying out Envoy, but held off since it's geared mostly towards kubernetes / web-apps rather than a general-purpose high-performance LB.\nIs there a list of reverse-proxy load balancers which are known to be compatible with gRPC?\n(ideally with samples of working configurations :). Here's an example of the gRPC client setup.\nThis OpenSSL command to get the public ssl/tls cert.pem proved to be quite useful, too.\nopenssl s_client -showcerts -servername $hostname -connect $hostname_or_ip:$port </dev/null 2>/dev/null | openssl x509 -outform PEM. Thanks for the info @dfawley.\nI currently have it sort of working with Apache2, but now thinking of putting nginx out in front, hooking up the gRPC app there, and bypassing apache.\nApache does so much, which can often be a really good and nice thing.  In this case, however, it's leading to difficulties such as sporadic client / server errors, occasional very high apache memory usage (climbing to consume 100% of what is available on the machine), as well as occasional difficulty for gRPC clients to attach to streams or detect that a stream has gone away.  Since none of these is consistently reproducible, diagnosis is quite a challenge.\nNginx is looking pretty appealing :) My experience so far is that proxying HTTP/2 is almost nothing like proxying HTTP/1.  Just my .02c~\nJay. ",
    "saicheems": "I signed it!\n. ",
    "kyleconroy": "I signed it!\n. @dfawley How is this stale? I've been waiting for a year to get a review. ",
    "tomq42": "Hmm.\nI can't reproduce this now, but I definitely did what my original description says in terms of how I got the packages, and it left with an incompatible pair. This was the first day I had tried to use go grpc, so there can't have been anything old lying around.\nHowever since I can't reproduce, I'll leave it.\n. ",
    "leonid-shevtsov": "@tomq42 you might have had a vendored outdated proto package that was installed as some package's dependency.\n. ",
    "zinuga": "Planning to add a single grpc-contrib repo across languages. have something to add?\n. @nicolasnoble can we do these changes now? Want to start with one repo to test?. ",
    "thinkerou": "I think that I should use Go vendor, not use godep or other in order to reduce dependeny.\n. I need to continue reading.\n. @menghanl done. OK, I prefer testdata.Path(\"testdata/ca.pem\") to testdata.Directory, but I not sure that we should use testdata.Path(\"ca.pem\") or testdata.Path(\"testdata/ca.pem\").\nSo, the PR have changed the follow points:\n- Deleted two functions abs and goPackagePath in benchmark/worker/util.go file.\n- Add new file testdata.go.\n- Deleted some ca.pem,server1.pem and server1.key.\n- Used new function testdata.Path().. I use the same as https://github.com/grpc/grpc-go/blob/master/go16.go#L60.. @dfawley Got it, thanks. Now I only adjust import order.. when I run go get -u google.golang.org/grpc and have opened export GO111MODULE=on, output the below info:\nshell\n\u279c  tttttt go get -u google.golang.org/grpc\ngo: finding google.golang.org/grpc v1.15.0\ngo: finding github.com/golang/lint v0.0.0-20180702182130-06c8688daad7\ngo: finding github.com/kisielk/gotool v1.0.0\ngo: finding github.com/golang/mock v1.1.1\ngo: finding github.com/client9/misspell v0.3.4\ngo: finding google.golang.org/genproto v0.0.0-20180817151627-c66870c02cf8\ngo: finding golang.org/x/net v0.0.0-20180826012351-8a410e7b638d\ngo: finding golang.org/x/oauth2 v0.0.0-20180821212333-d2e6202438be\ngo: finding golang.org/x/sync v0.0.0-20180314180146-1d60e4601c6f\ngo: finding golang.org/x/text v0.3.0\ngo: finding cloud.google.com/go v0.26.0\ngo: finding golang.org/x/sync latest\ngo: finding github.com/golang/glog v0.0.0-20160126235308-23def4e6c14b\ngo: finding golang.org/x/lint v0.0.0-20180702182130-06c8688daad7\ngo: finding github.com/golang/lint latest\ngo: finding google.golang.org/appengine v1.1.0\ngo: finding honnef.co/go/tools v0.0.0-20180728063816-88497007e858\ngo: finding golang.org/x/tools v0.0.0-20180828015842-6cd1fcedba52\ngo: finding github.com/golang/glog latest\ngo: finding github.com/golang/protobuf v1.2.0\ngo: finding golang.org/x/oauth2 latest\ngo: finding google.golang.org/appengine v1.2.0\ngo: finding golang.org/x/sys v0.0.0-20180830151530-49385e6e1522\ngo: finding golang.org/x/net v0.0.0-20180724234803-3673e40ba225\ngo: finding golang.org/x/lint latest\ngo: finding google.golang.org/genproto latest\ngo: finding honnef.co/go/tools latest\ngo: finding cloud.google.com/go v0.30.0\ngo: finding golang.org/x/net latest\ngo: finding golang.org/x/sys latest\ngo: finding golang.org/x/tools latest\ngo: downloading google.golang.org/grpc v1.15.0\ngo: downloading golang.org/x/net v0.0.0-20181017193950-04a2e542c03f\ngo: downloading google.golang.org/genproto v0.0.0-20181016170114-94acd270e44e\nright? thanks!. Oh, I got it, thanks.\n. the name of file is go17, but the tag of build is go1.6, the name of file should be go16.go?. context have been official lib on go1.7. done, thx!. OK, thank you!. ",
    "dominikh": "Libraries should not vendor their dependencies; vendoring is the responsibility of the application. You should vendor a suitable copy of grpc-go, as well as its dependencies.\nWhen libraries start vendoring what happens is that a program ends up with multiple versions of the same library, or vendored types leaking into the public APIs.\n. @dfawley Did a default staticcheck invocation actually flag these? It shouldn't have unless ST1014 was explicitly enabled; it's a specialised check for security relevant code; shadowing built-ins isn't generally a problem.. ",
    "bluecmd": "The problem is not the grpc-go library part, it's the protobuf compiler interface that should be vendored, and other utils that may or may not be included.\nI agree that gRPC as a library should ultimately be vendored in the final application that's being built.\nTake for example bootstrapping a build environment for building some gRPC project. In this scenario we're trying to build a Go project that is otherwise vendored, but has chosen to not supply generated protobuf files. This will require building protobuf's protoc but also grpc's go plugin. Right now there is no easy way of getting reproducible builds, as \"go get\" might fetch whatever - hence the need for vendoring. Makes sense?\n. > This is considered bad practice as you lose reproducible builds by not including the generated .go files in your library package.\nif your statement was the official statement from the gRPC team that \"Indeed, we expected everyone to ship generated pb files\" then I would be happy with that. This however I cannot find, and I actually remember the thread back in Februrary where this was discussed (https://groups.google.com/forum/#!activity/grpc-io/dLRtQJzOCQAJ) with no clear guidelines.\nAs long as gRPC+Protoc is vendored and you track which gRPC+Protoc version you're using to build it's reproducible. I don't see how this is different from YACC/LEX software that needs to generate their .c counterpart - or if you want to take it even further, any build environment bootstrapping golang or gcc.\nFWIW, in my experience committing any form of generated file to source control is bad practice - but maybe the Go community has some other general consensus on these matters that I'm not aware of. Besides, vendored deps kind of already violate my belief in that regard - I'm a bit torn.\nEDIT: It might be that I misunderstood how the code generation works. I started digging into this and it looks like maybe everything needed is in golang/protobuf: https://github.com/golang/protobuf/blob/master/protoc-gen-go/grpc/grpc.go. I'm travling so I'll have some latency in verifying this, but if that's the case then this changes things.\n. Why close the issue then? Can't it stay open in recognition of the needed work?. According to the (mistakenly) remaining Gopkg.toml the version was 1.2.0 which seems to be the latest one.. I reproduced the issue with the hints so far presented here.\nI can reproduce it by copying go get github.com/golang/protobuf and copy that result into vendor/github.com/golang/protobuf.\n```\n[sfu-diz-gw]$ go build\n[sfu-diz-gw]$ ./grpc &\n[sfu-diz-gw]$ grpc_cli call localhost:8080 bmc.ManagementService.PressButton 'button: BUTTON_POWER'\nconnecting to localhost:8080\n2018/09/25 09:48:05 Request: {BUTTON_POWER 0 {} [] 0}\nRpc succeeded with OK status\n[sfu-diz-gw]$ mkdir -p ../../vendor/github.com/golang/\n[sfu-diz-gw]$ cp -R ~/go/src/github.com/golang/protobuf ../../vendor/github.com/golang\n[sfu-diz-gw]$ fg\n[1]  + running    ./grpc\n^C\n[sfu-diz-gw]$ go build\n[sfu-diz-gw]$ ./grpc &\n[sfu-diz-gw]$ grpc_cli call localhost:8080 bmc.ManagementService.PressButton 'button: BUTTON_POWER'\nMethod name not found\nFailed to find method bmc.ManagementService.PressButton in proto files.\nMethod name not found\nMethod name not found\nFailed to parse request.\n```. Thanks for the investigation. I guess this was triggered by https://github.com/u-root/u-bmc/commit/6befb429b7c3d432b930b87af722598e5631526f in that case. Is there a way to detect this and not fall silently?\nI'll certainly add a test for it but if everybody in the whole ecosystem needs to do that it's a bit sub-optimal.. ",
    "fd": "\nIn this scenario we're trying to build a Go project that is otherwise vendored, but has chosen to not supply generated protobuf files.\n\nThis is considered bad practice as you lose reproducible builds by not including the generated .go files in your library package. A package that has generated code should always have the generated code committed along with the other code.\nyou should never have to do a  go generate ./vendor/...\n. If I'm not mistaken this is actually generated by golang/protobuf.\n. The simples way to achieve this is to do nat-traversal from the machines with private IPs (using upnp).\nThis will map a public ip/port pair to a service running on each machine. Note that the port is randomly selected and thus not predictable. Next each port-mapped service would signal it's public ip:port pair to a known signalling service (this needs service needs to be accessible to all participants). Then if you want to o connect to a port-mapped service you first ask the signalling service for the ip-port and the dial that address.\n. ",
    "sjpotter": "I want to do something different.  Im modifying cadvisor to support rkt.  Its support will be disabled if rkt's grpc service isnt running (i.e. a docker only system)  This isnt about me controlling the connection manually but making other decisions if the service isnt available.\n. ",
    "mberhault": "this definitely occurs in the face of nodes being taken down, so failures or timeouts are very likely.\nUnfortunately, it comes amidst a lot of traffic, so it's rather tricky to find exactly which happens. We should probably orchestrate some rpc logging, it may help diagnose this.\n. @iamqizhao: any updates?\n. ",
    "bdarnell": "No timeouts show up in the logs; the only failures are \"connection refused\" (there's a chance that timeouts or failures are happening but not showing up in the logs). There are some cancellations in the logs but the first one doesn't show up until after we've already seen several of these errors.\n. Updated. The test is much simpler now; for some reason I thought Accept() would give a cleaner error than Close() in this case.\n. +1. This is affecting CockroachDB. We occasionally send very large messages, which are getting held in GRPC for a long time if we have tracing enabled (these are the only requests to make it in to the larger time buckets, so they aren't getting flushed out by the more frequent smaller requests). We're going to turn tracing off for now, but I'd like to have finer-grained control of tracing. We've found the connection-level tracing very useful, but the message/stream-level tracing is less useful to us and much more expensive. I'd like to be able to turn off the logging of message bodies. (especially because they're often hidden behind [redacted] - it took me a while to realize that there was this hidden cost). \n. Any update on this?. A related question: how can an application distinguish a permanent error from an ongoing but temporary failure? \n. @xiang90 I don't think your wrapper is enough for this problem: Once a connection has gone into this \"permanently failed\" state, all future requests on that ClientConn will fail and you have no choice but to close it and try to reconnect. (and if you want to retry on closed connections, don't you want non-fail-fast mode?) \nWe're not currently using Balancers; maybe that changes things?\n. It's tricky to test this because the timeout is hard-coded, so we'd need to introduce some way to adjust it down to a reasonable level for testing. We'd like to go ahead and get this merged because it's having a pretty severe impact on CockroachDB. Our servers sometimes have long delays on startup, so we're running in to this often.\n. ECONNRESET counts as Temporary() (syscall/syscall_unix.go), so #864 should've fixed this, unless there's a different location where errors need to be checked (or if there's something that is throwing away the true type of the error).\nAnd my question in #846 still stands: As long as GRPC distinguishes between transient and temporary failures, client applications will need to be able to tell the difference too. What is the recommended way to do this? \n. > note that the supposed syscall was write and the supported error was ECONNREFUSED, which is not an error that write should ever return, as far as I can tell.\nWriting to a socket can return ECONNREFUSED (on linux, but not bsd) if the connect was in non-blocking mode and you haven't consumed the error with getsockopt (or maybe this is a quirk of the python socket wrappers? I haven't tried to reproduce this in go)\n```\n\n\n\nimport socket\ns=socket.socket()\ns.setblocking(False)\ns.connect(('localhost', 54321))\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python2.7/socket.py\", line 224, in meth\n    return getattr(self._sock,name)(*args)\nsocket.error: [Errno 115] Operation now in progress\ns.send(b'asdf')\nTraceback (most recent call last):\n  File \"\", line 1, in \nsocket.error: [Errno 111] Connection refused\n```\n\n\n\nWe've seen issues caused by write() returning unexpected errors before, in golang/go#14548. . This is happening right now on one of our test clusters. The IPs have not changed; one server crashed, was down for a time, and was restarted. The logs are full of this:\nW170127 02:50:04.374599 96914562 storage/intent_resolver.go:352  [n1,s1,r64/7:/Table/51/1/14{12136\u2026-29906\u2026}]: failed to resolve intents: failed to send RPC: sending to all\n 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refused\n\"\nW170127 02:50:04.798731 96914733 storage/intent_resolver.go:352  [n1,s1,r422/1:/Table/51/1/40{39115\u2026-57292\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refuse\nd\"\nW170127 02:50:04.848520 96914767 storage/intent_resolver.go:352  [n1,s1,r423/3:/Table/51/1/86{26570\u2026-44684\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refuse\nd\"\nW170127 02:50:04.908616 96914878 storage/intent_resolver.go:352  [n1,s1,r428/3:/Table/51/1/83{72888\u2026-90988\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refuse\nd\"\nW170127 02:50:04.996322 96915001 storage/intent_resolver.go:352  [n1,s1,r428/3:/Table/51/1/83{72888\u2026-90988\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 4 desc = context deadline exceeded\nW170127 02:50:05.004075 96914928 storage/intent_resolver.go:352  [n1,s1,r423/3:/Table/51/1/86{26570\u2026-44684\u2026}]: failed to resolve intents: failed to send RPC: sending to al\nl 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refuse\nd\"\nW170127 02:50:05.042729 96915124 storage/intent_resolver.go:352  [n1,s1,r45/5:/Table/51/1/13{41272\u2026-58971\u2026}]: failed to resolve intents: failed to send RPC: sending to all\n 3 replicas failed; last error: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.4:33844->192.168.1.10:26257: write: connection refused\n\"\nNote that the error is write: connection refused, so we're getting ECONNREFUSED from a Write call (which isn't really supposed to happen, but I can't tell whether it's explicitly forbidden). And the local port number (33844) is constant, so it's not trying to reconnect over and over. Instead, what appears to be happening is that because the \"connection refused\" error is happening in an unexpected context, grpc continues to retry on the failed connection instead of closing it so it can reopen a new one. . Yes, it still happens. Here's a recent log snippet:\ncockroach@cockroach-blue-0007.crdb.io: I170219 02:03:25.681933 693 vendor/google.golang.org/grpc/clientconn.go:866  grpc: addrConn.transportMonitor exits due to: connection error: desc = \"transport: write tcp 192.168.1.10:39162->192.168.1.9:26257: write: connection refused\"\ncockroach@cockroach-blue-0007.crdb.io: E170219 02:07:48.257687 12830805 storage/replica_command.go:1851  [replica consistency checker,n7,s7,r130/59:/Table/51/1/756{11808\u2026-23002\u2026},@c4216af500] could not CollectChecksum from replica {6 6 61}: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.10:39162->192.168.1.9:26257: write: connection refused\"\ncockroach@cockroach-blue-0007.crdb.io: E170219 02:11:12.643325 13043929 storage/replica_command.go:1851  [replica consistency checker,n7,s7,r2104/25:/Table/51/1/775{32005\u2026-43245\u2026},@c4212ed180] could not CollectChecksum from replica {6 6 26}: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.10:39162->192.168.1.9:26257: write: connection refused\"\ncockroach@cockroach-blue-0007.crdb.io: E170219 02:20:16.612271 13671648 storage/replica_command.go:1851  [replica consistency checker,n7,s7,r558/46:/Table/51/1/474{18512\u2026-29719\u2026},@c421240000] could not CollectChecksum from replica {6 6 38}: rpc error: code = 13 desc = connection error: desc = \"transport: write tcp 192.168.1.10:39162->192.168.1.9:26257: write: connection refused\"\nWe haven't managed to reduce this to a simple reproduction, but this is on a test cluster where we periodically restart random nodes from cron (when this happens, the process is killed with -9, left down for 5 minutes, then restarted). . Yes, we use streaming RPCs extensively (and we need h2c support as well). I think there are two sides to this problem, one in Go's net package and one in GRPC. \nOn the Go side, I hypothesized in golang/go#14548 that the net poller was subject to spurious wakeups, which could result in Dial returning with nil error before the connection was completed. On Darwin, premature writes to a socket that is not yet connected would fail, so a Darwin-specific fix was put in place to ensure that the connection was complete before Dial returns. On Linux, the same spurious wakeup scenario is not a problem for connections that succeed, but it does mean that connection-related errors like ECONNREFUSED can be observed from system calls like write, where they are not otherwise possible. I'll file a Go issue for further investigation about how this should be resolved, but for now let's look at how GRPC handles this error.\nIn GRPC, this shifts the handling of this error from dial, where every error is a temporary connectionError (unless FailOnNonTempDialError is set, which is not the case for us), to creds.ClientHandshake, which may return a permanent error according to the result of isTemporary. (ECONNREFUSED is not temporary). Elsewhere in grpc/transport, errors from Write always turn into temporary connectionErrors, regardless of what isTemporary says. \nI propose that if creds.ClientHandshake returns a net.Error and FailOnNonTempDialError is false, the connectionError should be considered temporary (if there are other more precise ways to distinguish the kinds of certificate errors that indicate a fatal misconfiguration, that would be nice, but it's not clear to me where exactly to draw the line). We could even go so far as to say that the TLS handshake is logically a part of the dial process and only return permanent errors for certificate problems if FailOnNonTempDialError is true, although that may be taking things too far. \nFinally in Cockroach, we should probably be detecting connections in the \"shutdown\" state and A) make loud noises about them and B) retry them with an appropriate backoff. . I don't understand how #1463 is the root cause of this issue. For one thing it looks like the code is already working as you say it should, and only returning fail-fast errors for the transient-failure state:\nhttps://github.com/grpc/grpc-go/blob/fa59b779e8729b2ae0d0cfca12a91220f690fa6f/clientconn.go#L1186-L1190\nBut even if fail-fast were changed to return errors less often, the errors still wouldn't let us know whether the request was possibly executed or not. \n\nIf you are maintaining multiple connections to multiple backends, you should do that within a single ClientConn, and gRPC should always pick one that is up for your RPCs.\n\nI don't think that's an option for us - we need to choose which backend(s) to use on an RPC-by-RPC basis. Is there some way I'm not seeing to configure gRPC to do this?. > Are you using a load balancer?\nNo. In part due to the documented experimental/unstable nature of the interface (as mentioned below). \n\nIs your connection actually in the transient failure state, possibly?\n\nYes. We want to distinguish between \"connection was transient-failed before this rpc was sent, so we didn't try\" and \"connection failed while this rpc was in flight\". \n\nNote that when retry support is added (ETA ~1 month)\n\nInteresting. That in conjunction with a custom balancer might do the trick, if the interfaces let us do what we want. But in general our usage is atypical enough that we're more comfortable taking manual control here; my first choice is to just get a distinct error code. . OK, so the spec clearly says we were wrong to assume that Unavailable means \"didn't send anything\" (I think it happened to be true of the go implementation at the time we made this assumption). And there's no standard error code we could use to be unambiguous (indeed, the spec explicitly notes the ambiguity of DEADLINE_EXCEEDED). \nI'm not sure if an error code is the right way to handle this, though. If I make an RPC to a server that makes another RPC, does the error code from the inner RPC get propagated back to my client or does it get turned into Unknown? It would be incorrect to allow the proposed \"unambiguous did-not-send\" error code to propagate, since it's only true of the client that generated it. So maybe this flag should be a property of the error object that is separate from the network-transmittable error codes. . Is there any update here? We just upgraded from 1.6 to 1.7.1 and it apparently changed some of the implementation details we were relying on to distinguish ambiguous from unambiguous failures (cockroachdb/cockroach#19708). We're probably going to have to revert to 1.6 for now.\nI see that the transparent retry change just landed, but remember that this is not enough for us - we also need control over routing. Have the balancer APIs mentioned in https://github.com/grpc/grpc-go/issues/1443#issuecomment-325050548 been stabilized yet? (note that #1388 is still open). Thanks! Can I request a 1.9.2 release with this fix?. ",
    "stxmendez": "@iamqizhao I'd be interested in understanding what your fix will be.  If you could share your thoughts on the matter I would appreciate it.  I would caution that I'm not sure if my hacky proposal for a fix is the best course of action as the wait method is used from a few call sites.\n. @ianrose14 it addressed the issue that I had at the time.  We have not picked up an update in a couple of months.  I'm in the process of updating to bigtable v2 and latest grpc and can test this again.\n. Great timing.  I was just about to respond back to @ianrose14 that I was able to reproduce the problem using the old snippet against the latest and greatest grpc and BT client v2 libs.\n@iamqizhao - what is the public issue that can be used to track this problem?  Should we reopen this issue or do you have one that we can use?\n. @iamqizhao I was just looking at the behavior of this instance of the issue and the timeout is around 4 minutes as the last time.  I'm looking at the http2_server code and it also uses the wait function.  I wonder if under certain error conditions quota fails to be released and then we only recover via the 4 minute timer?  In any case, http2Server.Write around 575 looks curious on first blush.\n. @iamqizhao thanks for confirming that the bigtable server does not use grpc-go.  Is there a public issue that we can follow and get update on the internal investigations?\n. I signed it!\nSent from my iPhone\n\nOn May 11, 2016, at 4:54 PM, googlebot notifications@github.com wrote:\nThanks for your pull request. It looks like this may be your first contribution to a Google open source project. Before we can look at your pull request, you'll need to sign a Contributor License Agreement (CLA).\nPlease visit https://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll verify. Thanks.\nIf you've already signed a CLA, it's possible we don't have your GitHub username or you're using a different email address. Check your existing CLA data and verify that your email is set on your git commits.\nIf you signed the CLA as a corporation, please let us know the company's name.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. Happy to have you close out the testing :)\n. Why does the type of error returned from the wait method matter here at all?  The wait method can also return ErrConnClosing if the shutdownChan is readable but it is not of type StreamError.\n\nI believe that per the original description of the problem, involving the pseudo random behavior of the Go select statement, it could have been readable on the invocation on line 282 and you get unlucky and quota gets picked instead.  You could argue that in a shutdown event you will tear everything down so the inaccuracy of the quota is irrelevant, but it is probably best to have the least number of special cases.  To that end, if you are here the type of error shouldn't matter.\nPerhaps I missed something?\n. Looking over the test code, how do you know that you hit transport/http2_client.go line 294?\n. So long as error instances returned from wait which are not StreamErrors result in a connection teardown you are correct.  Just wanted to push on that point.\n. The non-deterministic nature of the bug is key.  If you can't verify that you address the issue in the unlucky case then you can regress at a later point in time and you would not know it.\nAgain, not trying to block just commenting on the contents of the review.\n. ",
    "ianrose14": "@stxmendez Does your repro no longer fail after this merge?  I'm finding that if I use the current version of the bigtable & grpc packages, I can make your repro fail every so often (ballpark 1 out of 5 times, with para=3000).\n. In case this info is useful, I'm finding it much easier to repro this on my local machine (a mac) rather than a GCE (linux) instance.  My local repro works at least 1/5 times locally, but often takes ~100 attempts to work on GCE.  Not sure if the difference is the OS or the latency differences, but I'd guess the latter.\n. ",
    "dragonsinth": "When I ran the test harness myself and sent SIGQUITs while it appeared stuck, I always found a goroutine blocked here:\nEvery time I SIGQUIT a hung bt_fail, a goroutine is stuck here:\ngolang.org/x/net/http2.readFrameHeader(0xc8201a20f8, 0x9, 0x9, 0x99f1e8, 0xc820156a80, 0x2000000, 0xc800000000, 0x0, 0x0)\n    /Users/scottb/src/grpc-experiments/src/golang.org/x/net/http2/frame.go:237 +0xa5\nCalling io.ReadFull(r, buf[:frameHeaderLen])\nSo it seemed to be stuck waiting on data from the server as per @iamqizhao notes.\n. Perhaps this should be title, \"It would be helpful to have separate error types for server and client errors, to enable easier error processing\" . ",
    "vjpai": "Just like in other languages, I presume that the go workers stay alive for multiple tests, correct? If so, do you set the max_cpus back to the full system at the end of each test or otherwise set it back to all if not specified?\n. LGTM.\n. LGTM.\n. I suggest reopening and doing what you proposed. These are microbenchmarks, not application-level benchmarks. The goal of ping-pong latency testing is to know the ultimate limit of what's achievable with a given API, not the practical reality. The latter is where other tests come into play.. I'm going to suggest reopening this issue again. Not saying that it should be done, but I think we're missing the point of microbenchmarking otherwise. @xiang90 - disabling GC is not a realistic operating point for Go, but tuning the GC certainly is. Information that we discover from doing so should be used both to drive our own memory usage and to feed information up to the Go gc team. With regard to GOMAXPROCS, \"This call will go away when the scheduler improves\", and we don't know when that will be. Experimenting with this value gives us insights both into our own system and as potential feedback to the Go scheduling team. Likewise @iamqizhao - @apolcyn is noticing this effect today. Sure, let the Go default be the default, but report the number from the more effectively tuned version so that we're focusing on what gRPC can do, not the secondary effects created by the scheduler and other system vagaries that we actually have some control over. It certainly does not mean that it will continue in the future, but it is something that can be tested consistently. In the C++ performance benchmarking world, we have introduced daily sweep tests to experiment with various parameters; this could just as well fit into that.. So one issue with using the limit rather than the list is that the client and server could end up using the same cores if it's running on the same machine. But that should be less of an issue now that all of our performance experiment tests are on different machines. I would suggest considering adding a TODO about using core list if there's any scope for doing so in go.\n. ",
    "dhowden": "\n\nAFAIK you can use the connections concurrently between clients. The only problem may be the limitation of number of concurrent HTTP2 streams you can do, but these count in thousands.\n\n\nI am interested to know the official line on this.  I made a similar assumption as it appears to work for small messages,  however I have found that larger messages are corrupted when multiple goroutines (each with their own client) use the same underlying *grpc.ClientConn.\n. I think that the only way to properly get around this would be add a way for an encoder to expose the severity of the error, which could then separate unrecoverable errors (i.e. where the server should be killed), from less serious errors (i.e. when the current response should be aborted).\nFor instance, adding something simple to the API:\ngo\n// CodecError is an interface which defines behaviour to allow codec errors\n// to expose their severity.  By default any errors returned from Codec.Marshal\n// result in a Fatal error which kills the server process.  When an error returned from\n// Code.Marshal implements CodeError and Recoverable() returns true, then the server\n// will only fail the current request.\ntype CodecError interface {\n    // Recoverable returns true if the Codec can continue to be used\n    // despite the error. \n    Recoverable() bool\n}\nCurrently the server is logging the error to grpclog.Fatalf, in the case where Recoverable() == true, could use grpclog.Printf instead.\n. Looks like this was fixed in https://github.com/grpc/grpc-go/pull/1276, so closing for now.. ",
    "seungryulchoisc": "\nClientConns can safely be accessed concurrently, and RPCs will be sent in parallel.\n\nWe are planning to use gRpc without TLS. But without TLS, most of HTTP2 does not support concurrent access. Does grpc-go implementation support h2c? (I found @bradfitz mentioned a related issue at https://github.com/golang/go/issues/14141 but I am still not clear about it.). ",
    "myles-mcdonnell": "@dfawley \n\nClientConns can safely be accessed concurrently, and RPCs will be sent in parallel.\n\nSo I should create a single connection when a client process bootstraps and close it when the process terminates? Could anything environmental 'break' a connection such that I would need to detect and re-connect?. ",
    "dotdoom": "I signed it! \ud83d\ude04 \n. For instance, I have a subcomponent that may not be available at times.  I want the client to retry with backoff, and thus serve UNAVAILABLE status.  I looked through the code, and convertCode seems to be the only connecting part between the handler and RPC status code.  For obvious reasons I wouldn't like to rely on .Error() string.\nIs there a working solution I could use?\n. Err, I must have mentioned, it is server side that I'm trying to implement in Go.  There are no issues in receiving the code on the client (which is in Java/Android).\n. Shame on me!  I totally missed that method.  Thank you \u2013 and sorry for unnecessary bothering.\nClosing this PR, as there's no practical use in it.\n. ",
    "brian-brazil": "In addition this would need to know about any newly registered services/methods.\n. ",
    "derekchiang": "@iamqizhao without State(), how do you tell if a ClientConn is closed?  We used to reuse connections and only re-dial if one has been closed.  Do we basically have to resort to making a new connection every time now?\n. @iamqizhao thanks for the prompt reply.  I didn't word my question clearly.  When I said \"closed\", I really meant that the connection is entirely unresponsive (potentially due to the other side hanging up, for instance).  How do I tell if my connection is in this non-functional state without using the State API?\n. ",
    "rod6": "Just implemented resolver & watcher for etcd/consul service discovery backend, to support grpc client-side LB, hope this is helpful: wothing/wonaming\n. ",
    "sunwangme": "detect state by unary rpc with timeout!\n:-(\n. the latest code\uff0cProtoPackageIsVersion3 in    https://github.com/grpc/grpc-go/blob/master/    rpc_util.go\uff0cbut ProtoPackageIsVersion2 in protobuf-codegen\u3002\nfail to compile grpc-go\u3002\n. still fail to compile, i update grpc-go proto protoc-gen-go to lastest!\nhttps://github.com/golang/protobuf/blob/master/protoc-gen-go/generator/generator.go:65\nconst generatedCodeVersion = 2\nhttps://github.com/golang/protobuf/blob/master/protoc-gen-go/generator/generator.go:1181\n        g.P(\"const _ = \", g.Pkg[\"proto\"], \".ProtoPackageIsVersion\", generatedCodeVersion, \" // please upgrade the proto package\")\n. ",
    "karthequian": "@iamqizhao @mikeatlas @derekchiang Thanks for your discussion on this. I also ran into using State() earlier to check whether connections were valid. After reading this discussion, I'm realizing that was a bad idea.\nHowever, looking at ClientConn today, I can't tell what the best practices to use the clientconn. @iamqizhao in the usecase of a web application calling out to a grpc server on a periodic basis, would you reccomend dialing a clientconn at the initialization of the webapp, and then reusing that same connection throughout the lifecycle of the application, or just  calling dial, do your work, and do a connection.close() after that (like the greeter example shows).\nIn the latter scenario, I've noticed that there are a lot of connections that go into a time wait state after I had called a .close() on them. Is that normal?\nThanks in advance!\n. Thanks for your reply @iamqizhao. That makes sense with reusing a connection, and I'll plan to do that.\nI was talking about TCP TIME_WAIT state when open/closing connections (netstat -plant | grep TIME_WAIT)\n. ",
    "annismckenzie": "You can pass grpc.FailFast(false) to the client methods as the last argument (these are the call options). Because we're not ready to use the balancing feature we default to non-fail-fast for now.\n. What timeframe are you thinking of for implementing this? It looks comprehensive and while it does add another 2 layers for the transports and resolvers I really do like to see this done.. So sorry, I didn\u2019t mean to be condescending or plain wrong in that assertion \u2013 thanks for all the hard work!\n\nOn Aug 1, 2018, at 12:46 AM, dfawley notifications@github.com wrote:\n@annismckenzie I'm targeting something in the next month or so. I am taking a couple weeks off soon, but hopefully I can wrap things up pretty quickly when I return since I already have a working prototype.\nI'm not sure what you mean about adding \"another 2 layers\". This is basically just drawing a more consistent line between our existing, hard-coded HTTP/2 transport and the grpc package, and making it pluggable.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @dfawley I'm having a hard time understanding how this is implemented and supposed to be used. Setting the env variable will turn retry support on? Or does that only work when using a Service Config which is also not documented anywhere? I love gRPC but the docs stop right after the basic tutorial. \ud83d\ude48 (as a sidenote: we've been using gRPC in production for over two years now so I'm not a first-time user and have read the source to some degree; the stab at the docs also isn't meant to be condescending) Any pointers would be appreciated.. We have the same general setup with an HAProxy terminating TLS, announcing ALPN (this was crucial) and working in tcp mode. We only have servers written in Go but have clients in Go and Python. For the Go clients we do use grpc.WithTransportCredentials(credentials.NewClientTLSFromCert(nil, \"*.our-domain.tld\")). Our gRPC servers themselves are not using TLS of course, which is why that works. What I don't know for your case is whether Apache can do TLS offloading in TCP mode.. \n",
    "danruehle": "Working on it. There are several tests that use the comparing of error interfaces being equal. I question whether this could possibly be a breaking change for users of the package?\n. I signed it!\n. @iamqizhao This is ready when you get a chance\n. ",
    "alexdeefuse": "Sorry, I've read your comment a number of times, then looked at the code and I don't get what you are saying. Could you elaborate?\n. Thank you for taking the time to write further explanations.\nI now understand how you see this implemented.\nThe reason I was in the dark is because the manner in which context would be used here goes against: https://github.com/golang/net/blob/master/context/context.go#L18 (grpc-go/transport/transport.go).\n. ",
    "jtblin": "Thanks @iamqizhao. The same certificate loads perfectly fine with the Go http server as per the screenshot above so it looks like the certificate is correct.\nDid the example code above for the grpc client and server looked correct to you? I haven't seen real example of using TLS so I am not entirely sure I am doing the right thing.\nExample code for the http side:\n```\n    listener, err := tls.Listen(\"tcp\", \":8443\", tlsConfig)\n    if err != nil {\n        panic(\"Listener: \" + err.Error())\n    }\nmux := http.NewServeMux()\nmux.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"ok\")) })\n\n// To enable http2, we need http.Server to have reference to tlsConfig\n// https://github.com/golang/go/issues/14374\nserver := &http.Server{\n    Addr:      \":8443\",\n    Handler:   mux,\n    TLSConfig: tlsConfig,\n}\nserver.Serve(listener)\n\n```\nThe tlsConfig is initialised exactly the same for grpc, the certificate is returned using the GetCertificate method of *tls.Config.\nThe browser definitely can see the authority and recognize it:\n\nBut in the case of grpc, the error comes from the client and says it cannot recognize it:\n\ntransport: x509: certificate signed by unknown authority\n\nDoes that look correct? conn, err := grpc.Dial(address, grpc.WithTransportCredentials(credentials.NewClientTLSFromCert(nil, \"\"))) \n. Actually you're right, I get the same error when using the Go http client to make the request so Chrome knows the CA but not Go so it looks like the CA is not loaded properly as you said. Sorry to bother you with that.\n. ",
    "ggriffiths": "signed the CLA\n. Makes sense if that's the spec. Looking forward to adding it to the contrib repo.\n. oops, yes my bad\n. ",
    "weiwei04": "Thanks, It seems like an old version protoc-gen-go in another library override the new version, after update, the problem solved.\n. ",
    "jgrusewski": "After I terminate the server or the client the goroutines are stopped.\nThe problem is that this should be a long running service and the gRPC server should not be stopped unless it supposed to. \nThe problem is the gRPC server will crash over time because of the amount of goroutines created.\nEach RPC request starts a new goroutine and is never stopped until the server crashes or the client is stopped. \nBr. Jeroen \n. @menghanl This is strange, for now I cannot reproduce it in my example application either. \nHowever I did pulled a from the grpc-gp repo from github.com again to be sure. \nUnfortunately I still have this issue in my own project.\n```\ngoroutine profile: total 164\n74 @ 0x42db7a 0x4284bb 0x427c19 0x595bf8 0x595c64 0x5973e1 0x5a8080 0x5dae5c 0x5db4bc 0x502af5 0x502c68 0x93d29b 0x93dac4 0x92c2ff 0x925a1f 0x79ecee 0x79eb4a 0x79e732 0x45e521\n0x427c18    net.runtime_pollWait+0x58                       /usr/local/go/src/runtime/netpoll.go:160\n0x595bf7    net.(*pollDesc).wait+0x37                       /usr/local/go/src/net/fd_poll_runtime.go:73\n0x595c63    net.(*pollDesc).waitRead+0x33                       /usr/local/go/src/net/fd_poll_runtime.go:78\n0x5973e0    net.(*netFD).Read+0x1a0                         /usr/local/go/src/net/fd_unix.go:220\n0x5a807f    net.(*conn).Read+0x6f                           /usr/local/go/src/net/net.go:173\n0x5dae5b    bufio.(*Reader).fill+0x10b                      /usr/local/go/src/bufio/bufio.go:97\n0x5db4bb    bufio.(*Reader).Read+0x1bb                      /usr/local/go/src/bufio/bufio.go:209\n0x502af4    io.ReadAtLeast+0xa4                         /usr/local/go/src/io/io.go:307\n0x502c67    io.ReadFull+0x57                            /usr/local/go/src/io/io.go:325\n0x93d29a    golang.org/x/net/http2.readFrameHeader+0x7a             /home/jgrusewski/go/src/golang.org/x/net/http2/frame.go:237\n0x93dac3    golang.org/x/net/http2.(*Framer).ReadFrame+0xa3             /home/jgrusewski/go/src/golang.org/x/net/http2/frame.go:464\n0x92c2fe    google.golang.org/grpc/transport.(*framer).readFrame+0x2e       /home/jgrusewski/go/src/google.golang.org/grpc/transport/http_util.go:417\n0x925a1e    google.golang.org/grpc/transport.(*http2Server).HandleStreams+0x1ee /home/jgrusewski/go/src/google.golang.org/grpc/transport/http2_server.go:243\n0x79eced    google.golang.org/grpc.(*Server).serveStreams+0x15d         /home/jgrusewski/go/src/google.golang.org/grpc/server.go:354\n0x79eb49    google.golang.org/grpc.(*Server).serveNewHTTP2Transport+0x3d9       /home/jgrusewski/go/src/google.golang.org/grpc/server.go:341\n0x79e731    google.golang.org/grpc.(*Server).handleRawConn+0x421            /home/jgrusewski/go/src/google.golang.org/grpc/server.go:318\n0x0\n74 @ 0x42db7a 0x43cdd5 0x43bbdc 0x929b27 0x45e521\n0x929b26    google.golang.org/grpc/transport.(*http2Server).controller+0x596    /home/jgrusewski/go/src/google.golang.org/grpc/transport/http2_server.go:660\n0x0\n```\nI will do some more tests trying to figure what goes wrong...\nThe number 164 adds up quickly... \n. It's fixed turned out I made a mistake, by calling the wrong function in my app.\nThe issue really did happen in my example so I think that's why I got confused.\nThis issue can be closed.\n. ",
    "mischief": "cc @milesward\n. @mwitkow sure, it could be. however, if go-grpc simply used ProxyFromEnvironment it would be fixed for all users.\n. @ejona86 sounds fantastic. my end goal is similar to the enterprise use case, except that i want to run etcd v3 through tor :-) even though tor uses a socks5 proxy natively, i wrote a little program that acts as a http proxy and converts those connections to socks5 proxy requests, so native support in grpc is not strictly necessary - but native support for http proxies in grpc-go is what i want, so all grpc-go programs can benefit from it. ideally, it would be done through ProxyFromEnvironment().\n. ",
    "chandradeepak": "yeah it would be good if grpc supports it by default\n. ",
    "matihost": "I've just entered this issue because i'm behind socks proxy. \nHow can I define different proxy or remove using proxy for gRPC client?\nI'm looking for a solution in the code itself, not via setting system environments.\n. ",
    "nscavell": "@menghanl am i reading the code correctly, does the proxy not support authentication ? For example setting Proxy-Authorization for an http proxy ?. ",
    "robertabcd": "I signed it!\n. ",
    "griesemer": "@puellanivis Another work-around is to use a wrapper closure. Here's an example: https://play.golang.org/p/dbQc6HLGoV\n. FWIW, we are working on a related issue which is a language mechanism that allows us to say that X declared in package p1 is really the same X (p2.X) as declared in package p2; i.e., a mechanism to declare one object (type, variable, function) as an alias of another one. An alias is simply an alternate name for the same object. This is becoming very important if one wants to evolve systems with large numbers of dependencies which cannot all change at the same time.\nWe will post an official proposal soon - probably after the 1.7 release candidate is out.\n. ",
    "aaronjheng": "golang/protobuf#217\n. ",
    "Merovius": "@puellanivis Would it be possible to handle this with a flag to the codegen? i.e. telling the codegen to emit either x/net/context or context code? It would still be painful (it would simply add one edge to the problem, from the project generating the proto code to the project importing that project), but at least it would be a stop-gap measure.\nI want to use grpc, but I also want to use packages that use context and wrapping everything wildly will make me go nuts (no pun intended).\n. @pkieltyka See above. The issue isn't really with the context.Context type itself, but with higher-order types (e.g. generated FooServers, whose methods take a context). x/net/context still defines it's own distinct type (it's assignable and has the same underlying type, but it's still distinct). golang/go#16339 or golang/go#8082 could solve that, but neither exists in 1.7, so there really is no way to resolve this, you have to use either one or the other, in practice.\n. @puellanivis I am aware that it's not possible to generate code that works for both go1.6 and go1.7. I'm asking whether we could have a switch for the codegen that let's you decide to give up go1.6 and x/net/context compatibility. I, personally, don't really care about that; my project likely won't be published until the community sorted out this whole context-mess (likely in go1.8 at the earliest. As you pointed out, an earlier move is infeasible).\n. go 1.8 has been released, but the context problem still isn't resolved, as aliases got rolled back. So, nothing really has changed, in regards to this issue.. The issue isn't about what go versions to support. The issue is, that there is no way to upgrade from x/net/context to context without breaking potential importers (regardless of go version used). If grpc-go would switch to context, literally every package using grpc would suddenly break, because they wouldn't implement the generated interfaces for services anymore. \nThis will hopefully change in go 1.9 with the addition of type aliases and once that is released there is a debate about what go versions need to stay supported (because aliases will only be in 1.9). But currently, as I said, nothing has changed about the technical issues preventing an upgrade to the stdlib context package.\n(Disclaimer: All of this are my opinions and it's not like I have anything to decide here. Just trying to explain why your arguments don't really apply and really nothing has changed). @ariens-shopify that seems to be a different problem, though. It seems you are importing both a vendored grpc package and a non-vendored one and mixing the types of the two.. @peterbourgon How would delaying the switch until go 1.9 and type aliases introduce a breaking change? (or rather: How is doing it now not strictly worse than waiting until type aliases?). @peterbourgon very likely, very likely and loaded question (\"what about the users who, for whatever reason, can't or won't upgrade to 1.6?\" seems to be deemed unimportant). These seem like pretty flimsy excuses to me.\nAnyway. I'm not a decider. If the maintainers decide to introduce a breakage based on these arguments, that's fine, I guess. I don't understand the impatience, though.. So, as a compromise, I would suggest that grpc (and protobuf) provides a branch that targets the dev.typealias branch of go (or, even better, have the master-branch generate code with the appropriate build-tags, but that would probably complicate things). That way a) people who want to move to context can do so explicitly by pinning some packages, b) no one gets broken and c) once 1.9 is released, the changes can just be merged into master.\nJust a suggestion.. Thanks for replying. However, even a loopback-connection will still require the serialization and de-serialization part of all the requests and metadata. That's kind of a waste if we already have the correctly typed things at either end.\nSo this feature request is specifically about bypassing all of that. I haven't yet found a way to achieve that. For example, even if I were to try to wrap it myself and use a codegen, I can't actually inspect the passed CallOptions (and thus any sent metadata, which makes it kind of a no-starter) due to the way it's set up. So, from what I can tell, support for this would need to come from the grpc package itself.\n. ",
    "ariens-shopify": "I'm currently running into this issue importing cloud.google.com/go/pubsub.  I'm just looking for a basic pubsub client. I'm using Go 1.7 and govendor.  Are there any quick hacks that could get me up and running before there's proper fixes?\nvendor/cloud.google.com/go/pubsub/pubsub.go:60: cannot use conn (type *\"github.com/Company/projectvendor/google.golang.org/grpc\".ClientConn) as type *\"google.golang.org/grpc\".ClientConn in argument to option.WithGRPCConn. ",
    "adamryman": "protoc-gen-go has updated to generating code with stdlib \"context\" for reference.\nhttps://github.com/golang/protobuf/pull/275#event-1023244317. ",
    "apolcyn": "@adamryman +1\nseems that regenerated proto files with latest protoc-gen-go don't build against grpc now.. @dup2X we do actually use sync.Pool in the the default \"protobuf\" Codec (see https://github.com/grpc/grpc-go/blob/master/codec.go#L114), since those protobuf.Buffer objects were showing up as a pain point in some benchmarks.\nWe might be able to do this in more places if there are more objects costing a lot that are doable to cache.\nWhat's the scenario and what object allocs are you seeing?. results from a jenkins run, with the cpu usages at https://grpc-testing.appspot.com/job/gRPC_performance_adhoc/119/console\n. Results from the latest update here, rebased onto the updated master branch, on the perf adhoc workers, is https://grpc-testing.appspot.com/job/gRPC_performance_adhoc/133/console\nLooking at secure proto streaming from that run, it looks like it's now at about full cpu utilization on the 32-core server.\n(note that run is from a rebased version of this pr)\n. btw I updated to add pprof server in. I had used these changes to get info locally but would be helpful in general.. latest update also made a change to not launch the debugging pprof server by default. the latest update just squashed and rebased on top of master\n. Closing this, working on a reimplementation in progress\n. @tamird as the underlying slice that's passed to bytes.NewReader is changed on every call to it, thanks but I'm actually having trouble seeing how that could work without first copying into the reset buffer, and then reading back out of it, and causing an extra copy.\n. ah I think I see, it was https://godoc.org/bytes#Reader.Reset rather than https://godoc.org/bytes#Buffer.Reset :), thanks!\n. on second thought, IMO I'd actually lean towards keeping the current code. It's a minor detail, but the change ends up being the same size since we need to change initialization of recvBuffer, plus it's another object to allocate per stream, open to it though...\n. @tamird  for the extra allocation, I was just thinking about the per-stream overhead, where the new recvBufferReader is created per stream. But as noted this can probably be avoided some other way if it's necessary, and I don't have any evidence of it being a potential issue in that case anyways...\nlatest updates use a Reader.Reset instead of copy functions, small detail but used an extra null check instead of changing constructors.. It's looking like the problem on Travis right now is due to *bytes.Reader - Reset now being available on earlier go versions (1.5.4 and 1.6.3 is failing) - unfortunately I think I may have to stick with the original copy used earlier.. Just squashed and rebased, hopefully fixed. New tests picked up when rebasing revealed a bug in the initial slice reuse used in this PR - after marshaling, the byte slice was held onto and put back into the pool, and the marshalled byte slice result could be stomped on by a subsequent marshal from the same pool. Noticeable when marshals that use the same pool use different protos. Added a new test in codec_test.go to cover this.\nThe latest commits revert the slice caching that was causing this, but the main benefit of this change still seems to be just in the proto.Buffer use - saw a small improvement with the slice reuse, but just pooling and using proto.Buffer causes the bulk of the cut in memory use. (in experimental setup, QPS still goes from ~560K QPS to ~650K QPS).\ntotal allocated memory profile, with the latest commit, turns into:\n7295.77MB of 7331.49MB total (99.51%)\nDropped 64 nodes (cum <= 36.66MB)\n      flat  flat%   sum%        cum   cum%\n 1093.05MB 14.91% 14.91%  1093.05MB 14.91%  golang.org/x/net/http2.parseDataFrame\n 1090.05MB 14.87% 29.78%  1090.05MB 14.87%  google.golang.org/grpc/transport.(*Stream).write\n 1071.05MB 14.61% 44.39%  1071.05MB 14.61%  google.golang.org/grpc/transport.(*recvBufferReader).Read\n 1056.55MB 14.41% 58.80%  4933.67MB 67.29%  google.golang.org/grpc/benchmark.(*testServer).StreamingCall\n  713.52MB  9.73% 68.53%  2466.59MB 33.64%  google.golang.org/grpc/benchmark/grpc_testing.(*benchmarkServiceStreamingCallServer).Recv\n  693.52MB  9.46% 77.99%   693.52MB  9.46%  google.golang.org/grpc/benchmark.newPayload\n  681.52MB  9.30% 87.28%   681.52MB  9.30%  reflect.unsafe_New\n  362.01MB  4.94% 92.22%   362.01MB  4.94%  github.com/golang/protobuf/proto.(*Buffer).enc_len_thing\n  346.51MB  4.73% 96.95%   714.01MB  9.74%  google.golang.org/grpc.encode\n  180.50MB  2.46% 99.41%  1270.55MB 17.33%  google.golang.org/grpc/transport.(*http2Server).handleData\n    5.50MB 0.075% 99.49%   367.51MB  5.01%  google.golang.org/grpc.protoCodec.Marshal\n    1.50MB  0.02% 99.51%  4935.17MB 67.31%  google.golang.org/grpc.(*Server).processStreamingRPC\n    0.50MB 0.0068% 99.51%  1071.55MB 14.62%  google.golang.org/grpc.(*parser).recvMsg\n         0     0% 99.51%   362.01MB  4.94%  github.com/golang/protobuf/proto.(*Buffer).Marshal\n         0     0% 99.51%   681.52MB  9.30%  github.com/golang/protobuf/proto.(*Buffer).Unmarshal\n         0     0% 99.51%   681.52MB  9.30%  github.com/golang/protobuf/proto.(*Buffer).dec_struct_message\n         0     0% 99.51%   362.01MB  4.94%  github.com/golang/protobuf/proto.(*Buffer).enc_len_struct\n         0     0% 99.51%   362.01MB  4.94%  github.com/golang/protobuf/proto.(*Buffer).enc_struct\n         0     0% 99.51%   362.01MB  4.94%  github.com/golang/protobuf/proto.(*Buffer).enc_struct_message\n         0     0% 99.51%   681.52MB  9.30%\nThe latest change that reverts the slice reuse shows up in \n5.50MB 0.075% 99.49%   367.51MB  5.01%  google.golang.org/grpc.protoCodec.Marshal, but the overall total allocated space still reduces about 50%\n. this shouldn't have to affect the transport layer like it currently is, moved to https://github.com/grpc/grpc-go/pull/1010. # Reasoning for the change, from experiments and comparisons of profiles on benchmarks\nThe grpc-go server has some room to combine write flushes on unary responses without sacrificing latency. Currently, for unary calls on the server side, there is a flush after each part of the response (metadata, message, and status). By using some delay flags we can combine the response pieces into one flush, and can actually reduce flushes further by combining them between calls. Doing so seems to improve unary throughput by around 20-30% depending on the environment. (note this isn\u2019t an issue on the client side since the metadata and message do end up getting combined as long as there aren\u2019t any user headers).\nShown here are key results I saw from profiles and system call counts of the server during the unary \u201cping pong\u201d and QPS benchmark tests.\nSystem calls on the server were counted across the whole benchmark (including warmup).\nUsed:\nperf stat -p -e syscalls:sys_enter_write,syscalls:sys_enter_read,probe:tcp_sendmsg\nCan confirm these writes are going to the network with the probe. For all of these counts, tcp_sendmsg gets called once for almost every call to sys_enter_write. There are maybe 10-80 fewer calls to tcp_sendmsg than to sys_enter_write on each run.\nThe servers were profiled by sampling the server during the steady phase of the benchmark, \nUsed:\nperf record -e cpu-clock -g --sleep 20\nThese separate profiles were taken in separate runs. I noticed a small overhead with perf stat, but a larger overhead (maybe 5-10%) with perf record.\nNote these experiments and comparisons are using https://github.com/grpc/grpc-go/pull/955 as a baseline, just to rule out things that may have reduced, and to take those improvements. The PR for this (https://github.com/grpc/grpc-go/pull/973) doesn\u2019t include the changes to protoCodec, but it seem to need those changes to see the improved QPS. Without it, the proto Codec seems to override the write flushes.\nBaseline\nBaseline server under unary ping pong test:\n- 282.7 us, 3344.5 QPS (expect ~117K calls over 35 second benchmark plus warmup)\n- 349,724 - syscalls:sys_enter_write (about 3 write flushes per call, for metadata, message and status).\n- 233,156 - syscalls:sys_enter_read (unsure exactly why there are two reads per call. This is done within bufio go library, but the second read doesn\u2019t seem to cost much).\n- 349,709 - probe:tcp_sendmsg\n  - Cumulative 25.25% cpu-clock events in syscall.Syscall\n- 50.39% syscall.Write (of syscall.Syscall)\n- 49.45% syscall.Read (of syscall.Syscall)\nBaseline server unary QPS test:\n- 205180.6 QPS (expect ~7.181M calls over 35 second benchmark plus warmup).\n- 18,770,602 - syscalls:sys_enter_write (about 2.6 write flushes per call. Overlapping message sends are currently able to batch together).\n- 415,454 - syscalls:sys_enter_read\n- 18,769,460 - probe:tcp_sendmsg\n  - Cumulative 15.74% in syscall.Syscall\n- 96.72% syscall.Write (of syscall.Syscall)\n  2.74% syscall.Read (of syscall.Syscall; reads seem batched much better than writes).\n  - Note the proportion of total time in syscall is lower in the QPS test, but it seems like this is due to a much larger amount of time spent in memory allocating functions.\n- Cumulative 29.61% in runtime.newobject, much larger than in the latency test.\nCombining metadata, message, and status into one flush\nServer after combined flush, under unary ping pong test:\nBy combining the grpc metadata, message, and status into one flush on server unary responses, we can reduce to one write flush per unary call.\n- 3176.4 QPS, 295.7 us (observed some noisiness, experimentally saw around the same latency with maybe up to ~5% drop).\n- 110,101 - syscalls:sys_enter_write (at 1 RPC per write flush).\n- 220,165 - syscalls:sys_enter_read\n- 110,086 - probe:tcp_sendmsg\n  - Cumulative 16.80% of cpu-clock events in syscall.Syscall (about \u2157 from before change)\n- 71.53% syscall.Read (of syscall.Syscall)\n- 28.21% syscall.Write (of syscall.Syscall).\nServer after combined flush, under unary QPS test:\nQPS 238645.9 (expect ~8.35M unary RPCs over 35 second benchmark plus warmup)\n- 8,399,651 - syscalls:sys_enter_write (now 1 write flush per unary call).\n- 468,378 - syscalls:sys_enter_read\n- 8,398,336 - probe:tcp_sendmsg \n  - Cumulative 10.42% of cpu-clock events in syscall.Syscall (about \u2154 from before)\n- 93.27% syscall.Write (of syscall.Syscall)\n- 5.79% syscall.Read (of syscall.Syscall)\nCombining flushes of overlapping responses\nWe can combine unary responses of concurrent and overlapping calls into one flush, in the same way that plain message sends are currently batched. Doing so seems to reduce flushes by another ~25%, and add ~5% to QPS. The code added for this uses some atomic increments/decrements of a per-transport counter in the unary handler (counter is uncontended for the ping pong test).\nServer under QPS test, by combining overlapping unary responses into one flush:\n- QPS: 248748.3 \n  - expect 8.7M calls over 35 second benchmark plus warmup\n  - experimentally, this seems to add another ~5%.\n- 6,325,553 - syscalls:sys_enter_write\n  reduced about 25%, and now at about 0.7 write flush per call.\n- 463,198 - syscalls:sys_enter_read\n- 6,324,563 - probe:tcp_sendmsg\n- 9.14% in syscall.Syscall (a slight decrease but not a big change from before)\n- 91.10% syscall.Write\n- 7.82% syscall.Read\nA perf adhoc run with the change on top of the protoCodec change improved unary QPS by ~30%\n. There was a bug in the original commits here - because unary calls weren't flushing all data frames on the server, large messages could use up their send quota and deadlock. This still passed large message tests because the window sizes were greater than the IO buffer size - flushes happened anyways when it filled up.\nThe latest commit fixes this by checking the stream and transport send quotas and ignoring the delay flag if they get too low. It adds a unit test to check the http2_server Write method for this.\nRunning on the benchmarks, this does seem to increase write flushes slightly, QPS drops maybe 5% from the original bug fix, but the drop is somewhat within noise.\nOn one run, it did ~8.6M write flushes with ~227K QPS. But I'd note that this last run was without the change to protoCodec, where the stats above were with this combined with that one. \n. update: actually, the threshold set on the transport quota to ignore the delay flag was too high in the original fix - it set the transport send quota threshold for delay flag ignoring to t.StreamsQuota/2*numActiveStreams. With 64K stream quota and 1M transport quota this is reached easily with lots of concurrent calls. The latest commit lowers the transport quota threshold to the same as the stream quota threshold. \nAnother run, still without the protoCodec change, goes up about 20% since the last one, and writes go down a lot.\nWith ~247K QPS in the last run, it does about 5.2M write flushes. Slightly >1 flush per RPC in the last one to about 0.6 flush per RPC with lowered threshold.\nI'm thinking this low transport quota flush threshold would only become a performance issue with large unary messages, lots of outstanding calls, and a small client-side receive window - otherwise we'll either be flushing quickly anyways, or I the transport quota should just get updated by the updated stream windows. But I think this is tricky to set, PLMK.\n. I'd like to get back to this but will probably create a different PR later that only removes flushes after the headers on server unary, since removing the flush after the message, at least as is done here, I think is making things too complicated.. This was just metrics gathering. I was just trying to get a ballpark idea of where time was getting spent. This is just more data for the record, so closing it.. The first commit here used a non-atomic global counter to index into one of sharded pool. (used a token only to alloc and free from same pool). \nAs far as I can see the non-thread-safety of the earlier commit shouldn't be a correctness problem, but it shows up in the automatic race detector. The latest update changes the counter to an atomic one, which appears to cost about 3% QPS. Looking at the profile, actually about this much new CPU appears to be spent.\n3.02%     2.83%  worker  worker  [.] sync/atomic.AddUint32\nIt still makes an overall QPS improvement over 10% though, there's a small new cost but earlier was breaking the race detection test... . on top of all of the changes referenced in #1031, the latest two commits reduce object alloc counts by from about 15-20% and reduce space allocation ~300MB. \nIt fixes the alloc seen in proto.Buffer.enc_len_struct by adding 4 to the pre-calculated marshalled size (to prevent an alloc in builtin append function).\nIt also saves a slice alloc in proto Codec marshal that appears unnecssary.\nQPS isn't improved beyoned noise much. more on what the last two commits do:\nwith all of the changes referenced in #1031, the QPS streaming benchmark alloc space profile looks like: (showing functions that alloc within - flat in the profile)\n2548.58MB of 2574.68MB total (98.99%)\nDropped 78 nodes (cum <= 12.87MB)\n      flat  flat%   sum%        cum   cum%\n 1365.56MB 53.04% 53.04%  1365.56MB 53.04%  golang.org/x/net/http2.parseDataFrame\n  462.51MB 17.96% 71.00%   462.51MB 17.96%  github.com/golang/protobuf/proto.(*Buffer).enc_len_thing\n  460.01MB 17.87% 88.87%   935.51MB 36.34%  google.golang.org/grpc.encode\n  245.50MB  9.54% 98.40%   245.50MB  9.54%  google.golang.org/grpc/transport.(*http2Server).handleData\n      13MB   0.5% 98.91%   475.51MB 18.47%  google.golang.org/grpc.protoCodec.Marshal\n       1MB 0.039% 98.95%   941.51MB 36.57%  google.golang.org/grpc.(*Server).processStreamingRPC\n       1MB 0.039% 98.99%   940.51MB 36.53%  google.golang.org/grpc/benchmark.(*testServer).StreamingCall\nafter these changes it changes to:\n2298.58MB of 2325.21MB total (98.85%)\nDropped 79 nodes (cum <= 11.63MB)\n      flat  flat%   sum%        cum   cum%\n 1440.57MB 61.95% 61.95%  1440.57MB 61.95%  golang.org/x/net/http2.parseDataFrame\n     226MB  9.72% 71.67%      226MB  9.72%  google.golang.org/grpc/transport.(*http2Server).handleData\n  162.50MB  6.99% 78.66%   479.01MB 20.60%  google.golang.org/grpc.(*serverStream).SendMsg\n  159.50MB  6.86% 85.52%   160.50MB  6.90%  google.golang.org/grpc.protoCodec.Marshal\n     155MB  6.67% 92.19%      155MB  6.67%  google.golang.org/grpc.(*parser).recvMsg\n     154MB  6.62% 98.81%   314.50MB 13.53%  google.golang.org/grpc.encode\n       1MB 0.043% 98.85%   635.01MB 27.31%  google.golang.org/grpc.(*Server).processStreamingRPC\n462.51MB 17.96% 71.00%   462.51MB 17.96%  github.com/golang/protobuf/proto.(*Buffer).enc_len_thing appears to have been allocing when appending the length of the protobuf struct to the marshalled proto, in https://github.com/golang/protobuf/blob/62e782f47e4c57935994133e44f1716d281504e4/proto/encode.go#L1311. (previously the buffer was set with the size returned from proto.Size, but it appears that this length isn't enough. \ncc @carl-mastrangelo it seems kind of strange that setting a proto.Buffer's buffer to proto.Size isn't enough?\n(On the removal of the second make([]byte) in protoCodec.Marshal, it was just unnecessary and straightforward to remove.). for previous comment, discussed offline but main benefit of the memory-saving seems to be by caching the proto.Buffer objects rather than the slices (these appear much more expensive than the slices at least on small messages).. Heads ups on another change for this PR: cc @carl-mastrangelo actually revisiting the use of sync.Pool rather than the cache created in this PR is indeed showing the sync.Pool doing about as well (sometimes better but improvements within noise).\nA small micro-benchmark comparison of the two pools (repeatedly doing concurrent marshals/unmarshals) show similar results, with maybe a slight (within 10%) improvement with the custom cache in here. But if anything overall effect seems better with sync.Pool\nThis can probably be a lot simpler using just the sync.Pool. it seems i ended up needing to go back to not using testing.B.Run(), since it was failing tests here on go platforms <= 1.6.\n. ftr latest update removed use of defer. Re-running benchmarks shows a small but seemingly noticeable difference of something like 95ns -> 90ns. And seems simple to remove it... cc @carl-mastrangelo I think this is ready for another look - added subtests back in only for go-1.7+. ping on this - this should ready for another look.\nFTR the main problem here seems to come from the fact that escape analysis doesn't place the proto.Buffer on the stack. Looks like that's a current limitation of go escape analysis - (see a Q/A posted in https://groups.google.com/forum/#!searchin/golang-nuts/protobuf%7Csort:date/golang-nuts/Duib6o2FFgg/Yw-iMmn7AAAJ). the test failure is on go-1.5 only, on the TestFlowControlLogicRace (https://github.com/grpc/grpc-go/blob/master/test/end2end_test.go#L3332)\nThis PR shouldn't be touching the flow control logic at all but this error needs some looking into.. FTR the previous travis run seemed to have hit a flake in TestFlowControlLogicRace on go-1.5.\nBut travis is now green - this PR was rebased on master, with go-1.5 builds no longer running. . @xiang90 makes sense, but just to be clear, setting core_limit = 1 in the scenario config is actually just setting GOMAXPROCS to 1 (the default for go). We're currently breaking defaults by setting GOMAXPROCS = 32 (on 32 core machine), which seems to be hurting performance for in this one test.\nThough I'm not sure if a high GOMAXPROCS on a server might be common if not default.\n(core_limit is just a test parameter for benchmark client/server) . I see, sorry I was mistaken here, closing as this is probably not going to be commonly done. Just since more runtime tuning is under discussion here, I'll add that experimentally I have seen good increases in throughput by raising GOGC in benchmarks - understand this is definitely under discussion but there's a request that does this in https://github.com/grpc/grpc/pull/8448 (tuning params provided by the benchmark runners in these requests). @xiang90 I'd add that there are published suggestions to tune these - I believe these knobs are meant to be used.\nfrom https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs\n\"So it can make sense to set GOGC to a higher value (200, 300, 500, etc) if you can afford extra memory consumption.\"\nfrom https://blog.golang.org/go15gc:\n\"If you want to lower the total time spent in GC, increase GOGC\". also FTR the server is running on go-1.8beta2 for these tests, to see possible change in GC costs.. FTR the remaining sources appear to be more difficult to remove:\nfor the rest of the alloc sources, it appears that:\n1330.56MB 53.20% 53.20%  1330.56MB 53.20%  golang.org/x/net/http2.parseDataFrame from within go http2 library, for new structs per data frame read. \n463.51MB 18.53% 71.73%   928.51MB 37.12%  google.golang.org/grpc.encode from https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L295\n456.01MB 18.23% 89.96%   456.01MB 18.23%  github.com/golang/protobuf/proto.(*Buffer).enc_len_thing undetermined but within proto library.\n214MB  8.56% 98.52%      214MB  8.56%  google.golang.org/grpc/transport.(*http2Server).handleData from https://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L415\n9MB  0.36% 98.88%   465.01MB 18.59%  google.golang.org/grpc.protoCodec.Marshal from new slices in https://github.com/grpc/grpc-go/pull/1010/files#diff-365652330e79ea705fe1e6d0a561ee13R65 and https://github.com/grpc/grpc-go/pull/1010/files#diff-365652330e79ea705fe1e6d0a561ee13R73\nBut profile with this shows ~15%  cum. CPU clock in transport.(*recvBufferReader).Read and 22% in write syscalls.\n. @tamird thanks but I think the removal of the interface effect ends up having the same benefit of copy-by-ref to copy-by-value.\nI was thrown off by this as changing the pure recvMsg{} struct to implement item interface was tried first, but I when looking at the memory profiler, the same total allocation that used to be in transport.(*Stream).write was just switched to the go runtime function that converts structs to interfaces (runtime.convT2E I think it was). Looked into this and from what I found converting a go struct to an interface causes a heap alloc of the struct and puts its pointer in the new interface object, so the problem of passing a pointer to alloc'd object was still there.. I see, updated title. done, thanks!. Just updated, I think this should still be good.\ncc @mehrdada for how this incorporates your recent change to transport.go. cc @MakMukhi btw just cc'd for #940; #940 is still pending, but not sure if this one overrides that one.. just updated, sorry for delay. latest update fixes a bug in this PR that appeared with census turned on. It's now using an alternate Codec interface to pre-allocate the slice once and not mutate the slice after marshaling.\nI think the latest change is more explicit and clear though anyways. Performance effects are the same, but the ~30M allocs totaling ~450MB for msg marshaling just gets moved to the encode function.\nmemory profile changes to:\n461.51MB 61.56% 61.56%   461.51MB 61.56%  google.golang.org/grpc.encode\nNote that the performance improvements from this change are much smaller than those of other PR's such as #1010, #940, or #1029, but mainly this seems to be because the messages in this benchmark are small (~10 bytes), and larger struct allocs are having bigger effects. But still the alloc this saves grows with the message size.\n. Closing this for now. Thinking caching of slices could be done at a larger level - protoCodec should probably use cached slices from the protobuf marshal to the complete grpc encoded message.. I think this method of simulated latency might be unfair.\nAs is, it looks like the server is basically sleeping for the full latency (e.g., 32ms) before each write to the network. AFAICS this blocking behavior might be realistic when e.g., sending the large 1M message from a fast sender to a slow receiver, or possibly sending across a faulty network, but I think it's probably unfair otherwise. It looks like the 13 byte window updates coming from the server (receiver) are taking much longer than they should, with effects of small window/update-threshold larger than they should be.\nFWIW, looking into the logs posted by @rsc:\nAt first the client has a fresh 64K window and so it sends the first 64K of it's 1M message. It doesn't arrive at the server until 32ms later because of latency. The window update isn't sent until ~64ms later (took 32ms to send settings ack at 0.138?).\nWhen server receives 64K at 0.138 (32ms after send), it should be ready to give 4 window updates of 16K each right away (GRPC uses 16K threshold to send update), and then the server should be again be able to again send 64K when all of these arrive (32ms later). Because each write is blocking though, the window updates are spread out with 32ms between each.\n0.105s\n    <-65702\n0.138s\n    ->9\n0.171s\n    ->13\n    <-16393\n0.206s\n    ->13\n    <-16393\n0.243s\n    ->13\n    <-16393\n0.281s\n    ->13\n    <-16392. I tried simulating latency on my localhost by using:\ntc qdisc add dev lo root handle 1:0 netem delay 100msec\nrunning the test here with 0ms latency induced from the golang test code, and got:\n```\nDuration    Latency Proto\n3.41255655s 0s  GRPC\n3.412687395s    0s  GRPC\n3.411504574s    0s  GRPC\n3.411832411s    0s  GRPC\n3.411427349s    0s  GRPC\n4.241488937s    0s  HTTP/2.0\n3.413378604s    0s  HTTP/2.0\n3.412536441s    0s  HTTP/2.0\n3.413333573s    0s  HTTP/2.0\n3.413284508s    0s  HTTP/2.0\n1.637170729s    0s  HTTP/1.1\n602.145344ms    0s  HTTP/1.1\n601.863744ms    0s  HTTP/1.1\n406.623978ms    0s  HTTP/1.1\n406.15212ms 0s  HTTP/1.1\n```\nMy main observation was that now the network writes calls can happen much more asynchronously (not blocking the app for full RTT on each write). The strict ping-pong between window updates and data transfers is gone (previously gRPC's relatively small window size and frequent window updates were seeing a really large penalty for this).\nPrinted out the sums of the transfers and their times again with this \"tc induced 100ms latency\" (200 RTT). see https://gist.github.com/apolcyn/ad0068220c74130427f0030cdc095e55\ne.g., at line 220 from that log: (middle of gRPC message send)\n\n\nnote how the server is able to now send multiple window updates within a short time period, as I believe is realistic - so the client's view of window size stays higher.\n```\n1.994s\n<-27648\n->13\n<-21530\n->26\n\n\n2.193s\n2.193s\n <-2048\n\n2.194s\n2.194s\n <-14345\n\n ->13\n\n <-22528\n\n```\n. Sorry if unclear in last two comments, just to summarize: \n- Definitely issues with grpc and net/http2 vs. http1 for large message because of the http2 flow control window size. Possible solutions noted by @iamqizhao\n- net/http2 and grpc look like they're both using the same fixed window size - their performance in this situation looks about the same. \n- Differences in when receiver sends window refresh to sender should be irrelevant. Mainly I think the latency simulation should be changed to rule out things like that.. @petermattis \n\nIs there any progress I can follow or help I can give? Perhaps a short term unsupported solution which allows configuration of initialWindowSize and initialConnWindowSize on a per-server/client basis.\n\ncc @menghanl. Maybe some temporary API options to client conns and grpc servers is an option? There's #1073 as another option\n@c4milo \n\nIs there an issue open to follow up on the migration to the standard HTTP2 package? I couldn't find it.\n\ncc @sailat . But no I think this might be brought up in some other issues but there is no issue about that.\n. @petermattis would the temporary API be ok, yes I think https://github.com/grpc/grpc-go/compare/master...petermattis:pmattis/configurable-window-size?expand=1 makes sense.. made one update for golint error after opening. I think this should be ready now. actually looks like this is unnecessary. cc @menghanl, this is the Read -> ReadFull change to transport.Stream discussed. cc @sailat . @MakMukhi \n\nThe http2 framer.WriteWindowUpdate(streamID, incr uint32) doesn't support int64. It's better to let the sizes be in uint32.\n\nI think this points to a bug:\nThe max legal window update and window is (2^31 - 1), but with the max possible grpc message being (2^32 - 1), this PR could give an illegal/overflowing window update.. This is UNSAFE as is - there is a major bug here: if we do a pre-emptive window read that puts the pendingData in the window below zero, then receive data on the stream but the app doesn't read it, and then the stream is cancelled, then the pending data on that stream won't be given back to the connection (as is the case currently) - the connection will eventually deadlock if it happens enough.\n. latest updates tentatively fix the race in which pending data in a closed stream's recvBuffer isn't given back to the transport.\nit also moves the API break where transport.Stream exports ReadFull instead of Read. @glycerine there was a major bug here due to a simple misuse of io.LimitReader which I think you were hitting. The latest updates fix that.\nThough this could still use some more cleanup, I'd like to get this in as a fix to the high-latency/large-message issues, since it doesn't need the user knobs.\nThis should really help for scenarios involving:\n * high latency\n * large messages\n * low concurrency (the stream window is dynamic but the 1M conn window still doesn't move).\nOf course more than welcome to retest with the latest commits here - I'd expect about 15x speedup for the situation above (with window bottleneck moving from 64K to 1M). go1.6.3 only failed on travis on FlowControlLogicRace - likely a real issue.. actually re-running travis has FlowControlLogicRace pass, and I haven't been able to reproduce that failure either on this branch or master, with go1.6.3 or go1.8. cc @dfawley btw this is the PR I mentioned offline. Just squashed and rebased to resolve conflicts. I think this is ok for a look.. superseded by #1248. Created pr to attempt to test/verify this btw, in https://github.com/grpc/grpc/pull/9776.\nIt does appear that master currently times out after running out of window quota, but the test case passes with this. fixed by #1076. lgtm on latest update!. What benchmark are these measurements from?\nId' note the goal of the change that started using cached proto.Buffers was not to save byte slice allocations, but rather to save proto.Buffer allocations.\nMeasuring total allocations by the benchmark in https://performance-dot-grpc-testing.appspot.com/explore?dashboard=5636470266134528&widget=952298825&container=846542667 (using small messages), over a roughly 30 second perioud, showed about 16GB of total allocations on the server, with a little more than 8GB due to allocation of proto.Buffer structs, (in https://github.com/golang/protobuf/blob/master/proto/encode.go#L235 and https://github.com/golang/protobuf/blob/master/proto/decode.go#L412). \nOne thing I'm wondering is if the protos generated from an updated generator would have an effect here though.. > Also for the benchmarks you linked above, are those nightly master runs? I'm not sure how to navigate it and what/how to infer, are the benchmarking programs publicly accessible?\nYes those are ran about daily. Running manually is slightly involved but there are some docs on it in https://github.com/grpc/grpc/blob/master/tools/run_tests/performance/README.md.\n\nI still think golang/protobuf#418 is the right way to solve this in conjunction with this changeset. Presumably this what you meant with \"if the protos generated from an updated generator would have an effect here\".\n\nI see, I think that change in protobuf can make sense. Also yes that is what I meant to say - current proto.Buffer caching saves allocs when created protos don't satisfy those interfaces.. test failure seems unrelated?\n-- FAIL: TestLargeMessageWithDelayRead (10.00s)\n    transport_test.go:986: s.Read(_) = _, stream error: code = DeadlineExceeded desc = \"context deadline exceeded\", want _, <nil>\n    transport_test.go:986: s.Read(_) = _, stream error: code = DeadlineExceeded desc = \"context deadline exceeded\", want _, <nil>\nFAIL\nFAIL    google.golang.org/grpc/transport. This seems to be hitting the flake described in https://github.com/grpc/grpc-go/issues/2088. a couple of minor post-review changes made per the \"go vet\" checker. sorry I'm not sure if I understand here. \nOf the time elapsed since the last reset, I was neglecting the time taken to merge histograms. Do you mean to add the keep sampling the timeElapsed after merging histograms?\n. @carl-mastrangelo I proto.Size once at the beginning, is it being called again here implicitly?\n. done. done. reworded this. done. done. done. done. done, tweaked the if statement. done. done. done. Wanted to keep just so slice is no longer ref'd and can be GC'd if ok? also saw this usage elsewhere. woops thanks good catch I missed that. put all of the cleanup into a defer. done. done. done. I think this should be getting compiled in. Keeping since it' only used locally? (e.g., go bench memory stats of show no difference). At least local ProtoCodec micro benchmarks showed no difference beyond usual noise with and without defers. If it sounds ok, thinking we should keep them in?. I replaced the proto.Size calculation with a guess based on length of buffer.GetBytes from the last call. Micro benchmarks had something around 20% improvement! E.g., under SetParallelism(1), usual times went from ~130-140ns to ~100-110ns.. done. all of this changed to use the RunParallel framework. done. I actually noticed that go proto objects in micro benchmarks aren't doing the realloc in enc_len_thing as is happening in the large full grpc benchmarks. I took this out for right now. Is this only needed on certain protos, fixed in new versions of go proto compiler?. The latest update wraps the cached proto.Buffer structs into a larger struct with a cached size too, so atomics are no longer used.\nTried the sync.Pool too, though benchmark speeds weren't decreased, the mutex contention profiler seemed to show a lot higher contended mutex counts and times with the separate pools rather than single pools. (something like 2mins vs. 1 min.)\nResults I'm getting are a bit noisy or hard to make complete sense of, but the latest updates seem to definitely make an improvement in micro benchmarks, over the previous proto.Size calls. . fixed. a bit stuck, isn't p loop-local? or at least closed over each round?. scratch the closed part :) but I thought should be loop local. Also moved to inside inner function... I tried this one btw but unfortunately the differences in runs seemed usual with noise . just updated to move this on client and server. actually the Codec interface is here currently, this PR moves it from where it was (used to be in https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L57).\nAdded a comment though to mention that implementations must be thread safe.. done. changed lastMarshalledSize field to a uint32 and capped it to maxInt32. \n(Though it's probably not API safe, I think protobuf marshalling will never return something greater than 2^31-1\n- going off of https://github.com/golang/protobuf/blob/62e782f47e4c57935994133e44f1716d281504e4/proto/encode.go#L87). renamed elapsedUtime -> uTimeElapsed and elapseStime -> sTimeElapsed here and elsewhere.. done. removed this. still haven't addressed this yet but might revisit. done. I think was needed on both sides only since I wanted to have a good chance that the reader will have started trying to read before any bytes have arrived, on both client and server. \nBut that said this is racey and I'm wondering if this would be better replaced by unit tests on an inFlow instance ... doing some experimentation.. should the custom intitialConnWindowSize and InitialWindowSize should be capped to something smaller, and lower-bounded to current values here?. It looks like iwz should be reset to the custom value only if config.InitialWindowSize >= initialWindowSize, rather than the defaultWindowSize, since defaultWindowSize could potentially be less than initialWindowSize?. since currently connection window is initialConnWindowSize, should icwz should be reset only if custom value is greater than initialConnWindowSize, to keep the current value as the lower limit?. It looks like after their inFlows are created and the connection window updates sent, the initialConnWindowSize fields are no longer used by the transport structs, can they be gotten rid of as fields in the structs?. s/max/min here?. scratched above comment. Optional nit, I think this can be done in a follow-up: \nmaybe this time.Sleep can be replaced with internal access to settings frame sends and acks - then this could wait on a settings frame acks. (It looks like it risks flakiness as is).. related to comment above, maybe this sleep can be replaced by starting an RPC and then waiting for something on the server to make sure it's been started.. Yeah I can't see any way myself to avoid the race/sleep without something \"testonly\".\nMy idea for it could be something like:\nMaybe the handler that apply's settings could notify a test-only \"settings applied\" channel, or the same  thing could be done with the \"settings ack\" handler in the client.. small wording nit: rename to estimatedSenderQuota and estimatedUntransmittedData, for clarity here?. nit: I think the use of fullReader and other changes to this file can be reverted, since the transport is still an io.Reader.. optional nit, sorry one more naming suggestion:\nchange to maxPossibleSenderQuota and minPossibleUnTransmittedData (keep estUnTransmitted)?. nit: s/want %v/want 0. @mehrdada done for first comment, sorry for delay.\n\nAlso, do we ever close the channel in recvMsg?\n\nI don't think we do explicitly close this channel, but I think it's ok for this channel, since the blocking read can unblock when the context is Done\n. Might be wrong, but for the scenario in which the client has an inbound flow control which is less than the size of the server's response message, IIC this change could cause deadlock, with the server running out of flow control and the client not having yet received anything.\nThis looks similar to the change in https://github.com/grpc/grpc-go/pull/973, which ran into that issue.. My suggestion is ugly here, but in order to be consistent with Java and C++, mind switching the command arg from  \"--use_google_default_credentials\" to  \"--custom_credentials_type\", and then when use google default credentials when  \"google_default_credentials\" is passed in as the value?\nDoing so would just make it consistent with the other languages (See e.g.https://github.com/grpc/grpc/blob/master/tools/run_tests/run_interop_tests.py#L782). nit: to be consistent with how perRPCCreds is set, initialize transportCreds := opt.TransportCredentials at the top and then set:\nif t := b.TransportCredentials(); t != nil {\n        transportCreds = t\n}\n?. Hmm, I'm not sure it's necessary to make the also check that we're running on GCP within this credentials bundle; doesn't ALTS credentials itself already make this check?. The connection to the grpclb server doesn't actually need OAuth credentials.\nShould probably be consistent with Java and C++ on this. \ncc @jiangtaoli2016 @yihuazhang can we confirm that Java and C++ don't send OAuth creds to the balancer?. General comment: going by the name and documentation for this SwitchMode API, it seems that there's an implementations have an implied state change when it's called. But statefulness doesn't actually seem to be needed for the the GoogleDefaultCredentials implementation. Is this API just geared to handle the general case where a state change on the creds bunde can occur? If not, is there some way to make this stateless?. yep, and done. done. no good reason. And good point on using the proto, done. no good reason, removed per above comment. let's chat offline. For now, I'm using SplitHostPort. done. good point, done. The test suite that's using it needs it. But I could also leave it out now and then add it in a follow up.\nIt's a useful test scenario for exercising some timing edge cases, and seems to reveal some unexpected behaviors so far.. hmm, I agree this is a little weird, but the current flags do allow specifying insecure (by keeping both of these flags off), and it's convenient to be able to pass the same flags to this server as for the interop server\nWhat do you think?. this was a messy way to handle the case when --backend_addrs is an empty string (which is actually valid and used for some test scenarios). Revised to clean this up. done. done. sounds good, done. ",
    "y0ssar1an": "Why clutter the code with type aliases when we can just drop support for Go < 1.7? Anybody stuck on Go 1.5 or 1.6 can vendor the current grpc version. If they can't upgrade to at least Go 1.7, they've got bigger problems than being able to run the latest grpc.. Recompiling pb.go files doesn't sound like a crippling problem. I think we should just rip off the band-aid.. ",
    "flisky": "I don't think we should worry about this on golang >=1.9, since golang/net@b3756b4b77d7b13260a0a2ec658753cf48922eac makes two types the same.\n. I've sighed it\n. Wait a minute, Re 'sleeping is not desirable since it cannot be interrupted by the server stop signal':\nI know Python's time.sleep cannot be interrupted in a child thread, and prevent the main program from exitting.\nHowever, time.Sleep can be interrupted when OS signal occurs in golang.\nDo I misunderstand anything?\n. Updated. Ping @menghanl, @iamqizhao.\n. Done.\nFYI, the travis CI failure seems irrelative.\n. Sorry, my bad. I misunderstand the go version tag.\nThe go1.6 means go1.6 and above.. Actually you could. I'd like to share my hack-but-simple way to archive this -\n\n\nedit the .pb.go file to change the server interface to return ([]byte, error)\n\n\nimplement custom codecs\n```\nimport \"github.com/gogo/protobuf/proto\"\n\n\ntype Codec struct{}\nfunc (Codec) Marshal(v interface{}) ([]byte, error) {\n        return v.([]byte), nil\n}\nfunc (Codec) Unmarshal(data []byte, v interface{}) error {\n        return proto.Unmarshal(data, v.(proto.Message))\n}\nfunc (Codec) String() string {\n        return \"no-op-marshal codec\"\n}\n```\n\nconstruct the server with grpc.CustomCodec\n\nI was surprised by grpc-go's customizability when I digged into this, so enjoy:). The magic goes to Codec.Marshal, and the default codec treats value as proto.Message. \nI think you may keep the default codecs and return a specific type with smallest marshal overhead.. I think it's more appropriate to ask a package manager tool like dep to support this feature.\nSee their discuss on golang/dep#248. so we just log and retry without any sleep?\n. Err, how about Serve returns when lis.Accept fails except for temporary network errors, or Serve returns a non temporary network error when lis.Accept fails? Which do you prefer, or do you suggest any one?\n. and this? ping @menghanl \n. ",
    "johanbrandhorst": "Bump, is this possible now that we have aliases in 1.9?. Just to make it clear for us on 1.9 - whats required in the code to use context and golang.org/x/net/context interchangeably?. It's been almost 6 months since the latest status update from @dfawley, what is the latest status of this effort?. #974 silently changed behaviour, this restores the behaviour to what it was, is that correct?\n. @menghanl Thanks for your reply. The nested nature of the proto.Message definition is a problem, as gogo/protobuf has managed to avoid importing anything defined in golang/protobuf thus far. This could still be solved with a client compatibility layer wrapping a Protoer implementation in gogo/protobuf since any gogo/protobuf.Message and golang/protobuf.Message will be functionally equivalent, if technically separate types.\nInviting @awalterschulze (maintainer of gogo/protobuf) to chime in.\n. @prasek fantastic work! What would be required to integrate this with gogo/golang/gRPC? I'm guessing we'd need these wrappers added to each implementation? I'm sure @dsnet will have opinions on this as well.. I've signed the CLA. @dfawley That's certainly a middle ground, and better than today, but unfortunately it's still not a very ergonomic experience for users of gogoproto registered types. I think it would look something like this:\n```go\npackage server\nimport (\n    \"context\"\n\"google.golang.org/grpc/codes\"\n\"google.golang.org/grpc/status\"\nspb \"google.golang.org/genproto/googleapis/rpc/status\"\n\"github.com/gogo/googleapis/google/rpc\"\n\"github.com/golang/protobuf/ptypes/any\"\n\"github.com/gogo/protobuf/proto\"\n\n)\nfunc GetError(ctx context.Context, req rpc.DebugInfo) (rpc.DebugInfo, error) {\n    st := status.New(codes.InvalidArgument, \"some error\")\n    br := &rpc.BadRequest{\n        FieldViolations: []*rpc.BadRequest_FieldViolation{\n            {\n                Field:       \"Some field\",\n                Description: \"Some error description\",\n            },\n        },\n    }\n    byts, err := proto.Marshal(br)\n    if err != nil {\n        return nil, err\n    }\n    stWithDetails, err := st.WithDetails(&any.Any{\n        TypeUrl: \"type.googleapis.com/\" + proto.MessageName(br)\n        Value: byts\n    })\n    if err != nil {\n        return nil, st.Err()\n    }\n    return nil, stWithDetails.Err()\n}\n```\nThis isn't all that much better than just constructing the spb.Status ourselves and using status.FromProto as you suggested in your reply to #1885. The primary problems with this proposal is:\n\nThe user has to deal with a lot of boilerplate that status.WithDetails can hide.\nThe user has to import both gogo/protobuf and golang/protobuf and types from both generated googleapis repos. This goes against the desire of the GoGo maintainers to allow users to use one or the other.\n\nThe approach taken in this PR would allow something like this instead (using https://github.com/gogo/status in place of the gRPC status package):\n```go\npackage server\nimport (\n    \"context\"\n\"github.com/gogo/googleapis/google/rpc\"\n\"github.com/gogo/status\"\n\"google.golang.org/grpc/codes\"\n\n)\nfunc GetError(ctx context.Context, req rpc.DebugInfo) (rpc.DebugInfo, error) {\n    st := status.New(codes.InvalidArgument, \"some error\")\n    stWithDetails, err := st.WithDetails(&rpc.BadRequest{\n        FieldViolations: []*rpc.BadRequest_FieldViolation{\n            {\n                Field:       \"Some field\",\n                Description: \"Some error description\",\n            },\n        },\n    })\n    if err != nil {\n        return nil, st.Err()\n    }\n    return nil, stWithDetails.Err()\n}\n```\nBecause gogo/status.Status.Err() implements the interface exposed in this PR, the gRPC runtime can transparently convert the gogo/status produced errors into grpc/status types. On the other side, because gogo/status can convert grpc/status error types, the user can stay entirely within the GoGo ecosystem.\nI hope that further clarifies the intent. Please see my blog post on errors, in particular the problems associated with using GoGo protobuf with the gRPC status package for more information: https://jbrandhorst.com/post/grpc-errors/.. I think you misunderstood the point of my reply. I was showing the implications of this PR versus the implications of your suggestion, for gogoproto users, when creating a status error. The FromError case is used when parsing an error from a gRPC call, which I was not trying to show.\nI pointed you to https://github.com/gogo/status which indeed implements the proposed interface: \nhttps://github.com/gogo/status/blob/b2af61acbd13657c2e3f375c04281e38d7c941f3/status.go#L43.\nI further direct you to the implementation of gogo/status.FromError to give you the full picture:\nhttps://github.com/gogo/status/blob/b2af61acbd13657c2e3f375c04281e38d7c941f3/status.go#L153.\nIndeed, just to be explicit, the point of this PR is to allow gogo/protobuf users to work effortlessly within the gogo/protobuf ecosystem which they are forced to by the design of the golang/protobuf project.\nThe interface is created so that we can return gogo/status.statusError from a users function and have it correctly parsed by the gRPC runtime. On the other side of the gRPC transport, a user will again be able to use gogo/status.FromError to translate from a grpc/status to a gogo/status.. @dfawley that sounds promising, I'll give it a go and update the PR when I have a moment. Thanks!. I've given this some testing, and it works just as well as the previously suggested code, so I've updated the PR with the simpler interface.. :tada: thanks a lot! Excited to see this go in :).. Just curious, is there a specific case where this clashes currently?. That's interesting! Thanks for sharing.. This API existed before https://github.com/grpc/grpc-go/pull/1904 and was removed.. @dfawley that's great to hear, and would potentially provide a nicer solution for #2174 \ud83d\ude0a.. I tested this against an Improbable gRPC-Web proxy with https://github.com/improbable-eng/grpc-web/pull/213 applied. It currently only builds against tip of Go.. This can be built using the fresh-off-the-presses go1.11beta1: https://golang.org/dl/#go1.11beta1. See https://github.com/johanbrandhorst/wasm-experiments/blob/1bb4e7fb1e3081fa807f5f22a46f79f07520f2c3/grpc/frontend/frontend.go#L19.. I made this a little bit easier with a separate repo: https://github.com/johanbrandhorst/grpcweb-wasm-example.. Cool, I eagerly await updates on #2112 then :).. I do plan on submitting a PR adding transparent WASM client support to this library when the transport interface is available. It'll either be merged and activated under the WASM build tag or I'll publish and maintain it on my public github. Watch this space :).. If you want to see #2174 in action, check out the demo on my talk about WASM: https://youtu.be/iTrx0BbUXI4?t=847.. @hsaliak the motivation for me was to have a gRPC client written in Go transparently work if the build target changes to WASM. It would automatically use a gRPC-Web compatible client, of course, but it would allow for some cool stuff, and maybe even compiling existing go gRPC clients for use in the browser via WASM.. Done. Do these backticks translate correctly to monospace in GoDoc?. ",
    "jonathaningram": "@dfawley if it helps, this is mostly resolved for my project after upgrading to 1.9, except that GAE standard will probably not push out 1.9 that soon so for those projects of mine using GAE, the problem still exists.. ",
    "edmund-troche": "Have we considered starting a 2.x branch which would have the new context? Usually a change in the version major implies a break in compatibility, so it would not be a surprise for someone moving to 2.x. There could be some parallel development going on 1.x and 2.x during a deprecation period and then eventually is just 2.x. \nI'm coming in late to this discussion, so maybe I've missed previous discussions along the lines of what I've suggested, also maybe what I'm suggesting is just not feasible, but just curious why/if this would be a reasonable approach.. ",
    "Ulexus": "@edmund-troche For just this issue, that would be far overkill.  With Go 1.9, this has mostly turned into a non-issue.. ",
    "achillesss": "Since I can not ensure using Day_WeekDay on client side, I may check by Day_WeekDay_name.\nThanks for your help.\n. ",
    "bronze1man": "Sorry,I looks like the SupportPackageIsVersion2 means a lot of commitid.\nhttps://github.com/grpc/grpc-go/issues/663\nI found a way to get the right commitid .\n. ",
    "reterVision": "Thanks for the info. It makes sense and I'm closing this.\n. Completed the CNCF form.. Re-Trigger Travis build to solve\noci runtime error: exec failed: container_linux.go:265: starting container process caused \"could not create session key: disk quota exceeded\". ",
    "sarietta": "Fair enough. This must be an issue with the C library in that case, because our client repeatedly fails to issue gRPCs to a go server with the error:\ntransport: http2Server.HandleStreams saw invalid preface type *http2.WindowUpdateFrame from client\nIs it possible that an empty SettingsFrame is causing the issue? \nI will also make an issue on the C library for grpc to see what that side of equation has to say about it.\n. ",
    "adityadani": "Hello,\nI am also seeing the same error. I am trying to run the helloworld example where the server is running in go and client is in cpp. I am pretty new to grpc and protobufs.\nI was unable to find any issue related to this in grpc C library.I was wondering if there is a workaround for this issue? \n. ",
    "promiseofcake": "Seeing issues with this from requests coming from PHP clients as well on 0.15.0. @sarietta, what grpc version/hash were your non-go clients pinned to at the time?\n. ",
    "phbcanada": "We are running into this problem with clients using 0.14.0. Does this still occur with the 1.0.0 release?\n. ",
    "R-omk": "But how to identify during request  that is the same connection?\n. But PerRPCCredentials set up http2 header on each request in the same tls connection,  or it is not?\n. Yes, it's correct, I am interested in unary type.\nHow to distinguish one from the other connection on the server side based on the context in a method or in a grpc.UnaryServerInterceptor\n. And another question, does client identification with private key?\nAnd how to identify a client that connects to grpcServer with own exclusive private key?\n. The short answer to my question\np: = peer.FromContext(ctx)\n//  if p.AuthInfo.AuthType()== \"tls\"\ntls, _ := p.AuthInfo.(credentials.TLSInfo)\nuniqConnectionId := tls.State.TLSUnique  // byte array\n. ",
    "AmandaCameron": "I mean the opposite, sometimes I'll just stop receiving messages from the\nstream while not being given any kind of error, I suspect it's during some\nkind of network fault.\nOn Fri, Jun 24, 2016, 14:44 Qi Zhao notifications@github.com wrote:\n\nI am not sure I understand what exactly your question is. Do you mean\n\"conn.Recv()\" sometimes falls over with \"os.Exit(1)\"? If yes, this is\nexpected, streaming RPC is NOT tolerant to the network errors.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/734#issuecomment-228428051, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAG_zn1Qn9xm7r6ULLK991akO2aLfDe5ks5qPCWPgaJpZM4I-AmI\n.\n. Events being published to the stream by the server are never appearing on\nthe client, dispite being dispatched from the server's side.\n\nOn Fri, Jun 24, 2016 at 3:17 PM Qi Zhao notifications@github.com wrote:\n\nI am still confused. What do you mean \"stop receiving messages\"?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/734#issuecomment-228436590, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAG_zrNMKJQ45BxzphegTDspt64smohFks5qPC04gaJpZM4I-AmI\n.\n. I believe the issue is that there's some kind of network event happening that is causing the TCP stream to be made invalid, and the error isn't being detected / propogated through the stack. \n\nI had talked about this briefly in the GRPC irc channel and @ejona86 pointed me at https://github.com/grpc/grpc-java/issues/1648 -- which I'm planning on attempting as a fix for now.\n. While looking around the grpc-go code that raises codes.Internal, I noticed that one of them was a RST packet, which can be sent during the header parsing stage on the server. It appears that my metadata containing an empty string as one of the metadata values is causing issues with the grpc server, causing it to send a RST which is causing the empty codes.Internal error.\n. I'd say ideally I should be getting an error somewhere other than the RPC call itself when I'm trying to add an invalid header to the call, but I'm not sure where that should be or go.\n. ",
    "dennwc": "Why not to optimize merged transport? Most likely the optimizations will be done on the client/server initiated streams and not the transport as a whole, as I understood from current code.\n. ",
    "idcmp": "I signed it!\n. ",
    "resouer": "ping @dsymonds , could you please take a look?\n. @menghanl It works now: there's another place crashed instead of my panic(), my bad.\n. ",
    "markdroth": "I think I've figured out how to run the interop tests (using the script from the grpc/grpc repo).  However, the new test is failing with the following error:\n2016/07/01 15:51:43 /TestService/UnaryCall unexpected RPC error message: rpc error: code = 2 desc = test status message\nexit status 1\nFAILED: cloud_to_cloud:go:go_server:status_code_and_message [ret=1, pid=4074]\nFAILED: Some tests failed\nI'm guessing that this is being triggered by my code to return an error from the method handlers on the server side.  I was thinking that would cause the server to return a non-successful status, but it looks like that doesn't work.  What's the right way to have the server return a different status?\n. Actually, nevermind that last comment -- it looks like that error is actually coming from the new test itself, where err.Error() returns more than just the RPC error message.  Is there a way to extract just the RPC error message from the error object?\n. The interop test passes now, at least when run with the Go client and Go server.  I wanted to run against C++ as well, but it looks like this test is not yet implemented for C++.\n. Note that I've changed grpc/grpc#7201 to test this against the C++ implementation, which now passes.\n. Ping?\n. Looks like the travis build is still failing, but I don't see any obvious errors in the log, so I'm not sure what's wrong here.\n. Still waiting for an approval on this.\n. I've also changed the types of the max message size fields from uint32 to uint64.  For documentation and corresponding C-core change, see grpc/grpc#9477.. I notice that this code is still using a Go struct instead of JSON for the service config.  This means that it's not easily extensible -- i.e., it doesn't provide a way for third parties to add new fields to be used in their own interceptors.  I assume that we're going to fix that in a separate PR?. @dfawley Ultimately, yes, those are requirements.  The design was intended to be extensible by third parties.. Note that the http_proxy environment variable is specifically intended to enable use of HTTP CONNECT proxies -- i.e., it's for TCP-level proxies, not HTTP/2-level proxies.\nIf you want to use an HTTP/2-level proxy, then you can probably just point the client directly at the proxy's address.  There is no need to use the http_proxy environment variable.\nFor details on gRPC's TCP-level proxy support, see https://github.com/grpc/proposal/blob/master/A1-http-connect-proxy-support.md.. It's worth noting that this may impact the retry code that @dfawley is working on (and I'm running into a similar problem in C-core).  The peer that we are talking to when we send the first message may not be the one that ultimately successfully handles the request, so if we set the peer when we send the first message and then retry on a different backend later, the peer information we originally set may be incorrect.  We probably need to discuss the right semantics for returning peer information in the context of retries (and the hedging case is even more unclear, since we may be talking to multiple backends simultaneously).. I agree.  In general, I think we should always ignore empty serverlists from the balancer, since that's semantically meaningless.  If the balancer actually wants us to not contact any backends, it should send a serverlist containing a single drop entry.. I take back what I said above.  We've run into an internal case where ignoring empty server lists causes us to fail to go into TRANSIENT_FAILURE and stop sending to the last good backend when that backend goes bad and the balancer sends us an empty serverlist.  So I think the right behavior here is to not ignore empty serverlists.\nI'll fix C-core to do the right thing here.. Doesn't look like I have permission to close this, but it should be fixed by the PR I just merged.. The more I think about this, the less convinced I am that randomization is the right behavior.\nAs we've discussed, there are two cases here:\n1. A new picker is created because one of the subchannels changed its connectivity state into or out of READY.\n2. A new picker is created because the resolver gave us a new list of addresses.\nIn case 2, I think starting at zero is fine.  Starting at a random index could be harmful if there are cases where there are a small rate of very expensive RPCs and a resolver or balancer gave us addresses in a particular order and expected us to follow that order.  Admittedly, that's probably a rare case, so we could probably get away with using a random index in this case, but since starting at zero is probably easier to implement than randomizing, I still think it's the better choice.\nCase 1 is the more interesting case.  Starting at a random index is strictly better than starting over at zero, for the reasons you note in this PR.  However, I still think the right behavior is that we should keep going from the index we were at previously.  Otherwise, this means that any time an individual backend is flapping, we will be constantly skipping randomly throughout the list.  Even worse, all clients will be doing that random skipping, which means that round_robin winds up degenerating into pick_random.. > That doesn't sound that bad, as a degenerate case. Being the literal worst case, degenerating to pick_random... meh. Like, I have trouble caring.\nA task flapping is not that unusual of a case.  I think this is common enough that it's something we should handle gracefully.\n\nTo change that behavior would require a larger change in how subchannels are passed to the picker.\n\nI'm not sure exactly how Go (or Java) are passing the subchannels to the picker, but it's not obvious to me why that has to change.  Presumably, the picker stores the index into its list.  So couldn't we have the client channel let the new picker get some state from the old picker before swapping the new picker in?  That would give us the ability to retain the last index used in the previous picker.\n\nSo it sounds like this PR should still be merged, even if it is expected to be replaced later.\n\nIf this PR only adds randomization for the case 1, not case 2, then I agree that it can go forward, since it's a strict improvement over resetting to 0 every time a subchannel's state changes.  But we should follow up with a better fix in a separate PR.. 1. I think we should start from index 0, and perform the usual RR algorithm from there.  That usual algorithm is basically \"while subchannel[i] is not READY, ++i; return subchannel[i]\".  In other words, we aren't required to start with the first subchannel in the list if it's not the first one to become READY, but we should prefer it if it is.  In particular, I think we should require that once all subchannels are in state READY, we iterate over them in the order in which the addresses were given to RR by the resolver.\n\n\nYes, I think we should absolutely maintain picker state when subchannel states change, for the reasons I described in #2579.  We do not want a single flapping task to cause RR to devolve to pick_random.\n\n\nI do not think this is a requirement.  I think it's fine to reset the index to 0 when we get a new list of addresses from the resolver.. > For (3), if we always start with 0 it seems it is baaaad for a flapping task causing the name resolver to add/remove the entry.\n\n\nThat's a good point.  In a dynamically scheduled environment, a flapping server will cause the resolver to return a new address list.  Actually, this will probably be true even for DNS, since we always re-resolve when one of the subchannels gets disconnected.\nUnfortunately, while I think it's clear that it makes sense to retain the index when a task's state changes, I don't think we can easily do that when we get a new list of addresses from the resolver, since the addresses may not be in the same order or even be the same size.  And if we can't do that when we get a new resolver list, then it doesn't make much sense to do it when individual subchannels' states change, since the two cases are likely to be triggered by the same event.\nIs there anything better we can do here?  If not, maybe randomization is the least-bad answer. :(. > MAX_CONNECTION_AGE and MAX_CONNECTION_IDLE don't make sense in rr\nI don't think that's true.  In fact, you're the one who has advocated for using this approach to allow round_robin clients to discover new backends; see discussion in grpc/grpc#12295. :)\n\nThe most obvious case of \"subchannel state changes\" is during startup. If you have 100 subchannels to rr over, when the 50th connection is made... I still can't prove that random is wrong.\n\nThat's a great example.  I think it's pretty clear that that's the wrong thing to do.  Each time we get a new connection, we should start distributing load evenly across those connections.  Otherwise, round_robin devolves to pick_random.\n\nAlso, startup already has the problem that we'll bombard the first subchannel that comes up, so any nuanced discussions of \"fairness\" are probably not worth our time.\n\nI think there's a difference between saying \"we won't try to be fair at all\" and \"we'll do the best we can based on the set of subchannels that are currently connected\".  The latter seems like a much more reasonable stance.\n\nI think I'm in the same position as before: during subchannel state changes maintaining the previous index is probably nice, but it's unclear whether it would actually matter to anyone.\n\nWhile it may be that many people will not notice, I would be very surprised if this never causes a problem for anyone.. Done.\n. Can't use that here, because I'm using the value of err again on line 473, outside of this if statement.\nBut probably no longer relevant, given the change you suggested below.\n. Done.\n. In this case, we don't want any error, so any failure is a problem.\n. We don't want any error here.\n. Done.\n. Done.\n. Done.\n. Done.\n. Done.\n. Done.\n. Done.\n. Done.  (I'm surprised the compiler didn't catch this.)\n. Looks like the tab got expanded into spaces.  Fixed.\n. Done.\n. We don't.  It's only being called once.  (The call right below this one is to GetResponseParameters(), not GetResponseStatus().)\n. Fixed.\n. I could do that on line 461.  However, I can't seem to make it work on line 463; when I try, the compiler says this:\ninterop/test_utils.go:463: cannot use codes.Unknown (type codes.Code) as type *int32 in field value\nSince I'd like both lines to use the same value, it seemed better to define an int32 and use it in both places.\n. Done.\n. None of the other interop tests seem to use fail-fast, so let's keep things consistent.  (If we want to change all of the tests to use fail-fast, we can do that in a separate PR.)\n. See above.\n. Is this the mechanism by which resolver implementations will return service config data to the grpc client code?  If so, then I think this should return the service config in JSON form, as per previous design discussions.\nAlso, we should have an API for returning the service config in JSON form to the application, for end-to-end debugging purposes.. We should definitely fix this (although it doesn't need to be part of this PR).  We need to support the case where (e.g.) a server name switches from pick_first to grpclb (or vice-versa).. I think this code needs to merge the values from the client API and the service config, as per the descriptions of the individual service config fields.  In particular:\n\nIf the client has explicitly specified wait_for_ready via the client API (as opposed to just letting it use the default), that should override the value from the service config.\nFor the timeout value, we should use the min of the values specified via the client API and the service config.. We should also make sure that we have an API for setting the service config in tests, although I'm guessing that people can do that by specifying a fake service config channel.. We had an email thread about the JSON thing, which you were CC'ed on, so I thought you already knew about it.  To summarize:\n\nThe intent is to provide a common format for the service config that can be used by all grpc implementations.  This will make it easier for teams that use multiple languages to use the same configuration for all of their tests.  (It is also likely that we will wind up using JSON as the encoding for the service config in the OSS service config distribution mechanism that we will be designing in Q1.)\nWe discussed the possibility of making protobuf the common format, but we did not want to require a dependency on proto for teams not using proto as their IDL.  And since it's trivial to convert protos to JSON format, JSON is fairly easy to use even for teams that are using proto as their IDL.\nI suspect that there are other reasons why grpc-go will need a JSON dependency in the future anyway.  For example, C-core uses JSON for several of its credentials implementations.. To be clear, I'm not talking about interprocess communication here.  This is really an API issue, and it has nothing to do with how the code is spread across processes.\nThe desired goal here is that when a team uses multiple languages, they will want to write resolvers and tests in each language, and they should be able to encode the service config in the same form in all languages.  I think that means that we need to use JSON in the primary API that will be used in resolvers and in tests to pass the service config through.  And we should use this same form for the API that returns the service config to the application from the client channel.\nIf there's a way to meet this goal without picking up a JSON dependency in the grpc package, then I'm fine with that.  But that basically requires that whatever external package has the JSON support is always available for teams that use the service config, and that users never directly use the underlying APIs from the grpc package.\nIf you'd like to discuss this further, please throw something on my calendar.  Thanks!. I don't think that choosing JSON as our API prevents users from using whatever format they want externally; it just means that they need to convert it to the form in which we expect the data before they pass it to us.  That's true no matter what form we choose for the API -- whether it be JSON or a custom struct -- so IMHO, this is not a good argument against JSON.\nBTW, another reason for using JSON instead of a custom struct is that JSON allows users to add their own per-method parameters, which they might use in their own interceptor implementations.  A struct is not extensible the way that JSON is.  In the C-core implementation, we allow individual filters to parse the JSON structure for the parameters that they're interested in, which provides a lot of extensibility.\nUltimately, I think that whatever we choose, we are making a decision for the users.  While it's certainly true that not everyone will be happy with whatever we choose, I think we should still make the choice that we think will work best overall.  I think it's important to provide the same format across all gRPC language implementations, and I think JSON is the best choice for that format.\nI understand that this adds a dependency, and I understand that that's not ideal.  But I think it's worth adding a build-time dependency in order to support having the same format in all languages.\nAs I said before, if you want to discuss this further, please throw something on my calendar.  Thanks!. ",
    "heyitsanthony": "@tamird ok will nuke the extra goroutine\n. all fixed up ptal @xiang90 @tamird\n. @tamird test case added PTAL\n. @menghanl I believe @xiang90 wanted to know the justification for that decision, not what the behavior would be. Completely failing if any connection fails on WithBlock() (instead of succeeding if any connection can be established) seems to defeat the purpose of having more than one endpoint as a hedge against downtime. What's the reasoning?\n. @xiang90 Yeah, this isn't necessary; can work around. Closing.\n. @iamqizhao OK, fixed the comment\n. @menghanl are you suggesting putting the timeout in the rpc ctx? That's no better than FailFast. I want to submit a blocking RPC such that if the first selected endpoint is down, then grpc will select a new one. The alternative is wrapping every RPC call in retry logic that, as far as I can tell, belongs in the transport layer. Using the dial timeout for selecting a new endpoint makes sense since waiting for the transport is essentially waiting for a dial to complete.\n. @menghanl @iamqizhao this is my workaround: https://github.com/coreos/etcd/pull/5845/commits/e4946c964c597dcd2c3d7bb7301deed96ae2d696 so that blocking grpc failover will even work using a balancer. It's still incomplete because it needs the wrapper to be plumbed through everything so that the dial timeout is propagated from the client configuration.\nSimilarly, it's still inferior to handling it in grpc because if rpcTime > dialTimeout, it'll always fail even if a connection is available (this is not the case if the timeout is in wait()). Presumably one could inspect the incoming ctx deadline to get the timeout then take the max, but that is still unsatisfactory for the regular blocking / no timeout case because the dial timeout would fail it too early.\nFrom a broader scope, anything that wants to use the raw grpc client interface will also have to know about the timeout/retry wrapper instead of depending on the grpc connection to recognize the endpoint is down and get another address.\nAm I doing something wrong? This workaround is objectively garbage and I'd rather not ruin perfectly good code doing it this way if it's not necessary.\n. > With this change, the real timeout for each RPC becomes min(context-timeout, dial-timeout).\nOK, it looks like all that had to change is getTransport callers need to recognize ErrClientConnTimeout so that they retry on !FailFast. Updated PR to reflect this.\n. @menghanl OK, this passes the etcd balancer failover test that was failing before. Thanks!\n. Any chance these errors could be exported?\n. @iamqizhao OK, thanks!\n. The data race that's showing up:\n```\nWARNING: DATA RACE\nWrite at 0x00c423dbbcf9 by goroutine 683:\n  google.golang.org/grpc/transport.(http2Server).handleData()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/transport/http2_server.go:379 +0x3aa\n  google.golang.org/grpc/transport.(http2Server).HandleStreams()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/transport/http2_server.go:287 +0xa15\n  google.golang.org/grpc.(Server).serveStreams()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/server.go:451 +0x1db\n  google.golang.org/grpc.(Server).serveNewHTTP2Transport()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/server.go:438 +0x4e3\n  google.golang.org/grpc.(*Server).handleRawConn()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/google.golang.org/grpc/server.go:415 +0x5e7\nPrevious read at 0x00c423dbbcf8 by goroutine 422:\n  reflect.typedmemmove()\n      /usr/local/go/src/runtime/mbarrier.go:253 +0x0\n  reflect.packEface()\n      /usr/local/go/src/reflect/value.go:112 +0x11c\n  reflect.valueInterface()\n      /usr/local/go/src/reflect/value.go:949 +0x18c\n  reflect.Value.Interface()\n      /usr/local/go/src/reflect/value.go:919 +0x51\n  fmt.(pp).printValue()\n      /usr/local/go/src/fmt/print.go:694 +0x3aea\n  fmt.(pp).printValue()\n      /usr/local/go/src/fmt/print.go:848 +0x27dc\n  fmt.(pp).printArg()\n      /usr/local/go/src/fmt/print.go:682 +0x1b1\n  fmt.(pp).doPrintf()\n      /usr/local/go/src/fmt/print.go:998 +0x1cad\n  fmt.Sprintf()\n      /usr/local/go/src/fmt/print.go:196 +0x77\n  context.(valueCtx).String()\n      /usr/local/go/src/context/context.go:471 +0x16a\n  fmt.(pp).handleMethods()\n      /usr/local/go/src/fmt/print.go:596 +0x40a\n  fmt.(pp).printArg()\n      /usr/local/go/src/fmt/print.go:679 +0x132\n  fmt.(pp).doPrintf()\n      /usr/local/go/src/fmt/print.go:998 +0x1cad\n  fmt.Sprintf()\n      /usr/local/go/src/fmt/print.go:196 +0x77\n  context.(valueCtx).String()\n      /usr/local/go/src/context/context.go:471 +0x16a\n  fmt.(pp).handleMethods()\n      /usr/local/go/src/fmt/print.go:596 +0x40a\n  fmt.(pp).printArg()\n      /usr/local/go/src/fmt/print.go:679 +0x132\n  fmt.(pp).doPrintf()\n      /usr/local/go/src/fmt/print.go:998 +0x1cad\n  fmt.Sprintf()\n      /usr/local/go/src/fmt/print.go:196 +0x77\n  context.(valueCtx).String()\n      /usr/local/go/src/context/context.go:471 +0x16a\n  fmt.(pp).handleMethods()\n      /usr/local/go/src/fmt/print.go:596 +0x40a\n  fmt.(pp).printArg()\n      /usr/local/go/src/fmt/print.go:679 +0x132\n  fmt.(pp).doPrintf()\n      /usr/local/go/src/fmt/print.go:998 +0x1cad\n  fmt.Sprintf()\n      /usr/local/go/src/fmt/print.go:196 +0x77\n  context.(cancelCtx).String()\n      /usr/local/go/src/context/context.go:332 +0xcc\n  fmt.(pp).handleMethods()\n      /usr/local/go/src/fmt/print.go:596 +0x40a\n  fmt.(pp).printArg()\n      /usr/local/go/src/fmt/print.go:679 +0x132\n  fmt.(pp).doPrintf()\n      /usr/local/go/src/fmt/print.go:998 +0x1cad\n  fmt.Sprintf()\n      /usr/local/go/src/fmt/print.go:196 +0x77\n  github.com/coreos/etcd/clientv3.(watcher).Watch()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/github.com/coreos/etcd/clientv3/watch.go:250 +0x48c\n  github.com/coreos/etcd/clientv3/concurrency.(Election).observe()\n      /home/jenkins/workspace/etcd-proxy/gopath/src/github.com/coreos/etcd/clientv3/concurrency/election.go:178 +0x461\n``. @menghanl OK, all fixed.. @xiang90 it's still used by thegrpc-timeoutheader\n. sure\n. because if the first message overerrCis non-nil,Dial` will fail; the intended behavior of this patch is if any connection succeeds within the timeout, dial will succeed as well\n. It's related to the connection path making sense, but not this particular commit. Reverted.\n. ",
    "barthr": "My excuse, \nthis is the function I was referring to:\n``` Go\nfunc (this *GrpcClient) DialServer(targetUrl string, options grpc.DialOption) error {\n    var err error\nthis.Connection, err = grpc.Dial(targetUrl, options)\n\nif err != nil {\n    return err\n}\n\nreturn nil\n\n}\n```\nThe problem is that when the socket which get's dialed is not available the program just blocks with this error:\ngrpc: addrConn.resetTransport failed to create client transport: \nconnection error: desc = \"transport: dial tcp 127.0.0.1:8080: \ngetsockopt: connection refused\"; Reconnecting to {\"127.0.0.1:8080\" <nil>}\nBut instead I want to send the messages to another server. I was looking into load balancing on the client side but I'm not entirely sure how to implement it\n. It is not truly blocking, it just keeps reconnecting to the socket and my program doesn't allow any other connection than\n``` Go\nfunc main() {\n    ql := client.NewQueueListener(rabbitMQURL, \"crawl\")\nrequestClient = crawler.NewGrpcClient()\n\nif err := requestClient.DialServer(remoteAddr, grpc.WithInsecure()); err != nil {\n    log.Printf(\"DialServer failed because: %v\", err)\n}\n\nbrokerclient, err := requestClient.Register(&clienthandlers.UrlRequestController{})\n\nif err != nil {\n    log.Printf(\"cannot connect: %v\", err)\n}\n\ngo readMessageQueue(ql, brokerclient)\n\ndefer requestClient.Connection.Close()\n\nvar a string\nfmt.Scanln(&a)\n\n}\nfunc readMessageQueue(listener client.QueueListener, brokerClient interface{}) {\n    msgs := listener.ListenForMessages()\n    brokerMsg := reflect.ValueOf(brokerClient).Interface().(brokercontracts.BrokerUrlContractClient)\nfor msg := range msgs {\n\n    if len(msg.Body) == 0 {\n        continue\n    }\n\n    log.Printf(\"Received message: %s\\n\", msg.Body)\n\n    messageToBeSend := &brokercontracts.UrlRequest{\n        Url: string(msg.Body),\n    }\nmessage:\n    r, err := brokerMsg.CrawlWebsiteRequest(context.Background(), messageToBeSend)\n\n    if err != nil {\n        log.Printf(\"Could not send message: %v\", err)\n        log.Println(\"Starting server discovery.....\")\n        serverDiscovery()\n              goto message\n    }\n    // When there is a message from the RabbitMQ queue\n    log.Println(\"Message send to Remote\")\n\n    log.Println(r.Content)\n\n}\n\n}\nfunc serverDiscovery() {\n    for _, endpoint := range knownServers {\n        err := requestClient.DialServer(endpoint, grpc.WithInsecure())\n        if err == nil {\n            return\n        }\n    }\n}\n```\n*GrpcClient is just a wrapper to some of the methods from the grpc library, nothing special just adding handlers and such\n. That seems logical but by using this the underlying connection will keep trying to connect to All the sockets, and produce error messages\n. I solved it with using context.WithTimeout, the generated function from grpc was blocking because the rpc was not avalaible\n. ",
    "solher": "Jeez you're totally right, sorry about that...\n. ",
    "sleaze": "Also see:\n\nhttps://github.com/golang/protobuf/issues/156\nhttps://github.com/google/protobuf/issues/1321. \n",
    "runeaune": "This is biting me too; and I'm seeing 20-30x higher throughput when increasing defaultWindowSize. Could this be put in ConnectOptions? It seems that mismatch between client and server has the potential to cause breakage, do you have any ideas how to avoid that other than making sure they have it set to the same?\n. Mostly large workloads (can be several MB in each RPC). The benefit of increased window increases with the size of the workload. And obviously with increasing latency.\n. Some cursory testing on the current master version seems to indicate that the problem has been solved, even without setting a larger initial window size (my tests can't detect a difference). Could it be that there was an issue with the automatic adjustment of the window in the older version (the setting has initial in its name for a reason I presume)?\nThat said, my test isn't directed specifically at GRPC, it just happens to send large payloads over high-latency links.. ",
    "gyuho": "https://github.com/coreos/etcd/commit/07ed4da2ffe871444c83be63e508d2adf4c87175 with 95K is the direct parent commit of https://github.com/coreos/etcd/commit/d9a8a326dfe4691cf90d2c4d2674e3602de812d8 with 48K. So I think etcd doesn't have any other change.\n. @menghanl Is there any instruction to run grpc benchmark? By the way, we are developing in Google Cloud Platform as well.\nThanks.\n. Ok, will try that. Thanks!\n. Confirmed that this fixes the issue in etcd https://github.com/coreos/etcd/issues/6221.\n/cc @iamqizhao @menghanl \nThanks!\n. @edrex Can you try with https://github.com/grpc/grpc-go/pull/1546?. > Does this happen when the user application calls Close() on client conn and doesn't cancel the the stream's context?\nYes.\nIf I understand your question correctly, we get the server stream context first, and launch the Recv call on the stream [1]. And since the Recv call blocks, the cancel function never gets called, while we still close the client connection when terminating the cluster.\ngo\nfunc (ep *electionProxy) Observe(req *v3electionpb.LeaderRequest, s v3electionpb.Election_ObserveServer) error {\n    conn := ep.client.ActiveConnection()\n    ctx, cancel := context.WithCancel(s.Context())\n    _ = cancel\n    sc, err := v3electionpb.NewElectionClient(conn).Observe(ctx, req)\n    if err != nil {\n        return err\n    }\n    for {\n                // blocks here...\n        rr, err := sc.Recv()\n                ...\n\n[1] https://github.com/coreos/etcd/blob/master/proxy/grpcproxy/election.go#L46-L53. @MakMukhi \nCould we have another release with this patch?\nJust cut from master branch would work for us.\nWe will wait if there are other pending PRs.\nThanks!. @menghanl Thanks!. Can we do another release with this? Thanks!. We will see if we can temporarily mask timed-out endpoint in application layer.\n. Reproducible case:\n```go\nfunc TestHandlerTransport_HandleStreams_MultiWriteStatus(t testing.T) {\n    st := newHandleStreamTest(t)\n    handleStream := func(s Stream) {\n        if want := \"/service/foo.bar\"; s.method != want {\n            t.Errorf(\"stream method = %q; want %q\", s.method, want)\n        }\n        st.bodyw.Close() // no body\n    var wg sync.WaitGroup\n    wg.Add(5)\n    for i := 0; i < 5; i++ {\n        go func() {\n            defer wg.Done()\n            st.ht.WriteStatus(s, status.New(codes.OK, \"\"))\n        }()\n    }\n    wg.Wait()\n}\nst.ht.HandleStreams(\n    func(s *Stream) { go handleStream(s) },\n    func(ctx context.Context, method string) context.Context { return ctx },\n)\n\n}\n```\nWe are using grpc-go v1.6.0.\nThanks!. Could anybody review this, please? /cc @menghanl @MakMukhi @dfawley \nWe are seeing the panics on this code path pretty often when closing down grpc-server with inflight streams.\n```go\ntype ServerTransport interface {\n    ...\n// WriteStatus sends the status of a stream to the client.  WriteStatus is\n// the final call made on a stream and always occurs.\nWriteStatus(s *Stream, st *status.Status) error\n\n...\n\n```\nI believe serverHandlerTransport.streamDone check is there to prevent redundant error writes from SendMsg. But it is possible that following SendMsg calls on the same stream error and WriteStatus before serverHandlerTransport.do completes, thus panic-ing on closing of closed ht.writes channel.\nThis patch just marks the stream as done before calling serverHandlerTransport.do, so subsequent error-ing SendMsg calls do not trigger panic on WriteStatus.. @menghanl PTAL.\nAnd could we have patch release v1.6.1 with this?\nThanks a lot!. @menghanl Sounds good. Thanks!. @menghanl Just rebased with current master. Thanks!. Note: currently we are using grpc v1.6.0. And plan to upgrade to v.1.7.0.. e.g. https://github.com/grpc/grpc-go/blob/v1.7.x/call.go#L226-L240. @menghanl Are we getting this fix for v1.7.2 or v1.8?\nThanks!. Turns out our server was returning status.Error(codes.Unknown, context.DeadlineExceeded).\nWe fixed our server to return original context error and let gRPC server handle the error conversion.\nThanks!. > It's called with \"grpc: the connection is closing\" though.\nRight. We are also seeing that.\nBut our current balancer logic was built, based on v1.6.0 logic.\nSo, we were expecting to see errConnDrain message first.\nAnd then, grpc: the connection is closing.\nWas there any reason why errConnDrain was removed and not trigger down any more in v1.7.x?. > What do you mean see errConnDrain and then the connection is closing?\nv1.6.0 has this in lbWatcher:\nhttps://github.com/grpc/grpc-go/blob/84671c5e11bfb5b7c237ff1799a660ff11a518d2/clientconn.go#L651-L653\nSo when we delete connections, v1.6.0 tearDown calls down function, so custom balancer knows that this connection has been drained.\nv1.7.2 has this in lbWatcher:\nhttps://github.com/grpc/grpc-go/blob/57ebb0fa1c896a23101f49b61dd887313fbcd055/balancer_v1_wrapper.go#L216-L218\nAnd RemoveSubConn calls removeAddrConn and tearDown\nhttps://github.com/grpc/grpc-go/blob/57ebb0fa1c896a23101f49b61dd887313fbcd055/balancer_conn_wrappers.go#L178-L185\nhttps://github.com/grpc/grpc-go/blob/57ebb0fa1c896a23101f49b61dd887313fbcd055/clientconn.go#L646-L655\nBut v1.7.2 tearDown does not call down as I linked above.\nSo, I meant to say tearDown(errConnDrain) is called in both versions, but not the down function due to change in tearDown method.. > The HandleSubConnStateChange function in v1 balancer wrapper calls the down function whenever the SubConn's state changes from Ready to non-Ready.\nThen down(errConnClosing) is only called in such case, as\nhttps://github.com/grpc/grpc-go/blob/57ebb0fa1c896a23101f49b61dd887313fbcd055/balancer_v1_wrapper.go#L236-L242\nThis is different than how v1.6.0 handles drained connections, right?\nv1.6.0 calls directly down(errConnDrain) when removing a connection in lbWatcher.\nPlease correct me if I am wrong.\nThanks!. @menghanl I will see if our client can internally handle this.\nThanks!. Would this be released with v1.7.3 this week? Thanks!. @menghanl Could please you review this? This is affecting etcd production users. Thanks!. @menghanl \n(ht *serverHandlerTransport) Write call after (ht *serverHandlerTransport) WriteStatus on the same *serverHandlerTransport is possible when:\n\n(ss *serverStream==0x123) SendMsg exits from some kind of error https://github.com/grpc/grpc-go/blob/a62701e4aa1d276bec70311251d62a478404d63f/stream.go#L658-L667 Note: Currently, etcd does not log these errors from (x *watchWatchServer) Send (watch is an etcd stream API) yet, so we are not sure what the error was.\n(ss *serverStream==0x123) SendMsg's defer calls ss.t.WriteStatus https://github.com/grpc/grpc-go/blob/a62701e4aa1d276bec70311251d62a478404d63f/stream.go#L649-L652\nThe error from (ss *serverStream==0x123) SendMsg will be returned to etcd server. We expect server be rejecting requests to this server stream, right away. However, before the server stream is closed and before (s *grpc.Server) serveStreams calls transport.ServerTransport.Close, concurrent client writes call (ss *serverStream==0x123) SendMsg.\nThis time, no error and proceed to ss.t.Write https://github.com/grpc/grpc-go/blob/a62701e4aa1d276bec70311251d62a478404d63f/stream.go#L658-L667\nAt the moment, ss.t.WriteStatus had already been called in gRPC-side but transport.ServerTransport.Close has not been called. Now, ss.t.Write triggers panic: send on closed channel here https://github.com/grpc/grpc-go/blob/a62701e4aa1d276bec70311251d62a478404d63f/transport/handler_server.go#L166-L171\n\nServer stream is done but underlying stream layer in server application is not closed(or stopped) yet. Then server stream keeps receiving client writes, until server application rejects them.\netcd minimizes this small time window, rejecting requests when error is returned from SendMsg. But, there's still a chance that one request slips through (and crashes the whole server). In application layer, there's no easy to stop all client requests atomically, when SendMsg fails.\nI think gRPC should handle it by ignoring Writes after WriteStatus.\n. @menghanl \nOk, I double-checked our code and confirm that we serialize all Writes to the gRPC stream, so etcd's doing the right thing. There's no other Send caller to the same stream. But there's concurrent caller to Recv.\nhttps://github.com/grpc/grpc-go/blob/a62701e4aa1d276bec70311251d62a478404d63f/stream.go#L63-L74\nBasically, we are doing this:\n\nIt's safe to have a goroutine calling SendMsg and another goroutine calling recvMsg on the same stream at the same time. (reference: https://github.com/coreos/etcd/blob/b21180d198f6c87d711980686af388f71c665126/etcdserver/api/v3rpc/watch.go#L131-L144)\n\nNow it's possible that either WriteStatus in SendMsg or RecvMsg gets called first:\nhttps://github.com/grpc/grpc-go/blob/a62701e4aa1d276bec70311251d62a478404d63f/stream.go#L649-L652\nhttps://github.com/grpc/grpc-go/blob/a62701e4aa1d276bec70311251d62a478404d63f/stream.go#L689-L691\nFollowed by Write in the other goroutine. In this case, WriteStatus and subsequent Write call happen on the same *serverHandlerTransport.\nWhat do you think?. @menghanl @dfawley \nPTAL; https://github.com/grpc/grpc-go/pull/1687#discussion_r153988332 is addressed.\nThanks.. @dfawley \n\nWriteStatus is guaranteed to only execute once\nWe still need to do something similar for our server implementation in http2_server.go for the same kind of scenario.\n\nYeah, I will add similar locks around http2_server.go in a separate PR, unless someone is working on it.\nThanks!. @menghanl Can this be backported with v1.7.x release, sometime next week?\nWe've tried this with v1.8.x and tip, but both fail lots of etcd tests--we will eventually update to v1.8 or v1.9+, in the next few weeks.\n. Awesome. Thanks for the quick release!. @menghanl Just saw this comment. Sorry for delay. I will double-check if this happens with gRPC head, shortly.. The gRPC side leak seems to be gone with current grpc-go master 8a8ac82f1f7f141a47430dd8be7aa90b92141603. Will revisit when that happens again. Thanks!. @aequitas Can you file an issue to etcd?\nI was planning to debug this as well, while testing etcd with unix sockets.\nThanks.. @menghanl We want to see Warning/Error/Fatal, but opt out on Info (only when our --debug flag is off). So grpclog.NewLoggerV2WithVerbosity(ioutil.Discard, os.Stderr, os.Stderr, 0) would totally work.\nFeel free to close this.\nThanks!. @menghanl Understood. Manual error parsing in client-side works for us.\nPlease feel free to close this.\nThanks.. @menghanl We implement our custom grpc.Balancer interface passing multiple endpoints on Notify for disconnect failover. And get the initial *grpc.ClientConn with grpc.DialContext(ctx,host,grpc.WithBalancer(b)).\nMy question is whether the connection *grpc.ClientConn returned from grpc.DialContext needs to be closed when pinned address is down, or let gRPC handle connection switch with balancer. I mean connection switch by switching to other endpoints with Notify and Up.\nWe are asking because v1 balancer endpoint switch does not seem to close the previous connection (https://github.com/coreos/etcd/issues/9212) for our use case. We never close *grpc.ClientConn manually when endpoint switch happens in gRPC side.\nThanks.\n. Makes sense.\n\nDo you have pending RPCs on the old connection that you are trying to close? The connection will be kept open until all RPCs finish.\n\nWe will double-check on etcd side, and get back to you shortly.\nThanks!. @dfawley Was busy with new etcd release with gRPC v1.7.5.\nAnd we are rewriting our client balancer with new gRPC balancer interface, anyway.\nSo, we will double-check on this with new balancer implementation (next 2~3 months).\nThanks!. @menghanl @xiang90 This looks good to me.\nThanks for clarification.\nWe will make sure we handle this in our tests.. @menghanl ack. thanks!. Just to add more of our etcd use cases: we may call balancer.Register concurrently in our balancer unit tests to test different set of pickers, and if multiple tests run on a same balancer picker, we randomize balancer names to avoid balancer register conflicts, but racey writes/reads to grpc/grpc-go/balancer.m can still happen as described in this issue. It would be great if the shared map is protected in gRPC by default.\nThanks!. With that change, I was able to drop grpc.WithTimeout in etcd/clientv3.\n```diff\ndiff --git a/clientv3/client.go b/clientv3/client.go\nindex f9c725fd8..ef15a8dd2 100644\n--- a/clientv3/client.go\n+++ b/clientv3/client.go\n@@ -238,9 +238,6 @@ func (c *Client) dialSetupOpts(target string, dopts ...grpc.DialOption) (opts []\n        return nil, fmt.Errorf(\"unable to parse target: %v\", err)\n    }\n\nif c.cfg.DialTimeout > 0 {\nopts = []grpc.DialOption{grpc.WithTimeout(c.cfg.DialTimeout)}\n}\n    if c.cfg.DialKeepAliveTime > 0 {\n        params := keepalive.ClientParameters{\n            Time:    c.cfg.DialKeepAliveTime,\n@@ -266,10 +263,13 @@ func (c *Client) dialSetupOpts(target string, dopts ...grpc.DialOption) (opts []\n        default:\n        }\n        dialer := &net.Dialer{Timeout: t}\nconn, err := dialer.DialContext(c.ctx, proto, host)\ndctx, cancel := c.getDialCtxCancel()\nconn, err := dialer.DialContext(dctx, proto, host)\ncancel()\n        if err != nil {\n            select {\n            case c.dialerrc <- err:\n// TODO: handle this!\n            default:\n            }\n        }\n@@ -367,13 +367,33 @@ func (c Client) dial(endpoint string, dopts ...grpc.DialOption) (grpc.ClientCo\n    // Is it safe/sane to use the provided endpoint here?\n    //host := getHost(endpoint)\n    //conn, err := grpc.DialContext(c.ctx, host, opts...)\nconn, err := grpc.DialContext(c.ctx, endpoint, opts...)\ndctx, cancel := c.getDialCtxCancel()\nconn, err := grpc.DialContext(dctx, endpoint, opts...)\ncancel()\n    if err != nil {\n        return nil, err\n    }\n    return conn, nil\n }\n\n+func (c *Client) getDialCtxCancel() (ctx context.Context, cancel context.CancelFunc) {\n+   ctx, cancel = c.ctx, func() {}\n+   if c.cfg.DialTimeout > 0 {\n+       // if root context has no deadline, or its deadline expires\n+       // later than dial timeouts, set up sub-context with dial timeout\n+       deadline, ok := c.ctx.Deadline()\n+       untilDeadline := time.Until(deadline)\n+       timeout := untilDeadline\n+       if timeout < 0 {\n+           timeout = c.cfg.DialTimeout\n+       }\n+       if !ok || (untilDeadline > 0 && untilDeadline > c.cfg.DialTimeout) {\n+           ctx, cancel = context.WithTimeout(c.ctx, timeout)\n+       }\n+   }\n+   return ctx, cancel\n+}\n+\n // WithRequireLeader requires client requests to only succeed\n // when the cluster has a leader.\n func WithRequireLeader(ctx context.Context) context.Context {\ndiff --git a/clientv3/client_test.go b/clientv3/client_test.go\nindex d09e64330..68b435313 100644\n--- a/clientv3/client_test.go\n+++ b/clientv3/client_test.go\n@@ -23,6 +23,7 @@ import (\n\"github.com/coreos/etcd/etcdserver/api/v3rpc/rpctypes\"\n\"github.com/coreos/etcd/pkg/testutil\"\n\n\n\"google.golang.org/grpc\"\n )\n\nfunc TestDialCancel(t testing.T) {\n@@ -84,6 +85,7 @@ func TestDialTimeout(t testing.T) {\n        {\n            Endpoints:   []string{\"http://254.0.0.1:12345\"},\n            DialTimeout: 2 * time.Second,\n+           DialOptions: []grpc.DialOption{grpc.WithBlock()},\n        },\n        {\n            Endpoints:   []string{\"http://254.0.0.1:12345\"},\n```\nReproducible tests in https://github.com/coreos/etcd/blob/0a92ba66fa3b610e8806df15f2eddc8a5c745b81/clientv3/client_test.go#L80-L94.. @menghanl Will try and let you know if it would be enough for us tomorrow. Thanks!. @menghanl @jpbetz Got it to work, as you suggested (ref. https://github.com/gyuho/etcd/pull/8).\nThanks!. @menghanl @ericchiang I believe this happens when etcd client manually triggers down(errConnClosing) on unused connections (from old balancer interface in v1.7). We manually close connections after pinning the first connection out of multiple endpoints. We are upgrading gRPC with balancer rewrite. In our new implementation, we are getting rid of such manual connection close.. Can we release v1.12.1 with this cherry-picked? Thanks!. @menghanl No problem! And thanks a lot!. @jpbetz Yeah, will try that patch as well. Thanks.. @jpbetz Confirmed that https://github.com/grpc/grpc-go/commit/8f06f82ca394b1ac837d4b0c0cfa07188b0e9dee fixes the issue. Thanks.\ndiff\ndiff --git a/Gopkg.lock b/Gopkg.lock\nindex f20aa7d90..0387ef830 100644\n--- a/Gopkg.lock\n+++ b/Gopkg.lock\n@@ -323,6 +323,7 @@\n     \"balancer\",\n     \"balancer/base\",\n     \"balancer/roundrobin\",\n+    \"channelz\",\n     \"codes\",\n     \"connectivity\",\n     \"credentials\",\n@@ -345,8 +346,7 @@\n     \"tap\",\n     \"transport\"\n   ]\n-  revision = \"d11072e7ca9811b1100b80ca0269ac831f06d024\"\n-  version = \"v1.11.3\"\n+  revision = \"1fa3750c9a209f5efbdc2722d40f2657885af5e5\"\nWill try again v1.12 once the patch release is out.. @dfawley I wasn't clear enough.\nOur main use case for keepalive ping is to detect unresponsive \"server\" from \"client\" side for long-running stream RPCs. Keepalive is used for detecting disconnects in transport layer, but no way to extend it to support application-layer conditions (as mentioned above, partitioned node keepalive server handler will still receive keepalive pings from server, thus marked as active and making client stream be stuck with the partitioned node).\n\nIs there a reason you want to perform this check when you get keepalive pings as opposed to alternatives (e.g. polling periodically)?\n\nWe want to use keepalive since it's light (8-byte HTTP/2 ping) and built-in to gRPC server. We have a similar use case in gRPC server interceptor layer where every RPC goes through leader-check to see if there's an active leader or not. I would imagine we could do the same thing for keepalive handler.\nWe haven't tried https://godoc.org/google.golang.org/grpc/credentials#TransportCredentials but seems like this is for connection handshake?\nThanks!. @dfawley \n\nIf you were to kill the connection, how would the client know which node to reconnect to?\n\nOnce disconnected, etcd clientv3 balancer picker policy will either round robin or switch other nodes based on latest node health status to retry the requests. We just need some indication that this node is not responding.\n\nCould you perform this check in a server interceptor and kill the watch stream (instead of the connection itself) when the partitioning happens?\n\nYes, and we support that as an option (user can specify require-leader metadata in their contexts).\n\nInstead, if you really wanted connection-level control, the Listener you give to gRPC could be wrapped in something that wraps the Conns it hands out that do checks like this.\n\nI see. We will look into this.\nWas just curious if gRPC team has any plan to support the custom keepalive handler. If you recommend server interceptor pattern, we are also happy with it for now.\nThanks for response! And please feel free to close it if there's no plan to support the custom keepalive handler in the near future. We will revisit when we find other use cases.. I am not sure if this is the right fix, but at least this was the workaround that resolved the issue for us.. > In that case, how did the ClientConn get a different IP later?\nWe have a base balancer and picker to implement balancer.Balancer interface. And picker gets multiple endpoints, but all dial logic happens in gRPC? All we do is pass the dialer with grpc.WithBalancerName. Agree. Let me rework on the test with original change. Thanks.. All fixed.\nI can confirm this test fails without the patch.\nOur production blocking issue is fixed by this as well.\nThanks!. Q. Why is this not protected via bw.mu as in line 187-188?. Np. Thanks!. Aren't WriteStatus calls serialized in gRPC side? WriteStatus is called once, and subsequent calls would exit on ht.streamDone==true. Not sure how this triggers panics in WriteStatus.. ",
    "danp": "Fixed by https://github.com/grpc/grpc-go/pull/787\n. How about adding http2Server.readFrame which calls framer.readFrame, logs the error if it's not EOF, then returns it? The two current calls to framer.readFrame can be replaced with that which will collapse this a bit.\n. ",
    "cube2222": "After writing it down it seems to me that this would be inconsistent, as the rest of the logging in this file is also done in such top-level handlers.\n. I signed it!\n. In both cases it's appropriate, as the problem with this code is that it shouldn't log the error when it's EOF or unexpected EOF, because those are expected and will happen on proper requests. That's why we return early, before it gets to logging.\n. ",
    "nizsheanez": "Hello. Do you have any update on it? Can we help?. @Ch3ck, please, come back and say what was wrong :-). I also did miss-configuration in docker compose - used wrong hostname of target service. \nThanks. \nAnyway, connection is unavailable is not friendly error - in tons of microservices world need to know \"which host/port is unavailable?\". \nAnd it's even more important with Go - where we don't have stack traces. \nAnyway, thanks, everybody. . ",
    "wenbozhu": "@mwitkow  Not quite, grpc-web addresses the wire encoding of grpc frames, not the encoding of the payload of grpc frames.. ",
    "atishpatel": "Does this imply it's possible to do grpc with golang with an App Engine Flexible env? If so, does anyone have a link to a working project with grpc and App Engine Flexible? \n. For anyone looking for a workaround to do grpc on appengine until app engine adds http2: https://github.com/salrashid123/gcegrpc \n. ",
    "tjerkw": "A function that basically maps an http1 Request to an http2 Request and an http1 ResponseWriter to an http2 ResponseWriter would solve it for me.\nfunc ServeHTTP(w ResponseWriter, r *Request) {\n    grpcServer.serveHttp(toHttp2ResponseWriter(w), toHttp2Request(r));\n}\n\nNote: I don\"t need streaming support. I just want to have faster development by using the grpc IDLS. . ",
    "arystan-sw": "A suggested fix from golang-nuts:\n\nDelete the source from your $GOPATH and run go get again. There are other ways to fix the error but this is a simple solution. \n\nhttps://groups.google.com/forum/#!topic/Golang-nuts/t9dLLVwsMlM. ",
    "alexmullins": "I also think that the raw connection is leaked (never Closed()) in the case that ctx.Done() is closed while inside the tlsCreds.ClientHandshake(). This change fixes that.\nI could be wrong on this though. \n. Cleaned up the conn.Close() in tlsCred.ServerHandshake() too.\n. @tamird Is there an issue# over at cockroachdb that I could look at? Just curious.\n. ",
    "dup2X": "Less objects and less gc-latency. What is the reason for not using sync.Pool ? Thanks for your explanation.\n. @hsaliak Thanks a lot.\n@apolcyn I saw the same to you. \nAnd the field of pb_struct is ptr, that maybe incr gc...\n. ",
    "petermattis": "Yes, I did encounter a problem which motivated this. I have a scenario in which I wish to register a service on a server after initiating a connection to a server. If this isn't an allowed scenario, Server.register should disallow it.\n. Ah, I missed the comment above Server.RegisterService. Do you want me to add the protection against calling RegisterService after Server.Serve has been called or just drop this PR?\n. @menghanl This is what I was thinking of. Currently difficult to test this as there is no way to temporarily swap out the grpclog.logger for a single test. \n. @menghanl Rebased. . There is an asymmetry between http2Server.closeStream and http2Client.CloseStream. The former invokescancel while the latter doesn't. Is this fix as simple as adding a call to s.cancel() to http2Client.CloseStream?\n. CockroachDB just stumbled upon this same issue. GRPC has severe bandwidth limitations on high latency networks. The scenario I encountered was sending ~60MB of data between two servers with a 60ms RTT time (using streaming RPC to send the data in 1MB chunks). I naively expected this to take a few seconds as nc-based tests were able to demonstrate 40MB/s. Instead, the transfer took ~60s, just as you would expect with the fixed 64KB stream window size. \n\niii) public surface to allow user to tune flow control parameters and enforce a server-scoped flow control limit (under design).\n\nIs there any progress I can follow or help I can give? Perhaps a short term unsupported solution which allows configuration of initialWindowSize and initialConnWindowSize on a per-server/client basis. . > Maybe some temporary API options to client conns and grpc servers is an option? \nSomething like: https://github.com/grpc/grpc-go/compare/master...petermattis:pmattis/configurable-window-size?expand=1?. @apolcyn I'm on vacation for the next few days. I can spiff up that branch for a PR when I get back if nobody beats me to it.. > @apolcyn @petermattis Any chance of getting that DialOption API (for setting intial and conn windows sizes) merged?\nThis slipped off my plate. I'll try to get back to it this week, though I'm more than happy for someone else to pick it up and run with it. I think the main thing to finish off is the addition of a few tests and documentation comments.\nPS Changing defaultWindowSize violates the HTTP2 spec. You should really be changing initialWindowSize and initialConnWindowSize. . Related, I noticed today that the gRPC flushing behavior when responding to a unary RPC creates 3 packets where it seems only 1 is necessary. These 3 packets are created by 3 separate flushes in the grpc.Server.processUnaryRPC path:\n\nhttp2Server.Write calls http2Server.writeHeaders which always forces a flush after writing the header.\nhttp2Server.Write specifies forceFlush=true on the last call to writeData and it also directly calls flushWrite if this is the last writer.\nServer.processUnaryRPC calls http2Server.WriteStatus which calls writeHeaders and forces a flush.\n\nGiven that a unary RPC will always call WriteStatus, can we eliminate the forced flushes in http2Server.Write? I hacked something up to do this, and it reduced 99%-tile latencies in a simple test harness from 1.9ms to 1.1ms and 95%-tile latencies from 1.3ms to 0.9ms. . @MakMukhi Yep, go for it.. @dfawley I just re-tested this and the slowdown I see is 45%. Flipped around, not decoding headers provides an 84% improvement in throughput. I haven't looked at all at what about header decoding is causing this problem.\nThe only difference between the two tests (using pinger) is:\ndiff\ndiff --git a/y.go b/y.go\nindex 1bfb580..b2ab044 100644\n--- a/y.go\n+++ b/y.go\n@@ -36,7 +36,7 @@ func newYServer(conn net.Conn) *yServerConn {\n        y.wr = bufio.NewWriter(conn)\n        y.fr = http2.NewFramer(y.wr, y.rd)\n        y.fr.SetReuseFrames()\n-       y.fr.ReadMetaHeaders = hpack.NewDecoder(4096, nil)\n+       // y.fr.ReadMetaHeaders = hpack.NewDecoder(4096, nil)\n        y.sender.cond.L = &y.sender.Mutex\n        return y\n }\n@@ -185,7 +185,7 @@ func newYClient(conn net.Conn) *yClientConn {\n        y.wr = bufio.NewWriter(conn)\n        y.fr = http2.NewFramer(y.wr, y.rd)\n        y.fr.SetReuseFrames()\n-       y.fr.ReadMetaHeaders = hpack.NewDecoder(4096, nil)\n+       // y.fr.ReadMetaHeaders = hpack.NewDecoder(4096, nil)\n        y.sender.cond.L = &y.sender.Mutex\n        y.receiver.streamID = 1\n        y.receiver.pending = make(map[uint32]*yPending)\nWith header decoding:\n~ ./pinger -c 200 -n 1 -p 1000 -d 10s -t y localhost:50051\n...\n_elapsed____ops/s_____MB/s__p50(ms)__p95(ms)__p99(ms)_pMax(ms)\n     10s 232403.8     89.3      6.3      5.8      6.6      9.4\nWithout header decoding:\n~ ./pinger -c 200 -n 1 -p 1000 -d 10s -t y localhost:50051\n...\n_elapsed____ops/s_____MB/s__p50(ms)__p95(ms)__p99(ms)_pMax(ms)\n     10s 427379.2    164.3      2.2      3.4      4.1      8.9. Ok, I took a quick look and it appears that that ~50% of the performance delta is due to allocations of the MetaHeadersFrame and MetaHeadersFrame.Fields. It looks feasible to reuse the MetaHeadersFrame similarly to how DataFrames are reused. Ditto for MetaHeadersFrame.Fields. Unfortunately, those changes don't move the performance needle at all for gRPC.. > > ~50% of the performance delta is due to allocations of the MetaHeadersFrame and MetaHeadersFrame.Fields\n\nHow are you getting that number?\n\nI added caching of MetaHeadersFrame and MetaHeadersFrame.Fields to golang.org/x/net/http2.Framer. That eliminated ~50% of the performance delta. A bit more can be achieved by some restructuring of Framer.readMetaFrame. Right now it performs a number of small allocations so that variables can be captured by the emit function. \nConcretely (and these numbers differ from the ones above because they were gathered between 2 VMs in GCE):\n```\nold-decode-headers (the current golang.org/x/net/http2 code)\n_elapsed_ops/s__MB/s__p50(ms)__p95(ms)__p99(ms)_pMax(ms)\n     10s 168991.7     64.9      7.3      7.9      8.4     10.0\nnew-decode-headers (the changes mentioned above)\n_elapsed_ops/s__MB/s__p50(ms)__p95(ms)__p99(ms)_pMax(ms)\n     10s 256298.9     98.5      4.5      5.2      5.8     12.6\nno-decode-headers (send headers, but don't set Framer.ReadMetaHeaders)\n_elapsed_ops/s__MB/s__p50(ms)__p95(ms)__p99(ms)_pMax(ms)\n     10s 336653.7    129.4      2.6      4.2      5.0      7.1\n```\n\n\nUnfortunately, those changes don't move the performance needle at all for gRPC.\n\nDid you make a prototype change already?\n\nYes. . > @petermattis Are those latest numbers with gRPC or your x/y implementations?\nThese numbers were with my y protocol implementation (which uses http2 framing).\n\n\nUnfortunately, those changes don't move the performance needle at all for gRPC.\n\nSo you're saying optimizations in header decoding doesn't help gRPC at all? If so, that probably means the bottleneck is in the sending path right now.\n\nYes, these optimizations do not help gRPC at all right now. I would guess that the gRPC bottleneck is somewhere in the sending path, though it is also possible something is happening on the receiving path. \n\nMaybe we can turn this into an issue/PR against http2. Reusing header frames seems like a useful feature for everyone.\n\nSounds good to me.. Nice! I was wondering if avoiding the channel operations for quota acquisition was possible. . No, it has to be a distinct type. See peer/peer.go and metadata/metadata.go for other examples of doing this in the grpc code base.\n. Should cancel be a member of ConnectOptions?\n. A larger window size means more memory that can be buffered by gRPC without being acknowledged by the application. In addition to the extra memory usage, you've likely hurt latency for small requests sharing the same connection. \nThis change increases the stream and connection window sizes by 2048x for only a 5.5x throughput improvement. I think there is a less radical change that can provide most of the benefit. In my testing with CockroachDB, bumping the stream and connection window sizes to 2MB (increase of 32x) was the minimum size that provided close to max throughput on a link with 60ms RTTs. As a stop-gap until the window sizes dynamically adjust, I'd be much more comfortable with a change which left the defaults unchanged but made the stream and window sizes configurable.. > How would latency for small requests sharing the same connection be hurt by a bigger window; I don't see how their frames would be delayed since they are likely to be sent and reach the receiver sooner.\nAh, ignore me. I misremembered how framing when writing data to a connection works. See http2Client.Write. Data is interleaved in 16 KB chunks regardless of the stream/connection window sizes.\n. s/on same/on the same/g. Ditto (on the same).. I agree. It seems like the calls to adjustNumWriter and the associated flushing are at too low a level. For unary RPCs, the number of writers should be incremented before calling sendResponse and decremented after the WriteStatus calls. The flushing logic currently in sendResponse is a bit difficult to reason about given the interaction with flow control and it isn't obvious to me how to pull it to a higher level. Perhaps this is the subtle bug you're referring to. \nI'm happy to leave this work in your hands. Consider this PR more an indication of the importance of this work. The unnecessary flushes introduce a significant amount of latency!. @apolcyn I agree that this could lead to deadlock. Perhaps it would be sufficient to flush the buffered data whenever Write needs to loop. Also, I'm curious what the forceFlush logic is doing given that there is another check a few lines below to flush the data if we were the last writer. \n. ",
    "spencerkimball": "I signed the CLA.\n. What's up with the CI? There's no apparent cause to the failure.\n. @iamqizhao what's the process for merging?\n. Changed.\n. Calling cancel() asynchronously seemed like a good idea. I could call cancel immediately, but it seemed worthwhile to give DialWithContext() an execution pathway to success which we verify never occurs.\n. Simplified.\n. Changed.\n. ",
    "shangsony": "this is build error\n. in vendor dir build\n. google.golang.org Can not visit in China @therealplato \n. ",
    "therealplato": "i ran into this because I updated only the grpc package and not the children packages. try govendor fetch google.golang.org/grpc/...\n. @menghanl This blocked permanently (or at least, much longer than I was comfortable with in a test suite.) I've still got some test failures but they look unrelated to this issue. I ended up with:\nclientConn, err := grpc.Dial(\n               \"\",\n               grpc.WithInsecure(),\n               grpc.WithBlock(),\n               grpc.WithTimeout(30*time.Second),\n       )\n. ",
    "dmvk": "From method contract:\nIt should return an error if and only if Watcher cannot recover.\nCan you please give an example of such error? Only relevant thing I can come up with is unreachable DNS, which is still recoverable. \nOther thing is that error is used for flow control. When you close the watcher, it is the only way to exit from the goroutine executing Next method.\nThanks,\nD.\n. ",
    "xh3b4sd": "I see that the example does not even stops the server properly. The onboarding experience with this creates some kind of uncertainty with grpc-go. I am not sure if I am doing something wrong or there is some bug or undocumented desired behaviour in this direction. \nSee https://github.com/grpc/grpc-go/blob/master/examples/route_guide/server/server.go#L238\n. I do not stop the server before starting it. Why should somebody do that actually? Anyway, the \"problem\" here is that you stop a server and you get an error no matter what you do. I figured out that it is because of the Close method of https://golang.org/pkg/net/#Listener. Documentation also states this.\n\n// Any blocked Accept operations will be unblocked and return errors.\n\nSo my explanation would be that the gRPC server listens and thus calls net.Listener.Accept, which returns always the accept tcp host:port: use of closed network connection error when close is called. gRPC returns this error. \nI guess you cannot do anything about this. Anyway it is really odd because users start to ignore errors like this because errors are thrown even nothing was actually wrong. In worst cases this leads to ignoring root causes of serious problems.\n. IMO we should reach out to the go developers and hear what they think. This issue should probably not be solved in grpc-go itself and it should, IMO, definitely not be solved by ignoring the error. \n. In my former comment I pointed out that the error comes from https://golang.org/pkg/net/#Listener, which documentation puts this:\n\n// Any blocked Accept operations will be unblocked and return errors.\n\nSo why should something produce an error when I request it to stop any action? Errors indicate faulty behaviour. When I make proper use of the net.Listener API there shouldn't be an error produced. I want to make the listener stop listening. I am not providing wrong arguments. I make proper use of its API. So this is why I think there should be a discussion with the go developers. Otherwise the current behaviour will only lead to people hiding errors, which in the worst case scenario will cause serious harm. \n. I understood it this way that you call net.Listener.Accept and each of the blocking accept calls will return an error in case the listener is going to be stopped. This is then not a correct behaviour. In case you call Accept on a listener that is already closed this should return an error, but this is not the case we are talking about as far as I understood. \n. I do not mind. I am super off this issue at the moment to be honest. If only I have issues, then lets close it. At the moment I am not bothered anymore by it. I think it should be fixed though. I am only not able to invest time into the how and actual doing at the moment. Thanks for asking anyway. . ",
    "toontong": "when the  Server.GracefulStop() was celled, all connection from client will be closed, even which was waiting server-side to handling the request. when the server finish the request, it found the tcp-connection was closed. It not Graceful at all.  The best way was close the idle connection.\n\n2016/08/31 16:00:12 transport: http2Server.HandleStreams failed to read frame: read tcp 127.0.0.1:802->127.0.0.1:44188: use of closed network connection\n. \n",
    "favadi": "What is best way to deal with this error? Should we add a field to Server to make it knows that it has been graceful shutdown? and if the error is \"use of closed network connection\", we should just ignore it and return nil?\n. @xh3b4sd can you explain why you think it is a problem with go, not grpc-go?\n. > So why should something produce an error when I request it to stop any action?\nMaybe I understand something wrongs, but the error returns when we call net.Listener.Accept() after it is closed (with Close() method). I think it is very reasonable, what do you think net.Listener.Accept()`  should return in this case instead?\n. ",
    "cored": "@iamqizhao how does the balancer figure out an address with a blank target? \n. @iamqizhao so nothing happens with an empty target because since there's a custom balancer it's defaulting to localhost ?\n. ",
    "Helen-Xie": "When I go run example, also the same error:\n```\nlin@XYL:~/go/src/google.golang.org/grpc/examples/helloworld$ go run greeter_server/main.go \ngoogle.golang.org/grpc/transport\n../../transport/handler_server.go:209: undefined: http2.TrailerPrefix\n../../transport/http2_client.go:821: undefined: http2.MetaHeadersFrame\n../../transport/http2_client.go:918: undefined: http2.MetaHeadersFrame\n../../transport/http2_server.go:145: undefined: http2.MetaHeadersFrame\n../../transport/http2_server.go:281: undefined: http2.MetaHeadersFrame\n../../transport/http_util.go:379: f.fr.ReadMetaHeaders undefined (type http2.Framer has no field or method ReadMetaHeaders)\n../../transport/http_util.go:509: f.fr.ErrorDetail undefined (type http2.Framer has no field or method ErrorDetail)\n``\n. @menghanl So, I shoud delete all thegolang.org/xsource ? \n I delete thegolang.org/x/net` source, then run go get again \ngo get -u golang.org/x/net\npackage golang.org/x/net: no buildable Go source files in /home/lin/go/src/golang.org/x/net\n. Thanks\uff0cthe problem has been solved.\n. ",
    "jmacd": "We've looked for ways to reduce memory pressure caused by inbound RPC processing, and there aren't any ways to throttle these allocations we can find. Either I would like to re-use these buffers (ideally, have an upper bound on number of buffers) or be provided with a way to intercept the request and stall before its buffer is allocated (as requested here: https://github.com/grpc/grpc-go/issues/901).\nAs it stands, there's no way to prevent a new request on an existing stream from allocating a new buffer immediately, since it happens before the existing interceptor support.\nThere's no way to selectively drain a client, either. (https://github.com/grpc/grpc-go/blob/71d2ea4f75286a63b606aca2422cd17ff37fd5b8/transport/handler_server.go#L373)\nJust a quote from the code, as a signal boost.\n// TODO(bradfitz,zhaoq): garbage. reuse buffer after proto decoding instead\nhttps://github.com/grpc/grpc-go/blob/71d2ea4f75286a63b606aca2422cd17ff37fd5b8/rpc_util.go#L244\n. (^^^ this issue is not about an optimization, this is about unruly behavior. please prioritize.)\n. ",
    "neilgarb": "Thanks for your response. I pulled yesterday, so I'm running 79b7c349179cdd6efd8bac4a1ce7f01b98c16e9b on both client and server.  Still seeing this behaviour.\nMore info: In production (which is using an older version of grpc) I have a balancer with two endpoints.  There I also get connection reset by peer when I restart a server, but the client seems to sort itself out after a short while.\nMore more info: I'm going to run a test with multiple connections per endpoint (something like https://github.com/google/google-api-go-client/blob/master/internal/pool.go#L38), so even if one endpoint is provided the client establishes multiple connections.  Will provide feedback once I'm one.\n. Creating a pool of connections in the resolver seems to have fixed the symptoms:\nfor _, a := range addresses { // there's only one in this instance\n        for i := 0; i < poolSize; i++ {\n            upd = append(upd, &naming.Update{\n                Op:       naming.Add,\n                Addr:     a,\n                Metadata: i,\n            })\n        }\n    }\n. Frustratingly, I can no longer reproduce the problem.  Here are the errors that are logged:\n// http2_client.go#165: log.Printf(\"issue-870: %T %+v\", err, err)\n2016/09/01 09:40:41 issue-870: *net.OpError read tcp 172.17.0.4:36216->172.17.0.1:50053: read: connection reset by peer\n2016/09/01 09:40:42 issue-870: <nil> <nil>\nPerhaps I wasn't actually running https://github.com/grpc/grpc-go/pull/864. I'll try a server restart again in a few minutes just to confirm, and then close this issue.\n. Seems resolved - I must just have muddled up my deployment.\nThanks for all the help/info.\n. ",
    "pcj": "In my own informal tests I'm seeing about 20% slower performance than java. However, go is more consistent, probably due to periodic jvm gc runs.  Will try to incorporate gogo protobuf in the repo I'm maintaining and see what I come up with... \nhttps://github.com/pubref/rules_protobuf\n. ",
    "dyu": "Any plans of adding the go results back in the multi-language perf dashboard on http://www.grpc.io/docs/guides/benchmarking.html?\nFrom the earlier results, I was speculating it was go's non-compacting gc that caused the slowdowns.\nHopefully you can post it soon with the go 1.8.0 improvements.. ",
    "kerinin": "For example (working from the grpc tutorial):\ngo\nctx, _ := context.WithTimeout(context.Background(), 0)\nstream, _ := client.ListFeatures(ctx, &pb.Rectangle{})\nfor {\n    _, err := stream.Recv()\n    if err != nil {\n        return grpc.Error(grpc.Code(err), \"Failed to list features\")\n    }\n}\nIn this case if we're making a request inside a handler, the returned code would be codes.Unknown rather than codes.DeadlineExceeded.  In order to correctly report the code we'd have to do something like:\ngo\nfor {\n    _, err := stream.Recv()\n    if err, ok := err.(grpc.StreamError); ok {\n        return grpc.Errorf(err.Code, \"Failed to list features: %s\", err.Desc)\n    }\n    if err != nil {\n        return grpc.Error(grpc.Code(err), \"Failed to list features\")\n    }\n}\n. From another perspective, the method's documentation states \"Code returns the error code for err if it was produced by the rpc system\", and it seems like the StreamError in this case is being returned by the RPC system.\n. https://gist.github.com/kerinin/6ead62c92f7a63bada345e37798d5f80\n. ",
    "enm10k": "I had the same issue.\nAnd I want this to be fixed for our production use.\n\nwe are working on improving grpc logging (this should only appear in debugging mode).\n\nIs there any work-in-progress branch as public?\nIf no one is working on this now, I'll do.. Thanks.\nI'm waiting for his back.. ",
    "CocoaWang": "hi , do you have any solutions for this problem ? for i have met the same error. . @menghanl  i have resolved my problem . The reason is that i set the wrong server-hostname when running tls . ",
    "dyxushuai": "@imoverclocked \nThis error is still existing for me with version v1.7.0\nDo you have any solutions for this problem?. ",
    "gwvo": "Not at the moment. @royalharsh has been working on porting the GRPC-FlatBuffers integration to other languages, but I am not sure if he has any plans for go yet.\n. If anyone with Go experience wants to step in and help, that be very much appreciated.\n. Well, that will have to be refactored to be serialization system agnostic. For example UnMarshall would be a no-op in FlatBuffers since it works directly from serialized data. See the C++ code for some inspiration on how to structure this.\n. ",
    "hvardhanx": "Currently, I am working on porting the GRPC-FlatBuffers integration to python. Most probably java and Go are next. :)\n. @ekarlso  Haven't started working on this yet.\n. ",
    "ekarlso": "@royalharsh any news on this ?\n. ",
    "jronak": "I have been working on it, have been reading the source code for a while after speaking with Wouter.\nHow do we handle Codec support for flatbuffers?, as of now ProtoCodec is default. \nref:https://github.com/grpc/grpc-go/blob/master/rpc_util.go#L54\n. Flatbuffers now supports grpc go!\ngoogle/flatbuffers#4082 got merged. @Thomasdezeeuw flatbuffers code foot print is very small. \nIf we go ahead with the custom codec, codec has to be generated for every struct by flatbuffers compiler \n. @Thomasdezeeuw got it.\n@iamqizhao Let me explain\nIf we do not implement the Codec interface for flatbuffers here, then it has to be written in every package generated by flatbuffers compiler for grpc service, though it would solve the problem pointed out above.\n. ",
    "edrex": "I'm also wondering how to get those endpoints (/debug/requests and /debug/events) working. I've tried with the hello example (no tls, error similar to above) as well as the route_guide example with -tls (transport: http2Server.HandleStreams failed to receive the preface from client: EOF). Some basic working example of tracing would be very helpful.\n. I now understand this issue a lot better.\n\nIt looks like there's a race between the close(writes) on https://github.com/grpc/grpc-go/blob/master/transport/handler_server.go#L214 and the previous ht.do(func() ... block which runs in the ServeHTTP goroutine. I'm not sure the best way to avoid the race. Maybe putting the close in a defer inside the block?\nWe're using the ServeHTTP implementation, which is still very rough, because we're muxing some regular HTTP stuff on the same port. We'll probably stop doing this to avoid ServeHTTP.\n\nI think I can put together a minimal failing example if that's helpful.. ",
    "tejasmanohar": "I think this makes sense. Thanks!\n. ",
    "fatih": "For anyone who finds this via Google, here is a simple snippet that shows how the log the return error:\n```\nerrHandler := func(ctx context.Context, req interface{}, info *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (interface{}, error) {\n    resp, err := handler(ctx, req)\n    if err != nil {\n        log.Printf(\"method %q failed: %s\", info.FullMethod, err)\n    }\n    return resp, err\n}\nsrv = grpc.NewServer(grpc.UnaryInterceptor(errHandler))\n```. ",
    "vimalk78": "i am also looking for the same.\ni posted in the grpc-go mailing list\nhttps://groups.google.com/forum/#!topic/grpc-io/EIcQlLqlNQg\n. ",
    "juanpmarin": "Any update about this ? . ",
    "wangming1993": "Thanks, and I know it required a trailer for ending stream. What I really confused is how to send a trailer with nginx1.11.3.  Is it supported by nginx ?  My case is:  use nginx1.11.3 as server proxy to pasre http2 protocol,  then gprc-go as client, client dial 443 port then all request redirect index.php, then nginx send a response to rpc client.\n@iamqizhao \n. @iamqizhao please give some suggestion if I want such feature,  thanks!!\n. ",
    "shuoli84": "The upstream issue is merged. So this one is unblocked? I guess?\n. I guess what you need is Read()?\nhttps://github.com/grpc/grpc-go/blob/c5c66f12211080d29aa99adcf073946f6776fc7b/transport/transport.go#L314\n. ",
    "yugui": "Yes. Thank you for letting me know, @mwitkow \n. ",
    "tbg": "LGTM, thanks.\n. No comment here? Seems brittle.\n. ",
    "arun0009": "Well it just prints \"World\" if I print the bytes after using the Read(). Is there a way to convert the body back to proto request? (I am running the hello world e.g)\n```\nfmt.Println(\"stream is : \", frontStream)\nstream is :  &{1 0xc4201a2480 0xc42018bb90 0x139ec0        /helloworld.Greeter/SayHello   0xc42018bad0 0xc4201b2600 0xc420147720 0 0 0xbe6f0 0xc420147760  map[] map[] {{0 0} 0 0 0 0} false 2 false 0 }\nfmt.Println(\"method is \", frontStream.Method())\nmethod is  /helloworld.Greeter/SayHello\nn, _ := frontStream.Read(b)\nreq := &helloworld.HelloRequest{}\nerr := jsonpb.Unmarshal(strings.NewReader(string(b[:n])), req)\nif(err != nil) {\n    fmt.Println(\"req is :\", req.Name)\n}\nfmt.Println(\"data is \", string(b[:n]))\nreq is :\ndata is : \u0000\u0000\u0000\u0000\u0007\u0005world\n```\n. I was able to solve this:\ndata := make([]byte, 4096)\n    frontStream.Read(data)\n    var buf bytes.Buffer\n    lenBuf := bytes.NewBuffer(data[1:5])\n    var lenData uint32\n    binary.Read(lenBuf, binary.BigEndian, &lenData)\n    buf.Write(data[5 : lenData+5])\n    req := buf.String()\n    fmt.Println(\"req is : \", req)\n        helloReq := helloworld.HelloRequest{}\n    proto.Unmarshal(buf.Bytes(), &helloReq)\n    fmt.Println(\"name is : \", helloReq.Name)\nPrints:\nreq is :  \n\u0005world\nname is : world\n. ",
    "akaspin": "Thanks pal. You are steal two hours of my life.\n. ",
    "pongad": "Sounds good to me. I'll work around the issue in the meantime. I cannot seem to find a tracking issue for it. Do you have the link handy? I primarily want one so I can link my issue to it :)\n. That sounds good to me. Not sure if you've already have an implementation running, but since I already put something together, I'll open a PR for it.\n. This is a good point. For some reason, I didn't think of it before. My original thought (see #902) was to have a metadata.Join that just merges and leave the rest to the user. @menghanl ?\n. PTAL\n. @menghanl Saw your LGTM. Just wanted to make sure that you aren't waiting for me to merge myself since I don't have write access to do it :smile: \n. I assume that sgtm was to my bug description, not approving the idea for this repo \ud83d\ude06 . @menghanl noted. I don't see another issue for Balancer and Resolver failing specifically so I'll leave it up to you if you want to close this as duplicate.. Should we return as many details as we can unmarshal? IIUC, the server might add a new type of error and return it. The client might not know about this new type, and UnmarshalAny would error. . @ghasemloo Can I trouble you for the link to the comment? I see that this comment says \"it returns nil and the first error encountered\", but I don't see discussions on this point.\n@jba do you have an opinion? (I believe both @jba and @dfawley are at GopherCon).. Thank you for this. I lean towards 2 myself, but I think your argument is convincing, so SGTM.\nIf this isn't urgent, could you wait for @jba to also sign off?. ",
    "devnev": "PTAL.\n. Done.\n. ",
    "mephux": "@iamqizhao that's fair - what could I use on the client side to detect this? I see logs sent to stderr on connection loss and during back off but how can I hook into that.\nMaybe I am missing something in the docs for that or do I have to just create the connection logic myself?\n. ",
    "subfuzion": "It seems that this is already available for grpc development in Java:\nhttp://www.grpc.io/grpc-java/javadoc/io/grpc/inprocess/package-summary.html\nhttps://github.com/grpc/grpc-java/tree/master/core/src/main/java/io/grpc/inprocess\nhttps://github.com/grpc/grpc-java/tree/master/core/src/test/java/io/grpc/inprocess\n. ",
    "dennisdoomen": "In the .NET world, this is something to be expected. OWIN does it really well and allows you to build HTTP-enabled components and host them anywhere. If you host it in-process, all the requests happen completely in-memory. . ",
    "Random-Liu": "Any updates on this?\n/cc @stevvooe @crosbymichael  I don't think bufconn is enough for us. We can only wrap all the grpc services now.. > I don't really understand why you can't use bufconn though.\n@jhump Because we may also want to get rid of the serialization/de-serialization, :)\nWith https://godoc.org/github.com/fullstorydev/grpchan/inprocgrpc, can I make the grpc server serve both remote requests and inproc requests at the same time?. @jhump Nice. I'll look into it a bit more.\nOur use case is that containerd has a official grpc api. And containerd has a plugin for Kubernetes support is called cri-containerd which talks with containerd grpc api.\nPreviously cri-containerd is a separate process, but recently we decided to merge it into containerd process to get rid of the grpc overhead, but still integrate with the official grpc api. That is why we need the inproc grpc.. ",
    "Daniel-B-Smith": "Would this unblock gRPC over HTTP 1.1? Really, what we want is grpc-web for golang clients. Please let me know if there is a separate space where I should bring this up.. ",
    "veqryn": "I would like to be able to create a GRPC Client and have it talk to my existing GRPC Server within the same process, without sending network traffic through loopback/localhost (without dialing).\nI want to do this because I am hosting a GRPC Server and a GRPC Gateway in the same process (https://github.com/grpc-ecosystem/grpc-gateway).\nThe gateway requires a GRPC Client, which means that the GRPC Gateway uses the GRPC Client to send traffic on localhost loopback to the GRPC Server.\nFor example, I would like something roughly like this:\n```go\n// GRPC Server\ngrpcServer := grpc.NewServer()\npb.RegisterPixelServer(grpcServer, myService)\n// Serve and listen in a goroutine...\n// GRPC Gateway\nmux := runtime.NewServeMux()\ngrpcClient := pb.NewClientAdaptor(grpcServer, myService)\nerr := gw.RegisterPixelHandlerClient(ctx, mux, grpcClient)\n// Serve up the gateway mux in a goroutine...\n```\nI realize that it is possible to manually make myService satisfy both MyServiceServer and MyServiceClient interfaces on a generated golang proto file.\nHowever doing so would mean that grpc.CallOption's would get ignored, and I would also lose out on grpc's interceptors and stats handling, and other things that the grpc client and server are doing that would be bypassed by handling it directly.\n. Yep, it seems I was mistaken.. ",
    "alexmt": "One more use-case for custom transport is to support HTTP1.1 + gRPC-Web.\nThe project which needs it is https://github.com/argoproj/argo-cd. We have CLI client written in Golang which talks to gRPC API server. In most cases API server is running behind proxy and our users are struggling to configure the proxy to serve gRPC/HTTP/2 and terminate SSL at the same time.\nThe workaround was to run gRPC revese-proxy within CLI which converts gRPC/HTTP2 into gRPC-Web and send to API server over HTTP/1 . It would be great to just configure client to use gRPC-Web/HTTP1 transport.. ",
    "panamafrancis": "Ping... Any comment here?. ",
    "ccressent": "It turns out an unsuspected docker container was exposing its port 10000 on the host machine's port 10000... which is the default port for this example. The example server would still run fine, but the client would try to talk to the docker container, generating the errors because that container doesn't \"speak\" gRPC.\nThis is somewhat related to #120.\nClosing for now.\n. ",
    "yacovm": "I agree, this is useful to defend against attacks such as described in:\nhttps://mitls.org/pages/attacks/3SHAKE\n. ",
    "donovanhide": "Cool, thanks!\n. ",
    "ahmetb": "@menghanl I didn't configure anything (such as any loggers). I'm just following the tutorials.\nIt's odd that some go package is printing to stderr/stdout without my consent. Ideally this wouldn't happen. \n. +1 this is still an issue. I have a very basic use case of gRPC and the log output is flooded with this message every 4 minutes while it is sitting idle. This looks confusing for someone who just started using gRPC and therefore warrants a Google search which eventually lands people here. I am still wondering if I\u2019m doing something wrong.\n2017/02/22 09:37:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:41:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:45:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:49:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:53:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/02/22 09:57:14 transport: http2Client.notifyError got notified that the client transport was broken EOF.. > seems like the underlying transport breaks due to something\nI think we need to dig this one deeper. I understand the logger work is going on but this one might be pointing to an actual problem.\nThis is basically just me running multiple grpc services on my laptop and it starts happening after a while such as 2 minutes after services don't get any requests:\n```\nDEBU[2017-02-23T12:06:08-08:00] request completed                             elapsed=1.512785ms method=POST path=\"/coffee\" service=web\n2017/02/23 12:08:24 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n```. I have an easy repro for this, I just initialized one of cloud.google.com/go libraries, did not make any requests, and created an interactive prompt to read in from the user, and just waited for a couple of minutes in the prompt. \nSimplest repro:\n```\nimport iamadmin \"cloud.google.com/go/iam/admin/apiv1\"\nclient,err := iamadmin.NewIamClient(ctx)\nif err != nil {panic(err)}\nscanner := bufio.NewScanner(os.Stdin)\nfor scanner.Scan() {fmt.Println(scanner.Text())} // <-- do not hit Return when you run the program\nif err := scanner.Err(); err != nil {panic(err)}\n```\nshortly after, you'll start seeing:\n2017/06/12 12:37:39 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/06/12 12:41:39 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n...\n. Let me know if you need a repro program. I am assuming the fix will get rid of this error message without any change in the caller code. If that's not the case, we're not fixing it right.. @yifan-gu yes this is the server. \n@jwhitcraft I am on GKE so probably there's Heapster. I have health probes too. One hint is, this doesn't start happening right away when the server starts. It takes a while for this issue to show up. . Closing this as it's about the health/liveness probes, terminating tcp connections prematurely.. Closing in favor of #2318.. I understand that we're planning to go ahead with adding rpc Watch() to health.proto (it's rolled back only temporarily).\nSo this PR brings up master of grpc/grpc in sync with grpc/grpc-go in terms of google.golang.org/health/... packages. Right now they're out-of-sync because health.proto in the grpc/gprc repo doesn't have Watch() method \u2013but grpc-go does (in v1.15.0 tag).. Note that health.proto used by grpc-go v1.15.0 does not match grpc/grpc v1.15.0 health.proto so there's a version skew issue here due to rollback in grpc/grpc.\nSince there are plans to roll forward the rpc Watch I'm closing this revert.. (Context: This came up in #2313.)\nhttps://github.com/grpc/grpc-go/tree/master/health/grpc_health_v1 package just made a change (because health.proto in grpc/grpc was updated to add a new rpc).\ngrpc_health_v1 package in grpc-go v1.14.0 had:\ntype HealthServer interface {\n    Check(context.Context, *HealthCheckRequest) (*HealthCheckResponse, error)\n}\nso I implemented this interface in my code.\nThen I updated my grpc-go dependency to v1.15.0 which isn't supposed to break (per semver). But v1.15.0 introduced a new method:\ntype HealthServer interface {\n    Check(context.Context, *HealthCheckRequest) (*HealthCheckResponse, error)\n    Watch(*HealthCheckRequest, Health_WatchServer) error\n}\nas expected, having not implementing this Watch method yet, this broke all my programs using grpc-go.. ",
    "campoy": "+1 this should be documented clearly on the docs and probably on the comments of the Send method too\n. ",
    "gbbr": "This doesn't work. There is nothing found using peer.FromContext in a unary client interceptor. I wonder if I'm doing something wrong. I've only managed to extract this on the unary server interceptor.. If anyone ever finds this issue, the solution is to use the grpc.Peer option inside the UnaryClientInterceptor and to pass it to the invoker function. Afterwards, peer information will be obtainable.. ",
    "jgraettinger": "It's not fixed. recvResponse still closes the stream even if err is io.EOF, and then returns nil (such that invoke closes it again too).\n. Please try actually running the code before making assertions here. Returning nil is definitely not the same thing as setting err = nil to clear it, and the defer still sees err == io.EOF. Instrument it, run the tests, and see what you get.\n. I stand corrected! Agreed that #947 fixes. Apologies, it's been a day.\n. ",
    "jba": "@MakMukhi Do you have an ETA and/or gRPC version number for when the log spam will cease?. When you do drop 1.6, you should not also change \"golang.org/x/net/context\" to \"context\". It is true that \"context\" was introduced in 1.7, but there may still be users who use recent versions of Go (even 1.9) but who still are on golang.org/x/net/context, because one of their dependencies is. \nIn other words, dropping 1.6 support is not a breaking change for a 1.9 user, but changing the context import path may be.\nI believe we have to wait for 1.10 to come out, at which point we can assume everyone is on 1.9 or 1.10, and therefore everyone has type aliases. Then we will make golang.org/x/net/context an alias for the stdlib context, and only then can we change import paths.. You need to update go.opencensus.io/plugin/ocgrpc to the latest version.. I agree with Michael. This is an unusual case, because the dynamic nature of Any means there's a reasonable chance that an old client might not have the right protos linked in, and partial details are still likely to be useful.\nWhile it's true that returning nil with errors is a best practice, it's not a hard requirement, and there are counterexamples. The most notable is io.Writer.Write, which returns the number of bytes written along with an error. . What about \n\nreturn []proto.Message, but also, either\n   4a. return a second []error slice; which is non-nil exactly where the first slice is nil; or\n   4b. add a DetailsErrors method that returns []error.\n\nI kind of like 4b because preserves type-safety, handles the common case (people won't care about the error usually), but makes it easy to get the errors if needed (without duplicating the body of Details).. OK, I see your point about the need for a switch. I'm good with []interface{}, as long as the documentation makes everything clear.. ",
    "montanaflynn": "I'm getting these logs when using the gcloud vision client (cloud.google.com/go/vision)\ntransport: http2Client.notifyError got notified that the client transport was broken EOF.. ",
    "dmonay": "I am also getting these logs when using Google's Cloud Speech and Cloud Translate APIs.\nIs there an update on this?. ",
    "wedgeV": "2017/07/17 15:12:28 stdout: \"PRI http://* HTTP/2.0\" from 127.0.0.1:42022 - 405 0B in 17.907\u00b5s\n2017/07/17 15:12:28 stdout: \"PRI http://* HTTP/2.0\" from 127.0.0.1:42024 - 405 0B in 24.254\u00b5s\n2017/07/17 15:12:28 stdout: \"PRI http://* HTTP/2.0\" from 127.0.0.1:42026 - 405 0B in 12.321\u00b5s\n2017/07/17 15:12:28 stdout: \"PRI http://* HTTP/2.0\" from 127.0.0.1:42028 - 405 0B in 50.646\u00b5s\n2017/07/17 15:12:28 stdout: \"PRI http://* HTTP/2.0\" from 127.0.0.1:42030 - 405 0B in 20.908\u00b5s\n2017/07/17 15:12:28 stdout: \"PRI http://* HTTP/2.0\" from 127.0.0.1:42032 - 405 0B in 10.143\u00b5s\n2017/07/17 15:12:28 stdout: \"PRI http://* HTTP/2.0\" from 127.0.0.1:42034 - 405 0B in 16.86\u00b5s\n2017/07/17 15:12:28 stdout: \"PRI http://* HTTP/2.0\" from 127.0.0.1:42036 - 405 0B in 14.035\u00b5s\nI'm using the chi framework and I see a flood of these when I enable router.Use(middleware.Logger) in my server and a backend GRPC service it uses is offline.. ",
    "vtubati": "A side effect (it seems) of these rapid retries, leaving in hundreds of lingering TCP sessions,  in case of a mis-behaving remote server (which in our case was due to mis-configuration, pointing to a wrong server) is GC getting triggered every 15secs or so and taking up to 70% of a core for about 15secs. Below is a snip of perf report. Any way to expedite fix for the issue?:\nSamples: 49K of event 'cycles', Event count (approx.): 21149591915\n 29.95%  emsd  emsd                   [.] runtime.scanobject\n 21.11%  emsd  emsd                   [.] runtime.memclrNoHeapPointers\n 10.52%  emsd  emsd                   [.] runtime.greyobject\n  9.67%  emsd  emsd                   [.] runtime.heapBitsForObject\n  2.76%  emsd  emsd                   [.] runtime.gcDrain\n  1.52%  emsd  emsd                   [.] runtime.scanblock\n[snip]\n. Thanks for the details of the fix being thought about. Wonder if you are in a position to share when the patch is likely to be available. Thanks.. ",
    "ncteisen": "Will figure out a better way to do this\n. ping\n. Fixed all comments\n. Done\n. Addressed all comments. They will be consistent with cpp once my second PR goes through:\nhttps://github.com/grpc/grpc/pull/8819\nWe decided that having a server port and server host flag was hacky because\nis masks the server_addresses flag originally present in the stress client\nOn Wed, Nov 30, 2016 at 5:37 PM, adelez notifications@github.com wrote:\n\nCan you make the flags consistent with C++ stress client? Particularly,\ncan you define a server_port flag?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/997#issuecomment-264052991, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AF3QLPW1vAl84UiUAbOM0YE_ZPbJUFmKks5rDiTvgaJpZM4K47B1\n.\n. Ah whoops, you are right. I didn't realize e.Error() also prints the ErrorDesc. Closing this. Addressed all comments. Also the official documentation for this is WIP, and will be added to grpc/grpc/docs by EOD. Documentation for the new tests here. Done. So to address the comments we spoke about last week, I added a sleep in the GOAWAY test. It's not the cleanest solution, but I think it should work well enough for now.\n\nI put in an issue, grpc/grpc#9300, to track implementing the \"wait for GOAWAY\" behavior in all of the client languages.. @makdharma and I intend to get finish up these negative http2 interop tests by the end of the week. Are the changes I made sufficient? Though there is a race condition for the goaway test, this behavior is consistent for all client languages.. I see @menghanl is OOO for a few weeks. Adding @MakMukhi.. Ping. Hi, can you run the situation again with the c core's bdp probe disables?\nThat can be accomplished by setting the channel arg GRPC_ARG_HTTP2_BDP_PROBE to false. (Not sure about the exact way to do this through the Python wrapping layer.)\nAlternatively, if you are building from source, you could change this line to be false and the probe would disabled.\nThis would narrow the problem down from general flow control bookkkeping to our home grown optimization strategies.. Thanks that helps us narrow that down. The bdp correct probe is correct behavior, and this helps us ensure that the problem has to do with Go's handling of c core's manipulation of INITIAL_WINDOW.\nHave you tried a run with bdp enabled, and @MakMukhi's fixes patched in on top? Specifically I think #1367 is the one that hopes to fix Go's mishandling of c core's flow control. That would be great, thanks!\nIf performance is critical for your project, then enabling the bdp probe can really speed things up, so I don't think disabling it is a viable final solution. \nFlow control is being very actively developed in c core (grpc/grpc#11720) and in Go, and we appreciate your help in sorting these issue out!. Try GRPC_TRACE=bdp_estimator,flowctl,http\nif that is too much, then GRPC_TRACE=bdp_estimator,flowctl should suffice. Drive by comment: in c core this is implemented in a more generic fashion. We don't have methods for TraceChannelCreated, TraceAddressResolution, etc etc etc.\nThere are only two methods, AddTraceEvent and AddTraceEventWithReference. They take in any string as the description of the event. This allows developers in the future to add trace without having to change the channel trace object interface.\nIf your method is cleaner, and works better for Go, than that is cool. Just wanted to point out the way core has been implementing. I had the default set to foo.test.google.fr to match the default parameters in the stress clients of the other languages, like python. The validation for this test all occurs on the server side. The server sends a GOAWAY after the first rpc, and then asserts that the next rpc comes in on a different stream.. ",
    "Shamanoid": "I signed the CLA!\n. ",
    "tmc": "Is this intended to support use cases such as https://github.com/mwitkow/grpc-proxy?\n. https://github.com/grpc-ecosystem/grpc-gateway would like to move to this ASAP. IMO the longer this difference exists the more pain there will be.. ",
    "piotrkowalczuk": "Same problem here. grpc.SupportPackageIsVersion4 breaks public API. It looks like it should be version v4.0.0 but was released as v1.0.4.  Otherwise entire versioning is completely useless here. \n. @dfawley @menghanl thanks it works (I had to copy few internal packages as well)! Do you have any plans to implement configurable SRV resolver within the core gRPC package? . ",
    "ajayamohan": "@iamqizhao : go get -u on both grpc and protobuf/{proto, protoc-gen-go} doesn't solve this.\n. Yes, the proto files were regenerated. Same failure in building the project after the .pb.go and .gw.go files are generated.\n```\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the grpc package it is being compiled against.\nconst _ = grpc.SupportPackageIsVersion4\n```\nservice.pb.go:333: undefined: grpc.SupportPackageIsVersion4\n. ",
    "christianwoehrle": "Thanks, updating the grpc packages worked for me.. ",
    "havoc-io": "That sounds like a fine solution.  The change at line 153 of http2_client.go seems a bit fragile to me though given that the error propagates up through NewClientTransport, resetTransport, and resetAddrConn.  It seems like the behavior could very easily be broken by some future change at a higher level (especially given the degree of refactoring that's been done in resetTransport over the last year).  Would it be possible to document (perhaps as part of the documentation for WithDialer) that this interface{ Temporary() bool } check occurs and has the behavior of aborting re-dial attempts?\n. Thanks a lot for the quick response and elegant solution!  I'll watch for your PR to be merged.\n. @coocood\nFor your second code snippet, I think you'd actually want to compare against the buffer's capacity rather than length.  You can actually slice up to the capacity of a buffer, not just the length.  The gob package does something very similar:\nhttps://github.com/golang/go/blob/f3f507b2d86bfb38c7e466a465d5b6463cfd4184/src/encoding/gob/decode.go#L64\nThat way, if you need buffers of size X, X-1, and then X, you aren't allocating twice.. @MakMukhi It looks like #1987 was reverted by #2049.  Are there plans for a different strategy?. ",
    "secmask": "please working on this issue. ",
    "ruinanchen": "Very thank you for your suggestion. we solve the problem, but i feel a little strange.As below:\n1.we define an unary rpc client interface variable in package rpclient:\nvar BizClient pb.MessageServiceClient\nthe example we call the client:\n_, err := rpcclient.BizClient.OfflineDeliver(ctx, reqPacket)\n2.We don't want to judge whether BizClient is nil or not before called, So we guarantee that BizClient  is not a nil after function init:\n```\nfunc Init(etcdOpts conf.EtcdOpts, rpcOpts conf.RpcClientOpts) {\n    resolver, err := loadbalancer.NewEtcdResolver(etcdOpts.RadarAddrs, \"\", \"\")\n    if err != nil {\n        logger.Fatalf(nil, \"init etcd service resolver failed: %v\", err)\n        return\n    }\ntimeout = time.Second * time.Duration(rpcOpts.Timeout)\nnewClientConn := func(service string, balancer grpc.Balancer) (conn *grpc.ClientConn, err error) {\n    conn, err = grpc.Dial(\n        service,\n        grpc.WithInsecure(),\n        grpc.WithBlock(),\n        grpc.WithTimeout(timeout),\n        grpc.WithUserAgent(userAgent),\n        grpc.WithBalancer(balancer),\n        grpc.WithUnaryInterceptor(interceptor.NewUnaryClientInterceptor(timeout, true)),\n    )\n\n    if err != nil && conn != nil {\n        conn.Close()\n        conn = nil\n    }\n    return\n}\nbizConn, err := newClientConn(rpcOpts.BizServ,\n    loadbalancer.NewTraceAwareBalancer(grpc.RoundRobin(resolver)))\nif err != nil {\n    logger.Errorf(nil, \"failed to try connect bizServ, service name=%v\", rpcOpts.BizServ)\n    bizConnFake, err := grpc.Dial(\"unavailable\", grpc.WithInsecure())//no block mode, so return a clientConn even if it unavailable\n    if err != nil {\n        logger.Fatalf(nil, \"bizConn init, dial unavailable failed: %v\", err)\n    }\n    BizClient = pb.NewMessageServiceClient(bizConnFake)//guarantee it not nil\n    logger.Infof(nil, \"create an unavailable bizClient\")\n    go func() {\n        for {\n            bizConn, err = newClientConn(rpcOpts.BizServ,\n                loadbalancer.NewTraceAwareBalancer(grpc.RoundRobin(resolver)))\n            if err != nil {\n                continue\n            }\n            bizConnFake.Close()//close older\n            BizClient = pb.NewMessageServiceClient(bizConn)\n            logger.Infof(nil, \"create bizClient, service name=%v\", rpcOpts.BizServ)\n            return\n        }\n    }()\n} else {\n    BizClient = pb.NewMessageServiceClient(bizConn)\n    logger.Infof(nil, \"create bizClient, service name=%v\", rpcOpts.BizServ)\n}\n\n```\n3.I think dial can return a clientConn when balance strategy even if not address available, as an unblock ordinay(don't use balance) dial do. Actually, clientConn with no address allowed during execute, for example: Firstly an address avaible which make dial success, soonly the address unregistered from balancer's watcher. Why not allowd in init stage. \nSo i think dial with balancer also should has unblock mode, the dial's exception could be found until the rpc client called. If it did, we can simplify our code(^-^).\n. ",
    "tomwilkie": "I too am a bit surprised by this - would you at least consider an DialOption such that we can have a way of creating a client that is guaranteed to be non-blocking?. We're also seeing this - when a HTTP connection is prematurely closed, the context that is cancelled is from context, not golang.org/x/net/context.  Does grpc need to support pre-go1.7, or can we switch to using go1.7's context package?. I see #711 mentions that golang.org/x/net/context uses context on go1.7 - what had happened was I had an old version of golang.org/x/net/context vendored that didn't have the new go1.7 behaviour.  Might this explain why @MakMukhi and @rbewley4 couldn't reproduce - they had the up-to-date golang.org/x/net/context package?. Nice, thanks!. ",
    "EwanValentine": "@menghanl I deleted the vendor folder and re-added them, still the same error\n. My bad, I'm new to govendor, and I didn't realise 'add' was just adding my\nlocal version. I tried again and it worked, ish. I got a different error,\nbut I think that's something unrelated. Thanks for pointing me in the right\ndirection, were spot on!\nOn Wed, 9 Nov 2016 at 18:15 Menghan Li notifications@github.com wrote:\n\nI think the gRPC code is not up to date. I'm not sure how you did the\nupdate, but you can check you local code to see if it's updated.\nIn the error message you provided, error happens at line 425.\nBut in latest code, transport.NewServerTransport is called at line 443:\nhttps://github.com/grpc/grpc-go/blob/master/server.go#L443\ntransport.NewServerTransport is defined at:\nhttps://github.com/grpc/grpc-go/blob/master/transport/transport.go#L368\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/978#issuecomment-259484279, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABGgMyWNj4cyCBot2GoGBBqBmwfM5mDXks5q8g2xgaJpZM4KsMfd\n.\n. @Ch3ck What did you misconfigure? I'm having a similar issue in Kubernetes, but not locally :/ . \n",
    "Ch3ck": "@menghanl I misconfigured my docker-compose yml file which caused the issue. I erroneously thought it was coming from the grpc package. Thanks\n. Hi @EwanValentine @marcosArruda @nizsheanez I did not properly configure one of the services in the Docker compose file. I missed adding one of the services to the docker compose file. So when I started docker-compose there was the rpc error which meant one of the connections was not available which actually referred to the missing service. I think I fixed that in this commit: https://github.com/Ch3ck/go-serve/commit/15c63b2dc0089562c7f6b0e324b51326cc0f9be6. You could check out delve tool, It's the best golang debugger I've used so far. They provide stacktraces, although not very good.. ",
    "marcosArruda": "@Ch3ck same problem here. What did you misconfigured??. ",
    "FX-HAO": "Same problem, but my service works well for a few days, and then consistently got rpc error: code = Unavailable desc = grpc: the connection is unavailable, if i restart the container, it works again, but for a few days, repeatedly.. I got it, the problem is that i created a new connection for every RPC and don't close it, so the number of file descriptors increases infinitely, finally, it reach the limit of open files. \n```bash\nroot@de6ac7bde091:/# ss -s\nTotal: 26360 (kernel 26442)\nTCP:   25652 (estab 8147, closed 17503, orphaned 5, synrecv 0, timewait 216/0), ports 0\nTransport Total     IP        IPv6\n*     26442     -         -      \nRAW   0         0         0      \nUDP   1         1         0      \nTCP   8149      8104      45     \nINET      8150      8105      45     \nFRAG      0         0         0        \nroot@de6ac7bde091:/# ss dst 192.168.16.30 | wc -l\n8072\nIt seems i need a connection pool to reuse the connections.. Got the same issue on go 1.9. Branch `master` of `golang.org/x/net` does not fix the issue. Then I  used `release-branch.go1.9` branch, the issue was fixed.\ncd $GOPATH/src/golang.org/x/net\ngit fetch --all\ngit checkout origin/release-branch.go1.9\nI am using glide, the command is `glide get golang.org/x/net#release-branch.go1.9`. . Got the same issue on go 1.9. Branch `master` of `golang.org/x/net` does not fix the issue. Then I  used `release-branch.go1.9` branch, the issue was fixed.\ncd $GOPATH/src/golang.org/x/net\ngit fetch --all\ngit checkout origin/release-branch.go1.9\n``\nI am using glide, the command isglide get golang.org/x/net#release-branch.go1.9`. . ",
    "Astone-Chou": "@dfawley Can u give me some suggestion or some doc about, In stream model, one grpc.ClientConn for all user or multi-grp.ClientConn for all user. Thanks very much !. I found a interface grpc.UnaryServerInterceptor, looks good to me.. server.Stop()  will cancel request process by using context.Context. transport/http2_server.go maybe what u are looking for. ",
    "bhs": "(Cross-posting from that PR)\n@iamqizhao\n\nI do not read into your code. Is it possible to have a mutex to guard your MD accesses? grpc MD is not concurrency-safe.\n\nSince the GRPC MD is shared across all users of the GRPC connection, adding locking in this code (or any other) is not sufficient to make it concurrency-safe. We would need all readers and writers to share concurrency primitives. This is why I would argue that the MD struct should be made thread-safe intrinsically... otherwise how can one confidently write code that either reads or writes from that structure?\n. @kokaz one could make MD an interface and document that it is concurrency-safe. I recognize that this would be a backwards-incompatible change for some callers, but it's not like the documentation change in #986 is the only (or even the best or certainly the most robust) way to address the problem.\n. ",
    "zirkome": "@bensigelman The issue is that MD is not a struct but a map so anyone using MD can end up using it non thread-safely and so we can't really make MD intrinsically thread safe except for the method it has (i.e. Copy, Join).\n. ",
    "jbrook": "I think that must be what I am missing. I would simply like the client to reconnect automatically after I restart the server on deploying updated code. Is that a reconnection I should be handling in my own code?\n. ",
    "mattolson": "Thanks for the fix. We were bit pretty hard by the change in https://github.com/grpc/grpc-go/pull/974. ",
    "hsulei": "thanks\n. ",
    "olix0r": "Thanks for the prompt response, @iamqizhao. Are there any examples you can point me at?\n. ",
    "yangmls": "I signed it!. ",
    "jdoliner": "Hmm, that didn't seem to fix the problem. It just took the requests from failing to blocking indefinitely. Anything else we could try?. @hsaliak I'm not, we were using some third party libraries and once we got rid of those things seemed to work. Closing.. ",
    "yhhann": "I signed it!. This request fix panic 'close of closed channel' when there is no address available to dial, merge it please.. Thanks for  menghanl's comment.\n\"no address available to dial\" means the logic hit a grpc error 'errNoAddr'.\n. Here is the stack.\nE1130 17:52:11.822574    8270 client.go:91] Failed to dial service \"vexillary-demo\", grpc: there is no address available to dial\npanic: close of closed channel\ngoroutine 1 [running]:\npanic(0x820b00, 0xc420190b90)\n        .../src/runtime/panic.go:500 +0x1a1\ngoogle.golang.org/grpc.(*roundRobin).Close(0xc4201c6540, 0x0, 0x0)\n        mytest/third-party-go/vendor/google.golang.org/grpc/go_default_library.a.dir/jingoal.com/third-party-go/vendor/google.golang.org/grpc/balancer.go:396 +0xc9\ngoogle.golang.org/grpc.(*ClientConn).Close(0xc420210000, 0x0, 0x0)\n        mytest/third-party-go/vendor/google.golang.org/grpc/go_default_library.a.dir/jingoal.com/third-party-go/vendor/google.golang.org/grpc/clientconn.go:577 +0x172\ngoogle.golang.org/grpc.DialContext.func1(0xac5120, 0xc420014510, 0xc420167da0, 0xc420167da8, 0xc420210000)\n        mytest/third-party-go/vendor/google.golang.org/grpc/go_default_library.a.dir/jingoal.com/third-party-go/vendor/google.golang.org/grpc/clientconn.go:258 +0x86google.golang.org/grpc.DialContext(0xac5120, 0xc420014510, 0xc420128fa0, 0xe, 0xc420167e98, 0x4, 0x4, 0x0, 0xabd060, 0xc420015e40)\n    mytest/third-party-go/vendor/google.golang.org/grpc/go_default_library.a.dir/jingoal.com/third-party-go/vendor/google.golang.org/grpc/clientconn.go:331 +0x59f\n\ngoogle.golang.org/grpc.Dial(0xc420128fa0, 0xe, 0xc420167e98, 0x4, 0x4, 0x0, 0xabd060, 0xc420015ed0)\n        mytest/third-party-go/vendor/google.golang.org/grpc/go_default_library.a.dir/jingoal.com/third-party-go/vendor/google.golang.org/grpc/clientconn.go:236 +0x72\n...\nmy main func.\nI implement a Resolver which work with roundRobin together, when my resolver find out an empty server list, the logic will hit 'errNoAddr', and then it will close the roundrobin twice, so panic.\n        } else {\n            addrs, ok = <-ch\n            if !ok || len(addrs) == 0 {\n                waitC <- errNoAddr\n                return\n            }\n        }\n\n. I think so too, but I never call balancer.Close().\nAnd even if the balancer be closed twice, I think it should not throw a panic, isn't it?\nso I add this line.. ",
    "erjoalgo": "github.com/golang/protobuf/{proto,protoc-gen-go}\n```\nproto$ cd $GOPATH/src/github.com/golang/protobuf/proto\nproto$ git show --stat\ncommit 8ee79997227bf9b34611aee7946ae64735e6fd93\nAuthor: Bryan C. Mills bcmills@google.com\nDate:   Wed Nov 16 22:31:26 2016 -0500\ndescriptor: rename generated protobuf package on import.\n\nThis works around https://github.com/google/go-genproto/issues/8,\nrenaming the local package to match its expected name.\n\ndescriptor/descriptor.go      | 2 +-\n descriptor/descriptor_test.go | 2 +-\n 2 files changed, 2 insertions(+), 2 deletions(-)\nproto$ cd $GOPATH/src/github.com/golang/protobuf/protoc-gen-go\nprotoc-gen-go$ git show --stat\ncommit 8ee79997227bf9b34611aee7946ae64735e6fd93\nAuthor: Bryan C. Mills bcmills@google.com\nDate:   Wed Nov 16 22:31:26 2016 -0500\ndescriptor: rename generated protobuf package on import.\n\nThis works around https://github.com/google/go-genproto/issues/8,\nrenaming the local package to match its expected name.\n\ndescriptor/descriptor.go      | 2 +-\n descriptor/descriptor_test.go | 2 +-\n 2 files changed, 2 insertions(+), 2 deletions(-)\n```\ngoogle.golang.org/grpc\n```\nproto$ cd $GOPATH/src/google.golang.org/grpc\ngrpc$ git show --stat\ncommit 5e3de3f21767d4e0e8dd0ee8383bbbfe172d35de\nMerge: eca2ad6 b7d24ba\nAuthor: Menghan Li menghanl@google.com\nDate:   Mon Nov 28 14:26:00 2016 -0800\nMerge pull request #995 from yangmls/master\n\ncorrect errors in documentation\n\nDocumentation/grpc-auth-support.md | 4 ++--\n 1 file changed, 2 insertions(+), 2 deletions(-)\ngrpc$ \n```\n. I'm not aware of any vendored gRPC in my project. Could you please elaborate?\nAlso I did regenerate the .proto sources.. One workaround I used is to build the commit prior to the one in the OP:\ncd $GOPATH/src/google.golang.org/grpc\ngit checkout 727a60e\ngo install. ",
    "clintberry": "You need to be like Google and work out of one giant repo for all projects... :troll:. It doesn't have to be a \"deep dependency tree\". If you have one dependency generated by a different version it fails. Ultimately, I think for Go we won't be importing generated code, but each client/consumer will have to generate the client code for their own project. No dependency and no sharing... for better or for worse. Which may be what you were suggesting.... ",
    "tbillington": "I am also encountering this error.\nI ran these commands as per https://github.com/grpc/grpc-go#compiling-error-undefined-grpcsupportpackageisversion\nshell\ngo get -u github.com/golang/protobuf/{proto,protoc-gen-go}\ngo get -u google.golang.org/grpc\nThen this command to generate the go proto files\nprotoc --go_out=plugins=grpc:. *.proto\nIn one of the generated files there is these lines\ngo\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the proto package it is being compiled against.\n// A compilation error at this line likely means your copy of the\n// proto package needs to be updated.\nconst _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package\nAnd the error says\n/Users/trent/src/github.com/abc/proto/go/def/def.pb.go:195: undefined: grpc.SupportPackageIsVersion3\nRunning protoc --version gives libprotoc 3.1.0\nI tried the fix suggested by @erjoalgo but that didn't work.. Thanks @iamqizhao . Issue was an old copy of protoc-gen-go in my path which was in front of github.com/golang/protobuf/protoc-gen-go  :) my mistake.. ",
    "orian": "I've encountered this when using: https://hub.docker.com/r/grpc/go\ndocker run --rm -it -v $PWD/proto:/data grpc/go \\ \n  protoc --go_out=plugins=grpc,import_path=data:/data /data/data.proto\nIt seems that grpc/go image is kind of old. Thus I've downloaded it from: https://github.com/grpc/grpc-docker-library/blob/master/1.0/golang/Dockerfile and recreated. Probably it would make sense refresh the image.\ngo get -u github.com/golang/protobuf/{proto,protoc-gen-go}\ngo get -u google.golang.org/grpc. ",
    "jab": "I just hit this while going through Google's \"Building a gRPC service with Node.js\" codelab, see https://codelabs.developers.google.com/codelabs/cloud-grpc/index.html?index=..%2F..%2Findex#3\nFiled an issue in googlecodelabs/cloud-grpc#5 but posting here too in case anyone here is interested or can help. Thanks!. @MakMukhi I did that already and still get the same error. Is there anything else you recommend I try? Thanks for trying to help!. @MakMukhi I just moved aside my entire $GOPATH, reinstalled from scratch, and am getting the same error. Can you not reproduce? Does this show that the issue is not on my end, but rather upstream? Anything else I should try?\n```\n ~ $ echo $GOPATH\n/Users//golang\n ~ $ mv -v $GOPATH $GOPATH.bak\n/Users//golang -> /Users//golang.bak\n ~ $ which -a protoc-gen-go\n ! ~ $ go get -u -v github.com/golang/protobuf/{proto,protoc-gen-go}\ngithub.com/golang/protobuf (download)\ngithub.com/golang/protobuf/proto\ngithub.com/golang/protobuf/protoc-gen-go/descriptor\ngithub.com/golang/protobuf/protoc-gen-go/plugin\ngithub.com/golang/protobuf/protoc-gen-go/generator\ngithub.com/golang/protobuf/protoc-gen-go/grpc\ngithub.com/golang/protobuf/protoc-gen-go\n ~ $ which -a protoc-gen-go\n/Users//golang/bin/protoc-gen-go\n ~ $ go get -u -v google.golang.org/grpc\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nget \"google.golang.org/grpc\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc?go-get=1\ngoogle.golang.org/grpc (download)\ngithub.com/golang/protobuf (download)\nFetching https://golang.org/x/net/context?go-get=1\nParsing meta tags from https://golang.org/x/net/context?go-get=1 (status code 200)\nget \"golang.org/x/net/context\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/context?go-get=1\nget \"golang.org/x/net/context\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net?go-get=1\nParsing meta tags from https://golang.org/x/net?go-get=1 (status code 200)\ngolang.org/x/net (download)\nFetching https://golang.org/x/net/http2?go-get=1\nParsing meta tags from https://golang.org/x/net/http2?go-get=1 (status code 200)\nget \"golang.org/x/net/http2\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/http2?go-get=1\nget \"golang.org/x/net/http2\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/http2/hpack?go-get=1\nParsing meta tags from https://golang.org/x/net/http2/hpack?go-get=1 (status code 200)\nget \"golang.org/x/net/http2/hpack\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/http2/hpack?go-get=1\nget \"golang.org/x/net/http2/hpack\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/idna?go-get=1\nParsing meta tags from https://golang.org/x/net/idna?go-get=1 (status code 200)\nget \"golang.org/x/net/idna\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/idna?go-get=1\nget \"golang.org/x/net/idna\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/lex/httplex?go-get=1\nParsing meta tags from https://golang.org/x/net/lex/httplex?go-get=1 (status code 200)\nget \"golang.org/x/net/lex/httplex\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/lex/httplex?go-get=1\nget \"golang.org/x/net/lex/httplex\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/trace?go-get=1\nParsing meta tags from https://golang.org/x/net/trace?go-get=1 (status code 200)\nget \"golang.org/x/net/trace\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/trace?go-get=1\nget \"golang.org/x/net/trace\": verifying non-authoritative meta tag\nFetching https://golang.org/x/net/internal/timeseries?go-get=1\nParsing meta tags from https://golang.org/x/net/internal/timeseries?go-get=1 (status code 200)\nget \"golang.org/x/net/internal/timeseries\": found meta tag main.metaImport{Prefix:\"golang.org/x/net\", VCS:\"git\", RepoRoot:\"https://go.googlesource.com/net\"} at https://golang.org/x/net/internal/timeseries?go-get=1\nget \"golang.org/x/net/internal/timeseries\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/codes?go-get=1\nParsing meta tags from https://google.golang.org/grpc/codes?go-get=1 (status code 200)\nget \"google.golang.org/grpc/codes\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/codes?go-get=1\nget \"google.golang.org/grpc/codes\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc?go-get=1\nParsing meta tags from https://google.golang.org/grpc?go-get=1 (status code 200)\nFetching https://google.golang.org/grpc/credentials?go-get=1\nParsing meta tags from https://google.golang.org/grpc/credentials?go-get=1 (status code 200)\nget \"google.golang.org/grpc/credentials\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/credentials?go-get=1\nget \"google.golang.org/grpc/credentials\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/grpclog?go-get=1\nParsing meta tags from https://google.golang.org/grpc/grpclog?go-get=1 (status code 200)\nget \"google.golang.org/grpc/grpclog\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/grpclog?go-get=1\nget \"google.golang.org/grpc/grpclog\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/internal?go-get=1\nParsing meta tags from https://google.golang.org/grpc/internal?go-get=1 (status code 200)\nget \"google.golang.org/grpc/internal\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/internal?go-get=1\nget \"google.golang.org/grpc/internal\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/metadata?go-get=1\nParsing meta tags from https://google.golang.org/grpc/metadata?go-get=1 (status code 200)\nget \"google.golang.org/grpc/metadata\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/metadata?go-get=1\nget \"google.golang.org/grpc/metadata\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/naming?go-get=1\nParsing meta tags from https://google.golang.org/grpc/naming?go-get=1 (status code 200)\nget \"google.golang.org/grpc/naming\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/naming?go-get=1\nget \"google.golang.org/grpc/naming\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/stats?go-get=1\nParsing meta tags from https://google.golang.org/grpc/stats?go-get=1 (status code 200)\nget \"google.golang.org/grpc/stats\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/stats?go-get=1\nget \"google.golang.org/grpc/stats\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/tap?go-get=1\nParsing meta tags from https://google.golang.org/grpc/tap?go-get=1 (status code 200)\nget \"google.golang.org/grpc/tap\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/tap?go-get=1\nget \"google.golang.org/grpc/tap\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/transport?go-get=1\nParsing meta tags from https://google.golang.org/grpc/transport?go-get=1 (status code 200)\nget \"google.golang.org/grpc/transport\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/transport?go-get=1\nget \"google.golang.org/grpc/transport\": verifying non-authoritative meta tag\nFetching https://google.golang.org/grpc/peer?go-get=1\nParsing meta tags from https://google.golang.org/grpc/peer?go-get=1 (status code 200)\nget \"google.golang.org/grpc/peer\": found meta tag main.metaImport{Prefix:\"google.golang.org/grpc\", VCS:\"git\", RepoRoot:\"https://github.com/grpc/grpc-go\"} at https://google.golang.org/grpc/peer?go-get=1\nget \"google.golang.org/grpc/peer\": verifying non-authoritative meta tag\ngolang.org/x/net/context\ngolang.org/x/net/http2/hpack\ngolang.org/x/net/idna\ngolang.org/x/net/internal/timeseries\ngoogle.golang.org/grpc/codes\ngolang.org/x/net/lex/httplex\ngoogle.golang.org/grpc/credentials\ngoogle.golang.org/grpc/grpclog\ngolang.org/x/net/trace\ngoogle.golang.org/grpc/internal\ngolang.org/x/net/http2\ngoogle.golang.org/grpc/metadata\ngoogle.golang.org/grpc/naming\ngoogle.golang.org/grpc/stats\ngoogle.golang.org/grpc/tap\ngoogle.golang.org/grpc/peer\ngoogle.golang.org/grpc/transport\ngoogle.golang.org/grpc\n ~ $ cd src/cloud-grpc/start/\n ~/s/cloud-grpc  master  start $ go run client.go\n_/Users//src/cloud-grpc/start/books\nbooks/books.pb.go:107: undefined: grpc.SupportPackageIsVersion3\n. @MakMukhi Thanks for continuing to debug with me. I tried your quick fix and here is what happened:\n$ pwd\n/Users//src/cloud-grpc\n$ head .git/config\n[core]\n    repositoryformatversion = 0\n    filemode = true\n    bare = false\n    logallrefupdates = true\n    ignorecase = true\n    precomposeunicode = true\n[remote \"origin\"]\n    url = https://github.com/googlecodelabs/cloud-grpc\n    fetch = +refs/heads/:refs/remotes/origin/\n$ git pull\nAlready up-to-date.\n$ cd start/books\n$ protoc -I . books.pb.go --go_out=plugins=grpc:.\n[libprotobuf WARNING google/protobuf/compiler/parser.cc:546] No syntax specified for the proto file: books.pb.go. Please use 'syntax = \"proto2\";' or 'syntax = \"proto3\";' to specify a syntax version. (Defaulted to proto2 syntax.)\nbooks.pb.go:30:1: Expected \";\".\nbooks.pb.go:53:1: Expected top-level statement (e.g. \"message\").\n```\nAnything else you suggest so I can get past this? Thanks again!. Thanks so much @MakMukhi, I think that did the trick! Added the exact patch to https://github.com/googlecodelabs/cloud-grpc/issues/5#issuecomment-276835746 in case it helps someone else in the future.. ",
    "akocmaruk": "Thanks for the review. I'll submit the update later today.. ",
    "ceywj": "I solve a similar problem ,\nsee https://github.com/grpc/grpc-go/releases, \ncoment on v1.0.4   version is SupportPackageIsVersion4.\nso when my protoc-gen-go  generate still SupportPackageIsVersion3 , it can not work .\nthen I download grpc-go v1.0.0,  the two version is match, it work again.. ",
    "jordanorelli": "I signed it!. ",
    "ewang": "@menghanl On further testing, it seems like it's due to debugging w/ IPython. IPython might be monkeypatching some underlying libraries that causes this. Closing this issue for now. Thanks!. @MakMukhi We're seeing something similar but may or may not be the same issue. We're on grpc-go-1.0.5/go-1.7.5 and are seeing lots of unclosed connections which seems to lead to the related goroutines not getting garbage collected.\nI think it might be due to client side not properly closing connections so the server still leave them open. I'm thinking that upgrading to the latest grpc-go and implementing the new server side keepalive should clean up those connections on the server side. Would you be able to confirm if the server side keepalives would help w/ clients that don't properly close connections?. @MakMukhi In my case, the clients are in python and the connections no longer exist there. They're only lingering on server side.. @MakMukhi It's odd to me as well. My only guess so far is that the Python/C-core is cycling channel connections frequently when activity is low, causing more broken connections which only gets cleaned up by the keepalive on server side after an hour.. @dfawley This affects users of the GRPC-JSON transcoding part more, since if we use google.protobuf.Any, when serialized to JSON, it adds additional @type annotations of the full type name, which in my opinion is leaking unnecessary details. It also prevents us from replacing legacy APIs using grpc-go+transcoding since we won't be able to come up with a message type that mirrors existing JSON errors.\nI guess this could exist as a separate filter for Envoy, but it seemed like a better idea to keep the errors contained within the gRPC server code and having the transcoders simply serialize to JSON using jsonpb.. ",
    "avnish30jn": "I am not using any debugger still i am getting this issue. could you please point me to the fix that worked for you ?\nI am trying to connect from python client to a go grpc server.\nI have a go client inside a docker container which works fine.. ",
    "mightyguava": "Sorry, that was accidental.... Hey @menghanl, I think this issue should remain open.\nLet me preface the following with: I understand this is totally user error. However, this is also unexpected behavior given the way the API contracts within the client stream code are phrased, so I believe this is a real bug. The proper fix may be to update documentation.\nThe documentation for SendMsg says:\n// SendMsg blocks until it sends m, the stream is done or the stream\n// breaks.\n// On error, it aborts the stream and returns an RPC status on client\n// side. On server side, it simply returns the error to the caller.\n// SendMsg is called by generated code. Also Users can call SendMsg\n// directly when it is really needed in their use cases.\n// It's safe to have a goroutine calling SendMsg and another goroutine calling\n// recvMsg on the same stream at the same time.\n// But it is not safe to call SendMsg on the same stream in different goroutines.\nThere's no mention of buffering here. If closing the ClientConn causes messages to not be sent, SendMsg is breaking its API contract.\nSimilarly, the only documentation I see in code around needing to call Recv() is:\n// Always call Stream.RecvMsg() to drain the stream and get the final\n// status, otherwise there could be leaked resources.\nThese documentation in no way imply that the message will not be sent. For the repro example and my actual current use case, which is a short-lived commandline tool, it's totally fine to leak resources.\nFinally, what exactly is the purpose of CloseSend(), if the proper ways to close the client connection and flush the pipes are:\n- cancel the context\n- call SendAndRecv(). ",
    "a-robinson": "Hi @rogeralsing - I'm also curious about grpc-go's behavior under contention, and couldn't help but notice that your SVGs are no longer available. . CockroachDB recently started using grpc-go v1.12.0 and have almost immediately seen this race happen in our race tests (https://github.com/cockroachdb/cockroach/issues/25641, https://github.com/cockroachdb/cockroach/issues/25643). https://github.com/grpc/grpc-go/commit/d0a21a334714105261074d48f3c598b0fa86affd appears to have introduced it, although it seems as though you're already aware.\nDo you have a particular fix in mind beyond just re-adding the mutex or using an atomic compare-and-swap?. I've sent out https://github.com/grpc/grpc-go/pull/2090 as a possible fix, but I'm not at all attached to it if you guys have other plans.. Great to hear, thanks @MakMukhi!. ",
    "rogeralsing": "I believe the behavior was due to pre Go 1.8 GC.\nAfter 1.8 was released, I am seeing much more consistent performance under heavy load.\nAlthough somewhat lower than the previous max, I assume this is because of a more evenly spread GC.\n. I see that I get the error information when I interact with the stream, but that is not visible to me until I send something, is there no way to get notified that there is a disconnect w/o me trying to write to the stream?. I'm not following?\nPending as in sending heartbeat messages myself ?. Ah, currently the contract is defined as:\nproto\n  rpc Receive (stream MessageBatch) returns (Unit) {}\nSo the client/sender has nothing to wait for really, but if I add a stream of whatever as a response then I could Recv on the response stream, right?. I must be super dense, I get how I can do that on the server side.\nI'm not getting how I can do this on the client/sender side.\ngo\nc := NewRemotingClient(conn)\nstream, err := c.Receive(context.Background(), state.config.callOptions...)\nif err != nil {\n    return err\n}\n//can I do something here to monitor that the stream which we are Sending to is open/broken?\nstate.stream = stream. Thanks.\nBidirectional stream + go routine waiting for Recv did the trick.\n. For anyone interested in the actor model for Go, we now have support for watching remote actors and endpoint termination due to the above fix.\nhttp://proto.actor/docs/golang/eventstream#remote-termination. ",
    "mattklein123": "Is what true? Can you summarize the question?. @louiscryan any chance you could help us get this in the queue to be reviewed? . @MakMukhi yes that is correct. The \"standard\" way of handling this is for the server to send 2 HTTP/2 GOAWAY frames. The first has max stream ID, so new streams are still accepted. Then the server waits for some period of time (typically RTT), and then sends the real GOAWAY, then stops listening.. > I would expect to see a service unavailable when trying to establish a connection instead of making an RPC call.\nNot necessarily. If Envoy attempts to send a request, and it is reset (even with refused stream), Envoy will return a 503. One could argue that Envoy should retry if refused stream reset is returned, but that's orthogonal to this. This is a very common problem in HTTP/2 servers.\n\nAlso, even if two GoAways were sent the client behavior would remain the same; close the current connection and try to establish a new one. Since the server would stop listening after the 2nd GoAway the client would still see a 503 when it attempts to connect agan.\n\nNot quite. This is part of a graceful draining protocol. By the time GOAWAY is sent, a new listener is up and expecting new connections. This is actually how Envoy itself works. It does the double GOAWAY dance during drain and hot restart.. > Also going through the server code quickly it looks like the in-flight RPCs would just be accepted by the server even after having sent a GoAway. This might be a bug that we'd have to look into; I believe we should be more restrictive and not accept those RPCs(this pertains to the race you mentioned above).\nBTW, if this is actually true and you do accept new streams after sending GOAWAY, then the issue is definitely something else and we can close this issue and will need to do more debugging. I had assumed that this was the issue though since it fit.. > About the server accepting in-flight RPCs even after having sent a GoAway: I'll dig deeper into that and perhaps write a simple reproduction of it to make sure this is the case.\nI think this would be very useful to have confirmation of. I looked at the code very briefly, and I think you are correct, but would love to know for sure. (Also this means that when this issue is \"fixed\" the double GOAWAY will definitely need to be implemented).\nAssuming that the server is accepting new streams, there must be some issue in terms of how our code is doing the hot restart dance in some cases. We will need to debug further.. ",
    "liangzhiyang": "thx.. thanks. @lyuxuan \nthanks. ",
    "jsha": "@menghanl: So, assuming that I log all errors returned from Serve(), how do I accomplish a clean shutdown that does not log any errors?\nAlso, is there any situation in which Serve() will return without an error? If not, that should be documented.. >  As soon as you create at least as many streams allowed by the server, you have hit the limit.\nCorrect, that's exactly what I meant.\n\nwhy set a limit in the first place?\n\nGolang's HTTP/2 library sets a default limit of 250: https://github.com/golang/net/blob/master/http2/server.go#L56. gRPC inherits that limit.\nI am indeed planning to increase the limit from the default. However, I'd like future visibility into whether I'm hitting that new, higher limit. I have visibility into other parts of my system, like how long RPCs are taking, via https://github.com/grpc-ecosystem/go-grpc-prometheus (which uses interceptors). However, since the waiting happens after the client interceptor has already handed off control to the gRPC library, this particular source of latency falls between the cracks and is not currently possible to measure.. The info approach seems like the simplest and fastest, and would meet our most basic visibility needs: we could set an alert to go off when that line shows up in our logs.\nOne similar but slightly nicer alternative would be to record the time before and after this for loop, so we could print something like fmt.Sprintf(\"RPC %s spent %s blocking on stream availability; check MaxConcurrentStreams\", callHdr.Method, time.Since(loopBegan)). This would allow someone to immediately identify how bad the problem is (milliseconds or seconds).\nThe most thorough intervention would allow some way to graph the effect of blocking on available streams. For instance, the client could set a header to indicate when the request was created (before any in-client blocking). A server interceptor could examine that header, and compare it to the current server time to calculate an \"rpc_lateness\" stat: The different between when the client wanted to make the request, and when the server got it (subject to the vagaries of clock skew, of course).\n\nOut of curiosity, is gRPC-Go performance something that you guys care about. If so, we'd love to hear what scenario does your service run in?\n\nYep! I work on https://github.com/letsencrypt/boulder/, the server for Let's Encrypt, a free Certificate Authority. We use gRPC extensively and care about its performance. So far, performance is great! This is the first snag we hit. We have about 7 different gRPC services. Some of those services call other services in turn. Some are moderately low-latency, like our storage service, which fronts a database and has a timeout of 5 seconds. Others are very high-latency, like our validation service, which makes requests out to the Internet and has a timeout of 90 seconds.. Another interesting approach, perhaps even better, would be to allow the server to expose its current number of concurrent streams on a per-client basis. This would allow us to graph that number over time and alert if it comes within half of the limit, so that we could scale up well in advance of hitting any latency issues.. > Additionally, looking at your code(part of it) looks like you are using gRPC's native server implementation which defaults to math.MaxUint32 for MaxConcurrentStreams. How does your client get throttled to go HTTP2 server's default limit?\nInteresting. I didn't consciously choose a native server implementation vs a non-native one. What controls that? Is that something that's changed since v1.1?. Ah, I think there may be a bug with setting maxStreams to MaxUint32:\nhttps://github.com/grpc/grpc-go/blob/db0f0713e3a7f295e167341b621b10cce1fb9ffe/transport/http2_server.go#L131-L139\nNote that in the default case, maxStreams is set to MaxUint32, but that is not sent as a settings frame. That works fine if you assume the underlying HTTP/2 library isn't sending a settings frame for MaxConcurrentStreams, but in this case the Go library is sending a default value of 250.\n. There's no proxy. Here's a branch where I set up a toy client and server to test the behavior: https://github.com/letsencrypt/boulder/compare/chillcli. At first I used it with the default limit and verified the blocking behavior. Then I used grpc.NewServer(... grpc.MaxConcurrentStreams(1000),...) to increase the limit, and verified that that fixed the blocking behavior (so long as concurrent requests were below 1000).. Excellent, thanks for the explanation! I will work on upgrading our gRPC dependency. We'll need to do a little work since there was a change to balancers and/or certificate handling that broke our balancer/validator. I'll check back in once I've landed the fix.. FYI, I upgraded our gRPC dependency, and used the sample client/server I linked earlier. I can confirm that even without setting MaxConcurrentStreams explicitly, I can run a very large number of RPCs concurrently, suggesting that MaxConcurrentStreams is defaulting to MaxUint32 as expected. Thanks for the help! I'll leave this ticket open to track the topic of tracing.. I've filled out the CLA now.. invoker\n. handler\n. ",
    "kchristidis": "@menghanl Thank you. Do you happen to know approx. where in the codebase this is being handled, so that I can review?\nTo your question: I haven't bumped into any issues, but wanted to make sure that this case is covered.. (Thank you.). ",
    "F21": "I am also seeing something similiar, but in my case, I have only started using grpc since 708a7f9f3283aa2d4f6132d287d78683babe55c8. I am currently using Go 1.8Beta2.\nIn my case, I have a grpc server running as a http handler for an http server using ServeHTTP(). I also have mutual TLS authentication enabled and am using a self-signed certificate for the http server.\nThe set up is being used in an end-to-end test.\nThis is what is being logged:\n2017/01/12 09:18:13 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/01/12 09:18:13 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:18:14 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:18:16 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:18:19 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:18:22 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:18:29 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:18:41 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\nIf I start the server and use time.Sleep() to wait before dialling to the server, I still get these errors, which is really strange. According to the timestamps, these happen upon the first connection. Some how, the connection does succeed and the test passes.. I don't know if this is related. I have a grpc server and client running for an end-to-end test.\nAfter starting the client and asking it to connect to localhost:443, I get these errors:\n2017/01/12 09:13:15 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/01/12 09:13:15 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:13:16 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:13:18 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:13:20 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:13:24 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:13:32 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\n2017/01/12 09:13:43 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: dial tcp [::1]:443: getsockopt: connection refused\"; Reconnecting to {localhost:443 <nil>}\nThe errors appear to be transient and the tests pass fine. I tried adding a time.Sleep() after starting the server, but I always get those in the logs, no matter how long the duration is.. When will this be released?. ",
    "amosbird": "cool, thanks!. ",
    "presotto": "Perhaps a bit more is  happening.  At 0s latency grpc is still at most 1/10th the speed of https. Does grpc adjust window to rtt?. I believe you really want different bytes for each message, not just an\ninitial random setting.\nOn Tue, Jan 10, 2017 at 2:41 PM, Andrew Gerrand notifications@github.com\nwrote:\n\nThis change enables compression for GRPC and uses a randomly-generated\npayload to defeat the effect of the compression:\ndiff --git a/grpcbench/main.go b/grpcbench/main.go\nindex 101677d..c03ac0f 100644\n--- a/grpcbench/main.go\n+++ b/grpcbench/main.go\n@@ -1,6 +1,7 @@\n package main\nimport (\n+       \"crypto/rand\"\n        \"crypto/tls\"\n        \"flag\"\n        \"fmt\"\n@@ -41,6 +42,7 @@ func main() {\n                                grpc.WithBlock(),\n                                grpc.WithTimeout(3 * time.Second),\n                                grpc.WithInsecure(),\n+                               grpc.WithCompressor(grpc.NewGZIPCompressor()),\n                        }\n                        conn, err := grpc.Dial(*addr, opts...)\n                        if err != nil {\n@@ -59,7 +61,12 @@ func main() {\n            ctx := context.Background()\n\n\nmsg := strings.Repeat(\" \", *msgSize)\nrandomBytes := make([]byte, *msgSize)\n_, err := rand.Read(randomBytes)\nif err != nil {\nlog.Fatal(err)\n}\nmsg := string(randomBytes)\n                for i := 0; i < *numRuns; i++ {\n                        t1 := time.Now()\n                        var err error\n@@ -86,7 +93,7 @@ func main() {var server *grpc.Server\nif *useGRPC {\n\n\nserver = grpc.NewServer()\nserver = grpc.NewServer(grpc.RPCDecompressor(grpc.NewGZIPDecompressor()))\n            helloworld.RegisterGreeterServer(server, greeter{})\n    }\n    l, err := net.Listen(\"tcp\", *addr)\n\n\n\nThe numbers appear to be more or less the same:\n2.477774562s  32ms    GRPC\n2.462765459s  32ms    GRPC\n176.592079ms  32ms    HTTP/2.0\n69.485653ms   32ms    HTTP/2.0\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1043#issuecomment-271721362, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFM2PeI2jDuAtw4tioQcWaVe5UgC2tLZks5rRAkGgaJpZM4Le51s\n.\n. If they are using a learned dictionary compessor, you would only be sending\nthe data once.\n\nOn Tue, Jan 10, 2017 at 2:49 PM, Andrew Gerrand notifications@github.com\nwrote:\n\nHow would that change things?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1043#issuecomment-271723316, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFM2PWHWjZ7ZA-KVhq1AkaZFT42UFtpFks5rRAsJgaJpZM4Le51s\n.\n. I agree. Everyone in Australia should.\n\nOn Mon, Jun 5, 2017 at 2:43 PM, Jason E. Aten, Ph.D. \nnotifications@github.com wrote:\n\nI don't believe this should be closed until gRPC is as fast as http1 on\nthe 76msec benchmark; out of the box, with no tuning required.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1043#issuecomment-306318981, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFM2PTm0xvaAZkmDrd-Hus9s-G4pGcT-ks5sBHaNgaJpZM4Le51s\n.\n. \n",
    "rsc": "I checked in a variant of adg's code that shows the packet breakup and timing.\ngo get -d -u rsc.io/tmp/grpcbench\ncd $GOPATH/src/rsc.io/tmp/grpcbench\nbash run.sh\n\nIt only shows the output for 32ms round trip time. This is what HTTP/2 POST looks like (the actual output shows packet boundaries within a given millisecond as <-93+89+1024+1024+1024, but I've added all those up for easier reading):\n```\n0.036s\n    <-184\n0.079s\n    ->1297\n0.080s\n    <-93\n0.117s\n    ->51\n0.118s\n    <-12470\n0.119s\n    <-53573\n0.151s\n    ->56\n    <-38\n0.184s\n    ->120\n181.526851ms    32ms    HTTP/2.0\n0.184s\n    <-46\n0.217s\n    ->83\n    <-1024\n0.218s\n    <-64665\n0.250s\n    ->43\n65.878218ms 32ms    HTTP/2.0\n```\nThere is some connection setup in the first one, and then a whole bunch of uninterrupted data transfer. The second one omits the setup but still has the nice fast data transfer.\nCompare with GRPC, again two connections:\n```\n0.070s\n    ->9\n0.104s\n    ->13\n0.105s\n    <-65702\n0.138s\n    ->9\n0.171s\n    ->13\n    <-16393\n0.206s\n    ->13\n    <-16393\n0.243s\n    ->13\n    <-16393\n0.281s\n    ->13\n    <-16392\n0.316s\n    ->13\n    <-16393\n0.351s\n    ->13\n    <-15360\n0.352s\n    <-1024\n0.353s\n    <-9\n0.384s\n    ->13\n    <-16393\n0.417s\n    ->13\n    <-16392\n0.450s\n    ->13\n    <-16384\n0.451s\n    <-9\n0.483s\n    ->13\n    <-2048\n0.485s\n    <-14345\n0.516s\n    ->13\n    <-16393\n0.549s\n    ->13\n    <-16392\n0.582s\n    ->13\n    <-16393\n0.615s\n    ->13\n    <-16393\n0.648s\n    ->13\n    <-16393\n0.681s\n    ->13\n0.714s\n    ->13\n    <-16392\n0.747s\n    ->13\n    <-16393\n0.780s\n    ->13\n    <-16393\n0.813s\n    ->13\n    <-16393\n0.846s\n    ->13\n    <-16392\n0.879s\n    ->13\n    <-16393\n0.912s\n    ->13\n    <-16393\n0.945s\n    ->13\n    <-16393\n0.980s\n    ->13\n    <-16392\n1.014s\n    ->13\n    <-8192\n1.015s\n    <-8201\n1.051s\n    ->13\n1.052s\n    <-16393\n1.086s\n    ->13\n    <-16393\n1.120s\n    ->13\n    <-1024\n1.121s\n    <-15368\n1.154s\n    ->13\n    <-16393\n1.187s\n    ->13\n    <-16393\n1.220s\n    ->13\n1.221s\n    <-16393\n1.254s\n    ->13\n1.287s\n    ->13\n    <-16392\n1.320s\n    ->13\n1.321s\n    <-16393\n1.354s\n    ->13\n    <-16393\n1.387s\n    ->13\n    <-16393\n1.420s\n    ->13\n1.421s\n    <-16392\n1.458s\n    ->13\n    <-16393\n1.490s\n    ->13\n    <-16393\n1.527s\n    ->13\n    <-7168\n1.528s\n    <-9225\n1.560s\n    ->13\n    <-13312\n1.561s\n    <-3080\n1.598s\n    ->13\n    <-16393\n1.635s\n    ->13\n    <-16393\n1.671s\n    ->13\n    <-16393\n1.704s\n    ->13\n1.705s\n    <-16392\n1.737s\n    ->13\n    <-16393\n1.770s\n    ->13\n    <-16393\n1.803s\n    ->13\n    <-16393\n1.838s\n    ->13\n1.870s\n    ->13\n1.871s\n    <-16392\n1.904s\n    ->13\n    <-16393\n1.938s\n    ->13\n    <-13312\n1.939s\n    <-3081\n1.971s\n    ->13\n    <-16393\n2.004s\n    ->13\n    <-16392\n2.039s\n    ->13\n    <-16393\n2.072s\n    ->13\n    <-16393\n2.104s\n    ->13\n    <-16393\n2.139s\n    ->13\n2.140s\n    <-16392\n2.173s\n    ->13\n    <-16393\n2.205s\n    ->13\n2.206s\n    <-16393\n2.239s\n    ->13\n    <-16393\n2.273s\n    ->13\n2.274s\n    <-16392\n2.306s\n    ->13\n    <-34\n2.338s\n    ->13\n2.371s\n    ->23\n2.404s\n    ->13\n2.437s\n    ->14\n2.470s\n    ->13\n2.504s\n    ->33\n2.501003152s    32ms    GRPC\n2.507s\n    <-1024\n2.508s\n    <-64563\n2.538s\n    ->13\n2.571s\n    ->13\n    <-16393\n2.604s\n    ->13\n    <-15360\n2.605s\n    <-1033\n2.638s\n    ->13\n    <-16393\n2.671s\n    ->13\n    <-16392\n2.704s\n    ->13\n    <-7168\n2.705s\n    <-9225\n2.738s\n    ->13\n    <-16393\n2.771s\n    ->13\n    <-16393\n2.804s\n    ->13\n    <-4096\n2.805s\n    <-12296\n2.837s\n    ->13\n    <-16384\n2.838s\n    <-9\n2.871s\n    ->13\n    <-16393\n2.904s\n    ->13\n2.905s\n    <-16393\n2.937s\n    ->13\n    <-16392\n2.971s\n    ->13\n    <-16393\n3.004s\n    ->13\n3.005s\n    <-16393\n3.039s\n    ->13\n3.040s\n    <-16393\n3.072s\n    ->13\n3.106s\n    ->13\n    <-16392\n3.139s\n    ->13\n    <-16393\n3.171s\n    ->13\n    <-16393\n3.204s\n    ->13\n3.205s\n    <-16393\n3.238s\n    ->13\n    <-16392\n3.271s\n    ->13\n    <-16393\n3.304s\n    ->13\n3.305s\n    <-16393\n3.338s\n    ->13\n    <-16393\n3.371s\n    ->13\n    <-14336\n3.372s\n    <-2056\n3.405s\n    ->13\n    <-16393\n3.438s\n    ->13\n    <-16393\n3.471s\n    ->13\n    <-10240\n3.472s\n    <-6153\n3.505s\n    ->13\n    <-16392\n3.538s\n    ->13\n    <-16393\n3.571s\n    ->13\n    <-1024\n3.572s\n    <-15369\n3.605s\n    ->13\n    <-16393\n3.638s\n    ->13\n3.671s\n    ->13\n3.672s\n    <-16392\n3.705s\n    ->13\n    <-16393\n3.738s\n    ->13\n    <-16393\n3.771s\n    ->13\n    <-1024\n3.772s\n    <-15369\n3.805s\n    ->13\n    <-16392\n3.838s\n    ->13\n    <-16393\n3.871s\n    ->13\n3.872s\n    <-16393\n3.905s\n    ->13\n    <-16393\n3.938s\n    ->13\n    <-16392\n3.971s\n    ->13\n    <-1024\n3.972s\n    <-15369\n4.004s\n    ->13\n    <-16393\n4.041s\n    ->13\n    <-16393\n4.078s\n    ->13\n    <-16392\n4.113s\n    ->13\n    <-16393\n4.146s\n    ->13\n    <-16393\n4.179s\n    ->13\n    <-16393\n4.212s\n    ->13\n4.245s\n    ->13\n    <-16392\n4.278s\n    ->13\n    <-16393\n4.311s\n    ->13\n    <-16393\n4.344s\n    ->13\n    <-11264\n4.345s\n    <-5129\n4.377s\n    ->13\n    <-16392\n4.410s\n    ->13\n    <-16393\n4.443s\n    ->13\n    <-5120\n4.445s\n    <-11273\n4.476s\n    ->13\n    <-16393\n4.509s\n    ->13\n    <-16392\n4.542s\n    ->13\n    <-16393\n4.575s\n    ->13\n    <-16393\n4.608s\n    ->13\n    <-16393\n4.641s\n    ->13\n    <-13312\n4.642s\n    <-2048\n4.643s\n    <-1032\n4.674s\n    ->13\n    <-34\n4.707s\n    ->13\n4.740s\n    ->11\n4.772s\n    ->13\n4.805s\n    ->14\n4.838s\n    ->13\n4.872s\n    ->11\n2.367555959s    32ms    GRPC\n```\nIt looks like there are two problems. The first one is that GRPC appears to have added its own flow control on top of HTTP/2's flow control (on top of TCP's flow control), and it has a fixed 16 kB window. The second is that it looks like HTTP/2 POST is compressing while GRPC is not: in addition to having better (no added) flow control, the posts transfer only about 64 kB of data in total. Either that or the test is broken.\n. Can user client code change the fixed flow control window size?\nCan user client code enable compression?\n. Improvement, yes, but I wouldn't call it fixed. Even at a modest 8ms latency, GRPC (>=75ms) is almost 4X slower than HTTP/1.1 (>=20ms). At 32ms GRPC (>= 280ms) is 7X slower than HTTP/1.1 (>= 36ms).  Those used to be 40X and 30X, so yes improvement, but still more than users are going to want to give up.\n. ",
    "glycerine": "A one line diff that raises the defaultWindowSize by 2048 improves throughput by 5.5x when the actual real (not simulated) latency is 2.7msec; e.g. a 512MB total transfer chunked into 1MB messages goes from taking 32 seconds to taking just 5.8 seconds. \nNeedless to say, this makes a huge difference in the viability of actually using gRPC.\n```~/go/src/google.golang.org/grpc$ git diff\ndiff --git a/transport/control.go b/transport/control.go\nindex 64d22f8..5ce8cf3 100644\n--- a/transport/control.go\n+++ b/transport/control.go\n@@ -43,8 +43,8 @@ import (\n )\nconst (\n-       // The default value of flow control window size in HTTP2 spec.\n-       defaultWindowSize = 65535\n+       // 2048 * the default value of flow control window size in HTTP2 spec.\n+       defaultWindowSize = 2048 * 65535\n        // The initial window size for flow control.\n        initialWindowSize             = defaultWindowSize      // for an RPC\n        initialConnWindowSize         = defaultWindowSize * 16 // for a connection\n```\n@apolcyn @petermattis Any chance of getting that DialOption API (for setting intial and conn windows sizes) merged?. > PS Changing defaultWindowSize violates the HTTP2 spec. You should really be changing initialWindowSize and initialConnWindowSize.\nDone. \nI measured the same 5.5x improved throughput with that change. Now #1153 is spec compliant.. >  I think the main thing to finish off is the addition of a few tests and documentation comments.\nThe branch Peter made https://github.com/grpc/grpc-go/compare/master...petermattis:pmattis/configurable-window-size?expand=1 is pretty straight forward. @petermattis What tests and doc comments do you think are needed?. Could we (at the very least) start by fixing the initial window size setting? There's a super simple two line fix PR that can be merged today, it just needs someone with merge rights to do it -- \nhttps://github.com/grpc/grpc-go/pull/1153\nI've been using my fork with this change for a month in production. 5x better performance makes gRPC viable again.. @petermattis @andreimatei Did cockroachdb 1.0 ship with a workaround for this (#1043) problem?  If so, what was your workaround and how effective is it for WAN high-latency links?. I don't believe this should be closed until gRPC is as fast as http1 on the 76msec benchmark; out of the box, with no tuning required.. > It is important to close this issue since it contains misinformation (broken benchmarks that show gRPC's performance much worse than it is.)\nThe above is not a legitimate reason to close this issue--in fact just the opposite. The most recent measurements in this thread show a 3x throughput loss that has not been addressed, and thus the title is still accurate, and thus this remains, still, an open issue. The #1280 issue is not nearly as specific, and looses a ton of useful context for those trying to understand the defects of gRPC in its current form.. I just re-tested against tip (8de2dff78c3b968a51c99ec526d934f686537437), on an actual high latency 109msec (one-way) link, between New Jersey and Surry Hills, New South Wales, Australia. Changing the initial window sizes, as in the pull request #1153, doubles the obtained throughput. So #1153 was also pre-maturely closed without being implemented, and it gives a 2x throughput boost.\n. I tried this PR as a solution to the slow performance, but it just truncated my 512 message stream after 3 messages. It seems half-baked/buggy.. The EOF happens even when the clients are in two distinct processes, so its got to be something on the server side.. Nevermind. This was due to how I wrote the server. I was mistaken in thinking that a new instance of my structure would be instantiated for each of Serve's callbacks. Fixed now. Closing issue.. >    a change which left the defaults unchanged but made the stream and window sizes configurable\nI backed the multiplier back to 64x (4MB). I tried 32x but that was still loosing 8% compared to 64x on my 2.7msec latency link.\nOptions are great, but the defaults should be sane too. I would think. Because most users won't ever dig deep enough to discover the option.\n\nyou've likely hurt latency for small requests sharing the same connection\n\nThis comment I don't follow the reasoning, but I would like to. How would latency for small requests sharing the same connection be hurt by a bigger window; I don't see how their frames would be delayed since they are likely to be sent and reach the receiver sooner.\n. Travis tests in transport timeout at 64x, even though they pass locally on my laptop. Trying at 32x.. Yes, this (32x) looks good. It gives a 2MB initialWindowSize, and 32MB initialConnWindowSize. These seem to be pretty sane defaults, at least for my 2.7msec end-to-end wan setup.. This was not implemented by #1210, and so should not have been closed. \n1210 allows the defaults to be changed, but this PR actually improves the defaults. Hence they will less often need to be changed.\nI just re-tested this against tip, and on a high latency 109msec link, between New Jersey and Australia, these defaults double the throughput on tip (e.g. for https://github.com/grpc/grpc-go/issues/1043).. > a change which left the defaults unchanged but made the stream and window sizes configurable\nI backed the multiplier back to 64x (4MB). I tried 32x but that was still loosing 8% compared to 64x on my 2.7msec latency link.\nOptions are great, but the defaults should be sane too. I would think. Because most users won't ever dig deep enough to discover the option.\n\nyou've likely hurt latency for small requests sharing the same connection\n\nThis comment I don't follow the reasoning, but I would like to. How would latency for small requests sharing the same connection be hurt by a bigger window; I don't see how their frames would be delayed since they are likely to be sent and reach the receiver sooner.. what algorithm is this trying to implement? if estimate > b.estimate would seems to guarantee that one only ever increases, and never decreases.\nWhat I would think you would want here is an exponential moving average; to also maintain a standard deviation in an online fashion, and then give variance weighted estimates by combining the two using Satterthwaite approximation so that you aren't assuming constant variance. Here are examples of what I mean.\nhttps://github.com/glycerine/go-sliding-window/blob/master/rtt.go#L54\nhttps://github.com/glycerine/go-sliding-window/blob/master/stddev.go#L80\nhttps://github.com/glycerine/go-sliding-window/blob/master/sender.go#L677. ",
    "andreimatei": "(another CockroachDB dev here)\nJust making sure everybody's aware of the seemingly ongoing work in #1126 around BDP detection.\nFWIW, when we've forked grpc to increase the window size for CockroachDB, I've played with some values for the per-stream and per-conn windows sizes and with some values I got a failure in TestServerWithMisbehavingClient.\nWhen making the two values equal, I got a timeout in a different test.\n. CockroachDB 1.0 shipped with a 2MB stream window and a 32MB connection window. This let us use enough bandwidth over WAN links for our needs, but I don't have any numbers.\nThe problems that we have now, though, is that it's too easy to saturate a connection window with slow stream that exhaust their windows, and then new \"high priority\" RPCs don't get any window.\nWe really want a dynamic connection window, but don't really know what to do. More discussion in\nhttps://github.com/cockroachdb/cockroach/issues/14948. ",
    "robpike": "I don't believe this should be closed until the process of setting the window size is automatic and the process of making gRPC run fast is clearly documented.. Thanks, @dfawley, that sounds like a great plan.. ",
    "mjgarton": "Should I assume that since this bug is closed, there are no remaining issues with latency significantly affecting throughput?\nI only ask because from @dfawley s comments it sounds like there is more to do.\n@dfawley did you get around to creating the \"few extra issues\" that you mentioned?  If so can you link them please?\n. ",
    "aseure": "I'm experiencing the exact same error message as @rsc by using cloud.google.com/go/storage for writing files to Google Cloud Storage. It only happens from time to time and doesn't seem to impact the write operations in any way.\nWhat is the exact meaning of this error?. ",
    "junghoahnsc": "I'm also seeing this error message while using gcloud pubsub.\nBut after this error occurs, the subscriber stops pulling a new message.\nShould pubsub library handle this error? Or does the grpc reconnect automatically?\n. ",
    "steeve": "We have no real idea on how to reproduce it, we just saw it in the wild on one of our apps (deployed to 10-15 users so far), thanks to the log..\nI can tell you that the call was in fail fast mode, but alls the calls after this one failed from then on.. I signed it!. On a side note, this fixed the GC issues we had in production. Processes went from pretty much doing GC all the time back to a normal pattern.\nScheduling was also significantly improved as traces showed up to 800 runable goroutines all the time down to ~20 during GC, and 0 most of the time.. Seems it was not pushed, fixed. ",
    "dbudworth": "@menghanl Any chance your changes are going in soon?  I have to keep moving the patch forward when I pull due to current code being super-noisy when used behind a tcp balancer.. makes sense, updated.... ",
    "bioothod": "Yes, you are right, server is written in golang and client is written in python\nHere is a 6-line test app I use: https://github.com/grpc/grpc-go/issues/1054\nSince client application exits, all its sockets are pretty much closed, and it would be a little bit weird if new client would suddenly get a connection from some other client and server would want to write reply into that old connection.. I've posted detailed description of the problem and its root (or something closer to root - grpc-go servers fails to tolerate bufio.Flush() error when it writes not whole buffer into the connection`\nhttps://github.com/grpc/grpc-go/issues/1054. Client code (that's it):\n```\nchannel = grpc.insecure_channel('localhost:8080')\nstub = service_pb2_grpc.XXXStub(channel)\nreq = service_pb2.SearchRequest(content=['test'])\nreply = stub.SearchSecurity(req)\n```\nServer (simplified, dropped error checks and so on) :\n```\nlis, err := net.Listen(\"tcp\", \"localhost:8080\")\nsrv := grpc.NewServer()\npb.RegisterXXXServer(srv, fh)\nreflection.Register(srv)\nsrv.Serve(lis)\nfunc (fh XXX) SearchSecurity(ctx context.Context, req pb.SearchRequest) (pb.SearchReply, error) {\n    reply := &pb.SearchReply {\n        Securities: make([]pb.Security, 0, 0),\n    }\n    return reply, nil\n}\n``. This is the test I use: https://github.com/bioothod/grpc_test\nIt is not actually a grpc problem, but wine. I ran this test on linux and windows quite alot, and there were no problems, but it breaks quickly on wine.\nThis patch https://github.com/golang/go/issues/18537 helps, but problem persists - it just started to appear not after 5-10 requests, but after like 100k-500k, so there is a problem somewhere in the golang network stack when running on wine, which I will try to resolve. I'm not sure whether both these issues I opened (https://github.com/grpc/grpc-go/issues/1054, https://github.com/grpc/grpc-go/issues/1053) should not be closed by you, but since I found these while testing grpc, I filled them here first.. And the bug seems related to grpc code which doesn't tolerate 'short writes', i.e. when underlying connection writer'sFlush()` method is invoked and it fails to flush the whole buffer into the network.\nSo, here is a story.\nThere is grpc-go server and one or more python clients. Server runs under wine and this was the first suspect, but it turns out to be unlikely.\nServer frequently fails to accept new client, i.e. it accepts it and immediately closes connection with abovementioned error \ngrpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: short write\"\nThis is because of this Flush() method returns error: https://github.com/grpc/grpc-go/blob/master/transport/http_util.go#L408\nIt returns error because underlying bufio.Flush() invokes net.Conn.Write() which returns smaller number of bytes than buffer size: https://golang.org/src/bufio/bufio.go?s=14020:14050#L564\nThere are at least 2 places where this error appears:\nhttps://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L127\nhttps://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L499 (the latter ends up with use of closed network connection message described at https://github.com/grpc/grpc-go/issues/1053)\nBug is 100% reproducible under wine using https://github.com/bioothod/grpc_test\n$ make windows\n$ ./grpc_server\n$ python client.py\nrequires editing Makefile to specify correct go compiler and goroot. Yes, you are right, it could be closed, it went down to wine. ",
    "abdullah2993": "Same issue over here.... @menghanl i was saying something like this should be either in the docs or repo also I'm not sure about my implementation whether this is the best way to do it or not...an example?. @menghanl Thats great. ",
    "fengttt": "Also experienced this on 1.8.1, Centos 7.3.  Happening one out of 4 or 5 times.. ",
    "geeknoid": "Thanks for the follow-up. For now, I've added a 50ms delay in our test\nclient code to eliminate the issue internally for us. But that's not a good\nlong-term story of course...\nOn Wed, Feb 1, 2017 at 10:14 AM, MakMukhi notifications@github.com wrote:\n\nHey Martin,\nWe're working on adding levels to logging. This will reduce the verbosity\nfor more restrictive levels. The PR is under its way. However, I don't have\na time line for you at this moment since the guy working on this is out on\nvacation.\nRegards,\nMak\nOn Tue, Jan 31, 2017 at 5:49 PM, Martin Taillefer \nnotifications@github.com\nwrote:\n\nI have a client/server setup using bidirectional streaming. When the\nclient calls stream.CloseSend and then immediately closes the underlying\nTCP connection, the server produces the following spurious log entry:\nhttp2_server.go:320] transport: http2Server.HandleStreams failed to read\nframe: read tcp [::1]:9091->[::1]:38241: read: connection reset by peer\nAfter this, everything works fine. If I insert a 1 second delay on the\nclient between the call to CloseSend and the call to close the socket,\nthen\nthe log message doesn't get emitted.\nThis message is polluting our logs, that's why I'd like to get rid of it.\nThis is with Go 1.7.4\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1059, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ATtnR2aPvnyGNBwF3AXg_\nqZbnIgyoOWkks5rX-SMgaJpZM4LzZ2S\n.\n\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1059#issuecomment-276735448, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AVucHWb4v7ziBT-DB6YMOeZnBfLBZjpFks5rYMuKgaJpZM4LzZ2S\n.\n. This is in fact what we're doing at the moment, but this is unsatisfactory for a few reasons:\n\n\n\nWe want the state to apply across multiple methods. This is not possible if the state is associated with a specific gRPC stream.\n\n\nWe\u2019ve had to implement our own out-of-order handling logic since gRPC streaming doesn't support this.\n\n\nThere is a single context associated with a whole stream so client-side deadlines apply to the life of the whole stream and not to individual messages. If we want to support deadlines per-message, we have to roll our own solution.\n\n\nSimilarly, if we want to support cancellation semantics per message, we have to roll our own solution.\n\n\nLong-lived streams such as we use them effectively defeat dynamic load balancing. Even though there might be a fleet of servers available to service requests, our client would be bound permanently to a single specific server.\n\n\nThey're generally hard/clumsy to use for clients.\n\n\nSo we want to switch to a unary API surface to address all of these issues. We realize there will be an increase in RPC overhead (measured to about 25% or so) assuming we can continue to have a stateful protocol. If we have to eschew statefulness, then our measured overhead is about 2.5x more than what we have today with stateful streaming.\n. ",
    "rbewley4": "I ran your test program and it also failed to reproduce the issue for me. I even tried with some variations of timeout/deadline to no avail. \nTo add some more context to the problem, this issue arises when using Google Cloud PubSub via https://github.com/google/google-api-go-client. When it occurs, the panic from ContextErr originates from a goroutine inside of the pubsub (?) package, which cannot be recovered from userland. This is a huge thorn in my side, as it routinely crashes my application. Regardless of why it is happening, I think the panic in ContextErr should be replaced with an error return, similar to the other cases. \nThe code I posted in my previous message fixes the problem for me, so I am stumped as to why your test program did not reproduce the issue. Maybe the pubsub package is doing something naughty like creating its own Context implementation. Anyway, I shall endeavor to find cause the problem.. ",
    "linjohn": "Yes, and this seems only to happen under Windows. If I cross compile it to Linux it works without error.. ",
    "fho": "@linjohn also happens on Linux when I establish a connection and then call con.Close()\ntransport: http2Server.HandleStreams failed to read frame: read tcp [..]: read: connection reset by peer. signed. @menghanl Yes, we are using in the application our own logger.\nThe GRPC Info log messages are in general very useful. I wouldn't want to discard them in general.\nThe info log messages that are logged from other parts of the library are much less verbose.\nThe ones from balancerWrapper and ccBalancerWrapper seem to intended for debugging. They are producing both a message with very similar information for the user.\nFor example when a balancer tries to reconnect to a service that is unreachable the following is logged:\ngrpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: Error while dialing dial tcp 172.25.0.18:50060: getsockopt: no route to host\"; Reconnecting to {172.25.0.18:50060 0  <nil>}\nbalancerWrapper: handle subconn state change: 0xc42023e330, TRANSIENT_FAILURE \nccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xc4203e2ea0 \nbalancerWrapper: handle subconn state change: 0xc420218e20, CONNECTING \nccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xc42006f080 \nbalancerWrapper: handle subconn state change: 0xc420212730, CONNECTING \nccBalancerWrapper: updating state and picker called by balancer: CONNECTING, 0xc4203e2900 \ngrpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: Error while dialing dial tcp 172.25.0.20:50060: getsockopt: no route to host\"; Reconnecting to {172.25.0.20:50060 0  <nil>} \nbalancerWrapper: handle subconn state change: 0xc42025c1e0, TRANSIENT_FAILURE \nccBalancerWrapper: updating state and picker called by balancer: TRANSIENT_FAILURE, 0xc42052b920 \ngrpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: Error while dialing dial tcp 172.25.0.18:50060: getsockopt: no route to host\"; Reconnecting to {172.25.0.18:50060 0  <nil>}\nI think having thegrpc: log message in this scenario is sufficient.\nINFO log messages also should not contain memory addresses.. @menghanl putting them behind a verbosity level (lvl 2?) seem to be a good solution to me.\nI'm not sure how to implement it in the best way. Do you maybe have a suggestion?\nI could only find transport/log.go which does it so far.\nFor the balancer we would basically need the same but maybe for another verbosity level.\nI wouldn't want to duplicate the transport/log.go code.\nWhat do you think about adding variants of the log functions in grpclog.go that accept the verbosity level as first argument and only log them if grpclog.V() returns true?\nE.g.:\nfunc InfoVf(verbosity int, format string, args ...interface{}) {                                                                                           \n        if V(verbosity) {\n            logger.Infof(format, args...)                                                                                                                \n        }\n}. ",
    "rodaine": "I signed it!. > Is any TransportCredentials used in your case? Or you are using an insecure connection?\nThis would mostly be used with WithInsecure, but it's setup to allow you to still have a TransportCredentials and override the Authority independent of it.. @menghanl @jboeuf I can modify this to only work if dialed WithInsecure if that would be the safer option? . @jboeuf @menghanl updated the PR to only allow the :authority override for insecure connections. . @menghanl anything still outstanding here?. Done. ",
    "willfaught": "I don\u2019t have access to that code anymore. Sorry.. ",
    "ThibaultRiviere": "Make sens thanks :). ",
    "thinxer": "It seems that framer.writeData calls http2.Framer.WriteData, which will eventually flush some data into the framer's underlying net.Conn. net.Conn.Write cannot be canceled by the context and this gRPC call will block if the underlying net.Conn hangs.\nOne solution is to queue all frame writes into a channel, and use a goroutine to flush the frames from the channel into the underlying connection. This way, we can do select on the send channel and avoid blocking if the underlying connection hangs.. ",
    "ashwanthkumar": "Hello, is there any other update or workaround to this issue? . ",
    "tonobo": "What about read connections? I've a stuck connection in readFrame\n```\n0x4297f6        internal/poll.runtime_pollWait+0x56                                                             /usr/lib/go-1.9/src/runtime/netpoll.go:173\n0x498f8d        internal/poll.(*pollDesc).wait+0xad                                                             /usr/lib/go-1.9/src/internal/poll/fd_poll_runtime.go:85\n0x49900c        internal/poll.(*pollDesc).waitRead+0x3c                                                         /usr/lib/go-1.9/src/internal/poll/fd_poll_runtime.go:90\n0x499da9        internal/poll.(*FD).Read+0x189                                                                  /usr/lib/go-1.9/src/internal/poll/fd_unix.go:126\n0x50c7f1        net.(*netFD).Read+0x51                                                                          /usr/lib/go-1.9/src/net/fd_unix.go:202\n0x51ed8c        net.(*conn).Read+0x6c                                                                           /usr/lib/go-1.9/src/net/net.go:176\n0x540f5a        bufio.(*Reader).Read+0x30a                                                                      /usr/lib/go-1.9/src/bufio/bufio.go:213\n0x475335        io.ReadAtLeast+0x85                                                                             /usr/lib/go-1.9/src/io/io.go:309\n0x4754a7        io.ReadFull+0x57                                                                                /usr/lib/go-1.9/src/io/io.go:327\n0x78f6ca        .../vendor/golang.org/x/net/http2.readFrameHeader+0x7a                 /root/.go/src/.../vendor/golang.org/x/net/http2/frame.go:237\n0x790033        .../vendor/golang.org/x/net/http2.(*Framer).ReadFrame+0xa3             /root/.go/src/.../vendor/golang.org/x/net/http2/frame.go:492\n0x7c1542        .../vendor/google.golang.org/grpc/transport.(*http2Client).reader+0xc2 /root/.go/src/.../vendor/google.golang.org/grpc/transport/http2_client.go:1167\n```. ",
    "bjwatson": "SGTM. ",
    "vrecan": "Is this a known issue, seems odd that no one cares about a data race?. ",
    "23doors": "Encountered same issue with 1.3.0 when using a logger that discards output.\nWhen I disabled tracing, it worked just fine.\n```\nWARNING: DATA RACE\nWrite at 0x000004b6c4d0 by goroutine 87:\n  github.com/user/repo/pkg/lb.TestWorker()\n      /Users/V/Go/src/github.com/user/repo/pkg/lb/worker_test.go:36 +0x6b\n  testing.tRunner()\n      /usr/local/Cellar/go/1.8.1/libexec/src/testing/testing.go:657 +0x107\nPrevious read at 0x000004b6c4d0 by goroutine 83:\n  github.com/user/repo/vendor/google.golang.org/grpc/grpclog.Printf()\n      /Users/V/Go/src/github.com/user/repo/vendor/google.golang.org/grpc/grpclog/logger.go:87 +0x3e\n  github.com/user/repo/vendor/google.golang.org/grpc.(*ClientConn).resetAddrConn.func1()\n      /Users/V/Go/src/github.com/user/repo/vendor/google.golang.org/grpc/clientconn.go:613 +0x159\nGoroutine 87 (running) created at:\n  testing.(T).Run()\n      /usr/local/Cellar/go/1.8.1/libexec/src/testing/testing.go:697 +0x543\n  testing.runTests.func1()\n      /usr/local/Cellar/go/1.8.1/libexec/src/testing/testing.go:882 +0xaa\n  testing.tRunner()\n      /usr/local/Cellar/go/1.8.1/libexec/src/testing/testing.go:657 +0x107\n  testing.runTests()\n      /usr/local/Cellar/go/1.8.1/libexec/src/testing/testing.go:888 +0x4e0\n  testing.(M).Run()\n      /usr/local/Cellar/go/1.8.1/libexec/src/testing/testing.go:822 +0x1c3\n  main.main()\n      github.com/user/repo/pkg/lb/_test/_testmain.go:48 +0x20f\nGoroutine 83 (finished) created at:\n  github.com/user/repo/vendor/google.golang.org/grpc.(*ClientConn).resetAddrConn()\n      /Users/V/Go/src/github.com/user/repo/vendor/google.golang.org/grpc/clientconn.go:621 +0xae3\n  github.com/user/repo/vendor/google.golang.org/grpc.DialContext.func3()\n      /Users/V/Go/src/github.com/user/repo/vendor/google.golang.org/grpc/clientconn.go:403 +0x20c\n``. Ok nevermind. In my case, disabling tracing didn't to a thing. It seems like a separate issue caused by setting grpc logger when -race is tested (set bygrpclog.SetLogger(log.New(ioutil.Discard, \"\", 0))`)\nUPDATED: confirmed it as unrelated and pretty much issue with my own code. Setting grpclog logger once per package in TestMain fixed it. Disregard previous comment.. @menghanl Sure thing. Managed to reproduce it with helloworld example.\nhttps://github.com/23doors/grpc-graceful-stop-bug\nPut steps to reproduce in readme there.. ",
    "adelez": "@dgquintas. Based on an email discussion, it seems that the solution is still undecided.. @muxi . Done. The usage is inconsistent. In line 130 and several other places, MD is used. I'll change all of them to md then.. Another question: do you think using the object name \"md\" by itself is sufficient or should more readable name be used?. Done.. Done. Done.. Done. Also changed the two other mds to MDs.. Done.. Done.. ",
    "xoebus": "Unavailable is an error that the codes package claims can be retried. How should a credentials.TransportCredentials implementation return a fatal error to stop any unnecessary retries?. @menghanl Great, thanks.. ",
    "garye": "Can we submit this now that https://github.com/grpc/grpc/pull/9748 has landed?. @menghanl We're adding a string without special characters or newlines, and we leverage this code:\nhttps://github.com/GoogleCloudPlatform/google-cloud-go/blob/master/bigtable/bigtable.go#L779\nLooks like @dfawley has a PR to fix this for us:\nhttps://github.com/GoogleCloudPlatform/google-cloud-go/pull/627/files\nWhat's the status of that?. ",
    "poy": "OK thanks. I suspected as much, but wasn't sure if streaming involved an 'ack'.. If I do not have a bidirectional stream, would CloseAndRecv suffice?. Thanks that worked.. ",
    "rfoyard": "See #690 and #728. ",
    "phifty": "Brilliant! That was exactly what I was looking for. Thanks a lot!. ",
    "sathieu": "Is there a dumb dialer which doesn't use proxy envs? (for https://github.com/moby/moby/issues/34825). Thanks. ",
    "bharatnc": "Usually, the user-agent can be included on the client side and extracted from the server side. You can add a user-agent using the WithUserAgent(\"user-agent\") function that grpc provides when making the dial connection.\nserver := grpc.Dial(grpc_server, WithUserAgent(\"user-agent\",))\nOn the server side, you can extract it out from the incoming context meta-data:\nmD, _ := metadata.FromIncomingContext(ctx)\nwhere mD consists of key,val pairs and the way you would want to get hold of the user-agent is using its's key.\nmD[\"user-agent\"]   . ",
    "prune998": "I'm not sure I understand how to cleanly stop a Grpc Streaming feed from the client out of the comments here.... Yes, please. Sounds a lot of people are wondering what is the right way to handle this.\nThanks a lot !. ",
    "jadekler": "Hi @prune998 - this issue is currently under development, and the specifics are still a bit up in the air. I'm happy to post implementation details here after-the-fact if you'd like!. Also closed #1672 (seems to be similar). The linked PRs should cover all codes. There were many instances of the first two codes in the list (NOT_FOUND, INVALID_ARGUMENT) which is why I created separate PRs, but then the rest of the errors had hardly any occurrences heh. Oh well. Let me know if you'd like me to merge them.. @menghanl I believe this is finished, now. Shoot good point. I'll fix that tn. > Keep retrying with exponential backoff even if the error is non-temporary.\nWhat's the rationale here? In the case of missing certs, this could be painful.\nedit: referencing https://github.com/grpc/grpc-go/issues/1768. Small note - in my example I use pubsub.NewClient, which if you follow its calls down ends up at grpc.DialContext: pubsub.NewClient hangs on transport.DialGRPC -> gtransport.Dial -> dial -> grpc.DialContext. Ah ok, thanks @menghanl . @dfawley Ah, good point, thanks. I read cc.ctx, cc.cancel = context.WithCancel(context.Background()) and conflated it with ctx (passed in as a param).. I'm rather confused about the failing tests in travis ci; go test ./... -v works locally. Are these flaky tests?. @dfawley \ud83d\udc4d  thanks - if it fails again I'll dig and see what I can find. \ud83d\udc4d  thanks for these comments, I'll work on this tonight. @dfawley I took a look through some of the golang.org/x/oauth2 implementers (of TokenSource interface), and generally it looks like there's no consistent error statuses being returned. For example:\n\n\njwtSource returns fmt.Errorf(\"oauth2: cannot fetch token: %v\", err) for connection problems, as well as fmt.Errorf(\"oauth2: cannot fetch token: %v\", err) for unmarshalling problems\n\n\ncomputeSource returns whatever error metadata.Get returns, which may be any client.Do / ioutil.ReadAll / etc error\n\n\nCertainly nothing like the defined error structs in this library, unless I'm mistaken (which I may be!). Would you prefer me to continue by returning Unauthenticated for now?. \ud83d\udc4d  sounds good, give a shout if anything else I can do on this PR. Should https://github.com/grpc/grpc-go/blob/master/Documentation/grpc-metadata.md be updated as part of this?. Force pushed a new way to do this using AppendToOutgoingContext. This commit needs some work - maybe some cleanup, and certainly some tests in metadata_tests.go. Thoughts on the general approach though?\nAs the commit suggests, added AppendToOutgoingContext function, which should be more efficient than FromIncomingContext + Join (two MD{} and double for loop (in Join) down to just one MD{}). However, FromIncomingContext and FromOutgoingContext should be slower, as they now merge the slice of MD into a single MD.. @dfawley Yes, I certainly shall. Should I benchmark the adding as well as retrieving operations (FromOutgoingContext), or are we only concerned with the adding?. Benchmark added. Here are the results @dfawley:\n$ go test -bench . -benchmem\ngoos: darwin\ngoarch: amd64\npkg: google.golang.org/grpc/benchmark/metadata\nBenchmarkMetadataWithContextCopying-8        1000000          1317 ns/op        1288 B/op         13 allocs/op\nBenchmarkMetadataWithContextAppending-8      3000000           400 ns/op         520 B/op          7 allocs/op\nBenchmarkFromOutgoingContext-8               2000000           656 ns/op         464 B/op          5 allocs/op\nPASS\nok      google.golang.org/grpc/benchmark/metadata   4.944s\nSeems to take about 1/3 the time. I'm going to work on your proposal to store RawMD data structure this next week, which I'm sure will be even nicer.\nedit: Updated with benchmem\nedit2: Updated with benchmark for FromOutgoingContext (relevant in future benchmark comparisons). @dfawley See caa627bc28ce8a41bc7b4c7b5e574e5c340eff21 for rawMD implementation. Is that what you had in mind? Here are the bench results:\n$ go test -bench . -benchmem\ngoos: darwin\ngoarch: amd64\npkg: google.golang.org/grpc/benchmark/metadata\nBenchmark_AddingMetadata_WithContextCopying-8                                1000000          1262 ns/op        1280 B/op         12 allocs/op\nBenchmark_AddingMetadata_AppendToOutgoingContext_WithoutPriorMetadata-8     10000000           202 ns/op         176 B/op          4 allocs/op\nBenchmark_AddingMetadata_AppendToOutgoingContext_PriorMetadata-8             5000000           381 ns/op         304 B/op          7 allocs/op\nBenchmarkFromOutgoingContext-8                                               1000000          1097 ns/op         912 B/op         10 allocs/op\nPASS\nok      google.golang.org/grpc/benchmark/metadata   6.949s\nAppending metadata is about an order of magnitude faster than the originally reported method (New + Join + Pairs). Extracting it with FromOutgoingContext is quite slow in comparison, though, since that's where the joining is happening now.. @dfawley AH that's why you had it public. Aha, ok, I'll update the transport.. @dfawley If I understand you correctly you're suggesting we replace FromOutgoingContext with context.Value(..) in the transport. Let me know if that's wrong! If not:\nTo pull RawMD out of the context and use directly it in the transport, we'd have to make mdOutgoingKey{} public (so that http2_client.go can do raw, ok := ctx.Value(MdOutgoingKey{}).(RawMD). We'd also have to have RawMD's two fields be public. What are your thoughts on that direction - is changing the visibility of mdOutgoingKey acceptable to you?. Ah that makes sense. I like the former, because it means we can make private RawMD (which feels like a thing that would ideally be internal), and lets the internals of RawMD change in the future without breaking users. @dfawley Updated. Also added key checking in AppendToOutgoingContext that's present in Pairs (len(kv)%2==0, strings.ToLower()). The benchmark for a small number of kv suggest very similar performance; I figured that has to happen either there or in the transport, so may as well do it in the natural place. Benchmarks updated per comments. If we're feeling good about this approach I'm happy to update the documentation, too.. @dfawley Updated per your comments, squashed commits, cleaned up tests, and updated documentation. Ah, this is much smaller than expected. Consolidating this and the other PR.. Do you mean DataLoss? The FailedPrecondition changes in test files reflect the change in http_util, whereas the DataLoss removals are standalone.. Closing this to consolidate into a new PR (that includes all code changes).. I'm on it!. \ud83d\udc4d good deal - thanks for the @ notification for me to look at! I need to be a lot more careful with append... This is now done.. Whoops, turns out the lowest PASS when you runs go test just refers to the last package. Fixing tests... Closing this, the new solution will be a wholesale rewrite of what's in this PR.. One of the 6 builds in travis are failing. The reason is a little cryptic though, but I think it's telling me there's a gofmt error in clientconn.go. Is that right? And if so, how do I repro that? I'm running go1.10 and gofmt -s -d -l . brings up nothing.\n\n. @dfawley Is there a way to see what in clientconn.go caused it to fail? Or, repro locally? gofmt -s -d -l . returns nothing for me, but not sure if I missed a step.\nedit: vet.sh also didn't bring up any errors. Confusing :/. > Missed that. Are you not running 1.10.1? That's what travis used.\nUpdated and rebased with master, still not seeing anything with vet.sh. I took your advice and removed -l; we'll see what travis prints.. @MakMukhi Updated from recursion to loop\nedit: I should be able to get to your other couple of comments next couple of days. Just realized I was pushing the wrong branch heh. The updates I promised above are now actually here :). @MakMukhi PTAL.\n\nRefactor to use reader goroutine.\nRename back to resetTransport.\nVarious review comment fixes and cleanups.\nFound a home for resolving (in nextAddr).\nRebased with master.. Now that location differs, I'm closing this and re-opening in a new, clean PR.. @dfawley PTAL and let me know if these new names make sense. More info: it appears as though the following line\n\ngit ls-files \"*.go\" | xargs grep -L \"\\(Copyright [0-9]\\{4,\\} gRPC authors\\)\\|DO NOT EDIT\" 2>&1 | tee /dev/stderr | (! read)\nWorks perfectly until the (! read). It seems as though the set -e fails on read and never gets to the 'not' (! read) part.\nThis is probably some subtly different behavior in mac that we need to account for.\nAlso, the sed further down seems to not work either. Haven't investigated that.. (very unfamiliar with proto-gen-go) I'm looking at pubsub.pb.go, one of our autogenerated clients. I assume this is generated with proto-gen-go. It does not appear to have any method-level comments. @dfawley wrt documenting ctx usage, did you have in mind putting some method-level comments on these generated files, or?. @dfawley PTAL. Dupe https://github.com/grpc/grpc-go/pull/2099\n@lyuxuan . Is there a way to switch your review from \"blocking the merge\" to \"general feedback\"? I feel like I'm getting a bit into the rabbit hole and don't want to hold you up.. So, just as a note, I gave a lot of me-specific opinions. All my questions are answered in the documentation, so I approved, but I think it would be great for someone else (@menghanl ?) to also give a look-over.. Did a final round of cleanup. (no real content change) Does this look ok?\n\n. Shoot. I just hit squash and merge. I can open a new PR to fix those comments @dfawley.. https://github.com/grpc/grpc-go/pull/2184. All comments addressed; PTAL at your convenience.. Wait, ignore that, github was hiding ones from me. Fixing the outstanding ones!. @fastest963 Did this occur immediately after deploying, or some time after deploying? I'm unfortunately having a hard time reproducing this, which would help verify a solution I have for it. Any additional information you might have would be hugely helpful! :). Hi @fastest963, sorry for the slow reply; gophercon week last week. That would be extremely helpful. So that I don't waste your time, I'd like to try once more this week to reproduce it, and if I'm still unable to I'd love to take you up on that offer.. Hi @fastest963. I've done some more work to the branch, and have tested it by pushing an app to GKE and having it use cloud.google.com/go/pubsub to publish a message to pubsub every second. I've left it running for about a day and have not noticed any memory issues:\n\n\nMy main.go is here.\nMy Dockerfile is here.\nThe \"hopefully fixed\" branch containing this code is transport_refactor in https://github.com/jadekler/grpc-go.git\n\nWould you mind trying this yourself to see if you run into the issue again (with the branch mentioned above)? If you're still willing, another pair of eyes would be greatly appreciated.. Super, thank you. I've pushed another - more intensive - pubsub benchmark that does 10,000 publishes a second while creating topics every second, and the memory has been consistently and happily flatlined for the past 6 hours or so. If, however, you do see memory spikes or crashes tomorrow, I'd be very interested in hearing more about your setup so that I can further tune mine to match yours.\nThanks again for your help.. Woohoo! Thank you. Look forward to hearing further.. Woot! Thanks @fastest963 !. @uschen Note - transport errors can come about in a variety of ways. This particular issue is probably gRPC-specific, whereas I suspect yours may be with the google-cloud-go library. If you're seeing errors in google-cloud-go, we welcome you to file a new issue at https://github.com/GoogleCloudPlatform/google-cloud-go/issues so that we can triage.. One note: I've noticed occasionally the Go 1.6 test times out on some of the last tests (see: https://travis-ci.org/jadekler/grpc-go/jobs/427902282). This only started happening after I rebased against https://github.com/grpc/grpc-go/commit/acd1429515d896f3368863417570ff1bbb21b8ab.\nI suspect that we're running up against the 5m test timeout, but I might be wrong. I don't believe anything in the https://github.com/grpc/grpc-go/commit/acd1429515d896f3368863417570ff1bbb21b8ab rebase could have affected this PR much, so it's my best guess. One more data point: Go 1.6 tests are the slowest, and they are currently taking about ~8m in their entirety. I could see 3 minutes of that being downloading / setup and almost-5 minutes of that being test itself.\nAnyways, thought I'd mention.. PTAL at your convenience. I have approval, but it's before I consolidated all my PRs into this one.. Done. Consolidated into https://github.com/grpc/grpc-go/pull/2322. Done. Consolidated into https://github.com/grpc/grpc-go/pull/2322. Consolidated into https://github.com/grpc/grpc-go/pull/2322. Consolidated in https://github.com/grpc/grpc-go/pull/2322.. PTAL, new and updated to fix both bugs, and a new test that introspects the transport state periodically instead of relying on channelz.. Tested successfully on travis with -run TestStateTransitions -count 1000 -race -v: https://travis-ci.org/jadekler/grpc-go/builds/441973011?utm_medium=notification&utm_source=email. > > In clientconn_state_transitions_test.go, each time we set up a server to send\n\n\nSETTINGS, we should also set up the server to read. This allows the client\nto successfully send its SETTINGS.\n\nThis condition may happen in practice, though. Should we have a test case for each server behavior?\n\nI thought about that, but here's what that test looks like:\n\nSet up server to Accept and send preface receipt, but not read\nExpect either READY or TRANSIENT FAILURE [1]\n\nIs that what you'd expect, given [1]? Or is there some behavior that's defined for this?\n1: Apparently, you can do n, err := t.conn.Write(clientPreface) without a server having called Read and get a successful response. Some of the time, though, you get the error:\ntransport: failed to write client preface: write tcp 127.0.0.1:39066->127.0.0.1:38566: use of closed network connection\n(despite the fact that lis.Close has not been called yet)\nI naively would have expected a synchronous pipe on conn.Write until the server does conn.Read, or minimally an async-with-timeout API.. @dfawley PTAL. Added PipeListener, a new test to make sure client does not enter READY until both prefaces are sent/received, and code to do the needful.\nTests on travis with -run TestStateTransitions -count 1000 https://travis-ci.org/jadekler/grpc-go/builds/442905877.. PTAL. cc @jadekler . PTAL (used go mod tidy). TODO: tryUpdateAddrs should cause the addr loop to reset (it currently does not), and needs a test to assert this.. TODO finished.\n@dfawley Could you PTAL when you have a moment?\nAlso, have noticed a failure in TestHealthCheckHealthServerNotRegistered after -count 1000. Will investigate next week.. Rebased and took care of some of the easier code review comments. The rebase includes hybrid mode at the moment, but it should be easy to remove in the future. More changes to come for the rest of those comments next time I find a chunk of time.. @dfawley, @lyuxuan helped me resolve a few problems vis-a-vis healthchecks. Code has changed a bit around createTransport invocation. PTAL when you have a moment.. Apologies - you're totally right. This turns out to be our retry logic. Closing.. For posterity: some more digging reveals that we're getting codes.Unavailable, which the client library retries. Doing some digging as to why this code is chosen... That SGTM.. Shoot. Sorry. Done, and rebased.. SGTM.. > The change looks good. Can you think of a way to test this to make sure the timeout is being applied correctly?\nDone. I piggy-backed on getMinTimeout, which isn't great, but is a way to do this test with signals instead of timing. Let me know what you think.\nedit: And, I did verify that this correctly fails without the change and passes after the change.. FWIW, this just started popping up in our CI. I have no idea what could have precipitated it. It doesn't seem as though a recent commit to grpc-go would cause it, but...?. @menghanl Should I update this CL to take the whole lock (not just the read lock)?. Done. https://github.com/grpc/grpc-go/pull/2689. > Can you add a(n unexported) DialOption instead, and make this per-ClientConn instead of global? \nDone. That's much nicer. I know you mentioned this to me before - sorry for forgetting that approach.. @dfawley \ud83d\udc4d  done and done; I'll take a closer look at the error structs, apologies for that. I think the interface becomes more complex and suffers for it (e.g., I could see a user asking, if I have two values for one key, do I attach them as k1, v1, k1, v2 or k1, v1, v2).\nOn the other hand, users of this feature are probably pretty advanced-level and aren't averse to reading the usage comments? I'm happy to implement though, just wanted to share my thought on it. I'll have the change up soon.. Good call! I'm on it.. \ud83d\udc4d  that makes sense to me!. What's the method by which interceptors do this at the moment? Is there an interface to conform to, or is it as simple as defining a new type of CallOption that has some GetMetadata method receiver on it? (Please feel free to guide me towards a doc if I'm missing it! Poked around a bit but couldn't see anything). \ud83d\udc4d  sgtm. I tested locally before and it seemed fine. It evaluates to an empty array.. Correct. I'll have it cleaned up in the squash.. @dfawley Ok, but are you ready for something really spooky? https://play.golang.org/p/L6wpbrX_cyd \ud83d\ude06 . Ah shoot you already got it, sorry haha. I should have refreshed page. Yeah really strange behavior. Ahhh. Super good note, thanks. I'll submit a patch to address this. @dfawley I've looked into this behavior of append a bit more, and although I don't 100% understand it yet, it seems to be that the pointers are carried around and modified (the \"not 100% understanding\" part is, I don't fully get how far those get propagated and so on). I've believe I've fixed the problem by essentially doing a copy and append; append([]MD{md}, mds...).. \ud83d\udc4d  good point. I actually just included this benchmark as a comparison to what was happening in the original issue (#1390), in which case the original md would be populated. Since that'll no longer be the preferred approach to appending metadata, I reckon this benchmark can be deleted during the squash - thoughts?. Yes, this can be removed now; I'll do so. In a previous commit, allocation + merging would happen if a non-nil metadata was provided IIRC, which was slower. Thanks for the catch.. \ud83d\udc4d  I'll keep it. Done. Done (moved to transport). Done. Done (converted to non-named function). Done - good call, I'll try to keep an eye out for these cases. Done. One test with no existing MD, and one test with existing MD + two appends. Done. Done. Ah, shoot, more leftovers from before. Thanks for spotting. Done. Also changed the verbiage of the \"intended for grpc use only\" note aboveFromOutgoingContextRaw` to match other notes in the package.. Wow, serious monday brain not reading the second part of that. Done - moved to NewOutgoingContext. \ud83d\udc4d  on 'produced by'.\nHowever, regarding the second part - I'm confused by the \"otherwise\". Aren't all the errors (including the codes.Unknown ones I touched) \"produced\" by the status package via status.New? As in, in the first sentence you talk about errors, but the second you talk about codes.. Ahh ok - I think I see what you're going for. Would something like this make sense?\n\nAll errors returned by UnaryHandler will either be produced by the status package, or wrapped in a status.Status with codes.Unknown as the status code and err.Error() as the status message.\n\n^ trying to condense\nOtherwise, I'm happy to write it as you have it!. >  A UnaryHandler is something the user implements (it's their server's RPC handler), and we call\nAH shoot that's the part I was entirely missing. Thanks very much for the explanation! Incoming update.. Not sure if there's a better name for this?. What's the appropriate thing to do here (I assume grpclog.Error(err) is wrong)?. Right on \ud83d\udc4d . Thanks! I'll add it.. Could you clarify this? I believe the transport does not call onGoAway; only below in the for loop do we call onGoAway. Since the transport is destroy and re-created during resetTransport (inside ac.reconnect):\n\nOne of the two cases (onGoAway, onError) occurs. Let's assume onError\nonError calls ac.reconnect, who calls ac.resetTransport. Transport is recreated (old ac.transport.GoAway() is blown away with it, no matter if something wrote to it)\nFor loop on clientconn.go:911 wraps around again\nBrand new transport with no GoAway or errors on it, so goAway logic does not happen to the old transport. In the current implementation, the closures aren't passed into the transport. They're only invoked up at the connect() level. So, once a transport is reset (as part of onError or onGoAway), there is no chance for the closure to be called again on an old transport since the for loop is always concerned with the most recent transport.. Done. Naive way to get around needing the transports outside the lock (GracefulClose takes the lock), but not wanting to cause data races.\n\nIs there a better way to do this?. I couldn't figure out why this is happening, but sometimes I would get nil, true, nil which blows up a little below when we call t.NewStream. This conditional fixes the problem, but I thought I'd bring it up. I suspect it's a race between the transport closing - and getting deleted - and getTransport looking for, finding, and returning it.\nIf so, is this fix appropriate or should I make getTransport and transport deletion synchronized? Alternatively, should I document getTransport's race with closing transport? (again, all this predicated on my guess above). I'm not sure this solves the problem appropriately. AFAICT there's a race between the reader goroutine closing and onDeadline. This puts a plaster over the problem but I think there's a still a chance t.Close just below could happen incorrectly. Is that accurate? Is there a better way to handle this? Do I need to do more synchronization?. Is this appropriate? I'm a bit confused by the godoc in timer.Stop; it seems to suggest that we need to drain the channel, too. Is that right?. Done!. Done. Done. timer. I'll tighten it up.. Done. Done. Done. I believe that is the current behavior, no?. Good call. Done. Ah, never. Removed conditional. You're right. Done.. Just so I'm clear, since the diff is a bit hard to read; you're suggesting that we remove onDeadline and move the onDeadline logic into the transport reader goroutine by way of passing connectDeadline down to the transport?. \"client streaming RPCs\" could include bidirectional, which does not have CloseAndRecv right?\nCould you clarify how unidirectional is ambiguous? Are you referring to the fact that both clients and servers can unidirectionally stream? \nIf so, how about:\n\n\"For unidirectional RPCs, clients should ...\"\n\"For bidirectional RPCs, clients and servers should ...\". Whoops, I should and will remove this. Done. Done. What does \"that came in by\" mean?. s/maintains/keeps/ ?. \"Stream level flow control quota available\"?. So, loopyWriter has a control buffer and active streams. How do they differ? I know the latter holds streams with data to send, but what does the control buffer hold?\n\n(this is a question for me, not a passive-aggressive way to get you to update this comment - once I figure out the difference I may separately suggest an update to this). I would move this to the declaration of activeStreams (344).. I would remove this, or move it to the run() call.. > A documentation like this seems confusing given that a stream ends only with receiving either an EOF or an Error.\nI don't follow, could you elaborate? That detail is what this documentation attempts to address.\n\nFrom a user's point of view when or if a message is delivered should be opaque.\n\nThat would only be possible if individual messages (not frames) were acked. But, since they aren't, I don't think we can get around informing folks that they must wait for EOF or risk losing messages.\n\nPerhaps the documentation that we need is a stream's life cycle\n\nI think that would be valuable, but is separate from / not mutually exclusive with this documentation.. - How do the two relate?\n- Could you define \"control frame\"?\n- I know that somewhere in here is round robin behavior as well as splitting messages that are too large. Where do these two happen? IIRC it was processData with the control frame..?. Ok, thanks for this. I dug through the code some more. How about something like this (very rough draft):\n\nControl buffer is the mechanism by which other goroutines communicate with loopy. Control buffer holds frames - data frames, header frames, and so on. Separately, loop holds a linked list of streams, and each stream holds a linked list of frames to send for that stream. Once loopy receives a frame, it enqueues it into a stream.\nLater, as part of processData, loopy dequeues the next stream and peeks at the next frame to be sent. There is a limit to how much loopy can send each iteration; if the frame is too large, loopy will chop it up, send as much as it can, leave the rest of the frame to be sent next time loopy looks at the stream. If the frame is small enough, loopy sends it and dequeues it.\nSince loopy iterates over a list of a streams, sending one piece of stream data at a time, loopy's scheduling is described as round robin across streams.. My two goals:\n\n\nRemove the assumption that the message is delivered after calling SendMsg.\nEducate users that until they receive EOF, their message may not have been delivered. SendMsg seems a natural place to put an abbreviated warning about this since users expecting ack behavior are likely to look at SendMsg documentation.\n\n\nWhat I mean is perhaps changing the documentation from \"it blocks until the message is sent\" to \"blocks until a message is scheduled\" makes more sense, rather than warning users of what unexpected behavior may occur in the current implementation if they don't use the API right.\n\nDone, PTAL.. For context, this documentation is actually stuff that was already there. How about simplifying it to just, \"On error, SendMsg aborts the stream.\"? Perhaps that's what the previous documentation was shooting for? Alternatively, I'm happy to remove it entirely.. > I added documentation to controlBuffer\nChecked it out - it looks great!\n\nWe don't want to make a guarantee that it will stay this way.\nI don't see a user doing anything differently being equipped with this information.\n\nI agree with these points, but I don't think the purpose of documenting this code is for the user - I think it's for the developer who works on this code. The extent to which you want your code to be approachable to other developers should be the guide. Given how large and important of an open source project gRPC is, I think it should be pretty dialed in in terms of documentation and approachability.\n\nother than that the suggested documentation is already there\n\nA developer could put together all the disparate pieces, and look at some code, and figure out the whole story. I think it'd be pretty useful to save that time and confusion by handing the whole story on a platter. How about this simplified version that does away with the flow parts?\n\nLoopy receives frames from the control buffer. Each frame is handled individually; most of the work done by loopy goes into handling data frames. Loopy maintains a queue of streams, and each stream maintains a queue of data frames; as loopy receives data frames it gets added to the queue of the relevant stream. The scheduling of data frames to be written involves round robin process during which loopy sends up to a maximum amount of data from one stream at a time, as stream and connection-level flow control permits.. > Another idea, since the semantics are unfortunately different between client and server, despite having the same interface: let's document all of Stream's behavior in ClientStream and ServerStream and leave comments in Stream indicating that the user should find the documentation there instead. Stream probably isn't directly used anyway, so maybe this would be best?\n\nI think that's a great idea. Shall I do so, or would you prefer to? In either case, I'll close this PR.. Looks great!!. Done (copy paste from above).. Done.. Done.. Done. Good call.. That is really good. \ud83d\udc4d \nDone.. I like that a lot. \ud83d\udc4d \nDone.. Done.. Done.. Done, and applied similar change to ServerStream.. Done, and applied a similar change to ServerStream.. Done.. Who needs it! :)\nJk, added back in.. Done. (that make sense). nil, I assume?\n(changed it as such on that assumption). Right - this line alludes to that. I could more strongly state that by adjusting the line to:\n\"SendMsg does not wait until the message is received by the client. Users who need per-message acknowledgement are expected to implement it on top of gRPC.\"\nThough, I don't know how much value there is in that. I kind of think the brevity of just saying, \"we don't wait\" is enough of a warning. Thoughts?. Done. Good find.. Done. Good note - that's unexpected (to me).. Done. Ditto for ServerStream and Stream?. Done - added back in io.EOF documentation.\nHeh no prob :). Done.. Shoot, sorry to have missed this. Hm, good point - that just means the client has finished sending their messages. Bad copy paste. I'm not sure it's worth saying, but is it correct to say that once the server returns in the handler, all messages will have been sent successfully? Or is the promise still only that messages have been scheduled? In short, is there any way to ensure all server messages have arrived? (I think not, but want to make sure I ask)\nIn the meantime, I've simply removed the line about RecvMsg. I neglected to include the line about per-message acks, since it doesn't seem to add too much more.\n. Done. Removed. SGTM \ud83d\udc4d That would be nice. Future change sounds good. itself, via callbacks - added. Done. That's what Mak and I had thought, too. I think @menghanl had a reason for why this happens?\nI've removed this comment and the associated logic, for now.. So, this is only used in the case where ac.dopts.waitForHandshake is set. We basically pass a signal in the form of this specific err up to resetTransport that says, \"hey, waitForHandshake failed, go ahead and hard-bail out of creating this transport instead of doing another iteration\".. In conjunction with this, or instead of this?. You're right. Changed to connectDeadline. Went one step further and removed an unnecessary select that was using backoffDeadline - see https://github.com/grpc/grpc-go/pull/2219/commits/2e3a8d6bad33b216556e502a55431bb25f0be977. Now we should only be using backoffDeadline in nextAddr().. Done.. Hmm... we could do this. I don't love two things waiting for the same deadline (this timer + the reader). Following that train of thought, I wondered whether we can move the transport closing behavior out to here and not give the reader a deadline, but I don't think we're guaranteed a non-nil transport here. About to get off the bus, though. Will think on this more this weekend and maybe experiment.\nIf nothing else, it's not too bad to have two things waiting heh.. Looked into it a bit more, and the above is moot since we can kill the reader from clientconn (via t.Close). I've done the needful in https://github.com/grpc/grpc-go/pull/2219/commits/5687feffbf977c00dc0bd792b54927986bffb2a8.. Done https://github.com/grpc/grpc-go/pull/2219/commits/dd306ed4ed6a5ec8cc03a0b3cc6bd523d84362d5. Can't see where this comment is referring to anymore, but all these shutdown things were basically just pulled from the current impl. Let me know if there are any that looks suspicious to you in the current patchset.. No, it is as mentioned. Basically nextAddr will skip incrementing counters if it saw a successful handshake (because onPrefaceReceipt resets counters in that case). By setting it to true here, we make sure that the first iteration of the resetTransport loop does not increment counters (we want to start at 0, not 1).\nI'm not sure if there's a nicer way to do this... do you want me to look into simplifying this a bit? It's a bit arcane.. Done.. Done. Shoot, you're right.\nDone.. Done. Done (tldr: gracefulclose => close => onclose locks ac.mu). Yikes, good call. Done.\nTravis is probably green because that race doesn't come up very often. (I only observed the prior race in maybe 1 out of 5 CLs previously).. Done. I think this was an artifact from back when we were using conn.SetReadDeadline.. Done. Done. Not super reliable - maybe 1 in 1000. It relies on the race between addrConn past the if ac.state == SHUTDOWN before the state gets changed in ac.tearDown.. Done.. Wow, good spot. Done.. This doesn't seem to do anything AFAICT? I think you can entirely remove the variable.. Done.. So, because I now properly use withBackoff(infiniteBackoff{}), there's another problem here: the first backoff after a successful handshake is skipped and reconnection immediately happens.\nThat means that I have to add another Accept->close pair to the server to get to the infinite backoff (from close -> Accept -> signal blocks to close -> Accept -> client connects immediately -> close -> Accept -> backoff blocks).\nSo, now when I do close(killFirstConnection), the state transition is: \nREADY -> CONNECTING -> TRANSIENT FAILURE (no backoff) -> CONNECTING -> TRANSIENT FAILURE (infinite backoff)\nThis all happens really fast. When I try to write code that manually follows these five transitions, my test fails 50% of the time. I added sleep(50ms) to just skip over all of them. It's not great, but I don't have a better idea. It worked 100% of the time when I did -cpu 1,4 -count 1000 locally, FWIW.\nI did however go replace the sleep further below this with cc.WaitForStateChange, though, so there's that.. Done.. Just a pattern we use in google-cloud-go to compare what we got vs what we wanted to get. Do you prefer a different name?. Done.. Shoot. Done.. Done.. Done!. Hey! I just got an update out that I think mostly resolves this issue, but I need to take another pass over it to double check. In the meantime, let me know what you think.\nHere's the idea:\nI put state transition at the top of the retry loop (it used to be in nextAddr, but I think it is slightly more comprehendible outside). It fires when it considers it to be a reset of the addr list: when we have tried all the addrs and are about to start over at 0. This is also when we would backoff, which is also happening in nextAddr (in the same place this was happening).\nAll this ugliness is unfortunate, but I think it is or nearly is correct. In other news, I've made pretty good progress on our discussion of refactoring all this to be an addr loop nested inside a retry loop, with backoff and state transition happening after the fact. I think that approach is worth investigating more. (very WIP version here https://github.com/jadekler/grpc-go/tree/transport_cleanup). t.Helper appears to be Go 1.9+ unfortunately (https://blog.golang.org/go1.9). I've added if statements, though.. Ah - I didn't know that. A few questions:\n\n\nIs there a definitive flow diagram somewhere I can consult? I've looked around in grpc/grpc-go/Documentation and grpc/grpc/doc and didn't see anything at first glance.\n\n\nThe flow diagram would probably answer this, but is this the correct set of cases where we should transition into TRANSIENT FAILURE?:\n\n\n- Any time a READY connection experiences a retryable error (onClose and onGoAway).\n  - Any time we try and fail to dial the full list of addresses.\n^ if the above is correct, I think I can do:\nif ac.state == connectivity.Ready || (ac.addrIdx == len(ac.addrs)-1 && ac.state == connectivity.Connecting) {\n  ac.updateConnectivityState(connectivity.TransientFailure)\n  ac.cc.handleSubConnStateChange(ac.acbw, ac.state)\n}\nThis would cover:\n\nState is ready (if we got to the beginning of the retry loop with READY, that means onGoAway or onClose occurred)\n\nState is connecting and we reached the end of the address list (we tried to connect to all the addresses and none of them resulted in a READY, so we will enter TRANSIENT FAILURE, backoff, and retry the list). Done. Done. Done - it seems to do so in 8ce7e6babb5ad539d8448d25aaba0708af2626ed. Also, put the logic I suggested in with a minor addition of making sure we're also checking for first attempt.. This is unfortunate and probably needs to be changed. I thought about having the server also watch the notifications in testBalancer until it sees READY before doing this, but:\n\n\nYou'd have to set up a goroutine ahead of sending the preface receipt to avoid a race between the server starting to watch and the client changing state to READY.\n\nThe state change is currently just a single channel that's watched by the test on L147. Adding another watcher - the server - might means you'd have to have some way to send each state change to multiple watchers, which can get complicated.\n\n... and both of those could get a bit ugly. WDYT @dfawley ? . Done. Done. Done. Done. Fixed by making the test artisanal and causing it to wait for READY to be seen.. Done. Nice haha. Done, kind of: err needs a type, so for _, err := conn.Read.. Whoops, done.. Thought about doing that, but unfortunately we have that logic about \"very first connection attempt\" that's tied to successfulHandshake. \ud83e\udd22 \nThis seemed simplest until we sort out that whole mess. WDYT?. Done. Done. Done. You're right. Done.. You're right. I've replaced this with a much simpler design. PTAL.. > Why was this chan added?\nTo try to simulate the same behavior as net.Listener, which is that calling Close unblocks all Accepts.\nI've removed it and replaced it with a simple select in the dialer that causes an error to be returned if there's no corresponding Accept waiting for the dial. PTAL.. Done. Done. Done. Done. Done. By stream are you suggesting, stream, err := client.FullDuplexCall(ctx)?\nIf so, it does not appear to ever finish.\nedit: Is there something you're supposed to do to the stream other than server.Stop() to cause it to unblock?. Done. Done. Done. > Did you see problems calling context concurrent with a Send?\nI'm not sure I fully understand this question, but I did not see problems calling context's cancel function concurrently with a send, no. My problems were around calling CloseSend and SendMsg concurrently.\n\nDid you mean to put this on CloseSend?\n\nNo. The message applies to both CloseSend and SendMsg (they can't be used concurrently with each other), so I put it on SendMsg since there's already some discussion about concurrent calls there.. Shooooooooot. My fault entirely, thanks for being so tactful. Removed.. Done. Omg.. Done. . Fairly sure you don't need this (tested with -count 100), since you have WithWaitForHandshake. (the other location in this file does not have WithWaitForHandshake)\nWill shorten the test by .3s to remove it.. Because of this, the test ends up taking 9s. Can we drop it to 2*time.Second? Y'all probably have context on why it's so high elsewhere.. Ack, SG.. Thanks for the explanation!. Done. Done\nThanks, I previously had a way for it to go negative, removed it, and then forgot to fix this :). Done. Doesn't the fact that we're reading from cc prevent us from doing that? AFAICT we actually need both locks (I've made that change, PTAL). Done. SG. This is done, then, AFAICT. It is in the lock in the current code. Done. Ack. Ack. Second part done: see https://github.com/grpc/grpc-go/pull/2461/commits/484fc73cd8a3c1bbf499ed485bf8aa9bc355f4c0\nThat said - I don't think we should do #4. It seems outside the purview of updateConnectivityState. Also, there are many cases where you'd want to call updateConnectivityState without caring about addresses, such as in startHealthCheck.\nWDYT?. re: First part - could you expand on the ac.curAddr = nil idea? The retry loop would still pick the next address in the list regardless of what ac.curAddr is, right?. Done. Done. Done. You're right. This test is really just checking that the second server is consulted. I've simplified lis2 accordingly.. No, that's intentional. It prevents the client from shutting down too quickly. When the listener shuts down it will terminate the conn, so it's OK to not close individual conns.. Done. Done. Sorry, it's the client.Close that closes the conn, not the lis.Close. And, now that I look at this again, I don't think there's a problem with the client side shutting down too quickly (I had thought this test checks more than it does, but it really only checks that the connection was established o.O).\nComment added!. Done. - prefaceTimer: removed\n- onPrefaceReceipt: do you mind if I do that in a separate PR?. As mentioned over GVC, could we add a link or some documentation describing what this string should look like?. This is unfortunate. Users won't find out until much later (if at all). Can we provide a ValidateServiceConfig method or something like that?\nsc, err := grpc.ValidateServiceConfig(\"...\")\nif err != nil {\n    panic(err)\n}\nopts = append(opts, grpc.WithDefaultServiceConfig(sc)). s/tryAll/tryAllAddrs ?. Why add this block? It seems like very little code happens (and no i/o) until the next shutdown check in tryAll?. Ahh gotcha. Ack, your call about remove or keep, just thought I'd mention.. Done. Done. Ack. ",
    "lyuyun": "i found here: https://github.com/grpc/grpc-go/issues/403. ",
    "blakeyc": "Bingo, thank you. Yes forgot to switch the GOROOT when upgrading to 1.8.\nAll fixed thanks.. ",
    "mastahyeti": "Ah. This is because the main goroutine is exiting when Serve returns. I guess the main thread should call GracefulStop() also then to wait on the RPC to finish?\nI'll close this, since it doesn't seem to be a bug.. For posterity, here's how I fixed my example app.. It looks like the behavior was changed in https://github.com/grpc/grpc-go/pull/1485. Maybe there's a regression in how clients deal with the server closing.. ",
    "pawelkowalak": "@mastahyeti how does it fix your issue? I've just cloned your repo with the fix, and the client is still getting transport is closing.. A new version has been tagged (1.9.0) with this issue. Maybe it should be added to Known issues in release notes :). ",
    "hypnoglow": "This is finally fixed in #1734. ",
    "adam-26": "Using interceptors works. Thanks :). This being the case, i can just use a hard-coded value. Thanks. ",
    "rosun82": "Resolved. It turns out that Go gRPC sets :authority different than C++ client.\nThe C++ client by default set :authority to be the server + port, e.g., \"localhost:8080\", while\nthe go server sets just \"localhost\", without the port.\nWhen setting up C++ server using RegisterService(host, &service), the host would match C++ :authority, but not the Go one, causing the server throw back UNIMPLEMENTED error.\nIs it possible to make Go also set :authority to be exactly the same as host to avoid this problem?\n. ",
    "uguessmyid": "The Issue has been Resolved.. ",
    "lyuxuan": "@enocom Thank you for offering the help! Both example and documentation are great. Just to keep the example separate from the examples we already have.. ping @dfawley @menghanl . behavior documented.. Ping~~~. User is expected to handle the error returned from Send() and return the error in the handler if it's not nil. In this way, the client is notified immediately and will not hang. \nBecause of the potential confusion it may cause, we are planning to implement automatic error sending back for the streaming case.\n. We intentionally check the size twice, since a small compressed payload may be decompressed into a large one and potentially break the limit. In fact, server also checks the size after decompression, see here: https://github.com/grpc/grpc-go/blob/master/server.go#L762. \"[::]:33181\" and \"127.0.0.1:46679\" are valid address if you don't install you custom resolver, which may have different behavior. \nIf you don't use balancer&resolver, the valid input address should be the same as net.Dial. Otherwise, it depends on the resolver implementation.\nDo you know what's the error message for the failure, and how we can reproduce it so we could possibly debug the issue? Thanks!\n. ping. @MakMukhi @dfawley this PR is ready for review again. Thanks!. ping~~~~. I signed it!. I signed it.. fix #1549 . #1598. #1623 . fix #1590 . should be fixed by #1623.. cannot reproduce it.. continue the work with #1811 and close this.. The issue here should be covered by #1911. Closing this now.. BenchmarkAtomicValueStore-12            30000000            42.1 ns/op\nBenchmarkAtomicAddInt64-12              300000000            5.69 ns/op\nBenchmarkAtomicTimeValueStore-12        20000000            98.6 ns/op. ## summary\nuse atomic.StorePointer for best per operation execution time and best contention scenario performance.\ncontention benchmark:\nBenchmarkValueStoreWithContention/Atomic/10-12                           3000000           334 ns/op\nBenchmarkValueStoreWithContention/AtomicStorePointer/10-12          10000000           157 ns/op\nBenchmarkValueStoreWithContention/Mutex/10-12                        1000000          1112 ns/op\nBenchmarkValueStoreWithContention/Atomic/100-12                       500000          2764 ns/op\nBenchmarkValueStoreWithContention/AtomicStorePointer/100-12          1000000          1433 ns/op\nBenchmarkValueStoreWithContention/Mutex/100-12                        200000          9318 ns/op\nBenchmarkValueStoreWithContention/Atomic/1000-12                       50000         28546 ns/op\nBenchmarkValueStoreWithContention/AtomicStorePointer/1000-12          100000         14360 ns/op\nBenchmarkValueStoreWithContention/Mutex/1000-12                        30000         61149 ns/op\nBenchmarkValueStoreWithContention/Atomic/10000-12                       5000        309288 ns/op\nBenchmarkValueStoreWithContention/AtomicStorePointer/10000-12          10000        144596 ns/op\nBenchmarkValueStoreWithContention/Mutex/10000-12                        5000       4657600 ns/op\nBenchmarkValueStoreWithContention/Atomic/100000-12                       500       2929331 ns/op\nBenchmarkValueStoreWithContention/AtomicStorePointer/100000-12          1000       1468127 ns/op\nBenchmarkValueStoreWithContention/Mutex/100000-12                         30      39160125 ns/op\nper operation execution time\nBenchmarkAtomicAddInt64-12              300000000            5.15 ns/op\nBenchmarkAtomicTimeValueStore-12        20000000           113 ns/op\nBenchmarkAtomic16BValueStore-12         20000000            69.4 ns/op\nBenchmarkAtomic32BValueStore-12         20000000            92.7 ns/op\nBenchmarkAtomicPointerStore-12          200000000            9.66 ns/op\nBenchmarkAtomicTimePointerStore-12      200000000            9.93 ns/op. go version 1.9\nBenchmarkStoreContentionWithAtomic/Atomic/10-12             50000000            23.3 ns/op\nBenchmarkStoreContentionWithAtomic/Atomic/100-12            50000000            23.4 ns/op\nBenchmarkStoreContentionWithAtomic/Atomic/1000-12           50000000            23.3 ns/op\nBenchmarkStoreContentionWithAtomic/Atomic/10000-12          50000000            24.5 ns/op\nBenchmarkStoreContentionWithMutex/Mutex/10-12               10000000           127 ns/op\nBenchmarkStoreContentionWithMutex/Mutex/100-12              20000000           128 ns/op\nBenchmarkStoreContentionWithMutex/Mutex/1000-12             30000000           108 ns/op\nBenchmarkStoreContentionWithMutex/Mutex/10000-12            30000000            80.9 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/10-12              5000000           233 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/100-12             5000000           265 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/1000-12            5000000           451 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/10000-12           5000000           299 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/10-12         10000000           158 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/100-12        10000000           158 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/1000-12       20000000           143 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/10000-12      20000000           111 ns/op\n. BenchmarkStoreContentionWithAtomic-12       100000000           15.8 ns/op\nBenchmarkStoreContentionWithMutex-12        20000000           116 ns/op\nXXXX(set by  SetParallelism)/XXXX (extra work besides critical section)\nBenchmarkStructStoreContentionWithCAS/CAS/100000/100000000-12                  1    1421414467 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/100000/10000-12                2000000           608 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/100000/0-12                   10000000           174 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/10000/100000000-12                 300       4822022 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/10000/10000-12                 3000000           490 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/10000/0-12                    10000000           160 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/1000/100000000-12                  300       4771404 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/1000/10000-12                  3000000           484 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/1000/0-12                     10000000           162 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/10/100000000-12                    300       4888170 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/10/10000-12                    3000000           512 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/10/0-12                       10000000           168 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/1/100000000-12                     300       4891607 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/1/10000-12                     3000000           482 ns/op\nBenchmarkStructStoreContentionWithCAS/CAS/1/0-12                        10000000           163 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/100000/100000000-12            200       5778961 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/100000/10000-12            2000000          2876 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/100000/0-12                5000000           205 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/10000/100000000-12             200       5790696 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/10000/10000-12             1000000          3148 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/10000/0-12                10000000           265 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/1000/100000000-12              300       4830805 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/1000/10000-12              1000000          1896 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/1000/0-12                 10000000           452 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/10/100000000-12                300       4861557 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/10/10000-12                3000000           497 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/10/0-12                   20000000           105 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/1/100000000-12                 300       4927593 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/1/10000-12                 3000000           488 ns/op\nBenchmarkStructStoreContentionWithMutex/Mutex/1/0-12                    10000000           136 ns/op. BenchmarkStructStoreContention/CAS/100000000/100000-12              3000       4731686 ns/op\nBenchmarkStructStoreContention/CAS/10000/100000-12              30000000           483 ns/op\nBenchmarkStructStoreContention/CAS/0/100000-12                  100000000          155 ns/op\nBenchmarkStructStoreContention/CAS/100000000/10-12                  3000       4677181 ns/op\nBenchmarkStructStoreContention/CAS/10000/10-12                  30000000           475 ns/op\nBenchmarkStructStoreContention/CAS/0/10-12                      100000000          154 ns/op\nBenchmarkStructStoreContention/Mutex/100000000/100000-12            3000       4745838 ns/op\nBenchmarkStructStoreContention/Mutex/10000/100000-12             5000000          3442 ns/op\nBenchmarkStructStoreContention/Mutex/0/100000-12                30000000           365 ns/op\nBenchmarkStructStoreContention/Mutex/100000000/10-12                3000       4687433 ns/op\nBenchmarkStructStoreContention/Mutex/10000/10-12                30000000           479 ns/op\nBenchmarkStructStoreContention/Mutex/0/10-12                    200000000          100 ns/op. Well, it's not anomaly, quite repeatable. I also remembered it's of magnitude 1000 when we did experiments before.\nBenchmarkStructStoreContention/CAS/100000000/100000-12              3000       4808601 ns/op\nBenchmarkStructStoreContention/CAS/100000000/10-12                  3000       4694737 ns/op\nBenchmarkStructStoreContention/CAS/10000/100000-12              30000000           482 ns/op\nBenchmarkStructStoreContention/CAS/10000/10-12                  30000000           475 ns/op\nBenchmarkStructStoreContention/CAS/0/100000-12                  100000000          155 ns/op\nBenchmarkStructStoreContention/CAS/0/10-12                      100000000          153 ns/op\nBenchmarkStructStoreContention/Mutex/100000000/100000-12            3000       4757765 ns/op\nBenchmarkStructStoreContention/Mutex/100000000/10-12                3000       4680524 ns/op\nBenchmarkStructStoreContention/Mutex/10000/100000-12             3000000          3353 ns/op\nBenchmarkStructStoreContention/Mutex/10000/10-12                20000000           665 ns/op\nBenchmarkStructStoreContention/Mutex/0/100000-12                50000000           430 ns/op\nBenchmarkStructStoreContention/Mutex/0/10-12                    100000000          111 ns/op. [updated]\nServer started to send the message of size 4194315. And when client read the length of the message, it detected that the received message size violate the receive message size limit which is 1024000, and returned the error you saw (code here).\nOn server side, it will keep sending the message until it is blocked by flow control (what you capture through Wireshark), and hang there (caused by the bug described below). . \nReview status: 0 of 13 files reviewed at latest revision, 14 unresolved discussions.\n\nserver.go, line 1232 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nIs it a problem if RemoveEntry is called multiple times with the same id?\n\nI don't think it's a problem. We only delete if it exist in map.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 13 files reviewed at latest revision, 14 unresolved discussions.\n\nclientconn.go, line 644 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nchannelzID\n\nDone.\n\nclientconn.go, line 831 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis function is called Metric_s_, but the return value is type Metric.\n\nDone.\n\nclientconn.go, line 1029 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nchannelzID\n\nDone.\n\ngrpclb_remote_balancer.go, line 258 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\ngo\nctx := context.Background()\nif channelz.IsOn() {\n  ctx = channelz.WithParentID(context.Background(), lb.opt.ChannelzParentID)\n}\ncc, err := DialContext(ctx, \"\", dopts...)\n\nDone.\n\nserver.go, line 107 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nchannelzID\n\nDone.\n\nchannelz/types.go, line 3 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\n2018\n\nDone.\n\nchannelz/types.go, line 127 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThese two msg related functions should not be part of this interface. They should be defined as part of the transport package.\n\nDone.\n\ntransport/http2_client.go, line 117 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nchannelzID\n\nDone.\n\ntransport/http2_client.go, line 318 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis registering should happen at the same place as stats handler. Move this before go t.reader().\nAlso in that case, t.id doesn't need to be protected by t.mu because nobody will modify it.\n\nDone.\n\ntransport/http2_client.go, line 615 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThe mutex should not be necessary because id won't be changed.\n\nDone.\n\ntransport/http2_server.go, line 98 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nchannelzID\n\nDone.\n\ntransport/http2_server.go, line 238 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMove to the same place as stats.ConnBegin\n\nDone.\n\nchannelz/service/service.go, line 19 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDelete this file and add it later?\n\nDone.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 13 files reviewed at latest revision, 8 unresolved discussions.\n\ngrpclb_remote_balancer.go, line 252 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThese two var definitions are unnecessary.\n\ndone\n\nserver.go, line 470 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhy is this listenSocket needed?\nAnd there is no way to record numbers of bytes/messages received on this listenSocket.\nIsn't it enough to have channelz socket for connections received on this listener.\n\nI don't think listenSocket needs to report those metric (e.g. messages, etc). We are more interested in its address (port).\n\nchannelz/funcs.go, line 40 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nIf TurnOn is called multiple times, new storage will replace the old one. Is this by design?\n\nI think it is better to make TurnOn a one-time change (i.e. no replacement). Change made.\n\nchannelz/funcs.go, line 71 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDoes this need to be exported?\n\nIt is not a necessary API for users, but since I am making the TurnOn API a one-time change, I need to export this function to facilitate testing (clear existing channelz table for new tests).\n\ntest/channelz_test.go, line 3 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\n2018\n\ndone\n\ntransport/transport.go, line 508 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nChannelzParentID\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 8 of 13 files reviewed at latest revision, 5 unresolved discussions.\n\ngrpclb_remote_balancer.go, line 254 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWith a second thought, I think it's a bad idea to pass this to Dial in the context instead of as a DialOption. \nIn fact, the official context doc explicitly says context is not for passing optional parameters to functions: https://golang.org/pkg/context/#pkg-overview.\n\nadded a dial option specifically to set parent channelz ID.\n\nserver.go, line 1228 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis removes the server from channelz, and the following lis.Close, c.Close removes the sockets from channelz. This is a bit out of order here. Not sure if this matters much, but it would still be good to keep them consistent.\nAlso, does this have to happen inside the lock?\n\nmoved out of the lock.\nThe order doesn't matter, since logic is implemented inside channelz package that only all children of an entity has been deleted and removeEntry() has been called on it, will it then be deleted from channelz tracking table.  This logic is to prevent orphaned entry.\n\nserver.go, line 1274 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThe server is gracefully stopping, so there might be pending RPCs on this server.\nShould this Remove happen only when the server actually stops?\n\nsame as above\n\nserver.go, line 1274 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSimilar to Close(), the order of this and closing listener/connection is wrong.\n\nsame as above\n\nchannelz/funcs.go, line 71 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit in the comment, it doesn't return the data storage.\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 4 of 12 files reviewed at latest revision, 4 unresolved discussions.\n\nchannelz/funcs.go, line 265 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nI think this (and many other functions like this) is doing too much work on it's own, and is too primitive.\nBetter abstraction should be done for them.\nThink more about Object Oriented, and create classes for the components.\nThis function should be refactored into functions and methods. So it would end up with something like\n```go\nfunc removeEntry(id int64) {\n  c := findEntry(id)\n  p := findEntry(c.parent)\n  p.deleteEntry(cid)\nif p is not refered by any other children:\n    removeEntry(pid)\n}\n```\n\nrefactoring done.\n\nchannelz/types.go, line 66 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nI think this type is mixing fields to be set by the implementation of interface Channel and fields to be set by channelz package together.\nIf you don't expect the implementation to know this field, you should keep it separate from the stats to be collected by the Channelz implementation.\n\ndone\n\nchannelz/types.go, line 92 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAre those Lock() Unlock() called by anyone?\nIf the caller really needs those functions, I think getter/setter would be more appropriate.\n\ndeleted.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 11 of 12 files reviewed at latest revision, 27 unresolved discussions.\n\nserver.go, line 512 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nHow about registering channelz at the beginning of serve and remove when serve returns?\n\nServer can serve on multiple listeners, so register a server for each Serve() would not be correct.  However, I do find a problem here, which is, when the listener is closed but the server is still serve on other listeners, then we should not remove server from channelz. I move the logic from default case to <-s.quit case after <-s.done.\n\nchannelz/funcs.go, line 179 at r6 (raw file):\nPreviously, lyuxuan wrote\u2026\nadd ref string func RegisterServer(s Server, ref string) int64.\n\ndone\n\nchannelz/funcs.go, line 183 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit: make this consistent with other register functions (create a temp variable for &server{}).\n\ndone\n\nchannelz/funcs.go, line 277 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\n// c.mu must be held by the caller.\n\ndone\n\nchannelz/funcs.go, line 320 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDon't look up, just delete?\n\nThen you lose the chance for early return.  I think I can rearrange the conditionals here, to have sockets in the top and then subchannels and then channel and server, which reflects the frequency of the deletion of each type of entity.\n\nchannelz/funcs.go, line 343 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhat's deleteEntry and what's removeEntry? Add comments?\nAnd why are there two separate functions?\n\nI added comments. Basically, removeEntry is a request to remove an entry, however, it may not delete the entry if the entry still have children alive, or it will delete more than the entry specified, like the removal of the last socket of a gracefully shutting down server will lead to the deletion of the server. As for deleteEntry, it is a simple delete, just delete the entry from the map. \n\nchannelz/funcs.go, line 349 at r7 (raw file):\n\nsort.Slice\nsort.Slice was introduced by go1.8, so we still need this code for go1.7 and go1.6. So I think it would be better to just have it here instead of having two files with different build tags.\n\n\nchannelz/funcs.go, line 381 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nrename this variable\n\nrenamed to end.\n\nchannelz/funcs.go, line 386 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nhttps://golang.org/pkg/sort/#Search\n\ndone\n\nchannelz/funcs.go, line 400 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nwhy is this true?\n\nIt means no channel with an id larger than the specified id exist, so return true, as the we already reach the end, which indicates no higher id should be sent for request.\n\nchannelz/funcs.go, line 405 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSet ID and refname?\nAnd if those are really missing here, it means they are not checked by the tests.\n\nthe fields are set and tested in later PR (i.e. #1919).\n\nchannelz/funcs.go, line 409 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nShould this copy happen inside the previous c.mu.RLock()?\n\ndone.\n\nchannelz/funcs.go, line 427 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nrename\n\ndone\n\nchannelz/funcs.go, line 450 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit: does cn stand for channel? Rename cn, something general as item would work.\n\nok\n\nchannelz/funcs.go, line 481 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nUse another variable for the return value? Reusing ok is confusing.\n\ndone\n\nchannelz/types.go, line 41 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nCall this tryDelete?\n\ndone. replaced with deleteSelfIfReady\n\nchannelz/types.go, line 79 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nComment for each field\n\ndone\n\nchannelz/types.go, line 147 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nCall canDelete?\n\nright.\n\nchannelz/types.go, line 158 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nChange this to early return, similar to what's done in delete.\n\ndone\n\nchannelz/types.go, line 201 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nChange this to early return, and call this function from delete().\n\ndone\n\nchannelz/types.go, line 231 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nTODO: socket options?\n\nok\n\nchannelz/types.go, line 234 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nIs this a TODO?\n\nyes\n\nchannelz/types.go, line 311 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nTODO: trace?\n\ndone\n\nchannelz/types.go, line 356 at r7 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nChange this, too.\n\ndone\n\ntest/channelz_test.go, line 45 at r6 (raw file):\nPreviously, lyuxuan wrote\u2026\nAdd here\nfunc init() {\n  channelz.TurnOn()\n}\nReplace turnOnChannelzAndClearPreviousChannelzData with channelz.NewChannelzStorage, and remove the func def here as it is no longer needed.\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. https://travis-ci.org/grpc/grpc-go/jobs/355618634. Seeing failure again: https://travis-ci.org/grpc/grpc-go/jobs/396660661. Ops. (I remember it used to work???) It's not race detector error. I remember it's error caused by empty address list.. \nReview status: 8 of 17 files reviewed at latest revision, 13 unresolved discussions, some commit checks failed.\n\nclientconn.go, line 878 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMake this and other fields like this atomic?\n\nSince for the fields with type time.Time, the performance difference of mutex and atomic is not that large, it would be better to just keep code simple by having the mutex for all of them together. We can change to atomic if later we find the performance is suffered substantially. Here's the benchmark link with performance numhers https://github.com/grpc/grpc-go/pull/1788.\n\npicker_wrapper.go, line 78 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit: call this doneChannelzWrapper()\n\ndone\n\nrpc_util.go, line 278 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nRevert blank changes in this file\n\ndone\n\nserver.go, line 1338 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDelete this? It's already removed by the code above.\n\ndone\n\ntransport/flowcontrol.go, line 126 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMinus is not atomic, the result is potentially racy. Is it OK?\n\nnot ok. made change\n\ntransport/http2_client.go, line 111 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nComments on what succeed means on transport level.\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 8 of 17 files reviewed at latest revision, 9 unresolved discussions, some commit checks failed.\n\nbenchmark/benchmain/main.go, line 462 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDelete\n\ndone\n\nbenchmark/stats/stats.go, line 145 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nCleanup this file\n\ndone\n\ntransport/http2_client.go, line 1243 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit: either mark those as TODOs, or just remove them.\n\ntodo is in following PRs, prefer to leave as they are.\n\ntransport/http2_client.go, line 1272 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nCould this <- be blocked for a long time? It that OK?\n\nMaks says it should be fast, but probably I can add a 1s timeout to bound it.\n\ntransport/http2_client.go, line 1269 at r6 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMake this buf size 1?\nOtherwise the writer goroutine may block forever if t.ctx timeout. Not sure if that can happen though...\n\ndone\n\ntransport/http2_server.go, line 952 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSame as the comments on client side.\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 5 of 18 files reviewed at latest revision, 9 unresolved discussions.\n\ntransport/http2_client.go, line 626 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nstreamSucceeded counts for the stream with EoS bit set for both end points, the caller of this function doesn't have the information on whether EoS was sent, right?\nHow about adding two fields to Stream for EoS sent/received, and check that in this function?\n\nTalked to Carl, and we figured out with Eric that 1. on client side, receiving the frame with eos bit set is regarded as a success. 2. on server side, sending a frame with eos bit set is regarded as success on server side And it matches our current implementation. BTW, i deleted eosReceived field in the stream, as we no longer need it (because with Mak's scheduler change, we close the stream right away once an error occurs).\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 13 of 16 files reviewed at latest revision, 14 unresolved discussions.\n\nchannelz/types.go, line 246 at r8 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nUpdate the comment\n\ndone\n\ntest/end2end_test.go, line 535 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMove this and dialer wrapper to file rawConnWrapper.go and rename the file. Those are all conn related utilities.\n\ndone. I think the rawConnWrapper is ok. What do you think?\n\ntest/end2end_test.go, line 538 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhy is this a list in listener but only one pointer in dialer wrapper?\n\nI think I forgot to simply the code after some changes. Nice catch. Thanks!\n\ntest/end2end_test.go, line 563 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis is not necessary.\n\nlistenWrapper needs it to satisfy net.Listener interface.\n\ntest/end2end_test.go, line 579 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis is not necessary.\n\nlistenWrapper needs it to satisfy net.Listener interface.\n\ntest/end2end_test.go, line 676 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis global function overriding is not necessary.\nAdd another argument to listenAndServe as the listen function.\n\ndone\n\ntest/end2end_test.go, line 825 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nJust build opts and then return te.clientConn(opts...), dw\n\nthe dialer will gets overwritten, if we call te.clientConns(opts) after. But in that case it is a test design error, and should not happen. \n\ntest/rawConnWrapper.go, line 84 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSeems this function is never called, right?\nThere are other functions not called in this file.\n\nThey are just for completeness reason and are not currently used, but may be used in the future. Here, I just want to provide a wrapper that can do the writing of different types of frame . The majority of the code are  copied from servertester.go, which is for server side only.\n\ntest/rawConnWrapper.go, line 240 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis and the following header functions are not called.\n\nright, similar completeness reason as above.\n\ntest/rawConnWrapper.go, line 261 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis and the padded data functions are not called.\nDo you need to manually write data bytes?\n\nright, similar completeness reason as above.\n\ntest/rawConnWrapper.go, line 282 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nUnexport?\n\nok\n\ntest/rawConnWrapper.go, line 289 at r12 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nUnexport?\n\nok\n\ntransport/http2_client.go, line 114 at r8 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nUpdate the comment\n\ndone\n\ntransport/http2_server.go, line 119 at r8 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nUpdate\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. There shouldn't be any space between --go_out=plugins=grpc: and the destination path.\nPlease remove the space  between \"grpc:\" and \"proto\", i.e. change -go_out=plugins=grpc: proto into -go_out=plugins=grpc:proto, and try again. \nLet us know if you still have any problem. Thanks!\n. We have made some major changes to the way we manage connections recently. One of them is: if you set the WithBlock() dial option, we will retry on handshake error until deadline is exceeded no matter whether it is a temporary error or not, and that's why you see dial returns context  deadline exceeded. (#1856)\nWe have a workaround (https://github.com/grpc/grpc-go/pull/1855) which was unfortunately not making the cut to our 1.10.0 release. In short, with the workaround, you dial without the dial option WithBlock(), and when making the first fail-fast rpc on the ClientConn returned from the dial, you will most likely get the error containing info about failed handshake (e.g. protocol version not supported). Note that if fail-fast is not specified, then rpc will be blocked until deadline exceeded or until it gets connected to server.. \nReview status: all files reviewed at latest revision, 5 unresolved discussions.\n\nchannelz/service/service.go, line 37 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDelete this function? Or delete the following New function?\n\nkeep it. Delete NewCZServer()\n\nchannelz/service/service.go, line 42 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\ns/CZ/Channelz, or just NewServer().\n\ndeleted\n\nchannelz/service/service.go, line 52 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAvoid using unnamed fields?\ngo vet ./channelz/service/ shows warnings. But why is travis not complaining?\n\ndone. Interesting.\n\nchannelz/service/service.go, line 72 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nCould any of those pointers be nil?\n\nnot possible in current implementation, since ChannelzMetric() function will always return a non nil pointer by our current implementation.\n\nchannelz/service/service.go, line 137 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nindentation\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: all files reviewed at latest revision, 5 unresolved discussions.\n\nchannelz/service/service.go, line 42 at r1 (raw file):\nPreviously, lyuxuan wrote\u2026\ndeleted\n\nunexported\n\nComments from Reviewable\n Sent from Reviewable.io \n. #2599 is now merged. @Capstan please check out the current master head and see if it works for you. Thanks!. This issue should have been solved by #1940.. It's on our todo list for this quarter. Thanks!. After #1854, we no longer clean up orphan streams. Could you please check whether you explicitly end the stream you created by using the 4 ways described in the #1854? If you didn't end the streams you created accordingly, then the goroutine monitoring the context for each stream will sit there forever, and it might have caused the problem you saw.\nIf you do end every stream you created accordingly, then there might be a bug on our side. If so, could you please give us a simple reproducible example so we can investigate more? Thank you!. We suggest that you use custom dialer and handshaker in order to instrument and collect the stats you listed here.\n@rakyll. We wonder if opencensus is going to support the stats listed here. . Friendly ping! Do the custom dialer and handshaker solve your problem? . \nReview status: 3 of 17 files reviewed at latest revision, 15 unresolved discussions.\n\nchannelz/types.go, line 423 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMove this to a sub package?\nchannelz package defines the API between the clientconn and the channelz store.\nThis is more like a helper function for the clientconn to get info from creds. It's not part of the API.\n\nmoved to credentials package.\n\nchannelz/util_unix_go19.go, line 1 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhy do we need this build constraint?\nAnd why is this not just windows or not?\n\nhttps://github.com/golang/sys/blob/master/unix/syscall_unix.go contains definition for function like GetsockoptLinger, GetsockoptTimeval, and it has the build tag listed here. Therefore, I have to have the same build tag here. FYI, I've made some changes to the build tag.\n\nchannelz/util_unix_go19.go, line 29 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis is also a helper function, not part of the API.\nIn other words, this is not necessary for channelz, the clientconn could generate the same result without calling this function.\n\nIf we move this function to transport package, the it will complicate the package by adding two more files for different go version.\n\nchannelz/service/service.go, line 38 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMake a function to do the time conversion.\n\ndone\n\nchannelz/service/service.go, line 145 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMove }'s to one line\n\ndone\n\ncredentials/credentials.go, line 35 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDelete blank line.\n\ndone\n\ncredentials/credentials.go, line 172 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNames of the fields.\n\ndone\n\ncredentials/credentials_util_go19.go, line 5 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\n2018\n\ndone\n\ncredentials/credentials_util_go19.go, line 36 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSince we only care about method SyscallConn, do\ngo\nrawConn.(interface{\n  SyscallConn() (syscall.RawConn, error)\n})\nSo we don't need the _pre19.go file for this.\nBut this works only if syscall.RawConn is defined pre 1.9.\n\nUnfortunately, syscall.RawConn is not defined pre 1.9\n\ncredentials/credentials_util_pre_go19.go, line 5 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\n2018\n\ndone\n\ncredentials/credentials_util_pre_go19.go, line 30 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhy keeping the rawConn?\n\nThat's how we get the socket options, i.e. by doing the SyscallConn(rawConn) and then Control()\n\nchannelz/types_linux.go, line 24 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWill this import cause the build to fail on non-linux? Or just function calls will return error?\n\nnice catch!\n\nchannelz/types_linux.go, line 30 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis field is not defined in _windows.go, but is referenced in other files (e.g. service.go), even on windows.\n\nI added some dummy struct definitions for SocketOptionData for non unix build\n\nchannelz/util_default.go, line 1 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nRename this file. default sounds too normal.\n\nrenamed to util_nonlinux_pre_go19.go\n\ntest/channelz_test.go, line 125 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nRevert this.\n\nok\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: all files reviewed at latest revision, 14 unresolved discussions, some commit checks failed.\n\nchannelz/types_nonunixes.go, line 71 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis is a strong assumption that non-unix systems will have similar concepts with unix.\nLooking at the proto def for socket option, the real data field is a google.protobuf.Any. We should hide the Linger details in the platform specific files, and add a method that generates the Any proto (move MarshalAny function calls from service.go to platform specific files).\n\nDiscussed offline: to generate any proto, it will add pb dependency to channelz package, which will eventually lead to a cyclic import path. \n\nchannelz/util_test.go, line 31 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nRemove this blank line.\n\ndone\n\nchannelz/util_test.go, line 65 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nsomething is wrong with this startServer function\n\nfixed\n\nchannelz/util_test.go, line 73 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThe content of this struct will be modified by the syscall later, right?\nJust leave it empty? Or use new.\n\nthe value here is to set the fd, and we will read from the fd to verify the set and get functionality.\n\nchannelz/util_test.go, line 110 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\ndefer\n\ndone\n\nchannelz/service/service.go, line 38 at r2 (raw file):\n\nptypes.DurationProto\ndone\n\n\nchannelz/service/service.go, line 153 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMove this code around so you create one local variable and only set its field when error is not nil.\n\ndone\n\nchannelz/service/service.go, line 168 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMove this and other code that accesses internal fields of SocketOptionData to _unix.go and _nonunix.go files. So you won't need the dummy non-unix types, either.\n\ndone\n\nchannelz/service/service.go, line 168 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAnd put the fields of the struct to multiple lines.\n\ndone\n\nchannelz/service/service_test.go, line 35 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nchannelzpb \"google.golang.org/grpc/channelz/\"\n\nI think this is obsolete, right?\n\nchannelz/service/service_test.go, line 41 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMove this next to the definition of OtherSecurityValue, and add a comment that it's needed by UnmarshalAny\n\ndone\n\nchannelz/service/service_test.go, line 180 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSimilar to previous comments on making a function to convert duration, this and the following 1e3, 1e9 could also be a function.\n\ndone\n\ncredentials/credentials.go, line 237 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThe names for the interfaces are OK in channelz package, but inappropriate in credentials package.\nRename this or move it to channelz/subpackage\n\nrename to ExtraSecurityInfo. It's better to be in credentials package as people implementing credentials will notice it.\n\ncredentials/credentials_util_go19.go, line 35 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAdd a comment that this implements interface syscall.Conn\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 11 of 19 files reviewed at latest revision, 1 unresolved discussion.\n\nchannelz/util_test.go, line 65 at r3 (raw file):\n\ngo func() { ln.Accept() } ()\nyes. I was thinking something more complicated that probably need this and later change my idea. Fixed.\n\n\nComments from Reviewable\n Sent from Reviewable.io \n. In gRPC-go, for each ClientConncreated by DialContext(), a resolver and a balancer that only service this ClientConn will be instantiated through specific resolver builder and balancer builder, which are configured through scheme prefix in the target string and the DialOption WithBalancerName() respectively. For example, the when dialing with the DialOption WithBalancerName(\"ABC\"), grpc selects the balancer builder with the name \"ABC\" from the registered balancer builder table, and then use the selected builder to create a balancer just for this connection. The same goes for resolver.\nI think the there might be some confusion here about balancer and balancer builder, as we recently introduced the concept of balancer builder. In a simple way, users are expected to register all the balancers builder implementation possibly needed during int(). And gRPC will use them to create desired balancer for each connection. Resolver and balancer are not shared across different ClientConns. For more info about the architecture of resolver and resolver builder, balancer and balancer builder, you can refer to the deisgn doc here. Thanks!\n. Can you let us know your purpose of getting the transport.Stream? transport package is really for grpc internal purpose, and we don't want to expose things to users. Please let us know about your use case for the transport.Stream, we could probably figure out some thing for you. Thanks!. Thank you @ZachEddy. For your case, we provide Method(ctx) in grpc package to get the service and method strings from the context. \nAnd please kindly close the issue, if it works ;). Thanks for the suggestion. We are willing to change the log level from warning to info. Does that sound ok to you?. Our current dns resolver is implemented to have native support for key grpc features like grpclb and service config. And we suggest that you implement your own balancer, which is basically copy and paste current dns resolver implementation and change the SRV lookup func to serve your needs. . Both options will work for you. However, I don't recommend overriding the default dns scheme resolver, since it's easy to be confusing later when your project gets larger and more complex that you don't know whether you are using the grpc native resolver or you own implementation. I think it's better to keep them separate, and use SetDefualtScheme to make your dns resolver to be the default resolver. BTW, our current default resolver is \"passthrough\", so you still need to SetDefualtScheme if you prefer overriding.\nOur current implementation will resolve on demand (whenever a transport is down), and every 30mins. You can safely remove the every 30mins part, which is essentially the freq field in the builder, and related timer. In our code, timer has been used to trigger the first name resolution, once you remove it, you need to call ResolveNow() once to make the first resolution that start the whole proccess. \nLet us know if you have any problem during implementation. We are willing to help. Thanks!. Please let us know if you have any problem. Closing this now as it seems the original issue has been addressed.. https://travis-ci.org/lyuxuan/grpc-go/jobs/381529318. CloseSend() is called by client to signal server that it has sent all the messages it want to send, and will send no more message, i.e \n\ncloses the send direction of the stream.\n\nHowever, server is still free to send whatever message(s) it wants to send to client, and rpc is still alive. It's not until client receives  the trailer/RST_STREAM sent by server or rpc context has expired/cancelled, that the rpc is indeed ended.\nIn the comment of this commit, it describes four conditions for a stream to be fully cleaned up. I think you've found two of them, which are calling RecvMsg until non-nil error occurs and cancelling the context , i.e.\n\niterating the stream until EOF does not leak goroutines, nor does adding a new context before the function call and canceling it when I exit.\n\nIn summary, CloseSend() does not end an rpc call, and thus will not release the resources related to the stream. To cleanly end a rpc call, one must use one of the four ways as described here.\n . HTTP2 has a notion of half-closed for stream state. CloseSend() is the corresponding function call that will make client side stream enter the half-closed (local) state. The name of the function does precisely reflect its functionality. Please refer to the HTTP2 spec here for more details about half-closed.\n\nI'm having trouble understanding the benefits of the current design. When does someone want to ask the server to stop sending messages, but not terminate the connection? I guess my point here is based on what I've read the purpose of CloseSend is to indicate the client no longer wishes to receive messages by asking the server to stop sending. Since it is the clients intent to no longer Recv, that intent should be immediately honored by closing the connection.\n\nYou may have some misunderstanding here. CloseSend() means client will no longer send message to server, not asking the server to stop sending messages, and client will be willing to receive messages from server. It only stops local sending.\n. @him0, this feature is released since version 1.13.0. You can look at the commit page here, and figure out the versions having this feature. We forgot to include it in the 1.13.0 release note, which is an unfortunate oversight. Thanks!. Interceptor is supposed to be able to run hooks before and after the request is executed. For example, for client unary case:\nfunc yourInterceptor(ctx context.Context, method string, req, reply interface{}, cc *ClientConn, invoker UnaryInvoker, opts ...CallOption) error {\n    // hooks before the request\n    err := invoker(ctx, method, req, reply, cc, opts...)\n    // hooks after the request\n}\nPlease let us know if this doesn't work for you. Thanks!. Hi @bleleve, server interceptor should work the same way like client interceptor, applicable for both pre- and post- request processing.\nBTW, could you please let us know about the doc that says the interceptor is intended to be run only before so we could possibly fix it? Thanks!. Hi @bleleve, thanks for the info. I think the description they have is more for the middleware itself, not grpc-go in general. For their chaining feature, all the interceptors are executed before the handler is called. However, grpc itself is more flexible in that you can call the handler whenever you want (even not calling it).Please let us know if you encounter any issue using the interceptor. Thanks! (closing the issue as I think the initial question has been answered). Hi @jsha, thanks for the pr, could you please sign the CLA?. For the metadata approach, you can use a type safe mechanism(e.g. protobuf, json, etc) to enforce type safety. On one side, in the interceptor, define typed data, marshal it into bytes and then convert to string to be carried inside metadata. And on the other side, inside interceptor, you get the string, convert to bytes and use an unmarshaler to convert them to type safe data. Unfortunately, you have to do these conversions, marshaling and unmarshaling yourself, as grpc currently doesn't support such things over metadata. Please let us know if you have any further question. Thanks!. This has been discussed in #1879, please refer to it for more info.\n. Run go get -u github.com/golang/protobuf/{proto,protoc-gen-go} to update your local proto and protoc-gen-go should the solve the problem. \nWe regenerated pb.go files (#2039) using the latest proto and protoc-gen-go to keep them update. And if you are still using the old version of them, then the error you saw will occur. . \nReview status: all files reviewed at latest revision, 3 unresolved discussions.\n\nchannelz/util_test.go, line 2 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhy is this 1.10?\n\nThis is because SyscallConn() function for net.TCPListener is added after go1.10. https://github.com/golang/go/commit/eed308de31e32a42012fd916d70cfed19280bbe7\n\nchannelz/service/service_test.go, line 1 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nOnly the socket related tests are platform specific, other tests should cover all platforms.\n\nyeah, make sense. I split the test into 3 files, the base service_test.go which contains all the shared tests, service_linux_test.go, which contains linux necessary helper functions and tests, service_nonlinux_test.go, which contains nonlinux helper functions.\n\ntest/channelz_linux_go19_test.go, line 1 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThe file name is go19.\nAnd why is this go1.10 and above only?\n\nWrong file name, should be go1.10. Good catch.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 19 of 23 files reviewed at latest revision, 3 unresolved discussions.\n\nchannelz/util_test.go, line 2 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAdd comment to explain this.\n\ndone\n\nchannelz/service/service_linux_test.go, line 20 at r4 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAdd file level comment for what's different between this and non-linux.\n\ndone\n\ntest/channelz_linux_go19_test.go, line 1 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nIs this 1.10 the same reason as above? Add a comment as well.\n\nyes. done\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 7 files reviewed, 2 unresolved discussions (waiting on @menghanl and @lyuxuan)\n\ninternal/benchmarkutil/benchmarkutil_linux.go, line 21 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nName this package syscall?\n\nboth syscall and unix package functions are included here.\n\ninternal/benchmarkutil/benchmarkutil_linux.go, line 44 at r2 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMake this function return *Rusage instead?\n\ndone\n\nComments from Reviewable\n Sent from Reviewable.io \n. Hi @dreamflyfengzi it seems that you are not running the grpc v.12.0 release, as the error message shows grpc/transport/control.go:191, while control.go has been renamed to flowcontrol.go in #1962, which is the v.12.0 release. Can you please check again what is the grpc version that you are using? Thanks!. Hi @weiyougit, I am not sure what's your use case here. But this topic has been previously discussed here #1663, and we decided the NewBuilderWithFreq is not necessary for the use cases described there. Please kindly close the PR if that also solves your problem, or reply with your use cases so we can discuss more. Thanks!. @dfawley I've created #2145 for appengine. \ud83d\ude00. I believe our credential API allows user to implement reloading certificates and keys functionality themselves. We'll discuss whether we plan to add support for this in our TLS implementation of the credential API and update here. Thanks!. Could you please update your golang.org/x/sys/unix package by go get -u golang.org/x/sys/unix? \nGetsockoptLinger and GetsockoptTimeval are recent additions and your local unix package may not be up to date with the change.\nPlease kindly close the issue if it works, or let us know if it is still not working. Thanks!. The root cause of this problem is actually not even at grpc layer, it's actually the golang net package. In short, grpc gets no update from the net.Conn when the other side set a firewall rule against it. And therefore, there's no way we could unblock from waiting for the events on the connection. Below are some details.\nI've carried out the following experiments:\n1. Starts a grpc server and sets sudo iptables -A OUTPUT -d <client-hostname> -j DROP. Then do a net.Dial(...) on the client side. It will block forever. \n\nStarts a grpc server and starts a grpc client. The RPCs are just fine. Then sets the firewall up. And now all the rpcs' progresses, will be blocked on the transport reader goroutinue (here) forever (or until deadline exceeds). Stack trace is below. And it shows that we are really blocked on the net.(*conn).Read.\n\ngoroutine 35 [IO wait]:\ninternal/poll.runtime_pollWait(0x7f50d66daf00, 0x72, 0xc420061bb0)\n        /usr/local/go/src/runtime/netpoll.go:173 +0x57\ninternal/poll.(*pollDesc).wait(0xc4201b2198, 0x72, 0xffffffffffffff00, 0x8ef2a0, 0xad0618)\n        /usr/local/go/src/internal/poll/fd_poll_runtime.go:85 +0x9b\ninternal/poll.(*pollDesc).waitRead(0xc4201b2198, 0xc4201c0000, 0x8000, 0x8000)\n        /usr/local/go/src/internal/poll/fd_poll_runtime.go:90 +0x3d\ninternal/poll.(*FD).Read(0xc4201b2180, 0xc4201c0000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\n        /usr/local/go/src/internal/poll/fd_unix.go:157 +0x17d\nnet.(*netFD).Read(0xc4201b2180, 0xc4201c0000, 0x8000, 0x8000, 0x11, 0x0, 0x0)\n        /usr/local/go/src/net/fd_unix.go:202 +0x4f\nnet.(*conn).Read(0xc4201bc000, 0xc4201c0000, 0x8000, 0x8000, 0x0, 0x0, 0x0)\n        /usr/local/go/src/net/net.go:176 +0x6a\nbufio.(*Reader).Read(0xc4201b8060, 0xc4201da038, 0x9, 0x9, 0x9, 0x0, 0x0)\n        /usr/local/go/src/bufio/bufio.go:216 +0x238\nio.ReadAtLeast(0x8edba0, 0xc4201b8060, 0xc4201da038, 0x9, 0x9, 0x9, 0xc420061de8, 0x401afe, 0xc420061e97)\n        /usr/local/go/src/io/io.go:309 +0x86\nio.ReadFull(0x8edba0, 0xc4201b8060, 0xc4201da038, 0x9, 0x9, 0xa14d3a3, 0xa14d3a3200c856c, 0x5b3ac4bc)\n        /usr/local/go/src/io/io.go:327 +0x58\ngolang.org/x/net/http2.readFrameHeader(0xc4201da038, 0x9, 0x9, 0x8edba0, 0xc4201b8060, 0x0, 0xbec6cf0f00000000, 0xef06b7ad, 0xb163a0)\n        /home/yuxuanli/go/src/golang.org/x/net/http2/frame.go:237 +0x7b\ngolang.org/x/net/http2.(*Framer).ReadFrame(0xc4201da000, 0xc4200c8560, 0xc4200c8560, 0x0, 0x0)\n        /home/yuxuanli/go/src/golang.org/x/net/http2/frame.go:492 +0xa4\ngoogle.golang.org/grpc/transport.(*http2Client).reader(0xc4201de000)\n        /home/yuxuanli/go/src/google.golang.org/grpc/transport/http2_client.go:1136 +0x125\ncreated by google.golang.org/grpc/transport.newHTTP2Client\n        /home/yuxuanli/go/src/google.golang.org/grpc/transport/http2_client.go:265 +0xb38\nIn your case, keepalives is helpful in detecting the problematic connection, so you may play around with the parameter values to achieve the quick detection you want. You may still need the extra goroutine to handle blocked calls. \nPlease let us know if you have further questions. Thank you! . It is actually the expected behavior. Documented here that failfast RPC should not fail if the channel is in the CONNECTING stage. What you observed is grpc trying to reestablish the connection and get blocked on net.Dial, and the channel state is CONNECTING during this time, thus failfast RPC does not fail.\nOne way for you to work around this is before you issue an RPC, check the connectivity state by calling GetState() and do not fire the RPC if the connectivity state is CONNECTING. This is kind of tedious.\nAnother workaround is to pass in a custom dialer to grpc through WithDialer(...). What you can do is in your custom dialer, shorten the context timeout to what you see fit. Currently in grpc, we have a minimum 20s (specified here) timeout for dial. You may shorten the timeout to X seconds in your custom dialer, and your rpcs will only be blocked for at most X seconds.\nLet us know if none of them works for you. Thank you!\n. This issue looks related to #1362, where the problem is about too aggressive logging and not real connection issue. I wonder if you saw more Dial/RPC failures or you just observed the increase of this line in the log. And could you please provide us with some logs so we can better diagnose what is happening? Thanks!. It seems that this issue is unrelated to aggressive logging in grpc. (We used to have aggressive logging which results in log spamming.)\nHowever, the error you logged does not seem like what grpc will return for a RPC call. The grpc returned error should follow the format \"rpc error: code = %s desc = %s\".\nCould you please let us know how you make rpc calls and how error is logged in your application? You can refer to this to know more about the error returned by grpc. Thanks!. close for now, since the requirement has not been finalized.. This error means the RPC fails because the connection is draining. This could be caused by server sending goaway or balancer removing the address. I think this issue involves grpc-web as you mention grpc_pass, you may open an issue in their repo and reference this issue there. \nAt the same time you may run with GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info to gather more debug info. Thanks!. FYI #2205 seems relevant to this issue.. Hi @itizir, thanks for opening this issue. Good catch! I don't think it is necessary and we'll fix it soon.. Can you give us more info about your environment setup, your code and logs, so we can better investigate the issue? Thanks!. Hi @Derliang, our default DNS resolver does look up DNS SRV records (link), but it's for grpclb usage only, as we make the prefix to be  \"_grpclb._tcp\". If you want to use other prefix for SRV lookup, you can have your own custom DNS resolver (you can simply copy our DNS resolver code and change the line that specifies the prefix) and register it to be used. Please let us know if you want more info. Thanks!. grpc and grpc-go is independent regarding minor releases. Our next major release v.1.14.0 is scheduled for 07/31/2018 and will be in sync with grpc.. Hi @quanjielin, grpc and grpc-go are two independent repositories. grpc-go releases only include the PRs that get merged in grpc-go repository. Therefore, the PR https://github.com/grpc/grpc/pull/15909 will not be included our next release or current master as it is a PR for grpc repository. As said in the description of https://github.com/grpc/grpc/pull/15909, grpc-go has no problem supporting non-google secure tokens (call credentials) to be sent over insecure channels. So I think your testing should be fine with grpc-go. Please let us know if you encounter any problem. Thank you!. Hi @arturgspb, you could use a HTTP2 proxy as the auth proxy. The proxy examines the incoming header frame and decodes the header fields. Then use the JWT token key (depending on the grpc per RPC credential implementation you use, e.g. here uses \"authorization\" as the key for JWT token) to find the corresponding JWT token in the decoded header fields. After you verify the token, you can generate a new header frame with JWT header field removed and fields like UserID and scopes added.Thanks! Please let us know if you need further info.. I do not know about any go library that does HTTP2 proxy, you may need to write your own.\nFYI, nginx and envoy supports proxying grpc, you may want to look into them. Please let us know if you need any help. Thank you!. @ncteisen, Thank you for the suggestion. Generic API should make life easier in the future if we want to extend the events we trace. I've switched to having single API for tracing event.. GreeterClient is the stub(interface) for RPC calls. greeterClient is the proto generated implementation of GreeterClient interface, and greeterClient takes a ClientConn to be initialized (here). Therefore, to close the connection, you simply close the ClientConn you initialize greeterClient with (like here). greeterClient itself is not a connection, therefore cannot be closed. It only keeps a reference to a connection(i.e. ClientConn). Thanks!. Hi @kriskowal, I think having a test verifying DNS contacting the custom DNS authority when target string containing authority info is enough for this change. You are very welcome to bring up more test cases :). Thanks!. @elliots, friendly ping. Let us know if you have any question/problem. Thanks!. Thanks for the update, @elliots. For testing, I think it's not necessary to include the dial test right now. And\n\nAll the tests in testDNSResolver are now run twice, once with no custom authority (as before) and once with one.\n\nThis will run the exactly same logic twice. Can you make a separate test for verifying the right authority address is passed to the custom DNS dialer? Thanks!\n. Hi @elliots, thanks for the quick update. Appreciate it. \nI may not have been very clear about what I meant by \"dial test\". I was suggesting it's not necessary to have a dns server set up and dial to it in the test. But I expect the Dial here to still be invoked. Instead of assigning the inline function to Dial, we can define a global variable to be same as the inline function, and assign the global variable to Dial. In the test, we set the global variable to be a made up dial function which does not really dial, but just verifies the dial target is the authority specified (plus the port :53 if the authority does not have a port).\n. @tywkeene, can you provide more info about how this merge breaks you?. I am wondering how you updated your code base and got this failure(details like what's the command you use to update the code base and build, etc. will be helpful). In this merge, it added the netResolver interface and extended parseTarget to take two arguments (in dns_resolver.go). From the error message, it looks like you are getting the new go19.go file, but not getting the updated dns_resolver.go file. I am not sure what's happening here.. Could you please check whether your code base has the latest dns_resolver.go file (which includes the netResolver definition and updated parseTarget)? Thanks!. Our next major release is Oct 23rd. Does it work for you?. Hi @virtuald, thanks for bringing up the issue. Actually, #1856 void the effect of FailOnNonTempDialError, and we need to update the comment on the API as it no longer has the effect that it supposes to. May I you your use case that requires the fail on non temp behavior? Thank you!. @jsha Can you change the error string to \"empty service config JSON string provided\"? Thanks!. wait for handshake is needed due to successful Accept on a shutdown server (golang library issue).. Share some of my findings. I took a look at the issue description here(https://github.com/u-root/u-bmc/issues/36) for more info, and found that the vendor case does not have the line connecting to localhost:8080 like the successful case. And from the grpc_cli source code here(https://github.com/grpc/grpc/blob/master/test/cpp/util/grpc_tool.cc), it seems the parsing is failed in the grpc_cli tool locally (before connecting to the remote server for reflection service). Not sure why vendor caused it though.. \ud83d\ude35 I am having trouble with the github review UI.... Different page give me different views of the comments, and all of them are incomplete........ . TODO: implement GetServer API.. Can you turn on logging on both client and server side? i.e. using environment variable GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info.\nAlso can you provide a code snippet to show your set up? Thanks!. gRPC has a default 4MB recv message limit on client side. If you want to receive message that's larger than 4MB, then you need to set the call option MaxCallRecvMsgSize(s) to allow for message to be received up to size s. \n(Note: Use the dial option WithDefaultCallOptions(MaxCallRecvMsgSize(s)) to apply the same limit to all RPCs using the connection. )\nAs for the stream leak, since you are sending unary RPC, so by the time client encounter the error, server side should have already finished the stream. I suspect it is something else that's causing the memory to grow. Could you please try the call option above to make client successfully receive the message, and see if the memory still grow quickly? \nThanks!. @WQG6848 Thanks for your detailed explanation. It's very helpful. Yes, it's indeed a bug in grpc, we'll get it fixed soon. Let us know if you need any help when before it gets fixed. Thanks!. Hi @breznik, we don't know of such setting that kills the connection every 4 mins. We suspect that it's something related to your specific configuration/environment setup (e.g. container configuration, or platform configuration). If you find the root cause, please let us know about it. It will be very helpful. Thanks!. fixed by #2534. closed by #2531. grpc-go currently does not natively support dialing the unix socket. As discussed here, the best practice right now is to use a custom dialer along with the passthrough scheme, which is the default scheme.\nThe reason you see CONNECT proxy is used when the HTTPS_PROXY env var is set is because when grpc sees no custom dialer is provided, it will create a dialer that will connect to proxy first if necessary. Here, grpc does not distinguish whether the target is unix socket. https://github.com/grpc/grpc-go/blob/63ae68c9686cc0dd26c4f7476d66bb2f5c31789f/clientconn.go#L184-L191\nIn the future, once we provide native support for unix socket, the problem you saw should be resolved naturally.. friendly ping?. I think append should work for your case. I wrote a quick test code here. Please let me know if your need is different than what I understood and thus append does not work for you. Thanks!. Close it for now. Please let us know if the you still experience any problem. Thanks!. Can you turn on logging by running the program with env var GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info and paste the output log here? Thanks!.  As indicated by the second and third field of the resolved address {10.0.20.38:32933 1 e0a0685a-fa61-4c2d-8a88-d994a835600b._grpclb._tcp.apikey-service.chair.private. <nil>}, the resolved address is a grpclb remote balancer address. \nIn the v1.18.0 release, we have grpclb addresses filtered out when the balancer is not grpclb.(#2509) grpclb balancer is not registered by default, you need to import the grpclb package in order to use grpclb balancer. And if you do not import, then grpc will filter out all grpclb addresses, and only pass the backend addresses to the selected balancer.\nI wonder if you are trying to communicate with grpclb server yourself? \n. @rvdwijngaard If I am understanding your setup right, you would want to implement a custom DNS resolver yourself instead of using the one we provide, as the one we have is sort of specialized to grpc specific features, like grpclb.\nFrom your reply, it seems you don't have remote load balancers and would do local load balancing with round robin. In this case, grpclb should not be used as it is for remote load balancer implementation. And it seems that you prefer to do a SRV lookup instead of a direct AAA lookup, then a custom DNS resolver is required as our DNS resolver does SRV specifically for grpclb remote balancer.\nImplementing your own DNS resolver is not hard. You can copy most of the DNS resolver code, and modify the SRV lookup part (here). You can choose any \"service\" and \"proto\" as you like to register/lookup the service(not having to use \"_grpclb._tcp\"). Then you need to modify the line which returns the resolved address list here to not tag the address type as resolver.GRPCLB, do Type: resolver.BACKEND instead. Your code should work in the same way as before.\nPlease let us know if you have any further question. Thanks!\n. I just tried, and it worked on my desktop, what I am doing is cd into examples/helloworld directory. \nRun the server with the command:\n$ GRPC_GO_LOG_SEVERITY_LEVEL=info GRPC_GO_LOG_VERBOSITY_LEVEL=2 go run greeter_server/main.go\nRun the client with the command:\n$ GRPC_GO_LOG_SEVERITY_LEVEL=info GRPC_GO_LOG_VERBOSITY_LEVEL=2 go run greeter_client/main.go\nAnd I got the following output from client:\nINFO: 2019/01/16 14:35:29 parsed scheme: \"\"\nINFO: 2019/01/16 14:35:29 scheme \"\" not registered, fallback to default scheme\nINFO: 2019/01/16 14:35:29 ccResolverWrapper: sending new addresses to cc: [{localhost:50051 0  <nil>}]\nINFO: 2019/01/16 14:35:29 ClientConn switching balancer to \"pick_first\"\nINFO: 2019/01/16 14:35:29 Subchannel Connectivity change to CONNECTING\nINFO: 2019/01/16 14:35:29 pickfirstBalancer: HandleSubConnStateChange: 0xc000150010, CONNECTING\nINFO: 2019/01/16 14:35:29 blockingPicker: the picked transport is not ready, loop back to repick\nINFO: 2019/01/16 14:35:29 Subchannel Connectivity change to READY\nINFO: 2019/01/16 14:35:29 pickfirstBalancer: HandleSubConnStateChange: 0xc000150010, READY\n2019/01/16 14:35:29 Greeting: Hello world\nINFO: 2019/01/16 14:35:29 Subchannel Connectivity change to SHUTDOWN\nThe following output from server:\n2019/01/16 14:35:29 Received: world\nINFO: 2019/01/16 14:35:29 transport: loopyWriter.run returning. connection error: desc = \"transport is closing\"\nCan you try the exact same commands I use and see if it works? \n. GRPC_TRACE and GRPC_VERBOSITY are irrelevant for grpc go programs, they are for C core based programs.\nYou can try go get -u google.golang.org/grpc to update all dependencies. And you may restart your machine to see if this issue goes away. . Thanks for the fix!. Hi @tomwilkie, apology for my late reply. I took a look at the code, and found that you are proposing change to our DNS resolver for the deprecated resolver API v1. We keep this implementation of DNS resolve for backward compatibility.\nI would suggest having your own separate DNS resolver implementation (essentially DNS resolver v1 with your change here), instead of checking in the change here.\nFYI, DNS resolver which conforms to resolver API v2 is here: https://github.com/grpc/grpc-go/tree/master/resolver/dns.\nLet me know if you have further questions. Thanks!. A notifyChan (type chan struct{}) gets created when someone wants to be notified when state changes. And csm.notifyChan stores a reference to the channel, while the requester also gets passed a reference to the channel. connectivityStateManager uses the reference it has (as stored in csm.notifyChan) to close the channel when state changes. And the requester, listens on the the channel for notification using the reference it gets passed. In other words, csm.notifyChan itself is not shared by multiple goroutines, but the value assigned to it (i.e. the chan) is shared by having multiple copies of reference. . Hi, could you turn on the log, which is more helpful in this case. Thanks.\nGRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info your_command\n. Hi @yinjun622, I was able to build our library with GOOS=andriod without any issue and without the usage of -tags linux. Actually, as specified here: https://golang.org/pkg/go/build/\n\nUsing GOOS=android matches build tags and files as for GOOS=linux in addition to android tags and files.\n\nGOOS=andriod should pick up files for linux. \nFYI, I used the most recent andriod ndk: android-ndk-r19b with MacOS 10.13.6 and go version go1.12 darwin/amd64. Below is my command to build all grpc packages.\nCC=~/Downloads/android-ndk-r19b/toolchains/llvm/prebuilt/darwin-x86_64/bin/aarch64-linux-android28-clang GOOS=android GOARCH=arm64 CGO_ENABLED=1 go build ./...\nPlease let us know if you need anymore help with your issue. Thanks!\n. Use grpc.WithBlock() to block until a connection is established.. Hey @carl-mastrangelo, you can run your program with GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info to look into the internal channel/subchannel state changes and errors. Let me know if you find something suspicious or you can just paste the log here. Thanks!. It's actually on our repo's front page and the  debugging example page. But probably these are not the most obvious pages that someone will first look at.. Hi @BlueStalker, can you file this issue in the grpc-java repo. It depends on what java server does when engine.shutdown() is called. I believe the behavior you saw depends on timing, but I don't have the java implementation knowledge to explain here. Thank you. And it will be great if you can update this bug once you get your answer.. It seems that the original issue has been answered in the java repo, closing this issue now.. Absolutely. As long as your RPC timeout is set to be longer than the method running time, the RPC will be alive until the server handler returns. Please give it a try, and let us know if you encounter any problem. Thanks!. Hi @egonelbre, thanks for the detailed context.\nFirst, the impression that\n\nGiven an error on Send the recommendation is to use CloseAndRecv\n\nis not quite right. CloseAndRecv is called when stream is healthy (no error has occurred), the RPC is of client streaming type, and the client has finished sending all the stuffs. \nIf send errors, then the RPC has finished (i.e stream is done). To get the RPC status, you need to do Recv until an error occurs. You may get multiple messages before you get the error due to buffering. And the final RPC status may be OK, in which case, those buffered messages are valid and need to be processed. If the final status is not OK, some user applications may want to ignore all the buffered messages, but it totally depends on the application. Therefore, we need to deliver the buffered messages in order and error at last for all possible use cases.\nFor your question, given scenario 2, the server will not be able to send more messages because the stream is finished once send errors. So the case you mentioned about non-terminating loop won't happen.\nPlease let us know if you have any further question. Thanks!. I'd like to make some clarification here. Send fails means the stream is done (you can think of it as HTTP2 stream is done). However the transport connection is still up, if the failure wasn't caused by connection failure.\nThere are two concepts here, connection and stream. gRPC is based on HTTP2, which multiplexes multiple streams on the same transport connection. Each RPC corresponds to a stream, so when the RPC finishes the stream finishes. \nFor your question,\n\nOr in other words my main concern is either server/client being able to keep connection open indefinitely when I write such loop.\n\nonce server handler returns, your loop will exit eventually. . In that case, once you got enough info, you can just close the ClientConn you created through Dial. It will tear down all connections set up through the Dial.. Done. server defaults has been moved from server.go to clientconn.go:109. Done.. done. C implementation use this sentence. Do we want to be consistent with them or not?\nhttps://github.com/grpc/grpc/blob/master/src/core/ext/filters/message_size/message_size_filter.c#L141. recv() is also used by serverStream. Done. Done. Done. Done. done.. it worked ;). we chose another method,. deleted. deleted. done. since we need to compare the Metadata field, use map[string]*Update. convert to unexported. . done. unexported. done. it takes the target string and return the setting for host and port, which will later be used to initialize the watcher struct. if it doesn't sounds good, how about parseTarget?. cannot. done.. Deleted. making the length 0 can take advantage of append() and saving an extra index variable, which has to be defined outside the loop.. done. compile means collect info from oldAddrs and newAddrs and generate a list of delta, not a good name?. Use map[Update]bool instead, since there could be collision for Addr (key).. Yes and fixed.\nAn example failure case:\nlb1.test.com. A 1.3.5.7, 2.4.6.8\nlb2.test.com. A 1.3.5.7\n_grpclb._tcp.svr.test.com. SRV lb1.test.com, lb2.test.com. Done. we still need to generate the update. Or we compileUpdate and then return here?. done. Let's discuss offline. Some thoughts: dns resolver is the fallback resolver for target specified without a scheme, so it would be better if it support IP address and that's what C implementation does. . done. done. Use time.Timer instead of time.Ticker to make it happen. Use function can save field specifiers, e.g. Op:, Addr: each time we construct the Update. But I've changed to &Update{}, since I think the inline filed specifier make it more clear.. done. done (toMap). deleted. And change code segments related to this.. done. the second call to Next() will just block since there's no change in resolution (len(results)==0). Not sure what you mean by calling Next() in the main goroutine can make the synchronization simpler.. It seems none of \"localhost\", \"127.0.0.1\", \"[::1]\", can work in all cases, we should probably just don't support empty name and return error in such case. . done. done. Yes, logically w.t.Rest(w.r.freq) doesn't belong to lookup. But I was thinking if it could be regarded as a routine work to do after a lookup finishes, since it makes sense to set the time for the next lookup in the previous one. How about defer w.t.Reset(w.r.freq) at the beginning of the lookup()? Or it's better to separate the reset logic from lookup(). we cannot move the timer blocking inside lookup since then we also need to move the ctx inside lookup. I'll move out the reset logic for clarity.. done. done. done. done. here it says future work here, but I guess we can do it now.. empty can mean reset, e.g. switching from previous service config to no service config. But I just realize that, Mark mentioned that we should have an Option to turn on/off ServiceConfig check. So I may add some check here later I guess.. done. done. done. done. ok. ah, yes. will remove the lock also as it is unnecessary.. ok. sounds good.. done. done. we still need a go routine here: when user calls ResolveNow() on IPWatcher we can return the same IP again, so we need a go routine to monitor user calling ResolveNow() and send IP as needed.. This build tags says go1.6 <= build < go1.8, which essentially means go1.6 and go1.7. So the file name indicates that this build file is for build <= go1.7.. Yes, but LookupHost, LookupSRV and LookupTXT taking in a context starts from go1.8. see here. done. done. yes, and I am indirectly calling NewAddress() in ResolveNow() by sending an empty struct on a notification channel, so inside the watcher it will call NewAddress(). We can also making ResolveNow() running in a gorotine in place of watcher(), but I think our current way is more consistent and clearer.. done. done. done. done. done. this code logic has been deleted in recent commit.. done. done. done. done. right. change has been made to skip storing such method config.. done. done. done. done. filed names are different and go json package cannot handle it.. done. done. done. I agree that we should make maxInt const.\ngetMaxSize was created to handle pointers to avoid dealing with nil condition in caller. It hides the complexity from caller. And min() is just to keep logic simple as we don't need to convert int to *int when returned from it. Probably we should rename these two functions to avoid future confusions (like the input and output are all pointer type)? They are created very specifically for service config and not intended for general usage.. I don't think we can. callInfo.max*MessageSize is a pointer type because it will be used to receive the value set by user API MaxCall*MsgSize when we configure the call. And we need nil value to indicate that user doesn't set it. And then during invoke, we will reassign its value based on service config value, its current value and default value.. From benchmark results, time.Time (~100ns) takes twice the time of int (~50ns).. done.. deleted.. without v and put the constant (e.g. 123) there, it is ~11ns. While with the v, it is ~47ns. I think the compiler is doing something for constant value.. done.\nYeah, it's better to store the pointer to the variable.. done. done. . done. right, the scaling looks much different. Even more gap between atomic and mutex.. Some notes here: StorePointer vs. Store a pointer has the same per operation execution time. But StorePointer has much better (almost 2x) performance in case of contention.. Thanks for pointing out. . Missing the outter conditional?\nif EnableTracing {\n  if a.trInfo.tr != nil {\n  ....\n}. Add here\nfunc init() {\n  channelz.TurnOn()\n}\nReplace turnOnChannelzAndClearPreviousChannelzData with channelz.NewChannelzStorage, and remove the func def here as it is no longer needed.. add ref string func RegisterServer(s Server, ref string) int64.. delete this line, as SPEC has changed, and don't need the socket's metric. Only id and ref pair is need.. store ref string inside channel. Same for other register functions.. done. But the purpose of this check is to verify whether the write of the request succeed or not. errorStreamDone would also mean the write fails. And I am not seeing why it is necessary for the test, and that's how I decided to just delete it.. done. done. OK. Will do in a separate cleanup PR.. nice catch!. Why print the value of c here, did you forget to remove it?. done. done. done. It will benefit users that their peers enforce a header list size limit, but will add overhead to the case that peers don't have a limit.. No, the executeAndPut can be unsuccessful but without any error (i.e. connection doown). And hdrListSizeErr will indicate whether checkForHeaderListSize has any error.. Yes, I would like to do that in a separate PR, as things like max concurrent streams may also need this behavior.. The header would be incomplete and I am not sure what will happen if we return an incomplete header to grpc and the user.. Adding a size field would add overhead to non limit case.. Yeah, I was considering whether the default should be inside flowcontol.go, and decided that probably not. How about I go ahead and make a defaults.go to have all the defaults in one place?. Yes, I think we should. I was under the wrong impression that WriteHeader is the centralized space for both headers and trailers. . Could you please change the error message to \"Default DNS resolver does not support custom DNS server\"? Thanks!. Changed to decodeHeader.\nTried to move if frame.Truncated, but it seems to only save an declaration for a decodeState variable, but complicates the code substantially, e.g. add redundant code for error handling, have to specialize for server and client.. How about adding if size == 1 && ..., since in this way, it will save 1 comparison for each byte when the utf-8 encoding of the character takes more than 1 byte.. change to a status with details containing invalid utf8 message?. \ud83d\ude02fixed. Offline discussion decides that at current stage, enforcement after ack will not be necessary and add complexity to the code. . \"::\" also ends with colon but it is a valid case. \nI think more explanation will be better, may be just add a comment above the definition, like \"address ending with a colon that is supposed to be the separator between host and port is not allowed. e.g. \"::\" is a valid address as it is an IPv6 address(host only) and [::]: is invalid as it ends with a colon as the host and port separator.. Do we want to deprecate this function name and use WithDisableResolverServiceConfig?. I think frequency is minimum, interval is maximum.. changed to minFreq. What's the reason for making PreferGo true?. nit: delete blank line. change file name to pre_go19.go. change file name to go18_test.go. change file name to go19.go. Agree. Just pass network to DialContext, and let the golang net package handle the fallback.. Nit: delete blank line.\n. It's better not comparing the error string directly, as the library may change the error string in the future and break this. You can do something similar to this:\nhttps://github.com/grpc/grpc-go/blob/9907ae0e57ce159e835381ed54eb271206bf7b12/resolver/dns/dns_resolver.go#L321-L336\nAnd in case the authority is a malformed IP address, return an error. We may want to fallback to default dns resolver if the authority is invalid.. use net.JoinHostPort.. preGo18Resolver?. change file name to pre_go18_test.go. go18upResolver is unnecessary, defaultResolver netResolver = net.DefualtResolver. What's the reason for setting PreferGo to true?. Fallback to default resolver?. 2018. 2018. 2018. 2018. change file name to pre_go18.go. It's preferred not to pull in extra dependency.\nI agree with @kriskowal suggestion about having a global function pointer for usage inside customAuthorityResolver. We don't really need to have a round trip of DNS request/response, we just need to verify the resolver try to dial to the authority specified in the target string.. Instead of inline the function definition here, we define a global variable to be the same as the inline function, and assign the global variable to Dial here.\nAnd in the test, we substitute the global variable value to a fake dial function which does not dial to a dns server, but instead verifying the dial target is correct.. go18upResolver type seems unnecessary. *net.Resolver should satisfies the netResolver interface.\ngo\ndefaultResolver = net.DefaultResolver\n. The bare minimum test cannot test the logic(e.g appending \":53\" for authority without port) inside customAuthorityResolver. We need something like your first code snippet.. We needs more processing of the authority string. For example, net.SplitHostPort will fail to process ipv6 address without brackets(e.g. dns://::1/fake.com). Please take a look at the parseTarget function here and do something similar. And it will be helpful to have a comment for the valid formats supported for the authority string and write some tests for valid and invalid authority strings.. the &go18upResolver as commented above, is unnecessary. Just return &net.Resolver{...}. This is actually an issue with our testing script (not using the most recent sdk for appengine test). I've fixed it through #2311. \nAlso, let's just support custom authority dialer from go1.9 (where type alias is supported, and thus the two context types are regarded as the same). Please change the build tags accordingly.. nit: delete empty line.. Line 101-112 are unnecessary. ensureAuthorityPort will be called inside b.Build so we can just check the error returned from b.Build.. we don't need to dial in the test, just return something like nil, errors.New(\"no need to dial\").. check the error returned by b.Build, essentially the logic from Line 101-108.. It seems ensureAuthorityPort share substantial logic with parseTarget. Could you please merge these two functions into one. So parseTarget takes an extra argument like defaultPort. And here for ensureAuthorityPort, you calls parseTarget with the new arg to be \"53\", and then call the net.JoinHostPort on the returned host and port.. We don't care the error from the customAuthorityDialler, but we do want to check the error from customAuthorityResolver (which is essentially the error returned by ensureAuthorityPort if not nil). I think the error from customAuthorityResolver is returned by b.Build.. nit: delete empty line.. nit: delete empty line. why t.FailNow() is needed?. nit: delete empty line. nit: delete empty line. define a constant(e.g. defaultDNSSvrPort) for \"53\", and replace it here and in tests.. Since the the dialler will be called in a separate goroutine(i.e. the watcher goroutine), the Errorf here may not be called before the main test goroutine exit.\nThe better solution here is to create a channel of error type. And inside the custom dial function, push an error(or nil for no error) to this channel. And in the main goroutine, wait until an error is pushed to the channel, and check whether it is nil or not.\ngo\nfor _, a := range tests {\n    errChan := make(chan error, 1)\n    customAuthorityDialler = func(...) {\n        ...\n        return func(ctx context.Context, network, address string) (net.Conn, error) {\n            if authority != a.authorityWant {\n                errChan <- fmt.Errorf(...)\n            } else {\n                errChan <- nil\n            }\n        ...\n    }\n    ...\n    // wait until the check in the dial function completes\n    err := <- errChan\n    // verify whether there's an error or not.\n    if err != nil {\n        t.Errorf(...)\n    }\n}. add a blank line between functions.. add a blank line between functions.. add a blank line between functions.. add a blank line between functions.. It's better to change this line to \"4.3.2.1:\"+defaultDNSSvrPort,, so in the future if we need to change the default port for DNS server, we don't need to also change the tests.. same here, s/53/defaultDNSSvrPort. same here, s/53/defaultDNSSvrPort. same here, s/53/defaultDNSSvrPort. It will be invoked when user try to get socket option for a socket on a platform that's not supported. I think log it once may make it easy to be overlooked. Or we can do sync.Once(grpc.Warningf(...) to only log once but draw more attention?. nit: delete blank line. nit: delete blank line. nit: remove PrintStack(). suggestion\nfunc newClientHealthCheck(ctx context.Context, newStream func() (interface{}, error), reportHealth func(bool), service string) error {\nI've updated the update function to be renamed to reportHealth elsewhere, let's keep consistency here.. suggestion\n                reportHealth(true). suggestion\n                reportHealth(false). suggestion\n * Copyright 2018 gRPC authors.. I think the content after line 430 is not necessary for the test?. We need this for channelz (subchhannel stat will include health checking rpc). I've updated the code to initialize done correctly, which I forgot to do earlier.. Yes. I've verified that it is correct to do so. . This comment is for close(skipReset) not the newTr.Close(). But this comment is unnecessary anyway since ac is in SHUTDOWN state where no reconnecting will ever happen.. no need to hold the lock actually for ac.newClientStream. unlock before it.. done. done. I think it's ok to move. done. done. HealthCheckFunc will do that.. done. It does not necessarily need to block, but it saves creating a new go routine and potential delay due to that, so removed.. I want to let each of them have a 5s deadline, I don't think a single context can do.. shortened the error message and added a TODO to add a link to the health check doc later for users to better understand the error.. done. I've fixed in my merged PR that the returned error should be included in the test error message. Essentially, t.Fatalf(\"failed to listen due to err: %v\", err).. same here, add error message.. change error message to ClientConn is still in IDLE state when the context times out. since we now have a context that cover the whole test.. same here.. done. make a test copy of all the cipher suites for checking against.. right. I think the original idea is that we may want false, nil sometimes to skip creating intermediate errors. But it seems we don't really have such use case yet. We probably should just refactor to what you suggested.. why not just use the grpc/testdata package?. define server?. package description.. package description. package description?. package description?. Add a link to the resolver builder and resolver interface? \nAnd either in the readme or in the code, describe that user need to implement the resolver builder interface and the resolver interface to have a complete grpc resolver component implementation?\n. comment that exampleResolver implements the resolver interface. And add the link to the interface godoc?. comment that this step register the exampleResolverBuild with the scheme \"example\"? \nAnd say that user should register custom resolver at init time?. comment that exampleResolverBuilder implements the builder interface and link to that?. for this example there's just one backend, can we just start one server, which make the main function a bit simpler?. mentioned that \"pick_first\" and \"round_robin\" are the two types of balancer that is supported native by grpc?. db.get().GetServers(id, 1) may not get the result for server with the specified id. It will get the first server with id>= specified id. I think you need to implement a new function db.get().GetServer(id) on the channelMap struct, which will find the server with the specified id. . s/servers/server. Why client and server have different min values?. s/4000 * time.Millisecond/4 * time.Second?. Warning? as there's no formatting needed here.. s/4000 * time.Millisecond/ 4 * time.Second?. fr declared but not used.. done. moved comments.. Deleted the reset. I don't think reset is done, we just create a new struct every time. This is old comments from before.. rephrased. Mentioned Trailers (after headers) shouldn't even have a content-type, and we ignore checking it.. done. done. done. done. remove c, it's not useful. done. It's possible, but requires tricky ways, i.e compare error string is empty or not, and decide the final status error message. Decided not to do that from offline discussion.. delete. delete this, if it does not belong to the TODO above.. why take rand?. If don't do rand, is it possible to do some simple test?. net.JoinHostPort will be better.. Sleep for 1s does seem a bit risky under extreme race condition. . TODO is irrelevant now, channelz event is recored inside updateResolverState.. WIP to export a doc about canonical service config json string.. done. done. If we have something like\nfunc WithDefaultServiceConfig(s string) (DialOption, error) {\n    sc, err := parseServiceConfig(s)\n    if err != nil {\n        return nil, fmt.Errorf(\"the provided service config is invalid, err: %v\", err)\n    }\n    return newFuncDialOption(func(o *dialOptions) {\n        o.defaultServiceConfig = sc\n    }), nil\n}\nThen it breaks the idiomatic way of doing grpc.Dial(target, grpc.WithXXX(), grpc.WithYYY()). I think separating the validation is ok. I can add a comment saying that it's strongly recommended to verify the validity of the json string with the ValidateServiceConfig function.. done. done. ",
    "bernerdschaefer": "My initial expectation of ClientStream was that it would have a Close() error method, with usage like:\n```go\nconn, _ := grpc.Dial()\ndefer conn.Close()\nstream, _ := NewClient(conn).StreamUpdates(...)\ndefer stream.Close()\n```\nThe absence of such a Close method on the stream lead me to assume that closing the ClientConn would close open streams.\nI don't know that I have a strong preference either way, except to say that it's not obvious from the current functions and interfaces that you must manage cancelation of the stream context yourself.\nI can see how the second option might be a big \"gotcha\" if the interface can't be updated to include Close, as any documentation around that wouldn't end up in user's own generated client packages.. ",
    "egoldschmidt": "We still see this from time to time even with the patch. It's possibly less frequent but I can't be sure.. Also I don't believe we are setting deadlines nor doing 100+ concurrent RPCs in our tests, where we most frequently see this bug.. Correct it's still happening sporadically. I don't think metadata is\nrelated.\nWe are running through Lyft's envoy. We are not (knowingly) setting\ndeadlines. All calls are unary.\nOn Mon, Jun 19, 2017 at 1:58 PM dfawley notifications@github.com wrote:\n\nThanks for the follow-up.\nIt's unlikely the original issue was seen because of malformed metadata,\nbecause it sounds like it was happening sporadically. Is this still\nhappening? If so, we could use some help in reproducing it (please see\n@MakMukhi https://github.com/makmukhi's comment: #1134 (comment)\nhttps://github.com/grpc/grpc-go/issues/1134#issuecomment-288591079).\nThanks!\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1134#issuecomment-309571748, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AA42mLEaYZpS8Xrrwe3scc7eMRbFrWA4ks5sFuD6gaJpZM4MjV66\n.\n-- \n(phone)\n. \n",
    "tiziano88": "This started happening to me as well. I noticed that it seems related to setting a large value for a metadata value. The value happens to be a JWT token, so plain ASCII, but it seems that changing the metadata key to have the -bin suffix fixes the issue, even though the value is not actually binary. Will try and come up with a reproducible test case shortly.. Turns out, it was an extra newline in one of the metadata values (which I was reading from a file, and the editor was helpfully adding a trailing newline at the end and not making it obvious).. ",
    "zllak": "I can confirm we see it too in production from time to time. We end up being called with a canceled context. We initially thought it was due to the client context being canceled, but even with a test where the client call with a non cancelable context, the server-side context ends up canceled.\nAny idea what could cause this issue ? this is quite a huge problem for us.\nWe'll try to do some digging on our side to try and understand what's happening.\nWe have seen it mostly on unary calls, sometimes on streams.. @MakMukhi\n- Client side error is a \"context canceled\", not a \"stream terminated by RST_STREAM with error code: 8\"\n- Both server and client in golang (also, client is running on arm, might change nothing)\n- Unfortunately no, we still can't reproduce it 100%, it pops randomly in production. Once we can put together a real test case, I'll share it.. @menghanl I've signed the CLA, but it seems that the bot did not see it.\nAnd now when I go back on the website, it cannot find the PDF but it has registered that I've signed the CLA :/. ",
    "mehrdada": "@kahuang seems to have run into a similar issue here: https://github.com/grpc/grpc/issues/11586. In the latest version of grpc-go (on master), some things are changed, and you can feed the error object you have to [grpc/status.FromError][1] and it returns ok=false if it is not actually a statusError. Does something like the following resolve your issue?\ngo\nif st, ok := status.FromError(err); ok {\n    // use `st.Code()` here to get the status code\n} else {\n   // etc...\n   switch err.(type) {\n   }\n}\n[1]: https://github.com/grpc/grpc-go/blob/master/status/status.go#L128. If everything goes right, a 1.3 release should be right around the corner (probably in a week or so, maybe less).. I am closing the issue, but please reopen and do let us know if that time frame would not be acceptable to you.. UPDATE:\nI can now see the problem only after setting the tracing environment flags to the server process. Can you verify seeing the issue when tracing is disabled or does it only happen when it is on?\nPrevious message:\nI am unable to reproduce this problem on master for gRPC C++ & Go. Seems to work fine on my machine (Ubuntu 16.04.2 LTS, Linux 4.8.0-45-generic #48~16.04.1-Ubuntu SMP Fri Mar 24 12:46:56 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux, Go 1.8.1, gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609). Which versions of gRPC C++ and Go are you using and on what platform?\n~/go/src/google.golang.org/grpc/examples/route_guide$ go run client/client.go --server_addr localhost:50051\n2017/04/26 20:16:36 Getting feature for point (409146138, -746188906)\n2017/04/26 20:16:36 location:<latitude:409146138 longitude:-746188906 >\n2017/04/26 20:16:36 Getting feature for point (0, 0)\n2017/04/26 20:16:36 location:<>\n2017/04/26 20:16:36 Looking for features within lo:<latitude:400000000 longitude:-750000000 > hi:<latitude:420000000 longitude:-730000000 >\n2017/04/26 20:16:41 Traversing 10485760 points.\n2017/04/26 20:18:20 Route summary: point_count:10485760 distance:-1619001344 elapsed_time:98\n2017/04/26 20:18:20 Got message First message at point(0, 1)\n2017/04/26 20:18:20 Got message Second message at point(0, 2)\n2017/04/26 20:18:20 Got message Third message at point(0, 3). I'm still investigating the issue to get at the root cause. It is a bit more nuanced than initially expected. . @yancl Have you downgraded both the server and the client or are you just downgrading the C++ server?. This issue has been super hard to debug and needs more work, but just to give you an update, at this point it sounds like the problem might be in the http2 package's handling of settings frames appropriately.. It seems #1262 has the same root cause and https://github.com/grpc/grpc/commits/80ccebb2e16355666809eb5368751683149eb6a8 on the C repo is the commit beyond which the bug is manifested clearly. However, simply undoing the commit on master is not helping, which indicates the root cause is probably somewhere else and that commit just makes it more easily visible.. Integer overflow bug fixed by #1268. Please let us know if you see similar issues.. I commented here about the max message sizes at https://github.com/grpc/grpc-go/pull/1428#issuecomment-321037862 , but here might be a more appropriate place to discuss the max message size issue. Where does the max message size get enforced? Simply checking at the end of decompression is certainly not enough and not very useful either. Ideally, a decompressor would only allocate a small buffer and flushes it in the writer downstream, and that underlying writer should be aware of the max message size and return an error on Write if the decompressor is flushing too much data. That error should be then propagated by the decompression filter.. Cool! Thanks. Just wanted to make sure this requirement does not get forgotten.. Have you considered carrying additional data in context ([Context.WithValue][1]) to accomplish what you need?\n[1]: https://golang.org/pkg/context/#WithValue. We discussed this internally and some of us have mixed feelings about using a CallOption in that way, so we decided we are not ready to embrace expansion of the CallOption API, but we will keep that in mind as the situation evolves\u2014I have tagged the issue as \"suggestion\" so we can continue thinking about it. Thank you for the suggestion.. @mwitkow We do not think it is necessarily \"bad\" as you put it. Rather, it is not how CallOption were originally intended to be used.  For what it's worth, a CallOption from a user's perspective looks like not much more than an enum-esque thing, and that's why the before/after methods are private in the first place. Right now, we have the freedom to completely change how CallOptions are implemented internally with very few restrictions without breaking users.  \nIf I understand correctly, what you really want is an extensibility mechanism for adding custom hooks before and after a call. Right now, carrying additional data with context.WithValue that an interceptor can look into and check if it is of a certain type is one way to accomplish this\u2020.\nThat said, it might indeed be a worthy goal to consider introducing more generic custom before/after hooks and we should give it some deep thought when we get a chance, and compare the various possible designs (and whether going the shortest path and making CallOption the mechanism to do so is the best option). I don't think we should jump through it and make this decision lightly simply because it is easy to add such feature with minimal code changes. Please also note that adding an API may be easy, but it is very hard for us to change or remove bad APIs.\n[\u2020] It's not clear to me how it would have negative type-safety implications relative to the suggested solution, as both options seem to have to basically deal with an interface{} object anyway.. Thanks for reporting the issue. We are working on improving gRPC-go performance from various dimensions (QPS, latency, memory usage) and while I do not have a set schedule for you on that specific issue, we are actively working on performance in the next few quarters.. I am closing this issue as it falls into our general performance improvement work and goals, so it is easier to track in a centralized fashion and we will be addressing it there.. It's not clear how we can cancel a Send once you have started it without resetting the stream. This sounds like a protocol level limitation.. @louiscryan Right. That is absolutely correct. The OP's requirement of cancelling writes without resetting stream was what I was referring to. Having a cancellable API is not.\nHowever, I think doing that is going more than halfway the path of adding a fully async API, so if we were to do that, one might as well go all the way and make it return a Future.. PTAL. @dfawley I agree. It looks very much so.\n@olomix Can you please run the python server like the following and then run the go client and see what the error message it prints out?\nGRPC_TRACE=all GRPC_VERBOSITY=ERROR python client.py\n(if you don't see anything relevant, try GRPC_VERBOSITY=INFO or DEBUG as well.)\nIn particular, does it say something along the lines of \"flow control of X incoming window of Y\"?. @olomix  Thanks for the repro case and logs. The symptoms indeed look very similar to #1192.. I managed to narrow the initial revision where the repro case seems to be broken down to commit https://github.com/grpc/grpc/pull/9230/commits/80ccebb2e16355666809eb5368751683149eb6a8 on the C implementation. (PR: https://github.com/grpc/grpc/pull/9230), which, as expected, is a flow control-related change. I used to think that the go implementation is doing something wrong, however, based on my observations, while still inconclusive, I am more inclined to point to the C direction at this point.\nI'll look a bit more and probably will end up filing a bug on grpc/grpc C implementation and closing this one.. This patch on the C core (simply removing 1 + from https://github.com/grpc/grpc/blob/master/src/core/ext/transport/chttp2/transport/chttp2_transport.c#L2297) seems to work around the problem (though I can't reproduce it on current HEAD\u2014i.e. it may be already fixed\u2014and it might not be a fix for the root cause).... I am now quite certain this is the same issue and I'm closing this bug as a duplicate of #1192. @olomix please hop on that thread to track the status.. The issue seems to be caused by mishandling of flow control arithmetic due to an integer overflow. #1268 fixes this. . Thanks a lot for your PR. Last night, while debugging #1192, I concluded that we have some overflows that are causing the issue, so I'm glad that it's fixed automatically by someone else \ud83d\udc4d . I think we need to look more closely at all the flow-control arithmetics for potential overflows :). @ahmadsherif  Do you happen to know when GitLab signed the CLA? I am unable to find \"GitLab Inc.\" as a signer of Google CLA. . @ahmadsherif There seems to be no record of GitLab Inc. ever signing Google's CLA. Can you follow up with folks on your side? I am interested in merging this as soon as possible if we can figure out the CLA issues. Thanks!. Does the decompressor processing a stream get to know the max message size to be able to interrupt processing if the decompressed size starts to exceed it?. b.backlog[0] = recvMsg{} is more robust if we add fields to the struct in the future.\nAlso, do we ever close the channel in recvMsg?. s/user-applicaiton/user application\n(ditto for the comment further down). typo: connection. Typo: GIP -> GZIP on this line and above. sorry, but typo again: GIZP -> GZIP. ",
    "philipithomas": "Instead of doing a JWT, I'm trying to use protobuf for a token. It's triggering this same issue while in the \"authorization\" metadata. I suspect it has to do with newlines in the encoded message.\nSwitching the md key to \"authorization-bin\" seems to have fixed the issue.. ",
    "sslavian812": "I got nearly the same error about RST_STREAM with error code 2. \nCan't figure out what is it exactly about, but I believe that it's close to this issue.\nI described the problem in a SO question, but not one answered for a while.\nhttps://stackoverflow.com/questions/48174240/grpc-rendezvous-terminated-with-statuscode-internal-received-rst-stream-with\nIt would be very helpful to get some hints what to look at.. ",
    "btc": "@egoldschmidt Have you found the root cause of your disconnects?. I completed the CLA form.. Are there preliminary comments about the \"capabilities of replay\" you mention?. Got it. That's fairly problematic. Thanks for letting me know.. Note that end time was not being set here.. ",
    "nmccready": "\nI got nearly the same error about RST_STREAM with error code 2.\nCan't figure out what is it exactly about, but I believe that it's close to this issue.\nI described the problem in a SO question, but not one answered for a while.\nhttps://stackoverflow.com/questions/48174240/grpc-rendezvous-terminated-with-statuscode-internal-received-rst-stream-with\n\nFor whatever it is worth, I am seeing this same error in Node.js and the stack overflow hint at fork made me think it was something with jest. I removed jest and went to mocha and the error went away.. ",
    "charithe": "I signed it.. Great to hear that the issue has been fixed. I resolved the merge conflicts and the code is now ready to merge.. Removed the merge artifacts and also added back a test that I had missed during the merge.. ",
    "jwhitcraft": "I'm seeing this as well,  At first i thought it was the my LB pinging it for the health check, but that was not the case.  I believe ti might be cadvisor but i'm unable to confirm.  \nI'm deployed on kubernetes that was spun up with kops.    ~~I also have heapster installed, which i'm not sure if that would be causing this.  @ahmetb do you have heapster installed?~~\nEdit I removed heapster and it didn't change anything.. @yifan-gu ,\nI'm on AWS running a 15 node setup.   Here is what is being out in my pod container logs\n2017-03-24T03:02:50.875733495Z 2017/03/24 03:02:50 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:45846: write: broken pipe\"\n2017-03-24T03:03:00.875836691Z 2017/03/24 03:03:00 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:45874: write: broken pipe\"\n2017-03-24T03:03:10.875710388Z 2017/03/24 03:03:10 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:45896: write: broken pipe\"\n2017-03-24T03:03:20.875834934Z 2017/03/24 03:03:20 transport: http2Server.HandleStreams failed to receive the preface from client: read tcp 10.34.128.6:35001->10.34.128.0:45918: read: connection reset by peer\n2017-03-24T03:03:30.875756285Z 2017/03/24 03:03:30 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:45946: write: broken pipe\"\n2017-03-24T03:03:40.875790720Z 2017/03/24 03:03:40 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:45968: write: broken pipe\"\n2017-03-24T03:03:50.875740859Z 2017/03/24 03:03:50 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:45990: write: broken pipe\"\n2017-03-24T03:04:00.875691712Z 2017/03/24 03:04:00 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:46018: write: broken pipe\"\n2017-03-24T03:04:10.875776520Z 2017/03/24 03:04:10 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:46040: write: broken pipe\"\n2017-03-24T03:04:20.875751164Z 2017/03/24 03:04:20 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:46062: write: broken pipe\"\n2017-03-24T03:04:30.875698290Z 2017/03/24 03:04:30 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:46090: write: broken pipe\"\n2017-03-24T03:04:40.878118868Z 2017/03/24 03:04:40 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:46112: write: broken pipe\"\n2017-03-24T03:04:50.875753685Z 2017/03/24 03:04:50 transport: http2Server.HandleStreams failed to receive the preface from client: read tcp 10.34.128.6:35001->10.34.128.0:46134: read: connection reset by peer\n2017-03-24T03:05:00.875689813Z 2017/03/24 03:05:00 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:46162: write: broken pipe\"\n2017-03-24T03:05:10.875729180Z 2017/03/24 03:05:10 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: write tcp 10.34.128.6:35001->10.34.128.0:46184: write: broken pipe\"\nI do have prometheus scraping monitoring setup on port 35002, but my gRPC service is on 35001 and i have a k8s Service setup so i can access the gRPC service from inside the cluster.\n@ahmetb The issue starts up right away for me, however I do not see this issue when running locally.. @ahmetb and @yifan-gu \nI just figured this out.  if you have a livenessProbe hitting that port, it's was is causing that log, i just switched mine to hit the metrics port for prometheus and the logs went away.\n. ",
    "yifan-gu": "@ahmetb where does the log come from? From your grpc server?. @jwhitcraft Ahh, that makes sense, thanks for the info!. ",
    "ericgribkoff": "The Go docker image failed to build at https://grpc-testing.appspot.com/job/gRPC_interop_master/12001/console and https://grpc-testing.appspot.com/job/gRPC_interop_master/12006/console, both since this fix was merged. I rebuilt 12006 and it passed, but building Go is still flaky for some reason.. This might be a different error than originally reported here. I can reproduce the failure locally about one out of ten runs of the run_interop_tests.py script. Output here. I'm not too familiar with Go, but looks like it's running into a problem when fetching dependencies. Is this an issue with a third-party that we're just encountering intermittently?. It seems to have been a transient failure. The Go docker image failed to build twice in a row yesterday morning, but it's been stable on master since.. ",
    "earies": "I signed it!. Following up if anyone can pickup review or comment on this pull - thx. Following up again if this can be picked up for review/comment. Following up if anyone can pickup review or comment on this pull - thx. Following up again if this can be picked up for review/comment. ",
    "elvizlai": "Wow, this changes broke backward compatibility, especially our trace system.\nEvery request has a trace-id set to ctx.\nFor example, service A call B, B call C and then B resp to A.\nWe do not need handle ctx in v1.2.1.\nBut now we need to get MD from ctx received in B and put it into a new ctx before we call C.. Yes\uff0cI know using a new object or generated code can avoid this.\nWe has a framework using Invoke to make gRPC call more easily in our project. It handles conn automatic(we made a map of conn and service name). What we do is using service name and method name to make a rpc call.\nFor example\nRRC(ctx, \"greater\", \"SayHello\" ,in ,out)\nUnfortunately this failed now if in and out using same ptr.. Using a not clear output, it works like append too.\n```\n    for i := 0; i < 100; i++ {\n        x := &pb.HelloRequest{Name: []string{\"a\", \"b\"}}\n        y := &pb.HelloRequest{Name: []string{\"c\", \"d\"}}\n        err = grpc.Invoke(context.Background(), \"/pb.Greeter/SayHello\", x, y, conn)\n        if err != nil {\n            log.Fatal(err)\n        }\n        log.Printf(\"Greeting: %s\", y.Name)\n    <-time.After(time.Second)\n}\n\n```\nOutput:\n2017/05/05 09:38:37 Greeting: [c d a b]\n. @menghanl \nHow about adding v.(proto.Message).Reset() before https://github.com/grpc/grpc-go/blob/master/codec.go#L99. I signed it!. @menghanl  look like it not works well in k8s. because some time, the client will up first, but not the server. So the dns record maybe empty, and it will work after 30 min.\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: test\n\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: test\n  name: server\nspec:\n  clusterIP: None\n  ports:\n    - name: grpc\n      port: 1234\n  selector:\n    app: server\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: server\n  namespace: test\nspec:\n  replicas: 5\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        app: server\n        ver: v1\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: server\n        image: \"sdrzlyz/istio-demo-server:v1\"\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: grpc-port\n          containerPort: 1234\n        readinessProbe:\n          tcpSocket:\n            port: grpc-port\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: grpc-port\n          initialDelaySeconds: 15\n          periodSeconds: 20\n\napiVersion: apps/v1beta1\nkind: Deployment\nmetadata:\n  name: client\n  namespace: test\nspec:\n  replicas: 1\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        app: client\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: client\n        image: \"sdrzlyz/istio-demo-client\"\n        imagePullPolicy: Always\n        env:\n        - name: SADDR\n          value: \"dns:///server.test.svc.cluster.local:1234\"\n```. ",
    "JelteF": "I'm having the same problem. As a when calling grpc_cli call ... I now use the following extra arguments:\n--proto_path=.grpc/third_party/protobuf/src/ --protofiles=.grpc/third_party/protobuf/src/google/protobuf/timestamp.proto\nHere .grpc is the directory where I checked out the grpc source code.\nThis does not work for grpc_cli ls though, because it doesn't support these extra options.\n. As another datapoint. This has flooded our logs after updating to the latest gRPC. The reason for this is that we have a cronjob that connects to the process. We have now disabled all logging for gRPC because of this.. I signed it. @dfawley Could you explain why it's necessary to disallow any lower ping than 10s/1s? We've been using a keep alive interval of 100ms (both Timeout and Time) successfully for a long time now. We use it detect connection failures quickly. To give you some indication of our setup, it's within one datacenter with a limited number of clients. This change is blocking for us to upgrade to grpc 1.19.0.. Thanks for the quick response. To clear up one thing, we use 100ms as the client keep alive. This is to very quickly detect a broken server process so we can failover to another server.\nIt makes sense to follow the spec better, but in this case it would be really nice if the minimum keepalive could be configured by the library user as well. Especially since lower keep alives were possible before. And also because the server already has the minimum time enforcement policy.. ",
    "wltony": "get protoc release (any platform ) and  copy the protos files in include directory  to $GOROOT\\pkg\\include, then should work.. ",
    "marcushines": "I signed it!. ",
    "tamalsaha": "I tried the above code snippet. It does not work. metadata.MD converts headers keys into lower case. On the other hand Request.Header, converts all keys to upper case.So, you need to do the following to make things work:\n```\npackage auth\nimport (\n    \"fmt\"\n    \"net/http\"\n    \"testing\"\n\"google.golang.org/grpc/metadata\"\n\n)\nfunc TestGetFromCookie(t *testing.T) {\n    md := metadata.Pairs(\"Key-A\", \"Val-A\")\n    req := &http.Request{\n        Header: http.Header{},\n    }\n    for k, v := range md {\n        for _, val := range v {\n            req.Header.Add(k, val)\n        }\n    }\n    // req := &http.Request{Header: http.Header(md)}\n    fmt.Println(req.Header.Get(\"Key-A\"))\n}\n```. ",
    "allencloud": "\nAnd the new function FromError does what you need.\nThat is true.\n\nI need to make sure whether the error is an rpcError, then transfer the rpcError's code into http status code.. Yeah, I could get the answer in the latest version pf grpc-go. Thanks a lot @menghanl \nClosing this.. ",
    "breerly": "I signed it!. @dfawley looks like gimme has 1.9 now:\n```\ngoogle.golang.org/grpc-go - [go1.9] \u00bb gimme 1.9\nunset GOOS;\nunset GOARCH;\nexport GOROOT='/Users/grayson/.gimme/versions/go1.9.darwin.amd64';\nexport PATH=\"/Users/grayson/.gimme/versions/go1.9.darwin.amd64/bin:${PATH}\";\ngo version >&2;\n```. ",
    "tsl0922": "You may need to update your golang.org/x/net/http2 package.. ",
    "kristinn": "Ahh! That fixed it! Thanks! :facepalm:. ",
    "yancl": "How to produce:\n```\n// start server\ncd grpc/examples/cpp/route_guide\nGRPC_TRACE=all GRPC_VERBOSITY=DEBUG ./route_guide_server\n// edit client\ncd google.golang.org/grpc/examples/route_guide\nedit client/client.go\n 89 // runRecordRoute sends a sequence of points to server and expects to get a RouteSummary from server.\n 90 func runRecordRoute(client pb.RouteGuideClient) {\n 91     // Create a random number of random points\n 92     r := rand.New(rand.NewSource(time.Now().UnixNano()))\n 93     //pointCount := int(r.Int31n(100)) + 2 // Traverse at least two points\n 94     pointCount = 10 * 1024 * 1024 //emulate a lot of message\n// run client\ngo run client/client.go\n```\nthen you will find the following errors in client:\n2017/04/20 08:12:01 Traversing 10485760 points.\n2017/04/20 08:12:02 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/04/20 08:12:02 &{0xc4200746e0}.Send(latitude:520000000 longitude:500000000 ) = rpc error: code = Internal desc = transport is closing\nexit status 1\nand you will find the following errors in server:\nD0420 08:12:02.682009547   14262 chttp2_transport.c:2154]    set connectivity_state=4\nD0420 08:12:02.682035245   14262 connectivity_state.c:184]   SET: 0x7f7d440014c8 server_transport: READY --> SHUTDOWN [close_transport] error=0x7f7d442ca150 {\"created\":\"@1492675922.681983109\",\"description\":\"Delayed close due to in-progress write\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":521,\"referenced_errors\":[{\"created\":\"@1492675922.681977493\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":1971,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1492675922.681963731\",\"description\":\"frame of size 27 overflows incoming window of 3\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":412}]}]}\nD0420 08:12:02.682038018   14262 connectivity_state.c:211]   NOTIFY: 0x7f7d440014c8 server_transport: 0x7f7d44005360\nwhen make the same change with the grpc c++ client, no problem appears.\n// c++ client output:\nDB parsed, loaded 100 features.\nFinished trip with 10485760 points\nPassed 6708938 features\nTravelled 111149056 meters\nIt took 147 seconds\n@iamqizhao @menghanl  any thoughts? thanks.. and i add a go client with go server test.  no problem appears\n```\ngoogle.golang.org/grpc/examples/route_guide$ go run server/server.go\ngoogle.golang.org/grpc/examples/route_guide$ go run client/client.go\n2017/04/20 08:51:03 Traversing 10485760 points.\n2017/04/20 08:57:16 Route summary: point_count:10485760 distance:1483317118 elapsed_time:372\n```\nso it seems like the problem occurs between go and c++.. the c++ client with golang server, no problem appears\ngrpc/examples/cpp/route_guide$ ./route_guide_client\nDB parsed, loaded 100 features.\nFinished trip with 10485760 points\nPassed 10485760 features\nTravelled 733387647 meters\nIt took 355 seconds\n. @mehrdada, thank you for your reply. \nWhen i start a new clean ubuntu vm(Linux ip-10-1-7-106 3.13.0-48-generic #80-Ubuntu SMP Thu Mar 12 11:16:15 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux) and use the master branch of gRPC c++&golang code. The problem does not occur easily without tracing open. so i think maybe the c++ server consumes messages too fast. then i add a sleep(10) to the c++ server handler to slow down the message consumption, and the problem occurs easily now.\n```\n~/grpc/examples/cpp/route_guide$ ./route_guide_server\n//code changed\n133   Status RecordRoute(ServerContext context, ServerReader reader,\n134                      RouteSummary* summary) override {\n135     Point point;\n136     int point_count = 0;\n137     int feature_count = 0;\n138     float distance = 0.0;\n139     Point previous;\n140\n141     system_clock::time_point start_time = system_clock::now();\n142     while (reader->Read(&point)) {\n             // to slow down the process\n143       printf(\"sleep 10 seconds\\n\");\n144       sleep(10);\n145       point_count++;\n146       if (!GetFeatureName(point, feature_list_).empty()) {\n147         feature_count++;\n148       }\n149       if (point_count != 1) {\n150         distance += GetDistance(previous, point);\n151       }\n152       previous = point;\n153     }\n```\n```\n/go/src/google.golang.org/grpc/examples/route_guide/client$ go run client.go\n//errors from golang client\n2017/04/29 05:44:58 Traversing 10485760 points.\n2017/04/29 05:45:28 transport: http2Client.notifyError got notified that the client transport was broken EOF.\n2017/04/29 05:45:28 &{0xc42006e580}.Send(latitude:-680000000 longitude:-830000000 ) = rpc error: code = Internal desc = transport is closing\nexit status 1\n```. Thank you very very much for that @mehrdada .\nCurrently we downgrade grpc(c++) from 1.2.0 to 1.0.0 to workaround the problem and it works well as i know.. @mehrdada i downgraded the grpc c++ server code while using the latest grpc golang client code. well, it is just as a note and may be not that useful.\nYes, thank you for your update and hard work:). Thank you very much @mehrdada :). ",
    "nullaus": "@MakMukhi\nBad Commit: https://github.com/grpc/grpc-go/commit/6d0e6b04b3cd553e2007ef075fd2445a192e4c0d. Thanks! :). ",
    "utrack": "OK, so it seems that it happens if pre-1.7 x/net/context is vendored and some requests are canceled in-flight.\nSee https://github.com/utrack/grpc-go-poc-1201 , stable reproduce. Also, some (configurable) way to print the body we've received would be cool.. ",
    "damanloox": "I've a long-lived client (of gobgpd server) and a stream coming from the server. I need to be able to detect if/when the server goes offline and back to be able to reconnect the stream (and notify end user of the problem). Unfortunately I don't have control on the server (besides public API) so I need to be able to detect connection state changes on the client.... ",
    "anticpp": "Actually, It's on the client side, I need to get the service name or method name(known as FullMethodName) to identify a scope in OAuth.. Well, It seems fix at gRPC 1.4.1?\n2017/07/10 20:52:21 [xlsoa] [OauthKeySource] [Error] sync() error: SyncKey fail: rpc error: code = Unimplemented desc = Not Found. ",
    "mandarjog": "Returning a  Future would be even better.. The last option is more usable, and I agree that cancellation are very general purpose.\nHowever for the specific use case, \nstream := createStream().withWriteTimeout(\"1s\")  \nwould be even more usable. . ",
    "disksing": "Just in case someone run into this issue, the done := make(chan struct{}) in above code snippet should be done := make(chan struct{}, 1), or the sender goroutine will leak if receiver timeout. /cc @dfawley . ",
    "rfarnham": "I have adopted the workaround but it still doesn't solve the problem completely:\nerrChan := make(chan error, 1)\n  go func () {\n    // conn is a gRPC server stream\n    err := conn.Send(message)\n    errChan <- err\n    close(errChan)\n  }()\nIn the case the client has become unresponsive, the goroutine remains blocked indefinitely on conn.Send. Is there a way to forcibly close the connection from the server side, so that conn.Send will return abnormally and be able to bounce the sender goroutine?. ",
    "ZhiqinYang": "I think bytes.Buffer memory allocation strategy is good for this !. > Hey @ZhiqinYang,\n\nCould you please provide more context for the problem that this PR is attempting to solve?\n\nIn my env, the connections will auto closed, I want to keep connection not closed, so I used the keepalive,\nI have a request is  cost time more than one minutes,  I opened keepalive, the log see  too many pings.  . > @ZhiqinYang have you configured keepalive enforcement on the server accordingly to match the client's pings?\n\nhttps://godoc.org/google.golang.org/grpc/keepalive#EnforcementPolicy\n\nyes! I set like this : \nin client  side :\nvar DefaultKeepaliveParams = grpc.WithKeepaliveParams(keepalive.ClientParameters{ \n          Time:                20 * time.Second,   \n           Timeout:             30 * time.Second,  \n           PermitWithoutStream: true,  \n   })\nin server side :  \nKeepaliveEnforcementPolicy(keepalive.EnforcementPolicy{\n                          MinTime:             10 * time.Second,\n                           PermitWithoutStream: true,\n          })\nIn my case , some request handler cost time more than one minute !. > I'm not able to reproduce this with a simple test case. The test case will send a ping every 10ms, and the server has a MinTime of 5ms. Even with no activity and an active stream sleeping for 1 minute, the server correctly allows the pings.\n\nCan you provide a reproduction case I can run that shows the problem you are seeing?\nNote that pings are only sent every Time + Timeout period. So in your parameters above, that's every 50s. This is actually a bug and I will fix it soon.\nI thought this problem case may be by that each request with a ping req,  when the request no response (no data write to the header), the pingStrikes were not reset when keepalive sends ping req, the pingStrikes will increase!. \n",
    "ghasemloo": "I was thinking this:\n```\nfunc (s Status) Details() apb.Any\nfunc NewWithDetails(c codes.Code, msg string, details apb.Any) Status\n```\nMaybe also \nfunc ErrorWithDetails(c codes.Code, msg string, details *apb.Any) error. I see your point. Maybe we should look at use cases. Let me give a few that I have in mind. \n\nExample 1: resource info\nIn the following I am simplifying things. Say we have a resource Book and a Library API which has a Get method. A caller has written\ngo\nreq := GetBookRequest{Name: \"books/The Go Programming Language\"}\nbook, err := libClient.GetBook(ctx, req)\nIn my server I have to call the API for another system, e.g. let's say that the data for books are stored in a table called \"books\" in a database that the caller has configured and owns. I call the API for database to read from the table.\ngo\nreq := GetTableRequest{Name: \"table/books\"}\ntable, err := dbClient.GetTable(ctx, req)\nWhen I get a NotFound status error it means the table does not exist on the database. I want to let the caller know that the table does not exist. But errors are scoped to the resource of the method. I want to use error detail ResourceInfo to say that it is the table that is missing. How can I do that?\nRight now I would have to write something like this:\ngo\np := status.New(Codes.FAILED_PRECONDITION, \"the table books does not exist on the database\").Proto()\nd := edpb.ResourceInfo{\n  ResourceType: \"DB Table\",\n  ResourceName: \"table/books\",\n  Owner: \"project:xyz\",\n}\ndetail, _ := MarshalAny(d)\n// We can add error handling but it is almost impossible for MarshalAny to fail.\np.Details = append(p.Details, detail)\nreturn nil, status.ErrorProto(p)\nOr I can just go for google.rpc.status proto and google.rpc.codes proto and ignore the gRPC status object except for ErrorProto part:\ngo\nd := edpb.ResourceInfo{\n  ResourceType: \"DB Table\",\n  ResourceName: \"table/books\",\n  Owner: \"project:xyz\",\n}\ndetail, _ := MarshalAny(d)\n// We can add error handling but it is almost impossible for MarshalAny to fail.\np := spb.Status{\n  Code: cpb.FAILED_PRECONDITION, \n  Message: \"the table books does not exist on the database\",\n  Details: []*apb.Any{detail}\n}\nreturn nil, status.ErrorProto(p)\nNow I can write this and it is fine, but it essentially ignores most of gRPC's status package.\nI think a better way might be this:\nThe error_details are quite standard, it is not going to change much. So we can make them first class objects like gRPC status. We have an ErrorDetail interface, a type for each error detail in the error_details.proto (not sure if we need a new for them). We then add a status.AddDetail() function to gRPC status package. We then write something like this:\ngo\ns := status.New(\n  Code: cpb.FAILED_PRECONDITION, \n  Message: \"the table books does not exist on the database\",\n)\ndetail := ed.ResourceInfo{\n  ResourceType: \"DB Table\",\n  ResourceName: \"table/books\",\n  Owner: \"project:xyz\",\n}\ns = status.AddDetail(s, detail)\nreturn nil, s.Error()\n\nExample 2: retry info\nExample 2:\nSay we want to let the caller have more information about when it should retry calling the API. So we use error_details.proto's RetryInfo.\nWith an AddDetail we can have:\ngo\ns := status.New(codes.UNAVAILABLE, \"\")\ns = status.AddDetail(s, ed.RetryInfo{RetryDelay: time.Second})\nreturn s.Error()\nWe would have Details return a list of ErrorDetail interface. One the caller side we can have:\n```go\nreq := GetBookRequest{Name: \"books/The Go Programming Language\"}\nbook, err := libClient.GetBook(ctx, req)\ns := status.FromError(err)\nswitch s.Code() {\n  case codes.OK:\n    return book, nil\ncase codes.UNAVAILABLE:\n    wait := defaultWait\n    for _, detail := range status.FromError(err) {\n      if retryInfo, ok := detail.(ed.RetryInfo); ok {\n        wait = retryInfo.RetryDelay\n      }\n    }\n    time.Sleep(wait)\n    // jump to the location of gRPC call, if this is inside a retry loop we can simply continue\n    continue\ndefault:\n    return nil, err\n}\n```\n\nExample 3: debug info\ngo\ns := status.New(codes.INTERNAL, \"\")\nd := ed.DebugInfo{\n  StackEntries: []string{\n    \"this is a stack 1\",\n    \"this is a stack 2\",\n  },\n}\ns = status.AddDetail(s, d)\nreturn s.Error()\nOne the caller side we can have:\ngo\nreq := GetBookRequest{Name: \"books/The Go Programming Language\"}\nbook, err := libClient.GetBook(ctx, req)\nIf status.FromError(err).Code() != codes.OK {\n  log.Warningf(\"GetBook(_, %+v) failed: %v\", err)\n}\n\nNote 1: If we want to also allow handling of the error in case MarshalAny fails we can have AddDetail return a status and an error.\ngo\ns, err = status.AddDetail(s, detail)\nNote 2: we call have AppendDetail in place of AddDetail if that seems more suitable. If the status object was mutable we could use golang's own append:\ngo\ns := status.New(codes.UNAVAILABLE, \"\")\ns.Details = append(s.Details, ed.RetryInfo{RetryDelay: time.Second})\n\nWhat do you think?. I like your idea of WithDetails method, it seems cleaner than my AddDetails suggestion.\nI am not sure the code sample you have would work though, I think go would complain that we are using Err() on the pair (s, err) returned from WithDetails method.\nMaybe we can have a details.go package that hides the error_details.proto messages inside (like status.go does with status proto) to avoid marshalling/unmarshalling errors. If we created the object we can make sure no error will happen for marshaling/unmarshalling when using them with WithDeatils I think.. I see, not wanting to introducing a new dependency for this is understandable. sgtm. Hopefully will send a pull request next week. Thank you. :). Sorry for not getting this done yet, will send the pull request by next week.. Working on it. When I try to switch over the type of the error detail from Details() I get DynamicAny type in place of the actual type of error so switch as we discussed doesn't work. I am looking to see if there is a way to solve the issue so we can do \ngo\nswitch m := d.(type) {\n    case ed.RetryInfo:\n      // ...\n  }\nps:\nI think I find a way around the issue. In place of appending the dynamic any to details I am appending the message inside it and seems to work. Will send the pull request tomorrow.. Here is the draft: https://github.com/ghasemloo/grpc-go/commit/e63bb4a9ef96af2209e5e34733fbdea7d6afb502\nPlease have a look, if it looks good to you I will add a few more tests to have better test coverage and then send a pull request.. Thanks, will do today.\nA question: regarding returning error on nil input, no other function in the package returns a separate error, I am wondering if it would be more consistent with the rest of the functions if we return a bool in place of an error as in the FromError function.\nps: I cannot do s: New(...).WithDetails(...), because WithDetails returns two values.. One more question: right now we have two Status objects that represent OK: if you create a status using New the proto inside will never be nil, if we create it however using FromProto with nil, we get a Status with nil status proto. Should I be concerned that WithDetails on an OK Status will depend on how that OK is represented? Doesn't feel good that it behaves differently based on internal representation.\nOr should I treat the case of s.s == nil as a special case and act as by creating a status using New(codes.OK, \"\") and then adding details to it?\nShould I file a bug to make the FromStatus actually create a non-nil status proto so we never have to deal with a status that has nil status proto inside it? The assumption that the internal status proto inside Status object is never nil would also simplify the rest of the code. (However it can be a breaking change since someone might be relying on FromProto(p).Proto() == p.). Sounds logical to me that OK shouldn't have error details and go treats it as nil error as you said however is it documented as such on google.rpc.status? If not I think we should also file a bug with the folks responsible for google.rpc.status so that people don't try to depend on the fields of google.rpc.status when code is OK.\nps: I will make the changes and add the tests for errors and push it.. Btw, I think it would be also helpful to have a Is method to check the code of an error so one doesn't need to convert to state and then check the code:\ngo\n// Is checks the error code of a status error.\nfunc Is(err error, code codes.Code) bool {\n    if s, ok := status.FromError(err); ok {\n        return s.Code() == code\n    }\n    return false\n}\nThen we can have simple error handling like:\ngo\nresp, err := client.RPC(ctx, req)\nif status.Is(err, codes.NotFound) {\n    ...\n}\nOr if we want to support multiple codes at once:\ngo\n// Is checks the error code of a status error.\nfunc Is(err error, c ...codes.Code) bool {\n    s, ok := status.FromError(err)\n    if !ok {\n        return false\n    }\n    for _, code := range c {\n        if s.Code() == code {\n            return true\n        }\n    }\n    return false\n}\nI can include that in this pull request or open a separate bug for it.. sgtm. :). Hmm, looking at \nhttps://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto#L39\nit seems they use \"Cancelled\" but I guess changing the const may break things.\nAren't these files generated from googleapis repo?. @dfawley seems reasonable to me. I can send a pull request to fix that.\nps: I feel that if  WithDetails fails it is most likely indicates a bug in code and I would log it as error and try to continue, not just ignore.\n@steeve you may find these useful:\nhttps://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto\nhttps://cloud.google.com/apis/design/errors\nIf a resource is lock and you want the caller to retry I think Aborted would be the typical choice for error code I think.\n@c4milo there are a few examples in the tests in status_test.go, I can add some go examples based on https://github.com/grpc/grpc-go/issues/1233#issuecomment-301223262. PTAL :). Thank you. :). That is fine with me, however wouldn't that be a breaking change?\nps: we have something similar to \"Is\" internally, that's where the idea for it came from.. sgtm then. :)\nI think having an Is is also useful in addition for cases like:\ngo\nif _, err := client.RPC(ctx, req); status.Is(err, codes.NotFound) {\n    ...\n}\nwithout Is I think the if would need to be in two lines in place of one with an additional temp variable just to pass the status object between the two lines (because status.FromError returns two values, that is generally the issue with returning two values in go :).\nSo I think there is value in having Is as well, it leads to cleaner and shorter code.. SGTM. :). That was what I was originally doing but @dfawley said we shouldn't do it because if error is not nil the return value should be nil. See the discussion in https://github.com/grpc/grpc-go/issues/1233 . @pongad sorry, it was here:\nhttps://github.com/ghasemloo/grpc-go/commit/f7d5e1c74f959a61cce005617e48077d2f7c1b17#commitcomment-22976467\nps: It looks like @dfawley will back on Monday.. Yes, that was actually the reason I originally returned a partial result, I can change it back to returning a partial result, but it might be good to wait for @dfawley to weight on this as well and have a final decision. :). I agree we should return all the details that we can process without error. The question is what we do with those that result in error?\nThat is going to be a extremely uncommon thing in practice, so we should make design such that it is easy and clean to use in the >99% of cases where there is no error. That makes me lean against 1.\nBetween 2 and 3, I think 2 would be my personal preference normally, returning []interface{} is not very appealing, it doesn't really tell you what you got. It is true though that in both 2 and 3 we will be returning an interface that needs to be type switched, so it is not that bad.\nPersonally, when returning an error, I ask myself how that error would be used by the user. I generally feel that there are few cases where the users care about multiple errors or handle them. That is even more true in this context where they would be already handling a status error, and handling the errors occurring during getting the status error's details doesn't sound very appealing, my gut says >99.99% of use cases they would just ignore these. So it is additional complexity in the face of use cases that are going to be very uncommon. But this is a general library so there will be users that would be in that <0.01%. And since the common way to use the details will be switching then maybe 3 is indeed not that bad. What we lose compared to 2 is that we cannot assume that the returned result is a proto.Message, that complicates the cases where a user would want to use these details without type switching and directly as proto.Message, but again that is probably not going to be common case, and they can still do what they want with a bit more work. \nSo between these 3 and despite my personal dislike regarding return multiple errors from a function I think 3 is probably the better choice.. sgtm. @pongad @jba if you don't have an objection I will go ahead with:\n\nchange the return value to []interface{}, \none item per detail, \nif no errors occurs during decoding as before, \nif an error occurs during the decoding the error in place of the detail.. Thanks, sure, I will wait for @jba to comment as well. :). @jba, I am not sure, proto.Message is also an interface so you still need to switch or convert it, so practically for the typical case, the way Details will be used would be the same whether we return []interface{} or []proto.Message. \n\nI don't like 4.a, returning two arrays where one is nil iff the other one doesn't as a replacement for []interface{}. Also we will be returning an extra value that will be discarded almost always.\nI think 4.b. can be fine, but it is a bit more complicated than 3, and DetailsErrors can be confusing for users I think. I don't feel the benefit of returning proto.Message in place of interface{} justifies it, but I would be fine with it.. OK, I have updated the pull request. PTAL. :). OK, will update to return nil.\nShould it make so that in the case of s.Code() == codes.OK we also return nil?. Done.. Done.. Done.\nI didn't originally because it needs to be marshaled to any (I am creating the problematic one directly in the status proto and using FromProto to create the status object).. Done. :) PTAL.. ",
    "deafwolf": "@menghanl Thank you very much.\nI remove the goroutine in server.Start, it works fine. \nI just need an error when client disconnected, I don't care the real reason such as EOF/canceled. I don't know the Start RPC is done, so I think the client cancel the RPC and server receive an \"unexpected\" error.. ",
    "gkelly": "Whoops, I was too slow with this.  Menghan's PR covers this and more.. ",
    "arbarlow": "@menghanl thanks for taking a look.\nEssentially the problem is that CONNECT is trying to tunnel http2 (grpc) over a http1 CONNECT proxy which doesn't work. So using the http_proxy env var is sort of an undocumented behaviour?\nWhat I mean by commenting out the line is that when I used the proxy but don't do a CONNECT it works as expected.. OK, thanks for the clarification.\nI've left it to the linkerd guys to close their ticket but it would be nice to have http2_proxy support as well perhaps. Happy to close this and start something like that or kill it if need be.. ",
    "TheSalmonTapes": "Thanks for the update!\nOn Mon, Jun 5, 2017 at 11:04 AM, dfawley notifications@github.com wrote:\n\nWe are planning to implement the connectivity state API next quarter (by\nSeptember).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/1245#issuecomment-306260025, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AXQJs_W4ZQITUHm2ov_dCPtPbfUWEHTpks5sBENDgaJpZM4NdEQ6\n.\n. \n",
    "bbassingthwaite-va": "I've tried explicitly to do: metadata.NewOutgoingContext(ctx, nil) with no success either. Or would you expect this not to work?. So if that is the case: Why does context.Background() work but not metadata.NewOutgoingContext(ctx, nil) ?. So I had tried clearing both incoming and outgoing, but never both. This line is what ended up working for me.  No idea why this works, but it does:\ngolang\nnewCtx := metadata.NewOutgoingContext(\n    metadata.NewIncomingContext(ctx, nil),\n    nil,\n). @menghanl #2669 does not fix the issue for me either, but GRPC_GO_REQUIRE_HANDSHAKE=off does work for me.. We are multiplexing gRPC and HTTP traffic on a single port using cmux. Removing cmux from the equation fixes the problem.\n```go\nimport \"github.com/cockroachdb/cmux\"\nserver := grpc.NewServer()\nlis, err := net.Listen(\"tcp\", \":8081\")\nif err != nil {\n    log.Fatal(err)\n}\nmux := cmux.New(lis)\ngrpcL := mux.Match(cmux.HTTP2HeaderField(\"content-type\", \"application/grpc\"))\nserver.Serve(grpcL)\n```. ",
    "jefferai": "Thanks for the quick turnaround!. ",
    "zhengxiaochuan-3": "981:finish\n982:finish\n983:finish\n984:finish\n985:finish\n986:finish\n987:finish\n988:finish\n989:finish\n990:finish\n991:finish\n992:finish\n993:finish\n994:finish\n995:finish\n996:finish\n997:finish\n998:finish\n999:finish\n1000:finish\n1001:finish\n1002:finish\n1003:finish\n1004:finish\n1005:finish\n1006:^C\n[root@node-217 ~]# ulimit -a\ncore file size          (blocks, -c) 512000\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 127783\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 102400\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 127783\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\n. thanks for your test .\nmy message is very small . \nand I tried to reproduce it by modify grpc-go/examples/helloworld ,but couldn`t . \nI will test my project again. will update this issue when i get result .. Turns out it`s the systemd open file limits . My service is started by systemd .\nhere it is\n. my client action like this :\n300 goroutinues , every goroutinue loop to \"Dail  -> rpc -> Close \" ( short connection\uff0ctrans data is 1KB )\nIs ReadBufferSize and WriteBufferSize opt  effective for my server and client ? \n. maybe it`s golang problem ?\nI change client to long connetion:\n300 goroutinues , every goroutinue : Dail ->  loop  rpc \nqps : 35k/s ,  cpu only cost 1400%\n. thanks a lot. I`m closing this issue.. ",
    "qiyunfeng01": "I ran into the same problem these days, suspect it be introduced by grpc.. ",
    "olomix": "@mehrdada \n\nrun the python server like the following and then run the go client\n\nyou mean go server & python client.\nSure, I've uploaded logs (error, info & debug) from python client to google drive https://drive.google.com/open?id=0B-irbkAqk9i7YmVkSy1BTW91TlU\nUnfortunately those envs do not affect go server. All logs I can see from go server is via x/net/trace package\n2017/05/27 06:11:10.723813  10.635269   /Greeter/SayHello\n06:11:10.723899  .    86    ... RPC: from [::1]:58962 deadline:none\n06:11:10.724006  .   107    ... recv: name:\"you\"\n06:11:10.724162  .   156    ... sent: message:\"i=0\"\n06:11:10.724189  .    27    ... sent: message:\"i=1\"\n06:11:21.358981  .    18    ... (32622 events discarded)\n06:11:21.358999  .    19    ... sent: message:\"i=32624\"\n06:11:21.359043  .    44    ... sent: message:\"i=32625\"\n06:11:21.359065  .    21    ... rpc error: code = Canceled desc = context canceled\n06:11:21.359078  .    14    ... context canceled\n06:11:21.359081  .     3    ... rpc error: code = Canceled desc = context canceled. I have implemented a client based on grpc-java for my go-server. It works fine. You are right. Seems only c-based grpc implementations are affected. . ",
    "mackross": "We're not that concerned with serialization overhead but during development having all services in one binary and stack traces through RPC would be great.. ",
    "ahmadsherif": "My employer (GitLab) signed a CLA.. @mehrdada It was yesterday around 16:30 UTC.\ncc/ @jacobvosmaer. ",
    "jacobvosmaer": "I submitted the CLA on behalf of GitLab Inc. earlier this week but I got rejected because I'm not an executive. We will have someone else in our org resubmit the CLA.. ",
    "lhecker": "@MakMukhi I've got to disagree about it being a \"specific use-case\", since a TCP RST is surely as common as a (graceful) close, but I'm very happy & excited about you guys working on a proper solution for that already. Thanks for the hint. \ud83d\ude42. ",
    "pattyshack": "like google, we use bazel/blaze to manage target building/testing.  Unlike google, we do not compile go proto code on the fly via genfile (most of our BUILD files are auto-generated via code dependencies analysis tools; checking in the go proto code simplifies the toolchain).\nvendor does not play nicely with bazel (we have mentioned this issue to the golang core team previously).  how does google solve this general issue internally?. btw, pinning via vendor would not solve the issue either since that still requires all thirdparty packages to upgrade to the latest version at once (this is pretty impractical since we rely on several hundred thirdparty packages ...). I guess I didn't answer the real question.  Most stable thirdparty packages does a pretty good job of ensuring backward compatibility.  If I grab a new snapshot of a non-proto/grpc thirdparty package, things will generally compile.\ngrpc on the other hand intentionally breaks backward compatibility.  By defining only SupportPackageIsVersion(N), anything that was generated with SupportPackageIsVersion(N-1) will break unless it's upgraded at the same time.  (It's less of an issue for the proto pkg since the versions are more stable)\ngoogle3 component have to deal with this exact same issue (not sure if the project is still alive since Mike Borrows is now working on brains).  When advancing component version, the api is backward compatibility until the older api usage is fully replaced with the new api.. Hmm, sounds like grpc usage within google is small enough that you can still rewrite everything in a single cl.  Imagine grpc have replaced all of stubby3 (or pretend this project was bigtable or something), and your team needs to make api breaking changes.  It would be impossible to rewrite everything at once, even with the help of rosie.  All I'm saying is that your project would need to solve this issue anyways for google internal usage, so why not help us out. =P\nWe build everything at head for the same reason everything at google is built at head: it ensures that every project within to ecosystem has the latest/greatest fixes.  It's significantly harder to ensure every service have all the latest fixes if each service has its own vendor set.\nSpecific to bazel, yes, you can compile vendor-ed packages, but that trashes your build cache and significantly increases compilation time.. Thanks for looking into this!. o, in case you need to roll a new version in the future, one way to provide backward compatibility is to add a Version field to ServiceDesc and populate that field as part of codegen (if the field is not populated, the lib can assume it's v4 or whatever).  the grpc lib can check the version field and behave differently depending on the version value.  just food for thought.. ",
    "ilonajulczuk": "Hi, could someone help me debugging the tests failure?\nTravis doesn't provide any helpful message:\n Also it only fails for the go 1.6.\nWhen I test the code myself with \"make\" the tests pass and the command returns 0 status code.. ",
    "CMajeri": "That was a long week! I'm sorry.\nI had the time to review and test out the grpc load-balancing, and it works well enough for our needs, and after exploring the source, it's simple enough to work with. Thank you for directing me to it!\nWe also need to implement breaking and throttling patterns, but those are easy enough to just wrap around. Thanks again.. ",
    "vkedia": "That was quick. Thanks!. ",
    "Zeymo": "Instread of using grpclb,  I communicate with discovery component (write in java) agent  for certain balancer policy . I will try hook the resolver and watcher for my requirements .Thx for your time. same to me when client with timeout\nif we don't use tapHandle, to quick fix this is remove s.ctx right ? any side effect\uff1f \n@dfawley. @menghanl At beginning we use nghttp2 as client without timeout event schedule, when server s.ctx DeadlineExceeded/ Canceled  as dfawley say not always write the status and hang.  wait() with s.ctx don't close stream unless nghttp2 send RST_FRAME since s.ctx error\nnow we add timeout even above nghttp2 , also want grpc always write the status to client (maintain same logic/behavior to trailer for polyglot client) and close stream quickly. ping @dfawley  \n\nTo fix this, we will remove the s.ctx being passed to the wait() for acquiring the transport's writable channel. This will ensure that we always write the status even when the context is cancelled.\n\nwhat about this change ? any time schedule?. @menghanl Thx to reply. I already do what you say above (notify lbWatcher new addrs list without certain addr) but it will not graceful shut down. if I wrong  ,correct me\nbecause in tearDown(err error)[https://github.com/grpc/grpc-go/blob/master/clientconn.go#L1145] ac.transport.GracefulClose()[https://github.com/grpc/grpc-go/blob/master/transport/http2_client.go#L650] will success only when receive goAway  before fix #1393 \n```\nswitch t.state {\n    case unreachable:\n        t.mu.Unlock()\n        t.Close()\n        return nil\n    case closing:\n        t.mu.Unlock()\n        return nil\n    }\n    select {\n    case <-t.goAway:\n        n := t.prevGoAwayID\n        if n == 0 && t.nextID > 1 {\n            n = t.nextID - 2\n        }\n        m := t.goAwayID + 2\n        if m == 2 {\n            m = 1\n        }\n        for i := m; i <= n; i += 2 {\n            if s, ok := t.activeStreams[i]; ok {\n                close(s.goAway)\n            }\n        }\n    default:\n    }\n    if t.state == draining {\n    t.mu.Unlock()\n    return nil\n}\nt.state = draining\nactive := len(t.activeStreams)\nt.mu.Unlock()\nif active == 0 {\n    return t.Close()\n}\n\n``\n this server is serving so len(t.activeStreams)` != 0\nand don't close at all cos' err == errConnDrain in\nhttps://github.com/grpc/grpc-go/blob/master/clientconn.go#L1176\n. sry for my carelessness  : P\nif t.state == draining && len(t.activeStreams) == 0 {\n        defer t.Close()\n}. sry for my careless : (. sync.Pool is not enough as follow issue say, https://github.com/golang/go/issues/23199 . Unbound capacity buffer will cause worse GC or memory overhead . Maybe chunked capacity pool/buffer would be better. What about pooled transport.Options and transport.CallHdr ?. ",
    "stub42": "If Send gets EOF or other unrecoverable stream error, should it automatically call Recv() and return that error instead?. Thanks for the clarification. I've tracked down the underlying problem, which was at my end. An atavistic url.Parse was found, which worked ok for names but failed with IP addresses, and the error appeared like it was coming from grpc.Dial.. This seemed to be triggered by the client terminating with just:\ngo\nreturn stream.CloseSend()\nChanging it to the following seems to fix the issue, with the client correctly waiting for the server to report success/fail for the overall request and the context on the server side not being cancelled until this handshaking is over:\ngo\n// Finalize. Close the client stream, and wait for the result of\n// the overall streaming gRPC call.\nif err := stream.CloseSend(); err != nil {\n    return fmt.Errorf(\"failed to close client stream: %v\", err)\n}\nswitch m, err := stream.Recv(); err {\ndefault:\n    return fmt.Errorf(\"remote request failed: %v\", err)\ncase nil:\n    return fmt.Errorf(\"unexpected response: %+v\", m)\ncase io.EOF:\n}\nreturn nil\nIt seems odd to call stream.Recv() at the end, when I know there is no message waiting, and only check the error. I think this was the source of my confusion.. ",
    "jinleileiking": "Yes. I found middleware to solve this. Thanks for the patient reply.. sorry for the late response. At the server side, I want some rpc use some interceptor, some not. Now all the rpcs must run all the interceptors. @dfawley Thanks a lot \ud83d\udc4d . ",
    "zxy198717": "Hi @jinleileiking , which middleware you found? could you share it?. ",
    "overvenus": "Hi,\nHow about registering two different services on the same server?\nJust like https://github.com/coreos/etcd/blob/1df8b90d675d2b731305337f34dac211ff6fb5e7/etcdserver/api/v3rpc/grpc.go#L36. ",
    "Bankq": "thanks @menghanl !. ",
    "britt": "I would add a few questions related to how grpc.ClientConn manages HTTP2 connections for concurrent RPC calls.\n\nWhen does a connection form an HTTP2 connection to the server? \nDoes is form a connection for each concurrent RPC? (seems like yes)\nDoes it pool connections? (seems like no). \n",
    "cloudzhou": "yes, need more documentation on concurrency.\ntake a example: \ncli, cerr := clientv3.NewFromURL(\"http://localhost:2379\")\nr := &etcdnaming.GRPCResolver{Client: cli}\nb := grpc.RoundRobin(r)\nconn, gerr := grpc.Dial(\"my-service\", grpc.WithBalancer(b))\n1 does grpc.RoundRobin balancer concurrency safe ?\n2 what if we implement Resolver, does it required concurrency safe ?\n3 does conn  concurrency safe ?\nI think the easy way is: \nas default, all  method, struct is unsafe for concurrency.  mark it down if is safe.. ",
    "ostlerc": "can someone answer these questions? I came here looking for the answer to 'Can concurrent goroutines share use of a single client object?' but so far nobody knows.... ",
    "kahuang": "I will be reproducing with GRPC_VERBOSITY=INFO to get  more descriptive logging. DEBUG verbosity is a bit too high.... ...\nI0705 16:10:40.769051854   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f72d58c1fb8\nI0705 16:10:40.769061057   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:10:40.769065508   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:10:40.769069033   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:10:40.769256279   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:40.905785811   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:40.905829481   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296241, tv_nsec: 105822851, clock_type: 1 }, reserved=(nil))\nI0705 16:10:41.106557970   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:41.106598422   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296241, tv_nsec: 306593777, clock_type: 1 }, reserved=(nil))\nI0705 16:10:41.306676965   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:41.306716934   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296241, tv_nsec: 506712188, clock_type: 1 }, reserved=(nil))\nI0705 16:10:41.342002398   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6677520, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:10:41.342051096   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6677520 OK\nI0705 16:10:41.342079799   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:41.342093219   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:41.342224564   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:41.375467093   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296241, tv_nsec: 575461898, clock_type: 1 }, reserved=(nil))\nI0705 16:10:41.576761261   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:41.786440011   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296241, tv_nsec: 986430142, clock_type: 1 }, reserved=(nil))\nI0705 16:10:41.987720567   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:41.993958684   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296242, tv_nsec: 193954556, clock_type: 1 }, reserved=(nil))\nI0705 16:10:42.195206101   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:42.229069158   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296242, tv_nsec: 429063719, clock_type: 1 }, reserved=(nil))\nI0705 16:10:42.430364197   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:42.434871791   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296242, tv_nsec: 634855029, clock_type: 1 }, reserved=(nil))\nI0705 16:10:42.636137131   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:42.637518427   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296242, tv_nsec: 837515324, clock_type: 1 }, reserved=(nil))\nI0705 16:10:42.837979649   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:42.838122793   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296243, tv_nsec: 38118880, clock_type: 1 }, reserved=(nil))\nI0705 16:10:43.038350239   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:43.069067559   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296243, tv_nsec: 269062844, clock_type: 1 }, reserved=(nil))\nI0705 16:10:43.165275471   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:43.165292217   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:43.165297077   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:43.165303316   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:43.165313615   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0xba941b0, nops=1, tag=0x7f72d6b4c0c0, reserved=(nil))\nI0705 16:10:43.165318673   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f7302b68070\nI0705 16:10:43.165325432   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:10:43.165354083   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:10:43.165358174   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:10:43.165466395   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:43.269922946   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:43.269974850   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296243, tv_nsec: 469963780, clock_type: 1 }, reserved=(nil))\nI0705 16:10:43.471033682   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:43.471074151   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296243, tv_nsec: 671069176, clock_type: 1 }, reserved=(nil))\nI0705 16:10:43.672482637   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:43.672523609   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296243, tv_nsec: 872518531, clock_type: 1 }, reserved=(nil))\nI0705 16:10:43.874277394   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:43.874320298   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296244, tv_nsec: 74315235, clock_type: 1 }, reserved=(nil))\nI0705 16:10:43.903479439   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6b4c0c0, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:10:43.903525431   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6b4c0c0 OK\nI0705 16:10:43.903551772   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:43.903564795   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:43.903958253   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:43.927419385   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296244, tv_nsec: 127414166, clock_type: 1 }, reserved=(nil))\nI0705 16:10:44.127780207   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:44.205138830   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296244, tv_nsec: 405130408, clock_type: 1 }, reserved=(nil))\nI0705 16:10:44.406416090   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:44.437067981   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296244, tv_nsec: 637062754, clock_type: 1 }, reserved=(nil))\nI0705 16:10:44.638377153   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:44.905078906   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296245, tv_nsec: 105070586, clock_type: 1 }, reserved=(nil))\nI0705 16:10:45.105388810   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:45.121694932   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296245, tv_nsec: 321691070, clock_type: 1 }, reserved=(nil))\nI0705 16:10:45.321967704   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:45.321997463   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296245, tv_nsec: 521982290, clock_type: 1 }, reserved=(nil))\nI0705 16:10:45.523254403   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:45.523313457   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296245, tv_nsec: 723310730, clock_type: 1 }, reserved=(nil))\nI0705 16:10:45.724597838   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:45.735711047   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296245, tv_nsec: 935707221, clock_type: 1 }, reserved=(nil))\nI0705 16:10:45.769783467   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:45.769797803   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:45.769800431   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:45.769804724   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:45.769831676   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0x9a2f340, nops=1, tag=0x7f72d6b4cd70, reserved=(nil))\nI0705 16:10:45.769837173   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f72e1afafb8\nI0705 16:10:45.769844562   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:10:45.769849428   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:10:45.769853855   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:10:45.771181266   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:45.939573843   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:45.939615694   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296246, tv_nsec: 139610723, clock_type: 1 }, reserved=(nil))\nI0705 16:10:46.140002146   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:46.140032441   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296246, tv_nsec: 340028967, clock_type: 1 }, reserved=(nil))\nI0705 16:10:46.341298652   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:46.341338412   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296246, tv_nsec: 541333758, clock_type: 1 }, reserved=(nil))\nI0705 16:10:46.373389791   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6b4cd70, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:10:46.373433171   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6b4cd70 OK\nI0705 16:10:46.373462941   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:46.373480634   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:46.373809883   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:46.399140185   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296246, tv_nsec: 599134512, clock_type: 1 }, reserved=(nil))\nI0705 16:10:46.600433557   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:46.753897871   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296246, tv_nsec: 953892121, clock_type: 1 }, reserved=(nil))\nI0705 16:10:46.954881925   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:47.217075491   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296247, tv_nsec: 417067658, clock_type: 1 }, reserved=(nil))\nI0705 16:10:47.418372188   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:47.473067862   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296247, tv_nsec: 673062902, clock_type: 1 }, reserved=(nil))\nI0705 16:10:47.674372027   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:47.674401693   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296247, tv_nsec: 874399944, clock_type: 1 }, reserved=(nil))\nI0705 16:10:47.875667383   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:47.875712647   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296248, tv_nsec: 75691992, clock_type: 1 }, reserved=(nil))\nI0705 16:10:48.076220340   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:48.081065981   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296248, tv_nsec: 281062739, clock_type: 1 }, reserved=(nil))\nI0705 16:10:48.215795147   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:48.215828136   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:48.215830926   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:48.215834935   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:48.215862243   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0xdf28140, nops=1, tag=0x7f72d6b4c0c0, reserved=(nil))\nI0705 16:10:48.215880394   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f7300dcef88\nI0705 16:10:48.215885886   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:10:48.215907939   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:10:48.215911543   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:10:48.216021111   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:48.281699772   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:48.281748765   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296248, tv_nsec: 481740860, clock_type: 1 }, reserved=(nil))\nI0705 16:10:48.484126631   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:48.484168905   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296248, tv_nsec: 684164030, clock_type: 1 }, reserved=(nil))\nI0705 16:10:48.685387849   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:48.685427224   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296248, tv_nsec: 885421709, clock_type: 1 }, reserved=(nil))\nI0705 16:10:48.888243739   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:48.888285371   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296249, tv_nsec: 88280569, clock_type: 1 }, reserved=(nil))\nI0705 16:10:48.998504877   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6b4c0c0, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:10:48.998569674   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6b4c0c0 OK\nI0705 16:10:48.998608647   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:48.998620106   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:48.998961252   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:49.024442268   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296249, tv_nsec: 224435532, clock_type: 1 }, reserved=(nil))\nI0705 16:10:49.225753017   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:49.269083487   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296249, tv_nsec: 469077403, clock_type: 1 }, reserved=(nil))\nI0705 16:10:49.470373733   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:49.598746043   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296249, tv_nsec: 798735705, clock_type: 1 }, reserved=(nil))\nI0705 16:10:49.800185779   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:49.841071318   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296250, tv_nsec: 41065188, clock_type: 1 }, reserved=(nil))\nI0705 16:10:50.041470615   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:50.065069428   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296250, tv_nsec: 265064813, clock_type: 1 }, reserved=(nil))\nI0705 16:10:50.265335716   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:50.269062178   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296250, tv_nsec: 469059902, clock_type: 1 }, reserved=(nil))\nI0705 16:10:50.470288339   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:50.477062452   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296250, tv_nsec: 677059841, clock_type: 1 }, reserved=(nil))\nI0705 16:10:50.678291005   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:50.729068366   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296250, tv_nsec: 929062777, clock_type: 1 }, reserved=(nil))\nI0705 16:10:50.855241160   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:50.855257326   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:50.855260012   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:50.855264359   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:50.855291670   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0x90e8280, nops=1, tag=0x7f72d6fa1050, reserved=(nil))\nI0705 16:10:50.855297753   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f72d58c1c70\nI0705 16:10:50.855304930   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:10:50.855309588   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:10:50.855331677   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:10:50.855448411   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:50.930079518   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:50.930116259   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296251, tv_nsec: 130111796, clock_type: 1 }, reserved=(nil))\nI0705 16:10:51.130533531   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:51.130570602   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296251, tv_nsec: 330566560, clock_type: 1 }, reserved=(nil))\nI0705 16:10:51.330667755   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:51.330709313   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296251, tv_nsec: 530704570, clock_type: 1 }, reserved=(nil))\nI0705 16:10:51.476589399   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6fa1050, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:10:51.476633741   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6fa1050 OK\nI0705 16:10:51.476662380   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:51.476672988   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:51.476891153   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:51.511133309   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296251, tv_nsec: 711127312, clock_type: 1 }, reserved=(nil))\nI0705 16:10:51.712464144   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:52.040601558   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296252, tv_nsec: 240595319, clock_type: 1 }, reserved=(nil))\nI0705 16:10:52.242528274   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:52.319220513   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296252, tv_nsec: 519215741, clock_type: 1 }, reserved=(nil))\nI0705 16:10:52.520511893   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:52.525063052   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296252, tv_nsec: 725060395, clock_type: 1 }, reserved=(nil))\nI0705 16:10:52.726327859   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:52.729062571   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296252, tv_nsec: 929060009, clock_type: 1 }, reserved=(nil))\nI0705 16:10:52.930325323   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:52.969067867   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296253, tv_nsec: 169063003, clock_type: 1 }, reserved=(nil))\nI0705 16:10:53.169567958   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:53.173112067   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296253, tv_nsec: 373108482, clock_type: 1 }, reserved=(nil))\nI0705 16:10:53.373380416   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:53.373429685   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296253, tv_nsec: 573427240, clock_type: 1 }, reserved=(nil))\nI0705 16:10:53.479884835   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:53.479900346   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:53.479904771   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:53.479910440   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:53.479939267   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0x10dd2ea0, nops=1, tag=0x7f72d6b4c050, reserved=(nil))\nI0705 16:10:53.479944068   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f72e0a3fa30\nI0705 16:10:53.479969294   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:10:53.479973196   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:10:53.479976018   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:10:53.480096763   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:53.575567833   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:53.575612168   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296253, tv_nsec: 775607162, clock_type: 1 }, reserved=(nil))\nI0705 16:10:53.776906761   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:53.776952073   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296253, tv_nsec: 976945912, clock_type: 1 }, reserved=(nil))\nI0705 16:10:53.979419363   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:53.979460851   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296254, tv_nsec: 179455743, clock_type: 1 }, reserved=(nil))\nI0705 16:10:54.181459488   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:54.181500293   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296254, tv_nsec: 381494764, clock_type: 1 }, reserved=(nil))\nI0705 16:10:54.289516330   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6b4c050, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:10:54.289566519   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6b4c050 OK\nI0705 16:10:54.289598960   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:54.289610928   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:54.289913632   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:54.319550430   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296254, tv_nsec: 519544476, clock_type: 1 }, reserved=(nil))\nI0705 16:10:54.520826921   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:54.665299768   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296254, tv_nsec: 865258804, clock_type: 1 }, reserved=(nil))\nI0705 16:10:54.866281586   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:54.885072246   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296255, tv_nsec: 85067259, clock_type: 1 }, reserved=(nil))\nI0705 16:10:55.085708509   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:55.086143922   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296255, tv_nsec: 286139868, clock_type: 1 }, reserved=(nil))\nI0705 16:10:55.287404074   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:55.298287962   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296255, tv_nsec: 498283199, clock_type: 1 }, reserved=(nil))\nI0705 16:10:55.499563512   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:55.505064333   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296255, tv_nsec: 705061131, clock_type: 1 }, reserved=(nil))\nI0705 16:10:55.706329335   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:55.753065449   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296255, tv_nsec: 953060790, clock_type: 1 }, reserved=(nil))\nI0705 16:10:55.954335316   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:55.962387551   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296256, tv_nsec: 162383960, clock_type: 1 }, reserved=(nil))\nI0705 16:10:56.162945168   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:56.227737236   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296256, tv_nsec: 427728240, clock_type: 1 }, reserved=(nil))\nI0705 16:10:56.429017739   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:56.429104993   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296256, tv_nsec: 629101754, clock_type: 1 }, reserved=(nil))\nI0705 16:10:56.630382200   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:56.630461331   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296256, tv_nsec: 830458760, clock_type: 1 }, reserved=(nil))\nI0705 16:10:56.640076716   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:56.640087480   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:56.640090136   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:56.640094008   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:56.640120849   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0xe0f0f20, nops=1, tag=0x7f72d6170210, reserved=(nil))\nI0705 16:10:56.640125758   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f730196a328\nI0705 16:10:56.640131564   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:10:56.640135719   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:10:56.640139134   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:10:56.640315975   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:56.830823929   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:56.830863725   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296257, tv_nsec: 30859077, clock_type: 1 }, reserved=(nil))\nI0705 16:10:57.030874188   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:57.030897039   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296257, tv_nsec: 230894391, clock_type: 1 }, reserved=(nil))\nI0705 16:10:57.231794824   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:57.231836376   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296257, tv_nsec: 431831788, clock_type: 1 }, reserved=(nil))\nI0705 16:10:57.302366636   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6170210, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:10:57.302410446   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6170210 OK\nI0705 16:10:57.302440017   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:57.302452662   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:57.302713333   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:57.327697052   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296257, tv_nsec: 527691216, clock_type: 1 }, reserved=(nil))\nI0705 16:10:57.528989637   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:57.537064033   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296257, tv_nsec: 737060448, clock_type: 1 }, reserved=(nil))\nI0705 16:10:57.737833274   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:57.931979093   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296258, tv_nsec: 131970695, clock_type: 1 }, reserved=(nil))\nI0705 16:10:58.132401338   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:58.222804386   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296258, tv_nsec: 422799398, clock_type: 1 }, reserved=(nil))\nI0705 16:10:58.423059831   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:58.444304202   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296258, tv_nsec: 644299365, clock_type: 1 }, reserved=(nil))\nI0705 16:10:58.645560690   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:58.677301969   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296258, tv_nsec: 877297207, clock_type: 1 }, reserved=(nil))\nI0705 16:10:58.878557105   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:58.917068318   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296259, tv_nsec: 117063092, clock_type: 1 }, reserved=(nil))\nI0705 16:10:59.117491028   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:59.145065752   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296259, tv_nsec: 345060708, clock_type: 1 }, reserved=(nil))\nI0705 16:10:59.345341915   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:59.373064481   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296259, tv_nsec: 573060398, clock_type: 1 }, reserved=(nil))\nI0705 16:10:59.524435556   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:59.524446901   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:59.524449709   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:59.524453507   16015 init.c:228]                 grpc_init(void)\nI0705 16:10:59.524460566   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0xfd60cf0, nops=1, tag=0x7f72d6d18830, reserved=(nil))\nI0705 16:10:59.524483077   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f72d60dfee0\nI0705 16:10:59.524489351   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:10:59.524493639   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:10:59.524497101   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:10:59.524675006   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:10:59.574110145   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:59.574165762   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296259, tv_nsec: 774158347, clock_type: 1 }, reserved=(nil))\nI0705 16:10:59.776798030   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:59.776840017   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296259, tv_nsec: 976835185, clock_type: 1 }, reserved=(nil))\nI0705 16:10:59.977134365   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:10:59.977174819   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296260, tv_nsec: 177169885, clock_type: 1 }, reserved=(nil))\nI0705 16:11:00.042538105   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6d18830, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:11:00.042568446   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6d18830 OK\nI0705 16:11:00.042586276   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:00.042591974   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:00.042766116   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:00.061383067   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296260, tv_nsec: 261378101, clock_type: 1 }, reserved=(nil))\nI0705 16:11:00.261712654   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:00.519334875   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296260, tv_nsec: 719315192, clock_type: 1 }, reserved=(nil))\nI0705 16:11:00.720299466   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:00.753067725   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296260, tv_nsec: 953061674, clock_type: 1 }, reserved=(nil))\nI0705 16:11:00.954339272   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:01.009067856   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296261, tv_nsec: 209062302, clock_type: 1 }, reserved=(nil))\nI0705 16:11:01.209495621   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:01.253067195   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296261, tv_nsec: 453061198, clock_type: 1 }, reserved=(nil))\nI0705 16:11:01.453361211   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:01.502033478   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296261, tv_nsec: 702028842, clock_type: 1 }, reserved=(nil))\nI0705 16:11:01.680486042   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:01.680502512   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:01.680505164   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:01.680509226   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:01.680536284   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0xd39c9c0, nops=1, tag=0x7f73001ec2f0, reserved=(nil))\nI0705 16:11:01.680540189   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f72d5d33fa0\nI0705 16:11:01.680546482   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:11:01.680550362   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:11:01.680553171   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:11:01.680730041   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:01.704202235   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:01.704226818   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296261, tv_nsec: 904224091, clock_type: 1 }, reserved=(nil))\nI0705 16:11:01.904650249   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:01.904668688   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296262, tv_nsec: 104666841, clock_type: 1 }, reserved=(nil))\nI0705 16:11:02.024214667   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f73001ec2f0, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:11:02.024236840   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f73001ec2f0 OK\nI0705 16:11:02.024251475   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:02.024256131   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:02.024344668   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:02.043669493   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296262, tv_nsec: 243664511, clock_type: 1 }, reserved=(nil))\nI0705 16:11:02.244085993   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:02.369678037   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296262, tv_nsec: 569665528, clock_type: 1 }, reserved=(nil))\nI0705 16:11:02.571091091   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:02.571109070   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296262, tv_nsec: 771107117, clock_type: 1 }, reserved=(nil))\nI0705 16:11:02.772369267   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:03.048948518   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296263, tv_nsec: 248937183, clock_type: 1 }, reserved=(nil))\nI0705 16:11:03.249155167   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:03.249274631   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296263, tv_nsec: 449272031, clock_type: 1 }, reserved=(nil))\nI0705 16:11:03.449539144   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:03.449556588   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296263, tv_nsec: 649554798, clock_type: 1 }, reserved=(nil))\nI0705 16:11:03.650814738   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:03.669175503   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296263, tv_nsec: 869171163, clock_type: 1 }, reserved=(nil))\nI0705 16:11:03.863148916   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:03.863164821   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:03.863167615   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:03.863171848   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:03.863199225   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0x8bfdf60, nops=1, tag=0x7f72d61708a0, reserved=(nil))\nI0705 16:11:03.863219605   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f73006a9f88\nI0705 16:11:03.863227290   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:11:03.863250739   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:11:03.863253852   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:11:03.865250798   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:03.870508222   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:03.870533687   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296264, tv_nsec: 70532127, clock_type: 1 }, reserved=(nil))\nI0705 16:11:04.071071501   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:04.071089444   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296264, tv_nsec: 271087512, clock_type: 1 }, reserved=(nil))\nI0705 16:11:04.092193264   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d61708a0, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:11:04.092208794   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d61708a0 OK\nI0705 16:11:04.092238782   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:04.092242320   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:04.092311274   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:04.105689224   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296264, tv_nsec: 305665993, clock_type: 1 }, reserved=(nil))\nI0705 16:11:04.306748552   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:04.318336084   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296264, tv_nsec: 518331001, clock_type: 1 }, reserved=(nil))\nI0705 16:11:04.519611587   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:04.781071374   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296264, tv_nsec: 981063809, clock_type: 1 }, reserved=(nil))\nI0705 16:11:04.982348338   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:04.983098369   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296265, tv_nsec: 183094764, clock_type: 1 }, reserved=(nil))\nI0705 16:11:05.183527692   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:05.205069903   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296265, tv_nsec: 405064800, clock_type: 1 }, reserved=(nil))\nI0705 16:11:05.405376934   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:05.489066681   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296265, tv_nsec: 689061072, clock_type: 1 }, reserved=(nil))\nI0705 16:11:05.681509522   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:05.681523160   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:05.681525899   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:05.681529589   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:05.681556404   16015 call.c:1759]                grpc_call_start_batch(call=0x113041f8, ops=0x8c20ce0, nops=1, tag=0x7f72d6ba3440, reserved=(nil))\nI0705 16:11:05.681561440   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f72e2d48fe8\nI0705 16:11:05.681566870   16015 client_channel.c:1243]      OP[client-channel:0x11304858]: [COVERED] RECV_MESSAGE\nI0705 16:11:05.681570452   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc965b38]: [COVERED] RECV_MESSAGE\nI0705 16:11:05.681573439   16015 connected_channel.c:70]     OP[connected:0x7f72cc965b68]: [COVERED] RECV_MESSAGE\nI0705 16:11:05.683519890   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.689330513   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: QUEUE_TIMEOUT\nI0705 16:11:05.689348133   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296265, tv_nsec: 889346368, clock_type: 1 }, reserved=(nil))\nI0705 16:11:05.808593754   16978 parsing.c:580]              HTTP:1:TRL:CLI: grpc-status: 30 '0'\nI0705 16:11:05.808608068   16978 parsing.c:580]              HTTP:1:TRL:CLI: grpc-message: \nI0705 16:11:05.808643409   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d6ba3440, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304d98, storage=0x11304da0)\nI0705 16:11:05.808695801   16978 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0xd8629d0, tag=0x7f72d38e1050, error=\"No Error\", done=0x7f733a74df80, done_arg=0x11304cc8, storage=0x11304cd0)\nI0705 16:11:05.808719614   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d6ba3440 OK\nI0705 16:11:05.808731561   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.808751113   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.808832754   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.817199178   16978 completion_queue.c:359]     grpc_completion_queue_next(cc=0xd8629d0, deadline=gpr_timespec { tv_sec: 1499296266, tv_nsec: 17194023, clock_type: 1 }, reserved=(nil))\nI0705 16:11:05.817215853   16978 completion_queue.c:444]     RETURN_EVENT[0xd8629d0]: OP_COMPLETE: tag:0x7f72d38e1050 OK\nI0705 16:11:05.834900837   16978 metadata_array.c:42]        grpc_metadata_array_init(array=0x7f72d427e3d0)\nI0705 16:11:05.834919407   16978 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f73058b97c8)\nI0705 16:11:05.834932974   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.834938546   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.835251683   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.835324708   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.835328688   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.835331529   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.835334188   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.835353797   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:05.835356540   16978 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:06.886278823   16015 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f73058b97c8)\nI0705 16:11:06.886295769   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:06.886300454   16015 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f73058b9a38)\nI0705 16:11:06.886303017   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:06.886308583   16015 call.c:536]                 grpc_call_destroy(c=0x113041f8)\nI0705 16:11:06.886332318   16015 init.c:233]                 grpc_shutdown(void)\n05/02/2017 - 05/09/2017: 337.0s\nI0705 16:11:06.941009347   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941044436   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941049074   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941053705   16015 metadata_array.c:42]        grpc_metadata_array_init(array=0x7f73058b99d8)\nI0705 16:11:06.941070332   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941078589   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941087541   16015 call.c:1759]                grpc_call_start_batch(call=0xcaff2e8, ops=0x8bfdf60, nops=1, tag=0x7f7305212ad0, reserved=(nil))\nI0705 16:11:06.941092811   16015 call.c:1402]                ops[0]: RECV_INITIAL_METADATA ptr=0x7f73058b99d8\nI0705 16:11:06.941099264   16015 client_channel.c:1243]      OP[client-channel:0xcaff948]: [COVERED] RECV_INITIAL_METADATA\nI0705 16:11:06.941108348   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:06.941113781   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941117497   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941122740   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941128324   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941132540   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941136027   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941138805   16015 metadata_array.c:42]        grpc_metadata_array_init(array=0x7f73058b9828)\nI0705 16:11:06.941142465   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941147407   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941153608   16015 call.c:1759]                grpc_call_start_batch(call=0xcaff2e8, ops=0xe7ffd70, nops=4, tag=0x7f7305212bb0, reserved=(nil))\nI0705 16:11:06.941158422   16015 call.c:1402]                ops[0]: SEND_INITIAL_METADATA(nil)\nI0705 16:11:06.941162602   16015 call.c:1402]                ops[1]: SEND_MESSAGE ptr=0x8bf6530\nI0705 16:11:06.941166276   16015 call.c:1402]                ops[2]: SEND_CLOSE_FROM_CLIENT\nI0705 16:11:06.941171514   16015 call.c:1402]                ops[3]: RECV_STATUS_ON_CLIENT metadata=0x7f73058b9828 status=0x7f730571c9c0 details=0x7f730571c9c8\nI0705 16:11:06.941187452   16015 client_channel.c:1243]      OP[client-channel:0xcaff948]: [COVERED] SEND_INITIAL_METADATA{key=3a 70 61 74 68 ':path' value=2f 6d 6f 6e 67 6f 5f 70 72 6f 78 79 2e 4d 6f 6e 67 6f 50 72 6f 78 79 2f 46 69 6e 64 49 74 65 72 '/mongo_proxy.MongoProxy/FindIter', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 31 30 2e 31 33 38 2e 31 30 34 3a 35 30 30 35 31 '10.10.138.104:50051', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip' deadline=6155543.747898568} SEND_MESSAGE:flags=0x00000000:len=180 SEND_TRAILING_METADATA{} RECV_TRAILING_METADATA\nI0705 16:11:06.941212574   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:06.941362644   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296267, tv_nsec: 141360510, clock_type: 1 }, reserved=(nil))\nI0705 16:11:06.941466462   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941478962   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941482565   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941486825   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:06.941494059   16015 call.c:1759]                grpc_call_start_batch(call=0xcaff2e8, ops=0x925f5e0, nops=1, tag=0x7f73052128a0, reserved=(nil))\nI0705 16:11:06.941498562   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f72d69bd640\nI0705 16:11:06.941503474   16015 client_channel.c:1243]      OP[client-channel:0xcaff948]: [COVERED] RECV_MESSAGE\nI0705 16:11:06.941511305   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:06.946857177   17021 http_client_filter.c:427]   OP[http-client:0x7f72cc96b2e8]: [COVERED] RECV_INITIAL_METADATA\nI0705 16:11:06.946869413   17021 connected_channel.c:70]     OP[connected:0x7f72cc96b318]: [COVERED] RECV_INITIAL_METADATA\nI0705 16:11:06.946885142   17021 http_client_filter.c:427]   OP[http-client:0x7f72cc96b2e8]: [COVERED] SEND_INITIAL_METADATA{key=3a 70 61 74 68 ':path' value=2f 6d 6f 6e 67 6f 5f 70 72 6f 78 79 2e 4d 6f 6e 67 6f 50 72 6f 78 79 2f 46 69 6e 64 49 74 65 72 '/mongo_proxy.MongoProxy/FindIter', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 31 30 2e 31 33 38 2e 31 30 34 3a 35 30 30 35 31 '10.10.138.104:50051', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip' deadline=6155543.747898568} SEND_MESSAGE:flags=0x00000000:len=180 SEND_TRAILING_METADATA{} RECV_TRAILING_METADATA\nI0705 16:11:06.946908093   17021 connected_channel.c:70]     OP[connected:0x7f72cc96b318]: [COVERED] SEND_INITIAL_METADATA{key=3a 73 63 68 65 6d 65 ':scheme' value=68 74 74 70 'http', key=3a 6d 65 74 68 6f 64 ':method' value=50 4f 53 54 'POST', key=3a 70 61 74 68 ':path' value=2f 6d 6f 6e 67 6f 5f 70 72 6f 78 79 2e 4d 6f 6e 67 6f 50 72 6f 78 79 2f 46 69 6e 64 49 74 65 72 '/mongo_proxy.MongoProxy/FindIter', key=3a 61 75 74 68 6f 72 69 74 79 ':authority' value=31 30 2e 31 30 2e 31 33 38 2e 31 30 34 3a 35 30 30 35 31 '10.10.138.104:50051', key=67 72 70 63 2d 61 63 63 65 70 74 2d 65 6e 63 6f 64 69 6e 67 'grpc-accept-encoding' value=69 64 65 6e 74 69 74 79 2c 64 65 66 6c 61 74 65 2c 67 7a 69 70 'identity,deflate,gzip', key=74 65 'te' value=74 72 61 69 6c 65 72 73 'trailers', key=63 6f 6e 74 65 6e 74 2d 74 79 70 65 'content-type' value=61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc', key=75 73 65 72 2d 61 67 65 6e 74 'user-agent' value=67 72 70 63 2d 70 79 74 68 6f 6e 2f 31 2e 33 2e 35 20 67 72 70 63 2d 63 2f 33 2e 30 2e 30 20 28 6d 61 6e 79 6c 69 6e 75 78 3b 20 63 68 74 74 70 32 3b 20 67 65 6e 74 6c 65 29 'grpc-python/1.3.5 grpc-c/3.0.0 (manylinux; chttp2; gentle)' deadline=6155543.747898568} SEND_MESSAGE:flags=0x00000000:len=180 SEND_TRAILING_METADATA{} RECV_TRAILING_METADATA\nI0705 16:11:06.946934806   17021 http_client_filter.c:427]   OP[http-client:0x7f72cc96b2e8]: [COVERED] RECV_MESSAGE\nI0705 16:11:06.946938668   17021 connected_channel.c:70]     OP[connected:0x7f72cc96b318]: [COVERED] RECV_MESSAGE\nI0705 16:11:06.946972874   17021 chttp2_transport.c:1206]    HTTP:0:HDR:CLI: :scheme: http\nI0705 16:11:06.946977008   17021 chttp2_transport.c:1206]    HTTP:0:HDR:CLI: :method: POST\nI0705 16:11:06.946979992   17021 chttp2_transport.c:1206]    HTTP:0:HDR:CLI: :path: /mongo_proxy.MongoProxy/FindIter\nI0705 16:11:06.946983072   17021 chttp2_transport.c:1206]    HTTP:0:HDR:CLI: :authority: 10.10.138.104:50051\nI0705 16:11:06.946986058   17021 chttp2_transport.c:1206]    HTTP:0:HDR:CLI: grpc-accept-encoding: identity,deflate,gzip\nI0705 16:11:06.946989022   17021 chttp2_transport.c:1206]    HTTP:0:HDR:CLI: te: trailers\nI0705 16:11:06.946991884   17021 chttp2_transport.c:1206]    HTTP:0:HDR:CLI: content-type: application/grpc\nI0705 16:11:07.141734747   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:07.141782042   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296267, tv_nsec: 341775117, clock_type: 1 }, reserved=(nil))\nI0705 16:11:07.343170367   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:07.343227106   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296267, tv_nsec: 543219421, clock_type: 1 }, reserved=(nil))\nI0705 16:11:07.544510149   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:07.544567237   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296267, tv_nsec: 744559507, clock_type: 1 }, reserved=(nil))\nI0705 16:11:07.745911478   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:07.745974693   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296267, tv_nsec: 945967906, clock_type: 1 }, reserved=(nil))\nI0705 16:11:07.946877714   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:07.946929997   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296268, tv_nsec: 146923244, clock_type: 1 }, reserved=(nil))\nI0705 16:11:08.148247890   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:08.148299178   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296268, tv_nsec: 348291177, clock_type: 1 }, reserved=(nil))\nI0705 16:11:08.349577536   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:08.349675558   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296268, tv_nsec: 549665538, clock_type: 1 }, reserved=(nil))\nI0705 16:11:08.551029630   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:08.551084619   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296268, tv_nsec: 751076111, clock_type: 1 }, reserved=(nil))\nI0705 16:11:08.655816721   17021 parsing.c:500]              HTTP:1:HDR:CLI: :status: 32 30 30 '200'\nI0705 16:11:08.655872179   17021 parsing.c:500]              HTTP:1:HDR:CLI: content-type: 61 70 70 6c 69 63 61 74 69 6f 6e 2f 67 72 70 63 'application/grpc'\nI0705 16:11:08.655891838   17021 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0x104c6830, tag=0x7f7305212ad0, error=\"No Error\", done=0x7f733a74df80, done_arg=0xcaffce8, storage=0xcaffcf0)\nI0705 16:11:08.656308122   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: OP_COMPLETE: tag:0x7f7305212ad0 OK\nI0705 16:11:08.656334796   17021 metadata_array.c:42]        grpc_metadata_array_init(array=0x7f72d427e3d0)\nI0705 16:11:08.656351682   17021 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f73058b99d8)\nI0705 16:11:08.656494658   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296268, tv_nsec: 856488126, clock_type: 1 }, reserved=(nil))\nI0705 16:11:08.834506968   17021 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0x104c6830, tag=0x7f73052128a0, error=\"No Error\", done=0x7f733a74df80, done_arg=0xcaffe88, storage=0xcaffe90)\nI0705 16:11:08.834525225   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: OP_COMPLETE: tag:0x7f73052128a0 OK\nI0705 16:11:08.834537168   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:08.834554312   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:08.845830593   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296269, tv_nsec: 45826676, clock_type: 1 }, reserved=(nil))\nI0705 16:11:09.046301217   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:09.054537018   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296269, tv_nsec: 254531329, clock_type: 1 }, reserved=(nil))\nI0705 16:11:09.255776210   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:09.345079998   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296269, tv_nsec: 545071614, clock_type: 1 }, reserved=(nil))\nI0705 16:11:09.546359163   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:09.561483708   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296269, tv_nsec: 761479444, clock_type: 1 }, reserved=(nil))\nI0705 16:11:09.762723798   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:09.763123287   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296269, tv_nsec: 963119134, clock_type: 1 }, reserved=(nil))\nI0705 16:11:09.963733267   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:09.969065595   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296270, tv_nsec: 169063426, clock_type: 1 }, reserved=(nil))\nI0705 16:11:10.170339044   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:10.170520531   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296270, tv_nsec: 370517581, clock_type: 1 }, reserved=(nil))\nI0705 16:11:10.371797851   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:10.376260013   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296270, tv_nsec: 576256802, clock_type: 1 }, reserved=(nil))\nI0705 16:11:10.577525685   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:10.577666686   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296270, tv_nsec: 777646598, clock_type: 1 }, reserved=(nil))\nI0705 16:11:10.605871460   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:10.605884435   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:10.605887127   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:10.605890989   16015 init.c:228]                 grpc_init(void)\nI0705 16:11:10.605917938   16015 call.c:1759]                grpc_call_start_batch(call=0xcaff2e8, ops=0x119de950, nops=1, tag=0x7f730092ead0, reserved=(nil))\nI0705 16:11:10.605922343   16015 call.c:1402]                ops[0]: RECV_MESSAGE ptr=0x7f73006defb8\nI0705 16:11:10.605928746   16015 client_channel.c:1243]      OP[client-channel:0xcaff948]: [COVERED] RECV_MESSAGE\nI0705 16:11:10.605932732   16015 http_client_filter.c:427]   OP[http-client:0x7f72cc96b2e8]: [COVERED] RECV_MESSAGE\nI0705 16:11:10.605936100   16015 connected_channel.c:70]     OP[connected:0x7f72cc96b318]: [COVERED] RECV_MESSAGE\nI0705 16:11:10.606104888   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:10.750909797   17021 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0x104c6830, tag=0x7f730092ead0, error=\"No Error\", done=0x7f733a74df80, done_arg=0xcaffe88, storage=0xcaffe90)\nI0705 16:11:10.750927680   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: OP_COMPLETE: tag:0x7f730092ead0 OK\nI0705 16:11:10.750938856   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:10.750941974   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:10.750997775   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:10.764237837   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296270, tv_nsec: 964234257, clock_type: 1 }, reserved=(nil))\nI0705 16:11:10.964824910   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: QUEUE_TIMEOUT\nI0705 16:11:11.205073102   17021 completion_queue.c:359]     grpc_completion_queue_next(cc=0x104c6830, deadline=gpr_timespec { tv_sec: 1499296271, tv_nsec: 405065600, clock_type: 1 }, reserved=(nil))\nI0705 16:11:11.212421534   17021 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f72d427e4d0, cc=0x104c6830, tag=0x7f7305212bb0, error=\"No Error\", done=0x7f733a74df80, done_arg=0xcaffdb8, storage=0xcaffdc0)\nI0705 16:11:11.212494734   17021 completion_queue.c:444]     RETURN_EVENT[0x104c6830]: OP_COMPLETE: tag:0x7f7305212bb0 OK\nI0705 16:11:11.213062977   17021 metadata_array.c:42]        grpc_metadata_array_init(array=0x7f72d427e3d0)\nI0705 16:11:11.213068090   17021 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f73058b9828)\nI0705 16:11:11.213079705   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:11.213086232   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:11.213117524   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:11.213172954   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:11.213177089   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:11.213179326   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:11.213181348   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:11.213183935   17021 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:11.213186141   17021 init.c:233]                 grpc_shutdown(void)\n<_Rendezvous of RPC that terminated with (StatusCode.UNAVAILABLE, Failed parsing HTTP/2)>\nI0705 16:11:12.884827228   16015 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f73058b9828)\nI0705 16:11:12.884852063   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:12.884859917   16015 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f73058b99d8)\nI0705 16:11:12.884864725   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:12.884873988   16015 call.c:536]                 grpc_call_destroy(c=0xcaff2e8)\nI0705 16:11:12.884956162   16015 init.c:233]                 grpc_shutdown(void)\nI0705 16:11:12.994702758   16015 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f7333501f18)\nI0705 16:11:12.994722966   16015 init.c:233]                 grpc_shutdown(void)\n. Looks like an overflow issue:\nD0706 15:25:34.131850734   18525 combiner.c:359]             C:0x7f39bc005cd0 finish old_state=9\nD0706 15:25:34.131864662   18525 connectivity_state.c:185]   SET: 0x7f39d80029a0 subchannel: READY --> SHUTDOWN [reflect_child] error=0x7f39bc26a3f0 \n{\"created\":\"@1499379934.121527265\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":2166,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1499379934.121521435\",\"description\":\"frame of size 241 overflows incoming window of -21442\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":413}]}\nD0706 15:25:34.131895383   18525 combiner.c:289]             C:0x7f39bc005cd0 \ngrpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.131905453   18525 combiner.c:308]             C:0x7f39bc005cd0 maybe_finish_one n=0x7f39bc005ad8\nD0706 15:25:34.131912824   18525 chttp2_transport.c:2253]    ipv4:10.10.137.123:50051: Start BDP ping\nD0706 15:25:34.131920942   18525 timer_generic.c:323]        TIMER 0x7f39bc005c48: CANCEL pending=false\nD0706 15:25:34.131928947   18525 bdp_estimator.c:81]         bdp[ipv4:10.10.137.123:50051]:start acc=10311377 est=65536\nD0706 15:25:34.131936566   18525 combiner.c:359]             C:0x7f39bc005cd0 finish old_state=7\nD0706 15:25:34.131944877   18525 combiner.c:289]             C:0x7f39bc005cd0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.131952516   18525 combiner.c:308]             C:0x7f39bc005cd0 maybe_finish_one n=0x7f39bc005b00\nD0706 15:25:34.131959969   18525 chttp2_transport.c:2266]    ipv4:10.10.137.123:50051: Complete BDP ping\nD0706 15:25:34.131972843   18525 bdp_estimator.c:91]         bdp[ipv4:10.10.137.123:50051]:complete acc=0 est=65536\nD0706 15:25:34.131982600   18525 combiner.c:359]             C:0x7f39bc005cd0 finish old_state=5\nD0706 15:25:34.131990394   18525 combiner.c:289]             C:0x7f39bc005cd0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=1\nD0706 15:25:34.132001394   18525 combiner.c:340]             C:0x7f39bc005cd0 execute_final[0] c=0x7f39bc002828\nD0706 15:25:34.132009817   18525 chttp2_transport.c:800]     W:0x7f39bc002780 CLIENT state WRITING -> IDLE [begin writing nothing]\nD0706 15:25:34.132019891   18525 combiner.c:359]             C:0x7f39bc005cd0 finish old_state=3\nD0706 15:25:34.132031979   18525 timer_generic.c:323]        TIMER 0xb1d8ff0: CANCEL pending=true\nD0706 15:25:34.132049102   18525 call.c:690]                 get_final_status CLI\nD0706 15:25:34.132077594   18525 call.c:693]                   1: {\"created\":\"@1499379934.132046431\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.c\",\"file_line\":939,\"grpc_message\":\"Failed parsing HTTP/2\",\"grpc_status\":14}\nI0706 15:25:34.132093985   18525 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7f39c51fd4d0, cc=0x1120e060, tag=0x7f39f5218ad0, error=\"No Error\", done=0x7f3a2a497f80, done_arg=0xb1d9258, storage=0xb1d9260)\nD0706 15:25:34.132110108   18525 combiner.c:289]             C:0x9fa86f0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.132121551   18525 combiner.c:308]             C:0x9fa86f0 maybe_finish_one n=0x7f39d80015b0\nD0706 15:25:34.132134935   18525 connectivity_state.c:185]   SET: 0x7f39d8001600 (null): READY --> SHUTDOWN [selected_changed] error=0x7f39bc26a3f0 {\"created\":\"@1499379934.121527265\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":2166,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1499379934.121521435\",\"description\":\"frame of size 241 overflows incoming window of -21442\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":413}]}\nD0706 15:25:34.132149242   18525 connectivity_state.c:212]   NOTIFY: 0x7f39d8001600 (null): 0x7f39bc001b38\nD0706 15:25:34.132159464   18525 combiner.c:218]             C:0x9fa86f0 grpc_combiner_execute c=0x7f39bc001b38 cov=0 last=3\nD0706 15:25:34.132169182   18525 combiner.c:359]             C:0x9fa86f0 finish old_state=5\nD0706 15:25:34.132179565   18525 combiner.c:289]             C:0x9fa86f0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.132189697   18525 combiner.c:308]             C:0x9fa86f0 maybe_finish_one n=0x7f39bc001b38\nD0706 15:25:34.132281020   18525 combiner.c:218]             C:0x9fa86f0 grpc_combiner_execute c=0x7f39bc000aa0 cov=0 last=3\nD0706 15:25:34.132299018   18525 connectivity_state.c:185]   SET: 0x12ae2f38 client_channel: READY --> TRANSIENT_FAILURE [lb_changed] error=0x7f39bc26a3f0 {\"created\":\"@1499379934.121527265\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":2166,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1499379934.121521435\",\"description\":\"frame of size 241 overflows incoming window of -21442\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":413}]}\nD0706 15:25:34.132314814   18525 combiner.c:359]             C:0x9fa86f0 finish old_state=7\nD0706 15:25:34.132311910   18661 combiner.c:218]             C:0x9fa86f0 grpc_combiner_execute c=0x7f39bc001b00 cov=0 last=5\nD0706 15:25:34.132328923   18525 combiner.c:289]             C:0x9fa86f0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.132386533   18525 combiner.c:308]             C:0x9fa86f0 maybe_finish_one n=0x7f39bc000aa0\nD0706 15:25:34.132415126   18525 connectivity_state.c:185]   SET: 0x7f39d8001600 (null): SHUTDOWN --> SHUTDOWN [shutdown] error=0x7f39bc001f30 {\"created\":\"@1499379934.132398728\",\"description\":\"Channel shutdown\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c\",\"file_line\":104}\nD0706 15:25:34.132433335   18525 combiner.c:218]             C:0x7f39bc005cd0 grpc_combiner_execute c=0x7f39bc002140 cov=0 last=1\nD0706 15:25:34.132446815   18525 combiner.c:359]             C:0x9fa86f0 finish old_state=5\nD0706 15:25:34.137056411   18525 combiner.c:289]             C:0x9fa86f0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.137121871   18525 combiner.c:308]             C:0x9fa86f0 maybe_finish_one n=0x7f39bc001b00\nD0706 15:25:34.137148954   18525 combiner.c:218]             C:0x9fa86f0 grpc_combiner_execute c=0x12ae2f10 cov=0 last=3\nD0706 15:25:34.137159624   18525 combiner.c:359]             C:0x9fa86f0 finish old_state=5\nD0706 15:25:34.137168684   18525 combiner.c:289]             C:0x9fa86f0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.137176744   18525 combiner.c:308]             C:0x9fa86f0 maybe_finish_one n=0x12ae2f10\nD0706 15:25:34.137241667   18525 connectivity_state.c:110]   CONWATCH: 0x7f39bc0025b0 (null): get IDLE\nD0706 15:25:34.137256519   18525 connectivity_state.c:185]   SET: 0x12ae2f38 client_channel: TRANSIENT_FAILURE --> IDLE [new_lb+resolver] error=(nil) \"No Error\"\nD0706 15:25:34.137265954   18525 connectivity_state.c:135]   CONWATCH: 0x7f39bc0025b0 (null): from IDLE [cur=IDLE] notify=0x7f39bc001ec8\nD0706 15:25:34.137274255   18525 combiner.c:359]             C:0x9fa86f0 finish old_state=3\nD0706 15:25:34.137283963   18525 combiner.c:289]             C:0x7f39bc000d50 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.137297985   18525 combiner.c:308]             C:0x7f39bc000d50 maybe_finish_one n=0x7f39bc001c90\nD0706 15:25:34.137308381   18525 combiner.c:218]             C:0x7f39bc005cd0 grpc_combiner_execute c=0x7f39bc005b58 cov=0 last=3\nD0706 15:25:34.137318461   18525 combiner.c:218]             C:0x7f39bc005cd0 grpc_combiner_execute c=0x7f39bc005b80 cov=0 last=5\nD0706 15:25:34.137325842   18525 combiner.c:359]             C:0x7f39bc000d50 finish old_state=5\nD0706 15:25:34.137334149   18525 combiner.c:289]             C:0x7f39bc000d50 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.137344412   18525 combiner.c:308]             C:0x7f39bc000d50 maybe_finish_one n=0x7f39bc0010c0\nD0706 15:25:34.137353768   18525 combiner.c:359]             C:0x7f39bc000d50 finish old_state=3\nD0706 15:25:34.137363753   18525 combiner.c:289]             C:0x7f39bc005cd0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.137374004   18525 combiner.c:308]             C:0x7f39bc005cd0 maybe_finish_one n=0x7f39bc002140\nD0706 15:25:34.137383977   18525 connectivity_state.c:132]   CONWATCH: 0x7f39bc0029c8 client_transport: unsubscribe notify=0x7f39d80015b0\nD0706 15:25:34.137394740   18525 combiner.c:359]             C:0x7f39bc005cd0 finish old_state=7\nD0706 15:25:34.137405203   18525 combiner.c:289]             C:0x7f39bc005cd0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.137415060   18525 combiner.c:308]             C:0x7f39bc005cd0 maybe_finish_one n=0x7f39bc005b58\nD0706 15:25:34.137424540   18525 combiner.c:359]             C:0x7f39bc005cd0 finish old_state=5\nD0706 15:25:34.137434800   18525 combiner.c:289]             C:0x7f39bc005cd0 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0706 15:25:34.137444813   18525 combiner.c:308]             C:0x7f39bc005cd0 maybe_finish_one n=0x7f39bc005b80\nD0706 15:25:34.137454677   18525 combiner.c:359]             C:0x7f39bc005cd0 finish old_state=3\nI0706 15:25:34.137469482   18525 completion_queue.c:444]     RETURN_EVENT[0x1120e060]: OP_COMPLETE: tag:0x7f39f5218c90 OK\nI0706 15:25:34.137494953   18525 init.c:233]                 grpc_shutdown(void)\nI0706 15:25:34.137505755   18525 init.c:233]                 grpc_shutdown(void)\nI0706 15:25:34.175386381   18525 completion_queue.c:359]     grpc_completion_queue_next(cc=0x1120e060, deadline=gpr_timespec { tv_sec: 1499379934, tv_nsec: 375379208, clock_type: 1 }, reserved=(nil))\nI0706 15:25:34.175410982   18525 completion_queue.c:444]     RETURN_EVENT[0x1120e060]: OP_COMPLETE: tag:0x7f39f5218ad0 OK\nI0706 15:25:34.185066014   18525 metadata_array.c:42]        grpc_metadata_array_init(array=0x7f39c51fd3d0)\nI0706 15:25:34.185083363   18525 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7f39c7dc3348)\nI0706 15:25:34.185096135   18525 init.c:233]                 grpc_shutdown(void)\nI0706 15:25:34.185101304   18525 init.c:233]                 grpc_shutdown(void)\nD0706 15:25:34.185107858   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 8192; free_pool -> 174848\nD0706 15:25:34.185113317   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 8192; free_pool -> 183040\nD0706 15:25:34.185116808   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 7936; free_pool -> 190976\nD0706 15:25:34.185122073   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 7936; free_pool -> 198912\nD0706 15:25:34.185127912   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 7936; free_pool -> 206848\nD0706 15:25:34.185132315   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 7936; free_pool -> 214784\nD0706 15:25:34.185136593   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 7936; free_pool -> 222720\nD0706 15:25:34.185140867   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 7936; free_pool -> 230656\nD0706 15:25:34.185145475   18525 resource_quota.c:814]       RQ anonymous_pool_7f39bc000c80 ipv4:10.10.137.123:50051: free 7936; free_pool -> 238592. 1. It's one RPC at a time\n2. Data sizes are approximately 10MB per send() call (10,000 objects, 1KB each)\n3. The connection is long lived\nI'll see if I can get a minimum reproduction... but the above should fairly accurately represent what I'm doing. The use-case right now is long-lived cron jobs that crash halfway through because of this bug.. ```\nD0710 13:11:17.861470979   19635 combiner.c:218]             C:0x7fbe64002720 grpc_combiner_execute c=0x7fbe64004430 cov=0 last=1\nD0710 13:11:17.861474152   19635 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861477149   19635 combiner.c:308]             C:0x7fbe64002720 maybe_finish_one n=0x7fbe64004430\nD0710 13:11:17.861484725   19635 chttp2_transport.c:2397]    set connectivity_state=4\nD0710 13:11:17.861499534   19635 connectivity_state.c:185]   SET: 0x7fbe64004558 client_transport: READY --> SHUTDOWN [close_transport] error=0x7fbe640025e0 {\"created\":\"@1499717477.861483877\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":2166,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1499717477.861481746\",\"description\":\"frame of size 84 overflows incoming window of -16490\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":413}]}\nD0710 13:11:17.861503726   19635 connectivity_state.c:212]   NOTIFY: 0x7fbe64004558 client_transport: 0x7fbe640012b0\nD0710 13:11:17.861506710   19635 combiner.c:218]             C:0xa4e0000 grpc_combiner_execute c=0x7fbe640012b0 cov=0 last=1\nD0710 13:11:17.861509615   19635 connectivity_state.c:212]   NOTIFY: 0x7fbe64004558 client_transport: 0x7fbe64007df0\nD0710 13:11:17.861522121   19635 combiner.c:218]             C:0x7fbe64002c30 grpc_combiner_execute c=0x7fbe64002460 cov=0 last=1\nD0710 13:11:17.861525976   19635 timer_generic.c:323]        TIMER 0x7fbe640077d8: CANCEL pending=false\nD0710 13:11:17.861529809   19635 chttp2_transport.c:800]     W:0x7fbe64004310 CLIENT state IDLE -> WRITING [rst_stream]\nD0710 13:11:17.861532817   19635 combiner.c:407]             C:0x7fbe64002720 grpc_combiner_execute_finally c=0x7fbe640043b8; ac=0x7fbe64002720; cov=0\nD0710 13:11:17.861536787   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 15616\nD0710 13:11:17.861539567   19635 combiner.c:218]             C:0x7fbe64002c30 grpc_combiner_execute c=0x7fbe64002fa0 cov=0 last=3\nD0710 13:11:17.861542740   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 31232\nD0710 13:11:17.861545852   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 46848\nD0710 13:11:17.861548844   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 62464\nD0710 13:11:17.861551683   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 78080\nD0710 13:11:17.861555240   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 93696\nD0710 13:11:17.861558954   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 109312\nD0710 13:11:17.861562162   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 124928\nD0710 13:11:17.861565281   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 140544\nD0710 13:11:17.861567995   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 156160\nD0710 13:11:17.861574807   19635 chttp2_transport.c:1086]    complete_closure_step: 0xa4ba600 refs=0 flags=0x0003 desc=recv_trailing_metadata_finished err=\"No Error\"\nD0710 13:11:17.861578310   19635 combiner.c:218]             C:0x7fbe64002720 grpc_combiner_execute c=0x7fbe64007668 cov=0 last=5\nD0710 13:11:17.861581375   19635 combiner.c:218]             C:0x7fbe64002720 grpc_combiner_execute c=0x7fbe64007690 cov=0 last=7\nD0710 13:11:17.861584530   19635 combiner.c:359]             C:0x7fbe64002720 finish old_state=9\nD0710 13:11:17.861588675   19635 connectivity_state.c:185]   SET: 0x7fbe64001c10 subchannel: READY --> SHUTDOWN [reflect_child] error=0x7fbe640025e0 {\"created\":\"@1499717477.861483877\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":2166,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1499717477.861481746\",\"description\":\"frame of size 84 overflows incoming window of -16490\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":413}]}\nD0710 13:11:17.861596257   19635 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861599703   19635 combiner.c:308]             C:0x7fbe64002720 maybe_finish_one n=0x7fbe64007668\nD0710 13:11:17.861602953   19635 chttp2_transport.c:2253]    ipv4:127.0.0.1:50051: Start BDP ping\nD0710 13:11:17.861605839   19635 timer_generic.c:323]        TIMER 0x7fbe640077d8: CANCEL pending=false\nD0710 13:11:17.861608654   19635 bdp_estimator.c:81]         bdp[ipv4:127.0.0.1:50051]:start acc=10327381 est=65536\nD0710 13:11:17.861611064   19635 combiner.c:359]             C:0x7fbe64002720 finish old_state=7\nD0710 13:11:17.861614108   19635 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861617139   19635 combiner.c:308]             C:0x7fbe64002720 maybe_finish_one n=0x7fbe64007690\nD0710 13:11:17.861619921   19635 chttp2_transport.c:2266]    ipv4:127.0.0.1:50051: Complete BDP ping\nD0710 13:11:17.861622351   19635 bdp_estimator.c:91]         bdp[ipv4:127.0.0.1:50051]:complete acc=0 est=65536\nD0710 13:11:17.861625086   19635 combiner.c:359]             C:0x7fbe64002720 finish old_state=5\nD0710 13:11:17.861628049   19635 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=1\nD0710 13:11:17.861631047   19635 combiner.c:340]             C:0x7fbe64002720 execute_final[0] c=0x7fbe640043b8\nD0710 13:11:17.861633964   19635 chttp2_transport.c:800]     W:0x7fbe64004310 CLIENT state WRITING -> IDLE [begin writing nothing]\nD0710 13:11:17.861636715   19635 combiner.c:359]             C:0x7fbe64002720 finish old_state=3\nD0710 13:11:17.861640499   19635 timer_generic.c:323]        TIMER 0xa4b9fb8: CANCEL pending=true\nD0710 13:11:17.861645660   19635 call.c:690]                 get_final_status CLI\nD0710 13:11:17.861652736   19635 call.c:693]                   1: {\"created\":\"@1499717477.861644736\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.c\",\"file_line\":939,\"grpc_message\":\"Failed parsing HTTP/2\",\"grpc_status\":14}\nI0710 13:11:17.861657040   19635 completion_queue.c:224]     grpc_cq_end_op(exec_ctx=0x7fbe71a994d0, cc=0xa4de3b0, tag=0x7fbe7273f830, error=\"No Error\", done=0x7fbea7f46f80, done_arg=0xa4ba220, storage=0xa4ba228)\nD0710 13:11:17.861661401   19635 combiner.c:289]             C:0xa4e0000 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861664627   19635 combiner.c:308]             C:0xa4e0000 maybe_finish_one n=0x7fbe640012b0\nD0710 13:11:17.861668531   19635 connectivity_state.c:185]   SET: 0x7fbe64001300 (null): READY --> SHUTDOWN [selected_changed] error=0x7fbe640025e0 {\"created\":\"@1499717477.861483877\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":2166,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1499717477.861481746\",\"description\":\"frame of size 84 overflows incoming window of -16490\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":413}]}\nD0710 13:11:17.861672541   19635 connectivity_state.c:212]   NOTIFY: 0x7fbe64001300 (null): 0x7fbe64002548\nD0710 13:11:17.861675295   19635 combiner.c:218]             C:0xa4e0000 grpc_combiner_execute c=0x7fbe64002548 cov=0 last=3\nD0710 13:11:17.861678107   19635 combiner.c:359]             C:0xa4e0000 finish old_state=5\nD0710 13:11:17.861681240   19635 combiner.c:289]             C:0xa4e0000 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861684331   19635 combiner.c:308]             C:0xa4e0000 maybe_finish_one n=0x7fbe64002548\nD0710 13:11:17.861709962   19635 combiner.c:218]             C:0xa4e0000 grpc_combiner_execute c=0x7fbe640036c0 cov=0 last=3\nD0710 13:11:17.861714348   19635 connectivity_state.c:185]   SET: 0xa4eefc8 client_channel: READY --> TRANSIENT_FAILURE [lb_changed] error=0x7fbe640025e0 {\"created\":\"@1499717477.861483877\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":2166,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1499717477.861481746\",\"description\":\"frame of size 84 overflows incoming window of -16490\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":413}]}\nD0710 13:11:17.861718477   19635 combiner.c:359]             C:0xa4e0000 finish old_state=5\nD0710 13:11:17.861721506   19635 combiner.c:289]             C:0xa4e0000 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861724868   19635 combiner.c:308]             C:0xa4e0000 maybe_finish_one n=0x7fbe640036c0\nD0710 13:11:17.861731309   19635 connectivity_state.c:185]   SET: 0x7fbe64001300 (null): SHUTDOWN --> SHUTDOWN [shutdown] error=0x7fbe64001b40 {\"created\":\"@1499717477.861727432\",\"description\":\"Channel shutdown\",\"file\":\"src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.c\",\"file_line\":104}\nD0710 13:11:17.861736497   19635 combiner.c:218]             C:0x7fbe64002720 grpc_combiner_execute c=0x7fbe64002330 cov=0 last=1\nD0710 13:11:17.861740036   19635 combiner.c:359]             C:0xa4e0000 finish old_state=3\nD0710 13:11:17.861743097   19635 combiner.c:289]             C:0x7fbe64002c30 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861746040   19635 combiner.c:308]             C:0x7fbe64002c30 maybe_finish_one n=0x7fbe64002460\nD0710 13:11:17.861750026   19635 combiner.c:218]             C:0x7fbe64002720 grpc_combiner_execute c=0x7fbe640076e8 cov=0 last=3\nD0710 13:11:17.861752221   19635 combiner.c:218]             C:0x7fbe64002720 grpc_combiner_execute c=0x7fbe64007710 cov=0 last=5\nD0710 13:11:17.861754291   19635 combiner.c:359]             C:0x7fbe64002c30 finish old_state=5\nD0710 13:11:17.861756410   19635 combiner.c:289]             C:0x7fbe64002c30 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861758555   19635 combiner.c:308]             C:0x7fbe64002c30 maybe_finish_one n=0x7fbe64002fa0\nD0710 13:11:17.861760646   19635 combiner.c:359]             C:0x7fbe64002c30 finish old_state=3\nD0710 13:11:17.861762735   19635 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861764886   19635 combiner.c:308]             C:0x7fbe64002720 maybe_finish_one n=0x7fbe64002330\nD0710 13:11:17.861767837   19635 connectivity_state.c:132]   CONWATCH: 0x7fbe64004558 client_transport: unsubscribe notify=0x7fbe640012b0\nD0710 13:11:17.861770073   19635 combiner.c:359]             C:0x7fbe64002720 finish old_state=7\nD0710 13:11:17.861772273   19635 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861774529   19635 combiner.c:308]             C:0x7fbe64002720 maybe_finish_one n=0x7fbe640076e8\nD0710 13:11:17.861772286   19643 combiner.c:218]             C:0xa4e0000 grpc_combiner_execute c=0x7fbe640028f0 cov=0 last=1\nD0710 13:11:17.861776502   19635 combiner.c:359]             C:0x7fbe64002720 finish old_state=5\nD0710 13:11:17.861784607   19643 combiner.c:289]             C:0xa4e0000 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861786222   19635 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861796622   19635 combiner.c:308]             C:0x7fbe64002720 maybe_finish_one n=0x7fbe64007710\nD0710 13:11:17.861791165   19643 combiner.c:308]             C:0xa4e0000 maybe_finish_one n=0x7fbe640028f0\nD0710 13:11:17.861800685   19635 combiner.c:359]             C:0x7fbe64002720 finish old_state=3\nD0710 13:11:17.861812815   19643 combiner.c:218]             C:0xa4e0000 grpc_combiner_execute c=0xa4eefa0 cov=0 last=3\nI0710 13:11:17.861816919   19635 completion_queue.c:444]     RETURN_EVENT[0xa4de3b0]: OP_COMPLETE: tag:0x7fbe7273f750 OK\nD0710 13:11:17.861817782   19643 combiner.c:359]             C:0xa4e0000 finish old_state=5\nI0710 13:11:17.861825215   19635 metadata_array.c:42]        grpc_metadata_array_init(array=0x7fbe71a993d0)\nD0710 13:11:17.861827125   19643 combiner.c:289]             C:0xa4e0000 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:17.861832746   19643 combiner.c:308]             C:0xa4e0000 maybe_finish_one n=0xa4eefa0\nI0710 13:11:17.861828155   19635 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7fbe72b0a138)\nD0710 13:11:17.861856196   19643 connectivity_state.c:110]   CONWATCH: 0x7fbe6c001990 (null): get IDLE\nD0710 13:11:17.861861333   19643 connectivity_state.c:185]   SET: 0xa4eefc8 client_channel: TRANSIENT_FAILURE --> IDLE [new_lb+resolver] error=(nil) \"No Error\"\nD0710 13:11:17.861864747   19643 connectivity_state.c:135]   CONWATCH: 0x7fbe6c001990 (null): from IDLE [cur=IDLE] notify=0x7fbe6c000d88\nD0710 13:11:17.861868032   19643 combiner.c:359]             C:0xa4e0000 finish old_state=3\nI0710 13:11:17.861893351   19635 completion_queue.c:359]     grpc_completion_queue_next(cc=0xa4de3b0, deadline=gpr_timespec { tv_sec: 1499717478, tv_nsec: 61891667, clock_type: 1 }, reserved=(nil))\nI0710 13:11:17.861898837   19635 completion_queue.c:444]     RETURN_EVENT[0xa4de3b0]: OP_COMPLETE: tag:0x7fbe7273f7c0 OK\nI0710 13:11:17.861914225   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:17.861917673   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:17.919513786   19635 completion_queue.c:359]     grpc_completion_queue_next(cc=0xa4de3b0, deadline=gpr_timespec { tv_sec: 1499717478, tv_nsec: 119507785, clock_type: 1 }, reserved=(nil))\nI0710 13:11:17.919532765   19635 completion_queue.c:444]     RETURN_EVENT[0xa4de3b0]: OP_COMPLETE: tag:0x7fbe7273f830 OK\nI0710 13:11:18.083299724   19635 metadata_array.c:42]        grpc_metadata_array_init(array=0x7fbe71a993d0)\nI0710 13:11:18.083316157   19635 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7fbe72b07eb8)\nI0710 13:11:18.083332443   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:18.083339551   19635 init.c:233]                 grpc_shutdown(void)\nD0710 13:11:18.083347219   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 8192; free_pool -> 164352\nD0710 13:11:18.083353578   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 16128; free_pool -> 180480\nD0710 13:11:18.083359446   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 8192; free_pool -> 188672\nD0710 13:11:18.083366084   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 204288\nD0710 13:11:18.083372923   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 219904\nD0710 13:11:18.083377820   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 235520\nD0710 13:11:18.083384870   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 251136\nD0710 13:11:18.083390188   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 266752\nD0710 13:11:18.083395096   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 282368\nD0710 13:11:18.083400410   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 297984\nD0710 13:11:18.083406754   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 313600\nD0710 13:11:18.083412454   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 329216\nD0710 13:11:18.083418759   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 344832\nD0710 13:11:18.083425311   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 360448\nD0710 13:11:18.083431760   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 376064\nD0710 13:11:18.083437561   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 391680\nD0710 13:11:18.083443555   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 407296\nD0710 13:11:18.083449565   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 422912\nD0710 13:11:18.083455485   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 438528\nD0710 13:11:18.083461791   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 454144\nD0710 13:11:18.083468139   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 469760\nD0710 13:11:18.083474476   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 485376\nD0710 13:11:18.083480694   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 500992\nD0710 13:11:18.083487112   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 516608\nD0710 13:11:18.083493596   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 532224\nD0710 13:11:18.083500107   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 547840\nD0710 13:11:18.083506472   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 563456\nD0710 13:11:18.083512939   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 579072\nD0710 13:11:18.083519281   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 594688\nD0710 13:11:18.083525727   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 610304\nD0710 13:11:18.083531947   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 625920\nD0710 13:11:18.083538355   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 641536\nD0710 13:11:18.083544703   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 657152\nD0710 13:11:18.083551115   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 672768\nD0710 13:11:18.083557513   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 688384\nD0710 13:11:18.083563898   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 704000\nD0710 13:11:18.083570217   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 719616\nD0710 13:11:18.083576557   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 735232\nD0710 13:11:18.083582899   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 750848\nD0710 13:11:18.083589141   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 766464\nD0710 13:11:18.083595547   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 782080\nD0710 13:11:18.083601877   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 797696\nD0710 13:11:18.083608169   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 813312\nD0710 13:11:18.083614519   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 828928\nD0710 13:11:18.083620852   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 844544\nD0710 13:11:18.083627134   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 860160\nD0710 13:11:18.083633546   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 875776\nD0710 13:11:18.083639851   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 891392\nD0710 13:11:18.083646306   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 907008\nD0710 13:11:18.083652781   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 922624\nD0710 13:11:18.083659048   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 938240\nD0710 13:11:18.083665394   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 953856\nD0710 13:11:18.083671886   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 969472\nD0710 13:11:18.083678358   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 985088\nD0710 13:11:18.083684733   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1000704\nD0710 13:11:18.083691707   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1016320\nD0710 13:11:18.083698117   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1031936\nD0710 13:11:18.083704502   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1047552\nD0710 13:11:18.083710877   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1063168\nD0710 13:11:18.083717253   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1078784\nD0710 13:11:18.083723627   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1094400\nD0710 13:11:18.083729948   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1110016\nD0710 13:11:18.083736203   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1125632\nD0710 13:11:18.083742537   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1141248\nD0710 13:11:18.083748926   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1156864\nD0710 13:11:18.083755392   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1172480\nD0710 13:11:18.083761695   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1188096\nD0710 13:11:18.083768109   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1203712\nD0710 13:11:18.083774429   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1219328\nD0710 13:11:18.083780806   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1234944\nD0710 13:11:18.083788662   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1250560\nD0710 13:11:18.083795112   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1266176\nD0710 13:11:18.083800797   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1281792\nD0710 13:11:18.083806808   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1297408\nD0710 13:11:18.083813192   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1313024\nD0710 13:11:18.083819529   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1328640\nD0710 13:11:18.083825962   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1344256\nD0710 13:11:18.083832394   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1359872\nD0710 13:11:18.083838602   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1375488\nD0710 13:11:18.083844981   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1391104\nD0710 13:11:18.083851372   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1406720\nD0710 13:11:18.083857686   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1422336\nD0710 13:11:18.083864153   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1437952\nD0710 13:11:18.083870436   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1453568\nD0710 13:11:18.083876668   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1469184\nD0710 13:11:18.083883195   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1484800\nD0710 13:11:18.083889457   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1500416\nD0710 13:11:18.083895719   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1516032\nD0710 13:11:18.083902185   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1531648\nD0710 13:11:18.083908538   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1547264\nD0710 13:11:18.083914884   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1562880\nD0710 13:11:18.083921248   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1578496\nD0710 13:11:18.083927537   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1594112\nD0710 13:11:18.083933873   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1609728\nD0710 13:11:18.083940341   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1625344\nD0710 13:11:18.083946778   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1640960\nD0710 13:11:18.083953099   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1656576\nD0710 13:11:18.083959439   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1672192\nD0710 13:11:18.083965762   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1687808\nD0710 13:11:18.083971979   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1703424\nD0710 13:11:18.083978356   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1719040\nD0710 13:11:18.083984702   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1734656\nD0710 13:11:18.083991067   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1750272\nD0710 13:11:18.083997381   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1765888\nD0710 13:11:18.084003634   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1781504\nD0710 13:11:18.084010099   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1797120\nD0710 13:11:18.084016502   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1812736\nD0710 13:11:18.084022820   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1828352\nD0710 13:11:18.084029188   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1843968\nD0710 13:11:18.084035642   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1859584\nD0710 13:11:18.084041892   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1875200\nD0710 13:11:18.084048388   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1890816\nD0710 13:11:18.084054769   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1906432\nD0710 13:11:18.084061000   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1922048\nD0710 13:11:18.084067252   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1937664\nD0710 13:11:18.084073584   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1953280\nD0710 13:11:18.084079876   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1968896\nD0710 13:11:18.084086139   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 1984512\nD0710 13:11:18.084092606   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2000128\nD0710 13:11:18.084099019   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2015744\nD0710 13:11:18.084105302   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2031360\nD0710 13:11:18.084111615   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2046976\nD0710 13:11:18.084118097   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2062592\nD0710 13:11:18.084124531   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2078208\nD0710 13:11:18.084130867   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2093824\nD0710 13:11:18.084137243   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2109440\nD0710 13:11:18.084143595   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2125056\nD0710 13:11:18.084150062   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2140672\nD0710 13:11:18.084156541   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2156288\nD0710 13:11:18.084162916   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2171904\nD0710 13:11:18.084169292   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2187520\nD0710 13:11:18.084175540   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2203136\nD0710 13:11:18.084182332   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2218752\nD0710 13:11:18.084188835   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2234368\nD0710 13:11:18.084195094   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2249984\nD0710 13:11:18.084201323   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2265600\nD0710 13:11:18.084207810   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2281216\nD0710 13:11:18.084214266   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2296832\nD0710 13:11:18.084221028   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2312448\nD0710 13:11:18.084227237   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2328064\nD0710 13:11:18.084233526   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2343680\nD0710 13:11:18.084239886   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2359296\nD0710 13:11:18.084246182   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2374912\nD0710 13:11:18.084252566   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2390528\nD0710 13:11:18.084259078   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2406144\nD0710 13:11:18.084265305   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2421760\nD0710 13:11:18.084271702   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2437376\nD0710 13:11:18.084277842   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2452992\nD0710 13:11:18.084284269   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2468608\nD0710 13:11:18.084290688   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2484224\nD0710 13:11:18.084296979   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2499840\nD0710 13:11:18.084303357   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2515456\nD0710 13:11:18.084309782   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2531072\nD0710 13:11:18.084316306   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2546688\nD0710 13:11:18.084322739   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2562304\nD0710 13:11:18.084328879   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2577920\nD0710 13:11:18.084335264   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2593536\nD0710 13:11:18.084341764   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2609152\nD0710 13:11:18.084347898   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2624768\nD0710 13:11:18.084353933   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2640384\nD0710 13:11:18.084360016   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2656000\nD0710 13:11:18.084365911   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2671616\nD0710 13:11:18.084372393   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2687232\nD0710 13:11:18.084378828   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2702848\nD0710 13:11:18.084385404   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2718464\nD0710 13:11:18.084391913   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2734080\nD0710 13:11:18.084398368   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2749696\nD0710 13:11:18.084404773   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2765312\nD0710 13:11:18.084411067   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2780928\nD0710 13:11:18.084417592   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2796544\nD0710 13:11:18.084424051   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2812160\nD0710 13:11:18.084430256   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2827776\nD0710 13:11:18.084436772   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2843392\nD0710 13:11:18.084443173   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2859008\nD0710 13:11:18.084449573   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2874624\nD0710 13:11:18.084456102   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2890240\nD0710 13:11:18.084462448   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2905856\nD0710 13:11:18.084468819   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2921472\nD0710 13:11:18.084475352   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2937088\nD0710 13:11:18.084481652   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2952704\nD0710 13:11:18.084487975   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2968320\nD0710 13:11:18.084494390   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2983936\nD0710 13:11:18.084500935   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 2999552\nD0710 13:11:18.084507391   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3015168\nD0710 13:11:18.084513778   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3030784\nD0710 13:11:18.084520210   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3046400\nD0710 13:11:18.084526570   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3062016\nD0710 13:11:18.084533015   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3077632\nD0710 13:11:18.084539330   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3093248\nD0710 13:11:18.084545681   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3108864\nD0710 13:11:18.084552009   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3124480\nD0710 13:11:18.084558445   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3140096\nD0710 13:11:18.084564763   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3155712\nD0710 13:11:18.084571089   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3171328\nD0710 13:11:18.084577448   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3186944\nD0710 13:11:18.084583743   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3202560\nD0710 13:11:18.084590059   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3218176\nD0710 13:11:18.084596531   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3233792\nD0710 13:11:18.084602881   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3249408\nD0710 13:11:18.084609338   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3265024\nD0710 13:11:18.084615670   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3280640\nD0710 13:11:18.084621992   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3296256\nD0710 13:11:18.084628519   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3311872\nD0710 13:11:18.084634895   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3327488\nD0710 13:11:18.084641502   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3343104\nD0710 13:11:18.084648562   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3358720\nD0710 13:11:18.084655732   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3374336\nD0710 13:11:18.084661944   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3389952\nD0710 13:11:18.084667573   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3405568\nD0710 13:11:18.084672539   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3421184\nD0710 13:11:18.084678457   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3436800\nD0710 13:11:18.084684331   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3452416\nD0710 13:11:18.084690248   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3468032\nD0710 13:11:18.084696625   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3483648\nD0710 13:11:18.084703094   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3499264\nD0710 13:11:18.084709457   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3514880\nD0710 13:11:18.084715583   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3530496\nD0710 13:11:18.084721841   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3546112\nD0710 13:11:18.084728310   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3561728\nD0710 13:11:18.084734660   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3577344\nD0710 13:11:18.084741048   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3592960\nD0710 13:11:18.084747343   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3608576\nD0710 13:11:18.084753613   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3624192\nD0710 13:11:18.084759951   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3639808\nD0710 13:11:18.084766276   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3655424\nD0710 13:11:18.084772545   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3671040\nD0710 13:11:18.084778756   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3686656\nD0710 13:11:18.084785170   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3702272\nD0710 13:11:18.084791599   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3717888\nD0710 13:11:18.084798191   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3733504\nD0710 13:11:18.084804560   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3749120\nD0710 13:11:18.084810941   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3764736\nD0710 13:11:18.084817192   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3780352\nD0710 13:11:18.084823698   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3795968\nD0710 13:11:18.084830142   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3811584\nD0710 13:11:18.084836435   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3827200\nD0710 13:11:18.084842425   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3842816\nD0710 13:11:18.084848487   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3858432\nD0710 13:11:18.084854571   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3874048\nD0710 13:11:18.084861002   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3889664\nD0710 13:11:18.084867456   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3905280\nD0710 13:11:18.084873878   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3920896\nD0710 13:11:18.084880344   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3936512\nD0710 13:11:18.084886709   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3952128\nD0710 13:11:18.084893005   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3967744\nD0710 13:11:18.084899383   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3983360\nD0710 13:11:18.084905762   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 3998976\nD0710 13:11:18.084912216   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4014592\nD0710 13:11:18.084918631   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4030208\nD0710 13:11:18.084924880   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4045824\nD0710 13:11:18.084931270   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4061440\nD0710 13:11:18.084937557   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4077056\nD0710 13:11:18.084943800   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4092672\nD0710 13:11:18.084950066   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4108288\nD0710 13:11:18.084956594   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4123904\nD0710 13:11:18.084962881   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4139520\nD0710 13:11:18.084969221   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4155136\nD0710 13:11:18.084975423   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4170752\nD0710 13:11:18.084981800   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4186368\nD0710 13:11:18.084988165   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4201984\nD0710 13:11:18.084994525   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4217600\nD0710 13:11:18.085000828   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4233216\nD0710 13:11:18.085007312   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4248832\nD0710 13:11:18.085013596   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4264448\nD0710 13:11:18.085019985   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4280064\nD0710 13:11:18.085026306   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4295680\nD0710 13:11:18.085032687   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4311296\nD0710 13:11:18.085038953   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4326912\nD0710 13:11:18.085045400   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4342528\nD0710 13:11:18.085051577   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4358144\nD0710 13:11:18.085057909   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4373760\nD0710 13:11:18.085064268   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4389376\nD0710 13:11:18.085070653   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4404992\nD0710 13:11:18.085077008   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4420608\nD0710 13:11:18.085083477   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4436224\nD0710 13:11:18.085089738   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4451840\nD0710 13:11:18.085096119   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4467456\nD0710 13:11:18.085102464   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4483072\nD0710 13:11:18.085108690   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4498688\nD0710 13:11:18.085115003   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4514304\nD0710 13:11:18.085121521   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4529920\nD0710 13:11:18.085127942   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4545536\nD0710 13:11:18.085134321   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4561152\nD0710 13:11:18.085140640   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4576768\nD0710 13:11:18.085147076   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4592384\nD0710 13:11:18.085153481   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4608000\nD0710 13:11:18.085160071   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4623616\nD0710 13:11:18.085166390   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4639232\nD0710 13:11:18.085172812   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4654848\nD0710 13:11:18.085179239   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4670464\nD0710 13:11:18.085185674   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4686080\nD0710 13:11:18.085191879   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4701696\nD0710 13:11:18.085198150   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4717312\nD0710 13:11:18.085204829   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4732928\nD0710 13:11:18.085211174   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4748544\nD0710 13:11:18.085217575   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4764160\nD0710 13:11:18.085223930   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4779776\nD0710 13:11:18.085230346   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4795392\nD0710 13:11:18.085236776   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4811008\nD0710 13:11:18.085243115   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4826624\nD0710 13:11:18.085249277   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4842240\nD0710 13:11:18.085255693   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4857856\nD0710 13:11:18.085261935   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4873472\nD0710 13:11:18.085268278   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4889088\nD0710 13:11:18.085275222   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4904704\nD0710 13:11:18.085281415   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4920320\nD0710 13:11:18.085287758   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4935936\nD0710 13:11:18.085294096   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4951552\nD0710 13:11:18.085300383   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4967168\nD0710 13:11:18.085306734   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4982784\nD0710 13:11:18.085313129   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 4998400\nD0710 13:11:18.085319568   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5014016\nD0710 13:11:18.085325939   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5029632\nD0710 13:11:18.085332393   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5045248\nD0710 13:11:18.085338786   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5060864\nD0710 13:11:18.085345008   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5076480\nD0710 13:11:18.085351380   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5092096\nD0710 13:11:18.085358270   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5107712\nD0710 13:11:18.085364200   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5123328\nD0710 13:11:18.085370290   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5138944\nD0710 13:11:18.085375800   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5154560\nD0710 13:11:18.085381020   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5170176\nD0710 13:11:18.085387021   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5185792\nD0710 13:11:18.085390829   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5201408\nD0710 13:11:18.085395982   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5217024\nD0710 13:11:18.085401687   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5232640\nD0710 13:11:18.085406346   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5248256\nD0710 13:11:18.085411043   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5263872\nD0710 13:11:18.085415832   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5279488\nD0710 13:11:18.085420578   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5295104\nD0710 13:11:18.085425190   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5310720\nD0710 13:11:18.085430147   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5326336\nD0710 13:11:18.085434852   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5341952\nD0710 13:11:18.085439791   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5357568\nD0710 13:11:18.085446159   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5373184\nD0710 13:11:18.085451890   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5388800\nD0710 13:11:18.085457874   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5404416\nD0710 13:11:18.085463605   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5420032\nD0710 13:11:18.085469434   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5435648\nD0710 13:11:18.085475788   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5451264\nD0710 13:11:18.085482130   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5466880\nD0710 13:11:18.085488450   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5482496\nD0710 13:11:18.085494796   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5498112\nD0710 13:11:18.085501249   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5513728\nD0710 13:11:18.085508779   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5529344\nD0710 13:11:18.085514710   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5544960\nD0710 13:11:18.085520973   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5560576\nD0710 13:11:18.085526630   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5576192\nD0710 13:11:18.085532781   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5591808\nD0710 13:11:18.085539046   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5607424\nD0710 13:11:18.085545169   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5623040\nD0710 13:11:18.085551495   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5638656\nD0710 13:11:18.085557815   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5654272\nD0710 13:11:18.085563869   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5669888\nD0710 13:11:18.085569848   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5685504\nD0710 13:11:18.085576016   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5701120\nD0710 13:11:18.085582124   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5716736\nD0710 13:11:18.085588522   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5732352\nD0710 13:11:18.085594854   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5747968\nD0710 13:11:18.085601352   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5763584\nD0710 13:11:18.085607588   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5779200\nD0710 13:11:18.085613847   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5794816\nD0710 13:11:18.085620195   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5810432\nD0710 13:11:18.085626453   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5826048\nD0710 13:11:18.085632867   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5841664\nD0710 13:11:18.085639192   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5857280\nD0710 13:11:18.085645445   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5872896\nD0710 13:11:18.085651854   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5888512\nD0710 13:11:18.085658198   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5904128\nD0710 13:11:18.085664560   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5919744\nD0710 13:11:18.085670713   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5935360\nD0710 13:11:18.085676990   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5950976\nD0710 13:11:18.085683491   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5966592\nD0710 13:11:18.085689789   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5982208\nD0710 13:11:18.085696113   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 5997824\nD0710 13:11:18.085702387   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6013440\nD0710 13:11:18.085708669   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6029056\nD0710 13:11:18.085714902   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6044672\nD0710 13:11:18.085721088   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6060288\nD0710 13:11:18.085727442   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6075904\nD0710 13:11:18.085733772   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6091520\nD0710 13:11:18.085740142   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6107136\nD0710 13:11:18.085746297   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6122752\nD0710 13:11:18.085752593   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6138368\nD0710 13:11:18.085758787   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6153984\nD0710 13:11:18.085765189   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6169600\nD0710 13:11:18.085771497   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6185216\nD0710 13:11:18.085778050   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6200832\nD0710 13:11:18.085784328   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6216448\nD0710 13:11:18.085790733   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6232064\nD0710 13:11:18.085796962   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6247680\nD0710 13:11:18.085803416   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6263296\nD0710 13:11:18.085809829   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6278912\nD0710 13:11:18.085816060   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6294528\nD0710 13:11:18.085821430   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6310144\nD0710 13:11:18.085826111   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6325760\nD0710 13:11:18.085830770   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6341376\nD0710 13:11:18.085835733   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6356992\nD0710 13:11:18.085841878   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6372608\nD0710 13:11:18.085848400   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6388224\nD0710 13:11:18.085854740   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6403840\nD0710 13:11:18.085860541   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6419456\nD0710 13:11:18.085866715   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6435072\nD0710 13:11:18.085872403   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6450688\nD0710 13:11:18.085878804   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6466304\nD0710 13:11:18.085885399   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6481920\nD0710 13:11:18.085891831   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6497536\nD0710 13:11:18.085898043   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6513152\nD0710 13:11:18.085904339   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6528768\nD0710 13:11:18.085910597   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6544384\nD0710 13:11:18.085916940   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6560000\nD0710 13:11:18.085923191   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6575616\nD0710 13:11:18.085929457   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6591232\nD0710 13:11:18.085935784   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6606848\nD0710 13:11:18.085942258   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6622464\nD0710 13:11:18.085948607   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6638080\nD0710 13:11:18.085954876   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6653696\nD0710 13:11:18.085961022   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6669312\nD0710 13:11:18.085967536   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6684928\nD0710 13:11:18.085973719   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6700544\nD0710 13:11:18.085979966   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6716160\nD0710 13:11:18.085986187   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6731776\nD0710 13:11:18.085992530   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6747392\nD0710 13:11:18.085998620   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6763008\nD0710 13:11:18.086004959   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6778624\nD0710 13:11:18.086011258   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6794240\nD0710 13:11:18.086017628   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6809856\nD0710 13:11:18.086023810   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6825472\nD0710 13:11:18.086030248   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6841088\nD0710 13:11:18.086036373   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6856704\nD0710 13:11:18.086042742   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6872320\nD0710 13:11:18.086049180   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6887936\nD0710 13:11:18.086055527   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6903552\nD0710 13:11:18.086061822   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6919168\nD0710 13:11:18.086068007   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6934784\nD0710 13:11:18.086074268   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6950400\nD0710 13:11:18.086080634   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6966016\nD0710 13:11:18.086086880   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6981632\nD0710 13:11:18.086093099   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 6997248\nD0710 13:11:18.086099529   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7012864\nD0710 13:11:18.086105737   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7028480\nD0710 13:11:18.086111922   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7044096\nD0710 13:11:18.086118275   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7059712\nD0710 13:11:18.086124753   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7075328\nD0710 13:11:18.086130965   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7090944\nD0710 13:11:18.086137322   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7106560\nD0710 13:11:18.086143493   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7122176\nD0710 13:11:18.086149834   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7137792\nD0710 13:11:18.086156219   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7153408\nD0710 13:11:18.086162435   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7169024\nD0710 13:11:18.086168797   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7184640\nD0710 13:11:18.086175114   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7200256\nD0710 13:11:18.086181439   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7215872\nD0710 13:11:18.086187775   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7231488\nD0710 13:11:18.086193978   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7247104\nD0710 13:11:18.086200253   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7262720\nD0710 13:11:18.086206726   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7278336\nD0710 13:11:18.086213151   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7293952\nD0710 13:11:18.086219455   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7309568\nD0710 13:11:18.086225655   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7325184\nD0710 13:11:18.086231979   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7340800\nD0710 13:11:18.086238317   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7356416\nD0710 13:11:18.086244607   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7372032\nD0710 13:11:18.086250993   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7387648\nD0710 13:11:18.086257475   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7403264\nD0710 13:11:18.086263883   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7418880\nD0710 13:11:18.086270281   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7434496\nD0710 13:11:18.086276713   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7450112\nD0710 13:11:18.086283137   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7465728\nD0710 13:11:18.086289574   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7481344\nD0710 13:11:18.086295772   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7496960\nD0710 13:11:18.086302140   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7512576\nD0710 13:11:18.086308408   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7528192\nD0710 13:11:18.086314563   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7543808\nD0710 13:11:18.086320816   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7559424\nD0710 13:11:18.086326962   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7575040\nD0710 13:11:18.086333211   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7590656\nD0710 13:11:18.086339589   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7606272\nD0710 13:11:18.086346004   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7621888\nD0710 13:11:18.086352324   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7637504\nD0710 13:11:18.086358695   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7653120\nD0710 13:11:18.086365162   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7668736\nD0710 13:11:18.086371408   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7684352\nD0710 13:11:18.086378475   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7699968\nD0710 13:11:18.086384138   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7715584\nD0710 13:11:18.086390447   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7731200\nD0710 13:11:18.086395978   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7746816\nD0710 13:11:18.086402467   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7762432\nD0710 13:11:18.086408255   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7778048\nD0710 13:11:18.086413881   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7793664\nD0710 13:11:18.086420522   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7809280\nD0710 13:11:18.086426744   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7824896\nD0710 13:11:18.086433181   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7840512\nD0710 13:11:18.086439462   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7856128\nD0710 13:11:18.086445850   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7871744\nD0710 13:11:18.086452081   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7887360\nD0710 13:11:18.086458558   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7902976\nD0710 13:11:18.086464855   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7918592\nD0710 13:11:18.086471246   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7934208\nD0710 13:11:18.086477515   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7949824\nD0710 13:11:18.086483913   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7965440\nD0710 13:11:18.086490192   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7981056\nD0710 13:11:18.086496685   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 7996672\nD0710 13:11:18.086502995   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8012288\nD0710 13:11:18.086509192   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8027904\nD0710 13:11:18.086515459   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8043520\nD0710 13:11:18.086521887   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8059136\nD0710 13:11:18.086528180   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8074752\nD0710 13:11:18.086534435   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8090368\nD0710 13:11:18.086540830   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8105984\nD0710 13:11:18.086547119   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8121600\nD0710 13:11:18.086553517   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8137216\nD0710 13:11:18.086559783   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8152832\nD0710 13:11:18.086566141   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8168448\nD0710 13:11:18.086572549   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8184064\nD0710 13:11:18.086578794   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8199680\nD0710 13:11:18.086585103   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8215296\nD0710 13:11:18.086591637   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8230912\nD0710 13:11:18.086598086   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8246528\nD0710 13:11:18.086604191   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8262144\nD0710 13:11:18.086610546   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8277760\nD0710 13:11:18.086616751   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8293376\nD0710 13:11:18.086623126   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8308992\nD0710 13:11:18.086629374   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8324608\nD0710 13:11:18.086635668   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8340224\nD0710 13:11:18.086642015   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8355840\nD0710 13:11:18.086648390   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8371456\nD0710 13:11:18.086654358   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8387072\nD0710 13:11:18.086660748   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8402688\nD0710 13:11:18.086666451   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8418304\nD0710 13:11:18.086671924   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8433920\nD0710 13:11:18.086677872   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8449536\nD0710 13:11:18.086684232   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8465152\nD0710 13:11:18.086690524   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8480768\nD0710 13:11:18.086696667   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8496384\nD0710 13:11:18.086702901   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8512000\nD0710 13:11:18.086709184   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8527616\nD0710 13:11:18.086715529   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8543232\nD0710 13:11:18.086721880   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8558848\nD0710 13:11:18.086728282   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8574464\nD0710 13:11:18.086734461   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8590080\nD0710 13:11:18.086740860   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8605696\nD0710 13:11:18.086747309   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8621312\nD0710 13:11:18.086753532   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8636928\nD0710 13:11:18.086759815   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8652544\nD0710 13:11:18.086766152   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8668160\nD0710 13:11:18.086772440   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8683776\nD0710 13:11:18.086779160   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8699392\nD0710 13:11:18.086785471   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8715008\nD0710 13:11:18.086791909   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8730624\nD0710 13:11:18.086798204   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8746240\nD0710 13:11:18.086804598   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8761856\nD0710 13:11:18.086810873   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8777472\nD0710 13:11:18.086817228   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8793088\nD0710 13:11:18.086823385   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8808704\nD0710 13:11:18.086829746   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8824320\nD0710 13:11:18.086836116   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8839936\nD0710 13:11:18.086842288   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8855552\nD0710 13:11:18.086848563   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8871168\nD0710 13:11:18.086854841   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8886784\nD0710 13:11:18.086861173   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8902400\nD0710 13:11:18.086867547   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8918016\nD0710 13:11:18.086873841   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8933632\nD0710 13:11:18.086880159   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8949248\nD0710 13:11:18.086886397   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8964864\nD0710 13:11:18.086892659   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8980480\nD0710 13:11:18.086899053   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 8996096\nD0710 13:11:18.086905399   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9011712\nD0710 13:11:18.086911693   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9027328\nD0710 13:11:18.086917999   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9042944\nD0710 13:11:18.086924188   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9058560\nD0710 13:11:18.086930744   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9074176\nD0710 13:11:18.086936941   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9089792\nD0710 13:11:18.086943387   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9105408\nD0710 13:11:18.086949720   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9121024\nD0710 13:11:18.086956012   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9136640\nD0710 13:11:18.086962308   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9152256\nD0710 13:11:18.086968596   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9167872\nD0710 13:11:18.086974996   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9183488\nD0710 13:11:18.086981248   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9199104\nD0710 13:11:18.086987633   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9214720\nD0710 13:11:18.086993938   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9230336\nD0710 13:11:18.087000128   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9245952\nD0710 13:11:18.087006338   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9261568\nD0710 13:11:18.087012684   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9277184\nD0710 13:11:18.087018856   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9292800\nD0710 13:11:18.087025065   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9308416\nD0710 13:11:18.087031721   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9324032\nD0710 13:11:18.087037780   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9339648\nD0710 13:11:18.087043274   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9355264\nD0710 13:11:18.087050192   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9370880\nD0710 13:11:18.087055505   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9386496\nD0710 13:11:18.087061127   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9402112\nD0710 13:11:18.087067003   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9417728\nD0710 13:11:18.087072989   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9433344\nD0710 13:11:18.087078820   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9448960\nD0710 13:11:18.087084855   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9464576\nD0710 13:11:18.087090588   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9480192\nD0710 13:11:18.087096441   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9495808\nD0710 13:11:18.087102639   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9511424\nD0710 13:11:18.087107934   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9527040\nD0710 13:11:18.087112762   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9542656\nD0710 13:11:18.087118657   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9558272\nD0710 13:11:18.087124101   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9573888\nD0710 13:11:18.087128976   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9589504\nD0710 13:11:18.087133768   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9605120\nD0710 13:11:18.087138424   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9620736\nD0710 13:11:18.087143262   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9636352\nD0710 13:11:18.087147978   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9651968\nD0710 13:11:18.087152688   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9667584\nD0710 13:11:18.087157501   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9683200\nD0710 13:11:18.087162275   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9698816\nD0710 13:11:18.087166938   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9714432\nD0710 13:11:18.087171780   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9730048\nD0710 13:11:18.087176431   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9745664\nD0710 13:11:18.087181072   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9761280\nD0710 13:11:18.087185877   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9776896\nD0710 13:11:18.087190533   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9792512\nD0710 13:11:18.087195197   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9808128\nD0710 13:11:18.087200049   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9823744\nD0710 13:11:18.087204775   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9839360\nD0710 13:11:18.087209449   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9854976\nD0710 13:11:18.087214309   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9870592\nD0710 13:11:18.087219061   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9886208\nD0710 13:11:18.087223758   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9901824\nD0710 13:11:18.087228726   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9917440\nD0710 13:11:18.087233739   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9933056\nD0710 13:11:18.087239408   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9948672\nD0710 13:11:18.087246040   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9964288\nD0710 13:11:18.087250952   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9979904\nD0710 13:11:18.087255563   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 9995520\nD0710 13:11:18.087260393   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10011136\nD0710 13:11:18.087265136   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10026752\nD0710 13:11:18.087270291   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10042368\nD0710 13:11:18.087275151   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10057984\nD0710 13:11:18.087279869   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10073600\nD0710 13:11:18.087295792   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10089216\nD0710 13:11:18.087302258   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10104832\nD0710 13:11:18.087307087   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10120448\nD0710 13:11:18.087311849   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10136064\nD0710 13:11:18.087316763   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10151680\nD0710 13:11:18.087321522   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10167296\nD0710 13:11:18.087326218   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10182912\nD0710 13:11:18.087331525   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10198528\nD0710 13:11:18.087337987   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10214144\nD0710 13:11:18.087343139   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10229760\nD0710 13:11:18.087348128   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10245376\nD0710 13:11:18.087352876   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10260992\nD0710 13:11:18.087357869   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10276608\nD0710 13:11:18.087362726   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10292224\nD0710 13:11:18.087367453   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10307840\nD0710 13:11:18.087372152   19635 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10323456\nI0710 13:11:18.087377282   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:18.108712121   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:18.108732795   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:18.108739597   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:18.108744688   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:18.108751968   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:18.108757864   19635 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:20.276807272   19613 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7fbe72b07eb8)\nI0710 13:11:20.276828779   19613 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:20.276836157   19613 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7fbe72b0a138)\nI0710 13:11:20.276840561   19613 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:20.276848765   19613 call.c:536]                 grpc_call_destroy(c=0xa4b9750)\nD0710 13:11:20.276855021   19613 call.c:690]                 get_final_status CLI\nD0710 13:11:20.276860775   19613 call.c:693]                   1: {\"created\":\"@1499717477.861644736\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.c\",\"file_line\":939,\"grpc_message\":\"Failed parsing HTTP/2\",\"grpc_status\":14}\nD0710 13:11:20.276872278   19613 combiner.c:218]             C:0x7fbe64002720 grpc_combiner_execute c=0xa4ba6f8 cov=0 last=1\nD0710 13:11:20.276880789   19613 combiner.c:218]             C:0x7fbe64002720 grpc_combiner_execute c=0xa4e9180 cov=0 last=3\nD0710 13:11:20.276888297   19613 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:20.276893317   19613 combiner.c:308]             C:0x7fbe64002720 maybe_finish_one n=0xa4ba6f8\nD0710 13:11:20.276908154   19613 chttp2_transport.c:731]     FLOW  destroy: client CREDIT                                  STREAM[3]:incoming_window_delta  -147375 by                                        -s->incoming_window_delta   147375 giving 0\nD0710 13:11:20.276914858   19613 combiner.c:359]             C:0x7fbe64002720 finish old_state=5\nD0710 13:11:20.276920663   19613 combiner.c:289]             C:0x7fbe64002720 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:20.276925368   19613 combiner.c:308]             C:0x7fbe64002720 maybe_finish_one n=0xa4e9180\nD0710 13:11:20.276933394   19613 resource_quota.c:814]       RQ anonymous_pool_7fbe64002b60 ipv4:127.0.0.1:50051: free 15616; free_pool -> 10339072\nD0710 13:11:20.276952434   19613 combiner.c:218]             C:0x7fbe64002c30 grpc_combiner_execute c=0x7fbe64003090 cov=0 last=1\nD0710 13:11:20.276964774   19613 combiner.c:161]             C:0x7fbe64002720 really_destroy old_state=3\nD0710 13:11:20.276972221   19613 combiner.c:359]             C:0x7fbe64002720 finish old_state=2\nD0710 13:11:20.276977463   19613 combiner.c:151]             C:0x7fbe64002720 really_destroy\nD0710 13:11:20.276983791   19613 combiner.c:289]             C:0x7fbe64002c30 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=0 elems=0)->0 exec_ctx_ready_to_finish=1 time_to_execute_final_list=0\nD0710 13:11:20.276989757   19613 combiner.c:308]             C:0x7fbe64002c30 maybe_finish_one n=0x7fbe64003090\nD0710 13:11:20.276994983   19613 combiner.c:407]             C:0x7fbe64002c30 grpc_combiner_execute_finally c=0x7fbe64002b98; ac=0x7fbe64002c30; cov=1\nD0710 13:11:20.277000845   19613 combiner.c:359]             C:0x7fbe64002c30 finish old_state=5\nD0710 13:11:20.277007267   19613 combiner.c:289]             C:0x7fbe64002c30 grpc_combiner_continue_exec_ctx workqueue=(nil) is_covered_by_poller=(final=1 elems=0)->1 exec_ctx_ready_to_finish=1 time_to_execute_final_list=1\nD0710 13:11:20.277013489   19613 combiner.c:340]             C:0x7fbe64002c30 execute_final[0] c=0x7fbe64002b98\nD0710 13:11:20.277019615   19613 combiner.c:161]             C:0x7fbe64002c30 really_destroy old_state=3\nD0710 13:11:20.277023553   19613 combiner.c:359]             C:0x7fbe64002c30 finish old_state=2\nD0710 13:11:20.277028644   19613 combiner.c:151]             C:0x7fbe64002c30 really_destroy\nI0710 13:11:20.277032897   19613 init.c:233]                 grpc_shutdown(void)\nI0710 13:11:20.397475113   19613 metadata_array.c:47]        grpc_metadata_array_destroy(array=0x7fbea04e4e58)\n```. I was just able to reproduce this by running my test script twice in separate processes in parallel on a single local grpc golang server with just the ping patch you provided.\n*(Running it twice may not be related to the underlying issue...). @MakMukhi I haven't made a standalone test script yet, will post here when I have one.\n@ncteisen I've found the string representation of that option defined as \"grpc.http2.bdp_probe\", so I'm passing that hardcoded string in to the channel creation, which hopefully gets passed down correctly. Anyway I can confirm on my end that that's what's happening from the logs?\n. Disabling the bdp_probe on the client side seems to have resolved the issues.\nI was still receiving errors with just the new patch (without the one ping one you sent me earlier). Will try again with the ping one attached.\nI will try disabling the bdp_probe in production, and see if we can still reproduce.. @ncteisen I have tried with bdp enabled + @MakMukhi most recent patch, but without the ping patch. That setup was able to reproduce the HTTP/2 Parsing error - so I am retrying with bdp enabled + both patches.\nWill let you know what I find.. Quick update - wasn't able to reproduce the problem locally with both of @MakMukhi's patches, so I pushed it out to production. It's looking much better, but still happening at a very low rate (once every 10 minutes or so vs many times per second). \nI'm also going to push out the disable bdp_probe patch on the client side, and see if that helps.. Disabling bdp seems to have done the trick in production.\nStill an http/2 parsing error with bdp probe enabled, and both patches. But it happens very rarely, and since I can't enable DEBUG logging in production, it'll be somewhat difficult to pull out the more detailed exception.\nI'll give reproducing locally another shot tomorrow, and see If i can pull out the exception with DEBUG logging.. @MakMukhi @ncteisen Local and production are identical, only difference is hardware and scale. I am able to reproduce locally tho, got a couple of errors here. This was several runs done in parallel, so there may be interleaving:\nD0712 11:37:22.119232355   17094 connectivity_state.c:185]   SET: 0xbff00f8 client_channel: SHUTDOWN --> SHUTDOWN [resolver_gone] error=0xc3f38d0 {\"created\":\"@1499884642.119220727\",\"description\":\"Got config after disconnection\",\"file\":\"src/core/ext/filters/client_channel/client_channel.c\",\"file_line\":516,\"referenced_errors\":[{\"created\":\"@1499884642.119168948\",\"description\":\"Resolver Shutdown\",\"file\":\"src/core/ext/filters/client_channel/resolver/dns/native/dns_resolver.c\",\"file_line\":119},{\"created\":\"@1499884642.119219522\",\"description\":\"No load balancing policy\",\"file\":\"src/core/ext/filters/client_channel/client_channel.c\",\"file_line\":366}]}\nD0712 11:37:22.119353452   17094 connectivity_state.c:185]   SET: 0x7fa50c0042f8 client_transport: READY --> SHUTDOWN [close_transport] error=0xc3f3ac0 {\"created\":\"@1499884642.119345913\",\"description\":\"Transport destroyed\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":566,\"grpc_status\":14,\"occurred_during_write\":0}\nD0712 11:37:22.119447545   17094 tcp_posix.c:227]            read: error={\"created\":\"@1499884642.119381617\",\"description\":\"FD shutdown\",\"file\":\"src/core/lib/iomgr/ev_poll_posix.c\",\"file_line\":456,\"referenced_errors\":[{\"created\":\"@1499884642.119345913\",\"description\":\"Transport destroyed\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":566,\"grpc_status\":14,\"occurred_during_write\":0}]}\n11:37:22.765071498   17001 call.c:693]                   1: {\"created\":\"@1499884641.781960769\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.c\",\"file_line\":939,\"grpc_message\":\"Failed parsing HTTP/2\",\"grpc_status\":14}\nD0712 11:37:22.104509073   17193 connectivity_state.c:185]   SET: 0xc3f3d08 client_channel: READY --> TRANSIENT_FAILURE [lb_changed] error=0x7fa510022e60 {\"created\":\"@1499884642.104032074\",\"description\":\"Failed parsing HTTP/2\",\"file\":\"src/core/ext/transport/chttp2/transport/chttp2_transport.c\",\"file_line\":2166,\"grpc_status\":14,\"referenced_errors\":[{\"created\":\"@1499884642.104026598\",\"description\":\"frame of size 4683 overflows incoming window of -26083\",\"file\":\"src/core/ext/transport/chttp2/transport/parsing.c\",\"file_line\":413}]}\n. Haven't been able to reproduce locally after the first instance today (on both versions), so just need to try some more...\nI'll be out until Monday so hopefully I'll have more news for you guys by then. Will do some additional testing today to pull out the more detailed exception. D0724 10:38:51.817118206    4774 chttp2_transport.c:2094]    ipv4:127.0.0.1:50051: update initial window size to 138765\nD0724 10:38:52.443525404    4779 chttp2_transport.c:2094]    ipv4:127.0.0.1:50051: update initial window size to 142244\nD0724 10:38:52.608531554    4769 chttp2_transport.c:2094]    ipv4:127.0.0.1:50051: update initial window size to 136125\nD0724 10:38:53.151636379    4779 chttp2_transport.c:2094]    ipv4:127.0.0.1:50051: update initial window size to 138872\nD0724 10:38:53.187807688    4769 chttp2_transport.c:2094]    ipv4:127.0.0.1:50051: update initial window size to 133991\nD0724 10:38:53.188149151    4769 bdp_estimator.c:81]         bdp[ipv4:127.0.0.1:50051]:start acc=71808690 est=65536\nD0724 10:38:53.188162595    4769 bdp_estimator.c:91]         bdp[ipv4:127.0.0.1:50051]:complete acc=0 est=65536\nD0724 10:38:53.188181632    4769 call.c:690]                 get_final_status CLI\nD0724 10:38:53.188203841    4769 call.c:693]                   1: {\"created\":\"@1500917933.188179227\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.c\",\"file_line\":939,\"grpc_message\":\"Failed parsing HTTP/2\",\"grpc_status\":14}\n. What GRPC_TRACE environment variables should i set? GRPC_TRACE=all is too verbose. I ran that with GRPC_TRACE=bdp_estimator,call_error. tmp.txt\nNot sure if that contains all the log output (not sure what I'm looking for). I had multiple processes running the same script, so output may be interleaved and not complete. I'm re running now all in separate terminals to get a cleaner output. Here is the full log of a single process: \ntmp.txt\nGo log shows nothing interesting, but no environment variables are set to anything. Will revert back tomorrow with more logs\nGo server logs show nothing but client disconnecting after the script terminates. I'm now on master for go-grpc, but adding the environment variables:\nGRPC_GO_LOG_VERBOSITY_LEVEL=2 and \nGRPC_GO_LOG_SEVERITY_LEVEL=INFO\nDon't seem to print any logs.... I get logs when running that command. Only difference is i'm using glide to vendor the dependencies when I run my main go grpc server. Could that be causing this?. Sorry - meant not gettings logs when I run my go grpc server with those same environment settings.\n. Everything in the server is default. Again, can't get logging to work so don't see anything on that end.... It seems that applying #1367 (at least the ones I could) cause immediate failure of parsing the HTTP/2 stream. . Still happening, no additional logs on the python side. To move forward I think we're going to need to get logging working on the grpc server side - any suggestions for that?. ",
    "sashabaranov": "@flisky wow, thanks! But is it possible to do such thing only for some server endpoints?. ",
    "nghialv": "@googlebot I signed it!. @menghanl Could you take a look at this and #1354?. Thanks for your review.\nYou're right. Removing the old update before sending is a great idea.. @googlebot I signed it!. Hi\nThanks for your review.\nYes, I think we should keep cc.conns consistent with the addresses of balancer.\nAnd rather than recreating a new addrConn, it is better to only reset the underlying transport connection. However, we need to carefully consider the impact from that change. (Maybe that change should be done in the future)\nAnd in the scope of this PR, if we ensure that the tearDownError is always non-nil while replacing the addrConn, we should use stale and that error to determine whether or not the new addrConn should be added.. Hi @menghanl \nHow about this PR?\nAnd can you tell me the date of the next release?\n(We want to apply bug fixes of #1353, #1354 into our code). I tried with the code at master and it worked fine.\nAbout the test in this PR, I think it is not necessary after merging #1369 because we made sure that resetAddrConn is only called when a new address is added. (maybe the name of resetAddrConn method should be changed)\nSo you can close this PR without merging.\nI'm hoping for a minor release (1.5.1) in this week.\nThanks @menghanl . @menghanl Can you tell me the date of the next release (1.5.1)?. Great! Thank you.. ",
    "dzbarsky": "https://github.com/grpc/grpc-go/pull/1361 would solve my problem. Let's say you do a blocking dial with a timeout and you connect to a server. By the time you do an RPC it may fail if the server has already gone down. So you must handle this case whether or not the dial succeeds.\nIf the initial dial times out, you get back an error and a nil ClientConn, so you have to handle this error as well. If we instead returned a ClientConn that failed at RPC time, that is fewer errors for client code to handle.\nAlso, imagine a situation where you create a ClientConn at process startup time, but you don't use it yet and the server you are connecting to is not up yet. At some point later, the server comes up, and you try to issue an RPC to it. It would be nice if this \"just worked\".\nWe are migrating a large set of services (100+) to gRPC, and our in-house RPC system had the behavior I am pushing for here. In particular, all call sites assume that client creation does not fail if you pass in valid arguments, and RPCs may fail if you are not connected (which is the case no matter what). At the same time, we do block for a short period of time, so if you create a client to a service that is currently available, you can immediately issue an RPC and it will work. We believe this is a better API since it makes client code simpler and makes it easier to write non-flaky tests. In order to make the migration easier (aka avoid breaking all the clients), we want to implement a \"soft block\" which would be a non-blocking dial that waits in our library for some timeout, but returns early if grpc ClientConn is created.\nIn general, ConnectivityState is what we want (as I mentioned in the issue title), but the PR is a quick bandaid that lets us get the right behavior. We are currently blocked on deploying grpc because of this issue, if ConnectivityState API is not coming soon, it would be nice to take the linked PR or something similar in the meanwhile. . No, we do not want to set FailFast(false) on our RPCs because that causes cascading slowdown failures when upstream services are down. We really do want to block on the client creation up to a specified timeout, but want to fail fast on each RPC.. We've already tried the approach you suggested, however it doesn't work because grpc assumes ownership of the Balancer DialOpt (i.e. calls Close on it when done). We also tried doing a wrapper class around the Balancer that would block the Close, but that still doesn't work because grpc will call Start twice, which leads to data races.\nWe need to support our services calling into our wrapper framework with custom balancers to support special resolving logic (like cross-data center request failover, etc.)\nI understand your hesitation to change this, if you have an estimate for when you can expose what we actually want here (ConnectivityState) that can help our decision whether to fork in the meantime to unblock ourselves.. I signed it!. @dfawley thanks for the warning, we're already using this since we were blocked on it for a while. It looks like this just moved it to a separate package so that shouldn't be a problem for us. Please let us know if there are any actual breaking changes, thanks!. OK, https://github.com/grpc/grpc-go/pull/1434. ok updated, i mostly did what you suggested but kept a unified StatsHandler in transport to avoid making more changes, what do you think?. Will run gofmt before checking in, but I want this to compile/run tests first, not sure the transport changes are right. Unfortunately I don't currently have time to work on this one, if you want to land it feel free to pick it up. Otherwise I can try to find someone else at Dropbox to finish it.. We have our own stream wrappers now that do proper deadline checking, I'm not sure if this is still an issue in vanilla grpc or not. https://github.com/grpc/grpc-go/pull/1434 would let you have more than one stats handler :) I've been having some trouble finding time to finish it off, maybe next week (or you can take it over if you want). @dfawley can you please take a look? thanks!. Yep, yyour fix makes sense. Thank you.. We hit the same thing, would love to avoid copy-pasting the code this time around especially given that these APIs are marked experimental and keep changing out from under us.. great, that works for me. thanks!. great, that works for me. thanks!. Unfortunately I can't provide the tests, but the gist of it is the tests bring up a bunch of services, health check them (i.e. send an rpc and get response). tear them down, and bring them up again (to test idempotency).\nI doubt we were relying on client staying connected to an unresponsive server indefinitely given that the tests are timing out with this change and completing in time before.\nAlso this is an intermittent failure that occurs in CI at scale so I haven't spent much time trying to reproduce locally or figure out where exactly they are stuck, since that could take quite a while. I was hoping you folks would have some clues :)\nIf you have some logging variables I can set I'm happy to run this through our CI again and provide relevant output for failing tests.. Ah, that's promising! what's happening is our \"service controller\" process creates a health checking client for each service that is brought up, previously the client would continue connecting indefinitely until server came up. So now with this change it will fail permanently after 20 seconds or after some number of retries? \nIf the above is correct, is there a way to disable this behavior?. I tried using grpc.WithWaitForHandshake() but it did not help (also it is an experimental API so I'd like to avoid using it, we've been burned by these grpc experimental APIs before).  I checked the code and we actually do 3 retries on the healthcheck rpc, so i doubt grpc.WaitForReady() will help. Is it possible that there are unintentional impactful changes? . Well, we've discovered several times that our test coverage finds things that slip through your test coverage, so I don't necessarily buy the correctness of the change. I probably won't have time to debug this in detail in the foreseeable future, which is why I asked for some logging (so I can easily show you exactly which calls we are making to grpc). We might end up just hacking around this in the interest of time, which means your next release is potentially breaking your consumers.. will check if this is fixed by https://github.com/grpc/grpc-go/commit/e975017b473bd9b2a3d5b23428a8549fe20b1153. Ok I've got tests passing with some hacks around our own backoff policy. Our distribution for time spent inside backoff/sleep got worse though, especially at high percentiles which hurts even lower percentiles due to test sharding in CI. is there any way to tune the grpc backoff strategy?. We had already worked around it on our end before your patch landed.. OK, let me try to come up with a repro. We use a custom resolver but even with that, all my attempts to reduce the problem led to the same error you posted (which is what we want). Since this was only affecting one of our services internally out of hundreds I suspect there is something buggy in it. In any case the service was not setting timeouts properly and once that was fixed the context was no longer being cancelled due to shorter timeout. Going to close this out since it's not actionable for you, thanks for following up. LGTM, thanks!. nit: balancer is typo'ed. nit: begining, conneciton. it can only notify 1 goroutine, right?. nit: connection. nit: maybe buildResolverUserOptions? or userOptionsForBuildResolver?. ",
    "danjacques": "It seems prudent to update the default implementation to conform to this then, right? ATM it does not call os.Exit.. This is obviously not that big of a deal. The concern that I have is that the interface explicitly states:\ngo\n// This function should call os.Exit() with a non-zero exit code.\n... and yet the canonical implementation of that interface doesn't do this. IMO, an appropriate solution is to update the interface comment to say something like, This function may call os.Exit() to terminate. If it doesn't, os.Exit will be called after this function returns. This makes it clear that the function is permitted to, but not required to, terminate the application.. I think the comments in grpclog/grpclog.go helpfully document why there is an os.Exit as a failsafe.\nHowever, I think the comments in grpclog/loggerv2.go don't resolve the underlying concern, which is that grpclog.loggert does not conform to the grpclog.LoggerV2 interface's soft (comment) API. If the safety net is gRPC defending itself from improper LoggerV2 implementations, then grpclog.loggert is improper and should be fixed. Otherwise, if it is permitted for a LoggerV2 to rely on gRPC's failsafe, the interface requirement should be softened from should to may.\nAnyways, this is pretty trivial, and the comments in the patch do clarify that os.Exit is omitted as an optimization. Thanks for spending time on this.\nFor posterity, I encountered this problem b/c I have a custom Logger implementation that was written with the expectation that os.Exit was the full responsibility of the Logger. I relied on this behavior in a unit test, where I disabled the Exit call in my custom Logger so that I could assert that Exit was called without actually exiting my test. This started failing with the LoggerV2 patch because of the introduction of the higher-level failsafe os.Exit, and when I dove into it, it was not clear to me whether my assumption was valid or not. As I mentioned above, I still don't think the responsibility is clear even with the additional comments, but they do shed some light on the design decision involved :). This is more what I had in mind, and matches my expectations. Thanks!. ",
    "hnakamur": "Thanks for your comment.\nNo, I didn't see any error on the client side.\nAnd I used the commit of https://github.com/grpc/grpc-go/pull/1340 and confirmed the server prints no error.\nI'm closing this issue now.\nThanks!. ",
    "brotherlogic": "Not sure, but this change appears to have broken compilation on ARM compilers. Running a simple binary in a raspberry pi results in this:\n```\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x4 pc=0x1265c]\ngoroutine 36 [running]:\nsync/atomic.addUint64(0x1087c514, 0x1, 0x0, 0x0, 0x0)\n    /usr/local/go/src/sync/atomic/64bit_arm.go:31 +0x4c\ngoogle.golang.org/grpc/transport.(http2Server).applySettings(0x1087c460, 0x108737c0, 0x3, 0x4)\n    /home/simon/code/src/google.golang.org/grpc/transport/http2_server.go:938 +0x130\ngoogle.golang.org/grpc/transport.(http2Server).controller(0x1087c460)\n    /home/simon/code/src/google.golang.org/grpc/transport/http2_server.go:1044 +0x1b8\ncreated by google.golang.org/grpc/transport.newHTTP2Server\n    /home/simon/code/src/google.golang.org/grpc/transport/http2_server.go:221 +0x860\n```. I'm not sure it's strictly 32 bit - I found that this: https://patch-diff.githubusercontent.com/raw/grpc/grpc-go/pull/1410.patch fixes the problem. The change you suggested above did not work.. I do not pretend I understand why - it's something related to https://github.com/golang/go/issues/599; I robbed the fix from https://github.com/fsouza/go-dockerclient/pull/522/commits/7a6bd151da14159761c6d4e368e2b2a4849c2b66. Ah didn't see the second fix - that did the trick also.. Awesome thanks!. ",
    "moypray": "I signed it!. @menghanl  I found that issue in docker-containerd project link:\nhttps://github.com/containerd/containerd/tree/v0.2.x\nIn containerd project, We start health check grpc server first (to make sure the health check should be first served), And then load data from disk, after all the data done, we register the grpc real service of containerd. (loading data from disk maybe cost lots of time, and docker will do health check after containerd process starts up, if lost health check for 3 times , docker will kill containerd and restarted it. That's why we need start health check first.)\nYeap, this is not a standard initialization process, but it should not be panic.\nAnd more, Maybe containerd used the old version of grpc, it does not have the #828. And it is not crashed in grpclog.Fatalf(\"grpc: Server.RegisterService xxxx) function. But in grpc/server.go:776+0x604 fp=0xc4206a3760,\nhttps://github.com/containerd/containerd/blob/v0.2.x/vendor/google.golang.org/grpc/server.go#L776\nThis is caused by concurrent read/write map.  I think this is a golang issue. we should protect the map.\nBTW, I see #828, new grpc code prohibit users registering new service after grpc server starts. Is there any issue about this?? Why not use a RWMutex to protect the map as I fixed, it won't have efficiency problem.. > Correct me if I'm wrong, I think concurrent read from a map doesn't cause panic.\nYes, concurrent read is not a problem, but concurrent read and write will cause panic. \nThis is a golang issue. I have do some test, you could run this panic within 10 seconds:\n```\nfatal error: concurrent map iteration and map write\ngoroutine 5 [running]:\nruntime.throw(0x4ab248, 0x26)\n        /usr/local/go/src/runtime/panic.go:596 +0x95 fp=0xc420041e38 sp=0xc420041e18\nruntime.mapiternext(0xc420041f68)\n        /usr/local/go/src/runtime/hashmap.go:737 +0x7ee fp=0xc420041ee8 sp=0xc420041e38\nmain.main.func1(0xc420010120)\n        /root/test/map/a.go:12 +0x19c fp=0xc420041fd8 sp=0xc420041ee8\nruntime.goexit()\n        /usr/local/go/src/runtime/asm_amd64.s:2197 +0x1 fp=0xc420041fe0 sp=0xc420041fd8\ncreated by main.main\n        /root/test/map/a.go:17 +0x73\ngoroutine 1 [sleep]:\ntime.Sleep(0x174876e800)\n        /usr/local/go/src/runtime/time.go:59 +0xf9\nmain.main()\n        /root/test/map/a.go:27 +0xa8\ngoroutine 6 [runnable]:\ntime.Sleep(0x3b9aca00)\n        /usr/local/go/src/runtime/time.go:57 +0xb8\nmain.main.func2(0xc420010120)\n        /root/test/map/a.go:23 +0x6a\ncreated by main.main\n        /root/test/map/a.go:25 +0x95\nexit status 2\nroot@localhost:~/test/map# cat a.go\npackage main\nimport (\n        \"fmt\"\n        \"time\"\n)\nfunc main() {\n        mp := make(map[int]int)\n        go func() {\n                for {\n                        for k, v := range mp {\n                                fmt.Println(\"k=%d,v=%d\", k, v)\n                                time.Sleep(0 * time.Second)\n                        }\n                }\n        }()\n        go func() {\n                i := 0\n                for {\n                        i++\n                        mp[i] = i\n                        time.Sleep(1 * time.Second)\n                }\n        }()\n    time.Sleep(100 * time.Second)\n\n}\nroot@localhost:~/test/map#\n```. @hqhq yeap, But I am wondering if there is any other issue about registering services after grpc.Serve?  Why not using a RWLock to protect the map? From #828 I don't  get the answer. . @menghanl OK\uff0c thanks~~ l will close this PR now.. ",
    "keloyang": "I'm not sure if that's a problem, if you call all your register before starting the service ,I think there is no need to add a lock to handleStream.\n. ",
    "hqhq": "@moypray The point is, if the scenario you got this panic is caused by registering grpc service after grpc server started, and it's already cached by https://github.com/grpc/grpc-go/pull/828 , and the containerd code which triggered this issue didn't include that code, I think it's not a grpc issue now. Before #828 it is an issue, but after that, grpc explicitly consider that scenario as invalid, maybe we should update grpc vendor and fix that invalid grpc use case.. @menghanl Thanks for clarifying, I think we can close this PR now.. ",
    "gjnoonan": "Can this issue be closed now? I believe it was fixed in #1485 . ",
    "topecongiro": "I singed CLA.. ",
    "iordaniordanov": "It seems that the problem was introduced in This commit. Thanks. Any help with that?\n```\ngo get -u golang.org/x/net/http2\ncd /home/jordan/goprojects/src/golang.org/x/text; git pull --ff-only\nYou are not currently on a branch. Please specify which\nbranch you want to merge with. See git-pull(1) for details.\ngit pull <remote> <branch>\n\npackage golang.org/x/text/secure/bidirule: exit status 1\n```. Ahh, thanks a lot problem is fixed. Closing the issue :). ",
    "gitsen": "@MakMukhi The new server is definitely up and listening. We start the new server, see it bind to port and then gracefully stop the old server.. Thanks for the update @MakMukhi . @MakMukhi when do you envision this will be released?. Thanks! Will wait for that.. Thanks @MakMukhi ! Please drop us a note when you release this.. Thanks @menghanl we will update and report our findings here. ",
    "trusch": "Just for reference,I tested this for my usecase (see #1694) and I noticed two things:\n\nThe query interval of the DNS resolver is not configurable (30min)\nDead connections are also retried 30mins before getting removed. DNS in k8s only returns the Virtual IP of the service and not all IP's of the replicas. This will not work when reusing a single client conn. \n\nI saw a k8s resolver implementation which resolves natively via the k8s api.. @ejona86 yes, I got a setup with gateways and workers and would like to have one ClientConn to the workers which loadbalances on a per call basis. I think using headless services like @kop suggested and the new DNS resolver (in combination with MAX_CONN_AGE) will do the job for me. . Thanks for that hint @menghanl ! This seems to work now, but unfortunately it doesn't seem to query the DNS server that often. I saw the requery frequency in another package set to 30 minutes. Is this configurable?\nedit:\nIt was not another package, It is the requery frequency of the dns resolver and it doesnt seem to be configurable. I think it would be usefull to make this configurable. I know that frequent polling is generally a bad idea, but i'm currently in the comfortable position of building a complete stateless system and I would not like to introduce something big like etcd or zookeeper into my stack, just for loadbalancing.\nPerhaps a WithResolveNowInterval(time.Duration) option and a independent goroutine in the ClientConn which calls ResolveNow() in a loop when this optin is set?\nI could write that If it would help. I think it should be a very good and small task to get started ;)\nPlease let me know if I can help @menghanl . I tried the MAX_CONNECTION_AGE plus #1679 approach but It doesn't trigger the re-resolving. I dont know if perhaps the MAX_CONNECTION_AGE parameter in the server keep alive parameters of the workers is not respected, or if the resolver is not invoked when the connection closes normally (without error). I could imagine that this happens. I do not even see SubConn state changes in the logs.\nWhen killing one of the worker pods everything works fine and the resolver returns the new address set.. I tried today, but can not reproduce the issue anymore! . ",
    "ilyakamens": "I've signed it!. ",
    "chensunny": "through Flame Graph , \nCPU was consumed on the function 'metadata.Join' and 'metadata.New'. ",
    "eekwong": "Can someone check if this is related to this change?\ngithub.com/grpc-ecosystem/grpc-gateway/examples/server/a_bit_of_everything.go:137: undefined: metadata.FromContext. ",
    "redbaron": "This breaks API, why there were no major version bump?. ",
    "waytai": "go run greeter_server/main.go\ngo run greeter_client/main.go \n2017/07/28 10:35:38 Greeting: Hello world\nthis example is ok. and I have change to grpc-go-1.4.x\ngo run server/server.go\ngo run client/client.go\nand it's ok. thank you. ",
    "snktagarwal": "IIUC you're suggesting that RPC tear down may be happening properly; we're not tearing down the backbone (i.e. the connection sockets) on which these RPCs are running?\nIs this something that the server should do or client should initiate. We have a bidirectional RPC.. Thanks for the detailed explanation.\nTwo comments:\n1. We see one more such goroutine (on server) for each RPC that is being called. The number keeps getting higher w\n2. Can you expand on \"Users decide to close the ClientConn\". If this a teardown the client needs to initiate? When does this happen?. Yeah, I manage to get these logs on client side. But interestingly I don't see any server gRPC logs when the RPC is in action. I am running the server as:\nGRPC_GO_LOG_SEVERITY_LEVEL=info go run examples/helloworld/greeter_server/main.go. Is there a DEBUG level?\nTo give more motivation, we're trying to figure out why sometimes we see backup (or so it seems) in our gRPC calls. Basically we have NodeJS <-> GoLang gRPC and we see that sometimes the client is not able to push data to the RPC. My understanding is that could happen if the server blocks or other possible issues on NodeJS side itself.\nTo get more information I am trying to get more debugging information printed. Do you have suggestion on how I should about it?. There's two concerns with the above design that you might be able to answer:\n1. My streaming server does a few things. It is a streaming audio server, so it (a) does storage and (b) does ASR transcription. Storage is the higher priority task. If (b) fails I still want to store all the incoming audio and notify the client that an error has occurred. I thought GRPC errors was the way to go but it seems like building this as application error is a better choice.\n\nRecv() is a blocking call which makes it harder for me to do select {Recv(), ContextCancellation(), ...}. For this purposes I put Recv() is a go routine as such:\n```\n// Converts the recv() call into a channel so that we can use select on various inputs\n// TODO: This leaks today, figure out the right behavior here\nfunc (s *VoiceAgentServer) grpcRecvToChan(\n    ctx context.Context,\n    dialog voice.MsVoiceAgent_DialogServer,\n    ch chan interface{}) {\n    for {\n        req, err := dialog.Recv()\n        ch <- &reqWrapper{req: req, err: err}// Exit on non-nil error\nif err != nil {\n    return\n}\n\n}\n}\n```\n\n\nThis leaks if I return error from the main GRPC thread. Any suggestions how to not make this goroutine leak? One idea is to make the main GRPC thread never return an error and hence Recv() shall always finish (assuming we make sure GRPC thread always returns)?. ",
    "breznik": "Does the same apply to the NodeJS gRPC client in terms of a pool of underlying transport clients that are handling the individual rpc calls? Is there a best practice on how to terminate a bidirectional stream between a nodejs gRPC client and a golang gRPC server?. @ekrengel I spent some time unsuccessfully debugging further and haven't had time to take a 2nd look at it, although I still see the same issue, just, not sure how impactful it really is.. Hi,\nI'm using glide (for now) as my dependency manager for go. When I look in my vendor/github.com/grpc/grpc-go/ directory, I do indeed see the internal directory.\nI tried this again on v1.16.0,  using the following code:\n```go\nimport (\n    \"google.golang.org/grpc/credentials/google\"\n)\nvar test = google.NewDefaultCredentials()\n```\nThis compiles fine, and chose this as the google.go credentials file directly imports the internal pkg.\nI then tried to modify it (still on v1.16.0) to:\n```go\nimport (\n    \"github.com/grpc/grpc-go/health\"\n    \"google.golang.org/grpc/credentials/google\"\n)\nvar test = google.NewDefaultCredentials()\nvar healthSvr = health.NewServer()\n```\nAnd I once again get the same build error as initially reported.. Ah~ha! My import statement was wrong - that was indeed the issue and works on my side now. Thanks for noticing that!. ",
    "geoffgarside": "The yourMux is just another http.Handler, so whatever would handle your non grpc requests. You might create it from http.NewServeMux() or any of the various http router packages that are available.\nYou'd probably have this snippet inside some kind of request director ServeHTTP() or http.HandlerFunc middleware.. ",
    "xinzhangx": "I signed it!. ",
    "grantr": "Thanks @menghani! This now compiles:\ngo\nif sts, ok := status.FromError(event.Error); ok {\n  //do stuff with sts\n}. ",
    "LK4D4": "@dfawley I think I've fixed problem. Added a comment to clarify what's going on.\nI think grpclb has the same problem, maybe material for another PR.\nThanks!. @dfawley Notify can be called outside of grpc(that's what happened in our code). . @dfawley Okay, I got it. I don't know why we did it, but we're not gonna do it again. Thanks!. I find this a little confusing. timer.C isn't used after time.Stop at all, so there is not point in\nif !timer.Stop() {\n    <-timer.C\n}\nidiom because there is no chance in the occasional spawn of the channel.\nI will do this change though if you still think that it's better, though.\nThanks!. ",
    "ZhouyihaiDing": "Update end2end_test.go to make it pass travis. Can you help me review it again? \nThank you very much!. Ready for review. Thanks!. Sorry. Didn't notice there was new conflict. Solved them.. Sorry this file doesn't connected to any files.\nIt was created when my commit and commit --amend doesn't update pull request, it needs .please-update to force pull request update new commit, but I forgot to add it into gitignore.\nhttps://github.com/grpc/grpc-go/pull/1800. git commit --amend + git push origin branch -f doesn't work neither.\nIt was really strange.. When (stat.max-stat.min = 1). There is a parameter oneOverLogOnePlusGrowthFactor: 1 / math.Log(1+opts.GrowthFactor), which mean GrowthFactor can't be 0. I can also add a 0.001 to oneOverLogOnePlusGrowthFactor but it is the same.\nThe reason it happens is because the latency is divided by a int64 time unit. For example,  If adding 40ms latency to the network, it may take 80.xxx ms for a round trip, so the final result only contains 80ms(stat.min) and 81ms(stat.max).. Forgot to delete when I was testing. Change it to k seems better.. Initiate it each time as \"0\" at the beginning of resetTransport. It also avoids the race when other thread want to use conn[current address] to find current addrconn.\nAnd also set to back to \"0\" when calling ac.teardown(). Oh this usage seems really cool!. Oh, I'll change that. In my first implementation, I thought the word was findFirst, so I used ff. \nAnd also I forgot to update the comment because I was using metadata type(findFirstMD) to decide which balancer in that version.. Returns won't change cause it has to be unlock before return anyway. But add some lock and unlock around continue and 'for' loop seems make the code hard to read and be modified next time.\nSeems the second for loop (iterate each address) can create a single function like resetClientTransport([]Address) ?. Done. Didn't know go can use nil as key.. Don't we need to check ac.ctx.Done()? Cause eg. there are 6 address and the ac.ctx.Done() return at the 2nd loop, the rest 4 loops don't need to continue?. You are right. I thought buf = b will assign new alloc for buf, but it turns out it doesn't.. If the size can be larger than 4GB, we need to add one more byte (or 2 to 3 bits?) in the header for every message sent to save the length of the message. It's that worth? . Done. Done. Done. Done. Done. Done. Done. Assert both header and data.. Done. Done. Done. Done. Done. Done. Done. Done. Done. Done. Done. Done. Done. done. done. done. Done.. Done. gzip.New() returns an encoding.Compressor. gzip.compressor has 2 pools like the old compressor and decompressor.. Not Done. checkRecvPayload() in the recv() won't ignore it and returns \"compressor not implement\" error. \nIn the encode() part to do the same thing before encode.\nDo you mean check it and return the error when calling UseCompressor? So it can return the error as early as possible?. Done.. Done. But change it to \"gzip_register\" cause \"gzip\" will fail some tests in end2end_test.. Done. Define a gzipReader wrapper around gzip.Reader, and defer pool.put() after read EOF.. Done. Define a gzipWriter wrapper around gzip.Writer, and defer pool.put() after Close().. Not Done. I only removed part from client. Server will receive a string of compressor name and it may not be set by UseCompressor.\nAlso, for the end2end_test.go/TestCompressServerHasNoSupport, if the server register \"gzip\" during init(), this test won't pass. Do I need to copy gzip compressor and name it \"gzip_tmp\" for this test?. Done. Done. Done. That's cool!. Done. Done. Done.. Done.. Done. Change it to if.. In the runStream, it need the position to indicate which steam is using. \nSince runUnary and runStream shares the same function runBenchmark, runUnary set a postion without using it.. Done. I do the same with runCompressionMode. false by default, true if set.. Done. I don't know which name is better, so I change it from IntSlice to Input.\nReady for review.. Done.. Done. Transport.Write will append data to the header slice.. Not Done. I need some time to check the overhead if using slice directly.. Done.. Done.. Done. I move this part to stats.go because later fmt.go can use it to format output.. Done. But in a simple way. Network can be set with one of those values and it overwrites latency/kbps/mtu slices.. Done.. Done.. Done. I move this part to the benchresult/main.go because only printing table needs it. The benchmain output the raw data.. I noticed that when maxConcurrentCalls > 1, the average latency get by runtime.ReadMemStats varies a lot from the one get by elapse. Maybe it doesn't notice some call are called by goroutine. I'll replace this value with the latency calculated by the elapse.. Done.. Done.. Done.. Done.. Done.. Done.. Done.. Done. . Done.. Done. Start at 0.. Done.. Done.. Done.. Done.. Done.. Done.. I have to add a return either here of inside \"default\". Otherwise it has a \"no return\" error.. Done. My old version is read those data as string, factor is used when data has different unit.. Done.. Done.. Done. I am not sure how to describe run twice, but to update how to compare inside benchresult/main.go.. Done.. Done. . Done.. Done.. Done. printline receives interface and all printed as %v. . Done.. Done.. Done. I was trying to do what runtime.MemProfileRate does, and pasting it's comment to here.\nNow I change it: only when memProfile is set, runtime.MemProfileRate will read profile rate from it. . I'm not sure I understand. I update it as: c.decomperssorPool.put(newZ) if there is no err.. You mean change the var name from cpWriter to compressor in encode() as well?. Done. Done. deleted.. Done. deleted. Done. deleted.. Done.. Done.. Done.. Done.. Done. I understand now. It doesn't show me the review for the encode part in the \"conversation\" page. I saw them until I click \"file changed\".. Done.. Not done. \n\"cbuf = new(bytes.Buffer)\" inside the check if the compressor exists. I think it is the same pattern as the old way is? Alloc when the compressor exists.. Done. Only add comment for this function. The other two are \"This package is experimental\" for encoding.go and gzip.go.. Done. Removed it.. ",
    "SuhasAnand": "@MakMukhi : Thanks for your reply and sorry for not being clear:\nHow about if I have a gRPC server which has a two (or more) rpc called SendABC() and SendXYZ() both of which are server side stream. From what I understand MaxConcurrentStreams (lets say for this example I set it to 2)  will not restrict client to have single invocation of SendABC() and SendXYZ() right ? since MaxConcurrentStreams is 2 in this example there could be two streams of SendABC() or two streams of SendXYZ(). is there a way to limit at max one stream per rpc per client ?. ",
    "anshupitlia": "Any update on this? . ",
    "shevchenkodenis": "I am developing these features and will send PR today.. I signed it.\n\u0447\u0442, 17 \u0430\u0432\u0433. 2017, 4:55 PM googlebot notifications@github.com:\n\nThanks for your pull request. It looks like this may be your first\ncontribution to a Google open source project. Before we can look at your\npull request, you'll need to sign a Contributor License Agreement (CLA).\n\ud83d\udcdd Please visit https://cla.developers.google.com/\nhttps://cla.developers.google.com/ to sign.\nOnce you've signed, please reply here (e.g. I signed it!) and we'll\nverify. Thanks.\n\n\nIf you've already signed a CLA, it's possible we don't have your\n   GitHub username or you're using a different email address. Check your\n   existing CLA data https://cla.developers.google.com/clas and verify\n   that your email is set on your git commits\n   https://help.github.com/articles/setting-your-email-in-git/.\nIf your company signed a CLA, they designated a Point of Contact who\n   decides which employees are authorized to participate. You may need to\n   contact the Point of Contact for your company and ask to be added to the\n   group of authorized contributors. If you don't know who your Point of\n   Contact is, direct the project maintainer to go/cla#troubleshoot.\nIn order to pass this check, please resolve this problem and have\n   the pull request author add another comment and the bot will run again.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/pull/1447#issuecomment-323080787, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AEHGrGRvv94Yx0wsq7XXmd2Prz1_Vo5_ks5sZEZagaJpZM4O6TIA\n.\n. Fixed tests and added case for using custom proxy.. @dfawley no problem. I will implement it asap.. @menghanl  sorry for delay, I was on vacation. I am planning to implement this on Monday.. \n",
    "sameervitian": "I signed it!. @dfawley @lyuxuan @menghanl  - could you pls have a look at it. \nI am stuck due to this issue. . @dfawley  - Thanks. Seems 1454 PR  will fix the problem as the duplicate is removed from the lookupSRV & lookupHost  . ",
    "chy168": "Hi @menghanl, i didn't run go build, i just go get -v google.golang.org/grpc/grpclb and got error.\nMy previous error message is not clear enough, so i update the error message with the command for test.. ",
    "muirrn": "Is someone actively working on this? I've been playing around with different buffer pool strategies, but don't want to look too much more if I'm just duplicating work.\nI've been trying bucketed []byte buffer pool where there are tiers of buffer sizes. For example, if you ask for a 700 byte buffer, it might pull a buffer from the 1024 byte pool, slice it down to 700 bytes and return it.\nI'm trying a sync.Pool implementation (one sync.Pool per buffer size bucket) and a leaky buffer implementation (one buffered chan []byte per buffer size bucket). They certainly help, but neither one has plucked all the low-hanging garbage fruit on the first try.. There is notable garbage produced outside of proto marshaling in the http2 transport, so I was hoping to address that too. I was looking at more generic []byte buffer reuse since it could be used by the existing codec, and used by the http2 transport.\nHow is pooling the codec better than just pooling the []byte slice? The existing codec could pull its []byte from []byte pool, and also implement similar gogoproto optimized logic taking advantage of Size() and MarshalTo() as appropriate.\n. My thought was instead of returning the buffer to the codec, return the buffer to the generic buffer pool instead. Then anyone can pull a buffer from the pool, and consumers of the buffers can put them back in the pool when they are done. It is safe to never return a buffer, or even to return a buffer that you did not get from the pool originally. On the other hand it is very unsafe to return a buffer and then still use it, or a slice of it. We would need some approach to make ourselves confident that use-after-release doesn't happen.. FYI still working on signing the license.. I think I signed the license.. I associated my GH account, so maybe now?. Thank you for looking. I have pushed the Unmarshaler change as requested. Below is benchmark comparison output.  I used these benchmark options:\ngo run benchmark/benchmain/main.go -benchtime=10s -workloads=all   -compression=on -maxConcurrentCalls=1 -trace=off   -reqSizeBytes=1,1048576 -respSizeBytes=1,1048576 -networkMode=Local   -cpuProfile=after_cpuProf -memProfile=after_memProf -memProfileRate=10000 -resultFile=after\n```\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1048576B-respSize_1048576B-Compressor_true\n     Title       Before        After Percentage\n  Bytes/op     16794264     16750595    -0.26%\n Allocs/op          196          197     0.51%\n50 latency  13.695719ms  12.325266ms   -10.01%\n90 latency  14.947742ms  14.132171ms    -5.46%\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1B-respSize_1B-Compressor_true\n     Title       Before        After Percentage\n  Bytes/op        13135        13135     0.00%\n Allocs/op          217          217     0.00%\n50 latency     153.65\u00b5s     143.05\u00b5s    -6.90%\n90 latency     203.31\u00b5s    198.995\u00b5s    -2.12%\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1B-respSize_1B-Compressor_true\n     Title       Before        After Percentage\n  Bytes/op       542367       597505    10.17%\n Allocs/op          124          126     1.61%\n50 latency    215.157\u00b5s    221.049\u00b5s     2.74%\n90 latency    564.593\u00b5s    583.952\u00b5s     3.43%\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1B-respSize_1048576B-Compressor_true\n     Title       Before        After Percentage\n  Bytes/op     10550046     10549798    -0.00%\n Allocs/op          638          621    -2.66%\n50 latency   2.719907ms   2.760703ms     1.50%\n90 latency   3.162787ms   3.237892ms     2.37%\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1B-respSize_1048576B-Compressor_true\n     Title       Before        After Percentage\n  Bytes/op      9139821      9137864    -0.02%\n Allocs/op          179          179     0.00%\n50 latency   6.342682ms   6.413199ms     1.11%\n90 latency   6.821812ms   7.127117ms     4.48%\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1048576B-respSize_1B-Compressor_true\n     Title       Before        After Percentage\n  Bytes/op     10549525     10549711     0.00%\n Allocs/op          634          651     2.68%\n50 latency   2.656818ms   2.709885ms     2.00%\n90 latency    3.15594ms   3.262535ms     3.38%\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1048576B-respSize_1B-Compressor_true\n     Title       Before        After Percentage\n  Bytes/op      9162913      9175603     0.14%\n Allocs/op          180          180     0.00%\n50 latency   6.409991ms   6.404367ms    -0.09%\n90 latency   7.028107ms   7.018095ms    -0.14%\nUnary-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1-reqSize_1048576B-respSize_1048576B-Compressor_true\n     Title       Before        After Percentage\n  Bytes/op     21095620     21094787    -0.00%\n Allocs/op         1061         1020    -3.86%\n50 latency   5.375525ms   5.260222ms    -2.14%\n90 latency   6.223487ms   5.913482ms    -4.98%\n``. I looked at some of our Unmarshalers generated via gogoproto and they don't reset themselves in theirUnmarshal`. I think it would be dangerous to not reset here. See https://github.com/gogo/protobuf/issues/334 and https://github.com/golang/protobuf/issues/424.. ",
    "coocood": "@MakMukhi \nI've got another idea to reuse the buffer for stream.\n\n\nAdd a buf field in parser\ngo\ntype parser struct {\n    r io.Reader\n    header [5]byte\n        buf []byte\n}\n\n\nMake the buf if the length is greater than buf length\ngo\nif length > cap(p.buf) {\n        p.buf = make([]byte, int(length))\n}\nmsg = p.buf[:length]\n\n\nWhat do you think?\n. @havoc-io \nsnippet updated, thanks.. @MakMukhi \nHow about just adding a CallOption to disable the buffer reuse for the extreme use case? \nFreeing a buffer by another goroutine adds a lot of complexity.. @MakMukhi \nGreat!\nI'll send a PR in a few days.. @MakMukhi \nThe buffer reused is only enabled in ClientStream, not in ServerStream.\nThis is different than what we have discussed because I think it's very unlikely a client keeps long-lived connections to thousands of servers.. @MakMukhi \nI'll post the benchmark result later.\nAnd the failed test is \n--- FAIL: TestDialWaitsForServerSettings (5.67s)\n    clientconn_test.go:159: Error while dialing. Err: context deadline exceeded\n    clientconn_test.go:138: Error while accepting. Err: accept tcp 127.0.0.1:44053: use of closed network connection\nIt seems not related to this PR, and it only failed on a particular Go version.. To focus on the issue I've found, I only post 1M response for now.\nThe result of after:\n```\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_1048576B-Compressor_false: \n50_Latency: 6.8084 ms   90_Latency: 10.3493 ms  99_Latency: 111.5983 ms     Avg latency: 16.0996 ms     Count: 2485     5123585 Bytes/op    478 Allocs/op \nHistogram (unit: ms)\nCount: 2485  Min:   3.6  Max: 114.2  Avg: 16.10\n\n[        3.564034,         3.564035)     1    0.0%    0.0%\n[        3.564035,         3.564042)     0    0.0%    0.0%\n[        3.564042,         3.564095)     0    0.0%    0.0%\n[        3.564095,         3.564514)     0    0.0%    0.0%\n[        3.564514,         3.567793)     0    0.0%    0.0%\n[        3.567793,         3.593464)     0    0.0%    0.0%\n[        3.593464,         3.794469)     3    0.1%    0.2%\n[        3.794469,         5.368326)   237    9.5%    9.7%  #\n[        5.368326,        17.691521)  2020   81.3%   91.0%  ########\n[       17.691521,       114.181329)   223    9.0%  100.0%  #\n[      114.181329,              inf)     1    0.0%  100.0%\n```\nThe result of before\n```\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_1048576B-Compressor_false: \n50_Latency: 7.5285 ms   90_Latency: 11.0990 ms  99_Latency: 112.7684 ms     Avg latency: 15.2175 ms     Count: 2629     5964215 Bytes/op    449 Allocs/op \nHistogram (unit: ms)\nCount: 2629  Min:   3.9  Max: 116.1  Avg: 15.22\n\n[        3.883256,         3.883257)     1    0.0%    0.0%\n[        3.883257,         3.883264)     0    0.0%    0.0%\n[        3.883264,         3.883318)     0    0.0%    0.0%\n[        3.883318,         3.883738)     0    0.0%    0.0%\n[        3.883738,         3.887039)     0    0.0%    0.0%\n[        3.887039,         3.912927)     0    0.0%    0.0%\n[        3.912927,         4.115956)     3    0.1%    0.2%\n[        4.115956,         5.708251)   311   11.8%   12.0%  #\n[        5.708251,        18.196155)  2118   80.6%   92.5%  ########\n[       18.196155,       116.135119)   195    7.4%  100.0%  #\n[      116.135119,              inf)     1    0.0%  100.0%\n```\nThe result is odd, reuse buffer branch allocates more than before.\nThen I removed the double length, allocate exactly the same length if it's more than cap, the result is much better:\n```\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_1048576B-Compressor_false: \n50_Latency: 6.6709 ms   90_Latency: 8.8538 ms   99_Latency: 109.6082 ms     Avg latency: 8.3528 ms  Count: 4790     5062523 Bytes/op    432 Allocs/op \nHistogram (unit: ms)\nCount: 4790  Min:   3.4  Max: 114.9  Avg: 8.35\n\n[        3.432767,         3.432768)     1    0.0%    0.0%\n[        3.432768,         3.432775)     0    0.0%    0.0%\n[        3.432775,         3.432828)     0    0.0%    0.0%\n[        3.432828,         3.433248)     0    0.0%    0.0%\n[        3.433248,         3.436538)     0    0.0%    0.0%\n[        3.436538,         3.462321)     1    0.0%    0.0%\n[        3.462321,         3.664369)     4    0.1%    0.1%\n[        3.664369,         5.247720)   378    7.9%    8.0%  #\n[        5.247720,        17.655699)  4338   90.6%   98.6%  #########\n[       17.655699,       114.891160)    67    1.4%  100.0%\n[      114.891160,              inf)     1    0.0%  100.0%\n```\n. @MakMukhi \nIt seems like the allocation number and average latency not stable on each bench test.\nIt has nothing to do with double length.\nI ran many times, the 50_latency reduced about 9%.\nHere is the result of different response size:\nAfter:\n```\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_1024B-Compressor_false:\n50_Latency: 159.7260 \u00b5s     90_Latency: 222.1620 \u00b5s     99_Latency: 485.1390 \u00b5s     Avg latency: 172.4190 \u00b5s    Count: 231633   8518 Bytes/op   55 Allocs/op\nHistogram (unit: \u00b5s)\nCount: 231633  Min:  75.9  Max: 1593.7  Avg: 172.42\n\n[     75.897000,      75.898000)       1    0.0%    0.0%\n[     75.898000,      75.901862)       0    0.0%    0.0%\n[     75.901862,      75.920638)       0    0.0%    0.0%\n[     75.920638,      76.011923)       0    0.0%    0.0%\n[     76.011923,      76.455742)       0    0.0%    0.0%\n[     76.455742,      78.613530)       4    0.0%    0.0%\n[     78.613530,      89.104400)     158    0.1%    0.1%\n[     89.104400,     140.109594)   60321   26.0%   26.1%  ###\n[    140.109594,     388.089960)  167790   72.4%   98.5%  #######\n[    388.089960,    1593.737000)    3358    1.4%  100.0%\n[   1593.737000,            inf)       1    0.0%  100.0%\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_65535B-Compressor_false:\n50_Latency: 656.3540 \u00b5s     90_Latency: 1211.1790 \u00b5s    99_Latency: 102631.8410 \u00b5s  Avg latency: 3609.9480 \u00b5s   Count: 11080    375379 Bytes/op83 Allocs/op\nHistogram (unit: \u00b5s)\nCount: 11080  Min: 263.9  Max: 104126.2  Avg: 3609.95\n\n[      263.865000,       263.866000)      1    0.0%    0.0%\n[      263.866000,       263.872775)      0    0.0%    0.0%\n[      263.872775,       263.925455)      0    0.0%    0.0%\n[      263.925455,       264.335059)      0    0.0%    0.0%\n[      264.335059,       267.519856)      2    0.0%    0.0%\n[      267.519856,       292.282632)      1    0.0%    0.0%\n[      292.282632,       484.820832)   2172   19.6%   19.6%  ##\n[      484.820832,      1981.864555)   8495   76.7%   96.3%  ########\n[     1981.864555,     13621.839964)     97    0.9%   97.2%\n[    13621.839964,    104126.229000)    311    2.8%  100.0%\n[   104126.229000,              inf)      1    0.0%  100.0%\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_1048576B-Compressor_false:\n50_Latency: 6.6519 ms   90_Latency: 9.0018 ms   99_Latency: 109.1591 ms     Avg latency: 8.4781 ms  Count: 4719     5153053 Bytes/op    430 Allocs/op\nHistogram (unit: ms)\nCount: 4719  Min:   3.2  Max: 113.8  Avg: 8.48\n\n[        3.162232,         3.162233)     1    0.0%    0.0%\n[        3.162233,         3.162240)     0    0.0%    0.0%\n[        3.162240,         3.162293)     0    0.0%    0.0%\n[        3.162293,         3.162712)     0    0.0%    0.0%\n[        3.162712,         3.165991)     0    0.0%    0.0%\n[        3.165991,         3.191661)     0    0.0%    0.0%\n[        3.191661,         3.392662)     1    0.0%    0.0%\n[        3.392662,         4.966478)   192    4.1%    4.1%\n[        4.966478,        17.289304)  4451   94.3%   98.4%  #########\n[       17.289304,       113.775874)    73    1.5%  100.0%\n[      113.775874,              inf)     1    0.0%  100.0%\n```\nBefore:\n```\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_1024B-Compressor_false:\n50_Latency: 162.9540 \u00b5s     90_Latency: 233.4670 \u00b5s     99_Latency: 525.4040 \u00b5s     Avg latency: 177.6840 \u00b5s    Count: 224778   9666 Bytes/op   56 Allocs/op\nHistogram (unit: \u00b5s)\nCount: 224778  Min:  71.2  Max: 2502.2  Avg: 177.68\n\n[     71.225000,      71.226000)       1    0.0%    0.0%\n[     71.226000,      71.230123)       0    0.0%    0.0%\n[     71.230123,      71.251246)       0    0.0%    0.0%\n[     71.251246,      71.359461)       1    0.0%    0.0%\n[     71.359461,      71.913856)       0    0.0%    0.0%\n[     71.913856,      74.754073)       2    0.0%    0.0%\n[     74.754073,      89.304767)     189    0.1%    0.1%\n[     89.304767,     163.849307)  114519   50.9%   51.0%  #####\n[    163.849307,     545.747826)  108026   48.1%   99.1%  #####\n[    545.747826,    2502.249000)    2039    0.9%  100.0%\n[   2502.249000,            inf)       1    0.0%  100.0%\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_65535B-Compressor_false:\n50_Latency: 777.7100 \u00b5s     90_Latency: 1360.8740 \u00b5s    99_Latency: 102505.5090 \u00b5s  Avg latency: 3696.4090 \u00b5s   Count: 10821    444362 Bytes/op84 Allocs/op\nHistogram (unit: \u00b5s)\nCount: 10821  Min: 304.8  Max: 105989.1  Avg: 3696.41\n\n[      304.817000,       304.818000)      1    0.0%    0.0%\n[      304.818000,       304.824790)      0    0.0%    0.0%\n[      304.824790,       304.877689)      0    0.0%    0.0%\n[      304.877689,       305.289792)      0    0.0%    0.0%\n[      305.289792,       308.500213)      0    0.0%    0.0%\n[      308.500213,       333.510505)      7    0.1%    0.1%\n[      333.510505,       528.349317)   2143   19.8%   19.9%  ##\n[      528.349317,      2046.210983)   8222   76.0%   95.9%  ########\n[     2046.210983,     13870.877808)    144    1.3%   97.2%\n[    13870.877808,    105989.118000)    303    2.8%  100.0%\n[   105989.118000,              inf)      1    0.0%  100.0%\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_64B-respSize_1048576B-Compressor_false:\n50_Latency: 7.5659 ms   90_Latency: 12.1183 ms  99_Latency: 112.5573 ms     Avg latency: 13.3949 ms     Count: 3005     5975819 Bytes/op    435 Allocs/op\nHistogram (unit: ms)\nCount: 3005  Min:   3.7  Max: 117.6  Avg: 13.39\n\n[        3.671680,         3.671681)     1    0.0%    0.0%\n[        3.671681,         3.671688)     0    0.0%    0.0%\n[        3.671688,         3.671742)     0    0.0%    0.0%\n[        3.671742,         3.672165)     0    0.0%    0.0%\n[        3.672165,         3.675489)     0    0.0%    0.0%\n[        3.675489,         3.701603)     0    0.0%    0.0%\n[        3.701603,         3.906753)     3    0.1%    0.1%\n[        3.906753,         5.518406)   198    6.6%    6.7%  #\n[        5.518406,        18.179525)  2626   87.4%   94.1%  #########\n[       18.179525,       117.645016)   176    5.9%  100.0%  #\n[      117.645016,              inf)     1    0.0%  100.0%\n```\n. @dfawley \nOK, I'll try to do it.\nSince some server applications need to maintain thousands of idle client connections, in this case, keeping a buffer increases the memory usage dramatically. So we need to add a server option to disable it.. @dfawley \nI enabled the server side buffer, and here is the benchmark result.\nOnly enable client-side buffer:\n```\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_65536B-respSize_1024B-Compressor_false:\n50_Latency: 875.3420 \u00b5s     90_Latency: 1349.9730 \u00b5s    99_Latency: 103020.3270 \u00b5s  Avg latency: 3807.4580 \u00b5s   Count: 10594    444244 Bytes/op84 Allocs/op\nHistogram (unit: \u00b5s)\nCount: 10594  Min: 302.8  Max: 105551.6  Avg: 3807.46\n\n[      302.849000,       302.850000)      1    0.0%    0.0%\n[      302.850000,       302.856787)      0    0.0%    0.0%\n[      302.856787,       302.909634)      0    0.0%    0.0%\n[      302.909634,       303.321142)      0    0.0%    0.0%\n[      303.321142,       306.525459)      0    0.0%    0.0%\n[      306.525459,       331.476744)      2    0.0%    0.0%\n[      331.476744,       525.766696)   1602   15.1%   15.2%  ##\n[      525.766696,      2038.658137)   8594   81.1%   96.3%  ########\n[     2038.658137,     13819.197919)     91    0.9%   97.1%\n[    13819.197919,    105551.568000)    303    2.9%  100.0%\n[   105551.568000,              inf)      1    0.0%  100.0%\n```\nEnable client-side and server-side buffer:\n```\nStream-traceMode_false-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_4-reqSize_65536B-respSize_1024B-Compressor_false:\n50_Latency: 695.1510 \u00b5s     90_Latency: 1253.2250 \u00b5s    99_Latency: 102783.2740 \u00b5s  Avg latency: 3156.0240 \u00b5s   Count: 12752    377945 Bytes/op83 Allocs/op\nHistogram (unit: \u00b5s)\nCount: 12752  Min: 260.8  Max: 106054.7  Avg: 3156.02\n\n[      260.767000,       260.768000)      1    0.0%    0.0%\n[      260.768000,       260.774791)      0    0.0%    0.0%\n[      260.774791,       260.827703)      0    0.0%    0.0%\n[      260.827703,       261.239955)      0    0.0%    0.0%\n[      261.239955,       264.451911)      0    0.0%    0.0%\n[      264.451911,       289.477036)      2    0.0%    0.0%\n[      289.477036,       484.453870)   2218   17.4%   17.4%  ##\n[      484.453870,      2003.565754)  10113   79.3%   96.7%  ########\n[     2003.565754,     13839.335561)    122    1.0%   97.7%\n[    13839.335561,    106054.694000)    295    2.3%  100.0%\n[   106054.694000,              inf)      1    0.0%  100.0%\n```. @MakMukhi \nThe new design sounds good.\nHow long does it take to implement?. @dfawley \nGood idea, I'll try it out.. @dfawley PTAL. The variable is defined in each goroutine, so there is no contention.. ditto. @MakMukhi \nI did a micro benchmark, using math functions takes more than 100ns but multiply by 2 in a for loop takes less than 10ns. \nFor growing 64B to 1MB.. ",
    "scotthew1": "@MakMukhi \nany update on the reworked stream flow to reduce buffer use as mentioned here? we've noticed an issue where bombarding a stream endpoint with small messages causes lots of memory to be allocated on recvBuffer. backlog by HandleStreams(). since our handler function cannot pull messages off the backlog quickly enough, memory usage balloons until the server is killed.\nit'd be nice if the buffer were at least some fixed length so that we could block the client's sends until there was room in the buffer. as it stands now, the server doesn't have any way of throttling the client and we have to rely on the client to send at a manageable rate.. no problem @MakMukhi, thanks for the response. i look forward to the change!. ",
    "bbeaudreault": "Thanks for tracking this down guys. One comment on the suggested fix at https://github.com/grpc/grpc-java/issues/3378#issuecomment-324776574:\nSince it takes time for the goAway to reach the client and for it to process and stop sending new streams, I'd just like to bring up that the proposed fix could possibly cause errors for any streams that may be created in the space between measuring t.activeStreams() and the client getting and processing the goAway. I'm not super familiar with the context here, so would be pleased to be proven wrong about that.. Gotcha, thanks for the clarification. Sounds good!. ",
    "kkishi": "I signed it!. Ah, that's handy! Done. Also separated the Go code regeneration to a new section.. ",
    "sunshangpp": "I signed it!. Hi @menghanl thanks for the reply!\nOriginally I hesitated to block Serve because this can be easily done in the app code so we have more flexibility. Also, as soon as server.lis is closed, the gRPC server stops Serving new requests, so semantically it's not really serving anymore. But I can totally see the benefits of blocking as well, I'll make the change.\nI think it'll be easier for me to do it in the same PR, I will make a new commit!\n. hey @menghanl mind taking another look at this? thanks!. I signed. ah good catch, will fix!. @menghanl Fixed by using a sync.Once to close the channels, this way when s.quit or s.done is closed, all go-routines running Serve() will be unblocked.. hey @menghanl I only added test for testing Serve() should return nil on GracefulStop(). \nI found it tricky to actually test the order of Serve and GracefulStop, the only way I can think of is to make both functions send a message to a shared channel when they return, and compare the order of the messages in the channel, but this requires modifying Serve and GracefulStop themselves.. ",
    "roadrunner": "net/trace has such a bad implementation as well as grpc-go tracing decision.\nwhat is the point of registering debug handlers at package-init level? for customisation, there is nothing you can do.\nand grpc-go enables tracing default. there should be a better way to configure it, other than that mutating init level global variables.\nit's not possible to define something like only enable tracing for specific requests or api methods. \nI think this is not suitable for a production use case, should be disabled by default.\n. ",
    "janardhan1993": "@MakMukhi I ran the benchmark code on loopy_writer branch and there wasn't much difference when compared to master.\nAs you can see with 1000 concurrent calls, request and response size of 1kb and no round trip latency, average time for grpc is 44ms which is a lot.\nWe use grpc in our database and latency is critical for reads and getting 1000 concurrent requests is quite normal in production. We use grpc for communicating between nodes in database(if running in cluster mode). We usually have a single connection between nodes and do all rpcs over it. What do you suggest we do to achieve good latency.(Like having more connections or setting some client options etc ?)\n```\n/usr/bin/time -v go run main.go -benchtime=10s -workloads=all  -maxConcurrentCalls=1000 -traceMode=false -reqSizeBytes=1000 -respSizeBytes=1000 -latency=0s -kbps=0 -mtu=0\nUnary-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1000B-respSize_1000B-Compressor_false:\n  226017         44287 ns/op    26879 B/op       190 allocs/op\nHistogram (unit: ms)\nCount: 226017  Min:   7.7  Max: 175.9  Avg: 44.25\n\n[        7.740154,         7.740155)       1    0.0%    0.0%\n[        7.740155,         7.740162)       0    0.0%    0.0%\n[        7.740162,         7.740221)       0    0.0%    0.0%\n[        7.740221,         7.740706)       0    0.0%    0.0%\n[        7.740706,         7.744682)       0    0.0%    0.0%\n[        7.744682,         7.777295)       0    0.0%    0.0%\n[        7.777295,         8.044820)       0    0.0%    0.0%\n[        8.044820,        10.239320)      24    0.0%    0.0%\n[       10.239320,        28.240719)     629    0.3%    0.3%\n[       28.240719,       175.905518)  225362   99.7%  100.0%  ##########\n[      175.905518,              inf)       1    0.0%  100.0%  \nStream-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1000B-respSize_1000B-Compressor_false\nlatency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1000B-respSize_1000B-Compressor_false\n  491891         20349 ns/op    17133 B/op        37 allocs/op\nHistogram (unit: \u00b5s)\nCount: 491891  Min:  77.4  Max: 77139.2  Avg: 20329.41\n\n[      77.353000,       77.354000)       1    0.0%    0.0%\n[      77.354000,       77.360522)       0    0.0%    0.0%\n[      77.360522,       77.409576)       0    0.0%    0.0%\n[      77.409576,       77.778546)       0    0.0%    0.0%\n[      77.778546,       80.553826)       0    0.0%    0.0%\n[      80.553826,      101.428625)       0    0.0%    0.0%\n[     101.428625,      258.442420)       7    0.0%    0.0%\n[     258.442420,     1439.451739)      13    0.0%    0.0%\n[    1439.451739,    10322.639411)    4774    1.0%    1.0%\n[   10322.639411,    77139.236000)  487095   99.0%  100.0%  ##########\n[   77139.236000,             inf)       1    0.0%  100.0%  \nCommand being timed: \"go run main.go -benchtime=10s -workloads=all -maxConcurrentCalls=1000 -traceMode=false -reqSizeBytes=1000 -respSizeBytes=1000 -latency=0s -kbps=0 -mtu=0\"\nUser time (seconds): 74.90\nSystem time (seconds): 4.77\nPercent of CPU this job got: 372%\nElapsed (wall clock) time (h:mm:ss or m:ss): 0:21.37\nAverage shared text size (kbytes): 0\nAverage unshared data size (kbytes): 0\nAverage stack size (kbytes): 0\nAverage total size (kbytes): 0\nMaximum resident set size (kbytes): 123668\nAverage resident set size (kbytes): 0\nMajor (requiring I/O) page faults: 0\nMinor (reclaiming a frame) page faults: 104139\nVoluntary context switches: 155742\nInvoluntary context switches: 8481\nSwaps: 0\nFile system inputs: 0\nFile system outputs: 19088\nSocket messages sent: 0\nSocket messages received: 0\nSignals delivered: 0\nPage size (bytes): 4096\nExit status: 0\n\n```. Strangely i get more throughput if i set maxConcurrentCalls  to 100 when compared to 1000 on my machine. In 10 seconds the count is 277203 when we have 100 connections and is 215057 when we have 1000 concurrent connections.\n```\ngo run benchmark/benchmain/main.go -benchtime=10s -workloads=unary   -compression=off -maxConcurrentCalls=1000 -traceMode=false   -reqSizeBytes=1024 -respSizeBytes=1024   -latency=0s -kbps=0 -mtu=0\nUnary-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1024B-respSize_1024B-Compressor_false:\n  215057         46572 ns/op    28423 B/op       190 allocs/op\nHistogram (unit: ms)\nCount: 215057  Min:  11.9  Max: 128.1  Avg: 46.52\n\n[       11.890526,        11.890527)       1    0.0%    0.0%\n[       11.890527,        11.890534)       0    0.0%    0.0%\n[       11.890534,        11.890588)       0    0.0%    0.0%\n[       11.890588,        11.891014)       0    0.0%    0.0%\n[       11.891014,        11.894368)       0    0.0%    0.0%\n[       11.894368,        11.920773)       3    0.0%    0.0%\n[       11.920773,        12.128659)       0    0.0%    0.0%\n[       12.128659,        13.765331)       5    0.0%    0.0%\n[       13.765331,        26.650745)     574    0.3%    0.3%\n[       26.650745,       128.096754)  214473   99.7%  100.0%  ##########\n[      128.096754,              inf)       1    0.0%  100.0%  \n\u279c  grpc git:(loopy_writer) go run benchmark/benchmain/main.go -benchtime=10s -workloads=unary   -compression=off -maxConcurrentCalls=100 -traceMode=false   -reqSizeBytes=1024 -respSizeBytes=1024   -latency=0s -kbps=0 -mtu=0 \nUnary-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_100-reqSize_1024B-respSize_1024B-Compressor_false:\n  277203         36081 ns/op    28211 B/op       189 allocs/op\nHistogram (unit: \u00b5s)\nCount: 277203  Min: 172.5  Max: 24015.4  Avg: 3606.44\n\n[     172.491000,      172.492000)       1    0.0%    0.0%\n[     172.492000,      172.497602)       0    0.0%    0.0%\n[     172.497602,      172.534593)       0    0.0%    0.0%\n[     172.534593,      172.778819)       0    0.0%    0.0%\n[     172.778819,      174.391317)       0    0.0%    0.0%\n[     174.391317,      185.037789)       0    0.0%    0.0%\n[     185.037789,      255.330810)       1    0.0%    0.0%\n[     255.330810,      719.438459)     102    0.0%    0.0%\n[     719.438459,     3783.695846)  172890   62.4%   62.4%  ######\n[    3783.695846,    24015.367000)  104208   37.6%  100.0%  ####\n[   24015.367000,             inf)       1    0.0%  100.0%\n```\nI tried setting maxConcurrentStreams to 100 in server opts in benchmark  and ran with 1000 concurrent requests but the latency was still high.\n```\ngo run benchmark/benchmain/main.go -benchtime=10s -workloads=unary   -compression=off -maxConcurrentCalls=1000 -traceMode=false   -reqSizeBytes=1024 -respSizeBytes=1024   -latency=0s -kbps=0 -mtu=0\nUnary-noTrace-latency_0s-kbps_0-MTU_0-maxConcurrentCalls_1000-reqSize_1024B-respSize_1024B-Compressor_false:\n  184611         54420 ns/op    28246 B/op       189 allocs/op\nHistogram (unit: \u00b5s)\nCount: 184611  Min: 222.3  Max: 89047.7  Avg: 54298.48\n\n[     222.280000,      222.281000)       1    0.0%    0.0%\n[     222.281000,      222.287641)       0    0.0%    0.0%\n[     222.287641,      222.338390)       0    0.0%    0.0%\n[     222.338390,      222.726182)       0    0.0%    0.0%\n[     222.726182,      225.689441)       0    0.0%    0.0%\n[     225.689441,      248.332771)       0    0.0%    0.0%\n[     248.332771,      421.358659)       0    0.0%    0.0%\n[     421.358659,     1743.512134)       0    0.0%    0.0%\n[    1743.512134,    11846.565689)       9    0.0%    0.0%\n[   11846.565689,    89047.657000)  184600  100.0%  100.0%  ##########\n[   89047.657000,             inf)       1    0.0%  100.0%\n```. ",
    "manishrjain": "So, to achieve low latency, what do you recommend? Should we create multiple connections, and round robin over them for every request? 5ms just to do a heartbeat (in Raft) for e.g., is still too much -- we should be going into micro second durations for these cases where both the servers are in the same datacenter.. It's not just heartbeat. We have one connection per pair of servers, so it's a mix of everything. All the communication flows through the same connection. Heartbeats, small requests, big requests, etc. We just push everything onto one connection at the moment. So, heartbeats, plus queries to other server, with payload sizes ranging from small to big.\nSome of the issues we've noticed is that for small sized requests, the latency is unexpectedly high; presumably because of the client side batching that's going on.\nWill look into keepalive. (On a side note, if it can give us a way to test if a connection is active, we can use it to avoid reference counting to determine if a connection can be closed.)\n@janardhan1993 is trying with setting the number of concurrent calls by setting max concurrent streams. Looks like latency is still an issue there as well. Let us know if we should do this at our end instead, by not allowing more than X number of calls on the same connection.. ",
    "jimjibone": "Wow, I didn't realise the change was that recent! I have only just attempted to run it on a 32-bit system.. ",
    "wallaceicy06": "1524. #1524 . Sorry for the delay; took me a bit to figure out the build process. Should be good now.. Closing this PR in favor of a new, cleaner PR: #1537 . #1524 .",
    "ppacher": "@jhump thanks for the hint. This seems like a reasonable workaround and avoids using the file paths. Though I think addind ServiceDesc and MethodDesc to the UnaryServerInfo/StreamServerInfo structs is still a good idea. But for now I can go with your approach. . ",
    "nate-meta": "@jhump thanks for the pointer!\nIs there an easy way to convert the MethodDescriptor back to the original proto extension?. Sorry, my question is not entirely related to this thread, but was how to get the proto value described in the extension set on a method.\nI.e. how do I unmarshal the value for NestedMessage of \"asdf\" in this example:\n```\nservice MyService {\n  rpc MethodWithOption (FooRequest) returns (FooResponse) {\n    option (some_value_option) = {stuff: \"asfd\"};\n  }\n}\nextend google.protobuf.MethodOptions {\n  NestedMessage some_value_option = 50006;\n}\nmessage NestedMessage {\n  string stuff = 1;\n}\nmessage FooRequest {}\nmessage FooResponse {}\n```\nI vaguely remember how to do this in Java, but can't find any documentation for how its done in Go.. ",
    "vbatts": "CI failed. ",
    "theobouwman": "@dfawley and how could I do that?. ",
    "jkinkead": "Would adding a value to the ServerStream.Stream's Context really break clients? Unless they're inspecting the entire Context object, that oughtn't be the case.\nI was interested in running a proxy-like server for a service that wasn't known at boot time - in my particular case, as gRPC layer on top of a REST-ful API.\n. I think I like option 3 best - although option 2 also seems good. Wouldn't 2 also require exposing serverStream?. ",
    "sandeepkdpl": "We have a android application which is making connection in every 30 seconds. Please see below code for how we are creating connection and closing connection in every request to server. We are using async task to perform grpc server call.\n```java\nprivate ManagedChannel mChannel;\nprotected GRPCResponse doInBackground(GRpcType... params) {\ntry {\n   // Initializing Channel  with timout 15 seconds \n   mChannel = ManagedChannelBuilder.forAddress(mHost, mPort).build();\n    yegoappinfsrvGrpc.yegoappinfsrvBlockingStub stub = yegoappinfsrvGrpc.newBlockingStub(mChannel)\n            .withDeadlineAfter(15, TimeUnit.SECONDS);\n    // Some more code \u2026.\n  \u2026.\n\u2026\u2026\n}\nprotected void onPostExecute(GRPCResponse result) {\n    try {\n        mChannel.shutdown().awaitTermination(1, TimeUnit.SECONDS);\n    // Some more code \u2026.\n     \u2026.\n     \u2026\u2026\n}\n```\nAs per our observation normally when internet connection is running fine then there is no issue but if internet connection is not running properly then some connection gets ideal after at server.. In Async Task class, DoInBackground() runs in separate thread other than UI thread, while OnPostExecute() runs after DoInBackground() and runs on UI thread. \n\"I'm not familiar with the Java API. You say you're using the async task, and you have a shutdown call in onPostExecute, but your stub is created using newBlockingStub. Is this correct?\" \nYes we are doing exactly the same way !!!\nWe have already set MaxAge to 60 seconds on server side. Do we need to set it on client side also ?. MaxConnectionAge means the maximum time for which connection is active. \nIf we will set this time to very short like 1 second that means it will disconnect the connection after 1 second if client is active and communicating with server?  Am I Right? \nPlease see below parameters we are using and suggest us which parameters are to be updated for properly killing the connections from the server side.\nSee these parameters.\nRPC Server Keep alive Parameters \nMaxConnectionIdle = 60 seconds\nMaxConnectionAge = 60 seconds\nMaxConnectionAgeGrace = 30 seconds\nTimeout = 20 seconds\nTime = 1 second.\nSystem Keep alive parameters are\nKeep Alive Time = 60 Seconds\nKeep Alive Probes = 5 time\nKeep Alive Interval = 24 Seconds . @menghanl  and @dfawley any more input is required? . @dfawley  Yes looks like problem is resolved. Since last 6 days we have not faced this issue after upgrading.\nThanks. ",
    "zwass": "We found a workaround for this in our case: https://play.golang.org/p/VwcUcALPGN\nThe concept is to check for this specific error and explicitly mark it as temporary so that the gRPC dialer will retry dialing.. Sounds like our workaround is consistent with your suggestion. Thank you.. ",
    "balshetzer": "I was convinced to restore backwards compatibility in golang/mock#118. ",
    "Civil": "Am I right that currently specifying balancer with \"ServiceConfig\" is not implemented and for now I should use \"WithBalancerBuilder\" with the aim to switch to ServiceConfig as soon as corresponding PRs will be merged?. Actually same problem also affects client. I've set 'grpc.WithMaxMsgSize' to 8589934592 (8GB) and got error message:\ngrpc: received message larger than max (23 vs. 8589934592). ",
    "srinarayanant": "I have added the working copy to the vendor directory\nhttps://github.com/srinarayanant/terraform-provider-vcloud-director/tree/master/go/src/vendor/google.golang.org   . https://gist.github.com/srinarayanant/168e400af5b617aa6173597fcfe6a262. ",
    "chrissnell": "I tried again.  Still no-go:\n```\nsocotra% govendor fetch google.golang.org/grpc@94f1917696488c633b485fe76feb764c935f2561\nsocotra% go build\nsocotra% ./weather-bar\npanic: runtime error: invalid memory address or nil pointer dereference\n    panic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x7a2f17]\ngoroutine 1 [running]:\ngithub.com/chrissnell/weather-bar/vendor/google.golang.org/grpc.(ClientConn).Close(0x0, 0x1, 0x1)\n    /home/cjs/dev/go/src/github.com/chrissnell/weather-bar/vendor/google.golang.org/grpc/clientconn.go:824 +0x37\npanic(0x836680, 0xb37df0)\n    /usr/lib/go/src/runtime/panic.go:491 +0x283\ngithub.com/chrissnell/weather-bar/vendor/google.golang.org/grpc.NewClientStream(0xb045a0, 0xc42009c240, 0xb39ce0, 0x0, 0x8c48be, 0x17, 0x0, 0x0, 0x0, 0xc42009a530, ...)\n    /home/cjs/dev/go/src/github.com/chrissnell/weather-bar/vendor/google.golang.org/grpc/stream.go:100 +0x29\ngithub.com/chrissnell/weather-bar/protobuf.(weatherClient).GetLiveWeather(0xc420098080, 0xb045a0, 0xc42009c240, 0xb67570, 0x0, 0x0, 0x0, 0xc42007fd20, 0x18, 0xc4201707e8, ...)\n    /home/cjs/dev/go/src/github.com/chrissnell/weather-bar/protobuf/grpcweather.pb.go:137 +0xb5\nmain.main()\n    /home/cjs/dev/go/src/github.com/chrissnell/weather-bar/main.go:64 +0x504\n```\nIf you want to try weather-bar, use this for your config file:\n```ini\n[server]\nhostname = home.chrissnell.com\nport = 7500\ncert = /home/cjs/.config/weather-bar/fullchain.pem\n[format]\nweather-format = \"\uf2c9 %temperature-fahrenheit%\u00b0F  \uf1fb %humidity%%  \uf11d %windcardinal% @ %windspeed% MPH  \uf0c2 %rainfall%\"\n. OK, printed the connection error and here's what it says now:\n2017/10/20 15:37:18 Could not connect to gRPC weather server: failed to build resolver: could not get resolver for scheme: \"\"\n```\nHere is the cert if you want to test:\nhttps://gist.github.com/chrissnell/15baef1c095df6eb0ec96539c9e688e7. I deleted my $GOPATH/pkg directory and rebuilt.  It works now.  Crazy!  \nThanks for the help.. ",
    "rookiejin": "Thankyou. only one error occured.  I will try your method tomorrow . It looks helperful .  thanks again . good night\u3002. ",
    "irfn": "@dfawley why was this issue closed. \ni am still seeing this issue on go1.9.2 with go-grpc 1.8.2\n. @menghanl issue was fixed the issue by choosing different versions of net/http2 and grpc.. ",
    "TrackClimbCom": "We need this pull request so much!. ",
    "eldadru": "@dfawley thanks!. ",
    "minaevmike": "@menghanl thank you for your answer.\nBut with load balancing i can't set ServiceName, because i don't know witch server will be choose for connection.\nfor example, my balancer gives me addr-01.some-host.com and addr-02.some-host.com, and mb in future one server can be added like addr-03.some-host321.com. So i set it to tls.Config, because for each server it must be different authorities.\nP.S.\nI update my question,\nP.S.S.\nas for me this change is bad\n```\n@@ -173,7 +174,7 @@ func newHTTP2Client(ctx context.Context, addr TargetInfo, opts ConnectOptions, t\n        )\n        if creds := opts.TransportCredentials; creds != nil {\n                scheme = \"https\"\n-               conn, authInfo, err = creds.ClientHandshake(connectCtx, addr.Addr, conn)\n+               conn, authInfo, err = creds.ClientHandshake(connectCtx, addr.Authority, conn)\n. Even if i rewrite my balancer and it would return `IP`'s not server names i can't understand 1 thing.\nI have 1 logical connection(`*grpc.ClientConn`) inside it can be load balanced and so on. So it can produce several physical connections to different hosts. But i can set Authority only on logical connection(same authority for different hosts). And in case of load balancing to different servers using one authority will cause a lot of problems. Because while certifcate validation will compare this authority and server name from server certificate.\ni will check tomorrow, but i think that this can be cheap fix\ndiff --git a/clientconn.go b/clientconn.go\nindex 5f5aac4..04c32b7 100644\n--- a/clientconn.go\n+++ b/clientconn.go\n@@ -946,7 +946,11 @@ func (ac *addrConn) resetTransport() error {\n                        sinfo := transport.TargetInfo{\n                                Addr:      addr.Addr,\n                                Metadata:  addr.Metadata,\n-                               Authority: ac.cc.authority,\n+                       }\n+                       if ac.cc.balancerWrapper != nil {\n+                               sinfo.Authority = addr.Addr\n+                       } else {\n+                               sinfo.Authority = ac.cc.authority\n                        }\n                        newTransport, err := transport.NewClientTransport(ac.cc.ctx, sinfo, copts, timeout)\n                        if err != nil {\n``. i rewrite fix: ifAddris IP - use authority fromcc, otherwiseAddris authority . Ok i agree with you. Mb logic can be similar tostats.Handler`? . Also i'll want to notice that there is some misleading code:\n- https://github.com/grpc/grpc-go/blob/master/transport/http2_client.go#L1229\n- https://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L134\nso by default on server max streams - 4294967295, on client - 2147483647, so, as for me, client limitation must be same as server - math.MaxUint32. I add conn.Close to defer and there is still an error. \nAs i can see the problem is:\nhttps://github.com/grpc/grpc-go/blob/master/transport/controlbuf.go#L295 , this done chan is ctx.Done() for this connection. I know that ClientCons are long-living but in this case this is just simple console client for manual testing. So i close connection on client side, server try to read frame and got an io.EOF https://github.com/grpc/grpc-go/blob/master/transport/http2_server.go#L448 and method Close called. In Close method context cancled and this error appears. . Ok, I will rewrite my fix- check if address returned from balancer is IP address or not. ",
    "crazed": "Sorry for commenting on a closed issue, but I'm having a similar issue here. Looks like a custom dialer is the recommended approach. Just wanted to raise the point that others with custom discovery probably don't want the resolver.Target's endpoint to be used for x509 validation provided by credentials.TransportCredentials. Would be nice if there was a way to update the ServerName to expect in the resolver. . ",
    "emacsliu1107": "I signed it!. It's my pleasure.. ",
    "sougou": "We have a workaround for this via https://github.com/youtube/vitess/pull/3348:\n WithDialer was not a viable option because it bypasses the proxy detection layer, which may be useful.\n Overriding the DNS would have caught some use cases, but not all.\nSo, we decided to just redirect grpc logging into our own logs. They are a little spammy, but it felt like the best trade-off.\nBased on internal discussions, I understand that the design is such that it's hard for the outermost layer to know the underlying error. But it would be definitely something that the vitess community will appreciate if it could be made to work.\nI'll leave the issue open in case you think of a solution. Otherwise, you may close it.. An update on this bug: the work-around of redirecting grpc logs to our own has allowed us to better trouble-shoot issues. However, users have not been happy with the amount of spam the logs generate. Sometimes, they also get confused that something is wrong when everything is actually working as intended.\nIdeally, it would still be nice if we could figure out a way to return the underlying failure. Another option would be to look at reducing spam (without loss of info). The source of most spam seems to come from these two lines:\nW1202 14:47:20.861791   11030 clientconn.go:1028] grpc: addrConn.transportMonitor exits due to: context canceled\nW1202 14:47:20.862964   11030 clientconn.go:934] grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: Error while dialing dial tcp: lookup myserver-7 on 10.1.2.3:53: dial udp 10.1.2.3:53: operation was canceled\"; Reconnecting to {myserver-7:16302 0  <nil>}. Thanks for this fix! We'll let you know how it works out once we upgrade.. If it was intentional, we're good.\nThe open source version is behaving differently from the google import: Connect in 1.7.1 still returns the underlying error. But that's our problem.. Thanks for the quick response. This is basically from code that works with 1.6.0, but fails on 1.7.1. Commenting out the Close calls fixed the problem.\nIt's possible that we're not using the connections correctly in the first place, which might have accidentally worked with 1.6.0. I'll post back here with what I find.. I wan't able to reproduce this using a trivial test. The additional data point is that the user was getting this error in a tight loop until the call failed:\nW1114 19:16:07.712290 330818 clientconn.go:1028] grpc: addrConn.transportMonitor exits due to: connection error: desc = \u201ctransport: error while dialing: dial tcp: operation was canceled\"\nSeems to be originating from go runtime, but also due to a canceled context.. cc @demmer @tirsen. More update: I was able to reproduce that error by adding grpc.FailOnNonTempDialError(true). So, that's not it.\nMy theory right now is that the continuous spam of opening and closing connections is causing one of the servers (vttablet) to be too slow to respond, and fail a request we don't retry on.\nIf we didn't close connections, it's likely that grpc reused the underlying connection and multiplexed the requests over it, which avoided the timeout error. We'll run some more tests tomorrow to confirm.. The user (@tirsen) worked around this by just going back to an older snapshot. They were in a hurry, and couldn't wait.\nFor now, I'll add a retry on that failed call, and we'll see how it goes the next time they retry the workflow. I'll close this bug for now. We can always re-open if the work-around fails.. ",
    "keelerh": "Protoc now supports an idempotency_levelmethod option: https://github.com/google/protobuf/blob/9e745f7/src/google/protobuf/descriptor.proto#L630. The method option can be used to mark the idempotency of a gRPC method.\nservice Greeter {\n  // Sends a greeting\n  rpc SayHello (HelloRequest) returns (HelloReply) {\n    option idempotency_level = NO_SIDE_EFFECTS;\n  }\n}\nIdempotencyLevel would be set to a non-default value by explicitly setting the idempotency_level method option for a given service.\nThe purpose of the change is to include this idempotency_level information with the previously defined UnaryServerInfo such that it can be used at a later point, e.g. by server-side gRPC interceptors designed to ensure that services marked as idempotent do not suddenly become non-idempotent by calling a non-idempotent method.. I signed it!. ",
    "explodingcamera": "When using the command specified on https://grpc.io/docs/quickstart/go.html it works: \nbash\n$ protoc -I helloworld/ helloworld/helloworld.proto --go_out=plugins=grpc:helloworld. ",
    "rra": "Thanks for the fix! We set failfast by default in our internal code, so I think this works for us, but @dzbarsky to confirm.. ",
    "kop": "\nAn easy workaround would be to add a new function dns.NewBuilderWithFreq(freq time.Duration)\n\nYes, this sounds good.\n\nI'm working on a change to make resolver re-resolve the name whenever a connection goes down.\nBut the resolver won't re-resolve if all connections are working. Does this work for K8S?\n\nI'm not sure it will work great. I mean it will handle cases when pod is destroyed/recreated but it will not handle cases when service is scaled up and new pods are ready to serve requests, isnt?\nI think polling frequency should stay. It will be possible to set much higher values for polling frequency if targets will be reevaluated on every disconnect, but we still need a mechanism to discover new instances of the service.. > IMO a better solution for this would be to push the updates instead of polling.\nTotally agreed.\n\nBut that's out of the scope of DNS.\n\nDNS is a native service discovery mechanism in K8S and i think it would be really great if it will work with gRPC out of the box :). @trusch but we can use Headless Services - they return multiple A records, this should solve the problem.. ",
    "HannesMvW": "I agree with the OP, we'd benefit from setting the DNS polling frequency shorter than 30 minutes.\nWhat is the alternative method to alert client(s) when new instenaces have been added behind a Headless service?. Thanks @ejona86 - how do I access this API? I'm using the \"grpc\" package from a Go application.. Thanks, @ejona86. My apologies for not being more specific - I'd like my gRPC clients (using client side load balancing) to refresh the list of servers (listed for a Kubernetes service name), via DNS, more frequently than every 30 minutes.\nOf course, instead of polling, a prettier solution could somehow subscribe to DNS changes, but I haven't yet figured out if or how this is possible with gRPC.\nPS. Thanks to your help I got MaxConnectionAge to work, and then realized it keeps killing my gRPC connections - which is not what I intended. I'd like to keep server connections up throughout the lifetime of the server, but also make use of new servers as quickly as possible (which I guess requires DNS polling).. Great, thanks @ejona86 for your feedback! I understand polling in general is a bad idea. In this case \"a few minutes\" should be good enough frequency, since we'd want the gRPC client side load balancing to pick up new servers reasonably quick.\nMy application is a small number of client side load balancers, working towards a huge set of servers, possibly in the 100's or 1000. (This is why I want to avoid tearing down gRPC connections unnecessarily.) When scaling up the number of servers, I'd like to see them go into use a bit quicker than 30 minutes...\nI had a look at the gRPC source code, and I see no solution short of writing my own load balancer builder...all just to change \"30m0s\" to something like \"3m0s\". Or is there an easier way?\nOr, perhaps we can also hook into dockerd to get notifications when new servers are added.\n--\nOn a side note, I've noticed gRPC DNS for service \"foo\" does this:\n1. Query DNS SRV record for \"grpclb.tcp.foo\"\n2. Query DNS for each target returned from this query (think hundreds of DNS queries here...)\n3. Query DNS for A record for \"foo\" (this returns the complete set of server IP addresses)\n4. Query DNS for TXT record for \"foo\" (in hope of getting a service config, not supported by K8s)\nI'd love to skip steps 1, 2, and 4, since 3 gives us the complete list of servers we're looking for.\nDo I misconfigure/misuse gRPC client side load balancing somehow?\nAlthough related to the thread above, I realize this is probably better off on a different thread.. Thanks s much for looking into this. Yes, I'm looking for really really high performance. Currently I've opted out of naming my service ports 'grpclb' to make the 1st lookup fail, which, yes, does skip the 2nd lookup, utilizing only the 3rd lookup. 4 always fails. (I've seen requests for K8s support for TXT records, for gRPC service configs - which I also would be interested in trying.). ",
    "seeruk": "Bump to verify CLA.. ",
    "hotdust": "\nOne other thing to mention here is, balancer.go is the old balancer APIs. If you are trying to create your own custom balancer, please take a look at the new balancer package.\n\nthank you for additional information\n. ",
    "predmond": "I don't explicitly call NewAddress with an empty array it just happens that way during shutdown. The resolver doesn't know anything other than the list of address is now empty (which could happen normally if all services went away for some reason). I'm getting another crash when I call cc.NewServiceConfig() before any calls to NewAddress()\n```\nI1122 13:39:00.510892   23302 swarm.go:55] Starting swarm version  () go1.9\nI1122 13:39:00.512396   23302 node.go:162] [NEST] serf agent binding to address 0.0.0.0:37017\nI1122 13:39:00.512425   23302 node.go:178] [NEST] serf agent joining cluster at localhost:7946\nI1122 13:39:00.514699   23302 resolver_conn_wrapper.go:65] [GPRC] dialing to target with scheme: \"serf\"\nI1122 13:39:00.514920   23302 resolver.go:38] [NEST|resolver] building resolver for {serf  nest}\nI1122 13:39:00.515056   23302 resolver_conn_wrapper.go:114] [GPRC] ccResolverWrapper: got new service config: {\n        \"loadBalancingPolicy\": \"round_robin\"\n    }\nI1122 13:39:00.515137   23302 clientconn.go:649] [GPRC] ClientConn switching balancer to \"round_robin\"\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0xdae039]\ngoroutine 98 [running]:\nswarm/vendor/google.golang.org/grpc.(ccBalancerWrapper).close(...)\n    /home/predmond/src/swarm/go/src/swarm/vendor/google.golang.org/grpc/balancer_conn_wrappers.go:154\nswarm/vendor/google.golang.org/grpc.(ClientConn).switchBalancer(0xc42015e780, 0xc4202ca0e0, 0xb)\n    /home/predmond/src/swarm/go/src/swarm/vendor/google.golang.org/grpc/clientconn.go:662 +0x139\nswarm/vendor/google.golang.org/grpc.(ClientConn).handleServiceConfig(0xc42015e780, 0x192bacc, 0x2b, 0x1, 0x1)\n    /home/predmond/src/swarm/go/src/swarm/vendor/google.golang.org/grpc/clientconn.go:820 +0x11f\nswarm/vendor/google.golang.org/grpc.(ccResolverWrapper).watcher(0xc4202a67b0)\n    /home/predmond/src/swarm/go/src/swarm/vendor/google.golang.org/grpc/resolver_conn_wrapper.go:115 +0x2d6\ncreated by swarm/vendor/google.golang.org/grpc.newCCResolverWrapper\n    /home/predmond/src/swarm/go/src/swarm/vendor/google.golang.org/grpc/resolver_conn_wrapper.go:84 +0x2e1\nexit status 2\n```. both issues are fixed by the PR. thanks!. Sorry for the delay..\nLooking at it again I don't think just a slice of SubConns will do. We need some place to store the computed rendezvous weight for each address (for example computed in HandleResolvedAddrs when new addresses are added)\nGiven a map from address to SubConn we could compute the weight each time but that would not be efficient.. I think this will work fine :) I made one comment about Name() . Is this getting merged? I could use this feature... I think the pickerBuilder needs to provide the name now.. ",
    "dwbuiten": "I think we're experiencing the same issue (huge memory growth). I bisected the first bad commit to 0d399e6307b06024368bb2624572e1271c6eda3d.\n@kaoet May be useful if you can confirm it's the same for you, so I'm not wasting anyones time by confusing two different bugs, if that's the case.. ",
    "kaoet": "@dfawley  I cannot reproduce the memory leak now, even if I don't add MaxConcurrentStreams.\nSo I cannot confirm whether it works for me.. ",
    "drausin": "For future reference to others, I have run into this linear heap and goroutine growth when I haven't properly managed grpc connections, e.g., when creating new connections from the client to the server instead of using an existing (pooled) connection. Proper connection pooling solved the problem.. ",
    "jpbetz": "This appears critical for kubernetes users as well. Once this is available and we can get a new etcd release incorporating it, I'd like to fast track it into kubernetes.. Was able to resolve this. Once the grpc/grpclb/grpc_lb_v1 package was excluded (godep get google.golang.org/grpc/... will exclude it) as will removing it from Godeps.json, godeps correctly ignored the directory.. godep issue: https://github.com/tools/godep/issues/554. @dfawley If you're willing to merge it, https://github.com/grpc/grpc-go/pull/1743 would be a huge help to kubernetes for getting grpc upgraded. We're currently stuck on 1.3.0. We'd need a new 1.7.x cut with it though..\nI was hoping that maybe we could get this fixed in godep, but the project has had no activity since February this year, so I'm doubtful that even if we authored the PR that it would get merged. The only other paths forward are https://github.com/grpc/grpc-go/pull/1743 or to wait till a large number of kubernetes' transitive dependencies are on grpc 1.7.x.. @dfawley Totally understand. We're just grasping for workarounds until we can migrate of godep entirely. Patching 1.7.x only would work for us.. @dfawley I've created PR https://github.com/grpc/grpc-go/pull/1747 for merging this to the 1.7.x branch.. @dfawley . Test failure:\ngrpclb/grpc_lb_v1/doc.go:21:1: don't use an underscore in package name. Added pkg to list of ignored golint files.. Thanks @menghanl ! Applying 1.7.5 to kubernetes now. Tests are passing locally and I expect https://github.com/kubernetes/kubernetes/pull/57160 to merge once e2e tests pass.. Should this be applied to http2_client.go as well? https://github.com/grpc/grpc-go/blob/291de7f0abf52bab39206f2da79e3b27b2ca70d1/transport/http2_client.go#L120. @menghanl Thanks. This is from WIP on development branch which currently registers balancers in a etcd clientv3 New() function that could be called at any time. I'll look into moving registration to init().. I was able to register the balancer in an init function. Closing this. Thanks!. @menghanl I may have spoken too soon. For etcd we might need to be able to register load balancers on the fly.\nI'll try to summarize the use case:\nFor the etcd \"clientv3\" interface, we need to support instantiation of etcd clients to any number of etcd server clusters within the same go process. Each etcd \"clientv3\" can be configured with a distinct set of endpoints that it should loadbalance across.  E.g.:\nc1 := clientv3.New(clientv3.Config{Endpoints: []string{\"http://127.0.0.1:1001\", \"http://127.0.0.1:1002\", \"http://127.0.0.1:1003\"}})\nc2 := clientv3.New(clientv3.Config{Endpoints: []string{\"http://127.0.0.1:2001\", \"http://127.0.0.1:2002\", \"http://127.0.0.1:2003\"}})\nAnd our balancer picker would be able to select the appropriate endpoint for each client for each request.  Our WIP approach has been instantiate a resolver per \"clientv3\" instance.\nUsing a balancer per \"clientv3\" almost works, except we have the Register / Get issue we originally opened this issue about:\n```\nr1 := NewEtcdResolver(\"etcd-client1\", endpoints1) // creates and registers the resolver on the fly\nb1 := NewEtcdBalancer(\"etcd-client1-balancer\") // creates and registers the balancer on the fly\ngrpc.DialContext(ctx, \"etcd-client1:///http://127.0.0.1:1001\", grpc.WithBalancerName(\"etcd-client1-balancer\"))\nr2 := NewEtcdResolver(\"etcd-client2\", endpoints2)\nb2 := NewEtcdBalancer(\"etcd-client2-balancer\")\ngrpc.DialContext(ctx, \"etcd-client1:///http://127.0.0.1:2001\", grpc.WithBalancerName(\"etcd-client2-balancer\"))\n```\nIf we were to instead use a single balancer, I'm not sure how to set things up properly. We can't just do:\n```\ninit() {\n  balancer.Register(NewEtcdBalancer(\"shared-balancer\"))\n}\nr1 := NewEtcdResolver(\"etcd-client1\", endpoints) // creates and registers the resolver on the fly\ngrpc.DialContext(ctx, \"etcd-client1:///http://127.0.0.1:1001\", grpc.WithBalancerName(\"shared-balancer\"))\nr2 := NewEtcdResolver(\"etcd-client2\", endpoints) // creates and registers the resolver on the fly\ngrpc.DialContext(ctx, \"etcd-client2:///http://127.0.0.1:2001\", grpc.WithBalancerName(\"shared-balancer\"))\n```\nBecause this results in two resolvers sending different sets of endpoints in NewAddress calls to a single balancer.\nIs there a way to do this with a statically defined balancer that I'm missing or do we need to register balancers on the fly here?. Thanks @lyuxuan that makes a lot of sense, I'll adjust our implementation to use the builders properly, that should solve my problem.. Resolved issue by registering the balancer statically in an init function.. Thanks @dfawley I'll have a look at interceptors, and sync back with you about retry support periodically, we'd like to switch over to that once it becomes available.. Nvm. Looks like this is fixed by https://github.com/grpc/grpc-go/commit/8f06f82ca394b1ac837d4b0c0cfa07188b0e9dee on head. We'll try the next release when it's published.. url.Parse also fails on unix:///tmp/etcd-unix-so-325611655. ",
    "dnephin": "I've updated 2 of 3. I'll try and get the last one sometime soon.. The build failure on go1.9 seems unrelated, but I'm not sure. I think this is ready for a review. Fixed the conflicts, build is green. ",
    "cdelguercio": "So my use case outlined in the Envoy issue is that I have open streaming gRPC connections to clients who often disconnect ungracefully. Because Envoy doesn't proxy the keep alive frames I am then left with hanging connections. It seems hacky that there is no good way for gRPC to manage its own streaming connections when it is being proxied. Right now I am considering making my streaming connection bidirectional and making my client->server payload a oneof with an initial message or a heartbeat. This feels like an awful solution. Am I missing another gRPC pattern that could be put to use in this situation?. Ah so here's the next problem. When a client ungracefully disconnects, triggering a Send() doesn't cause an error. Here's my code:\nmyProto := &pb.MyProto{}\nerr = stream.Send(myProto)\nif err != nil {\n    fmt.Printf(\"Stream failed: %v\", err)\n    return err\n}\nI execute that code every 15 seconds on the server and it never causes an error.\nP.S.: When I bypass Envoy it Succeeds 2 extra times before it fails with \"Stream failed: rpc error: code = Canceled desc = context canceled\". I've turned off retries on Envoy and it has no effect.. Envoy never notices the failure. Only when I bypass Envoy do I get a delayed response to the dropped connection.. ",
    "mpuncel": "Is RST_STREAM the mechanism used by grpc to cancel a stream (e.g. when you cancel a context.Context)?. closed because now my understanding is that the entire Balancer API is experimental, but if using it, WithBalancerName() is preferred. I suspect that setting WaitForReady(true) in the call options will effectively prevent this issue for me, but I'm curious if there is a way to get more information about why all SubConns would be in the failure state and wondering if there's a lurking bug because there seems to not be any connection errors. ",
    "groob": "CLA signed. . Yes, @menghanl I'm willing to update the PR to make it compatible with 1.6+ by adding that function. That's the feedback I was looking for :). Updated to account for 1.6 vs 1.7+. ",
    "cmceniry": "CLA looks good to go.. ",
    "zolotov": "CLA is completed. ",
    "cristiangraz": "@MakMukhi Thank you! Removing the client keepalive params seems to have resolved the issue. I'll make sure those match up with the server if I change those in the future. This was even clearly mentioned in the docs \ud83e\udd26\u200d\u2642\ufe0f . @lyuxuan Can you elaborate or provide an example on what you mean by \"aggressive logging and not a real connection issue\". The logs I'm seeing our written by our application when a grpc request fails.\nWhen we encounter err != nil in the grpc requests, we log the error in our application with a message e.g. \n\ntimestamp=2018-07-02T16:09:32.012697389Z level=error message=\"cannot find orders\" context=order:get_all error=\"read tcp [redacted]->[redacted]: read: connection reset by peer\"\n\n(I omitted other fields like request id, user id, etc)\nRecently if I'm in our product's dashboard and encounter a 500 error (what we return for various unexpected error messages), I can often pull up the logs and find an error similar to the one above linked to the request id in my browser. I often have to retry 1-2x for the request to go through.\nWith the pub/sub workers when the error occurs we log it in the same fashion as above but also have to let the message retry using our exponential backoff (1-2x) because the job wasn't completed due to the error.. Thanks @lyuxuan. The info about the rpc error piece was actually really helpful. Turns out this is not a grpc error but is happening as part of a grpc request (where the work is done). Appreciate all of your help, going to close this one out as it's not an issue with this library.. ",
    "willhug": "cc @peter-edge, @abhinav. @MakMukhi this may actually not be necessary.  Just modified the client function to have a time.Sleep(time.Minute) at the end of the main, and all the errors turned into io.EOF.\nThis looks like it might have been legitimately \"the client doesn't exist anymore\" context canceled.  Still weird, but likely the right thing in most cases.\nI think my problem was invalid, going to close.. \n. oh cool! thanks for the tip!. We should lowercase the codec string right?  Or will that get handled automatically?. we might want to replace t with contentType... just to be super explicit. can we verify this now?. So now default outbound clients will set the content-type header to application/grpc+proto as opposed to application/grpc?  Should we default to leaving the old functionality unless one of the calloptions (codec or contentSubtype) are set?. ",
    "aequitas": "I think I am stumbling on this. I try to get a embedded Etcd server running over unix domain sockets in (https://github.com/purpleidea/mgmt/) but running into this error:\nWARNING: 2018/02/20 13:24:57 grpc: addrConn.createTransport failed to connect to {clients.sock:0 0  <nil>}. Err :connection error: desc = \"transport: Error while dialing dial tcp: lookup clients.sock: no such host\". Reconnecting...\n\nI have boiled it down to: https://github.com/grpc/grpc-go/blob/3926816d541db48f3e4c1c87cff75ceeb205309e/clientconn.go#L438\nWhere a TCP dialer is created if none exists yet, even though a correct dialer is passed in via: https://github.com/coreos/etcd/blob/3903385d1b50c26ec0c18e99fc55be33ee0d97e3/clientv3/client.go#L272 afaict, but my go knowledge is yet lacking in this part.\nChanging this from \"tcp\" to \"unix\" makes it work, but obviously breaks TCP support. I have not yet found a way to properly inspect the opts passed into DialContext in order to make a logical switch here, also target is passed in without scheme by Etcd, but adding it makes no difference. \nA similar error message is generated when running Etcd directly using unix sockets:\n# etcd --listen-peer-urls unix://etcd:0 --listen-client-urls unix://etcd:1 --advertise-client-urls unix://etcd:1\n\nWARNING: 2018/02/20 13:34:20 grpc: addrConn.resetTransport failed to create client transport: connection error: desc = \"transport: Error while dialing dial tcp: lookup etcd on 10.5.8.1:53: no such host\"; Reconnecting to {etcd:0 0  <nil>}\n\nIs this due to lack of support for unix domain sockets in grpc? Or is Etcd/mgmt invoking grpc wrongly in this point?. @menghanl thanks for the quick response.\nAfaict Etcd passes only the 'host:port' part to the DialContext. When giving this url to Etcd: unix://etcd-address:0 only etc-address:0 is passed (etcd requires host:port format even for unix adresses due to input validation, the socket filename created will be host:port in the current directory). \nI tried adding the unix:/// and passthrough:///unix:// prefix to host on this line: https://github.com/coreos/etcd/blob/master/clientv3/client.go#L352 and sctx.addr in https://github.com/coreos/etcd/blob/master/embed/serve.go#L195 but it did not completely solve the problem. It has now moved from resetTransport to createTransport:\nWARNING: 2018/02/20 20:46:23 grpc: addrConn.createTransport failed to connect to {clients.sock:0 0  <nil>}. Err :connection error: desc = \"transport: Error while dialing dial tcp: lookup clients.sock: no such host\". Reconnecting...\n\nI missed the difference in these error messages when debugging earlier, and also that there was a second entrypoint in etcd/embed/serve.py so I will pick up my investigation with the information. If you have any more pointers I'm glad to know.. @menghanl will do, thanks for the help so far.\n@gyuho sure thing: https://github.com/coreos/etcd/issues/9340\n. ",
    "notbdu": "Thanks @dfawley for the quick response and feedback. . ",
    "josephusmv": "hi, dfawley, \nvery glad to see there are go grpc developers agree that SendAndClose is poorly named. And I think this lead to some confusion in my application level codes and engineers who read my codes complain about it. So I opened an information issue #2156 to ask about whether you have any plan to do some compensation about it. Please have a look.\nthanks. ",
    "weiyougit": "your attempt seem to reproduce the issue, \"Server exited\" is never printed. . yeah I tried the code with the second commit, and the program still hangs. Does your repo include this change? https://github.com/grpc/grpc-go/pull/1745. sounds good, thanks for the prompt replies. . signed the CLA. Thanks for your review. #1663 is clear enough, so I will close this PR. . Thanks for your reply. It makes sense. I will just implement my own resolver. . ",
    "slomek": "@dfawley I just did.. ",
    "RichiH": "@fhibler - it's fixed.\n@MakMukhi / @viru - thanks!. @wladh Ah, thanks for clearing that up.. ",
    "wladh": "@RichiH I think you misunderstood @viru\u2019s message. He meant that the version that was just released (1.9.0) is affected by this issue, not that it\u2019s fixed in that version. . ",
    "robotlovesyou": "@menghanl Thanks for pointing me towards the correct documentation. Using the \"dns:///\" prefix removes the need to call resolver.SetDefaultScheme\n  . Thanks. ",
    "dikshantad": "I can perhaps take a look at this but I am new to gRPC and only an intern trying to fix some bugs. So it might take a while.. This has since been resolved by PR #1840 . ",
    "bradylove": "@dfawley thank you for your feedback.\nThe reason I want to avoid reflection is because it is not currently enabled on the servers (now wishing we had enabled it already).\nWith your feedback I have this working with the following:\n\nOpen stream to new API\nCall CloseAndRecv()\nCheck status code.\n  a. If status code == 12, open stream deprecated API\n  b. If status code == 0, open new stream to new API\n\nDo you see any issues with this approach?\n. Terribly sorry, we had apparently added that as a dependency in our code and I have removed it. Thanks for your help.. ",
    "blairkutz": "Signed up to CNCF. ",
    "vadimi": "I'm having exactly the same issue with dns resolver. If my VPN connectivity drops for some time it takes about 30 mins for my client to reconnect. I'm seeing this line in the log and after that ClientConn state changes to IDLE:\nINFO: 2018/06/21 00:30:25 ccResolverWrapper: sending new addresses to cc: []\nWhen I use passthrough scheme reconnects happen way faster.\nIs there a plan to fix \"1. The resolver keeps retry itself\"?. ",
    "zhexuany": "We already upgrade to latest grpc and high throughout/ performance has been proven. Closing this issue since it is not relevant anymore. Thanks again !!! . I did miss you last post. Thanks for pinging me. My hands are tied, I will reply you next week. Sorry for the delay. . Closing due to problem is not existed in latest grpc. Thanks for you work! . ",
    "skipor": "Nice to Have, for Interceptors redesign: extract context.Context from ServerStream and pass it as the first argument in stream interceptor.\nNow every context.Context wrapping requires also ServerStream wrapping, which is too complex. In my experience Context wrapping with value or timeout is a very common pattern, and it would be really nice to use it as everywhere else.\n. That was transport.StreamFromContext(ctx context.Context) (s *transport.Stream, ok bool). It was right desition to remove it, because it was using transport.Stream, but not new grpc.ServerTransportStream. ",
    "hashstone": "thx!. ",
    "euroelessar": "Hi,\nDue to internal optimizations we can't serialize all options to a string due to shared data structures between different resolver objects.\nExposing WithResolver instead of WithResolverUserOptions is fine for us though, our Build function looks like the following anyway:\nfunc (*builder) Build(..., opts resolver.BuildOption) (resolver.Resolver, error) {\n  return opts.UserOptions.(resolver.Resolver), nil\n}\nThere is slight chance though that we can solve this issues on balancer level but I have to evaluate it properly first for all our use cases.. > That's pretty vague. Is there a way to describe the use-case such that we could make design decisions?\nSure thing:\n1. We use it mostly for sharding (there are several thousands of shards per service)\n2. Process initiates single streaming rpc to resolver server asking for (streamed) mapping from shard to list of associated endpoints.\n3. Based on the response we create resolver for every individual shard which is later used to initialize per-shard grpc clients, effectively allowing us to re-utilize single request with client streaming to resolver server for all this clients.\n4. Later each client makes its own balancing decisions, in some cases custom balancer is used.. > How do you determine which shard to use for an RPC? Can it be semi-automatic, e.g., by looking at a field of the request?\nApplication logic usually splits original request to per-sharded ones and potentially is capable to pass this information further in stack using context or some other mean. It's usually non-trivial to determine actual shard by just looking on the request as it involves some computation & knowledge about topology.\n\nI don't quite understand your (4), since I don't know why/how some shards would be \"special\" to receive custom configuration. Unless that was also received in your (2)?\n\nOne of the examples is graceful deployment process - client should prefer opening new transactions on new endpoint while allowing transactions on old endpoint to be finished. Balancer is currently stateful to determine which endpoint is new and which is old.. Oh also, regarding two - it's not some shards are special, but some services. (i.e. all shards for one service have the same balancer logic). @dfawley hi, can you review please? can we also include this fix into upcoming 1.11.1 fix release (if any)? I don't feel comfortable upgrading without it.. thanks!. I've used string([]byte{0xff, 0xfe, 0xfd}) as a message, issue can be reproduced by altering SayHello method of greeter server from grpc-go examples: https://gist.github.com/euroelessar/dde1d70cf8e8fe8dd27a81d6b2b9025f\nHere is a panic stack trace caused by non-utf-8 message:\n```\npanic: proto: invalid UTF-8 string\ngoroutine 10 [running]:\ngoogle.golang.org/grpc/transport.(http2Server).WriteStatus(0xc420172000, 0xc4201520f0, 0xc4201be018, 0xc420184001, 0xc420182070)\n    /Users/elessar/go/src/google.golang.org/grpc/transport/http2_server.go:763 +0x141c\ngoogle.golang.org/grpc.(Server).processUnaryRPC(0xc420154000, 0x1742a60, 0xc420172000, 0xc4201520f0, 0xc42007dc80, 0x17752a0, 0x0, 0x0, 0x0)\n    /Users/elessar/go/src/google.golang.org/grpc/server.go:1023 +0xe65\ngoogle.golang.org/grpc.(Server).handleStream(0xc420154000, 0x1742a60, 0xc420172000, 0xc4201520f0, 0x0)\n    /Users/elessar/go/src/google.golang.org/grpc/server.go:1249 +0x1528\ngoogle.golang.org/grpc.(Server).serveStreams.func1.1(0xc420018750, 0xc420154000, 0x1742a60, 0xc420172000, 0xc4201520f0)\n    /Users/elessar/go/src/google.golang.org/grpc/server.go:680 +0x9f\ncreated by google.golang.org/grpc.(*Server).serveStreams.func1\n    /Users/elessar/go/src/google.golang.org/grpc/server.go:678 +0xa1\n```\n\nIIUC, encoded messages should all be ASCII. It's not clear to me how this will result in violation of http2 protocol. An example would help here.\n\ngRPC HTTP2 protocol is a bit more precise:\n\nThe value portion of Status-Message is conceptually a Unicode string description of the error, physically encoded as UTF-8 followed by percent-encoding.\n\nBut grpc-go doesn't perform any validation of message returned by application handler and therefore sends ASCII encoded string followed by percent encoding which is not always a valid UTF-8 string.. @MakMukhi hi, any chance this can be part of patch release?. that works for me, thanks!. Currently the difference is 40MBps vs 100MBps for cross-zone traffic (\u2248150% delta), and 300MBps vs 450MBps for traffic within (\u224850% delta). But within a metro we're likely hitting CPU utilization bottleneck at 450MBps, as iperf can do ~1.2GBps over single TCP connection.\nIt does worth noting that this numbers are for gRPC over TLS connection.\nRegarding iperf test:\nserver: $ iperf iperf -s -p 6000\nclient: $ iperf -c ${remote_server} -p 6000 -P 1\noutput:\n[  5] local $my port 6000 connected with $remote port 26460\n[  5]  0.0-10.1 sec  1004 MBytes   838 Mbits/sec\n[  4] local $my port 6000 connected with $local port 51102\n[  4]  0.0-10.0 sec  10.7 GBytes  9.22 Gbits/sec. Sure,\n$ cat /proc/sys/net/ipv4/tcp_rmem\n4096    135168  16777216. It causes over-reporting of errors, but nothing is majorly broken.. @jhump @dfawley this looks like data race in case of concurrent access with non-empty opts ...CallOptions as they override the same elements in cc.dopts.callOptions (given sufficient capacity). Small note here, in fact gRPC allocates buffer of WriteBufferSize * 2 size, is it expected and maybe we should update documentation?. ",
    "henmja": "Fixed by updating golang version. ",
    "benlangfeld": "Thanks dfawley. I apologise, I got mixed up while tracking this issue down and thought this was an issue on kubernetes/helm, then deleted my comment when I realised my mistake.\nThanks for the pointers!. ",
    "joyzheng": "My bad, I was walking through this again and realized that the delay is due to our backoff logic on failed grpc requests, rather than due to the grpc client not timing out. \nThanks for taking a look!. ",
    "xiaoyulei": "OK, I get the reason, it is my program, not grpc. ",
    "upeediak": "This is the first link coming up when searching for the error. @YuleiXiao if you don't mind can you share what was the issue and fix in your case?. ",
    "NIPE-SYSTEMS": "@upeediak I had this error too because I used the wrong address for grpc.Dial(). At the wrong address is also a gRPC-server located but with an other service so that's where my error comes from. This helped: https://github.com/brocaar/lora-app-server/issues/100#issuecomment-325288552. ",
    "go-fish": "i found the problem, sorry for the issue. ",
    "dtjm": "Thank you for the quick response.. ",
    "etdub": "Thanks for the feedback! I've incorporated your suggestions in the latest commit.. ",
    "Richard87": "I guess this is a problem that we shouldn't connect to a server with the IP 0.0.0.0 :). ",
    "tux21b": "@dfawley thanks for your detailed answer. The passthrough resolver seems like a good solution for the meantime. Many thanks for the tip!. ",
    "thurt": "yes i think so!\na public API for injecting the stream into a context, and then have the ability to write a fake implementation for a stream interface.\n. ",
    "dim13": "Note: after further investigations, not a message size, but rather non-zero processing time on servers side caused deadlock occurring.. ",
    "wanghaiyang1930": "We could not visit google.golang.org/grpc (China....crying...). ",
    "mgsouth": "\nSelects are inherently non-deterministic, as races between the incoming signals will determine the result.\n\nTrue, but the (reasonable) expectation which the current code invalidates is:\nFor any particular goroutine, or set of goroutines properly synchronized:\n1. Once get() returns wc.ctx.Err() it always returns wc.ctx.Err()\n1. Once get() returns ErrConnClosing it always returns ErrConnClosing, or the above\n1. Once get() returns io.EOF it always returns io.EOF, or one of the above\n1. Once get() returns errStreamDrain it always returns errStreamDrain, or one of the above\n1. Otherwise it always returns a non-zero quota, or one of the above\nThere are many selects which behave in this fashion. Does that cause any particular problems? Don't know\u2013that would require a careful inspection of all of the code. Even if it currently doesn't, it seems to be brittle, and every code change would require the same inspection.\n. ",
    "Asarew": "I just ran into the same 'issue' as @alecthomas. i've build a custom naming resolver and passing all configuration as a naming.Target is not only tedious but also blocks use cases. The current WithBalancerName api is very limiting in flexibility compared with the already deprecated WithBalancer DialOption, I really would like to see a similar solution for configuring a per connection naming.Resolver.. ",
    "pohly": "Thanks for pointing out #1485. However, I don't think it addresses this particular problem. I had had a look at master and while the code was indeed a bit different, the underlying problems still seemed to be there. I think what happens is this:\n\na Server instance with empty s.lis is created (https://github.com/grpc/grpc-go/blob/master/server.go#L334)\na goroutine with Serve() is created, but does not run; the new listener socket is bound to the goroutine\nStop() is called and sets s.lis to nil (https://github.com/grpc/grpc-go/blob/master/server.go#L1189)\nbecause s.lis was still empty, Stop() does not close any socket and returns\nthe goroutine calls Serve() with  the listener socket\nServe() detects that the server is meant to shut down and quits after closing the socket (https://github.com/grpc/grpc-go/blob/master/server.go#L474)\n\nOne can argue that Server works as intended. But then how can a user of  the Server prevent this unexpected behavior, given the current API?\nI suspect that it might require an API change (something along the lines of a non-blocking Server.StartServe which takes over the socket, then starts a goroutine under the hood and returns; when Close is called, it can close that socket before returning). But before speculating about a solution, let's clarify whether we agree that there is a problem ;-}. On Tue, 2018-02-13 at 09:24 -0800, dfawley wrote:\n\nThe 1.7.x branch is sufficiently old that we would rather not back-\nport a fix to it. Are you able to update gRPC in Kubernetes?\n\nWhile I don't think that it would help in this case (see my other\ncomment), this is something that might be worthwhile by itself.\nI'm new to Kubernetes, so I'll have find out what the process is for\nupdating components like gRPC.\n. On Tue, 2018-02-13 at 21:15 +0000, dfawley wrote:\n\nIt seems like you should be able to synchronize your test code such\nthat Serve() is not called if Stop() has already been called, if you\ndon't want the listener to be closed.\n\nThe listener needs to be closed, but it has to be closed in the main\ngoroutine before Close() returns. Then once Close() is done, the code\nthat follows in that goroutine can continue to reuse the path.\nThey key point is that a Unix domain socket exists as both a file\ndescriptor and as an entry in the filesystem. It's the filesystem entry\n which needs to be managed carefully, because it is possible to remove\nthat entry even if the code doesn't own the corresponding file\ndescriptor.\nLet me link to the code:\n- start server: https://github.com/kubernetes/kubernetes/blob/f88c4c0b5e95eb2c559e2a00c80f7a9dee902d38/pkg/kubelet/cm/deviceplugin/manager.go#L217\n- stop server: https://github.com/kubernetes/kubernetes/blob/f88c4c0b5e95eb2c559e2a00c80f7a9dee902d38/pkg/kubelet/cm/deviceplugin/manager.go#L303\n- test code calling start/stop (indirectly through setup/cleanup): https://github.com/kubernetes/kubernetes/blob/f88c4c0b5e95eb2c559e2a00c80f7a9dee902d38/pkg/kubelet/cm/deviceplugin/manager_test.go#L52\nThis all looks rather straightforward, and yet there is this subtle\nrace that I explained earlier where Server.Stop() doesn't actually stop\n everything.\n\nOr you could re-initialize your socket for every test case.\n\nIt does get re-initialized. The problem is that TestNewManagerImplStart\ncreates /tmp/device_plugin/server.sock, then \nTestDevicePluginReRegistration creates another one under the same name,\nbut before that one can get used, the remaining goroutine from\nTestNewManagerImplStart unlinks the socket in the filesystem, which\nmakes it impossible to connect to it.\nTo phrase it differently, the path of incarnation 2 of the socket gets\nremoved by the cleanup code for incarnation 1 of the socket.\nOne can work around that by not reusing the same socket path and I have\n  https://github.com/kubernetes/kubernetes/pull/59489 pending which\ndoes exactly that. But the race around Server.Server/Stop remains,\ntherefore  I reported it here - others might run into the same issue.\nI can imagine that some wrapper around Server can handle this issue,\nbut then other users will have to use the same wrapper code if they\nwant to avoid the race. To me that sounds like something that gRPC\nshould support itself.\n. On Tue, 2018-02-13 at 22:56 +0000, dfawley wrote:\n\nSo one fix is to manually call Close on your listener before your\ntest case exits. Then a late call to Start() with the old listener\nwould not unlink the file again.\n\nYes, that looks like the easiest solution. I hadn't considered that\nClose() can safely be called multiple times - well, for some definition\nof \"safely\". It won't crash and for this particular case (Close() after\nStop()) one can even be certain that Serve() won't fail.\nI wonder whether it is worth mentioning this in the API documentation\nfor Serve(). Calling it in a goroutine can't be that unusual. But I'm\nnew to all this, so perhaps Kubernetes really is doing something\nunusual.\n\nEither way, you should make sure your listener is closed before you\ncreate a new one with the same filename, which is the source of your\nproblem.\nI don't think we can change grpc in a way that fixes your problem. \nClose should always be called on every listener you create to avoid\nleaking resources, so if grpc didn't call Close on it, you would\nstill have to.\n\ngrpc is guaranteed to call Close(), eventually. But I can see how with\nthe current API it is the responsibility of the caller to ensure that\nthis has happened before proceeding.\n. ",
    "jiangtaoli2016": "@therc This code is for silent launch. The code is not ready for public consumption yet. Hopefully soon. Once we are beta launch, we will provide corresponding public user guide.. I am pretty sure c++ strips call credentials (Oauth creds) when communicating with the balancer.\ncc @ejona86 @zhangkun83 Could you pls confirm Java case?. ",
    "cesarghali": "Thanks for the review Menghan. All comments are addressed.\n\nReview status: 2 of 31 files reviewed at latest revision, 13 unresolved discussions.\n\ncredentials/alts/alts.go, line 22 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDeclare this package as experimental?\n\nDone.\n\ncredentials/alts/alts.go, line 53 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhat's the purpose of this flag?\nAlso, I kind of don't like the idea of adding a flag for it.\nHow about moving this to constructors NewClientALTS() and NewServerALTS() as a function parameter?\n\nAck!\n\ncredentials/alts/alts.go, line 62 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nShould we add to the comment that:\nThis interface is to be implemented by alts. Users should not need a\nbrand new implementation of this interface. For the situations like\ntesting, the new implementation should embed this interface. This allows\nalts to add new methods to this interface.\n\nDone.\n\ncredentials/alts/alts.go, line 80 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit: this still says \"Google Transport Security\".\n\nGood catch, thanks!\n\ncredentials/alts/alts.go, line 129 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMake a static variable for versions struct so we don't need to re-create it every time?\n\nDone.\n\ncredentials/alts/alts.go, line 139 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWill chs be properly closed if something goes wrong (e.g. this function returns non-nil error)?\nAlso see https://github.com/grpc/grpc-go/pull/1854 for possible leaks on streaming RPCs.\n\nDone.\n\ncredentials/alts/alts.go, line 151 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nDoes the underlying handshaker need the RPCVersions if the check is done here?\nIMO the check should happen before we create the secConn, so we can avoid the overhead in error cases.\nIt would also be a bit clearer if this check is done in chs.ClientHandshake, so we can save the type assertion.\n\nAck!\n\ncredentials/alts/alts.go, line 173 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nMake a static variable?\n\nDone.\n\ncredentials/alts/alts.go, line 183 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSimilar to client side, shs clean up on errors cases.\n\nDone.\n\ncredentials/alts/alts.go, line 195 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nSimilar to client side on where the check should happen.\n\nAck!\n\ncredentials/alts/utils.go, line 53 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nNit: rename this function to something more meaningful.\n\nDone.\n\ncredentials/alts/utils.go, line 86 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nThis fatal will print the error returned by os.Open or cmd.Output.\nDo a Fatalf with more information? Or include more information in readerFunc.\n\nDone.\n\ncredentials/alts/utils.go, line 99 at r1 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWhy is this a panic not a Fatalf?\n\nDone.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 31 files reviewed at latest revision, 4 unresolved discussions.\n\ncredentials/alts/alts.go, line 87 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nWill we want to extend the return value?\n\"account\" sounds like more than just a string to me...\nIf so, make it a struct instead?\n\nServiceAccount is just a string, usually it's an email address or some other identifier.\n\ncredentials/alts/alts.go, line 91 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAll functions in this interface will be exposed to users, including this one.\nThis function is needed by checkRPCVersions().\nWill users also need this? If no, this should be moved to a separate (probably also unexported) interface.\n\nAll the functions in this interface might be used for the user for logging purposes or to check whether a specific RPC was actually executed as expected. We need all of them to be exported.\n\ncredentials/alts/alts.go, line 131 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nShould this be moved to newALTS()?\n\nnewALTS() also reads *enableUntrustedALTS\ninit should be done in the \"package entrance\", which should be newALTS() for this package.\n\n\nGood call. Done.\n\ncredentials/alts/utils.go, line 78 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nAlso move this into the sync.Once?\nThis and flag.Parse() are all part of the initialization process.\nOtherwise if users import but don't use alts, they may still get Fatal.\n\nI'm not sure I'm following. Do you mean that calling isRunningOnGCP should be called in a once.Do? Should this be in init()?\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 31 files reviewed at latest revision, 4 unresolved discussions.\n\ncredentials/alts/utils.go, line 78 at r3 (raw file):\nPreviously, cesarghali (Cesar Ghali) wrote\u2026\nI'm not sure I'm following. Do you mean that calling isRunningOnGCP should be called in a once.Do? Should this be in init()?\n\nisRunningOnGCP was actually in init() but now I remember why I moved it here, it was one of Doug's suggestions :)\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 31 files reviewed at latest revision, 1 unresolved discussion.\n\ncredentials/alts/utils.go, line 78 at r3 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nI meant moving this to the same sync once for flag.Parse(). Because we don't need to do this if no alts functions are ever called.\n\nDone.\n\nComments from Reviewable\n Sent from Reviewable.io \n. \nReview status: 0 of 31 files reviewed at latest revision, 1 unresolved discussion.\n\ncredentials/alts/alts.go, line 117 at r5 (raw file):\nPreviously, menghanl (Menghan Li) wrote\u2026\nHmm, this is not thread safe... newALTS could be called in parallel.\n\nMy understanding is that Do blocks if it's called multiple times and only the first time will trigger the function:\nhttps://golang.org/src/sync/once.go?s=1137:1164#L25\n\nComments from Reviewable\n Sent from Reviewable.io \n. Thanks for the comments Doug.\nRegarding the flag, there was a discussion about it in #1865. I'm happy to discuss it more offline and we do further changes in a separate PR.. Thanks for the PR Menghan. Can you please also copy the other fields, accounts and side?. I can think of two scenarios:\n\nReloading the credentials without reconnecting: (@dfawley correct me if I'm wrong) this cannot be done today with the current gRPC code because it requires a new TLS handshake. Note that if this is a server, all connections to all clients need to be re-handshaked.\nReloading the credentials without with reconnecting but without terminating and rerunning the application binary: this can be done using the Go TLS library. Depending on your code but you might be able to do something like the suggestion in this post.. Taking a look into this. Thanks for CCing me Doug!. Yes I think @apolcyn can test this after merging.. My bad, this was suppose to be \"failed to call\". However, I changed the text since operateHeaders is not actually failing here. Instead, it's just logging the error. Thanks for the suggestion.. Done!. That's a good point. Consts are renamed.. Done!. My bad. Done.. I don't even know how this code ended up with two switches :)\nFixed now.. Thanks Doug. I just rebased this PR. It's ready now.. #1906 is merged. Please rebase this PR.. This is not commonly used. I'm expecting callers will set this in very few situations.. Done!. Done.. Done.. Done.. Changed to pointer.. I prefer to keep this function here instead of the test.. I also prefer keeping this here.. Can you please add a comment that Google Default Credentials are only use by clients, so we only create a new ALTS client creds, or something like that.. Do we need to prefix the error message with something like google default creds:, similar to the other error before.. Same RE the prefix.. How about changing the name to something like WithCredentialsBundle, or something more expressive. WithCreds might be a bit ambiguous about the creds type, e.g., the above API is for transport credentials.. Maybe call this transportCreds?. ALTS does the check. However, would oauth.NewApplicationDefault work properly if we're not on GCP?. Sounds good, thanks for checking.. \n",
    "prasek": "@awalterschulze, I'm porting github.com/jhump/protoreflect to gogo and have an experimental branch with a working protoer interface along the lines of what @johanbrandhorst suggested with zero dependencies: https://github.com/prasek/protoreflect/blob/protoer/proto/protoer.go\nThe gogo and golang wrappers implement UntypedProtoer\nhttps://github.com/prasek/protoreflect/blob/protoer/proto/gogo/protoer.go \nhttps://github.com/prasek/protoreflect/blob/protoer/proto/golang/protoer.go\n```go\ntype Protoer interface {\n    MessageType(name string) reflect.Type\n    MessageName(pb Message) string\n    FileDescriptor(file string) []byte\n    Unmarshal(b []byte, pb Message) error\n    Marshal(pb Message) ([]byte, error)\n    GetExtension(pb Message, field int32) (extval interface{}, err error)\n    EnsureNativeMessage(pb Message) (pbout Message, err error)\n}\ntype Message interface {\n    Reset()\n    String() string\n    ProtoMessage()\n}\ntype UntypedProtoer interface {\n    MessageType(name string) reflect.Type\n    MessageName(pb interface{}) string\n    FileDescriptor(file string) []byte\n    Unmarshal(b []byte, pb interface{}) error\n    Marshal(pb interface{}) ([]byte, error)\n    GetExtension(pb interface{}, field int32) (extval interface{}, err error)\n    EnsureNativeMessage(pb interface{}) (pbout interface{}, err error)\n}\n```. @awalterschulze, EnsureNativeMessage should really be called NativeDescriptor. There are two use cases for NativeDescriptor, one internal to the wrappers to ensure GetExtension() and RegisteredExtensions() get types they recognize, and the other allows libraries to accept either gogo or golang descriptor types by converting them with NativeDescriptor of a separate protoer instance that matches the package used by the library. \nFor example https://github.com/prasek/protoreflect/blob/protoer/desc/builder.go#L19 (experimental branch) accepts gogo and golang descriptors for some methods but internally has to pick one to use. NativeDescriptor() checks the descriptor type (gogo or golang) and converts it if not the desired type so it will work with descriptor types (gogo or golang) used by the library. The downside to this approach is descriptors are exported as Message and have to be type asserted by the caller, so I'd be fine removing NativeDescriptor from the Protoer interface and just using it internally in the wrappers.. @johanbrandhorst, ideally the packages would get split into grpc, golang/protobuf, and gogo/protobuf respectively. There are no dependencies across the packages.\n\ngrpc: https://github.com/prasek/protoreflect/blob/protoer/proto/protoer.go\ngolang/protobuf: https://github.com/prasek/protoreflect/blob/protoer/proto/golang/protoer.go\ngogo/protobuf: https://github.com/prasek/protoreflect/blob/protoer/proto/gogo/protoer.go\n\nThe only difference between the gogo and golang protobuf protoer are the imports and default aliases.. Here are the updated interfaces with RegisteredExtensions added and NativeDescriptor removed.\nhttps://github.com/prasek/protoreflect/blob/protoer/proto/protoer.go\n```go\ntype Protoer interface {\n    MessageType(name string) reflect.Type\n    MessageName(pb Message) string\n    FileDescriptor(file string) []byte\n    Unmarshal(b []byte, pb Message) error\n    Marshal(pb Message) ([]byte, error)\n    RegisteredExtensions(pb Message, desiredType interface{}) (extensions interface{}, err error)\n    GetExtension(pb Message, field int32) (extval interface{}, err error)\n}\ntype Message interface {\n    Reset()\n    String() string\n    ProtoMessage()\n}\ntype UntypedProtoer interface {\n    MessageType(name string) reflect.Type\n    MessageName(pb interface{}) string\n    FileDescriptor(file string) []byte\n    Unmarshal(b []byte, pb interface{}) error\n    Marshal(pb interface{}) ([]byte, error)\n    RegisteredExtensions(pb interface{}, desiredType interface{}) (interface{}, error)\n    GetExtension(pb interface{}, field int32) (extval interface{}, err error)\n}\n```\nNote for RegisteredExtensions() the desiredType is needed to check and convert if needed.\ngo\nextensions, err := proto.RegisteredExtensions(pb, (map[int32]*gogo.ExtensionDesc)(nil)). ",
    "dsnet": "A protobuf reflection API that has Unmarshal and Marshal in its interface is probably too high level. Just as Go reflection is abstraction around the Go language itself, protobuf reflection should be an abstraction around the protobuf language itself. As such, it needs to go even lower-level than Unmarshal and Marshal and provide behavior at the field level in order to be reflexive over all aspect of the proto language.\nAdding reflection functionality into the mainline golang/protobuf repo has been a project I've been working on for some time now and turns out to be a difficult task as there are many devils in the details. A reflection API needs to thought in the context of the totality of how Go protobufs operates. As I've been working on a reflection API, I've had to scrap my design and start over many times.\nI recently finished writing up the design and published some documents regarding the design: https://github.com/golang/protobuf/issues/364#issuecomment-384123374. ",
    "steve-gray": "@snowzach - I've implemented a change because I've observed the same behaviour - hoping it gets picked up. What I found was implementing locking around Send() bottlenecked because of the compression and serialisation in send - I found this particularly vexing for high volume bi-directional streaming. The change is at #2356 (Issue #2355 ) if you're keen to take a look and see if there's any commonalities in our respective situations.. For the sake of a single mutex at the stage where it interacts with the stream, that can be resolved - doing this now myself in code around Recv and Send calls - but that means stuff that doesn\u2019t need to be serialised (ie: marshalling/unmarshal) is also inside the block, whereas if this was done on the GRPC package the scope could be minimised\nGet Outlook for iOShttps://aka.ms/o0ukef\n\nFrom: Scott Moeller notifications@github.com\nSent: Monday, May 21, 2018 1:19:29 AM\nTo: grpc/grpc-go\nCc: Steve Gray; Author\nSubject: Re: [grpc/grpc-go] SendMsg Method Not-Concurrency Safe (#2094)\nFWIW I ran across your report while troubleshooting another proximal issue - the documentation for\nSendMsg indicates it is NOT safe to simultaneously use from two goroutines:\n// It's safe to have a goroutine calling SendMsg and another goroutine calling\n// recvMsg on the same stream at the same time.\n// But it is not safe to call SendMsg on the same stream in different goroutines.\nhttps://godoc.org/google.golang.org/grpc#Stream\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHubhttps://github.com/grpc/grpc-go/issues/2094#issuecomment-390490006, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AN45vx0Zap2iHb8C4ItWVOUhjmucyu4mks5t0YmBgaJpZM4UFpA0.\n. @suyashkumar - The change in question is to only mutex around the send itself to the underlying stream, there is no concurrency guard requirement at the compress/encode phases, which is why it's positioned further down the chain. The work in #1879 does permit separated encoding/compression, but at considerable conceptual cost to both sides of the handling.\n@dfawley - seems like an odd position to take, given that the issue only exists because of mis-handing of concurrent operations against network buffers.\n\n\nOrder may matter, but it depends on use case - 'Unless you serialize calls, concurrent calls may arrive in an undefined order' - this is consistent with other language patterns and this allows scenarios such as order-required (serialise as today/no-change), order irrelevant (simply call in parallel), or semi-ordered (divide and serialise by some key). For example, if you publish in parallel to Kafka via Sarama, you get this behaviour.\n\n\nThe cost of the mutex is a single atomic memory check - and running the benchmarks before/after showed no appreciable cost to this - both with and without compression. What did you run to observe a meaningful impact so I can re-validate?\n\n\nAre you able to articulate some more detail on this? It appears to defy my own understanding and experience, as well as the other changes in the issue you mentioned. With this change and calling from multiple routines, serialisation and compression can be run parallel - the locking is only to guard the HTTP/2 stream going out, and is sufficiently late in the process that instead of saturating a single core, I was able to saturate connections with both large and small messages, whereas previously enabling compression eventually consumes a core and caps throughput, and without it the ceiling is still there, but naturally higher, owing to the proto serialisation.\n\n\nIn terms of fairness, leaving this undefined at a specification level is simple enough - the scenario you're proposing is an artificially complex one - and its an application level consideration if they want to multiplex concurrent sets of data into a single stream - and they can still achieve that by not multiplexing writes on the producer side (i.e. nothing breaks)\n\n\nThe workaround in #1879 puts enough of the framework level overhead into the per-application logic, and renders it difficult to maintain a cross-language producer/consumer - as now you're bypassing core features of the framework. Today the framework hangs, deadlocking the callee goroutines if you perform concurrent writes with no application level way to recover or even differentiate this has occured.. @thelinuxfoundation CLA signed.. ",
    "srini100": "Consolidating streaming API documentation issues in https://github.com/grpc/grpc-go/issues/1894 and closing this one.. Using this bug to consolidate issues regarding documentation around streaming API. \nhttps://github.com/grpc/grpc-go/issues/1880\nhttps://github.com/grpc/grpc-go/issues/1876. When the issuer and subject domains don't match, an issuer CA cert is needed to validate the cert. Unlike Python API, the Go API doesn't take cert chain as a parameter. Perhaps the Go SSL lib is using the system store and in this case finds the appropriate CA cert while Python depends on CA cert to passed in when issuer and subject names don't match.\ncc/ @jiangtaoli2016, any thoughts?\n. @spl0i7, can you confirm if you have the issuer CA cert in your store that Go client may be using for successful handshake? See the above comment.. Go crypto lib allows partial cert chain validation. This allows a client trusted cert to be used as trust anchor without the need to chain back to root CA cert. gRPC Python is using BoringSSL default that doesn't allow this. Closing this issue as Go behavior is correct. Python behavior is being improved in https://github.com/grpc/grpc/pull/17868. ",
    "PraserX": "Similar/same issue here. We've got a one client, one 'relay' server and one server. If we create many concurrent requests (thousands as its mentioned above), we got same error on client:\nrpc error: code = Unavailable desc = transport is closing\nThanks to @MakMukhi\n\n...  try turning it off by setting WithInitialWindowSize and WIthInitialConnectionWindowSize dial options on the client.\n\nThat solve our problem.. @MakMukhi It seems that it is working perfectly. Thank you for response.. ",
    "hexfusion": "Please excuse retests but I seem to be getting inconsistent results with the same code.. @menghanl I made a few updates per your recommendations please let me know your thoughts. If this seems reasonable I will add a few tests. Thanks!. @menghanl kind ping.. @dfawley understood, appreciate the followup.. @menghanl per comments by @dfawley should we close this?. @menghanl, thank you for the review I really appreciate it. This gives me enough to run with it.. @jpbetz thanks for the notes:\nhttps://tools.ietf.org/html/rfc3986#appendix-B  defines this regex.\n^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?\nSo if we did something like\n```golang\ntype TargetInfo struct {                                                                                                        \n    Scheme, Authority, Path, Query, Fragment string                                                                                                                                                                                                                            \n} \n[..]\n   // named captures for reference, \n    p := ^((?P<Scheme>[^:/?#]+):)?(//(?P<Authority>[^/?#]*))?(?P<Path>[^?#]*)(\\?(?P<Query>[^#]*))?(#(?P<Fragment>.*))? \npm := regexp.MustCompile(p)                                                                                                   \nm := pm.FindStringSubmatch(target)\n\nt := &TargetInfo{                                                                                                             \n    m[2], // Scheme                                                                                                           \n    m[4], // Authority                                                                                                        \n    m[5], // Path                                                                                                             \n    m[7], // Query                                                                                                            \n    m[9], // Fragemnt                                                                                                         \n}\n\n[..]\n```\nDo we really need to bother with url.Parse?\n. @menghanl PTAL\nUpdated based on your review. Coming from Perl my first instinct is to use regex but I can see the added costs here. Curious your thoughts on the failure and let me know regarding Joe's comment on transport/http2_client.go.. >The dialContext function in transport/http2_client.go is called only when there's no dialer set. But it's never going to happen because the dialer will always be set. It's either the user's custom dialer, or the default dialer you just fixed.\nthanks for the clarification.\n\nThe failed test looks unrelated to this change.\n\nnoted.\nThe function is no longer exported, thanks again for the help with this. I am looking forward to getting this implemented for our use case.. @menghanl different error this time would you mind retesting, please?. Would it be possible to get this backported to v1.11.x?. @menghanl thanks!\n/cc @gyuho . @SamuelMarks it looks like you will need to update your vendor deps?. > https://golang.org/pkg/net/url/#Parse would be better here.\n@menghanl I appreciate the review and agree. I hope to review and test over the weekend.. understood, will update. ",
    "peczenyj": "@hexfusion an go url.URL is, in fact, an URI and if you want support unix:///path or unix:/path you may need check Path or Opaque properties but it is easier to handle than Regular Expressions (and Go does not support all the er fun)\nI am watching this topic because my workaround is too ugly and I want delete lines in a near future :). ",
    "azmb": "Looks similar to: https://github.com/grpc/grpc-go/issues/576. ",
    "yogeshpandey": "Added email in github account. @dfawley Thanks for your suggestions. Updated the pull request as per that. @dfawley thanks for the guidance. Can you please tell, how can I take it to master?. Thanks. ",
    "odeke-em": "/cc @dfawley . /cc @dfawley . Actually I was thinking too 32 bit and this PR is wrong, the size of int on a 64 bit machine is 1<<63-1\nand the spec only guarantees that the size of:\nint = uint https://golang.org/ref/spec#Numeric_types\n\nThus the old code is right as it is, the PR is only valid for 32 bit implementations.. Sorry for the noise.. what happens though if we get a rpcError or an *rpcError, only the *rpcError will be accepted. IMO we can keep compatibility by defining a small function like this\ngo\nfunc rpcErrorLike(e interface{}) bool {\n  switch e.(type) {\n  case *rpcError, rpcError:\n     return true\n  default:\n     return false\n  }\n}\nand then use it in those checks this way the code will be compatible with all users\nso we'll change it to\ngo\nif !rpcErrorLike(err) {\n   t.Fatalf(\"grpc.Invoke(_, _, _, _, _) receives non rpc error.\")\n}\nLet me know what you think.\n. ah, thanks @tamird for the correction. @danruehle never mind my comments :)\n. ",
    "stevenroose": "So it's not really possible to do send and receive both from the same goroutine? I recently did an implementation of a notification system using bidirectional streams (where the client could change the topics it is interested in via messages and the server sends notifications over the stream).\nI ended up having a small goroutine putting incoming messages on a channel manually:\nhttps://github.com/btcsuite/btcd/pull/1075/commits/d638d336d4fdd6dd64204f5ab33219748700aefe#diff-c830ab1edc3089a502e68c02d7f576bfR1272\nSo that I had a single goroutine that can do both sending and receiving with a select (sending on incoming notifications via a channel).\nIt seems that I should exclude io.EOF in that small goroutine from closing the channel. So that the client can still receive notifications when it no longer wants to change it's subscription. Or explicitly document that the server closes the stream when the client does. (Seems a lot easier for the client as well, since otherwise canceling a stream needs to be done by cancelling a context..). @dfawley I'd primarily like code documentation on the generated methods. So that I can read it (1) from godocs and (2) from going into the code with my IDE. Mostly the 2nd is where I was missing it. When you're implementing your server, you are using these interfaces. So it's normal that you want to go look at the documented behavior of them, right? Right now you get nothing.. ",
    "AkihiroSuda": "I expect the unix:// scheme to be consistent with the file:// scheme, which is defined in RFC 8089, although we might not need the equivalent of the file-auth part.\ni.e. +1 for unix:///a/b/c, -1 for unix:////a/b/c \ud83d\ude16 .\nEven unix:/a/b/c would be correct as well.\n. ",
    "trusktr": "For anyone stumbling here, this commit perfectly describes how to fix undefined: transport.StreamFromContext (or why there's no bin file after go install): https://github.com/jfyne/docker-grpcwebproxy/commit/da712301bde103014e7becbac0fa298384bf622f\nFor example, I had this problem with http://github.com/improbable-eng/grpc-web, and the following worked:\njs\ngo get -u github.com/improbable-eng/grpc-web/go/grpcwebproxy\ncd $GOPATH/src/github.com/improbable-eng/grpc-web\nbrew install dep # install golang/dep somehow\ndep ensure\ngo get -u github.com/improbable-eng/grpc-web/go/grpcwebproxy\nAfter that $GOPATH/bin/grpcwebproxy was available.. ",
    "Kevin005": "1.There is only one client, and it still has this concurrency problem:\nrpc error: code = Unavailable desc = transport is closing,\n2.This is my current version of gRPC:\n980d9e0 - ClientConn: add Target() returning target string (#2233). As a proto server, we are using a function of Golang: \n()unsafe.Pointer()\n. ",
    "dmcgowan": "\nNote: scheme:endpoint syntax will break all users dialing with hostname:port unless we apply the default resolver to cases where the scheme isn't registered.\n\nBased on the syntax from rfc8089, the \"endpoint\" in scheme:endpoint must be an absolute path. Would the parser be able to distinguish between hostname:port and file:/path based on the second element being an absolute path and not a port name/number?. ",
    "smola": "@stevvooe By now our projects rely on Dial broken behavior. Either we are feeding non-standard values or we have our own conversion process before passing them to Dial, I've seen both. So these projects are not necessarily broken, if you fix Dial itself you might break projects that had valid workarounds.. ",
    "HusterWan": "CLA done!!!. ",
    "mastersingh24": "Got it.  Thanks for the quick response.\nI also tried setting\ngrpc.WithDefaultCallOptions(grpc.FailFast(true)),\ngrpc.FailOnNonTempDialError(true),\nbut I guess WithBlock() negates those options . @dfawley  -  You are of course correct.  We actually do this today in one of my projects.  The tls.Config struct allows you to set a specific function GetCertificate func(*ClientHelloInfo) (*Certificate, error) for getting the tls.Certificate.  This function will always be called for new connections if you don't set the Certificates field. \nNot sure what you are thinking in terms of adding some type of built-in support for this, but happy to help.. Out of curiosity, I ran looked through the code and ran a few experiments and things are actually a bit different than in the write up above.\nThere is definitely a difference in behavior between Serve and ServeHTTP, but what I think I found is that Serve actually writes out response headers (not trailers) and ServeHTTP actually does write trailers but for some reason a \"Trailer\" response header is actually written out as well.\ncurl does not actually support displaying trailers as far as I know, so I used nghttp2 instead.\nHere's the response snippet for ServeHTTP:\n```\n[  0.022] recv (stream_id=13) :status: 200\n[  0.022] recv (stream_id=13) content-type: application/grpc\n[  0.022] recv (stream_id=13) trailer: Grpc-Status\n[  0.022] recv (stream_id=13) trailer: Grpc-Message\n[  0.022] recv (stream_id=13) trailer: Grpc-Status-Details-Bin\n[  0.022] recv HEADERS frame \n          ; END_HEADERS\n          (padlen=0)\n          ; First response header\n[  0.022] recv (stream_id=13) grpc-message: malformed method name: \"/\"\n[  0.022] recv (stream_id=13) grpc-status: 8\n[  0.022] recv HEADERS frame \n          ; END_STREAM | END_HEADERS\n          (padlen=0)\n[  0.022] send GOAWAY frame \n``\n and here's the snippet forServe`:\n```\n[  0.027] recv (stream_id=13) :status: 200\n[  0.027] recv (stream_id=13) content-type: application/grpc\n[  0.027] recv (stream_id=13) grpc-status: 8\n[  0.027] recv (stream_id=13) grpc-message: malformed method name: \"/\"\n[  0.027] recv HEADERS frame \n          ; END_STREAM | END_HEADERS\n          (padlen=0)\n          ; First response header\n[  0.027] send GOAWAY frame \n```. @dfawley  - Maybe someone smarter than me can pull it off, but as of right now I could not find any good way of getting Go's http2 implementation to send trailer only responses.  :(. I tried that ...but it did not seem to work ... I'll try it again though.\nI worked through several different order of operations, but none resulted in just the trailers being sent.\nOf course I can make it such that we just send headers ... but they would technically be response headers versus trailers. @MOZGIII  - For the browser, you may want to check out gRPC Web.  . Did you also set https://godoc.org/google.golang.org/grpc/keepalive#EnforcementPolicy on the server?   The default permitted value for time between client pings is 5 minutes ... if you set your client to ping every 30 seconds you'll need to set a matching policy on the gRPC server as well.\nWe've been using the gRPC keepalives on both the client and server in production for quite some time now.  The main thing is making sure you get the settings correct on both sides.. I believe the C++ server still wraps gRPC core.  You can find the settings here:\nhttps://github.com/grpc/grpc/blob/master/doc/keepalive.md\nMake sure to pay attention to the GRPC_ARG_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_MS,GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA, GRPC_ARG_KEEPALIVE_PERMIT_WITHOUT_CALLS settings.\n. You definitely need to ping more often than every 10 min when using Docker / Docker Swarm.  The Docker proxies timeout after 10 minutes ... we use 5 * time.Minute for the client keepalive time ... you can also try 6 as well ... anything less than 10 should keep idle connections open with Docker Swarm (or just Docker in general). @npuichigo  - hopefully you were able to resolve this?. ",
    "Capstan": "Even if you don't prioritize HTTP errors over gRPC errors, it should be better interleaved. Today, processHeaderField gets called before the the httpStatus is inspected, and will return if the Content-Type is invalid. But if there's a problem with a proxy, e.g., IAP, or urlmapping on a server with colocated HTTP services, you will very likely get text/html responses meant for human consumption. gRPC clients may still wish to try and recover programmatic data and give a better error responses.\nI suggest pulling out Content-Type from processHeaderField to be a special deferred error, to be returned if http status doesn't return an error status.. Also, some way of actually returning the html would be way helpful for diagnosis.. ",
    "Terminator637": "When will you fix it?. ",
    "sercand": "Due to this bug, we can't use latest nginx as load-balancer. A nginx ticket and commit mention this.. ",
    "lexfrei": "CLA Signed. @dfawley is this better? . ",
    "zhixinwen": "We rollback the version, and have no further info about the issue.\n\nFrom: Menghan Li notifications@github.com\nSent: Wednesday, June 13, 2018 2:44:53 PM\nTo: grpc/grpc-go\nCc: Zhixin Wen; Mention\nSubject: Re: [grpc/grpc-go] goroutine leak (#1936)\n@zhixinwenhttps://github.com/zhixinwen Do you still see this problem?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/grpc/grpc-go/issues/1936#issuecomment-397097705, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AJukqe9wV0C_LL2XqMi0ava5ef3I5m7bks5t8YfVgaJpZM4S1IJx.\n. ",
    "maksharma": "We have a custom dialer implemented already. This can solve tracking of client connection.\nStill trying to figure out how we can manage refreshes.. ",
    "zkry": "I also did a check to do nothing if Append or Sets values are empty. This seemed to best match the behavior of the Pair function as it is impossible to create a key value pair with a value of an empty string slice.. That sounds good. I'll do that.. ",
    "crosbymichael": "Should this be backported to a 1.10.1? . @menghanl Thanks for the backport!. ",
    "wy-zhang": "Client code:\nconn, err := grpc.Dial(addr, grpc.WithInsecure())\nclient := pb.NewRemoteCheckClient(conn)\nServer code:\ngrpcServer := grpc.NewServer(). @dfawley \n1.It works normal under small amount of concurrent. I am only increased the amount of concurrent, and appear this issue.\n2.yeah, it always succeed for the first 20 minutes.\n3.Client and server deploy in different virtual machine.\n4.I will check the firewall.\n\u201cThat server error, if it happens for every connection, should mean the connection is never fully established. \u201d How to avoid this warning log?\nThere has another phenomenon. When the client reconnect the server, but the old established connection always alive in server side, and can't find the connect in client side. It's strange.\n. @dfawley About the firewall, I don't  think that's the problem. Because when i tuning the period to 10s, 30s, 60s, it also works will first 20 minutes.. @MakMukhi Firstly, I use netstat to check the connection, it returns established. Secondly,the client can receive the response data from the server normally. \nI am confused about the server warning log, because i don\u2019t configured any parameters. And, i want to know how to resolve this warning.\nThanks . @MakMukhi @dfawley Thank you for your detailed explanation.\n1.Yes, the data responsed from the RPC server.\n2.The log level is already change to INFO. There's no more information from the log.\n3.When running the client and server in same machine. It works will, no errors in both sides. \n4.When running the client and only one server in different machine, the client error disappeared and get a lot of \"context deadline exceeded\" error, but the warning log in server also exist. Print one warning log every 2 minutes.\nAccording to your explain, the HTTP2 connection wasn't established, but form the test, it also can transfer data when connection not established. So, the warning log means what?. On the server side, also can geta log like this(not appear every time):\nWARNING: 2018/03/28 14:42:10 grpc: Server.Serve failed to create ServerTransport:  connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: read tcp 192.168.0.1:8080->192.168.0.1:52549: read: connection reset by peer\". @MakMukhi Thank you for your response. There has some problem of server vm. \nMy network colleague is already location this issue.\nThis issue should be closed.. ",
    "gertcuykens": "ok thx closing. Thank you. doh! First time I every heard about import path comments. Makes sense, although I expect other people to be confused as wel who are used to just copy the github url. ",
    "alltom": "That's exactly what I wanted to know. Thank you both.. ",
    "asimonov": "is it possible to do this in python?. ",
    "amandeepgautam": "I assumed this was the error. I use glog and this was in .FATAL file. I have lost the client side logs so I am not sure what exactly the error was but the end result was that the rpc call failed.. Yes. Should I reopen it or create a new issue later on?. Possibly this: https://github.com/grpc/grpc-go/issues/1955 was because of the same reason. I would be happy to submit a PR if the bug is confirmed and someone can give a little context on what is causing this issue..:). Your experiments are fine. I was also unable to reproduce it on Ubuntu using golang compiler so probably something with the gccgo compiler or something else. Thanks for looking though. . for the poor souls who end up here, here is how this was fixed:\nGOARCH=ppc64 CGO_ENABLED=1 go build -gccgoflags='-Wl,-bbigtoc'\nPlease build with the above flags and somehow it fixes this on AIX.. I have ca-certificates already installed but somehow go get is unable to access them. Output of pkgchk:\n-bash-3.2$ /usr/sbin/pkgchk -L CSWcacertificates\nNOTE: Couldn't lock the package database.\n/etc/opt/csw/ca-certificates.conf.CSW f cswpreserveconf 0755 root bin 773 2257 1519820251 CSWcacertificates\n/etc/opt/csw/pkg d none 0755 root bin CSWsudo CSWcacertificates CSWopenssh-client CSWgit CSWfontconfig CSWscreen CSWwget\n/etc/opt/csw/pkg/CSWcacertificates d none 0755 root bin CSWcacertificates\n/etc/opt/csw/pkg/CSWcacertificates/cswmigrateconf f cswmigrateconf 0644 root bin 37 3105 1519820260 CSWcacertificates\n/etc/opt/csw/ssl d none 0755 root bin CSWcacertificates\n/etc/opt/csw/ssl/certs d none 0755 root bin CSWcacertificates\n/opt/csw/etc/ssl d none 0755 root bin CSWcacertificates\n/opt/csw/etc/ssl/certs=../../../../etc/opt/csw/ssl/certs s none CSWcacertificates\n/opt/csw/sbin/update-ca-certificates f none 0755 root bin 4632 37496 1519820251 CSWcacertificates\n/opt/csw/share/cacertificates d none 0755 root bin CSWcacertificates\n/opt/csw/share/cacertificates/hash.db f none 0755 root bin 13504 50658 1519820251 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla d none 0755 root bin CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/ACCVRAIZ1.pem f none 0644 root bin 2775 30089 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/AC_RAIZ_FNMT-RCM.pem f none 0644 root bin 1974 32620 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/AC_RaAz_CerticAmara_S.A..pem f none 0644 root bin 2283 56897 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Actalis_Authentication_Root_CA.pem f none 0644 root bin 2051 37598 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/AddTrust_External_Root.pem f none 0644 root bin 1523 59701 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/AddTrust_Low-Value_Services_Root.pem f none 0644 root bin 1482 56103 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/AffirmTrust_Commercial.pem f none 0644 root bin 1205 32869 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/AffirmTrust_Networking.pem f none 0644 root bin 1205 32823 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/AffirmTrust_Premium.pem f none 0644 root bin 1893 25019 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/AffirmTrust_Premium_ECC.pem f none 0644 root bin 754 61298 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Amazon_Root_CA_1.pem f none 0644 root bin 1189 31365 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Amazon_Root_CA_2.pem f none 0644 root bin 1884 23196 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Amazon_Root_CA_3.pem f none 0644 root bin 656 53170 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Amazon_Root_CA_4.pem f none 0644 root bin 738 59284 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Atos_TrustedRoot_2011.pem f none 0644 root bin 1262 37895 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Autoridad_de_Certificacion_Firmaprofesional_CIF_A62634068.pem f none 0644 root bin 2169 47281 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Baltimore_CyberTrust_Root.pem f none 0644 root bin 1262 37824 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Buypass_Class_2_Root_CA.pem f none 0644 root bin 1917 27153 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Buypass_Class_3_Root_CA.pem f none 0644 root bin 1917 27650 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/CA_Disig_Root_R2.pem f none 0644 root bin 1937 30160 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/CFCA_EV_ROOT.pem f none 0644 root bin 1986 33612 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/COMODO_Certification_Authority.pem f none 0644 root bin 1490 54532 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/COMODO_ECC_Certification_Authority.pem f none 0644 root bin 941 10622 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/COMODO_RSA_Certification_Authority.pem f none 0644 root bin 2088 41943 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Camerfirma_Chambers_of_Commerce_Root.pem f none 0644 root bin 1706 10158 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Camerfirma_Global_Chambersign_Root.pem f none 0644 root bin 1718 11278 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Certigna.pem f none 0644 root bin 1331 43011 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Certinomis_-_Root_CA.pem f none 0644 root bin 1994 33561 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Certplus_Class_2_Primary_CA.pem f none 0644 root bin 1299 40385 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Certplus_Root_CA_G1.pem f none 0644 root bin 1941 28153 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Certplus_Root_CA_G2.pem f none 0644 root bin 795 64163 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Certum_Root_CA.pem f none 0644 root bin 1120 25981 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Certum_Trusted_Network_CA.pem f none 0644 root bin 1356 45279 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Certum_Trusted_Network_CA_2.pem f none 0644 root bin 2080 42173 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Chambers_of_Commerce_Root_-_2008.pem f none 0644 root bin 2596 18925 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/ComSign_CA.pem f none 0644 root bin 1303 41500 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Comodo_AAA_Services_root.pem f none 0644 root bin 1518 59067 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Cybertrust_Global_Root.pem f none 0644 root bin 1319 41237 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/D-TRUST_Root_CA_3_2013.pem f none 0644 root bin 1470 55826 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/D-TRUST_Root_Class_3_CA_2_2009.pem f none 0644 root bin 1518 58746 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/D-TRUST_Root_Class_3_CA_2_EV_2009.pem f none 0644 root bin 1539 60513 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DST_Root_CA_X3.pem f none 0644 root bin 1201 33233 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Deutsche_Telekom_Root_CA_2.pem f none 0644 root bin 1319 43022 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DigiCert_Assured_ID_Root_CA.pem f none 0644 root bin 1352 45858 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DigiCert_Assured_ID_Root_G2.pem f none 0644 root bin 1307 42512 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DigiCert_Assured_ID_Root_G3.pem f none 0644 root bin 852 4646 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DigiCert_Global_Root_CA.pem f none 0644 root bin 1340 44553 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DigiCert_Global_Root_G2.pem f none 0644 root bin 1295 41010 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DigiCert_Global_Root_G3.pem f none 0644 root bin 839 2711 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DigiCert_High_Assurance_EV_Root_CA.pem f none 0644 root bin 1368 46264 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/DigiCert_Trusted_Root_G4.pem f none 0644 root bin 1990 34281 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/E-Tugra_Certification_Authority.pem f none 0644 root bin 2246 53504 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/EC-ACC.pem f none 0644 root bin 1913 26764 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/EE_Certification_Centre_Root_CA.pem f none 0644 root bin 1453 54690 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Entrust.net_Premium_2048_Secure_Server_CA.pem f none 0644 root bin 1506 58356 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Entrust_Root_Certification_Authority.pem f none 0644 root bin 1645 3407 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Entrust_Root_Certification_Authority_-_EC1.pem f none 0644 root bin 1091 23326 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Entrust_Root_Certification_Authority_-_G2.pem f none 0644 root bin 1535 60094 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GDCA_TrustAUTH_R5_ROOT.pem f none 0644 root bin 1982 32886 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GeoTrust_Global_CA.pem f none 0644 root bin 1218 35152 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GeoTrust_Primary_Certification_Authority.pem f none 0644 root bin 1270 38991 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GeoTrust_Primary_Certification_Authority_-_G2.pem f none 0644 root bin 990 15239 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GeoTrust_Primary_Certification_Authority_-_G3.pem f none 0644 root bin 1445 53832 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GeoTrust_Universal_CA.pem f none 0644 root bin 1937 29691 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GeoTrust_Universal_CA_2.pem f none 0644 root bin 1941 28886 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GlobalSign_ECC_Root_CA_-_R4.pem f none 0644 root bin 713 57861 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GlobalSign_ECC_Root_CA_-_R5.pem f none 0644 root bin 795 64922 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GlobalSign_Root_CA.pem f none 0644 root bin 1262 37552 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GlobalSign_Root_CA_-_R2.pem f none 0644 root bin 1356 46253 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/GlobalSign_Root_CA_-_R3.pem f none 0644 root bin 1230 34832 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Global_Chambersign_Root_-_2008.pem f none 0644 root bin 2588 19381 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Go_Daddy_Class_2_CA.pem f none 0644 root bin 1449 53612 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Go_Daddy_Root_Certificate_Authority_-_G2.pem f none 0644 root bin 1368 45395 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Hellenic_Academic_and_Research_Institutions_ECC_RootCA_2015.pem f none 0644 root bin 1018 18024 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Hellenic_Academic_and_Research_Institutions_RootCA_2011.pem f none 0644 root bin 1514 59669 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Hellenic_Academic_and_Research_Institutions_RootCA_2015.pem f none 0644 root bin 2157 46922 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Hongkong_Post_Root_CA_1.pem f none 0644 root bin 1169 30585 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/ISRG_Root_X1.pem f none 0644 root bin 1941 30191 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/IdenTrust_Commercial_Root_CA_1.pem f none 0644 root bin 1925 28329 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/IdenTrust_Public_Sector_Root_CA_1.pem f none 0644 root bin 1933 29044 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Izenpe.com.pem f none 0644 root bin 2124 46314 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/LuxTrust_Global_Root_2.pem f none 0644 root bin 2059 39671 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Microsec_e-Szigno_Root_CA_2009.pem f none 0644 root bin 1462 54228 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/NetLock_Arany_Class_Gold_FAtanAsAtvAny.pem f none 0644 root bin 1478 56142 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Network_Solutions_Certificate_Authority.pem f none 0644 root bin 1413 51973 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/OISTE_WISeKey_Global_Root_GA_CA.pem f none 0644 root bin 1429 51898 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/OISTE_WISeKey_Global_Root_GB_CA.pem f none 0644 root bin 1348 43648 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/OpenTrust_Root_CA_G1.pem f none 0644 root bin 1945 29360 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/OpenTrust_Root_CA_G2.pem f none 0644 root bin 1945 28277 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/OpenTrust_Root_CA_G3.pem f none 0644 root bin 799 63825 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/QuoVadis_Root_CA.pem f none 0644 root bin 2080 42320 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/QuoVadis_Root_CA_1_G3.pem f none 0644 root bin 1925 28953 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/QuoVadis_Root_CA_2.pem f none 0644 root bin 2043 36715 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/QuoVadis_Root_CA_2_G3.pem f none 0644 root bin 1925 28324 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/QuoVadis_Root_CA_3.pem f none 0644 root bin 2356 64975 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/QuoVadis_Root_CA_3_G3.pem f none 0644 root bin 1925 27736 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/S-TRUST_Universal_Root_CA.pem f none 0644 root bin 1396 49349 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SSL.com_EV_Root_Certification_Authority_ECC.pem f none 0644 root bin 957 11210 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SSL.com_EV_Root_Certification_Authority_RSA_R2.pem f none 0644 root bin 2116 44782 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SSL.com_Root_Certification_Authority_ECC.pem f none 0644 root bin 945 10146 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SSL.com_Root_Certification_Authority_RSA.pem f none 0644 root bin 2096 43478 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SZAFIR_ROOT_CA2.pem f none 0644 root bin 1258 37915 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SecureSign_RootCA11.pem f none 0644 root bin 1250 36342 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SecureTrust_CA.pem f none 0644 root bin 1352 44193 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Secure_Global_CA.pem f none 0644 root bin 1356 44358 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Security_Communication_RootCA2.pem f none 0644 root bin 1262 38021 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Security_Communication_Root_CA.pem f none 0644 root bin 1226 33577 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Sonera_Class_2_Root_CA.pem f none 0644 root bin 1144 28289 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Staat_der_Nederlanden_EV_Root_CA.pem f none 0644 root bin 1950 30322 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Staat_der_Nederlanden_Root_CA_-_G2.pem f none 0644 root bin 2072 41199 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Staat_der_Nederlanden_Root_CA_-_G3.pem f none 0644 root bin 1954 32154 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Starfield_Class_2_CA.pem f none 0644 root bin 1470 56408 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Starfield_Root_Certificate_Authority_-_G2.pem f none 0644 root bin 1401 49941 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Starfield_Services_Root_Certificate_Authority_-_G2.pem f none 0644 root bin 1425 52340 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SwissSign_Gold_CA_-_G2.pem f none 0644 root bin 2047 39339 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SwissSign_Platinum_CA_-_G2.pem f none 0644 root bin 2059 39082 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/SwissSign_Silver_CA_-_G2.pem f none 0644 root bin 2051 38693 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Swisscom_Root_CA_2.pem f none 0644 root bin 2092 42191 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Symantec_Class_1_Public_Primary_Certification_Authority_-_G4.pem f none 0644 root bin 982 15572 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Symantec_Class_1_Public_Primary_Certification_Authority_-_G6.pem f none 0644 root bin 1437 53543 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Symantec_Class_2_Public_Primary_Certification_Authority_-_G4.pem f none 0644 root bin 982 15513 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Symantec_Class_2_Public_Primary_Certification_Authority_-_G6.pem f none 0644 root bin 1437 53464 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/T-TeleSec_GlobalRoot_Class_2.pem f none 0644 root bin 1368 49071 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/T-TeleSec_GlobalRoot_Class_3.pem f none 0644 root bin 1368 47157 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TARKTRUST_Elektronik_Sertifika_Hizmet_SaAlayAcAsA_H5.pem f none 0644 root bin 1502 58124 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TC_TrustCenter_Class_3_CA_II.pem f none 0644 root bin 1681 6390 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TUBITAK_Kamu_SM_SSL_Kok_Sertifikasi_-_Surum_1.pem f none 0644 root bin 1584 63712 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TWCA_Global_Root_CA.pem f none 0644 root bin 1884 25151 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TWCA_Root_Certification_Authority.pem f none 0644 root bin 1270 37689 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Taiwan_GRCA.pem f none 0644 root bin 1950 31263 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TeliaSonera_Root_CA_v1.pem f none 0644 root bin 1872 23562 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TrustCor_ECA-1.pem f none 0644 root bin 1494 55836 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TrustCor_RootCert_CA-1.pem f none 0644 root bin 1514 58057 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/TrustCor_RootCert_CA-2.pem f none 0644 root bin 2206 51917 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Trustis_FPS_Root_CA.pem f none 0644 root bin 1242 36169 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/USERTrust_ECC_Certification_Authority.pem f none 0644 root bin 949 13302 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/USERTrust_RSA_Certification_Authority.pem f none 0644 root bin 2096 42240 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/UTN_USERFirst_Email_Root_CA.pem f none 0644 root bin 1669 8146 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/VeriSign_Class_3_Public_Primary_Certification_Authority_-_G4.pem f none 0644 root bin 1283 42305 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/VeriSign_Class_3_Public_Primary_Certification_Authority_-_G5.pem f none 0644 root bin 1734 13055 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/VeriSign_Universal_Root_Certification_Authority.pem f none 0644 root bin 1701 10671 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Verisign_Class_1_Public_Primary_Certification_Authority_-_G3.pem f none 0644 root bin 1486 58461 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Verisign_Class_2_Public_Primary_Certification_Authority_-_G3.pem f none 0644 root bin 1482 56082 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Verisign_Class_3_Public_Primary_Certification_Authority_-_G3.pem f none 0644 root bin 1486 59179 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/Visa_eCommerce_Root.pem f none 0644 root bin 1323 42008 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/XRamp_Global_CA_Root.pem f none 0644 root bin 1514 59682 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/certSIGN_ROOT_CA.pem f none 0644 root bin 1177 30002 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/ePKI_Root_Certification_Authority.pem f none 0644 root bin 2035 37519 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/thawte_Primary_Root_CA.pem f none 0644 root bin 1494 57428 1519820194 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/thawte_Primary_Root_CA_-_G2.pem f none 0644 root bin 941 10992 1519820193 CSWcacertificates\n/opt/csw/share/cacertificates/mozilla/thawte_Primary_Root_CA_-_G3.pem f none 0644 root bin 1506 59455 1519820193 CSWcacertificates\n/opt/csw/share/doc/cacertificates d none 0755 root bin CSWcacertificates\n/opt/csw/share/doc/cacertificates/README.CSW f none 0755 root bin 1766 27852 1519820261 CSWcacertificates\n/opt/csw/share/doc/cacertificates/license f none 0644 root bin 202 16939 1519820259 CSWcacertificates. ",
    "tyler2018": "go version 1.10. ",
    "douglarek": "@tyler2018 would you like to  f**k the GFW ? yeah, maybe. or you can clone this repo to your $GOPATH/src/google.golang.org/grpc manually.. ",
    "zelahi": "I would like to take this ticket.  My only question is how do you like the PRs to be submitted?  One per file, or all of these in one PR?.  I was thinking of grouping them by level such as the benchmarking in one and interop as another. Sweet thanks, as I'm going through them, I'll note down the auto-generated packages\n. closing and resubmitting so I can fix the issue with the email. Is there a way to have travis-ci re-test?. ahh ty for the info.  apologies I was having trouble reading through the output.  I'll be more careful on reading the details next time.. Thank you for the feedback.  I'll go ahead and make the requested changes . apologies, I'll be addressing this in another commit. ",
    "tailnode": "@MakMukhi thank you,. The problem is still not solved. If there is progress I will continue to update.. after update grpc to 1.11, this problem had resolved. see this pr. ",
    "GingerMoon": "thank you very much!. ",
    "ZachEddy": "Thanks for the speedy response, @lyuxuan! \nI'm writing a server interceptor that uses the transport.Stream from an incoming request. Specifically, I use the transport.Stream to determine the service name (e.g. PetStore) and the method name (e.g. ListPets) of a given request.\nTo get the stream itself, I called transport.StreamFromContext(ctx) and after that point I could call stream.Method() to get the service and method names.\nIs that helpful?. Awesome \u2014 that did the trick \ud83c\udf89 \nThanks for your help!. ",
    "knweiss": "I've removed seven additional gosimple cleanups using time.Until() as Travis failed with go1.6 and go1.7. The function is not available in these old versions.. ",
    "Teddy-Schmitz": "That would work for us as well, thanks.  \nThough I think longer term it would still be good to be able to turn it off as to not waste CPU on the DNS resolver nor the local machine trying for a TXT record we know will fail.. ",
    "Xopherus": "Could you register a new resolver.Builder instead? Looks like we could do that and use SetDefaultScheme to set the custom resolver to be the new default. Alternatively I guess we could override the default dns scheme resolver, right?\nAm I on the right track? If so do you have any tips for making a new builder / resolver? There's a lot of stuff under the hood of the dns resolver involving spawning goroutines to periodically resolving the addr that I'm not sure if my use case will require, but I'd rather not create more problems for myself by changing too much.. > However, I don't recommend overriding the default dns scheme resolver, since it's easy to be confusing later when your project gets larger and more complex that you don't know whether you are using the grpc native resolver or you own implementation. I think it's better to keep them separate, and use SetDefualtScheme to make your dns resolver to be the default resolver. BTW, our current default resolver is \"passthrough\", so you still need to SetDefualtScheme if you prefer overriding.\nAgreed @lyuxuan - I just did a deep dive into the resolver package and I think it makes way more sense to just create a new resolver which specifies a new scheme. Reading naming.md made things a lot more clear.\nI'll get back to you when I come up with a POC, hopefully tomorrow.. ",
    "lrstanley": "https://golang.org/cmd/go/#hdr-Import_path_checking\ntl;dr: this package defines an expected way to import grpc-go (google.golang.org/grpc, which redirects to Github). You should use google.golang.org/grpc, and not github.com/grpc/grpc-go\nhttps://github.com/grpc/grpc-go/blob/4172bfc25e1c7dc4265a50b3ba73e87401fb75c1/doc.go#L24\n& https://github.com/grpc/grpc-go#installation. ",
    "colesimmons": "Thank you!. ",
    "Chyroc": "@thelinuxfoundation done.. @dfawley \nci falied because of benchmark/grpc_testing/services.pb.go:49:9: grpc.Invoke is deprecated: Use ClientConn.Invoke instead.  (SA1019) (https://travis-ci.org/grpc/grpc-go/jobs/371845620#L540)\nbut it is generated by https://github.com/golang/protobuf/blob/master/protoc-gen-go/grpc/grpc.go#L290\nshould i fix protoc-gen-go or add it to staticcheck -ignore ?. @dfawley done.. make a pr( https://github.com/golang/protobuf/pull/589 ) to update protoc-gen-go to not use it. ok, done.. ",
    "jmillikin-stripe": "Also, https://github.com/grpc/grpc-go/issues/1917 documents that RPCs with fail-fast behavior will somehow get access to the underlying connection error. My system isn't using fail-fast because we want the gRPC library to retry past transient network issues.. With that solution, would it be possible to obtain the underlying error from a failed blocking dial? As I said in the first post, we'd like to be able to have error accounting such that failure to connect to the remote will be reported before attempting to run an RPC.. ",
    "joshuarubin": "The Trailer got me enough info out of the error. Thanks a bunch.. ",
    "mumoshu": "@lyuxuan This worked like a charm! nginx-ingress-controller is now happy to load-balance grpc over h2c workloads to the grpc server w/ this change. Thanks a lot for the fix \ud83d\udc4d . ",
    "him0": "I cannot find this PR at release note , but found this changes in version 1.15.0(8dea3dc473e90c8179e519d91302d0597c0ca1d1). Has this pull request already released?\nhttps://github.com/grpc/grpc-go/blob/8dea3dc473e90c8179e519d91302d0597c0ca1d1/internal/transport/controlbuf.go#L31-L33. ",
    "igorbernstein2": "CLA signed. ",
    "pavelkalinnikov": "Hi guys, thanks for fixing this issue. Can this be merged? It blocks PRs in my team's codebase.. ",
    "lijianwh": "go get -u github.com/golang/protobuf/proto does't work for me, I use go get -u google.golang.org/grpc/health/grpc_health_v1 instead, it works. Thanks!. ",
    "matthoop": "sorry, should have read through the document. Seems protoc should be run as \nprotoc -I helloworld/ helloworld/helloworld.proto --go_out=plugins=grpc:helloworld\nclose issue.. ",
    "ayenter": "Bump. ",
    "bleleve": "Hi lyuxuan,\nThis is a client interceptor, can I do the same with the server interceptor (and call handler()) ? In the interceptor doc it is announced as intended to be run only before.\nBest regards.. Hi lyuxuan,\nOn the gRPC middleware repository, server side interceptors are only indicated as being available before the request (https://github.com/grpc-ecosystem/go-grpc-middleware/blob/master/README.md) :\ngRPC Go recently acquired support for Interceptors, i.e. middleware that is executed either on the gRPC Server before the request is passed onto the user's application logic, or on the gRPC client either around the user call.\nBest regards.\n. ",
    "JCzz": "Thanks @menghanl . ",
    "jusunglee": "Accident, please disregard, thank you. ",
    "avikio": "I understand your arguments but I want to expose my grpc server on internet.\nHow can I prevent malicious users to crash my server? \nIt took me only few minutes to make it consumes several gigabytes of memory just by running this simple code.. ",
    "perrynzhou": "@menghanl  i don't know grpc version,but accroding to code ,such as recvMsg function,why need to make to alloc memory for each message,you can use local stack memory on function stack,not on heap.gc for go to slow.if you delpoy  pod  that use grpc recv message,k8s will kill thid pod,because oom,if qps of your application to many.. @MakMukhi  \u201d local stack memory within function\u201c  that mean alloc memory on stack ,but not on heap.linux stack size more than 1 MB memory,you can resued this memory. ",
    "electronjoe": "FWIW I ran across your report while troubleshooting another proximal issue - the documentation for \nSendMsg indicates it is NOT safe to simultaneously use from two goroutines:\n// It's safe to have a goroutine calling SendMsg and another goroutine calling\n// recvMsg on the same stream at the same time.\n// But it is not safe to call SendMsg on the same stream in different goroutines.\nhttps://godoc.org/google.golang.org/grpc#Stream. ",
    "arthurgan": "Hi thanks for the answer. I solved the problem a different way, but you were on point about the root cause, which was proto and protoc-gen-go being out of date.\nHowever, because my project's dependency was managed by glide, I couldn't just manually \"go get\" the latest proto and protoc-gen-go because that would defeat the purpose of using glide.\nIn my case, there was some stale dependencies specified in my glide.yaml that was hard locking proto to a specific version that was too old for grpc.\nSo I fixed the issue by cleaning up/updating my glide.yaml, removing anything that looks out of date and got rid of as much hard coded versioning as possible, then I regenerated glide.lock and this time  the latest version of proto was recorded in glide.lock and my project built clean.. ",
    "shihp": "```\n go get -u github.com/golang/protobuf/{proto,protoc-gen-go}\ngithub.com/golang/protobuf/protoc-gen-go/descriptor\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:352:39: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:413:41: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:540:37: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:643:52: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:699:51: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:754:43: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:822:42: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:925:42: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:980:41: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:1053:59: undefined: proto.InternalMessageInfo\nprotobuf/protoc-gen-go/descriptor/descriptor.pb.go:1053:59: too many errors\n```. ",
    "mengll": "delete and go get -u github.com/golang/protobuf/{proto,protoc-gen-go}. ",
    "gnoack": "Hmm, the LinuxFoundation page says \"You are already authorized to contribute code to this project through your membership with Google.\" Not sure why this is still red. I'll give it another day or so, in case it's a propagation delay.. Ah, it worked :). ",
    "dreamflyfengzi": "I set MaxConcurrentStreams ,  will happen  \"error\": \"stream terminated by RST_STREAM with error code: REFUSED_STREAM\". the rpc requests don't be closed. . sorry  grpc version is 1.9.2 . ",
    "alexcarol": "@dreamflyfengzi We recently experienced a similar issue, in our case we found it was caused by Stream.waitOnHeader (https://github.com/grpc/grpc-go/blob/master/transport/transport.go#L236) being blocked in some cases. We haven't found out why it got stuck, but adding a timeout to the context fixed the memory leak for us.. ",
    "cy-zheng": "And I found that it's exactly 60 seconds after the connection established.\n2018-06-01 16:33:46.946724 I | http2: server connection from 127.0.0.1:57056 on 0xc4201676c0\n2018-06-01 16:34:46.949410 I | http2: Framer 0xc4207bc700: wrote RST_STREAM stream=1 len=4 ErrCode=INTERNAL_ERROR. Just find the bug in my code, sorry!. ",
    "nmarcetic": "Hey @menghanl Thank you for your answers.\nCan you try to update your vendored gRPC and see if that works? This fix my problem I did a update with  dep ensure -u. \nClosing this one. . ",
    "longXboy": "@menghanl Thanks for your reply!\nMy project crashed three times because of this unsafe datarace bug.\nHowever, I now find that I can just delete this configuration ServerParameters.MaxConnectionAge to solve this problem.. @menghanl thanks a lot!. ",
    "abeeshks": "HI Sir,\nI Want to pull the 1.4 version of GRPC for one of my project which have same protoc version library. May be by mistake I did some other changes. Please ignore the same.\nRegards,\nAbeesh KS\nFrom: mmukhi [mailto:notifications@github.com]\nSent: Thursday, June 07, 2018 10:16 PM\nTo: grpc/grpc-go grpc-go@noreply.github.com\nCc: Ks, Abeesh (Nokia - IN/Bangalore) abeesh.ks@nokia.com; Mention mention@noreply.github.com\nSubject: Re: [grpc/grpc-go] V1.4.x (#2131)\n@abeeshkshttps://github.com/abeeshks Can you clarify what's this about? Also seems like these changes were made on 1.4?\nAre you guys seeing data races in some scenario? If so, can you speak more on that. Possible provide a reproduction. We'd rather avoid using locks as much as possible.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/grpc/grpc-go/pull/2131#issuecomment-395488741, or mute the threadhttps://github.com/notifications/unsubscribe-auth/Al-k12P03LJfF2wm_obXWZi0FX2aik_Hks5t6Vi-gaJpZM4UeUf6.\n. ",
    "jittuu": "@lyuxuan  I think you miss !appengine build flag on credentials_util_go19.go file. I still got bad import \"syscall\" error on appengine. Thanks.. Thanks! \ud83d\udc4d . ",
    "nicolasnoble": "There's a soft limit in http2 on the number of parallel streams, which is 100 by default.\nI don't know if this is the problem here, nor how to configure it in go. Note that this needs to be tweaked on both the client and the server. . Wasm, by design, doesn't have low level networking support, so this request is inherently impossible. . ",
    "EricLoh": "I ran the command in $GOPATH/src/google.golang.org/grpc/examples/helloworld/helloworld. \nDid not encounter any error when generating the .pb.go file. \nThis is what I got when I generated \n```\n// Code generated by protoc-gen-go.\n// source: helloworld.proto\n// DO NOT EDIT!\n/*\nPackage helloworld is a generated protocol buffer package.\nIt is generated from these files:\n    helloworld.proto\nIt has these top-level messages:\n    HelloRequest\n    HelloReply\n*/\npackage helloworld\nimport proto \"github.com/golang/protobuf/proto\"\nimport (\n    context \"golang.org/x/net/context\"\n    grpc \"google.golang.org/grpc\"\n)\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ context.Context\nvar _ grpc.ClientConn\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ = proto.Marshal\n// The request message containing the user's name.\ntype HelloRequest struct {\n    Name string protobuf:\"bytes,1,opt,name=name\" json:\"name,omitempty\"\n}\nfunc (m HelloRequest) Reset()         { m = HelloRequest{} }\nfunc (m HelloRequest) String() string { return proto.CompactTextString(m) }\nfunc (HelloRequest) ProtoMessage()    {}\n// The response message containing the greetings\ntype HelloReply struct {\n    Message string protobuf:\"bytes,1,opt,name=message\" json:\"message,omitempty\"\n}\nfunc (m HelloReply) Reset()         { m = HelloReply{} }\nfunc (m HelloReply) String() string { return proto.CompactTextString(m) }\nfunc (HelloReply) ProtoMessage()    {}\nfunc init() {\n}\n// Client API for Greeter service\ntype GreeterClient interface {\n    // Sends a greeting\n    SayHello(ctx context.Context, in HelloRequest, opts ...grpc.CallOption) (HelloReply, error)\n}\ntype greeterClient struct {\n    cc *grpc.ClientConn\n}\nfunc NewGreeterClient(cc *grpc.ClientConn) GreeterClient {\n    return &greeterClient{cc}\n}\nfunc (c greeterClient) SayHello(ctx context.Context, in HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) {\n    out := new(HelloReply)\n    err := grpc.Invoke(ctx, \"/helloworld.Greeter/SayHello\", in, out, c.cc, opts...)\n    if err != nil {\n        return nil, err\n    }\n    return out, nil\n}\n// Server API for Greeter service\ntype GreeterServer interface {\n    // Sends a greeting\n    SayHello(context.Context, HelloRequest) (HelloReply, error)\n}\nfunc RegisterGreeterServer(s *grpc.Server, srv GreeterServer) {\n    s.RegisterService(&_Greeter_serviceDesc, srv)\n}\nfunc _Greeter_SayHello_Handler(srv interface{}, ctx context.Context, codec grpc.Codec, buf []byte) (interface{}, error) {\n    in := new(HelloRequest)\n    if err := codec.Unmarshal(buf, in); err != nil {\n        return nil, err\n    }\n    out, err := srv.(GreeterServer).SayHello(ctx, in)\n    if err != nil {\n        return nil, err\n    }\n    return out, nil\n}\nvar _Greeter_serviceDesc = grpc.ServiceDesc{\n    ServiceName: \"helloworld.Greeter\",\n    HandlerType: (*GreeterServer)(nil),\n    Methods: []grpc.MethodDesc{\n        {\n            MethodName: \"SayHello\",\n            Handler:    _Greeter_SayHello_Handler,\n        },\n    },\n    Streams: []grpc.StreamDesc{},\n}\n```\nAs compared to the original file pulled from go get google.golang.org/grpc\n```\n// Code generated by protoc-gen-go. DO NOT EDIT.\n// source: helloworld.proto\npackage helloworld\nimport proto \"github.com/golang/protobuf/proto\"\nimport fmt \"fmt\"\nimport math \"math\"\nimport (\n    context \"golang.org/x/net/context\"\n    grpc \"google.golang.org/grpc\"\n)\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ = proto.Marshal\nvar _ = fmt.Errorf\nvar _ = math.Inf\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the proto package it is being compiled against.\n// A compilation error at this line likely means your copy of the\n// proto package needs to be updated.\nconst _ = proto.ProtoPackageIsVersion2 // please upgrade the proto package\n// The request message containing the user's name.\ntype HelloRequest struct {\n    Name                 string   protobuf:\"bytes,1,opt,name=name,proto3\" json:\"name,omitempty\"\n    XXX_NoUnkeyedLiteral struct{} json:\"-\"\n    XXX_unrecognized     []byte   json:\"-\"\n    XXX_sizecache        int32    json:\"-\"\n}\nfunc (m HelloRequest) Reset()         { m = HelloRequest{} }\nfunc (m HelloRequest) String() string { return proto.CompactTextString(m) }\nfunc (HelloRequest) ProtoMessage()    {}\nfunc (HelloRequest) Descriptor() ([]byte, []int) {\n    return fileDescriptor_helloworld_71e208cbdc16936b, []int{0}\n}\nfunc (m HelloRequest) XXX_Unmarshal(b []byte) error {\n    return xxx_messageInfo_HelloRequest.Unmarshal(m, b)\n}\nfunc (m HelloRequest) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n    return xxx_messageInfo_HelloRequest.Marshal(b, m, deterministic)\n}\nfunc (dst HelloRequest) XXX_Merge(src proto.Message) {\n    xxx_messageInfo_HelloRequest.Merge(dst, src)\n}\nfunc (m HelloRequest) XXX_Size() int {\n    return xxx_messageInfo_HelloRequest.Size(m)\n}\nfunc (m HelloRequest) XXX_DiscardUnknown() {\n    xxx_messageInfo_HelloRequest.DiscardUnknown(m)\n}\nvar xxx_messageInfo_HelloRequest proto.InternalMessageInfo\nfunc (m *HelloRequest) GetName() string {\n    if m != nil {\n        return m.Name\n    }\n    return \"\"\n}\n// The response message containing the greetings\ntype HelloReply struct {\n    Message              string   protobuf:\"bytes,1,opt,name=message,proto3\" json:\"message,omitempty\"\n    XXX_NoUnkeyedLiteral struct{} json:\"-\"\n    XXX_unrecognized     []byte   json:\"-\"\n    XXX_sizecache        int32    json:\"-\"\n}\nfunc (m HelloReply) Reset()         { m = HelloReply{} }\nfunc (m HelloReply) String() string { return proto.CompactTextString(m) }\nfunc (HelloReply) ProtoMessage()    {}\nfunc (HelloReply) Descriptor() ([]byte, []int) {\n    return fileDescriptor_helloworld_71e208cbdc16936b, []int{1}\n}\nfunc (m HelloReply) XXX_Unmarshal(b []byte) error {\n    return xxx_messageInfo_HelloReply.Unmarshal(m, b)\n}\nfunc (m HelloReply) XXX_Marshal(b []byte, deterministic bool) ([]byte, error) {\n    return xxx_messageInfo_HelloReply.Marshal(b, m, deterministic)\n}\nfunc (dst HelloReply) XXX_Merge(src proto.Message) {\n    xxx_messageInfo_HelloReply.Merge(dst, src)\n}\nfunc (m HelloReply) XXX_Size() int {\n    return xxx_messageInfo_HelloReply.Size(m)\n}\nfunc (m HelloReply) XXX_DiscardUnknown() {\n    xxx_messageInfo_HelloReply.DiscardUnknown(m)\n}\nvar xxx_messageInfo_HelloReply proto.InternalMessageInfo\nfunc (m *HelloReply) GetMessage() string {\n    if m != nil {\n        return m.Message\n    }\n    return \"\"\n}\nfunc init() {\n    proto.RegisterType((HelloRequest)(nil), \"helloworld.HelloRequest\")\n    proto.RegisterType((HelloReply)(nil), \"helloworld.HelloReply\")\n}\n// Reference imports to suppress errors if they are not otherwise used.\nvar _ context.Context\nvar _ grpc.ClientConn\n// This is a compile-time assertion to ensure that this generated file\n// is compatible with the grpc package it is being compiled against.\nconst _ = grpc.SupportPackageIsVersion4\n// GreeterClient is the client API for Greeter service.\n//\n// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.\ntype GreeterClient interface {\n    // Sends a greeting\n    SayHello(ctx context.Context, in HelloRequest, opts ...grpc.CallOption) (HelloReply, error)\n}\ntype greeterClient struct {\n    cc *grpc.ClientConn\n}\nfunc NewGreeterClient(cc *grpc.ClientConn) GreeterClient {\n    return &greeterClient{cc}\n}\nfunc (c greeterClient) SayHello(ctx context.Context, in HelloRequest, opts ...grpc.CallOption) (*HelloReply, error) {\n    out := new(HelloReply)\n    err := c.cc.Invoke(ctx, \"/helloworld.Greeter/SayHello\", in, out, opts...)\n    if err != nil {\n        return nil, err\n    }\n    return out, nil\n}\n// GreeterServer is the server API for Greeter service.\ntype GreeterServer interface {\n    // Sends a greeting\n    SayHello(context.Context, HelloRequest) (HelloReply, error)\n}\nfunc RegisterGreeterServer(s *grpc.Server, srv GreeterServer) {\n    s.RegisterService(&_Greeter_serviceDesc, srv)\n}\nfunc _Greeter_SayHello_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {\n    in := new(HelloRequest)\n    if err := dec(in); err != nil {\n        return nil, err\n    }\n    if interceptor == nil {\n        return srv.(GreeterServer).SayHello(ctx, in)\n    }\n    info := &grpc.UnaryServerInfo{\n        Server:     srv,\n        FullMethod: \"/helloworld.Greeter/SayHello\",\n    }\n    handler := func(ctx context.Context, req interface{}) (interface{}, error) {\n        return srv.(GreeterServer).SayHello(ctx, req.(*HelloRequest))\n    }\n    return interceptor(ctx, in, info, handler)\n}\nvar _Greeter_serviceDesc = grpc.ServiceDesc{\n    ServiceName: \"helloworld.Greeter\",\n    HandlerType: (*GreeterServer)(nil),\n    Methods: []grpc.MethodDesc{\n        {\n            MethodName: \"SayHello\",\n            Handler:    _Greeter_SayHello_Handler,\n        },\n    },\n    Streams:  []grpc.StreamDesc{},\n    Metadata: \"helloworld.proto\",\n}\nfunc init() { proto.RegisterFile(\"helloworld.proto\", fileDescriptor_helloworld_71e208cbdc16936b) }\nvar fileDescriptor_helloworld_71e208cbdc16936b = []byte{\n    // 175 bytes of a gzipped FileDescriptorProto\n    0x1f, 0x8b, 0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02, 0xff, 0xe2, 0x12, 0xc8, 0x48, 0xcd, 0xc9,\n    0xc9, 0x2f, 0xcf, 0x2f, 0xca, 0x49, 0xd1, 0x2b, 0x28, 0xca, 0x2f, 0xc9, 0x17, 0xe2, 0x42, 0x88,\n    0x28, 0x29, 0x71, 0xf1, 0x78, 0x80, 0x78, 0x41, 0xa9, 0x85, 0xa5, 0xa9, 0xc5, 0x25, 0x42, 0x42,\n    0x5c, 0x2c, 0x79, 0x89, 0xb9, 0xa9, 0x12, 0x8c, 0x0a, 0x8c, 0x1a, 0x9c, 0x41, 0x60, 0xb6, 0x92,\n    0x1a, 0x17, 0x17, 0x54, 0x4d, 0x41, 0x4e, 0xa5, 0x90, 0x04, 0x17, 0x7b, 0x6e, 0x6a, 0x71, 0x71,\n    0x62, 0x3a, 0x4c, 0x11, 0x8c, 0x6b, 0xe4, 0xc9, 0xc5, 0xee, 0x5e, 0x94, 0x9a, 0x5a, 0x92, 0x5a,\n    0x24, 0x64, 0xc7, 0xc5, 0x11, 0x9c, 0x58, 0x09, 0xd6, 0x25, 0x24, 0xa1, 0x87, 0xe4, 0x02, 0x64,\n    0xcb, 0xa4, 0xc4, 0xb0, 0xc8, 0x14, 0xe4, 0x54, 0x2a, 0x31, 0x38, 0x19, 0x70, 0x49, 0x67, 0xe6,\n    0xeb, 0xa5, 0x17, 0x15, 0x24, 0xeb, 0xa5, 0x56, 0x24, 0xe6, 0x16, 0xe4, 0xa4, 0x16, 0x23, 0xa9,\n    0x75, 0xe2, 0x07, 0x2b, 0x0e, 0x07, 0xb1, 0x03, 0x40, 0x5e, 0x0a, 0x60, 0x4c, 0x62, 0x03, 0xfb,\n    0xcd, 0x18, 0x10, 0x00, 0x00, 0xff, 0xff, 0x0f, 0xb7, 0xcd, 0xf2, 0xef, 0x00, 0x00, 0x00,\n}\n```\n. Okay, found out why. \nTurns out I had previously installed protoc-gen-go using sudo apt-get install golang-goprotobuf-dev which is an outdated version. Removed that and it works now. Thanks.. ",
    "brettbuddin": "@cstockton Have you managed to open another issue for this newer API specifically? We here at @codeship are in a similar predicament. I'd be willing to help draft a proposal and work on an implementation for it.. ",
    "yangliCypressTest": "\nHow to check the grpc version (I am a newer to grpc)\nYes\nNo, it is happened in local env without any proxy configuration\nI tried to configure these two variables in system env, and add grpclog.Errorf around the error, the error message is still \"transport is closing\", did I miss anything else?. we found the root cause yet, will close this issue. Thank you. some implemention error, the global variables of grpc client were used by mistake.. \n",
    "arindam2": "@yangliCypressTest Could you share some details of the root cause, it may help others debug similar issues.. ",
    "sam-robison-kerson": "@yangliCypressTest  i am also having a similar issue please share what fixed it. ",
    "malengatiger": "Please explain the answer - this is not enough for us to know what to do to resolve this\n. ",
    "vaishalig2693": "Even I am facing this issue. Server side logs are as below - \nINFO: 2018/10/13 23:19:32 parsed scheme: \"\"\nINFO: 2018/10/13 23:19:32 scheme \"\" not registered, fallback to default scheme\nINFO: 2018/10/13 23:19:32 ccResolverWrapper: sending new addresses to cc: [{localhost:10000 0  <nil>}]\nINFO: 2018/10/13 23:19:32 ClientConn switching balancer to \"pick_first\"\nINFO: 2018/10/13 23:19:32 pickfirstBalancer: HandleSubConnStateChange: 0xc000148060, CONNECTING\ngrpc on port: 10000\nINFO: 2018/10/13 23:19:32 pickfirstBalancer: HandleSubConnStateChange: 0xc000148060, READY\nINFO: 2018/10/13 23:19:32 transport: loopyWriter.run returning. connection error: desc = \"transport is closing\"\nINFO: 2018/10/13 23:19:32 pickfirstBalancer: HandleSubConnStateChange: 0xc000148060, TRANSIENT_FAILURE\nCan anyone help me in understanding this issue?. I am using WithInsecure. Looks like as pointed by one of the issues, it doesn't work with \"WithInsecure\". I had been using this example - https://github.com/philips/grpc-gateway-example/issues/22. I only changed TLS options with WithInsecure and was getting \"transport is closing\" again and again. \nIn this issue, it has been suggested to use cmux if we want to have common insecure port for REST and grpc . ",
    "angadn": "FWIW server-configuration is what solved the problem for me: https://stackoverflow.com/a/54703234/382564. ",
    "krzysztofdrys": "@menghanl Thanks for handling this so quickly.\nNote taken about the recent changes in package structure. I think we will start migrating our client services, after 1.14.0 is released.. ",
    "elliots": "I meant, should I provide a pull-request that adds authority support to the default dns resolver? (at least, go 1.8+). I ran into some issues with different versions of go, and this got pretty ugly pretty quickly.. Rebased on top of #2263 and renamed version specific files as suggested. \nIs there anything I can add to the tests for this functionality? . Cleaned up the default resolver stuff a little. I think it's looking ok now. Still no tests, but not sure how to do that.. All the tests in testDNSResolver are now run twice, once with no custom authority (as before) and once with one. I check to see the value has been passed in, then resolve using the test resolver.. I'm not sure how to go about the dial test.... Added a new test that only checks that the resolver receives the custom authority.. Thanks @kriskowal, I'll try to get the fixes in a bit quicker so that shouldn't be necessary. But if I don't respond for a day, you should feel free to take it and get it across the line :). @lyuxuan I think much more time was spent reviewing my changes than the changes took, thank you :) . I think we have to set it to true in order to use a custom dialler \n( according to https://github.com/golang/go/issues/19268#issuecomment-345384876 ). I'm using github.com/miekg/dns to implement my dns server, would it be ok to bring in that dependency for a test? . Or perhaps something simple like https://github.com/d-podkorytov/one_dns_go/blob/master/one_dns.go . Aha. Gotcha. Will fix later.. The dial function needs the authority var from customAuthorityResolver, so i cant really move it to a global. \nI'd end up with something like this, which seems like a lot of code for no benefit.\n```golang\nvar customAuthorityDialler = func(authority string) func(ctx context.Context, network, address string) (net.Conn, error) {\n    return func(ctx context.Context, network, address string) (net.Conn, error) {\n        dialer := net.Dialer{}\n        return dialer.DialContext(ctx, network, authority)\n    }\n}\nvar customAuthorityResolver = func(authority string) (netResolver, error) {\n    if , , err := net.SplitHostPort(authority); err != nil && strings.Contains(err.Error(), \"missing port in address\") {\n        authority = authority + \":53\"\n    }\nreturn &go18upResolver{\n    resolver: &net.Resolver{\n        PreferGo: true,\n        Dial:     customAuthorityDialler(authority),\n    }}, nil\n\n}\n```\nWhat about this instead.. Bare minimum test.\n```golang\nfunc TestCustomAuthority(t *testing.T) {\n    defer leakcheck.Check(t)\ncustomAuthority := \"4.3.2.1\"\n\noldCustomAuthorityResolver := customAuthorityResolver\ncustomAuthorityResolver = func(authority string) (netResolver, error) {\n    if authority != customAuthority {\n        t.Errorf(\"wrong custom authority passed to resolver. expected: %s actual: %s\", customAuthority, authority)\n    }\n    return nil, errors.New(\"not implemented\")\n}\ndefer func() {\n    customAuthorityResolver = oldCustomAuthorityResolver\n}()\n\nb := NewBuilder()\ncc := &testClientConn{target: \"foo.bar.com\"}\nb.Build(resolver.Target{Endpoint: \"foo.bar.com\", Authority: customAuthority}, cc, resolver.BuildOption{})\n\n}\n```. I think it didn't satisfy it because the context was the wrong type, but ill take another look.. This was the error: https://travis-ci.org/grpc/grpc-go/jobs/429394117#L577-L580. I've added some tests to https://github.com/grpc/grpc-go/blob/91db3effc124962f8472ffd846cba21ae0d699d6/resolver/dns/dns_resolver_go19_test.go but I don't actually have any experience with ipv6, so could use some help on the test cases (especially the fail cases).. Errors from the dialler aren't returned from b.Build\n```golang\ncustomAuthorityDialler = func(authority string) func(ctx context.Context, network, address string) (net.Conn, error) {\n    return func(ctx context.Context, network, address string) (net.Conn, error) {\n        return nil, errors.New(\"NOPE\")\n    }\n}\nb := NewBuilder()\ncc := &testClientConn{target: \"foo.bar.com\"}\nr, err := b.Build(resolver.Target{Endpoint: \"foo.bar.com\", Authority: a.authority}, cc, resolver.BuildOption{})\nif r != nil {\n    r.Close()\n}\nif err == nil {\n    t.Errorf(\"Should not be nil\")\n}\n```\nAm I missing something, or is that an issue with the builder?. Cool, that's what I did. Just checking if it should be returning that error. Removed the todos and recommitted.. ",
    "castaneai": "@lyuxuan \ngo get -u golang.org/x/sys/unix works fine! Thank you for advise. \ud83d\ude04 . ",
    "SamuelMarks": "Hmm, that solution doesn't work for me. Any ideas?\n```\n~/go/src/github.com/SamuelMarks/gopherci$ go get -u golang.org/x/sys/unix\n~/go/src/github.com/SamuelMarks/gopherci$ go version\ngo version go1.10.3 linux/amd64\n~/go/src/github.com/SamuelMarks/gopherci$ go build\ngithub.com/SamuelMarks/gopherci/vendor/github.com/fsouza/go-dockerclient/internal/archive\nvendor/github.com/fsouza/go-dockerclient/internal/archive/archive.go:74:20: undefined: idtools.IDPair\ngithub.com/SamuelMarks/gopherci/vendor/google.golang.org/grpc/internal/channelz\nvendor/google.golang.org/grpc/internal/channelz/types_linux.go:41:15: undefined: unix.GetsockoptLinger\nvendor/google.golang.org/grpc/internal/channelz/types_linux.go:44:15: undefined: unix.GetsockoptTimeval\nvendor/google.golang.org/grpc/internal/channelz/types_linux.go:47:15: undefined: unix.GetsockoptTimeval\n. My glide.lock has:\n- package: golang.org/x/sys\n  subpackages:\n  - unix\n```\nWith my glide.yaml having:\n- name: golang.org/x/sys\n  version: d8e400bc7db4870d786864138af681469693d18c\n  subpackages:\n  - unix\n  - windows\nHmm, manually setting the commit hash to 1c9583448a9c3aa0f9a6a5241bf73c0bd8aafded worked. Weird, I switched the project to glide to try and bump all its dependencies to latest.\nThanks. ",
    "moooofly": "+1, meet the same problem.\nThis issue is relevant with glide version control.. ",
    "XinxiChen": "\nsetting the commit hash to 1c9583448a9c3aa0f9a6a5241bf73c0bd8aafded worked\n\nThanks a lot. Manually setting the commit hash also works for me.. ",
    "tskillian": "I'm getting this when building on an ubuntu machine, and setting the commit hash to 1c9583448a9c3aa0f9a6a5241bf73c0bd8aafded or to the latest commit hash. It works fine when I'm building on my mac. Thanks for the fast response - After specifying the latest commit hash explicitly in glide.yaml, I forgot to also push the new glide.lock file for the jenkins builder. After doing that it's picking up the latest version and is working now. Thanks!. ",
    "szabado": "What's the best thing to do in the mean time?  Continue to use the deprecated Codec? . ",
    "prannayk": "What if we change the CallCustomCodec(code Codec) CallOption to CallCustomCodec(interface{}) CallOption? And then use type assertions to call the correct function, as required. This would maintain backward compatibility, and fix the implementation. \nThe type of CallCustomCodecis not asserted or used anywhere so it won't break anything. . If this is still open, I would like to get assigned to it. @dsymonds solution seems doable. . This would require updating the grpc.Stream interface to get the required codec and compressors for the PreparedMsg. Right now there is no method to access those universally for any stream.  . I think a good idea is to do the following\nChange rpcInfo to:\n```\ntype rpcInfo struct {\n    failfast.      bool\n    codec        baseCodec\n    cp              Compressor\n    comp         encoding.Compressor\n}\n```\nFollowing that, we will have to change the implementation of grpc.SendMsg for every stream individually. This should type assert m to PreparedMsg and use the marshalled and compressed data inside it if the type assertion succceeds. If assertion fails, revert to the present implemenation. This would be same for all streams, but will have to be implemented for every stream individually, since we can not make any changes to grpc.Stream.\nSince we can not add methods to grpc.Stream we can not ensure that anyone implementing a new stream adheres to this, but this will efficiently implement it for the present implementation. \nAn alternate implementation is as follows : \n1. Create a type BaseStream struct which has a member as a function pointer called sendMsgFn\n2. BaseStream implements SendMsg which first type asserts to PreparedMsg. On success it passeds hdr, payload to the function pointer as sendMsgFn(hdr, payload). If not, then create the PreparedMsg on the fly. \nThis would ensure that every stream that extends BaseStream implements this feature in the right way. Any interceptor which wants to implement this can easily do this by extending the BaseStream struct. Otherwise, we will have to copy paste this, and implement this again everytime we implement a new stream that wants to implement this. . If possible, I would like to get assigned to this. What would be the best way to start implementing this?. Completed CLA. @dfawley waiting for feedback / approval.. So I ran said benchmarks. Added functionality for preloading messages while using unconstrainedStream, and benchmarked it. As expected if you do not preload messages, and hence compress messages over and over again, it is slower. But if we preload messages, and re-send messages it is much faster (upto 4 times speed up). \nShould I create some benchmark tests where we pre-process different messages while some messages are being sent? There is no easy way to do that, any suggestions?\nOutput file: grpc_preloader_benchmarks.out.txt shows this. \nThe speed up is quite high is messages are re-used. I assume the speed up factor would be the same if the bottleneck is the network, and not the compression. . Any feedback for me on this? @dfawley \n. ",
    "jborlum": "Thank you so much for the reply. I've done some more digging and what we're seeing does not align with what you see. I'll try and elaborate.\nIt looks like the keepalives does tear down the connection as it should when the connection with the server becomes interrupted. In my case by adding an uni-directional firewall setting. The connection maintenance will try and reestablish the connection as expected. It will use the backoff logic defined here: https://github.com/grpc/grpc-go/blob/master/clientconn.go#L1235\nThis blocks on the backoff context created.\nThis can be seen here:\ngoroutine 5 [IO wait]:\ninternal/poll.runtime_pollWait(0x7fc3dae1be30, 0x77, 0xc420001200)\n    /usr/local/Cellar/go/1.10.2/libexec/src/runtime/netpoll.go:173 +0x57\ninternal/poll.(*pollDesc).wait(0xc420246518, 0x77, 0xc4200c2a00, 0xc4200c2ab0, 0x432704)\n    /usr/local/Cellar/go/1.10.2/libexec/src/internal/poll/fd_poll_runtime.go:85 +0x9b\ninternal/poll.(*pollDesc).waitWrite(0xc420246518, 0xc4200c2a00, 0x28, 0x544d56)\n    /usr/local/Cellar/go/1.10.2/libexec/src/internal/poll/fd_poll_runtime.go:94 +0x3d\ninternal/poll.(*FD).WaitWrite(0xc420246500, 0x8b88a8, 0x8f0fc0)\n    /usr/local/Cellar/go/1.10.2/libexec/src/internal/poll/fd_unix.go:440 +0x37\nnet.(*netFD).connect(0xc420246500, 0x8f0fc0, 0xc42038a420, 0x0, 0x0, 0x8ec960, 0xc420258d40, 0x0, 0x0, 0x0, ...)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/fd_unix.go:152 +0x299\nnet.(*netFD).dial(0xc420246500, 0x8f0fc0, 0xc42038a420, 0x8f22e0, 0x0, 0x8f22e0, 0xc42032fc80, 0xc4200c2cd0, 0x55f6ce)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/sock_posix.go:142 +0xe9\nnet.socket(0x8f0fc0, 0xc42038a420, 0x897090, 0x3, 0x2, 0x1, 0x0, 0x0, 0x8f22e0, 0x0, ...)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/sock_posix.go:93 +0x1a6\nnet.internetSocket(0x8f0fc0, 0xc42038a420, 0x897090, 0x3, 0x8f22e0, 0x0, 0x8f22e0, 0xc42032fc80, 0x1, 0x0, ...)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/ipsock_posix.go:141 +0x129\nnet.doDialTCP(0x8f0fc0, 0xc42038a420, 0x897090, 0x3, 0x0, 0xc42032fc80, 0xb33ac0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/tcpsock_posix.go:62 +0xb9\nnet.dialTCP(0x8f0fc0, 0xc42038a420, 0x897090, 0x3, 0x0, 0xc42032fc80, 0x101fe9f379, 0xb16300, 0x4a810d86b)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/tcpsock_posix.go:58 +0xe4\nnet.dialSingle(0x8f0fc0, 0xc42038a420, 0xc420246480, 0x8ee940, 0xc42032fc80, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/dial.go:547 +0x375\nnet.dialSerial(0x8f0fc0, 0xc42038a420, 0xc420246480, 0xc42024d110, 0x1, 0x1, 0x0, 0x0, 0x0, 0x0)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/dial.go:515 +0x22d\nnet.(*Dialer).DialContext(0xc4200c31b8, 0x8f0fc0, 0xc42038a420, 0x897090, 0x3, 0x7ffe5b8f67cc, 0x14, 0x0, 0x0, 0x0, ...)\n    /usr/local/Cellar/go/1.10.2/libexec/src/net/dial.go:397 +0x678\ngoogle.golang.org/grpc.dialContext(0x8f0fc0, 0xc42038a420, 0x897090, 0x3, 0x7ffe5b8f67cc, 0x14, 0x0, 0x0, 0x0, 0xc42039c000)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/go17.go:38 +0xb3\ngoogle.golang.org/grpc.DialContext.func1(0x8f0fc0, 0xc42038a420, 0x7ffe5b8f67cc, 0x14, 0x0, 0x0, 0x8ec060, 0xc4200ccfb0)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/clientconn.go:542 +0x4d\ngoogle.golang.org/grpc.newProxyDialer.func1(0x8f0fc0, 0xc42038a420, 0x7ffe5b8f67cc, 0x14, 0xc4200c3350, 0x427fd9, 0xc42023a6b0, 0xc4200c3370)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/proxy.go:121 +0x12a\ngoogle.golang.org/grpc/transport.dial(0x8f0fc0, 0xc42038a420, 0xc4200ccff0, 0x7ffe5b8f67cc, 0x14, 0x5a, 0x140, 0x0, 0xc420394000)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/transport/http2_client.go:129 +0x5a\ngoogle.golang.org/grpc/transport.newHTTP2Client(0x8f0fc0, 0xc42038a420, 0x8f0f40, 0xc420330b00, 0x7ffe5b8f67cc, 0x14, 0x0, 0x0, 0x7ffe5b8f67cc, 0x14, ...)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/transport/http2_client.go:162 +0x101\ngoogle.golang.org/grpc/transport.NewClientTransport(0x8f0fc0, 0xc42038a420, 0x8f0f40, 0xc4200f0180, 0x7ffe5b8f67cc, 0x14, 0x0, 0x0, 0x7ffe5b8f67cc, 0x14, ...)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/transport/transport.go:506 +0xba\ngoogle.golang.org/grpc.(*addrConn).createTransport(0xc4201e6000, 0x2, 0x0, 0xbec6f82fd6cb58ff, 0xb9b5845ef, 0xb16300, 0xbec6f834aee00889, 0x101fe9f379, 0xb16300, 0xc420330ac0, ...)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/clientconn.go:1334 +0x301\ngoogle.golang.org/grpc.(*addrConn).resetTransport(0xc4201e6000, 0x8ee340, 0xc4200a0070)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/clientconn.go:1292 +0x4d9\ngoogle.golang.org/grpc.(*addrConn).transportMonitor(0xc4201e6000)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/clientconn.go:1477 +0x46b\ngoogle.golang.org/grpc.(*addrConn).connect.func1(0xc4201e6000)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/clientconn.go:999 +0x1b5\ncreated by google.golang.org/grpc.(*addrConn).connect\n    /Users/jborlum/gocode/src/google.golang.org/grpc/clientconn.go:990 +0xe1\nAll RPCs coming in while the connection is being reestablished are blocked by the blocking picker -Despite they are defined as failfast.\ngoroutine 20 [select]:\ngoogle.golang.org/grpc.(*pickerWrapper).pick(0xc4200f0140, 0x8f1020, 0xc4202cb7a0, 0xc4201df701, 0xc4202abaf0, 0x53090c, 0xc4201df7c0, 0x0, 0x0)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/picker_wrapper.go:124 +0x53e\ngoogle.golang.org/grpc.(*ClientConn).getTransport(0xc4201c8000, 0x8f1020, 0xc4202cb7a0, 0x8f0f01, 0xc4201df7c0, 0xc4202abb58, 0x410438, 0x120, 0x88a100)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/clientconn.go:1055 +0x53\ngoogle.golang.org/grpc.(*clientStream).newAttemptLocked(0xc420352000, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x8ec2a0, ...)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/stream.go:314 +0x139\ngoogle.golang.org/grpc.newClientStream(0x8f1020, 0xc4202cb7a0, 0xb15bc0, 0xc4201c8000, 0x8a272c, 0x19, 0xc4200cfa40, 0x3, 0x3, 0x0, ...)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/stream.go:274 +0x7dd\ngoogle.golang.org/grpc.invoke(0x8f0fa0, 0xc4200d6030, 0x8a272c, 0x19, 0x85bf80, 0xc4202cb710, 0x85ba80, 0xc4202d3280, 0xc4201c8000, 0xc4200cfa40, ...)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/call.go:66 +0x9c\ngoogle.golang.org/grpc.(*ClientConn).Invoke(0xc4201c8000, 0x8f0fa0, 0xc4200d6030, 0x8a272c, 0x19, 0x85bf80, 0xc4202cb710, 0x85ba80, 0xc4202d3280, 0x0, ...)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/call.go:37 +0x1b3\ngoogle.golang.org/grpc/examples/helloworld/helloworld.(*greeterClient).Write(0xc4200da048, 0x8f0fa0, 0xc4200d6030, 0xc4202cb710, 0x0, 0x0, 0x0, 0x0, 0x8ec2a0, 0xc420328460)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/examples/helloworld/helloworld/helloworld.pb.go:173 +0xd2\nmain.main.func2(0xc4200d6670, 0x8ee460, 0xc4200da048)\n    /Users/jborlum/gocode/src/google.golang.org/grpc/examples/helloworld/greeter_client/main.go:67 +0x232\ncreated by main.main\n    /Users/jborlum/gocode/src/google.golang.org/grpc/examples/helloworld/greeter_client/main.go:62 +0x56f\nIs this expected behaviour? It seems rather weird that failfast wouldn't fail these requests when no subconn is available. What would be a good workaround for solving this problem?. Unfortunately those are not good options for the behaviour we want to achieve. The system relying on these connections communicate thousands of requests per second, and introducing a delay on each request to malfunctioning node will cause the setup to grind to a halt.\nIn that case we will need to build an abstraction on top of gRPC which allows us to separate the concerns of sending the messages through the stream and adding the messages to a buffer / queue.. I've tested it out dfawley. This is exactly the behaviour I was looking for. Thank you for the input.. ",
    "ashi009": "@thelinuxfoundation I signed.. We set dial options based on the scheme of the server address.  For instance, https:// ones will use TLS security, and k8s:// ones will use an ALTS variant for in-cluster communication.\nRight now we use a simple parser to decide what dial options to use and then let resolver to do its work.. Yep. That's what we do ATM.  However, it comes with a few caveats: \n\nurl.Parse is designed to parses a URL, not URIs. eg. [2404:6800:4004:80b::200e] is a valid name per the doc, but will fail url.Parse.\nurl.Parse().String() cannot reassemble a target name. There are cases, where we need to re-write the target name. eg. https://abc.com -> dns:///abc.com:443\n\nGiven those issues, I think this method should be defined in grpc library.. ",
    "stephanustedy": "okay.\nwill open issue and debug more.\nThanks!. ",
    "RahulMitra": "Thanks, that seems to have fixed it. After setting the MinTime to 10 seconds, I no longer see the GOAWAYs.\nJust for my understanding, do you mind explaining why the issue was fixed by setting the server's KeepaliveEnforcementPolicy.MinTime to a value smaller than the keepalive time on the client?\nAccording to the documentation for KeepaliveEnforcementPolicy:\nMinTime is the minimum amount of time a client should wait before sending a keepalive ping.\nAnd according to the documentation for GRPC_ARG_KEEPALIVE_TIME_MS:\nAfter a duration of this time the client/server pings its peer to see if the transport is still alive.\nFrom reading this, I had understood it to mean that if I set the client keepalive to 20 seconds, then the server enforcement policy should have the same value.. Apologies for the late response - was traveling last week. \nThanks for the help with this. Will run these tests today and get back to you, and will also provide you a full gist of the trial code (allows for easy parameterizing of these various channel args) so you can further test.. @dfawley \nWhen following your suggestion to set GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA to 1 and have client and server both set to 20s ping time/enforcement as initially reported, I no longer see any GOAWAYS. However, if I set GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA to 0, the GOAWAYs do return as initially reported.\nJust for clarity, here are the parameters again:\nGolang server:\nkeepalive.ServerParameters Time: 20 Seconds\nkeepalive.ServerParameters Timeout: 10 Seconds\nkeepalive.EnforcementPolicy MinTime: 20 Seconds\nkeepalive.EnforcementPolicy PermitWithoutStream: True\nC++ client:\nGRPC_ARG_KEEPALIVE_TIME_MS: 20 seconds\nGRPC_ARG_KEEPALIVE_TIMEOUT_MS: 10 seconds\nGRPC_ARG_KEEPALIVE_PERMIT_WITHOUT_CALLS: 1\nGRPC_ARG_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_MS: 10 seconds\nGRPC_ARG_HTTP2_MIN_SENT_PING_INTERVAL_WITHOUT_DATA_MS: 20 seconds\nGRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA: 1\nSetting GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA to 0 will cause GOAWAYs to occur, so it sounds like your theory was correct.. Happy to run any additional tests if that's useful. \nAlso, here is the client and server code I've been using to test the streams, in case you'd like to investigate further on your own.\nIn my case, I'm working with a bi-directional stream called SeProxyService that sends SeProxyMessage back and forth:\nservice SeProxyService {\n    rpc Connect(stream SeProxyMessage) returns (stream SeProxyMessage);\n}\nC++ Client:\n```\nusing google::protobuf::Message;\nusing grpc::Channel;\nusing grpc::ChannelArguments;\nusing grpc::ClientContext;\nusing grpc::ClientReader;\nusing grpc::ClientReaderWriter;\nusing grpc::ClientWriter;\nusing grpc::Status;\ndefine DEFAULT_KEEPALIVE_TIME_SECONDS 10\ndefine DEFAULT_KEEPALIVE_TIMEOUT_SECONDS 10\ndefine DEFAULT_KEEPALIVE_PERMIT_WITHOUT_CALLS 1\ndefine DEFAULT_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_SECONDS 10\ndefine DEFAULT_HTTP2_MIN_SENT_PING_INTERVAL_WITHOUT_DATA_SECONDS 20\ndefine DEFAULT_HTTP2_MAX_PINGS_WITHOUT_DATA 0\nDEFINE_int32(keepalive_time_seconds,                DEFAULT_KEEPALIVE_TIME_SECONDS, \"in seconds\");\nDEFINE_int32(keepalive_timeout_seconds,             DEFAULT_KEEPALIVE_TIMEOUT_SECONDS, \"in seconds\");\nDEFINE_int32(keepalive_permit_without_calls,        DEFAULT_KEEPALIVE_PERMIT_WITHOUT_CALLS, \"\");\nDEFINE_int32(min_recv_ping_interval_without_data,   DEFAULT_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_SECONDS, \"in seconds\");\nDEFINE_int32(min_sent_ping_interval_without_data,   DEFAULT_HTTP2_MIN_SENT_PING_INTERVAL_WITHOUT_DATA_SECONDS, \"in seconds\");\nDEFINE_int32(max_pings_without_data,                DEFAULT_HTTP2_MAX_PINGS_WITHOUT_DATA, \"\");\nint main(int argc, char *argv[]) {\ngoogle::ParseCommandLineFlags(&argc, &argv, true);\n\nstd::string SE_RPC_PROXY_PORT = \"5091\";\nstd::string G_CTRL_IP = \"node1.controller.local\";\nstd::string target = G_CTRL_IP + \":\" + SE_RPC_PROXY_PORT;\n\nstd::cout << \"GRPC_ARG_KEEPALIVE_TIME_MS: \" << FLAGS_keepalive_time_seconds << \" seconds\" << endl;\nstd::cout << \"GRPC_ARG_KEEPALIVE_TIMEOUT_MS: \" << FLAGS_keepalive_timeout_seconds << \" seconds\" << endl;\nstd::cout << \"GRPC_ARG_KEEPALIVE_PERMIT_WITHOUT_CALLS: \" << FLAGS_keepalive_permit_without_calls << endl;\nstd::cout << \"GRPC_ARG_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_MS: \" << FLAGS_min_recv_ping_interval_without_data << \" seconds\" << endl;\nstd::cout << \"GRPC_ARG_HTTP2_MIN_SENT_PING_INTERVAL_WITHOUT_DATA_MS: \" << FLAGS_min_sent_ping_interval_without_data << \" seconds\" << endl;\nstd::cout << \"GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA: \" << FLAGS_max_pings_without_data << endl;\n\nint SECONDS = 1000;\ngrpc::ChannelArguments ch_args;    \nch_args.SetInt(GRPC_ARG_KEEPALIVE_TIME_MS, FLAGS_keepalive_time_seconds * SECONDS);\nch_args.SetInt(GRPC_ARG_KEEPALIVE_TIMEOUT_MS, FLAGS_keepalive_timeout_seconds * SECONDS);\nch_args.SetInt(GRPC_ARG_KEEPALIVE_PERMIT_WITHOUT_CALLS, FLAGS_keepalive_permit_without_calls);\nch_args.SetInt(GRPC_ARG_HTTP2_MIN_RECV_PING_INTERVAL_WITHOUT_DATA_MS, FLAGS_min_recv_ping_interval_without_data * SECONDS);\nch_args.SetInt(GRPC_ARG_HTTP2_MIN_SENT_PING_INTERVAL_WITHOUT_DATA_MS, FLAGS_min_sent_ping_interval_without_data * SECONDS);\nch_args.SetInt(GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA, FLAGS_max_pings_without_data);\n\nauto grpc_channel_ = grpc::CreateCustomChannel(target, grpc::InsecureChannelCredentials(), ch_args);\n//auto grpc_channel_ = grpc::CreateChannel(target, grpc::InsecureChannelCredentials());\n\nif (grpc_channel_ == nullptr) {\n    std::cout << \"ERROR: Failed to make GRPC Channel\" << endl;\n    return 0;                                   \n}\nstd::cout << \"created grpc_channel to target: \" << target << endl;\n\nwhile (true) {\n\n    // Create stream from stub\n    ClientContext context;\n    std::unique_ptr<SeProxyService::Stub> se_proxy_stub = SeProxyService::NewStub(grpc_channel_);\n    std::shared_ptr<ClientReaderWriter<SeProxyMessage, SeProxyMessage> > stream(se_proxy_stub->Connect(&context));\n\n    if (stream == nullptr) {\n        std::cout << \"ERROR: Stream did not connect, retrying in 5 seconds\" << endl;                             \n        sleep(5);\n        continue;\n    }\n\n    std::cout << \"Successfully connected stream. Listening on incoming messages\" << endl;\n\n    /* Listen on incoming messages */\n    SeProxyMessage se_proxy_message;\n    while (stream->Read(&se_proxy_message)) {\n    }\n    std::cout << \"Stream broke. Re-connecting in 1 second\" << endl;                             \n    sleep(1);\n}\n\n}\n```\nInvoke as such:\n./<binary_name> --keepalive_time_seconds=20 --max_pings_without_data=1 <etc.>\nGolang server:\n```\nfunc TestStreamServer(t *testing.T) {\n    port := 5091\n// Defaults\nserverParamsTime := 10\nserverParamsTimeout := 10\nenforcementPolicyMinTime := 10\n\n// Pass other values for testing\nif os.Getenv(\"SERVER_PARAMS_TIME\") != \"\" {\n    serverParamsTime, _ = strconv.Atoi(os.Getenv(\"SERVER_PARAMS_TIME\"))\n}\nif os.Getenv(\"SERVER_PARAMS_TIMEOUT\") != \"\" {\n    serverParamsTimeout, _ = strconv.Atoi(os.Getenv(\"SERVER_PARAMS_TIMEOUT\"))\n}\nif os.Getenv(\"ENFORCEMENT_PARAMS_MINTIME\") != \"\" {\n    enforcementPolicyMinTime, _ = strconv.Atoi(os.Getenv(\"ENFORCEMENT_PARAMS_MINTIME\"))\n}\n\nglog.Infof(\"keepalive.ServerParameters Time: %+v Seconds\", serverParamsTime)\nglog.Infof(\"keepalive.ServerParameters Timeout: %+v Seconds\", serverParamsTimeout)\nglog.Infof(\"keepalive.EnforcementPolicy MinTime: %+v Seconds\", enforcementPolicyMinTime)\n\ngrpcServer := grpc.NewServer(\n    grpc.KeepaliveParams(\n        keepalive.ServerParameters{\n            Time:    (time.Duration(serverParamsTime) * time.Second),\n            Timeout: (time.Duration(serverParamsTimeout) * time.Second),\n        },\n    ),\n    grpc.KeepaliveEnforcementPolicy(\n        keepalive.EnforcementPolicy{\n            MinTime:             (time.Duration(enforcementPolicyMinTime) * time.Second),\n            PermitWithoutStream: true,\n        },\n    ),\n)\n\naddr := fmt.Sprintf(\"localhost:%v\", port)\nlis, err := net.Listen(\"tcp\", addr)\nif err != nil {\n    glog.Errorf(\"[ServiceInitialize] ERROR: Failed to listen: %v\", err)\n}\nseRpcProxy := &seRpcProxyTest{}\naviproto.RegisterSeProxyServiceServer(grpcServer, seRpcProxy)\n\nif lis != nil {\n    go grpcServer.Serve(lis)\n}\n\n// Main goroutine sits idle\nfor {\n    time.Sleep(time.Duration(60) * time.Second)\n}\n\n}\nInvoke as such:\nSERVER_PARAMS_TIME=20 ENFORCEMENT_PARAMS_MINTIME=20  ./\n```. ",
    "yashykt": "The C++ client also has the same scaling mechanism. Without extensive logs I won't be able to tell for sure, but possibly, this is being caused by GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA being set to 0 which means that the client will continue sending pings even if it does not have data to send. Please set it to 1/2 instead.. I am guessing that scaling is being honored. It's just that even after scaling, we are sending pings without data even though are spaced out. The server is probably not expecting that.. Just for context - this is where C++ scales the keepalive time\nhttps://github.com/grpc/grpc/blob/f9f5c67aff91e4ad26371b0a2482a5011ab45226/src/core/ext/transport/chttp2/transport/chttp2_transport.cc#L1098. @RahulMitra One more thing to verify - Please try using the latest version instead. \nIf the error persists after these experiments, I would like to have a look at the logs too with env var GRPC_TRACE set to 'http' and GRPC_VERBOSITY set to DEBUG\nIt'll also be useful to get a gist of your trial code, so we can reproduce it easily.. @RahulMitra It would be easier if you just create a gist so that I have access to the entire code including the protos and the build files.\nLuckily, I had similar code lying around, so I was quickly able to reproduce this.\n@dfawley your intuition is right. C++ does not seem to be scaling this. The issue is that the updated keepalive_time is maintained locally as part of the transport struct but we create a new transport for each subchannel, which means that this updated value is now lost, unless we are somehow able to update the channel args from inside the http2 transport. This seems to be breaking layers and will need a bit of a design discussion. Let me get back to you on this.\n. Going to track this in https://github.com/grpc/grpc/issues/16210 instead since it is a grpc core issue. ",
    "itizir": "Indeed not sure what the right fix would be, but it did look like the break was perhaps meant to interrupt the for loop. Thanks for looking into it in any case! Happy I could be of some help at all.. ",
    "mfycheng": "Added more context to the FullMethodName field.. ",
    "takakawa": "It happens in our production envrionments from time to time . We tried to figure out this problem,It seemed that setting the FailFast true would cause the problem. When nginx closes connection after http2_max_requests  ,the client has a big possibility to show 'There is no connection available' .\nWhen we set   http2_max_requests 1000000 ,the possibility became very low.\nHere is a same question:\nhttps://trac.nginx.org/nginx/ticket/1590. ",
    "uschen": "documentation and example will be nice. especially there are timeout, frequency and other stuffs which can be confusing in the beginning.. seeing quite a few of this error with google cloud go\nhttps://github.com/GoogleCloudPlatform/google-cloud-go/issues/479. ",
    "Derliang": "@lyuxuan  Thanks!  documentation and example will be nice. Also I can read source code to know how to use this dns-resolver.. ",
    "MaerF0x0": "I forget the name of the laws of APIs, but it goes something like this: Given enough consumers of an API even if not documented/supported  someone is depending on it.  That was me for the internal/transport \nsomething like: \ngo\n            if err == transport.ErrConnClosing {\n                log.Printf(\"[ERROR err=%q\", err)\n                break SomeJump // The transport is closing, afaik this is not a bug\n            }\nw e'll pin to 1.13 . AFAIK that should have been a major version increment though. (2.0.0) \n. ",
    "damianoneill": "agree with @MaerF0x0 this is major version increment. . ",
    "zjshen14": "@dfawley, yes we bind to a fixed port. Thanks for explaining the root cause. ",
    "huyntsgs": "@dfawley thanks for your feedback. I set environment variables from my computer window. I added logs to grpc code and got correct these environment variables that have been set. Set to \"0\" and restart also not work, log messages still exist.. I tried to use \"setx\" but not work. I think setting env variables work. Because in gprclog loggerv2.go, I printed vLevel and  logLevel. They are the same values as I set. You are right, our system uses a custom logger for grpc. Thx!. ",
    "ShiningRush": "It looks like \"/internal/transport\" is a substitute,\nShould i replace all of old transport's reference with it?. @dfawley I am a user of micro, but i also want to help micro resolve the issue.\nI have read the code using transport package in micro, find out the reason depended on transport is that micro decoupled server's abstraction and implement, grpc plugin is a implement of server, it used transport package to accept connection and handle it and act like a standard grpc server.\nthe code detail can look here.\nso is there other way can do same thing without transport package?. @dfawley Yes, i already did the workaround, and i also will follow the issue on micro's slack, thanks.. ",
    "quanjielin": "it's my env issue, pls ignore.\nbtw, when v1.13.1 for go will be out ? https://github.com/grpc/grpc/releases/tag/v1.13.1 is out yesterday. . @lyuxuan, another question - \nif I updated grpc sha in our repo to point to your repo's latest commit ex (445634bdcc9393d2681e504aafd3efc9b28c4bf2)https://github.com/grpc/grpc-go/commits/master, will it include the PR we needed(https://github.com/grpc/grpc/pull/15909) ?  If, our testing could be unblocked; If not, to double confirm- the major release in 07/31 will include https://github.com/grpc/grpc/pull/15909(go/c++ version in sync) ? thanks !. @lyuxuan , thank you !  /cc @qiwzhang @wattli\nThe reason I asked is our feature need envoy to fetch some google credential(https://github.com/envoyproxy/envoy/pull/3577)(some token from GKE) and send it to server through UDS. \nA few more questions - \n1. does grpc-go already supported sending google credential through UDS securely ?\n2. if yes for 1, is TLS required(setting up channel_credentials) at the same time ?\nbtw, if possible, could you pls shoot us an internal email, that would be faster(sorry I couldn't find your ldap..)\n. I gave a try by setting up TLS\n1. envoy gRPC config\n```\ngrpc_services {\n  google_grpc {\n    target_uri: \"unix:/var/run/sds/uds_path\"\n    channel_credentials {\n      ssl_credentials {\n        root_certs {\n          filename: \"/etc/istio/nodeagent-root-cert.pem\"\n        }\n      }\n    }\n    call_credentials {\n      google_compute_engine {\n      }\n    }\n    stat_prefix: \"test\"\n  }\n}\n2. server side error\nransport: http2Server.HandleStreams failed to receive the preface from client: EOF\"\n2018-07-21T01:30:37.250966Z info    grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"\n2018-07-21T01:31:42.641122Z info    grpc: Server.Serve failed to create ServerTransport: connection error: desc = \"transport: http2Server.HandleStreams failed to receive the preface from client: EOF\"\n```\n\nserver side hit same error if remove call_credentials from client config, like \n```\n                grpc_services:\n                - google_grpc:\n                    stat_prefix: test\n                    target_uri: unix:/var/run/sds/uds_path\n                    channel_credentials:\n                      ssl_credentials:\n                        root_certs:\n                          filename: /etc/istio/nodeagent-root-cert.pem\n\n```\n\nenvoy uses c++; our server uses go;\n    if I wrote a simple grpc client using go to setup channel cred(TLS) without call cred, we didn't see this error. server side error is thrown from https://github.com/grpc/grpc-go/blob/11b582728a13ef54e14e4fa930d693de2e32420f/internal/transport/http2_server.go#L264 \n\nClientPreface are defined https://godoc.org/golang.org/x/net/http2, \n// ClientPreface is the string that must be sent by new\n    // connections from clients.\n    ClientPreface = \"PRI * HTTP/2.0\\r\\n\\r\\nSM\\r\\n\\r\\n\" \ngrpc-go client append ClientPreface, need to check if envoy/grpc-c++ do the same ?\nhttps://github.com/grpc/grpc-go/blob/11b582728a13ef54e14e4fa930d693de2e32420f/internal/transport/http2_client.go#L274. /cc @qiwzhang @wattli . envoy_grpc.log\n@htuch attached log file, let's know if it's good enough. /cc @yihuazhang\nwe built an envoy binary which takes https://github.com/grpc/grpc/pull/15909, and use this envoy to send request to server in GKE env.\n\n\nserver could receive request from envoy client(with client config below)\ntls_context:\n      common_tls_context:\n        tls_certificate_sds_secret_configs:\n        - name: \"spiffe://cluster.local/ns/default/sa/default\"\n          sds_config:\n            api_config_source:\n              api_type: GRPC\n              grpc_services:\n              - google_grpc:\n                  stat_prefix: test\n                  target_uri: unix:/var/run/sds/uds_path\n              refresh_delay: 60s\n\n\nserver didn't receive request from envoy client if call_credentials is set in client config\ntls_context:\n        common_tls_context:\n          tls_certificate_sds_secret_configs:\n          - name: \"spiffe://cluster.local/ns/default/sa/default\"\n            sds_config:\n              api_config_source:\n                api_type: GRPC\n                grpc_services:\n                - google_grpc:\n                    stat_prefix: test\n                    target_uri: unix:/var/run/sds/uds_path\n                    call_credentials:\n                      google_compute_engine: {}\n                refresh_delay: 60s\n\n\n@yihuazhang, no channel_cred/TLS is needed if envoy takes your PR(https://github.com/grpc/grpc/pull/15909), is that correct ?. close this issue as we reach the conclusion - \n1. to unblock local testing envoy->nodeagent, workaround is deploying nodeagent + envoy in same node in GKE, then apply TLS\n2.  eventually needs envoy/grpc support for local credential. ",
    "qiwzhang": "@lizan, @PiotrSikora,  @htuch, could you help double checking the channel credentials for google_grpc config?   . @yihuazhang Yes, definitely we need c++ interface for local credential.  Could you help?\nBTW, Is It possible for gRPC code to set the local credential automatically if UDS is used ?. ",
    "htuch": "@qiwzhang config seems legit, can you provide the Envoy-side trace with GRPC_TRACE=ssl,http GRPC_VERBOSITY=debug set in the environment? I'd like to see what is going on at the wire. Also a -l trace could be revealing to understand what Envoy's gRPC async client is seeing.. ",
    "lizan": "From the log, error code 13 means something happens in either client side or server side grpc library.\n@quanjielin instead of 4, what happen if you write a simple grpc client with C++?. @quanjielin  IIUC with https://github.com/grpc/grpc/pull/15909 you still need channel_cred, which introduced in the PR called local_credentials. It doesn't seems the PR have C++ interface but C interface only. I suspect it is not completed yet, @yihuazhang ?. ",
    "yihuazhang": "As Lizan pointed out (and also in the previous email thread), channel_creds\nare still needed to be provided since local credentials is another type of\nchannel credentials (same as ssl credentials). Yes, the PR (grpc/grpc#15909\nhttps://github.com/grpc/grpc/pull/15909) only provided C core interface,\nnot for C++. If you need that of C++, please let me know, I can add the\ninterface for that.\nOn Mon, Jul 23, 2018 at 2:58 PM Lizan Zhou notifications@github.com wrote:\n\n@quanjielin https://github.com/quanjielin IIUC with grpc/grpc#15909\nhttps://github.com/grpc/grpc/pull/15909 you still need channel_cred,\nwhich introduced in the PR called local_credentials. It doesn't seems the\nPR have C++ interface but C interface only. I suspect it is not completed\nyet, @yihuazhang https://github.com/yihuazhang ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/grpc/grpc-go/issues/2235#issuecomment-407214570, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AZr-a4mQ4GEKGdBZ7Bio9u0jaxt_7Sftks5uJkcTgaJpZM4VbdT-\n.\n. Sure, I can work on creating the C++ interface. For the 2nd question, if you want to use UDS to exchange security tokens which means we need to create a secure channel, I do not think the current C core surface API grpc_secure_channel_create allows you to do so as it takes credentials as one of the arguments.. \n",
    "tianxiaoliang": "Ok, that sounds easier, I will have a try. \nas you see I am trying to use http proxy, it is no working, plz help . so I must directly dial proxy port, but there is another problem, the client must tell me the target instead of IP:Port, the proxy will play the discovery role, which means it discovery ip port based on name and redirect\nSo I need client written like this \ngrpc.DialContext(context.TODO(), \"name\", grpc.WithInsecure())\nin that case I must use another option to specify the proxy IP and port, how?. if this is nessary, how I can make h2c and http1.1 listen on same port?. User must set http_proxy to easily use my http2 proxy, otherwise user has to modify there code and rebuild to use custom dialer. it is not good experience.\n. https://github.com/go-mesh/mesher-examples/commit/cd5b9075e8df002ec78763fe099a44f9179a5666#diff-24877fc8deec3e63a4c07324c1e19240\npreviously user of my proxy use custom dialer, because I implemented a grpc proxy, not a http2 proxy.\nto improve experience of usage, I rewrote my proxy from grpc proxy to http2 proxy. but now I am facing the issue of http1.1 hand shake. yes, I can set the custom dialer. but why default dialer will do the http 1.1 handshake to a proxy? grpc can use http1.1 proxy?. ok, so as a grpc proxy it must support http2 and http1.1 on same listen port. ",
    "leaxoy": "Sorry for the delay, last week I am a little busy and had no spare time to update, I will update it this weekend. . The resolver package can resolve upstream server from dns or passthrough, in addition, user can implemente other protocol like etcd(with a target maybe looks like etcd://etcd-server-list/endpoint), consul(with a target consule://consol-agent/endpoint), but we have not a unifying package to register a upstream server to etcd or consul when a server started, and deregister it at server shutdown,  and the resolver.Address is not enough for various strategy of balancer, like weight base round robin. . So I think it's useful to add a resolver to unify the interface register and deregister a service and give a more rich Address definition for client load balancing.. As the Resolver wrapped as resolverBuilder, and Balancer as balancerBuilder, those two act as a field of dialOptions struct, we can add registryBuilder to Server.options. \nThe Registry interface should has three method\n1. Name() string returns the name of this registry, like etcd, consul,zk.\n2. Register(server   struct act as a common service definition) error. Do real register action. maybe invoked after net.Listen started, or after the server started.\n3. Deregister(same with Register method) error. Do deregister action. Maybe invoked after server stop, and before the runtime shutdown . Yeah, but I still think it's convenient if grpc bind with this feature. In any case, thanks for your advice.. ",
    "fastest963": "@dfawley Thanks for the quick follow-up and revert! . It happened roughly 30 minutes after deployment. Not sure if the graphs help at all as far as timeline but if you could put the change back in a branch or in a fork that I could replace before build I could try and help repo it on my end on a staging server if you want to add some debugging logs or something else to the original change. Let me know.. @jadekler Thanks for update! I'll test this out tomorrow and report back.. @jadekler Everything looks good to me. I deployed\ngoogle.golang.org/grpc (git) - fa6e9ca fix cancel check\nto a single production server earlier today and the memory usage over the last 90 minutes seems to be stable:\n\nI'll keep it running overnight and report back if memory spikes or anything bad happens. Thanks for following up and working on this. @jadekler observing the last 24 hours shows no unbounded growth in memory and the memory looks similar to other boxes not running with your patch. \ud83d\udc4d . ",
    "Wang-Kai": "Thank you very much .\n\uff081\uff09I am true i got the correct addresses\uff0cin my Builder (implement resolver.Builder) watch function, i print received addresses , it is correct .\n```golang\nfunc (b *Builder) watch(keyPrefix string) {\n    var addrList []resolver.Address\n// first get all address under this keyPrefix\nresp, err := cli.Get(context.Background(), keyPrefix, clientv3.WithPrefix())\nif err != nil {\n    log.Fatal(err)\n}\n\nfor _, kv := range resp.Kvs {\n    addr := resolver.Address{Addr: strings.TrimPrefix(string(kv.Key), keyPrefix)}\n    addrList = append(addrList, addr)\n}\n\nfmt.Printf(\"%+v \\n\", addrList)\nb.cc.NewAddress(addrList)\n   .......\n\n}\n```\n[{Addr:127.0.0.1:10013 Type:0 ServerName: Metadata:<nil>}]\n[{Addr:127.0.0.1:10015 Type:0 ServerName: Metadata:<nil>}]\n(2) I don't know how to run client.go with GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info \nIt is  go run client.go GRPC_GO_LOG_VERBOSITY_LEVEL=99  GRPC_GO_LOG_SEVERITY_LEVEL=info  ?. Thank you very much .\nWhen i run   GRPC_GO_LOG_VERBOSITY_LEVEL=99 GRPC_GO_LOG_SEVERITY_LEVEL=info go run main.go , i got \nINFO: 2018/08/08 09:33:50 ccResolverWrapper: sending new addresses to cc: [{127.0.0.1:10015 0  <nil>}]\nINFO: 2018/08/08 09:33:50 ccResolverWrapper: sending new addresses to cc: [{127.0.0.1:10013 0  <nil>}]\nINFO: 2018/08/08 09:33:50 base.baseBalancer: got new resolved addresses:  [{127.0.0.1:10013 0  <nil>}]\nINFO: 2018/08/08 09:33:50 base.baseBalancer: handle SubConn state change: 0xc4202b2100, CONNECTING\nINFO: 2018/08/08 09:33:50 base.baseBalancer: handle SubConn state change: 0xc4202b2100, READY\nINFO: 2018/08/08 09:33:50 roundrobinPicker: newPicker called with readySCs: map[{127.0.0.1:10013 0  <nil>}:0xc4202b2100]\nMy serverA listen on 127.0.0.1:10013, and serverB listen on 127.0.0.1:10015\nAs my code :\n```golang\nfunc init() {\n    // register resolver\n    hiBuilder := hi.NewResolverBuilder([]string{\"localhost:2379\"})\n    resolver.Register(&hiBuilder)\n// build connection of serverB\nserverBConn, err := grpc.Dial(hiBuilder.Scheme()+\"://author/serverB\", grpc.WithInsecure(), grpc.WithBalancerName(\"round_robin\"))\nif err != nil {\n    log.Fatal(err)\n}\nprintln(serverBConn)\nConnMap[\"serverB\"] = serverBConn\n\n// build connection of serverA\nserverAConn, err := grpc.Dial(hiBuilder.Scheme()+\"://author/serverA\", grpc.WithInsecure(), grpc.WithBalancerName(\"round_robin\"))\nif err != nil {\n    log.Fatal(err)\n}\nprintln(serverAConn)\nConnMap[\"serverA\"] = serverAConn\n\n}\nfunc main() {\n    defer func() {\n        for _, cc := range ConnMap {\n            cc.Close()\n        }\n    }()\ntime.Sleep(time.Second * 3)\n\nfor range time.Tick(time.Second * 3) {\n    println(\"Call serverB\")\n    serverBClient := pb.NewServerBClient(ConnMap[\"serverB\"])\n\n    println(\"New Client\")\n    helloResp, err := serverBClient.Hello(context.Background(), &pb.HelloReq{Name: \"China\"})\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Printf(\"%+v \\n\", helloResp)\n\n    println(\"Call serverA\")\n    serverAClient := pb.NewServerAClient(ConnMap[\"serverA\"])\n    hiResp, err := serverAClient.Hi(context.Background(), &pb.HiReq{Name: \"kai\"})\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Printf(\"%+v \\n\", hiResp)\n}\n\n}\n```\nMy program is hanging  after calling serverB . I don't know what is the problem . \n. Thanks for your patient, thanks for your advice , i solved my problem . . ",
    "rfyiamcool": "run after go get -u golang.org/x/sys/unix. ",
    "felixwang66": "@aaronbee Thank you so much sir!! You saved my day. I haven't tested very much but it looks metadata will be a good solution. Will let you know!!. ",
    "trvsysadmin": "Thanks for the reply. \nWould it be possible for us to mock the simple retry deadline mechanism, even if it's hackish, while we wait for hedging to be implemented without severely affecting grpc internals? As we are not too familiar with the code base, any hints at the right direction would be deeply appreciated.. Another quick clarification: if the service does not provide a service/method config, is there a default time out set for all rpc calls? Asking this in case we have to use a unary interceptor and some hardcoded grpc deadline will also override the interceptor. Thanks!. Ok, we will try that out. Thanks again for the help!. Thanks @menghanl - We ended up using https://github.com/grpc-ecosystem/go-grpc-middleware/tree/master/retry. Will move over to hedging once it is implemented.. ",
    "arturgspb": "@lyuxuan, thx a lot!\nFrom similar programs I found only Google Cloud Endpoints, where not necessarily SSL and some complicated proxy configuration.\nBut there are no real simple working examples of how to do without using Google Cloud Endpoints.\nThere are examples where I need to register a service from a protobuf file on a proxy server, i.e. where the proxy should know which services it proxy, but it does not suit me.\nI would be very helped by a simple example of one file in which the structure generated from the proto file would not be used.\nFor example, something with the use of \"github.com/google/tcpproxy\" or something like that. It is important that the proxy does not know what data and service/method  proxy, but simply read and write the headers, and if unsuccessful authorization would return an error and would not proxy to the backend.\nI'm very sorry for the machine translation through google translate, I hope my problem is clear to you and you will help me.\nThank you in advance!\n. I decided my question.\nYou can run google endpoints api in your local cluster and check through JWT accesses check and with RPS quotas!\nYour actions:\n1) Create a project on google cloud, add a service account\n2) Generate api_descriptor.pb from your proto files and upload to google cloud endpoint\n3) Make api_config.yaml and specify there how to check the token\n4) Launch in your cluster image gcr.io/endpoints-release/endpoints-runtime:1 with the configured backend\nDocumentation:\n\nhttps://cloud.google.com/endpoints/docs/grpc/configure-endpoints\nhttps://cloud.google.com/endpoints/docs/grpc/running-esp-localdev\nhttps://cloud.google.com/endpoints/docs/openapi/authenticating-users#configuring_extensible_service_proxy_to_support_client_authentication\nhttps://github.com/GoogleCloudPlatform/python-docs-samples/tree/master/endpoints/getting-started-grpc\n\nExample configs:\napi_config.yaml\n```yaml\nhttps://cloud.google.com/endpoints/docs/grpc/quotas-configure\ntype: google.api.Service\nconfig_version: 3\nname: YOUR_API_NAME.endpoints.YOUR_PROJECT_ID.cloud.goog\ntitle: YOUR_API_NAME gRPC API\napis:\n- name: helloapi.HelloService\nusage:\n  rules:\n  - selector: \"*\"\n    allow_unregistered_calls: true\nauthentication:\n  # https://github.com/googleapis/googleapis/blob/master/google/api/auth.proto\n  providers:\n  - id: google_service_account\n    issuer: \"jwt-client.endpoints.sample.google.com\"\n    audiences: \"YOUR_API_NAME.endpoints.YOUR_PROJECT_ID.cloud.goog\"\n    jwks_uri: \"https://www.googleapis.com/robot/v1/metadata/x509/test-esp@YOUR_PROJECT_ID.iam.gserviceaccount.com\"\n  rules:\n  - selector: \"*\"\n    requirements:\n      - provider_id: google_service_account\nmetrics:\n  - name: \"ReadCallLimits\"\n    display_name: \"Read requests\"\n    value_type: INT64\n    metric_kind: DELTA\nquota:\n  limits:\n    - name: \"read-limit\"\n      metric: \"ReadCallLimits\"\n      unit: \"1/min/{project}\"\n      values:\n        STANDARD: 100\n  metric_rules:\n    - selector: \"*\"\n      metric_costs:\n        \"ReadCallLimits\": 1\n```\ndeployment.yaml\n```yaml\nhttps://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/endpoints/kubernetes/k8s-grpc-bookstore.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: grpc-hello\nspec:\n  ports:\n  - port: 80\n    targetPort: 9000\n    protocol: TCP\n    name: http2\n  selector:\n    app: grpc-hello\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: grpc-hello\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: grpc-hello\n    spec:\n      volumes:\n        - name: service-account-creds\n          secret:\n            secretName: service-account-creds\n      imagePullSecrets:\n        - name: registry-secret\n      containers:\n      - name: esp\n        image: gcr.io/endpoints-release/endpoints-runtime:1\n        args: [\n          \"--http2_port=9000\",\n          \"--backend=grpc://127.0.0.1:50051\",\n          \"--service=YOUR_API_NAME.endpoints.YOUR_PROJECT_ID.cloud.goog\",\n          \"--service_account_key=/etc/nginx/creds/service-account-creds.json\",\n          \"--rollout_strategy=managed\",\n        ]\n        ports:\n          - containerPort: 9000\n        volumeMounts:\n          - mountPath: /etc/nginx/creds\n            name: service-account-creds\n            readOnly: true\n      - name: python-grpc-hello\n        image: YOUR_BACKEND_IMAGE\n        ports:\n          - containerPort: 50051\n```. ",
    "bradyjoestar": "@lyuxuan   Thanks for help! Appreciate it!. ",
    "kriskowal": "Looks like CI fails a vet check, and cursory inspection suggests unrelated issue with generated code mismatching expectations.. Hello, @lyuxuan. We are very interested in this feature at Uber. Please let us know what we can do to support its progress to release. Thank you @elliots, I was assigned this issue the day before you submitted your change!. @lyuxuan do you have recommendations for improving test coverage? Asking for a friend ;). @elliots Please let me know if you would like me to take a round of addressing feedback. I can send a commit from a fork if need be.. Yes! :pray:. Filename becomes a bit of a misnomer. Idea: go18down, go19up.. Looks good, pending vet fix.. Should we pass network through instead of punching in \"udp\"? I believe DNS as a protocol at least has the option of falling back to TCP if messages exceed the UDP datagram size limit.. Creating a fake or mock for this interface might be the key to building up some coverage. We could also subvert a global function pointer for customAuthorityResolver for the duration of tests.\nAlternately we can stand up a UDP socket and fake the DNS protocol to verify that the custom authority catches the traffic.. ",
    "tywkeene": "Looks like this merge is broken\n```\ngoogle.golang.org/grpc/resolver/dns\n../../google.golang.org/grpc/resolver/dns/go19.go:30:18: undefined: netResolver\n../../google.golang.org/grpc/resolver/dns/go19.go:42:55: undefined: netResolver\n../../google.golang.org/grpc/resolver/dns/go19.go:43:32: too many arguments in call to parseTarget\n    have (string, string)\n    want (string)\n```\nBuilding in golang:1.10 on docker 17.12.1-ce\n. > @tywkeene, can you provide more info about how this merge breaks you?\nIt's just a pretty cut and dry typo/build error that I posted above.\nNot sure what else I could post that would help. Could you be more specific?. > I am wondering how you updated your code base and got this failure(details like what's the command you use to update the code base and build, etc. will be helpful).\nRUN go get google.golang.org/grpc\nIn my Dockerfile from the golang:1.10 image. I'm curious how this made it through the CI\nI've resolved this issue by pinning to the previous commit using dep. ",
    "nomis52": "Are there any estimates on when this will be available in a release ? We're eager to have it.. That works. We can hold until then.. I'd prefer the error. If I asked for a customer resolver there is likely a good reason I don't want to use the default one. . ",
    "virtuald": "To workaround this, I used a modified version of the blocking code in DialContext and ended up with using something similar to this instead of WithBlock:\n```\nerrChan := make(chan error, 1)\ndialer := grpc.WithDialer( func(addr, timeout) {\n    conn, err := dial()\n    if err != nil {\n        select { \n        case errChan <- err:\n        default:\n        }\n    }\n    return conn, err\n)\nconn, err := grpc.DialContext(ctx, addr, dialer)\nif err != nil {\n    return err\n}\nfor {\n    s := conn.GetState()\n    if s == connectivity.Ready {\n        return conn, nil\n    } else if s == connectivity.TransientFailure {\n        select {\n        case err := <-errChan:\n            if permerror.IsTemporary(err) != permerror.Temporary {\n                conn.Close()\n                return nil, err\n            }\n        case <-ctx.Done():\n            return nil, ctx.Err()\n        }\n    }\n    if !conn.WaitForStateChange(ctx, s) {\n        return nil, ctx.Err()\n    }\n}\n```. My use case is I was connecting to a GRPC server via a custom dialer. There are certain types of errors which I know I want to fail immediately so I was hoping this would do the trick. My client is effectively a proxy with expensive connection setup so I was caching my grpc connection objects, and if the underlying connection immediately fails I'd rather not cache it in that case.\nI get that it's really difficult to determine if an error is permanent or not -- but presumably if someone sets FailOnNonTempDialError, they would expect failure to occur if a non-temporary failure occurs. You get what you asked for, right? And if FailOnNonTempDialError isn't set, then it should retry over and over again as it does now.. Yes, that's about right. However, I also want it to fail fast -- basically, I want my workaround code above. I don't see how 2055 solves that.. Pushed a potential fix for this in #2276. Added CLA.. Also, thinking about it, should a note be added to the documentation that FailOnNonTempDialError is only useful if WithBlock is also used? It's not immediately clear to me that it does anything if not using WithBlock... . Added documentation note also.. Why don't any of the other tests do that then? . ",
    "dweomer": "I am seeing this in containerd/containerd#2576 when attempting to connect to unix sockets that either do not exist or the user doesn't have the proper permissions to read/write. My expectation is that the dial attempt would fail immediately (aka fail fast) in these two scenarios but instead I get \"context deadline exceeded\".. ",
    "jrick": "Yep, this fixes them.. ",
    "bwplotka": "\nNote that the default DNS resolver will be changed soon to not poll at all.\n\n@dfawley  Can you give more info about this one?. ",
    "davidklassen": "CLA signed. ",
    "zchee": "I want to sign-up CNCF CLA for this pull request, but hellosign displayed page not found.\nWhat should I do?\nThanks.. ",
    "lokanadhamm": "In that case why can't load balancer itself ignore those servers in serverlist and send only ready ones?. ",
    "MOZGIII": "@nicolasnoble understandable. On the other hand, wasm on it's own does not have any networking support, but we can declare an API that nodejs can fulfill. Browsers are not a major concern for this, but the browser may be able to fulfill that API is at some point in the future too.\nIn fact, maybe it should be not in Go, but C.\n@mastersingh24 thanks. I'm actually using gRPC-Web already, along with node-grpc.. @hsaliak tl;dr: it's purely a \"cool to have\" at this point.\nThe original idea for this PR was to use Go instead of node-grpc in a nodejs-based API gateway. I was elaborating on options to better integrate generated code with with TypeScript - which I resolved even before creating this issue (it doesn't seem to reduce the effort much) - so don't mind that, just mentioning it as a context.\nAfter some time I though a bit and created this issue. It's not on our critical path, and is more like a nice to have. The benefits of it on it's own (without TypeScript definitions to allow type-checked code) would be to share interceptors and metadata handling procedures with the the Go code bases. Potentially sharing load balancing / discovery logic too - but we're more likely to solve it by introducing service mesh (i.e. istio or sth). And even interceptors/metadata stuff is solved differently.\nWe're already using gRPC-Web in browser, but I mainly envisioned using wasm grpc-go at the server-side in a nodejs server. That means, Go-via-C or other kind of nodejs binding would work too.\nAlso, I was just curious if somebody is working on this - cause it's cool :)\nNow if I think about it, I'd rather have #906 done so that wasm support can be added by users on a need basis, rather than explicitly supporting it. However, it's a good idea to test the baseline - whether the code builds under wasm or not (with some dummy transport) - and keep that part working.. Transport support for wasm would be effectively solved by #906 (at least I'll be able to plug in my own transport), but ensuring that the code builds with dummy transport should be address explicitly. In that sense, I'd keep this issue blocked until #906 is finished, implemented tests to make sure wasm works (with dummy transport), and only then close this one.. ",
    "mikeraimondi": "No rush on this, so let's wait the 2 weeks for 1.8 support to be dropped. I addressed the linter issues and (I hope) the lack of Accept() and reading on the server side.. Unsure if the recent CI failure in 1.11 is due to my changes or a pre-existing flake.. @dfawley all set!. ",
    "srikrsna": "I've made a separate proto plugin that solves this: https://github.com/srikrsna/protoc-gen-defaults. ",
    "spzala": "/cc @gyliu513. @menghanl thanks. I am OOO but I quickly tried it without any luck of seeing any expected behavior. I am not sure if I am doing anything wrong with the way I am testing but if you get a chance, can you please try the modified code I have mentioned about? I guess they are simple and pretty quick to run. Thanks again! . @menghanl thank you so much, it works the way you suggested. Also, I could set up PermitWithoutStream: false on server side and that works too. However, it requires to set PermitWithoutStream: true on the client side. Even if I make call from client as below snippet, if PermitWithoutStream is set to false then it won't work. So a related question, is there a way I can keep PermitWithoutStream to false and still see the goaway messages? Thanks again!! \nc := pb.NewGreeterClient(conn)\n        // Contact the server and print out its response.\n        name := defaultName\n        ctx, cancel := context.WithTimeout(context.Background(), 20*time.Second)\n        defer cancel()\n        for {\n                r, err := c.SayHello(ctx, &pb.HelloRequest{Name: name})\n                if err != nil {\n                        log.Fatalf(\"could not greet: %v\", err)\n                }\n                <-time.After(4*time.Second)\n                log.Printf(\"Greeting: %s\", r.Message)\n        }\nI also tried examples/route_guide/client with adding keepalive param and some wait time, but that also requires me to set PermitWithoutStream:true to see EnhanceYourCalm goaway. . @menghanl thanks a lot!! The client do call RouteChat https://github.com/grpc/grpc-go/blob/master/examples/route_guide/client/client.go#L191 but I tried it as such without modifying. Lemme work on it and test it. I will be closing this issue soon then. . @menghanl With RouteChat it works, if I don't close the stream and do nothing it does produce http2.ErrCodeEnhanceYourCalm. I am good now and closing this issue. Thanks much again for your help understand this and creating https://github.com/grpc/grpc-go/pull/2342   . @menghanl very helpful doc updates. Thanks!!. ",
    "gyliu513": "/cc @meling @menghanl can you help check? As I saw you updated this example recently. Thanks!. ",
    "akshayjshah": "Thanks for the info, Doug. Will the discussion for this happen in an open venue (e.g., a Github issue), or is this something that'll be Google-internal?. A spec describing how the different implementations behave would be nice, especially if it gives some prescriptive advice for third-party implementations.\n\nI'd recommend just relying on grpc-status and grpc-message HTTP headers. The benefit of the details is really the repeated Any.\n\nInternally, it's (unfortunately) not viable for us to rely solely on the grpc-status and grpc-message headers. Our clients will be communicating with servers using a variety of codecs; a number of those servers use the standard HTTP/2+proto setup, and clients will be significantly handicapped if they can't access the structured error data (e.g., the server's recommended retry policy).. Any updates on this?. Are you open to a PR that (optionally) disables the checks that prevent us from writing to grpc-prefixed headers? I understand that there are a lot of interoperability concerns between the different gRPC implementations that need to be worked out, but it'd be nice to have an escape hatch available.. ",
    "banks": "There is a good chance this issue is bogus. Since writing it up I've observed some behaviour I can't really explain.\nFirst I noticed that I never saw the same Warning on stderr when running the same command that is run in a test in my terminal. After much debugging, i finally restarted VSCode which was the tool running go test when it failed and now I can't reproduce at all with or without a call to SetLoggerV2. That's somewhat good but I can't explain how this file is no longer being imported by the build when I've not changed anything in the code, imports or build environment.\nIt seems like internal/channelz is imported directly from server.go in the main package so it seems hard to understand why this init function doesn't always run... https://github.com/grpc/grpc-go/blob/a338994886e0fb42e12d1b3675d5499207e74cc3/server.go#L46 or how it could be non-deterministic whether that is printed given that init functions all run in a fixed order in a single thread before real execution begins.\n\ud83e\udd14 . Ah... mystery solved.\nI opened that VSCode from command line a few days ago, at the time I had GRPC_GO_LOG_SEVERITY_LEVEL=INFO in the terminal that opened it's ENV, clearly it inherited that.\nSo I can reproduce this if I run tests with that in ENV. That's not ideal but workable at least.\nI still think there is a good argument for removing any logging from init functions - I've seen a few other places do that in platform-specific code in this lib. Logging only if those features are actually used seems like better behaviour in general.\nBut I can see it's not a big issue for most people who tend not to hard-code GRPC_GO_LOG_SEVERITY_LEVEL into their environments.. ",
    "siggy": "Thanks for the thorough review @menghanl ! I've made your suggested changes, much simpler.\nConfirmed RST_STREAM is now sent from the client:\n\ngrpc-rst-stream.pcapng.zip\n. Just calling out this is where the behavior change is. I want to confirm that it's safe to attempt a http2Client.Write() here, particularly because our call stack originates with a read from the server (where it receives an END_STREAM):\n- http2Client.Write()\n- http2Client.closeStream()\n- http2Client.operateHeaders()\n- http2Client.reader()\nFrom debug.Stack():\ngithub.com/buoyantio/strest-grpc/vendor/google.golang.org/grpc/internal/transport.(*http2Client).closeStream(0xc00009ed00, 0xc0001d6000, 0x1765980, 0xc000090050, 0x1e8000, 0xc0001ea010, 0xc0001e8120, 0x1)\n    /Users/sig/code/go/src/github.com/buoyantio/strest-grpc/vendor/google.golang.org/grpc/internal/transport/http2_client.go:669 +0x302\ngithub.com/buoyantio/strest-grpc/vendor/google.golang.org/grpc/internal/transport.(*http2Client).operateHeaders(0xc00009ed00, 0xc0001e8090)\n    /Users/sig/code/go/src/github.com/buoyantio/strest-grpc/vendor/google.golang.org/grpc/internal/transport/http2_client.go:1156 +0x2f4\ngithub.com/buoyantio/strest-grpc/vendor/google.golang.org/grpc/internal/transport.(*http2Client).reader(0xc00009ed00)\n    /Users/sig/code/go/src/github.com/buoyantio/strest-grpc/vendor/google.golang.org/grpc/internal/transport/http2_client.go:1213 +0x70f\ncreated by github.com/buoyantio/strest-grpc/vendor/google.golang.org/grpc/internal/transport.newHTTP2Client\n    /Users/sig/code/go/src/github.com/buoyantio/strest-grpc/vendor/google.golang.org/grpc/internal/transport/http2_client.go:263 +0xaf8. ",
    "suyashkumar": "It is my understanding that concurrent calls to SendMsg are indeed not thread safe and not allowed:\nhttps://github.com/grpc/grpc-go/blob/491af2b0a46021e419be712e762e78290a333ba9/stream.go#L885-L888\nThis serialization of writes is a separate issue for me, because I stream a large volume of messages in a short amount if time and expected that at least the encode and compress operations could be performed concurrently even if the subsequent sends need to be serial. After some more digging about how this might affect performance, I may open a separate issue for that. . Indeed, I think I would implement concurrent encoding and compression in my case, and then send in a serial way. I would still likely send messages in order, but figure that while blocked on I/O of sending a message over a network interface it may make sense to be compressing and encoding subsequent messages if CPU is not maxed out (which for me it is not). \nJust for reference I'll throw my use case out here--I am transiently streaming 30-60 messages a second (each with 262,144 integers) and I find that my bottleneck is on sending the messages. I understand that the write (to the socket/network) operation should be serialized and should be in some deterministic order. However, when blocked on that send I/O, would it not make sense to be compressing and encoding subsequent messages to be sent (that could sit in a queue)? In other words, the send operation should happen in order, but the compress and encode of subsequent messages could still be concurrent right? \nMy CPU is not maxed out during send operations right now. Right now in SendMsg it appears that the compress, encoding, and writing are all handled there and will block until all three are complete--my comment is that perhaps compress/encoding could be done concurrently for a set of queued up messages. . @snowzach will do, thanks!. ",
    "andyxning": "/cc @menghanl @dfawley @lyuxuan. @menghanl @dfawley @lyuxuan Seems the CI error is not relevant with this change.. @dfawley PTAL.. @lyuxuan @menghanl @dfawley . /cc @dfawley @lyuxuan @canguler . ",
    "sword2ya": "ok, thanks for your review.. ",
    "nikkolasg": "\nDoes this only happen when reconnecting in an existing ClientConn? If you restart the ClientConn, will the connection be successful?\n\nYes I use the same ClientConn, I never create a new one. I need to try with a new ClientConn in case I have an error, but it's very likely to solve the problem I guess !\nI'm keeping in mind to make a small test, but as I said, it won't be very soon.... ",
    "adanteng": "@dfawley \n\nCreating extra connections can be done by having a custom balancer that creates more SubConns when load is heavy.\n\nHow heavy, do you have got any benchmark. H2 still need multi connections to improve performance? \n\nThere are discussions going on about detecting this situation and automatically creating new connections, but this seems like a pretty significant feature so I wouldn't expect anything for some time.\n\nGot any achievement or plan?\n. At sender side, I test again to find the position where cost too much time,  found that between \"complete put dataFrame to controlBuffer\" and preprocessData alway cost too much time when lots of  dataFrame need to be put into controlBuffer and many other dataFrame need processData, almost 100+ will cause ms level time consuming.\ncontrolBuffer only got one goroutine loopyWriter to handle all works. This seems not optimal, frame of different stream will interference each other. \nWrite in processData will cost 20+ microseconds, so as I said above single goroutine consume dataFrame may be not enough. Let's assert that write on single tcp conn now is not the bottleneck, we divide the stream into many queue, each queue got a goroutine to consume, a better choice?\n@dfawley \n. I accept that multi goroutine is not the solution. But my main problem is that why grpc-go got ms level vibrate.\ncontrolBuffer use Mutex\uff0cdoes this will cause all req which got data toput and use the same connection blocked by the send out operation of loopyWriter. \nYou can use my example in single machine, when you adjust concurrency, some time you will got >5ms time consuming. \n@canguler . I print the time cost of all funcs of handle in controlbuf.go, found that registerStreamHandler's time cost is not fixed, alway >1ms, so I give estdStreams a fixed size like 64*1024 when newLoopyWriter, performance is better than before.. My mistake for \"alway >1ms\" in above comment.\nHere is my test code:\nhttps://gist.github.com/adanteng/bbd59a0fce1b7228765b2eb39c21700e\nAdd following code to  controlbuf.go, I can see some time cost >1ms.\ncase *registerStream:\n        start := time.Now()\n        defer func() {\n            if time.Now().Sub(start).Nanoseconds()/1000 > 1000 {\n                fmt.Println(time.Since(start))\n            }\n        }()\n        return l.registerStreamHandler(i)\nActually my point is that I want to found out why sometimes client encounter the cost when call SayHello bigger than 5ms. And the qps is low If you use https://gist.github.com/adanteng/82fbc9634bab0acb4764bd5689e9b988 to test.\nI do the following, to improve the qps.\n1. As I said above, give estdStreams a fixed size map, like 128*1024\n2. Add grpc.InitialWindowSize(math.MaxInt32) and grpc.InitialConnWindowSize(math.MaxInt32) when grpc.NewServer, which may disable the flowcontrol, what is the side effect?\nAfter I do the above two points, qps is >10w in my machine, look pretty good to me. . ",
    "a-urth": "@menghanl yes, you're 100% right. have no idea why it didn't work no test environment. thanks for your help anyway!. Ok, so it works with regular client creation, but using balancer (consul in my case) it doesn't. This is how looks client creation in my case (yes, we're using old balancer api)\ngolang\ngrpc.Dial(\n        consulURI,\n        grpc.WithDefaultCallOptions(grpc.MaxCallRecvMsgSize(ClientMaxReceiveMessageSize)),\n        grpc.WithInsecure(),\n        grpc.WithBlock(),\n        grpc.WithTimeout(60*time.Second),\n        grpc.RoundRobin(wonaming.NewResolver(serviceName)),\n). It seems like its not consul lb either. it should be gateway http proxy for grpc. ",
    "kaushiksriram100": "@menghanl : Thanks for this. I am not sure if this is solving my case. I tried setting the http_proxy as env variable. And tried the fixes you provided. Possible, if this issue related to just squid proxy unable to support http/2 ?\nexport http_proxy=http://user:pwd@web-proxy:3128\nI am hitting this error (same as old). \nWARNING: 2018/11/01 10:44:28 grpc: addrConn.createTransport failed to connect to {pubsub.googleapis.com:443 0  1}. Err :connection error: desc = \"transport: Error while dialing failed to do connect handshake, response: \\\"HTTP/1.1 407 Proxy Authentication Required\\\\r\\\\nConnection: keep-alive\\\\r\\\\nContent-Type: text/html;charset=utf-8\\\\r\\\\nDate: Thu, 01 Nov 2018 17:44:28 GMT\\\\r\\\\nMime-Version: 1.0\\\\r\\\\nProxy-Authenticate: Digest realm=\\\\\\\"outbound_proxy\\\\\\\", nonce=\\\\\\\"fDvbWwPkQz1gAAAAA\\\\\\\", qop=\\\\\\\"auth\\\\\\\", stale=false\\\\r\\\\nProxy-Authenticate: Basic realm=\\\\\\\"outbound_proxy\\\\\\\"\\\\r\\\\nServer: squid\\\\r\\\\nVia: 1.1 web-proxy (squid)\\\\r\\\\nX-Cache: MISS from web-proxy\\\\r\\\\nX-Cache-Lookup: NONE from web-proxy1004\\\\r\\\\nX-Squid-Error: ERR_CACHE_ACCESS_DENIED 0\\\\r\\\\nContent-Length: 0\\\\r\\\\n\\\\r\\\\n\\\"\"\n. I checked with out network engineer - Looks like it supports only DIGEST AUTH and Basic Auth is disabled. . Thank you!\n\nOn Nov 5, 2018, at 2:06 PM, Menghan Li notifications@github.com wrote:\nSupporting digest auth would require more time to do, and we currently don't have the cycles. Sorry about that. I filed #2440 to track it.\nIn the mean time, if you really need this, you can try to install a custom Dialer and do digest auth with it.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "yjp211": "\nThis function blocks when the sender does not have sufficient flow control to send the current message. That happens when the remote side (the server in this case) isn't reading from the stream. Is the server actively calling stream.RecvMsg?\n\nThanks!!\nThis is the client-server pseudo-code. Is there any problem?\nClient side:\n(create a GRPC connection, then open a new  stream and circulates send business data until the stream error,  then open  another a new  stream again)\n```go\n// rpc greeter\nvar greeter pb.RpcGreeterClient\n// stream chain\nvar streamChain = make(chan pb.RpcGreeter_ReplicaClient)\nfunc main(){\n    // create greeter\n    createGreeter()\n//work loop\nfor {\nWAIT_STREAM:\n    stream, ok :=<- streamChain\n\n    for {\n        //get data from  business queue \n        data :=<- bizQueue\n\n        if err :=stream.Send(data); err != nil{\n            //open a new stream when error\n            log.Error(\"send error: %+v\", err)\n            go openNewStream()\n            goto WAIT_STREAM\n        }\n    }\n}\n\n}\nfunc createGreeter() {\n    if conn, err := grpc.Dial(addr, opts...); err == nil{\n        greeter = pb.NewRpcGreeterClient(conn)\n    }else{\n        //exit when error\n        log.Fatal(err.Error())\n    }\n}\nfunc openNewStream(){\n    for {\n        if stream, err := greeter.Replica(context.Background()); err == nil {\n            streamChain <- stream\n            return\n        }else{\n            //retry after one Second\n            <- time.After(time.Second)\n        }\n    }\n}\n```\nServer side:\n```go\nfunc (s *Server) Replica(stream pb.RpcGreeter_ReplicaServer) error {\n    for {\n        ctx := stream.Context()\n    switch val, err := stream.Recv(); {\n    case err == nil:\n        //TODO something\n\n    case err == io.EOF:\n        time.Sleep(1 * time.Second) // provoke race condition\n        select {\n        case <-ctx.Done():\n            log.Warn(\"Context is already DONE!\")\n        default:\n            log.Info(\"Context is OK\")\n        }\n        return nil\n    default:\n        log.Error(\"receive error: %+v\", err)\n        return err\n    }\n}\n\n}\n```\nunexpected logs were found on the Server side\uff1a\nreceive error: rpc error: code = Canceled desc = context canceled\nBut at the corresponding time, the client side also reported an error, and successfully opened a new stream\nsend error: EOF failed\nnothing unusual has been observed around the blocked steam\n. I haven't repeated \"WriteQuota\" block recently.\nHowever, some other exceptions were found. GRPC quic conn seems to be easy to \"time-out\".\nThe detailed log is as follows:\uff08left is the sending client \uff09\n\n\"open new  quic conn\" print at:\n\nreconnect interval seems to be fixed around 80 seconds\u3002 \nI don't have a timeout setting associated,  default settings used\n. GRPC heartbeat doesn't seem to work as expected. Deploy client&server both on my MAC , 100% recurrence.\nDuring the \"80 second timeout\", both sides of the GRPC stream can receive and send packages normally.\nThis is a log printed on my MAC.\nClient disconnects earlier than server (every time), but so close. \n\n. After extensive debugging, the problem seems at quic-go, I will timely report. ",
    "mitar": "It would be great if you would version your install links. Now go get -u google.golang.org/grpc fails on go 1.8 in my Docker file while before it was working. Maybe consider providing things like go get -u google.golang.org/grpc-v1.16 to be able to pin to a specific version.. ",
    "Generalomosco": "Solved by using envoy as reflection to Golang grpc server . ",
    "ciscoxll": "I have solved it because of the problem caused by the null pointer. thank you @lyuxuan.. ",
    "mattthym": "Could you please elaborate on what you had to do to fix this issue. I've got the same rpc error and do not know what causes it.. Thanks for your response!\nI'm currently rewriting a client I wrote in Python in Go. When trying to connect to the server without the right credentials with python, it returned the rpc error \"origin authentication failed\", so I guess I just expected a similar error to be returned with Go and now I am a little puzzled about why the rpc error in Go is the same if the server is down and if the connection failed on handshaking.... Thanks for your response.\nThe server did expect authentication, but I knowingly tried to connect using WithInsecure() and expected the error to be similar to the one in python where you get a different error, depending on if the server is down or if it expects authentication. \nI managed to achieve what I wanted, so thanks again.. ",
    "jesushernandez": "Hey, I just wanted to briefly detail how we solved this issue recently, just in case someone else has a similar setup and is facing these 'intermittent' connection drops\nOur environment (prior to solving the issue):\n - grpc v1.18 (in both client and server)\n - golang v1.10.8 (in both client and server)\n - connection from a k8s cluster in GCP to an instance in AWS. No meshes in between, but there's a network load balancer routing traffic to our AWS instance. This turned out to be the key part.\n - unary, non-TLS RPCs\n - no keepalive settings configured in neither client nor server\n - server and client verbose logging:\nGRPC_TRACE=all\nGRPC_VERBOSITY=DEBUG\nGRPC_GO_LOG_VERBOSITY_LEVEL=2\nGRPC_GO_LOG_SEVERITY_LEVEL=info\nWhat we were experiencing were random transport is closing errors on the client when making unary RPCs to our server (usually after ~5 minutes of inactivity). Most of the time, our server would not even get the request from the client, so we thought some other component in between should be causing the connections to be closed.\nAfter some digging, we eventually found this section in the AWS network load balancer docs:\n\nFor each request that a client makes through a Network Load Balancer, the state of that connection is tracked. The connection is terminated by the target. If no data is sent through the connection by either the client or target for longer than the idle timeout, the connection is closed. If a client sends data after the idle timeout period elapses, it receives a TCP RST packet to indicate that the connection is no longer valid.\nElastic Load Balancing sets the idle timeout value to 350 seconds. You cannot modify this value. Your targets can use TCP keepalive packets to reset the idle timeout.\n\nImmediately after seeing this, we decided to start using keepalive pings from the client. You can easily enable this on the client as a dial option (keepalive.ClientParameters) and on the server (grpc.KeepaliveEnforcementPolicy) as a server option which forces clients to comply with the keepalive policy.\nSince we just use unary RPCs, we had to set PermitWithoutStream=true, so that the client sends keepalive pings while not streaming. We also made sure that the enforcement policy has a KeepaliveEnforcementPolicy.MinTime=1 * time.Minute (it has to be lower than the LB's connection idle timeout of 350s) and the client would submit keepalive pings every 2 minutes (ClientParameters.Time=2 * time.Minute). As long as Time > MinTime, we're good!\nWe applied the changes to our clients and server (as others have mentioned, the clients must comply with the server policy, otherwise you'll have more connectivity issues) and the transport is closing issue is now totally gone, as the client will keep the connections alive.. ",
    "WQG6848": "When the client reads data error from Stream, why does estdStreams in the server's loopyWriter not delete this stream?\nI tracked this behavior and did find a Stream leak.. First thanks for your help @lyuxuan .\nI can set the size of the data received by the client through the MaxCallRecvMsgSize(s) method. The problem is why the stream leaks when the size of the data returned by the server is larger than s MB.\nDemo\nhello.proto\n```pb\nsyntax=\"proto3\";\npackage hello;\nservice Greeter {\n  // Sends a greeting\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\n}\n// The request message containing the user's name.\nmessage HelloRequest {\n  string name = 1;\n}\n// The response message containing the greetings\nmessage HelloReply {\n  string message = 1;\n}\n```\nclient.go\n```go\nfunc main() {\n    conn, err := grpc.Dial(\":8888\", grpc.WithInsecure())\n    if err != nil {\n        panic(err)\n    }\n    defer conn.Close()\nc := pb.NewGreeterClient(conn)\n\nfor i := 0; i < 200; i++ {\n    _, err := c.SayHello(context.Background(), &pb.HelloRequest{})\n    if err != nil {\n        fmt.Println(err)\n    }\n    time.Sleep(time.Second)\n}\n\ntime.Sleep(time.Hour)\n\n}\n```\nserver.go\n```go\nvar message = strings.Repeat(\"hello\", 1024*1024)\ntype server struct{}\nfunc (s server) SayHello(ctx context.Context, in pb.HelloRequest) (*pb.HelloReply, error) {\n    return &pb.HelloReply{Message: message}, nil\n}\nfunc main() {\n    fmt.Println(\"PID:\", os.Getpid())\nlis, err := net.Listen(\"tcp\", \":8888\")\nif err != nil {\n    panic(err)\n}\n\ngo func() {\n    // pprof\n    if err := http.ListenAndServe(\":8080\", nil); err != nil {\n        panic(err)\n    }\n}()\n\ns := grpc.NewServer()\npb.RegisterGreeterServer(s, &server{})\n\ns.Serve(lis)\n\n}\n```\nAfter analyzing the code, I suspect that the stream is leaking, so I added some logs to the GPRC source code.\ngoogle.golang.org/grpc/internal/transport/controlbuf.go :515\ngo\nfunc (l *loopyWriter) registerStreamHandler(h *registerStream) error {\n    str := &outStream{\n        id:    h.streamID,\n        state: empty,\n        itl:   &itemList{},\n        wq:    h.wq,\n    }\n    l.estdStreams[h.streamID] = str\n    fmt.Println(\"Register StreamID: \", h.streamID, \"Now Streams Length:\", len(l.estdStreams)) // log\n    return nil\n}\nFirst\nserver.go\ngo\nvar message = strings.Repeat(\"hello\", 1024*1024)\nIn this test, the client will report errors like this : \nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max (5242885 vs. 4194304)\n\nThe server will print logs like this:\nPID: 30168\nRegister StreamID:  1 Now Streams Length: 1\nRegister StreamID:  3 Now Streams Length: 2\nRegister StreamID:  5 Now Streams Length: 3\nRegister StreamID:  7 Now Streams Length: 4\nRegister StreamID:  9 Now Streams Length: 5\nRegister StreamID:  11 Now Streams Length: 6\nRegister StreamID:  13 Now Streams Length: 7\nRegister StreamID:  15 Now Streams Length: 8\nRegister StreamID:  17 Now Streams Length: 9\nRegister StreamID:  19 Now Streams Length: 10\n\n..............\n\nRegister StreamID:  385 Now Streams Length: 193\nRegister StreamID:  387 Now Streams Length: 194\nRegister StreamID:  389 Now Streams Length: 195\nRegister StreamID:  391 Now Streams Length: 196\nRegister StreamID:  393 Now Streams Length: 197\nRegister StreamID:  395 Now Streams Length: 198\nRegister StreamID:  397 Now Streams Length: 199\nRegister StreamID:  399 Now Streams Length: 200\n\n\nSo, it seems that the stream leaked\n\nNext, take a look at the memory.\n> top -p 30168\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n30168 wqg+    20   0 1521120 1.027g   7808 S   0.0 13.2   0:03.25 server\n\n\nMemroy leak did happen\n\npprof:\n\nSecond\nserver.go\ngo\nvar message = strings.Repeat(\"h\", 1024*1024)\nIn this test, The server will print logs like this:\nPID: 1662\nRegister StreamID:  1 Now Streams Length: 1\nRegister StreamID:  3 Now Streams Length: 1\nRegister StreamID:  5 Now Streams Length: 1\nRegister StreamID:  7 Now Streams Length: 1\nRegister StreamID:  9 Now Streams Length: 1\nRegister StreamID:  11 Now Streams Length: 1\nRegister StreamID:  13 Now Streams Length: 1\nRegister StreamID:  15 Now Streams Length: 1\n\n.......................\n\nRegister StreamID:  381 Now Streams Length: 1\nRegister StreamID:  383 Now Streams Length: 1\nRegister StreamID:  385 Now Streams Length: 1\nRegister StreamID:  387 Now Streams Length: 1\nRegister StreamID:  389 Now Streams Length: 1\nRegister StreamID:  391 Now Streams Length: 1\nRegister StreamID:  393 Now Streams Length: 1\nRegister StreamID:  395 Now Streams Length: 1\nRegister StreamID:  397 Now Streams Length: 1\nRegister StreamID:  399 Now Streams Length: 1\n\n\nIt looks like everything is ok\n\nNext, take a look at the memory.\n> top -p 1662\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n1662 wqg+     20   0  665296  14496   7932 S   0.0  0.2   0:01.96 server\n\n\nEverything is ok\n. I found something.\n\n```go\nfunc (s Server) processUnaryRPC(t transport.ServerTransport, stream transport.Stream, srv service, md MethodDesc, trInfo *traceInfo) (err error) {\n    ......\n    // Step 1\n    if err := s.sendResponse(t, stream, reply, cp, opts, comp); err != nil {\n        ......\n    }\n// Step 2\nerr = t.WriteStatus(stream, status.New(codes.OK, \"\"))\n....\n\nreturn err\n\n}\n```\nAfter step 1, outStream.itl.head.it is *dataFrame. At this time, the server may already be returning data to the client.\nAfter step 2,  outStream.itl.head.it is *dataFrame.  outStream.itl.head.next.it is *headerFrame.\nThe important thing is that the value of Stream.state changes from streamReadDone to streamDone. Then the program ignores the handleRSTStream frame sent by the client Stream.\nIn general, after all the data in the dataFrame is sent, the program will execute cleanupStreamHandler(headerFrame.cleanup) to delete the stream.\nUnfortunately, if the client Stream reports an error after receiving the first data frame, for example:\nrpc error: code = ResourceExhausted desc = grpc: received message larger than max.....\nThe client Stream sends a handleRSTStream frame to the server, after which the client Stream is closed.\nHowever, because the value of Stream.states is streamDone, the server will ignore the handleRSTStream frame.\nBut the server's loopyWriter will still write data to the exception Stream until sendQuota is exhausted.. ",
    "canguler": "fixed by #2610 . I was able to reproduce the data race with the version that you're using but not with the latest version. This commit from 6 months ago seems to fix the problem. Closing this now. Please reopen, if you witness the same data race with a version having the above mentioned commit.\nOn a side note, while investigating this issue, we have realized that RPC's status and trailer are not always consistent. (e.g. \"context cancelled\" status and a non-nil trailer) As they're set at the same time on the client side, one should see a nil trailer, if the RPC's returned status is \"context cancelled\". #2554 solves this problem.. fixes #2490 . Opened a new PR #2554.\nResolved ones are marked.. Hi @krhubert,\nTo better understand the problem, I have a couple of questions:\n\nWhat do you mean by a normal close? Can you give an example?\nYou mentioned \"... reconnect on any error ...\". What function's errors are we talking about? Error from the function establishing the stream, or error from Recv function?. It wasn't always the case. When loopyWriter wasn't the only function to handle writes, all functions that  write needed to acquire a single lock. That was creating contention for the lock and decreasing the performance. PR #1962 moved all writes to a single go routine, loopyWriter, to solve this problem.\n\nLet me know if this doesn't answer your question.. I will try to reproduce this. Can you share your measurements as well?. Hey,\nI've failed to reproduce that.\nMy findings for how long registerStreamHandler takes for each configuration.\nfor n = 1; c = 1000000\nAvg: 702.36 ns\nfor n = 10; c = 100000\nAvg: 632.26 ns\nfor n = 100; c = 10000\nAvg: 559.13 ns\nfor n = 1000; c = 1000\nAvg: 456.69 ns\nNot just the averages are very low but also in all cases, 100% of registerStreamHandler's invocations take less than 1ms.. @adanteng \nCan you share more details on the specifics of your observation? What code did you run?\nThanks!\nCan. Hi @adanteng,\nI tried to reproduce your initial results (without the suggested optimizations) with the latest grpc (master branch).\nClients run time percentiles - all in nanoseconds:\n75th: 400617\n95th: 665268\n99th: 882342\nregisterStreamHandler run time percentiles - all in nanoseconds:\n75th: 394\n95th: 1009\n99th: 2110\nI used your code with slight simplification to only capture the run times that we are interested in.\nThe code is here: https://gist.github.com/canguler/fc34c005a7faed2dcd6618827588ea54\nI ran client with -n=100000 -c=20\nI am not sure why you are getting so long run times. I realized that in your initial version, you are printing your run times if they are higher than a threshold. Whereas in your improved version, you are recording the run times and do the printing at the end. I believe second one gives more accurate results. So, I used this approach in my tests.\nThe results I have doesn't match with yours. Let me know if you can get my results.. Hey @ZhiqinYang,\nCould you please provide more context for the problem that this PR is attempting to solve?. @dfawley realized this. We were always returning SERVING for \"\". This shouldn't be the case.. Added the comment.\n. Even for retryCnt == 0, Backoff function returns 1 second. So it is treated as the number of re-tries, not the number of tries. But removing this extra boolean is appealing to me. \nSo, I will rename retryCnt as tryCnt and pass tryCnt-1 to Backoff function. I've removed the type. But I have another question: What should our maximum delay be?. Backoff function employs some randomness:\nhttps://github.com/grpc/grpc-go/blob/master/internal/backoff/backoff.go#L73. Seems like we're using 120 seconds in other places. I am going to use it as well.\n. I think this one is logged here: https://github.com/grpc/grpc-go/pull/2387/files#diff-e1550a73f5d25064c8b586ec68d81a64R1291\ncc @lyuxuan . Should I print only if it's an error? Or, should I print in both cases (whether it is an error or not)?. Why is that?\n. err is expected to be Cancelled. So logging non-EOF errors clutters the consoles.. They are not identical. If the context times out before the stream is established, then both of them (cntCanceled and cntPermDenied) can be zero. But I think I can still simplify it a bit.. Instead of duplicating this command here, can you provide a pointer to the other file?. suggestion\nclient, the user would need to launch the grpc server.. suggestion. suggestion\nAfter starting the server, the client can be run.  An example of how to run this. suggestion\nAn example for how to run this command on a different port is:. suggestion\nAfter starting the server, the client can be run separately and used to test. suggestion\n// Package stats registers stats used for creating benchmarks. Package level comments are generally put into a single file. So, this one isn't necessary.. As this is a package level comment, it should start with \"Package main ...\"\nsuggestion\n// Package main implements a server for Greeter service.. As this is a package level comment, it should start with \"Package main ...\"\nsuggestion\n// Package main implements a client for Greeter service.. You have double space after the sentences for the rest of the PR.\nsuggestion\nwhich is listening on port 50051.  An example to start the server would be to. @zelahi can you address this one as well?. :+1: . This is the mentioned file. Please provide the example on how to run the server here.. Why only for ErrConnClosing and not for every possible ConnectionError?\nsuggestion\n            if _, ok := err.(ConnectionError); ok {. ",
    "ekrengel": "@breznik Did you ever figure this out? We're seeing a similar issue.. ",
    "longbozhan": "@menghanl \nThanks for you reply. I set verbose export GRPC_GO_LOG_VERBOSITY_LEVEL=16, 16 is log.Lshortfile in glog, means will display file name, but I didn't see any file name. . I solved by import github.com/grpc/grpc-go/tree/master/grpclog/glogger, thank you very much. @menghanl . ",
    "ch3rub1m": "@dfawley There are three generated packages in go-genproto for error handling. They are status, codes and errdetails.\nHowever, there are just two packages status and codes in grpc to help constructing Status and Code proto messages except errdetails.\nI think if status and codes package are inside the scope of grpc, the errdetails package should be treated equally.. @dfawley Ok. Thanks for your reply. I will write a third party library to handle this situation.. ",
    "dalu": "I'm too tired to even begin understanding that and it sound like it's quite low-level too.\nI believe that issue should be made at the golang.org/x/oauth2 issue tracker. ",
    "vishal-uttamchandani": "Essentially, I would like the server to be able to close the connection, if based on some business logic it decides that a particular client does not meet all the criteria to proceed.. @dfawley thanks for the info. My scenario is one where we don't trust the client to do the right thing and close the connection in time. This means even when the server work is done it will have to hang on to the connection. At scale, this means waste of resources. Any suggestion regarding this ?. ",
    "Bo0km4n": "Thanks reply! I'm using standard json library. \nThen, I removed tag which omitempty, but never changed result.\nI'll try use jsonpb just now.. @menghanl I'm tried jsonpb. That's behavior is I'm expected!!! Thanks \ud83d\udc4d !. ",
    "wangjingpei": "i am wandering that what we should do when the goruntine num is too much, such as more than 30000, it seemed there is no limit to the gorunting num then the mem is useless and closed by the system. we just grpc to build a web servcie, and store client request data to sql, while the client request adding and the sql execing blocked, then goruntine seed more and more , then closed by the system. ",
    "NeoyeElf": "I have a problem here...\nI used grpc-client(v1.18.0) to send request grpc-server(v1.16.0), it turned out that request hang up (or exceed deadline when set timeout). After I have checked the change log, I found this feat: client: add GRPC_GO_REQUIRE_HANDSHAKE options to control connection behavior . With my doubt, I set the env variables \"GRPC_GO_REQUIRE_HANDSHAKE=off\" then it worked.\nhere is my questions: \nIs this the only way to make it work without downgrade google.golang.org/grpc to 1.16 for my grpc-client?\nThanks.... ",
    "mauriciogg": "Thanks for the quick reply. My connections weren't going through the retry/backoff logic because I'm connecting through a proxy that forwards the connections to the actual server. That is the connection always succeeds if it can connect to the proxy, but if the server is not up the http2 layer of the connection establishment was timing out in cases where the server was taking a while to come up. The simplest workaround was to only start listening for client connections in the proxy until we have a connection to the server. That way everything follows the right logic. Closing the issue.. ",
    "hxzhao527": "my certificate is sign for 10.1.230.47(my own local ip), grpc-server run with 10.1.230.47:50051.\nCalling server directly\nconn, err := grpc.Dial(\"10.1.230.47:50051\", opts...)\n...\nworks well.\nwhen integrated with consul which run on 127.0.0.1:5000, rpc-call failed.\ngolang\nconn, err := grpc.Dial(\"consul://pass/127.0.0.1:5000\", opts...)\n...\nthere is my code: grpcdemo.. I have tried conn, err := grpc.Dial(\"consul://127.0.0.1:5000/10.1.230.47:50051\", opts...). It did work.\nBut I still have some questions.\n1. My certificate is signed for ip(or many ip), no DNS required. In this case, what the Authority shoud be?\n2. 10.1.230.47:50051 is my server address. The  purpose of consul is to hide the address of server. Client only need to know which service it will call and where it can get the server address from with the service name. So I can't pass the server-address, even server-name,  to grpc.Dial. The address of consul is the only one parameter I can provide, and I wish I can.\n. ",
    "withlin": "But it's implemented.. >(One thing I noticed: the client is called ApplicationRegisterServiceClient, but the error message is about a ServiceNameDiscoveryService. Those seem to be two very different services.)\nSorry, that's my copy error. I've updated the issue.\nI am used  C # client request server, it works perfectly\nc#\nvar channel = new Channel(\"localhost:11800\", ChannelCredentials.Insecure);\nchannel.ConnectAsync(DateTime.UtcNow.AddMilliseconds(3)).ConfigureAwait(false);\nvar client = new ApplicationRegisterService.ApplicationRegisterServiceClient(channel);\nvar applicationMapping = client.applicationCodeRegister(new Application { ApplicationCode = \"test\" });\nConsole.WriteLine($\"Register Success{applicationMapping?.Application?.Value}\");\nConsole.Read();\nThe server is written in java. I debug the server and that c# is connected to it successfully, but golang can't connect to the server.\nThe client is not  connecting to the right server. So what should I do to connect the server correctly?\n. Because the client is not connecting to the right server, so I try to write a server in golang. The client of golang connects to the server of goalng. It is successful.\n```go\ntype test struct {\n}\nfunc (c test)ApplicationCodeRegister(ctx context.Context, in pb.Application)(*pb.ApplicationMapping,error){\n    a :=new(pb.ApplicationMapping)\n    b :=new(pb.KeyWithIntegerValue)\n    b.Value=12345\na.Application=b\nreturn  a,nil\n\n}\nfunc main()  {\ngo func() {\n\n    listen, err := net.Listen(\"tcp\", \"localhost:11111\")\n    if err != nil {\n        grpclog.Fatalf(\"failed to listen: %v\", err)\n    }\n\n    s := grpc.NewServer()\n\n\n    pb.RegisterApplicationRegisterServiceServer(s, &test{})\n\n    grpclog.Println(\"Listen on \" + \"localhost:11111\")\n\n    s.Serve(listen)\n}()\n\nconn, err := grpc.Dial(\"localhost:11111\", grpc.WithInsecure())\nif err != nil {\n    log.Fatalln(\"Can't connect: \" + \"localhost:11111\")\n}\ndefer conn.Close()\nfor ; ;  {\n    resp,err:=pb.NewApplicationRegisterServiceClient(conn).ApplicationCodeRegister(context.Background(),&pb.Application{ApplicationCode:\"aaaaa\"})\n    if err != nil {\n        log.Fatalln( err)\n    }\n    log.Println(resp)\n\n}\n\n}\n```\nThe c# client is not connecting to the right golang server.\n. My log output is as follows:\nINFO: 2018/11/30 09:48:42 Channelz: socket options are not supported on non-linux os and appengine.\nINFO: 2018/11/30 09:48:42 parsed scheme: \"\"\nINFO: 2018/11/30 09:48:42 scheme \"\" not registered, fallback to default scheme\nINFO: 2018/11/30 09:48:42 ccResolverWrapper: sending new addresses to cc: [{localhost:11800 0  }]\nINFO: 2018/11/30 09:48:42 ClientConn switching balancer to \"pick_first\"\nINFO: 2018/11/30 09:48:42 pickfirstBalancer: HandleSubConnStateChange: 0xc0000f6060, CONNECTING\nINFO: 2018/11/30 09:48:42 blockingPicker: the picked transport is not ready, loop back to repick\nINFO: 2018/11/30 09:48:43 pickfirstBalancer: HandleSubConnStateChange: 0xc0000f6060, READY\n2018/11/30 09:48:43 rpc error: code = Unimplemented desc = Method not found: skywalking_proto.ApplicationRegisterService/applicationCodeRegister\nexit status 1\nI don't really understand this log. It looks like I can connect to the server successfully.\n\nAre the generated codes in different languages in sync?\n\nThese are all generated using protoc tools. They are the same proto file.\n\nAnother suggestion is to look in the generated functions that register services (for go, it would be the RegisterApplicationRegisterServiceServer function. There's should be similar ones in Java and c#).\n\ngo implement:\ngo\nfunc (c *applicationRegisterServiceClient) ApplicationCodeRegister(ctx context.Context, in *Application, opts ...grpc.CallOption) (*ApplicationMapping, error) {\n    out := new(ApplicationMapping)\n    err := c.cc.Invoke(ctx, \"/skywalking_proto.ApplicationRegisterService/applicationCodeRegister\", in, out, opts...)\n    if err != nil {\n        return nil, err\n    }\n    return out, nil\n}\nc# implement:\nc#\n public virtual global::SkyWalking.NetworkProtocol.ApplicationMapping applicationCodeRegister(global::SkyWalking.NetworkProtocol.Application request, grpc::Metadata headers = null, global::System.DateTime? deadline = null, global::System.Threading.CancellationToken cancellationToken = default(global::System.Threading.CancellationToken))\n      {\n        return applicationCodeRegister(request, new grpc::CallOptions(headers, deadline, cancellationToken));\n      }\njava implement:\n```java\npublic void applicationCodeRegister(org.apache.skywalking.apm.network.proto.Application request,\n    io.grpc.stub.StreamObserver<org.apache.skywalking.apm.network.proto.ApplicationMapping> responseObserver) {\n  asyncUnimplementedUnaryCall(getApplicationCodeRegisterMethod(), responseObserver);\n}\n\n```\n\n(Another totally random guess:\nin proto.ApplicationRegisterService/applicationCodeRegister, the first letter of the method name is not capitalized. IMO this should work find and shouldn't cause any problems. But if you are still stuck, it's worth trying to change it to ApplicationCodeRegister.)\n\nI tried. It reported the same mistake.\n. According to the official demo, goalng can connect c# server. It makes me very confused.\nI'll check my code again. Thank you for your reply.. ",
    "dreambo8563": "I signed it. ",
    "egonelbre": "Sorry, made the issue against the wrong respository. This probably should be in golang/protobuf instead.. Nevermind, it seems this was the right repo nevertheless.. Ah ok, just to mirror back how I understood it, if Send fails (any error) the connection is guaranteed not to read more data from the socket (i.e. only what's in the buffers) for that particular Server. And this property also holds for a misbehaving Server. Or in other words my main concern is either server/client being able to keep connection open indefinitely when I write such loop.\nIf that is the case, yeah, then there isn't a problem with Recv in a loop until completed. Thanks.. @lyuxuan I'm implementing P2P and I do not know what the server is actually doing and it could be someone malicious. So I have no control over whether the server handler returns or not. I only know about the agreed upon protocol, if that is violated then the connection should be torn down with as much information as possible.\nThis connection is transient, so it's not being held up for a long time... once stream finishes, it will also close the connection.. Yeah, that could work. Close the underlying connection, then recv until the buffer is completely, parsed, although not sure whether the ClientConn supports it, have to test it. However I'm failing to find a way to shut it down on the server side, i.e. after detecting an unwanted client, feels like I'm missing something obvious. I guess I can capture the stream into a context via a context somehow, I'll take a deeper look tomorrow. Thanks.. ",
    "Lsquared13": "Well one of my dependencies takes issue with me removing it, but the update seems to make it work. Glad to hear the change to the generated code is coming down the pipeline. Thanks. ",
    "wellorange": "@dfawley \nThis is protobuffer:\n```proto\nservice System {\n  rpc RequestWord(Operation) returns (Status) {}\n  rpc Request(Query) returns (DmResponse) {}\n}\nenum Dataflag {\n    Dic= 0;\n    Wor= 1;\n  }\nenum Actionflag {\n  Add= 0;\n  Update = 1;\n  Delete=2;\n}\nmessage Operation {\nDataflag Dataflag=2;\n  Actionflag  Actionflag=3;\nWord  Word=1;\n  Dictionary Dictionary=4;\n}\n//  Dictionary is used fronetnd update lexicon\nmessage Dictionary{\n  string id=1;\n  string dic=2;\n  string value=3;\n}\n```\nclient:\ngo\n     var op pb.Operation\n     op.Actionflag=pb.Actionflag_Add\n     op.Dataflag=pb.Dataflag_Dic\n     op.Dictionary=&pb.Dictionary{Id:\"id\",Value:\"value\",Dic:\"\u6d4b\u8bd5\"}\n     r, err := c.RequestWord(ctx, &op)\nserver:\ngo\nfunc DbWatch(data *pb.Operation) (ok bool)  {\n    fmt.Println(data)\n    fmt.Println(*data)\n    g, _ := json.Marshal(data)\n       fmt.Println(string(g))\n}\nconsole log:\nDictionary:<id:\"id\" dic:\"\\346\\265\\213\\350\\257\\225\" value:\"val\n{Dic Add <nil> id:\"id\" dic:\"\\346\\265\\213\\350\\257\\225\" value:\"\n{\"Dictionary\":{\"id\":\"id\",\"dic\":\"\u6d4b\u8bd5\",\"value\":\"value\"}}\nWhen I am json encoded data(g) is stored to mysql\nAfter finding that fields Actionflag and Dataflag are gone. I have solved this problem.I skipped the 0 index. ",
    "jinq0123": "Need a write lock.. ",
    "thaJeztah": "I signed it. updated; PTAL. You're welcome; just scratching an itch \ud83d\ude02 \ud83d\udc4d . Ah, yes, that's better \ud83d\udc4d . ",
    "orange-jacky": "macbookpro:~ fredlee$ go get -u google.golang.org/grpc\ngo: finding github.com/golang/glog latest\ngo: finding cloud.google.com/go v0.26.0\ngo: finding google.golang.org/genproto latest\ngo: finding golang.org/x/lint latest\ngo: finding golang.org/x/oauth2 latest\ngo: finding golang.org/x/sys latest\ngo: finding golang.org/x/net latest\ngo: finding golang.org/x/tools latest\ngo: finding golang.org/x/sync latest\ngo: finding github.com/golang/lint latest\ngo: finding honnef.co/go/tools latest\ngo: cloud.google.com/go@v0.26.0: unknown revision v0.26.0\ngo get: error loading module requirements. ",
    "tomwei7": "CLA. ",
    "shettyh": "@menghanl Thanks for response.\nI tried checking the MD and sending the error if the key not found\n```\n    md, _ := metadata.FromIncomingContext(stream.Context())\nvals := md.Get(\"test\")\nfmt.Printf(\"Vals %v\",vals)\n\nif len(vals) == 0 {\n    fmt.Printf(\"Invalid request\")\n    return status.Errorf(codes.Internal, \"Test error\")\n}\n\n```\nBut client is not getting the error, client gets the stream and stream.Send() works fine. Seems like a weird issue not sure if i am doing something wrong here. Send is not failing with error. First of all i should not get stream right? if the headers are wrong as the server is sending the error ?\n```\n    conn, err := grpc.Dial(\"localhost:8090\", grpc.WithInsecure())\n    if err != nil {\n        panic(err)\n    }\nclient := NewStreamServiceClient(conn)\n_ , err  = client.StreamData(context.Background())\n\n    if err!= nil {\n         panic(\"Failed stream connection\")  // This should happen right, as i am not sending any headers.\n        // But it is not failing here, not failing when calling send also\n     }\n\n```. @menghanl \nTried using RecvMsg. Now i am getting the error when i am calling the RecvMsg, it works fine.\nBasically \n_ , err  = client.StreamData(context.Background())\nthis will only return err if there is any network errors right ? All the RPC implementation errors will be communicated with the stream.  Is that correct understanding ?\n. Okay Thanks. ",
    "npuichigo": "Thanks for reminding, but my grpc server is based on c++. Is there any counterpart of EnforcementPolicy in grpc c++?\n. I tried the following configurations on c++ server and golang client to make sure that the permitted ping interval of c++(5 minutes by default) is smaller than the golang client, but it still doesn't work.\ngrpc::ServerBuilder builder;\n// Listen on the given address without any authentication mechanism.\nbuilder.AddListeningPort(FLAGS_address, grpc::InsecureServerCredentials());\n// Enable to send HTTP2 keepalive pings over the transport.\nbuilder.AddChannelArgument(GRPC_ARG_KEEPALIVE_PERMIT_WITHOUT_CALLS, 1);\nbuilder.AddChannelArgument(GRPC_ARG_HTTP2_MAX_PINGS_WITHOUT_DATA, 0);\n```\nctx := context.Background()\n        ctx, cancel := context.WithCancel(ctx)\n        defer cancel()\n    mux := runtime.NewServeMux()\n    opts := []grpc.DialOption{\n            grpc.WithInsecure(),\n            grpc.WithKeepaliveParams(keepalive.ClientParameters{\n                    Time:                10 * time.Minute,\n                    Timeout:             20 * time.Second,\n                    PermitWithoutStream: true,\n            }),\n    }\n    err := gw.RegisterMyHandlerFromEndpoint(ctx, mux, *echoEndpoint, opts)\n\n```. @mastersingh24 - Yes, the problem has been solved. Thank you for your concern. . ",
    "krhubert": "Hi,\nI can't reproduce it easily right now, so to close for now. ",
    "NicolasMahe": "The problem we had was because of the timeout default configuration of docker network.\nBy default, Docker drops idle connection after 15 mins.\ngRPC server is configured by default to ping on an idle connection after 2h. This config is fine because most systems drop idle connection only after 2h.\nhttps://github.com/grpc/grpc-go/blob/6086f8d68f83701fc00a9549c30c2b48861c8a6f/keepalive/keepalive.go#L65-L68\nSetting a shorter time than 15min in gRPC server fix the problem.\nI also recommend to set the same config in gRPC clients (by default the client config is set to infinity). The reason is, if the client never try to publish a message on the connection, it will never knows that the connection drops (<-context.Done()), and will not get any error (context.Err()). ",
    "SavostinVladimir": "I have seen grpc logs, my certificate was expired, now it's ok! thank you!. ",
    "spl0i7": "I used certstrap to generate certificates, I do not think I had issuer CA cert.. I think it was similar to following - \n$ certstrap init --common-name \"ca.domain\"\n$ certstrap request-cert --common-name \"app.domain\"\n$ certstrap sign app.domain --CA ca.domain. ",
    "rvdwijngaard": "@lyuxuan thanks for our explanation. I didn't know this but it explains the difference. \nI assume my configuration is not correct. I have a bunch of services (and they are all running on different ports) which are registered as SRV records in dns. I added the _grpclb._tcp records because they are the only ones which are read by grpc dns resolve. \nIf I understand you correctly an import of grpclb package is sufficient?  So my client code will be \n```\n        import _ \"google.golang.org/grpc/balancer/grpclb\"\n    address := fmt.Sprintf(\"dns:///my-service.%s.private\", environmentName)\nconn, err := grpc.Dial(address, grpc.WithInsecure())\nif err != nil {\n    return nil, err\n}\nclient := pb.NewApiKeyServiceClient(conn)\n\n```\nis that how I should configure it? \nEDIT: I assume that the grpclb SRV records are meant for remote load balancers. I guess that's why you were asking if I want to interact with the grpclb server. . @lyuxuan thanks for your reply. I followed your suggestions and it works as expected. . ",
    "sohilgogri": "I ran the exact same comments. I cd'ed in the examples/helloworld directory and ran the above commands. I also made sure the other variables like GRPC_TRACE and GRPC_VERBOSITY are not set and they were not. I still get the same log on the client as 2019/01/16 14:44:41 Greeting: Hello world and on the server as 2019/01/16 14:44:41 Received: world. Can it be some issues with my version of the dependencies ?. go get -u google.golang.org/grpc worked for me. Maybe it was just a dependency issue. . ",
    "90wukai": "verify. verify. ",
    "g7r": "\nThis is implemented according to the grpc spec here:\n\nSome data transmitted (e.g., request metadata written to TCP connection) before connection breaks | UNAVAILABLE | Client\n\nIt's unlikely this can be changed.\n\nI can't agree with you. Spec says that \"Some data transmitted\" which clearly is not the same as \"All data transmitted\". I don't see that anything should be changed in spec to fix described behaviour.\n\nAs a general suggestion related to this, you may want to look into graceful shutdown for your servers:\nhttps://godoc.org/google.golang.org/grpc#Server.GracefulStop\nIt's common to call this, then -- in another goroutine -- wait for a timeout before calling Stop to allow existing RPCs extra time to finish before forcefully closing the server.\n\nThis is exactly what I do. But it leaves possibility for clients to receive Unavailable status.\nI think that returning Unavailable for fully transmitted requests breaks guarantees for retryability mentioned in status codes docs.. > \"All data transmitted\" is a special case of \"Some data transmitted\".\nWhile this statement is logically true, the cases are very different when we are trying to see them from retrying possibility point of view. For example, docs on transparent retries clearly differentiate the former from the latter: https://github.com/grpc/proposal/blob/master/A6-client-retries.md#transparent-retries\n\nThis would be a change to the grpc spec, and any proposals to change it should be filed against the grpc/grpc repo or brought up on the grpc-io mailing list.\n\nI don't think that fixing the problem will be against the grpc spec. Furthermore, the current state of client status codes is incompatible with the spec.\n\nWhat docs are you referring to?\n\nhttps://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto#L118 and https://github.com/googleapis/googleapis/blob/master/google/rpc/code.proto#L172\n\n\nAs shown in the table above, the gRPC library can generate the same status code for different cases. > > ... Therefore, there is no fixed list of status codes on which it is appropriate to retry in all applications.\n\nI agree this is unfortunate, but I'm not sure it will be feasible to change it.\n\nThe citation you've provided is about application specifics. Of course I can reuse in my application any gRPC status codes in any way I like to. And I can break retries too. But current Go gRPC implementation doesn't allow me any kind of retries (except transparent ones) because I can't be sure about the real cause of Unavailable.\nI've just checked what status code Java gRPC client returns in the same case. And it turns out that Go gRPC implementation isn't alone. It is very sad that the current state of the art in gRPC ecosystem effectively prevents retries on Unavailable status.. ",
    "wrfly": "There's a VSCode extension https://github.com/Jason-Rev/vscode-spell-checker\nI'm reading the source code recently and find these typos thanks to this plugin. \ud83d\ude03 . ",
    "tingtingr": "Where did you run go get -u google.golang.org/grpc and what's its output?\n- from our project directory\nAlso what does make build do? Is it just go build?\n- Yes. It just build and compile\nI have google.golang.org/grpc v1.17.0 in my go.mod file and it gives me the error. Maybe I am doing something wrong with that line. \n. Thank you for taking a look and I think the problem might be local. You are welcome to close the ticket now. =) . ",
    "asad-awadia": "@menghanl \nREADY\nINFO: 2019/01/31 09:31:20 transport: loopyWriter.run returning. connection error: desc = \"transport is closing\"\nINFO: 2019/01/31 09:31:20 pickfirstBalancer: HandleSubConnStateChange: 0xc00012c010, TRANSIENT_FAILURE\nINFO: 2019/01/31 09:31:20 pickfirstBalancer: HandleSubConnStateChange: 0xc00012c010, CONNECTING\nINFO: 2019/01/31 09:31:20 pickfirstBalancer: HandleSubConnStateChange: 0xc00012c010, READY\nI am assuming its reconnecting so quickly it goes back to READY immediately?\nIs there a way to turn retries/reconnections off? . ",
    "mullikine": "Hi @menghanl ,\nShane from CodeLingo here. We stuffed up, sorry.\nWe made an error when writing a Tenet that was used on your repository that resulted in incorrect changes to your code in some edge cases. We've fixed the bug and improved our internal process to prevent this happening again.\nWe're keen to learn how we can best help dev teams without getting in your way. Your feedback would really help us, my email's shane@codelingo.io.\nThanks,\nShane & the CodeLingo Team. ",
    "eau-io": "yep, legacy grpc.Decompressor is basically a black box so cannot be touched,  you've submitted the PR i was about to send with io.LimitReader() \ud83d\udc4d.\nSo I don't have to send it.. ",
    "jkryl": "Thanks for a quick reply! I tried version 1.11.0 but it did not help. Moreover I think that the solution described above (and implemented in the later versions) is not quite correct. http2 servers expect authority to have a form of hostname [+port] (the part of URI between scheme and path). Sending something that is not well understood in authority header can fail the whole request on server side (as it does in my case). Empty string is as bad as tcp://....\nThe problem seems bigger than just unknown schemes. I tried to use unix:// scheme which should be according to naming.md doc supported. Well, it turns out that at least with version 1.11.0 the authority header is empty again. The scheme is understood by grpc, but it tries to extract authority value using a kind of default algorithm which fails (yields empty string). And what should be the authority value in case of unix domain socket? If we inserted the path to the socket to authority header (i.e. /tmp/sock) it would be wrong because slashes are definitely not expected in authority section of the URI.\nI think it boils down to a simple rule of not setting the authority header if we are not sure  what the value should be. And in case of unix:// names it applies to all values.. FYI: in the meantime there has been a good discussion with folks who implement http2 library for rust (https://github.com/carllerche/h2/issues/345). The conclusion is, that sending arbitrary string in authority header is violation of http2 spec and they will not tolerate or implement workarounds for misbehaving clients.\nSo a short summary based on that is that: grpc-go  should be fixed to disable implicit authority header for unix domain socket transport. And in a broader context, for all other transports which valid authority value cannot be figured out for.. ",
    "zhyon404": "Run in to the same problem when upgrading to 1.18.0. \n1.17.0 works fine.. ",
    "jdejesus07": "I am currently using 1.19.0 and same issue described here was experienced. I added WaitForReady(true) to call options and worked as stated @mpuncel Ty \nThere definitely seems to be a bug here. I shall add this as a workaround for now and keep digging to see what I come up with.. ",
    "jdejesus007": "2019/03/01 14:17:38 http2: Framer 0xc4206b7260: wrote HEADERS flags=END_HEADERS stream=1 len=107\n2019/03/01 14:17:38 http2: Framer 0xc4206b7260: wrote DATA flags=END_STREAM stream=1 len=1285 data=\"\\x00\\x00\\x00\\x05\\x00\\t\\xbb'\\x0f\\v\\xb5v:@\\x11\\r\\xe0-\\x90\\xa0\\x04T\\xc0\\x18\\x012\\n\\b(\\x12\\x06\\b\\xb1\\xa3\\xdd\\xe3\\x052\\n\\br\\x12\\x06\\b\\x84\\xcd\\xe3\\xe2\\x052\\v\\b\\xb0\\x01\\x12\\x06\\b\\xa7\u06b5\\xe1\\x052\\v\\b\\xd5\\f\\x12\\x06\\b\\x96\\x8b\\xed\\xe2\\x052\\v\\b\\xd8 \\x12\\x06\\b\\xbd\\x95\\xdd\\xe3\\x052\\v\\b\\xa1-\\x12\\x06\\b\\xa7\\xa5\\xdc\\xe3\\x052\\v\\b\\xf6>\\x12\\x06\\b\\x96\\x8b\\xed\\xe2\\x052\\n\\b\\x0f\\x12\\x06\\b\\x96\\x8b\\xed\\xe2\\x052\\n\\b\\x12\\x12\\x06\\b\\x9b\\xc0\\xc9\\xe2\\x052\\v\\b\\x8c\\x01\\x12\\x06\\b\\xa3\\xa9\\xd1\\xe0\\x052\\v\\b\\xc7\\x01\\x12\\x06\\b\\xda\\xd1\\xdb\\xe3\\x052\\v\\b\\xff\\x02\\x12\\x06\\b\u0783\\x93\\xe3\\x052\\v\\b\\x99\\x1c\\x12\\x06\\b\\xb0\\x89\\xdc\\xe3\\x052\\v\\b\\xb8#\\x12\\x06\\b\\x96\\x8b\\xed\\xe2\\x052\\v\\b\\xc8,\\x12\\x06\\b\\xa7\\xbe\\xd2\\xe3\\x052\\v\\b\\xf04\\x12\\x06\\b\\xb5\\xe5\\xd2\\xe2\\x052\\v\\b\\xf6\\x01\\x12\\x06\\b\\xe8\\xa9\\xd6\\xe1\\x052\\v\\b\\xf9\\x01\\x12\\x06\\b\\xbd\\xe7\\xb6\\xe3\\x052\" (1029 bytes omitted)\nINFO: 2019/03/01 14:17:38 pickfirstBalancer: HandleSubConnStateChange: 0xc4203ccee0, TRANSIENT_FAILURE\nINFO: 2019/03/01 14:17:38 transport: loopyWriter.run returning. connection error: desc = \"transport is closing\"\nI removed the WaitForReady and turned off handshake but experienced the same behavior.. ",
    "aanm": "@menghanl that fixes the issue for me.. @menghanl patch #2669 does not solve the issue. it only works if GRPC_GO_REQUIRE_HANDSHAKE=off is set. \n\n@aanm\nIf GRPC_GO_REQUIRE_HANDSHAKE=off is required, it sounds like that the server didn't finish the handshake in time.\nHow is your server setup? Is it a proxy, a gRPC server, or server behind a mux (so port is shared by gRPC and http)?\n\n@menghanl  sorry for the late reply, the server is a binary already compiled with grpc 1.7.5 https://github.com/grpc/grpc-go/commit/5b3c4e850e90a4cf6a20ebd46c8b32a0a3afcb9e, it's an program called cri-o. In their vendor directory they have that commit that I was mentioning https://github.com/kubernetes-sigs/cri-o/blob/v1.13.1/vendor.conf#L24. They do have a cmux library in there https://github.com/kubernetes-sigs/cri-o/blob/v1.13.1/vendor.conf#L115.\nIf you want to install it in your computer, these instructions should be enough\n```bash\n   sudo apt-key adv --recv-key --keyserver keyserver.ubuntu.com 8BECF1637AD8C79D\ncat < /etc/apt/sources.list.d/projectatomic-ubuntu-ppa-artful.list\ndeb http://ppa.launchpad.net/projectatomic/ppa/ubuntu bionic main\ndeb-src http://ppa.launchpad.net/projectatomic/ppa/ubuntu bionic main\nEOF\n   sudo apt-get update\n   sudo apt-get remove cri-o-1.* -y || true\n   sudo apt-get install cri-o-1.13 -y\n```\nThe client that I'm using to test is an \"hello world\" type of code, which is compiled with grpc 1.18.0, that checks the status of the server\n```golang\nimport (\n    criRuntime \"k8s.io/kubernetes/pkg/kubelet/apis/cri/runtime/v1alpha2\"\n    \"context\"\n    \"fmt\"\n    \"google.golang.org/grpc\"\n    \"k8s.io/kubernetes/pkg/kubelet/util\"\n    \"runtime\"\n    \"sync\"\n    \"time\"\n)\nfunc getGRPCCLient(ctx context.Context) (*grpc.ClientConn, error) {\n    addr, dialer, err := util.GetAddressAndDialer(\"unix:///var/run/crio/crio.sock\")\n    if err != nil {\n        return nil, err\n    }\nc, cancel := context.WithTimeout(ctx, time.Duration(5*time.Second))\ndefer cancel()\n\nconn, err := grpc.DialContext(c, addr, grpc.WithDialer(dialer), grpc.WithInsecure(), grpc.WithBlock(), grpc.WithBackoffMaxDelay(15*time.Second))\nif err != nil {\n    return nil, fmt.Errorf(\"failed to connect: %s\", err)\n}\nreturn conn, nil\n\n}\nfunc newCRIClient(ctx context.Context) (*criClient, error) {\n    cc, err := getGRPCCLient(ctx)\n    if err != nil {\n        return nil, err\n    }\n    rsc := criRuntime.NewRuntimeServiceClient(cc)\n    return &criClient{rsc}, nil\n}\nfunc testCRI() {\n    c, err := newCRIClient(context.Background())\n    if err != nil {\n        panic(err)\n    }\n    sreq := &criRuntime.StatusRequest{\n        Verbose: false,\n    }\n    sresp, err := c.RuntimeServiceClient.Status(context.Background(), sreq)\n    fmt.Println(sresp)\n    if err != nil {\n        panic(err)\n    }\n}\n```\nIf this client program is executed with GRPC_GO_REQUIRE_HANDSHAKE=off everything works fine.. @gyuho We were hitting this [0] panic a couple of times and once we have applied this hot fix in etcd v3.3.11 and we haven't gotten any panic so far.\n[0] https://github.com/etcd-io/etcd/issues/9956. @menghanl Done!. It seems master branch is also failing on the same test. ",
    "mklencke": "I added my google.com account and the CLA should now be good. Please retry CLA verification.. bot, please retry :). I signed it. ",
    "taralx": "Done.. ",
    "vvakame": "I have a question.\nI found baseDelay = 1.0 * time.Second in here.\nI think this value is little bit big. Why we use 1 sec by default?\n1 sec is a too big from user-facing frontend side.\n. ",
    "mpipet": "I encountered the same issue using the gc bigtable go client (grpc based), bigtable grpc server seems to have a connection max age of one hour, which means that every hour, I litteraly stop handling my traffic for one second.\nI agree with @vvakame this default value is too high for a lot of use cases and should be easily configurable in the other google clients based on this repo.. ",
    "zoncoen": "Thank you for your reply!\nThe interceptor of dd-trace-go measures payload latency by wrapping SendMsg and RecvMsg.\nhttps://github.com/zoncoen/dd-trace-go/blob/3fbfdc796b3e9aed1c8a416de047b769d6a3a30e/contrib/google.golang.org/grpc/client.go#L23-L45\nI'd like to know how long did each payload take like it in the stats.Handler.\nWhere should I begin to measure latency?. Hmm... we can not measure the latency of each message in sendResponse because t.Write just append into the message queue.\nActually, loopyWriter gets the message from the queue and writes it asynchronously.\nIt is it correct?. Since I mistakenly believed that SendMsg writes the request on wire actually, I was planning to use for measurement of send latency like the intercepter.\nSometimes the request latency is increased on our Go microseevice. It seems caused by the spike of SendMsg latency. We'd like to investigate more detail but I understood this approach is not works...\nThank you for your explanation.. ",
    "yinjun622": "build by tags linux , it works.\nCC=$NDK/bin/aarch64-linux-android/bin/aarch64-linux-android-gcc GOOS=android GOARCH=arm64 CGO_ENABLED=1 go build -tags linux. @lyuxuan Thx.. ",
    "SpeedyCoder": "Ah cool, thank you \ud83d\udc4d . ",
    "BlueStalker": "my server grpc is in java, and I am using engine.shutdown() function to shutdown the server.\nAnd it is confusing me that why some of the stream is seeing Unavailable, and some is seeing cancel, this depends on what?. ",
    "kaixinbuyu": "Thank you!I'll try it at once!. ",
    "WeiranFang": "\n\"left-to-right\" is not clear to me. Instead, we should define which is the inner-most and which is outer-most.\nIn your tests, it seems left is assumed to be outer-most, so it both first and also last (it wraps other interceptors).\n\nYes the left-most interceptor passed in is the outer-most, and the right-most is the inner-most. I have updated the doc string to avoid confusion.. Nice. I have adapted your logic.. done. done. ",
    "aofei": "This problem has been fixed in 7c93bdf75abc54e01789b31b63bfdd3ddc648142. You just have to wait for the next version to be released.. ",
    "zsurocking": "extra space\n. ",
    "bcmills": "This call to wait occurs with t.mu still locked - is that safe?\n(The corresponding call to t.streamsQuota.add also requires t.mu.)\n. Do you need to re-do the quota and state checks after reacquiring the lock, or is all of that sticky?  (A comment would be helpful.)\n. The locking model here still seems inconsistent - the streamsQuota.add call here is with t.mu locked, but at 255 t.mu is still unlocked.  (Is the streamsQuota op atomic?  If so, could this be moved after t.mu.Unlock()?)\n. This looks like it races with the t.streamsQuota.add call at 255 - if the reset occurs before the add, the quota will permanently increase above the configured amount.\n. This still doesn't work, as far as I can tell.  The reset is atomic, but it doesn't take into account the number of streams in-flight.\nIf you want to swap out t.streamsQuota, you need to ensure that there are no streams in-flight.  Otherwise, you'll need to implement this in terms of acquire and/or add.\nOne alternative to consider: if you store the original quota size, then you can compute the delta between the new and old settings.  If new > old, you can just add it; if new < old, you can acquire the difference and just not add it back.\nBut that makes the handleSettings function take an arbitrarily long time while it waits for the existing streams to settle...\n. A minimal example:  what if you have two newStream calls that both update maxStreams, from X to Y and then to Z?\nThe first call does:\nLock\nms := X\nt.maxStreams = Y\nUnlock\nThe second does:\nLock\nms := Y\nt.maxStreams = Z\nUnlock\nreset(Z - Y)\nAnd then back to the first:\nreset(Y - X)\nAnd that's not even taking into account the t.streamsQuota.add calls that may have occurred in the interim, which AFAICT introduce problems of their own.\n(In general, when you're mixing atomic-increment and atomic-reset the resets need to be compare-and-swap, not blind writes.)\n. A \"go\" statement at the end of a goroutine seems redundant.  Just call \"c.transportMonitor()\" directly here?\n. Shouldn't this also keep retrying?\n. Shouldn't this keep retrying to connect to the server on failure, perhaps with some kind of backoff?\nOr, if this is due to e.g. a malformed target name or invalid transport parameters, perhaps a comment here would help clarify.\n. Does the Picker need to know anything else about the request in order to select the Conn, e.g. query cost or perhaps a sticky-backend scheduling token?\nWhat happens if there are no available Conns (e.g. due to all of them redialing)?  Should it return some particular error immediately, or block until one becomes available?\n. Having just a timeout here seems like a problem - what happens if the connection is closing or the caller is no longer interested in the request?\nSeems like a Context (http://blog.golang.org/context) would be more appropriate.\n. Shouldn't this backoff and retry if the picker error is transient?  (Or is Pick supposed to block - in which case why doesn't Pick take a Context?)\n. If users should not implement this interface, why is it an interface at all?\nIt seems like you could just as easily export some concrete type, and then you wouldn't have to contend with the (inevitable) inappropriate mocks of this interface.\nSomething like:\ntype Status struct {\n  \u2026\n}\ntype statusError struct { Status }\nfunc (s statusError) Error() string { \u2026 }\nfunc (s *Status) Err() error { return statusError{s} }. ",
    "awpr-google": "The spec isn't clear on what the result status should be when trailers are omitted.  My interpretation was that the RPC should still be treated as successful.  Perhaps it would make sense to record the situation in the log either way.\nIn my case the server wasn't so much \"broken\" as just \"partially implemented\".\n. ",
    "dmcaulay": "the current codec validation does not use string operations. does the spec indicate that the content type is case insensitive?. would you be able to get rid of the codec concept if you default the subtype to \"proto\"? it'd be nice to always grab the codec from the codecs map.. you might also want to push this into stream.ContentSubtype() and memoize so you don't do this multiple times per call.. \ud83d\udc4d. ",
    "jack0": "It's not needed, I'll remove it.. I remembered why you need it, you need to give it sufficient time to reach the block of code that sets the peer.. I've made the requested change, thanks!. ",
    "knz": "This truncation is incorrect if the string contains UTF-8 multibyte characters. It can produce invalid UTF-8 sequences.. "
}
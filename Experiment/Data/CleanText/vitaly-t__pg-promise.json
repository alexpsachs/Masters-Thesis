{
    "vitaly-t": "Implemented it with a small breaking change to how the database is to be initialized, though it is a very small change on the client side, in just one place.\nHowever, inside the library itself the change was huge - lots of rewrites and improvements.\n. I did yes, but thinking about it was as far as I got.\nFor one thing, I haven't done any proper testing using Karma or similar, so I don't have enough experience. But surely, I'd figure it out quick once I get started.\nFor another - I always thought that testing database frameworks is a major pain, because you need a database to do it properly; otherwise - what is it worth?\nI started with the first draft only 2 days ago, and since then made 15 releases, tons of documentation, and continue spending all my time on it.\nToday I added my first milestone to it to make some breaking changes to do proper support for using multiple databases. And right now, just 3 hours later I have already finished it - see the code in branch v.0.2. It all seems to be working, i'm just gonna give it a bit more testing and update documentation, and i'm done. It turned out to be a big rewrite, though doesn't affect the usage cases much.\nAnyhow, I really appreciate you offering your help with the test. I think that simply forking it out for yourself is the right way to go. The only hold-up, i'd say, I've just made huge changes, so writing the actual tests wouldn't be of much benefit, till I do another release shortly, but setting up the test frameworks for now - sure thing!\nRight now, I wouldn't start it myself, but after I finalize the ongoing changes. Feel free throwing in any suggestions in the meantime or forking it out to add a test framework ;)\n. Sound we just might make a good testing team :) let me get back to you once I've finished the current massive changes for 0.2.0 ;)\n. As an update... the code seems to be finished and working, and the documentation is also up-to-date.\nI'm going to merge this branch v.0.2 into the master branch tomorrow, perhaps improve the documentation and then release 0.2.0, at which point it will be possible to do some tests integration ;)\n. @emilioplatzer,\nThe new version just has been released, so you can fork it and start on the tests, if you want ;)\n- I opted for the C-style because I have 3 flags with all the combinations. I'm not sure if it would look as readable in any other form. If you think it would - please give me a complete example, so I can understand it.\n- Passing parameters isn't part of PG or Promise, so you are talking about just extra functionality for the library. I've seen a number of packages that do just that - format parameters, so I didn't want to get into that, focusing just on the core functionality between PG and Promise, or at least for now :)\n  Also, I do process parameters when calling a function or procedure ;)\n- I think that adding automated testing is much more important now than extending the library with new functionality ;)\n. UPDATE:\nI've done enough updates and even releases for today, I think :)\nWhat you may find interesting is section Understanding the query result, which explains in detail how Query Result works. So, if you still want to come back with a suggestion about changing it, that explanation should help ;)\n. Oh my, that bluebird promise implementation, makes you wonder: 700k of monthly downloads for promise, 600k monthly downloads for bluebird, makes it tough to choose which one to use :)\nNot that I have anything against blue birds, though it is a strange name, I prefer promise as the name :)\nThank you for pointing out at that pg-bluebird package, I will have to look through the code. Pity they do not have any demo for parameters usage to understand it better how it works.\nUPDATE\nOk, I've just looked at that pg-bluebird package, it is a joke of a library, just 50 lines of code where all he does - passes requests into PG and returns a bluebird's promise object. This means that any parameter formatting is done within the PG package. I will look at it later on.\n. For now my question -is this formatting thing handled by pglib or is it just a pg package feature?\nI mean, i'm trying to understand just how important it is. Is there a scenario where you could pass a parameter through PG but not through my library? I mean, after all, I am accepting a generic query syntax, so you are free to format it in any way that you want.\nLet me know what you think ;)\nCheers!\nUPDATE\nJust to give you an example, when I did query formatting in my projects, I used the following:\njavascript\n// global string formatting;\nif (!String.prototype.format) {\n    String.prototype.format = function() {\n        var args = arguments;\n        return this.replace(/{(\\d+)}/g, function(match, number) {\n            return typeof args[number] != 'undefined' ? args[number] : match;\n        });\n    };\n}\nThis allows string formatting anywhere in the code, like this:\njavascript\ndb.one(\"select name from agents where id = {0}\".format(id));\nThere are other types of formatting that exist out there. And which one is the best approach? :)\nFor the time being I have parameter formatting for function calls, because those turn into a comma-separated list of values, but when it comes to formatting query parameters - one is free to use any kind of syntax and formatting tool.\nThe bottom line is, I just want to be certain about one particular approach before adding it to the library, if it is worthwhile.\n. And while I'm still thinking about the value formatting, I did a fix in it, plus added as.csv, as a pre-emptive to our discussion about value formatting.\nNow one can call it like this:\njavascript\ndb.none(\"update users(name, active, updated) values({0})\".format(pgp.as.csv([\n\"John O'Dwyer\", true, new Date()\n])));\nI know this isn't an answer to what we are discussing, it's just a small step in that direction, in a generic way.\nWhen we call:\njavascript\npgp.as.csv([\"John O'Dwyer\", true, new Date()]);\nwere are getting this:\njavascript\n'John O''Dwyer',TRUE,'Fri, 06 Mar 2015 21:40:45 GMT'\nNote how a single quote in text was fixed, boolean and Date properly represented. So, it is a good helper, I believe ;)\n. Ok, after having given it some consideration, I have added support for parameter formatting, so now all query functions can be called like this:\njavascript\ndb.many('select * from users where name like $1', '%vit');\ndb.many('select * from audit where updated > $1 and entity = $2', [new Date(), 'test']);\nBecause of this, I had to make a breaking change in the function usage, so it is now:\njavascript\ndb.func('funcName', queryResult.one, [true, 'hello'])\ni.e. we have to pass the queryResult as the second parameter and then the values, as it is now consistent with method query. The reason is simple - queryResult is a required parameter, while formatting values are optional.\nFor any call there can be either a single value passed or an array of values.\nDocumentation has been updated to at least not to contradict the current implementation, but it needs more work to do justice to the implementation.\n. Since you haven't replied since Friday, and I have rewritten 80% of the library code over the weekend, updated the document and published the new release, which includes both value formatting that you asked for and much-much more.\nSo, I'm closing this issue, since it has become a topic lost in conversation. If you still want to contribute with a testing framework - feel free to fork it out. I'm not planning to make the same huge changes any more :)\n. @emilioplatzer,\nThank you for your long feedback. I've been doing so many updates of lately, I think it is important to get as much feedback as possible first (from a few people that is).\nBy the way, regarding your suggestion:\n\nIf you need you can add a method ctx.oneValue(\"SELECT one_field FROM ... \") that returns a value insted of a row.\n\nYou already can make such call, and the simplest form is:\njavascript\nctx.one(\"SELECT column from table\");\nI've just spent too much time writing all this, will try to stay away for couple days, see what kind of feedback I get :) So far the number of downloads on npmjs.org is promising :)\nI was in a rush because I knew I was introducing too many breaking changes, so I wanted finalize them all and release before the end of the weekend, which I have done. You can see how much of the documentation has changed also ;)\nUPDATE\nI still managed to do a few updates today, can't look away :)\nAnd I totally agree that a good testing approach here is a must, I just to figure out the best way to start it, especially when it comes to using a database. Any suggestions are welcome ;)\n. First test integration added ;) Will try writing the actual tests later.\n. Tests have been added, and new release with tests is out. If you have something to add, please do in a separate subject, in case there are issues that is, as this one should be just left in the past now.\nI'm locking it down ;)\n. Hi,\nThere is one already: https://github.com/Aphel-Cloud-Solutions/pg-bluebird\nIt is a far more simplistic one, doesn't really do much, if you look at the code. And I argued with the author that even his one code example is incorrect, the way he closes the database connection.\nI believe, that with a little tweak I could provide a switch from one promise library to another, provided there is a 100% compatibility, which I'm not sure even when it comes to between Promise and Bluebird.\nFor example, I'm using method done() to release a connection, not sure Bluebird has one, because it is not quite from A+ specification.\nAnyhow, to sum this up, yes, I think this is quite doable, just takes a little time and effort ;)\nJust so, when it comes to making calls to the database, performance of the promise library becomes absolutely irrelevant, as it will always be less than 1% of time from the query execution.\nSo, just for the performance reasons, the way you phrased it - I wouldn't consider it, only for the reason of flexibility of which promise library to use.\nBut then again, the two libraries are mostly compatible, which means you can use both in your application, which sort of makes it less relevant ;)\n. I referred to method done() of the Promise library, not one from the PG library :)\nFrom the official documentation:\n\nPromise#done(onFulfilled, onRejected)\nNon Standard\nThe same semantics as .then except that it does not return a promise and any exceptions are re-thrown so that they can be logged (crashing the application in non-browser environments)\n. Wonderful, then we are almost done here :)\n\nThis means all my code samples will work just the same, if you make a request from Promise and then continue the resolution chain through Bluebird.\nSurely, someone must have tried it already. \nAnd as I stated earlier, performance of promises isn't relevant when it comes to executing database queries.\n. You know, I'm beginning to like your idea. I've just checked in an initial support for the feature, new property promiseLib is now supported within the library options:\njavascript\nvar promise = require('bluebird');\nvar options = {\n    promiseLib: promise\n};\nvar pgp = pgpLib(options);\nI've tested it against the whole library, and everything seems to work just fine. That was easy :)\nRight now it works as an override, i.e. if you do not override it, it will use Promise library by default. I don't know if it is good or not right now, I'll just have to think about it. What do you think?\nI'm gonna give it a bit more testing and will include this feature into the next release ;)\nCheers!\n. Support for custom promise libraries has been added, tested and released. Closing the issue.\n. @lalitkapoor \nJust so, release 0.5.1 now includes a much better support for alternative promise libraries, to which end the documentation has been updated also :)\nAgain, thanks for the idea! :+1: \n. It is done.\n. After more investigation into all existing promise libraries I found out that it is not really needed. Support for more promise libraries has been added, but in a simpler way, so effectively the higher goal has been achieved. Closing it now.\n. Thank you!\n. You are welcome!\nThis library was developed as part of a large project, one that's still in development, and uses this library extensively in a live environment, that's why I'm not overly concerned that I didn't finish the tests against the database, though it will be nice at some point to finish them.\n. Although there were quite a few changes in the transactions logic, the library protocol remains unchanged, i.e. there are no breaking changes, which means there is no point moving to a higher version, just incrementing the low version, makes it 0.5.6.\n. @jcristovao , well spotted, the problem is there indeed, i'm looking into it right now, will provide an update here soon.\nI'm going to fix this myself asap, and update the tests for this case. The idea with reversing the sequence should work, though it would change the direction in which possible errors are reported.\n. @jcristovao, thank you again for finding this issue. The issue has been fixed and new release 0.6.5 is available with the fix now.\nIt was very interesting to figure out the best approach, and I tried a few. In the end though, I believe I ended up with the simplest one - see it in the code.\nRegarding your suggestion about reversing the search loop - this wouldn't help, it could only solve part of the problem. First of all, as a side effect, it would reverse the direction in which errors are being reported during formatting, which would be bad. And it does not solve the issue with longer variable names. Here's an example:\njavascript\npgp.as.format(\"$111, $1, $222\", [1, 2]);\npgp.as.format(\"$111, $2, $333\", [1, 2, 3]);\nIf we just reverse the search, the first call gives us 1, 1, 2 and the second one gives us 1, 2, 3, while what was expected - $111, 1, $222 for the first call and an error for the second one, because it doesn't contain variable with name $1, so it's all completely wrong.\nThe right approach was to search for the variable, and once found - check that it is not immediately followed by a digit, and if it is - continue the search further.\nInitially I implemented such search in a function, but in the end replaced it all with RegEx for much more elegant solution. The RegEx for variable replacement was also changed according to this new logic.\nI added 8 new tests in file indexSpec.js to cover every single scenario with variable formatting, including one where I generate 1000 variables and replacements, just to show it works now the same with any number of variables.\n``` javascript\n        // test that errors in type conversion are\n        // detected and reported from left to right;\n        q = pgp.as.format(\"$1,$2\", [{}, {}]);\n        expect(q.success).toBe(false);\n        expect(q.error).toBe(\"Cannot convert parameter with index 0\");\n    // test that once a conversion issue is encountered,\n    // the rest of parameters are not verified;\n    q = pgp.as.format(\"$1,$2\", [1, {}, 2, 3, 4, 5]);\n    expect(q.success).toBe(false);\n    expect(q.error).toBe(\"Cannot convert parameter with index 1\");\n\n    // testing with lots of variables;\n    var source = \"\", dest = \"\", params = [];\n    for(var i = 1;i <= 1000;i ++){\n        source += '$' + i;\n        dest += i;\n        params.push(i);\n    }\n    q = pgp.as.format(source, params);\n    expect(q.success).toBe(true);\n    expect(q.query).toBe(dest);\n\n    // testing various cases with many variables:\n    // - variables next to each other;\n    // - variables not defined;\n    // - variables are repeated;\n    // - long variable names present;\n    q = pgp.as.format(\"$1$2,$3,$4,$5,$6,$7,$8,$9,$10$11,$12,$13,$14,$15,$1,$3\", [1, 2, 'C', 'DDD', 'E', 'F', 'G', 'H', 'I', 88, 99, 'LLL']);\n    expect(q.success).toBe(true);\n    expect(q.query).toBe(\"12,'C','DDD','E','F','G','H','I',8899,'LLL',$13,$14,$15,1,'C'\");\n\n    // test that $1 variable isn't confused with $12;\n    q = pgp.as.format(\"$12\", 123);\n    expect(q.success).toBe(false);\n    expect(q.error).toBe(\"No variable found in the query to replace with the passed value.\");\n\n    // test that $1 variable isn't confused with $112\n    q = pgp.as.format(\"$112\", 123);\n    expect(q.success).toBe(false);\n    expect(q.error).toBe(\"No variable found in the query to replace with the passed value.\");\n\n    // test that variable names are not confused for longer ones;\n    q = pgp.as.format(\"$11, $1, $111, $1\", 123);\n    expect(q.success).toBe(true);\n    expect(q.query).toBe(\"$11, 123, $111, 123\");\n\n    // test that variable names are not confused for longer ones,\n    // even when they are right next to each other;\n    q = pgp.as.format(\"$11$1$111$1\", 123);\n    expect(q.success).toBe(true);\n    expect(q.query).toBe(\"$11123$111123\");\n\n``\n. UPDATE: I'm working on a breaking change for the wayas.format` works by default, hence the few commits so far, more coming. It will be released as 0.7.0.\nBasically, the way as.format works was derived from the fact that initially this was only used internally by the library, but since it became available externally, it isn't very logical that the method returns an object with the result. What it will do after the change instead - throw an error whenever something is wrong with the formatting request, and it will require optional se - suppress errors flag, as the third parameter to return an object as before, to remain compatible with its internal usage, and give the end user more control over how to handle errors during query formatting.\nThere is a number of tests that are being adjusted to this new logic.\n. 0.7.0. has been released, with a small breaking change as described.\n. @jcristovao \nYou might like the new support for Named Parameters even better ;)\n. @jcristovao Unrelated to this, but if you liked this library, I really appreciate if you can give me a feedback on this small addition: pg-monitor\n. @joeandaverde, I think it is generally a good idea, but your implementation for the variable syntax reminds me of an issue we had recently with the $1,$2 variables when it comes to long variable names.\nWhen we encounter $1, $2 variables, we make sure they are not followed by a digit, i.e. they are not $11, $22, or something like that, because those are then different variables. But in your case there cannot be a truncating symbol to be identified, because you use regular letters for your variables. As a result, how could you possibly tell the difference between variables :id and :idiom?\nNow, if you were to suggest using something like this: ${id} or even :id:, there wouldn't be any question, as it clearly identifies the delimiters, but you don't seem to use any, which creates a problem.\nAny thoughts on that? Any existing standard for that, perhaps, as a reference?\n. I'm going to look around for any kind of standard way for identifying a variable in that case, and it would help if you did too ;)\nAlso, I'd like to avoid enumerating an object-parameter for its properties, because it may have lots more than just the parameters needed, and I believe that using the query as the source for variable names is the way to go, i.e. we search through the query using regex to make the list of all variables to be replaced, and only then use the object to pull its properties, and not the other way round.\nHowever, this does imply use of strict variable syntax, to avoid misidentifying them.\n. Yes, this does create an ambiguity, with Postgres using :: for type casting.\n. Thank you for that link. The best syntax I can think of for now is ${propertyName}, as the most standard and reliable way for injecting object properties.\nIf you find a better one - please let me know ;)\n. Just so, about your concern of creating a conflict: http://updates.html5rocks.com/2015/01/ES6-Template-Strings\nAccording to that, ES6 specification requires use of `` symbols to declare a template, as opposed to '' simple quotes, and since we are using regular strings to inject variables, there wouldn't be a conflict with ES6 Template Strings, even if we use the exact same syntax for variables.\nIn fact, I see something more of a positive thing that we would have it compatible with ES6 that way ;)\n. I'm going to take some time to think about the best approach to doing this. But in the meantime, I was playing with the idea of how to pull names of all variables-properties in a query for the syntax of ${varName}, and I came up with the following code:\n``` javascript\n/////////////////////////////////////////////////////////////\n// Returns array of unique variable names in a text string;\n//\n// - variables are defined using syntax: ${varName};\n// - a valid variable starts with a letter or underscore symbol,\n//   followed by any combination of letters, digits or underscores.\n// - a variable can have any number of leading and trailing spaces;\nfunction enumVars(txt) {\n    var v, names = [];\n    var reg = /\\${\\s[a-zA-Z_][a-zA-Z0-9_]\\s*}/g;\n    while (v = reg.exec(txt)) {\n        var svn = v[0].replace(/[\\${\\s}]/g, ''); // stripped variable name;\n        if(names.indexOf(svn) === -1) {\n            names.push(svn);\n        }\n    }\n    return names;\n}\n// example:\nconsole.log(enumVars(\"${var_1} ${  var2  } ${ var3_} ${ var_1 }\"));\n// outputs: [ 'var_1', 'var2', 'var3_' ]\n```\nThe next step would be just to pull values of the corresponding properties from the object-parameter and do the replacements. Easy! :)\n. I have created a new branch named-params, to start working in this direction, with the above method checked in, as a starting point.\n. I'd like to see that easier way. I know that with RegEx it would be just one line of code, using the same RegEx I used to search for variables. I'm not sure the same simplicity can be accomplished with simple string functions, considering all the complex cases of white spaces that RegEx takes care of automatically. Please show me your simpler alternative.\n. It is difficult to understand the way it works there exactly, except for the fact that it deals with a different syntax other than ${varName} and that it does a replacement right there?\nWould you like to create a version of it that produces the same result as one I wrote above?\n. I have finished the initial implementation for supporting named parameters in the new branch - check it out ;)\nFor the time being I'm using my own RegEx approach, and I included one complex test there, see in file: indexSpec.js:\njavascript\n    it(\"must correctly format named parameters or throw an error\", function () {\n        // - correctly handles leading and trailing spaces;\n        // - supports underscores and digits in names;\n        // - can join variables values next to each other;\n        // - converts all simple types correctly;\n        // - replaces undefined variables with null;\n        // - variables are case-sensitive;\n        expect(pgp.as.format(\"${ NamE_},${d_o_b },${  _active__},${ file_5A  }${__Balance}\", {\n            NamE_: \"John O'Connor\",\n            d_o_b: dateSample,\n            _active__: true,\n            file_5a: 'something', // not to be found, due to case difference;\n            __Balance: -123.45\n        })).toBe(\"'John O''Connor','\" + dateSample.toUTCString() + \"',TRUE,null-123.45\");\n    });\nIf you think that you can replace the search-and-replace approach with a better one - give it a go, you have all the code, and even a good test to begin with.\nOther than that, before the branch is ready to be merged into master, I will be doing a number updates covering tests, documentation, and possibly implementation, if something better comes along.\n. P.S. Please star this package, if you like :) It helps with the search result on npm ;)\n. As there were no further comments, I finished all tests and documentation updates and released version 0.8.0 with the new feature. Details are in the release notes.\n. To my knowledge, there is no such thing for that which you want. Postgres doesn't support named parameters. See this: https://github.com/brianc/node-postgres/issues/268\nAnd SQL injection isn't a real problem on its own. One has to create a gap in URL-to variable mapping, or otherwise do a poor design overall. Therefore I'm not overly concerned about it.\nI extended the syntax to support named variables from objects, which works now, and this is about as good as it's gonna get :) I don't see how i can make it any better.\nThe current code for 0.8.1 will be released soon, with one change to parameter formatting - it will throw an error when a variable doesn't exist, as opposed to formatting it as null right now.\nAnd I don't see how possibly we could have been talking about something else when even your references to your own library suggested exactly what was done here.\n. > This type of query can be sanitized by the postgres database engine whereas one which allows for string substitution would not.\nWhat are you talking about? Postgres doesn't take any variable parameters. Nothing can be sanitized by Postgres database engine. Only the final query can be passed from the client, nothing else.\n. How do you pass $1, $2 into Postgres when it doesn't have a protocol for doing this?\n. Please, point at the Postgres online documentation about passing parameters with a query, because I've never heard of such thing's existence.\nP.S. I'm done for the day, back to this tomorrow.\n. Those are local formatting commands, with specific use of EXECUTE query USING, for use from  inside functions only. It is unusable in our case.\n. It seems that PG now has support for Prepared Statements, which may be what you are looking for.\nBy the way, if you like PG implementation of parameter formatting better, you can always use that instead. This library has always allowed that, see Initialization Options, parameter pgFormatting.\n. @joeandaverde Unrelated to this, but if you liked this library, I really appreciate if you can give me a feedback on this small addition: pg-monitor\n. Fixed in version 0.8.2\n. How would you expect an array - parameter to be resolved into query values?\nPlease give us a full example, so we can understand better how you are expecting this to work.\n. Ok, I have looked at the arrays in Postgres and I can see the conversion logic:\n- [1,2,3] becomes '{1,2,3}'\n- [[1,2],[3,4]] becomes '{{1,2},{3,4}}'\nThe thing is, not only the library would have to do this recursively, to support arrays within arrays, but since it cannot know what to expect as array elements, it would also need to apply generic conversion on each element.\nI'm not saying this is impossible, but I would need to research this for any other implications.\nIn the meantime, if you are dealing with one-dimension arrays, you can just convert it yourself and pass the result as a string.\n. > Just to be clear, I'm talking about assigning an array to a single $1 value.\nHere's the first problem (though easily solvable): When you pass an array to the formatting function it thinks you are using $1, $2,.. formatting, so it cannot tell this from when you are passing it as a field value. This means you could only pass it either as [[1,2,3]] or as {myArray: [1,2,3]}\nI think this is a good idea to be implemented. And in the meantime, you should be able to do an easy work-around by converting such an array into a text string yourself.\n. I've just done the conversion helper, to support arrays of any dimension:\njavascript\n// Converts array-value into Postgres format;\n// It supports arrays of any dimension;\nfunction formatArray(arr) {\n    var s = '{';\n    for (var i = 0; i < arr.length; i++) {\n        if (i) {\n            s += ',';\n        }\n        if (arr[i] instanceof Array) {\n            s += formatArray(arr[i]);\n        } else {\n            s += arr[i]; // or converted;\n        }\n    }\n    return s + '}';\n}\nI suppose there is no point limiting the number of dimensions supported.\nOne important aspect of such support: It is impossible to control that the array values adhere to Postgres support for arrays, i.e. we can only convert the values exactly as they are in the array, which means if you have an array of [1, 'two', true], you get '{1,'two',true}', which will never work with Postgres, so you have to control it yourself what's in array.\nDo you think this would present a problem? For the time being I do not see what can we do about it anyway.\n. Ok, I have checked in the implementation + a few basic tests, it seems to be working fine now.\nMethod pgp.as.array() has been added also.\nI will need to add full test coverage + update the documentation before I can release it, which will be either later today or tomorrow.\nIn the meantime, if you want to give it a go - just override your local formatting.js file with the latest from the master.\nI appreciate your feedback on that!\n. UPDATE: The formatting method has been updated to correctly throw errors when it cannot convert the array:\njavascript\n// Converts array-value into Postgres format;\n// It supports arrays of any dimension;\nfunction formatArray(arr, dim, step) {\n    if (!dim) {\n        dim = 0; // current array dimension;\n    }\n    if (!step) {\n        step = 1; // current iteration step;\n    }\n    var s = '{';\n    for (var i = 0; i < arr.length; i++) {\n        if (i) {\n            s += ',';\n        }\n        var v = arr[i];\n        if (v instanceof Array) {\n            s += formatArray(v, ++dim, step);\n        } else {\n            var value = formatValue(v);\n            if (value === null) {\n                throw new Error(\"Cannot convert type '\" + typeof(v) + \"' in array: level \" + dim + \", index \" + i + \", step \" + step);\n            }\n            s += value;\n        }\n        step++;\n    }\n    return s + '}';\n}\nWhen fails, it will report the dimension/level + index within that dimension + the total interation step. I will document it later on.\n. This one is much better, it reports the index properly in case of error...\njavascript\n// Converts array-value into Postgres format;\n// It supports arrays of any dimension;\nfunction formatArray(arr, idx) {\n    if (!idx) {\n        idx = []; // indexes;\n    }\n    var s = '{';\n    for (var i = 0; i < arr.length; i++) {\n        if (i) {\n            s += ',';\n        }\n        var v = arr[i];\n        if (v instanceof Array) {\n            idx.push(i);\n            s += formatArray(v, idx);\n        } else {\n            var value = formatValue(v);\n            if (value === null) {\n                idx.push(i);\n                throw new Error(\"Cannot convert type '\" + typeof(v) + \"' of array element with index \" + idx.join(','));\n            }\n            s += value;\n        }\n    }\n    return s + '}';\n}\n. Sorry, couldn't wait for you to get back on this, the feature has been fully tested and released as 0.9.2.\nThank you for the find.\n. Reopening this issue, because the initial implementation proved to work only for arrays of integers. Once you try using it for arrays of strings, nothing works there because of the way strings are formatted as array values - they use double quotes instead of single quotes.\nIn order to fix this problem a change has been made to use the alternative syntax of array[] supported by Postgres. This syntax allows any type of values in an open format (not as a text string), and text strings are passed using single quotes, so no issue there any more.\nThe fix has been committed and will be made available with 0.9.3 release.\n. The last fix was released with 0.9.3\n. There is a formatting mistake in your example: First, you convert your array into a text string and then you format that text through variable once again, that's double-formatting, which breaks things.\nCorrect alternatives are shown below:\n- regular array insertion:\njavascript\nreturn db.none('insert into foobar values ($1)', [['biz']]));\n- inserting array with explicit conversion:\njavascript\nreturn db.none('insert into foobar values ($1^)', pgp.as.array(['biz']));\nNote in the last one, we inject it as a raw text, not formatted text, because it is already formatted after calling pgp.as.array\nYou can also insert it using the Named Parameters. See all the examples.\nThe latest syntax also allows:\njavascript\nreturn db.none('insert into foobar values ($1)', function(){\n     return ['bix'];\n});\n. @chrisvariety , you are welcome!\nAnd as for the link that you added, that is quite interesting. I wasn't aware of an issue like that, but then again, it was logged back in 2012, and PostgreSQL had a few releases since. The oldest I tested it against was 9.2, and it didn't have any problem accepting array[] syntax with inserts. I have to presume this issue was resolved in one of the earlier versions of PostgreSQL. And for that reason, I don't think there is any point in looking back at the old {} format, for it is quite old and awkward when it comes to dealing with arrays of strings.\nAnyway, the problem you had was unrelated to that, but still, thanks for the link ;)\n. Do I take it right that you use the latest version 0.9.2? If that's the case, the only thing that changed there - the default Promise library was upgraded from 6.1 to 7.0.\nAnd according to your error log, this is exactly what's causing the problem. To prove it, use a different promise library, like Promise 6.1 or Bluebird. The default promise is very easy to override during the library's initialization: https://github.com/vitaly-t/pg-promise#promiselib\nLet me know if this solved the problem, and then we can investigate further what it is exactly that's causing the failure inside that promise library. But for the time being this really looks like something got broken in Promise 7.0\n. @ForbesLindesay I understand this question is for the original poster and not myself. I hope he will come back to us tomorrow with some results and details. But if you want to elaborate in the meantime about the implication from domains usage, that'd be excellent. Cheers!\n. @ForbesLindesay your example throws an error different from the one you described:\n..\\promise\\lib\\core.js:71\n  if (this.constructor !== Promise) return this._10(onFulfilled, onRejected);\nAnd I did a simpler test to understand it, so the following code works in both 6.1 and 7.0, printing test\njavascript\nfunction work(){\n    return new promise(function(resolve, reject){\n        resolve('test');\n    });\n}\nvar w = work();\nw.then(function(data){\n    console.log(data);\n});\nBut if we change it to:\njavascript\nfunction work(){\n    return new promise(function(resolve, reject){\n        resolve('test');\n    });\n}\nvar w = work().then;\nw(function(data){\n    console.log(data);\n});\nthen similarly the same code works with 6.1, but with 7.0 it throws the same error that I quoted above.\nI do not know anything about the domains and the implications of using them, but we haven't established that as the reason. In fact, my first guess was and still is, one of the updated dependencies in Promise got broken. This guess is also backed up by the fact that this library (pg-promise) never uses that broken pattern above.\nAnother point of concern and/or argument - Bluebird works for the guy,  no issues.\n@parashar I'm glad that you have resolved the issue in production with the default promise override, so you can help find the problem without being under pressure.\n. @ForbesLindesay I can only say the same about pg-promise that it runs every test I could think of without any issue. At some point though, I changed the test suit to use Bluebird, because I had to use method any for one test, which is missing in Promise, but that doesn't matter. I just changed it here locally to Promise 7.0, commented out one test that uses any, and it's all pass.\nAt this point I agree, we need more details from the author, otherwise we cannot proceed.\n. @parashar, Please get back to us with more details, otherwise we cannot proceed.\n. @parashar , there is a number of issues with your code:\n1. Variable param is set only if arguments.length == 3 , I don't know if this is by design or an omission;\n2. If an error happens during a connection, you do not return a promise, so the chain that follows it is broken;\n3. You call done(), but do not actually close the connection that you open above. The connection needs to be closed using conn.done(); in your case.\n4. Why do you open the connection in the first place, when it is not even needed there?\n5. If your function cb throws an exception anywhere, your code doesn't handle it, breaking the promise chain.\nIn all, your whole code example looks like an attempt to take a promise chain and turn it into a callback chain, which is reversing the point of using promises in the first place.\nSo, first, let's simplify the code:\njavascript\nexports.query = function (query, q_params, cb) {\n    var params;\n    if (arguments.length == 3) {\n        params = q_params;\n    }\n    if (arguments.length == 2) {\n        cb = q_params;\n    }\n    db.query(query, params)\n        .then(function (data) {\n            cb(null, data);\n        }, function (reason) {\n            cb(reason, null);\n        });\n};\nAnd second, you need to check whether your callback can throw an exception, and then also add .catch(err) to the end of your chain.\nOther than that, I really suggest that you look at how to use promises properly. And as for this library, Learn by Example tutorial might be of help also ;)\nIn the meantime, I'm closing the issue, because it is quite possible for the issues in your code to break promises. If you review and change it to a proper usage and still find errors happening, then we can reopen it and see what's going on.\n. @ForbesLindesay , thank you for your help! :+1: \n. @parashar, Unrelated to this, but if you liked this library, I really appreciate if you can give me a feedback on this small addition: pg-monitor\n. As far as I know, Postgres deals with JSON as strings, so there is no need for any special JSON support. So yes, you can just do JSON.stringify() and then inject it as a parameter.\nI will add another example in Learn by Example, to cover JSON usage.\n. I was curious about JSON myself, so I did a full test:\nCreated a new table with JSON type and tried to insert and retrieve data, everything works exactly as expected.\nI just have extended the Learn by Example tutorial to include INSERT JSON example.\nI think it is now safe to close the issue. It was a very good question though :+1: \n. @gniquil , just so you know, release 0.9.8 added native JSON support, among other things, so you no longer need to do manual serialization with JSON.stringify yourself.\nINSERT JSON example has been updated accordingly.\n. @gniquil , Unrelated to this, but if you liked this library, I really appreciate if you can give me a feedback on this small addition: pg-monitor\n. @mdvorak \n\nThat is needed mostly to get rowCount property\n\nBy that do you mean only rowCount? because I can't find any other useful information in that object that anyone might want.\nP.S. Good find, by the way :+1: \n. I think this is a serious one, and I'd like to discuss it where others can see it easily, and possibly contribute, as bug #17 \nPlease post all your ideas there. In the meantime, I am thinking of how to resolve it in the best way, without a breaking change, ideally.\n. @mdvorak , please post all replies in #17 . I'm temporarily closing this one here ;)\n. I'd like to see that no other fix is possible in the meantime, before opting for a breaking change. I'll try to come up with some ideas....\n. Method query already has all the information, it just needs to return the extra result. This can be achieved either through an extra option added to the method or another method name. I would't call it anything specific to update/insert/delete, because it should remain generic.\nSomething like queryCount?\n. You know, I think I have a better idea...\nHow about, we add method called something like raw and have it resolve with the entire result object as returned from pg? This would be your passthrough method.\nThe more I think about it, the more I like it, except perhaps for the method name :)\nThinking of a better name, please throw in some of your own ;)\n- direct\n- getAll\n- get\n- rawData\n- rawQuery\n- result\n- getResult\nRight now I think the one that best fits the architecture logic is rawQuery.\n. > I can't think of case where you would need both data and rowCount.\nWell, if you execute something like this:\nUPDATE users SET login = 'myName' RETURNING id\nI just tested it, and I got both. Although now the row.length is the same as rowCount, so probably not so important... I'm just not quite sure...\n. Very good. I'm adding support for queryRaw right now ;) It makes perfect sense. Cheers!\n. I have implemented, tested, and checked in the changes. You can get them from the current index.js in the master branch.\nNew method queryRaw(query, values) will resolve with the PG's result.\nThere will be some documentation, and perhaps extra tests added before I release it as 1.0.3 (it's not a breaking change).\n. Version 1.0.3 has been released. There were some changes in the source after all, to fix an issue found after adding a few good tests, which is always nice.\nI will be adding an example to the Learn by Example tutorial shortly, but in the meantime the issue is closed.\nThank you for your help! :+1: \n. Raw Result example has been added to the Learn by Example tutorial ;)\n. @mdvorak,  Unrelated to this, but if you liked this library, I really appreciate if you can give me a feedback on this small addition: pg-monitor\n. @mdvorak reflecting on some of the passed changes, added method result in Release 1.7.4 as another alias to queryRaw, because this is what it should have been called.\n. Where is this link declared?\nOn 10 December 2015 at 16:13, Mikhail Osher notifications@github.com\nwrote:\n\nhttps://github.com/vitaly-t/pg-promise/wiki/Learn-by-Example#affected-rows\nbroken. could you please update wiki?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/17#issuecomment-163673747.\n. @miraage The link has been updated: Raw Result\n\nCheers!\n. Implemented in 1.0.4\nAny issue in function parameter formatting will now be rejected gracefully and reported globally, the same as for regular queries.\n. What carries a semantic meaning here is that calling func implies that:\n1. we inject all passed parameters into the function as CSV;\n2. we execute select * from funcName(params).\nCalling exec implies none of that, so cannot be considered a semantic alias.\nAnd what exec does is already implemented by method query. And if you like calling it exec instead of query - just add it to your application; event extend allows you to do that easily:\njavascript\nvar options = {\n    extend: function (obj) {\n        obj.exec = function (query, values, qrm) {\n            return obj.query(query, values, qrm);\n        }\n    }\n};\nBut of course you can provide any implementation for your alias functions. For example, you can return queryRaw, or whatever else you want. Just keep in mind that a well-written code should rely on result-specific methods, like one, oneOrNone, many, any, none, etc., all of which execute query with predefined qrm (Query Result Mask).\n. Closing the issue, as there was no follow-up.\n. @paulovieira,  Unrelated to this, but if you liked this library, I really appreciate if you can give me a feedback on this small addition: pg-monitor\n. @paulovieira , I have added another good example of how to make the best out of event extend, by using entity repositories. See Extend Event\n. @paulovieira , thank you! Unfortunately it hasn't received the attention it deserves. It is the component that gives you the perfect picture of how your app is interacting with the database, and what is going on exactly in terms of the queries.\n. @paulovieira, this is a very good idea! I might do it later indeed. For the time being though, pg-promise is far more mature a product than pg-monitor, even though the latter has been vigorously tested.\n. @vastus exactly as shown in chapter Shared Connections, by calling method done() in the end.\nBut if you just start using the library, to let you know that you hardly will ever need to do that, becase the library is using PG's application pool, releasing the connection back to it after each query.\nAnd Learn by Example tutorial is a good place to start, once you got it all up and running ;)\n. Pardon, but your question was about how to release a client connection to the application pool, and now you are asking about how to shut-down the application pool. These are two different things :)\nThe second one is explained in chapter Library De-initialization\nPlease clarify the trouble you are having. Are you saying that you're making that call to shut down the pool and it doesn't work? If so - post you code sample here, we'll see what's wrong with it ;)\nI think I'm beginning to understand your questions :) You executed that example, as a demo, and your nodejs process hangs for 30 seconds after that - correct? It is supposed to happen, because that example doesn't show how to shut-down the application pool, to avoid confusion.\n. If I got you right, here's the same example, but one that also terminates PG's application pool in the end:\njavascript\nvar sco; // shared connection object;\ndb.connect()\n    .then(function(obj){\n        sco = obj; // save the connection object;\n        // find active users created before today:\n        return sco.query(\"select * from users where active=$1 and created < $2::date\",\n            [true, new Date()]);\n    })\n    .then(function(data){\n        console.log(data); // display all the user details;\n    }, function(reason){\n        console.log(reason); // display reason why the call failed;\n    })\n    .done(function(){\n        if(sco){\n            sco.done(); // release the connection, if it was successful;\n        }\n        pgp.end(); // End of application, so shutting down the PG's connection pool;\n    });\nI didn't want to include pgp.end() in the demo so people wouldn't think that they have to do that in the end of each connection. It's only used when exiting the application, if ever, since mostly we just kill the process :)\n. try the example I did just above, that should work nicely ;)\n. Any test conducted with functions would return multiple records, based on a table, so columns are provided automatically.\nTherefore, could you, please clarify your question, preferably with an example?\n. This looks like a very special case. Why not just execute a generic query then?\njavascript\ndb.query(\"select * from test() as f(a int, b int)\");\nand if you need to pass the function a set of parameters, you can do it like this:\njavascript\ndb.query(\"select * from test($1,$2,$3) as f(a int, b int)\", [1,2,3]);\nor like this:\njavascript\ndb.query(\"select * from test($1^) as f(a int, b int)\", pgp.as.csv([1,2,3]));\nI can't think of a good way to convert javascript to that list of aliases right now, and not sure if there is a point in doing that, given how unique the situation is.\n. After having played with the function you provided, I'd like to add the following...\nYou do not need to add any letters in front of the columns specification, i.e. the following syntax will suffice:\njavascript\ndb.query(\"select * from test() as (a int, b int)\");\nAnd when we call a function we do not have any information regarding the expected column names or their types, which means you would have to specify them all on the client side separately, and since it cannot be done automatically, there is no convenience in doing that at all, and thus no point, really.\nI will stay with my original recommendation of using regular queries for such function calls, as posted earlier.\nIf this answers your question, please close the issue.\n. I won't get a chance to look at it till tomorrow, but it looks like definitely an issue in pg, as pg-promise doesn't interpret that kind of result, just forwards what pg sends back.\n. I have logged this bug against node-postgres: https://github.com/brianc/node-postgres/issues/781\nYou can try and chase its author for a quick fix, but as far as pg-promise goes, it does no post-processing for the data returned from node-postgres, as was stated earlier. So, perhaps the issue should be closed here.\nIn the meantime, you can either use the following postgres workaround :\nsql\nselect moneys::numeric[] from MoneyArrayTest\nor when the data arrives:\njavascript\nvar data = '{$2.50,$1.99,$200.00}';\nvar moneys = data.replace(/[\\$\\{}]/g, '').split(',');\nmoneys.map(function (val, idx) {\n    moneys[idx] = parseFloat(val);\n});\n// moneys = [ 2.5, 1.99, 200 ]\n. Anything related to the issue will be posted at: https://github.com/brianc/node-postgres/issues/781\nAnd in the meantime, I am closing the issue here, as not related to pg-promise.\n. This issue won't be addressed by node-postgres, it seems, because of the locale involved in this.\nAnd it seems that the proper way to do it is via node-pg-types, like this:\njavascript\npgp.pg.types.setTypeParser(791, parseMoneyArray);\nwhere parseMoneyArray is your own function for converting the array of money, and 791 is the code for array of money.\nWhen I tried to submit a PR for this, I had the following code working for me:\n``` javascript\nvar parseMoneyArray = function (val) {\n    if (!val) {\n        return null;\n    }\n    var p = arrayParser.create(val, function (entry) {\n        if (entry !== null) {\n            entry = parseFloat(entry.replace('$', ''));\n        }\n        return entry;\n    });\nreturn p.parse();\n\n};\n```\nBut it was turned down, because it will only work for $ currency, which is configurable.\nAnd since you are doing it on your own side, you'd have to use the arrayParser.\nSorry, I wouldn't go into more detail, but really, this is an issue for node-postgres, not this library ;)\n. @Aysnine This got nothing to do with the issue, where arrays of money were discussed, specifically.\n. pg-promise doesn't control node-postgres settings, and if you need to access those, the library exposes property pgp.pg - instance of the node-postgres library that's used.\nSo, for example, to change the pool size, you can do:\njavascript\npgp.pg.defaults.poolSize = 20;\nNote that normally you do not need to change the pool size, using the automated connections provided by pg-promise.\nAs for the connection timeout, I'm not sure if node-postgres supports that, and if it does, this question should be addressed to node-postgres.\n. ES6 specification uses back-tick symbols ` to wrap text templates, to avoid conflicts with regular text wrapped in single or double-quotes.\nAnd in your example you do not use any of that. Or, did you not format your message correctly? If you want to type back-tick symbol here use slash+back-tick: `\n. It's actually like this:\njavascript\nvar test = \"test\";\nconsole.log(`this is a ${test}`);\nwhile for pg-promise you would use:\njavascript\nvar test = \"test\";\nconsole.log(\"this is a ${test}\");\n. Do I understand you correctly what you are saying? - \nYou want to join ES6 syntax with pg-promise named variables just for the sake of multi-line support?\nThat would produce a conflict, alright. I'm just not sure how much of a problem it really is right now...\n. Perhaps, but you are talking about a very new standard here, and with specific to multi-line usage.\nIf it really becomes an issue, it would be very easy to change. I can mark it as a feature request.\nAre you personally developing for ES6 right now or is it just a hypothetical?\n. It can be made backward compatible, through an extra option to override the prefix symbol explicitely with one that's ok for ES6, like ? or : - will have to decide.\nAlternatively, we could detect ES6 support and change it automatically, although this would be breaking when changing the environment from ES5 to ES6 for an existing project.\n. Anyway, let's consider this an improvement from now on. Feel free throwing in suggestions for the best way to change it, in terms of the syntax. I will try to get around implementing it this weekend, if we can make a decision.\n. The change will perhaps be only in the front symbol, from $ to something else, so the parsing speed and logic will be exactly the same. I just need to figure out what would be the best alternative symbol to use :)\n%&?:*\nAlternatively, I could change it to $() or $[] or $/ / what do you reckon? ;)\nNow I think that keeping $ in front is a good idea, because we still have syntax `$1, $2,... for simple variables, which is compatible with ES6.\n. I believe i can easily add support for $() in addition, as opposed to instead of. In fact, i think this will be the best solution, for all means and purposes.\n. I have already checked in all the changes to support both ${} syntax and $() at the same time. You can see that the change was minimal.\nThere will be a few updates in documentation and tests though before I do a release with it. Sometime before Monday, I hope.\n. This issue has been resolved in the new 1.2.0 release.\n. @pkoretic you may like the syntax update in release 1.3.1 ;)\n. In essence, this library is more specific and much simpler than knexjs, no learning curve, because it just joins node-postgres library with the standard promises, without building much of a custom protocol on top of that, like ORM-s do.\nAlso, consider this - the more protocol you try to build on top of the database logic, the more bugs you are likely to end up with.\nAnd yes, it has been production ready, the tests offer a very good coverage, with 392 checks at the moment - plenty for such a small library.\n. @waynehoover , I have added an introductory note in WiKi.\nIf this answers your questions, please close the issue, and if not - please let me know ;)\n. It is either one:\n- a connection object\n- a connection string\nThe library just passes it on to node-postgres without any pre-processing, except checking for it not being empty.\n. What really happens here:\nStep-1: $1 is replaced with the specified string;\nStep-2: $2 is replaced with the specified parameter.\nAnd since the result of Step-1 is another $2 introduced, it is replaced again in Step-2.\nThis is a very special case, one might consider to be a feature, but in a practical case, more like a bug indeed. I really should change it.\nThank you for reporting this.\n. @ZeusIV The current code in the master branch already has this issue fixed. I will finalize some other changes first, and then release it as 1.5.0 ;)\n. This issue has been resolved in Release 1.5.0\n@ZeusIV once again, thank you for finding and reporting this.\n. @macprog-guy which version of the library are you using? There was a change in 1.5.0 related to the same issue #27.\nPlease let me know if you can see the issue in the current version 1.5.1.\n. @macprog-guy that's ok. I did a test in the meantime against the current version 1.5.1 and I got the expected result.\nThank you for your support! :)\n. Hi Zeus,\n\nI love this library\n\nAnd you haven't even given it a Like :) \n\nlooks like it limits replacing to only 5 params\n\nIt looks or it does? It passes tests with any number of variables, so it isn't relevant what it looks like. And 1-5 in the reg-ex refers to the number of variations for open-close symbols, it got nothing to do with the number of variables being formatted.\nI can't replicate the issue you described. Could you, please, give one simple example where formatting breaks?\nAlso, regarding strings like this:\njavascript\nvar text = 'update username=\"$1\" where id=$2 and email=\"$3\" and addr1=\"$4\" and addr2=\"$5\" and addr3=\"$6\" and addr4=\"$7\" and addr5=\"$8\"';\nYou should not put double-quote symbols around variables, the formatting inserts single-quote symbols around strings automatically as it should.\n\np.s. the nice thing about pg is that you can call a query with an object \n\nWhy do you need to pass it an object?\nAnd if you want your own method, you can always use extend event to extend the interface. In your example this sounds like:\njavascript\nvar options = {\n    extend: function (e) {\n        e.objectQuery = function (obj, qrm) {\n            return e.query(obj.query, obj.values, qrm);\n        }\n    }\n};\nP.S. If you think you like PG's query formatting better, then this is what you should be using, instead of pg-promise query formatting. This is why option pgFormatting has been there from day one.\n. @ZeusIV this is confusing, you are saying that you are using pgFormatting = true, but then what issues are you having there?\nI'm trying to understand the original problem, if there is still any.\nP.S. Here Like = Star, in the right top corner ;)\n. @ZeusIV please either provide details about the issue or close it.\n. The last check-in makes sure that when pgFormatting is true, parameter query is never verified, and passed into pg as it is.\n. Resolved in release 1.6.5.\n. Version 1.4.3 of the library is quite out of date now.\nWhat you are describing looks and sounds exactly like issue #27, which was fixed in version 1.5.0 of the library.\nUpdating the library to the latest should resolve the issue. Please confirm that's the case, and if so,  then close the issue.\n. Since 1.4.3, yes, with 1,706 additions and 1,289 deletions, you could say that :) In fact, the library has been almost rewritten from scratch since 1.4.3, considering the number of changes made.\nBut that's probably normal for a library that had its first draft published about 4 month ago.\nBut the good news, the current version is very solid, no known issues. And most importantly, it has 100% test coverage since 1.6.1.\n. The following line makes no sense:\njavascript\n return t.one(function ( 'INSERT INTO table1(col1) VALUES($1) RETURNING id', [123])\nIn fact, it won't even compile. That's an invalid JavaScript syntax altogether.\nThe rest of it seems fine.\n. Ok, this way it looks fine. What error are you getting?\n. That's not possible, at least of them has got to be executing.\nI just tested this whole piece of code locally, and it worked just fine.\nAdd console.log(\"DATA\", data) and console.log(\"REASON\", reason) in the beginning of the corresponding result sections. One of them has got to be executing there.\nBy the way, your transaction, by design, will resolve with [ undefined, undefined ], which is perhaps not something you would want to return from a service ;)\n. As I mentioned above, the design of your transaction is such, it expects to be resolved with [undefined, undefined], which is what promise.all resolves with in that case.\nIf you need something else, then resolve with the data you need:\njavascript\ndb.tx(function (t) {\n    return t.one('INSERT INTO table1(col1) VALUES($1) RETURNING id', [123])\n        .then(function (data) {\n            return promise.all([\n                t.none('INSERT INTO table2(col1, col2) VALUES($1, $2)', [data.id, \"John\"]),\n                t.none('INSERT INTO table2(col1, col2) VALUES($1, $2)', [data.id, \"Mary\"])\n            ])\n                .then(function () {\n                    return promise.resolve(data.id);\n                });\n        });\n})\n    .then(function (data) {\n        // Success, do something with data...\n        return res.status(200).json(data);\n    }, function (reason) {\n        // Error\n        return res.json(reason);\n    });\nThis is really a question about promises and how they work, not so much about this library, which just uses promises.\n. You are welcome, and thank you for your support! :)\n. P.S. That's because the first draft was published just 4 month ago, while code coverage was added only 3 weeks ago, so it is simply a very young library. So hopefully, yes, it will get noticed later :)\n\nI'm surprised to see so few stars but I'm sure it will gain a lot of traction in the near future.\n. @Maximization just as an update, we found some issues related to the use of promise.all for executing a batch of queries.\n\nSince then the library received a much better, internal implementation for this: batch, which is now the recommended way for executing a batch of queries within tasks and transactions.\n. @BeeDi the information provided here is now obsolete, and is not correct in relation to the current version of the library.\nOne must never use promise.all anymore, only t.batch method, as shown in all examples.\n. > Should I put 'db.tx(function (t)' inside loop?\nNo, you should put the loop inside db.tx. And you should avoid use of .catch and .then like that inside the transaction, you should apply those on the transaction itself.\nAnd you should use a separate array of queries for the internal inserts.\nComplete change:\njs\ndb.tx(function (t) {\n    var queries = [];\n    for (let city in fileContent.Sheets) {\n        if (fileContent.Sheets.hasOwnProperty(city)) { // This kind of check must be executed as good practice\n            let citySheet = fileContent.Sheets[city];\n            var q = t.one(\"insert into common_city(name, state_id, time_zone_id) values($1, $2, $3) returning id\", [city, 5, 2])\n                .then(function (data) {\n                    var nbs = []; // neighbourhoods;\n                    for (let row in citySheet) {\n                        if (citySheet.hasOwnProperty(row) && citySheet[row].hasOwnProperty(\"v\") && citySheet[row].v !== city) {\n                            let neighborhood = citySheet[row].v;\n                            nbs.push(t.none(\"insert into common_neighborhood(city_id, name) values($1, $2)\", [data.id, neighborhood]);\n                        }\n                    }\n                    return t.batch(nbs);\n                });\n            queries.push(q);\n        }\n    }\n    return t.batch(queries);\n})\n    .then(function (data) {\n        // SUCCESS, transaction committed\n    })\n    .catch(function (error) {\n        // ERROR, transaction rolled back\n    });\n. Hard to say, given that pg-copy-streams has non-existing support. According to its build history, it hasn't built successfully for the last year, if ever at all.\nAnd its official branch still doesn't even work with PG > 3.0.0, which makes it of little value.\nA short-version answer would be No, and a full-version requires a library that works, which we do not have.\n. @napalm272 \nUPDATE:\nVersion 2.0 of the library will have support for streams as implemented by libraries node-pg-query-stream and node-pg-copy-streams.\n. This does look like #33, a misunderstanding of how promises work, although with more problems :). \nFor the logic you describe your code should look like this:\njavascript\ndb.tx(function (t) {\n    console.log(\"inserting user:\");\n    return t.one('insert into user (name) values ($1) returning id', [\"example\"])\n        .then(function (user) {\n            console.log(\"inserting pin:\");\n            return t.one('insert into pin (user_id, pin) values ($1, $2) returning id', [user.id, pin])\n                .then(function () {\n                    return promise.resolve(user.id);\n                });\n        });\n})\n    .then(function (data) {\n        console.log(\"SUCCESS\", data); // printing successful transaction output \n    }, function (reason) {\n        console.log(\"FAIL\", reason); // printing the reason why the transaction was rejected \n    });\nWhen you want the transaction to return data, you resolve with that data, you do not just return it.\nAnd you should skip those reject sections, they are provided by default, that's just an anti-pattern.\nLastly, you are making the wrong calls inside the transaction, i.e. into the wrong connection context. You are supposed to be making calls against the transaction context - parameter t as in the example, not the root database context, which sits outside of transaction.\nThere is still a problem there though, I've changed it to resolve with the user's id, but that poses a question as to why your query returns id for the inserted pin, if you are not using it....\n. Use of proper connection context is shown in every transaction example, and resolving data instead of returning it is the very basic principle of using promises, not so much relevant to this library that just uses promises everywhere.\n\nPerhaps you'll consider adding an example of a transaction like this to the documentation. It seems like a relatively common and simple use case that emphasizes key ideas (such as using the proper connection context) :)\n. I haven't, but what would be the advantage of supporting it, you think?\n. Speaking of boost in performance, I've just released v.1.8.0 to support pg-query-stream, which by the way doesn't even work with pg-native, which makes it of less value, I suppose.\n\nI will look at testing against it though, to see if it even works...\nUPDATE\nI have run into a number of issues, trying to switch over to require('pg').native. It doesn't seem to be that simple. I would have to evaluate the issues and the reasons for switching over, to see if it is justified.\nI would consider a PR for this this, if you want to try, but so far I haven't been able to make it work.\nThis also would carry a huge implication to testing. You would need pretty much every test run in two modes - native and non-native, so it would require a major addition to the tests.\n. @andrei-cocorean , ok, if you do, I will re-open the issue, but closing it for the time being.\n. @andrei-cocorean did you have a chance to look at it? I did, and found nothing but trouble, trying to use the native bindings, especially under Windows. Many of those were reported to the author of node-postgres, but every time he came back with the same lame excuse that he doesn't have a Windows machine to fix it. At some point I just stopped trying to use the native bindings, it doesn't work.\nAs a matter of fact, I've been thinking it over more than once - rewriting the entire low-level driver (node-postgres), because I'm not satisfied with how the existing one is supported, but I'm yet to find the time to embark on something this massive. I only know that it works in 99%, without the native bindings. The remaining 1% refers to all the known issues that have been ignored for a long time.\nTo make matters worse, the author of node-postgres recently black-listed my account, his biggest supporter, so I could no longer provide any more support for node-postgres. What an a-hole, truly!\n. @andrei-cocorean \nSupport for Native Bindings has been added in version 3.5.0.\n. oneOrNone is what you are looking for ;) It resolves with null when no record is found, with the row-object when 1 record is found, and will reject when more than one row is found.\nIt is documented also.\n. I'm trying to understand why the following code:\njavascript\nvar t = new Date(\"2015/08/01\");\nconsole.log(t);\nprints Sat Aug 01 2015 00:00:00 GMT+0100 (GMT Daylight Time);\nin the meantime the following code:\njavascript\nvar t = new Date(\"2015/08/01\");\nconsole.log(t.toUTCString());\nprints: Fri, 31 Jul 2015 23:00:00 GMT\nSomething doesn't add up here, and I'm gonna look into that. It's not so much the library issue, rather something in JavaScript's Date conversion...\nFeel free to follow up with any thoughts on this ;)\nUPDATE\nActually, this seems correct, since you do not specify the time, and we use the UTC for the time conversion. And UTC is the recommended Date/Time presentation for Postgres.\nAnd if your database type is timestamptz, then after reading it back the value should be correct.\nBy the way, Date/Time conversion is a known issue in node-postgres, which was raised more than once, but nothing changed there. In pg-promise we use the recommended UTC Date/Time formatting, as readily available within JavaScript.\n. @clicktravel-pavel is there still a question about use of Date/Time?\n. This however gave me an idea for adding support for generic custom-type conversion. The code is finished, will release once the documentation has been updated.\nFor absolutely any type, including the standard ones, like Date and Array, if it has function formatDBType on it, it overrides any other formatting in the library.\nSo, for example, if you want to format Date in your own way, you could do:\njavascript\nDate.prototype.formatDBType = function () {\n    return new Date(Date.UTC(this.getFullYear(), this.getMonth(), this.getDate()));\n};\nor you can return any custom result.\nUPDATE\nRelease 1.9.3 adds Custom Type Formatting which you can use to change the way formatting works for anything you want, including, of course, type Date, as was shown earlier.\n. F.Y.I. Important breaking change in v6.5.0.. You make the wrong call into the library object, not following the Getting Started instructions.\nYour pgp object is set to require(\"pg-promise\"), as opposed to:\njavascript\nrequire(\"pg-promise\")(/*options*/)\nAs a result, you are making a call to initialize the library with options, but instead you are passing it the connection string, which is part of the next step - database initialization.\nAnd as the library expects optional parameter options, giving it a connection string instead results in the error that you are seeing.\n. The library clearly defines it as a two-step process:\n1. Load and initialize the library: var pgp = require('pg-promise')(/*options*/)\n2. Create the database instance: var db = pgp(connection);\nAnd it is everywhere, in all the examples.\nSuch pattern is used in many libraries, I don't know why it seems weird. I will consider amending the manual though.\nYou are the second person asking about it, since ever, so I don't know if there is really an issue there....\n. @tjmcewan Thank you! I wasn't sure how to update the docs to make it more clear. Plus all the examples show the same thing.\nI can't say this is ideal, no, but this is the way it has been implemented from day one, so for the sake of compatibility... The bottom line, once you figure that once, you won't make the same mistake again ;)\nI will consider an update, should I start on version 3.0.0\nCheers!\n. @tjmcewan null / undefined or {} are the defaults.\nI didn't want newcomers to focus on something that is optional, that's why I did it that way...\n. Version 3.0 is out, and this small amendment is there also.\n. Getting Started explicitly says:\njs\n// Loading and initializing the library:\nvar pgp = require('pg-promise')({\n    // Initialization Options\n});\nTwo-step is what it takes to set up universal use of promises throughout the library, unfortunately.\n. I'm gonna look at making the error more intelligent/descriptive.\n. This functionality has been improved in version 3.4.3.\n. Could you please give me an example of the code that does that? Also, which version of the library are you using? The lines have changes recently, there is nothing on line 430 right now to cause any kind of problem.\nI would suggest, upgrade to the latest version (1.9.7), see which line is reported to give the problem, so we are looking at the exact same code, and give your code example that causes the problem ;)\nGut feeling, you are making a query request outside of the valid context, but the library has checks for that to detect and report with a predefined error, which doesn't happen here, so I wonder how you managed it :)\n. @oliversalzburg thank you for getting back with this.\nI do not recall any changes that could have affected the behaviour you described in the beginning since version 1.8.3, but then again, from the current version it is separated by 1,113 additions and 246 deletions, which is so many, even I can't be sure if something hasn't changed in between. We can only hope that the 100% test coverage that the library has is comprehensive enough and trustworthy.\n. @oliversalzburg I was really interested to research this better, trying to understand how this could have happened. So, I went through the entire history of changes between 1.8.3 and 1.9.7, only to become absolutely sure that nothing has changed to affect that kind of behaviour you described.\nThen I realised that the only way this may have happened, is through a theoretical bug within the specific library + version of the promise library that you were using. If you suddenly cannot see the issue after upgrading to 1.9.7, then I'm guessing that along the line you upgraded whatever promise library that you were using, which may in fact be the cause of the problem.\nThe problem itself, as I suspect it, I have 3 places in the code where I first initiate a reject and then immediately invoke an error event notification. The bug would have been if reject had immediately triggered the rejection call chain, which would have been invalid under the promise standard, and break the logic of the notification, i.e. the notification would attempt to use the connection details that was released prematurely.\nStrictly speaking, in this library we do not care whether we trigger the reject first and then provide a notification or vice-versa. So, for the sake of making it more robust, and protecting against such hypothetical promise glitch, I've just swapped the sequence in those 3 places, to provide a notification first, and only then trigger the reject.\nYou can see the code just checked in, labelled improved safety of the error notification, and line 618 is the theoretical culprit in your case.\nI appreciate if you try to track back your recent update and let me know whether what I was suspecting is true. And if you manage to reproduce the issue by restoring your version of the promise library, then you can manually swap those lines yourself, the reject and the notification to see if that removes the issue, so then we know if the problem was indeed in the promise library.\n. Reopening the issue, to keep track of it ;)\n. This is probably for the better, makes it easier to track the exact issue. If you can reproduce it, and my last check-in fixes it, then we will know what the issue is and have the fix ready.\n. Have you not tried the updated code as per my previous posts? I gave you more than a suggestion, you didn't follow up on that.\nAlso, again - what promise library are you using (name, version)?\nAnd can you give your transaction code example that fails?\n. @oliversalzburg since there was no update from you on this issue, there was another release 1.9.8 with the changes that address this issue, since I was almost certain why this happened. You can see all the details in the release notes.\nPlease let us know if the issue is gone using the newest version.\n. That's quite unexpected, because the only way that problem could be understood is through the break in the reject chain. Nothing else is logically possible there.\nOk, you can send your code to vitaly.tomilov@gmail.com, I'll have a look at it, provided you do not mind if I publish pieces of it here.\n. Yes, but you see, in 1.9.8 I swapped those two in places, so it invokes the callback first, and only then rejects, so it is impossible to lose the context for the callback. That I can't understand.\n. That issue related to the connection, while your error stack was pointing at the code inside the query execution, which means the connection was established successfully. Has the behaviour in your code changed somehow since then?\n. Thanx for the code, there is so much of it, I'd like to know which piece is actually failing.\nI won't be able to finish my analysis today, unfortunately, will do it tomorrow. Let me know if you find something yourself in the meantime.\nSo for now will just leave with a tip that you seem to overuse transactions a lot there, in places where they are not needed at all, like one or more select statements. Transactions only make sense when there is at least one INSERT or UPDATE involved.\nWhen it's just one query, you should just return the query result. And when it's a sequence of selects, Tasks should be used.\n. Also, since you are wisely tagging transactions (with name), this should help a lot analysing what's going on, if you connect pg-monitor. Try it, see if it gives you a better picture of what is happening.\n. You can configure it easily to log only certain things, like tasks and transactions:\njavascript\nmonitor.attach(options, ['task', 'transact']);\nAnd for debug logging, it has a very useful log event where you can decide also what you want logged into a file ;)\nThis is all just to narrow down the problem ;)\n. When I execute the code above, I'm always getting a reject from the transaction, with message No data returned from the query., which is a proper result.\nHere's how I'm running it locally:\n``` javascript\ndb.tx(function tx(transaction) {\n    return Promise.all([foo(), fails()]);\nfunction foo() {\n    return succeeds()\n        .then(succeeds)\n        .then(fails);\n\n    function succeeds() {\n        return transaction.one(\"SELECT 1 AS id\");\n    }\n}\n\nfunction fails() {\n    return transaction.one(\"SELECT login FROM users WHERE(FALSE)\");\n}\n\n})\n    .then(function (data) {\n        console.log(\"SUCCESS:\", data);\n    }, function (reason) {\n        console.log(\"FAILED:\", reason);\n    });\n```\nWithout being able to reproduce the problem it is hard to make any assumption.\n. I am beginning to understand why this may occur, after looking at the log file that you sent me, specifically the following lines:\n2015-09-07 19:50:00.566 [DEBUG ] (postgres.js) tx(person.createWith): rollback\n2015-09-07 19:50:00.568 [ERROR ] (postgres.js) error occurred on database transaction\n2015-09-07 19:50:00.568 [ERROR ] (postgres.js) SELECT id FROM \"fairDay\" WHERE (\"fairDay\".\"date\" = '2015-09-06T22:00:00.000Z')\n2015-09-07 19:50:00.571 [DEBUG ] (postgres.js) tx(person.createWith): INSERT INTO \"personTags\" (\"person\", \"tag\") VALUES ('fccc3053-9666-40dd-8732-824cffc6a5c7', '44f835e9-c0b6-46f3-8ac3-865d1de24355')\nYou manage to execute requests against a transaction after the transaction executes rollback. This is not supposed to happen, the transaction logic doesn't allow that. And the only way to make this happen is by creating loose requests against the transaction context. A loose request is any request created against transaction context that's not part of the transaction result chain.\nExample:\njavascript\ndb.tx(function tx(t) {\n    t.one(\"select login from users where(false)\"); // loose request;\n    return promise.reject(\"finished!\");\n})\n    .then(function (data) {\n        console.log(\"SUCCESS:\", data);\n    }, function (reason) {\n        console.log(\"FAILED:\", reason);\n    });\nAny loose request will break the transaction, naturally, because the transaction has no control over the execution of loose requests. It is generally a broken promise logic.\nYou need to check your code where you may be creating a loose request against transaction context.\nIn the meantime, I will consider adding detection for loose queries to the library, if it is possible.\nUPDATE\nLoose requests are quite hard to deal with in the promise architecture due to asynchronous code. The thing is, pg-promise already uses detection of out-of-context calls:\njavascript\nquery: function (query, values, qrm) {\n    if (!ctx.db) {\n        throw new Error(\"Cannot execute a query on a disconnected client.\");\n    }\n   return $query(ctx, query, values, qrm);\n},\nand\njavascript\n// generic query method;\nthis.query = function (query, values, qrm) {\n    if (!ctx.db) {\n        throw new Error(\"Unexpected call outside of \" + (isTX ? \"transaction.\" : \"task.\"));\n    }\n    return $query(ctx, query, values, qrm);\n};\nBut these are early-stage detections, not in case it happens while executing the query. I'm looking at how to extend the query itself to provide late detection for loose requests.\n. I have released version 1.9.9 that addresses the issue with loose request detection. See my release notes.\nUsing this version you should be able to see which query is made outside of the connection context.\nThis should get us much closer to solving this problem.\nUPDATE: I've updated the error message to be Loose request outside an expired connection. for the next release.\nAre you logging error event when using pg-monitor? If not, then you really should, it will show the complete error context.\nIt is now your turn to provide an update ;) I will be waiting ;)\n. A had a moment to look through your code again, to search for loose calls. I have found one obvious loose call:\n``` javascript\nPersonManager.prototype.txCreateWith = function PersonManager$txCreateWith( transaction, data ) {\n    var self = this;\ntransaction.any( \"SET TRANSACTION ISOLATION LEVEL SERIALIZABLE\" );\nreturn self.txCreate( transaction )\n    .then( updateRecord );\n\nfunction updateRecord( recordId ) {\n    return self.txUpdate( transaction, recordId.id, data );\n}\n\n};\n```\nYou cannot just call this:\njavascript\ntransaction.any( \"SET TRANSACTION ISOLATION LEVEL SERIALIZABLE\" );\nand not provide .then logic for it. This would result in the exact issue you are having. This is the classic recipe for a problem, which would work in most cases, and break under a heavy load.\nThat's the only one that's obvious. You may have more, if you have a function that returns a promise object, which you call somewhere without using .then on the result. It's just this kind of case is too difficult to spot in someone's code. I believe the new version though will point to such cases better than before. You will be able to see which queries are the result of a loose call.\n. When event error is reported, it has the query as part of the event, so you can see which query exactly causes the problem. Once you find it, you need to track all possible ways in which it is executed, and find the execution path where there is no .then following the query, which would constitute a loose promise call. That's basically it.\nThis is perhaps more of a promise-related issue than anything else.\nAs for the difference between your home and work setup, it is not that surprising because loose promise calls are always dependent on race conditions, and especially in a true asynchronous code.\nYou see, while V8 engine executes javascript as a single synchronous process, the IO is processed by Node JS asynchronously, which is its strength, and since queries are going through TCP-IP, the otherwise pseudo-synchronous promise requests become truly synchronous, and as such, open to all kinds of race condition conflicts.\n. What I can't understand from your log is the following 3 records:\n2015-09-08 10:08:32.477 [DEBUG ] (       postgres.js) tx(tx): rollback\n2015-09-08 10:08:32.478 [DEBUG ] (       postgres.js) tx(tx): SELECT id FROM person WHERE(FALSE)\n2015-09-08 10:08:32.478 [DEBUG ] (       postgres.js) tx(tx)/endduration: 9, success: false\nIt says that the transaction has finished in error, so it is rolled back, and then somehow before the transaction ends it manages to execute a SELECT. If you look at the transaction implementation, this is impossible, unless that call is loose, in which case it can still execute at an unpredictable time, i.e. it can happen just before the transaction disconnects or right after - the latter will be caught by the latest code. You must be making a loose call for SELECT id FROM person WHERE(FALSE) while inside a transaction.\n. You see, rollback can only execute after invoke() with all your transaction code has rejected. But according to your log, there is a request inside your transaction that hasn't finished execution after invoke() has rejected. That's not possible, unless the request is a loose promise object.\nUPDATE: Actually, if you are using promise.all in your transaction, then I think it might. The thing about promise.all, it will execute all requests, in no particular order, and reject when the first reject happens, while trying to execute the rest. However, the rest should be ignored as executed outside on rollback.\n. Why in your function foo() do you execute this logic? - if succeeds() resolves, then execute succeeds() again. That's what it does there ;)\nAnd was there any change with the error since your last change?\n. Does this piece of code show the problem still? It's design suggests that it shouldn't.\n. I'm still unable to reproduce the issue on my PC. Could be a result of a completely different configuration, impossible to know.\nI'm on NodeJS 0.12.7, 64-bit, on Windows 10.\nI remotely understand the issue, which looks like failsSecond is being resolved after the transaction execution, according to your log. But I can't reproduce it here, it always just works correctly.\nI will keep trying to break it :)\n. Ok, I've been able to reproduce it, and I can see what the problem is now...\nPromise.all stops resolving the chain returned by function foo when the promise returned by failsFirst has rejected. The promise chain continues to be resolved however, after promise.all has rejected and the transaction has ended.\nIt became possible to see after a small change in the code:\n``` javascript\n    function foo() {\n        return succeeds()\n            .then(succeeds)\n            .then(function () {\n                return new promise.Promise(function (resolve, reject) {\n                    setTimeout(function () {\n                        resolve();\n                    });\n                });\n            })\n            .then(failsSecond);\n    function succeeds() {\n        return transaction.one( \"SELECT 1 AS id\" );\n    }\n}\n\n```\nThis kind of behaviour suggests that perhaps the last change I made in the library isn't strictly correct. I'm trying now to formulate what it should do instead...\nI didn't expect loose query requests from a valid transaction chain. But in this kind of case they are possible.\n. I've just looked up method settle, and it looks like the way to go indeed for this situation. I just couldn't advise it in general documentation, because that method is outside the generic promise protocol.\nHowever, tell me if I'm wrong, but since 1.9.9, everything works, except you are getting an error logged that should be ignored - correct?\n. Basically, what needs to be changed - do not use promise chains as parameters for promise.all inside a transaction, as those may result in loose query requests.\nIn your case, you can just chain the entire transaction explicitly, without using promise.all at all.\nOr, you can just use settle instead :)\nBack when I was looking first at the issue with loose requests in large transactions I implemented method sequence (see Synchronous Transactions) that's available inside tasks and transactions, which takes care of loose queries. You can use it here also, if you want, it would also solve the problem ;)\nI will contemplate on making changes in the library to cater for the loose requests better.\n. Yeah, I know it quite well, but I needed an implementation that would work with any standard promise library, while reduce is outside the promise standard.\nAlso, reduce is not suitable for massive transactions. It doesn't cater for situations when creating all promises in advance would kill NodeJS process. With sequence you can execute a transaction with over 10 million queries, without using extra memory.\n. Cheers! This has been very helpful. And the library now can anticipate loose requests and reject them correctly. I don't know if there is anything else it can or should do in such cases, but I will think about it. Feel free throwing in your own ideas regarding it.\nIn the meantime, the issue has been resolved, it seems. Closing it.\n. @oliversalzburg \nPlaying around with promise.alla bit more, I found even more disturbing behaviour. Sometimes a loose change request (insert or update) would apply the change when executed after the rollback, which makes this a more dangerous premise. Makes me question how much is promise.all really suitable to transactions in general then.\nsettle on the other hand guarantees a consistent transaction, so does sequence, but they have they own issues: settle is proprietary, exists only in Bluebird, while sequence was designed for something different (massive transactions), and isn't as easy to use.\nRight now I'm thinking... maybe I should extend the library with a method similar to settle, just easier to use, so it would work the same with any promise library.\nWhat do you reckon?\nUPDATE: I'm looking at an idea for it: node-promise-settle\n. I have done lots of research on the subject of a proper promise implementation for transactions, and have come up with the first draft that works exactly the way a transaction needs:\njavascript\n// Joins `all` + `settled` logic for transactions;\nfunction allSettled(inputs) {\n    if (!inputs.length) {\n        return promise.resolve([]);\n    }\n    return new promise.Promise(function (resolve, reject) {\n        var error, success = true, remaining = inputs.length,\n            result = new Array(remaining);\n        inputs.forEach(function (item, i) {\n            item\n                .then(function (data) {\n                    result[i] = data;\n                    check();\n                }, function (reason) {\n                    if (success) {\n                        error = reason;\n                        success = false;\n                    }\n                    check();\n                });\n        });\n        function check() {\n            if (!--remaining) {\n                if (success) {\n                    resolve(result);\n                } else {\n                    reject(error);\n                }\n            }\n        }\n    });\n}\nIt will take a few tests and some documentation before it is ready for release, but now I'm confident that this is the way to go, after all the trouble we've had :)\nAlso, such method will need a good name. Any suggestions? allSettled is precise, but awkward. :)\nUPDATE: In the end I decided to call it batch, because it tries to execute a batch. I didn't want to go with something that resembles existing promise methods like all and settle, because it may cause a confusion. The method doesn't behave exactly the same. Here's the latest code:\njavascript\n    /**\n     * @method batch\n     * @memberof module:pg-promise.Task.prototype\n     * @summary Attempts to resolve every value in the array and return the summary.\n     * @param {array} values - array of the following types:\n     * - a value or an object, to resolve with by default;\n     * - a promise object to be either resolved or rejected;\n     * - a function, to be called with the task/transaction context,\n     *   so it can return a value, an object or a promise.\n     *   If it returns another function, the call will be repeated,\n     *   until the returned type is a value, an object or a promise.\n     *\n     * If parameter is anything other then an array, an error will be thrown:\n     * `Array of values is required for batch execution.`\n     *\n     * @returns {promise} Batch summary:\n     * - resolves with the summary array, if every value in the array resolved;\n     * - rejects with the summary array, if one or more array values rejected.\n     *\n     * Batch summary array is of the same size as the input array of values,\n     * and consists of objects with two properties:\n     * - success: true/false, indicates whether the corresponding value in the\n     *   input array was resolved.\n     * - result: resolved data, if success=true, or else the rejection reason.\n     */\n    this.batch = function (values) {\n        return npm.utils.batch.call(this, values);\n    };\nand implementation:\njavascript\nfunction $batch(values) {\n    if (!Array.isArray(values)) {\n        throw new Error(\"Array of values is required for batch execution.\");\n    }\n    if (!values.length) {\n        return $p.resolve([]);\n    }\n    var self = this; // needed to resolve functions;\n    return $p(function (resolve, reject) {\n        var failed, remaining = values.length,\n            result = new Array(remaining);\n        values.forEach(function (item, i) {\n            while (item instanceof Function) {\n                item = item.call(self, self);\n            }\n            if (isNull(item) || typeof item.then !== 'function') {\n                result[i] = {success: true, result: item};\n                check();\n            } else {\n                item\n                    .then(function (data) {\n                        result[i] = {success: true, result: data};\n                        check();\n                    }, function (reason) {\n                        result[i] = {success: false, result: reason};\n                        failed = true;\n                        check();\n                    });\n            }\n        });\n        function check() {\n            if (!--remaining) {\n                if (failed) {\n                    reject(result);\n                } else {\n                    resolve(result);\n                }\n            }\n        }\n    });\n}\n. That's alright, I'm throwing in some good tests at it now, and finalizing the protocol changes. Will try to do a new release soon with all the new functionality tested and documented.\n. @oliversalzburg I took my time to do it right, to thoroughly test, optimize and document, but now the new release is out.\nIt is a major improvement on top of what was initially contemplated. And while the new method batch resolves with the same result as promise.all, which makes it easy to replace the old calls, it rejects differently - see the release notes.\nI appreciate if you can find time to try it in your code and come back with a feedback here.\nAlso, following your code pattern,as I gather from the code snippets you sent, you should like that this new method can also take functions among array values to be called for getting back the actual value or promise. The functions can even return other functions, i.e. nested references are supported.\n. By the way, I've decided on a new project - spe where I'm moving things like sequence and batch methods, and will be adding more later. They just seem to be too genetic not to :) and quite useful too.\nI'm looking forward to hearing how you got along with the batch method that I added to help with the original problem.\n. @oliversalzburg You never came back to me about the use of method batch. Have you started using it?\n. Ok, here's the full batch API. Please keep me updated on how you're getting along with it ;)\nAlso, be sure to use the latest, version 1.10.5 ;)\nJust so, on your log, when property result is undefined, for your success=true cases, looks like your string serializer is hiding properties that are set to undefined, but the property is there ;) which i'm guessing in those cases the result from database method none, which resolves with undefined ;)\n. I probably should have added this kind of example a while ago :)\n. @oliversalzburg if you are still using this library...\nThis may be of interest to you: Performance Boost ;)\n. But it is exported from the library, into the global namespace. What makes you think that it is not? :)\nDon't you have direct access to the global queryResult from your code already?\n. Yep, that's the way it works, through implicit injection into the global namespace, which a module can do only without use strict.\n. Please follow up, and close the issue, if your question is answered.\n. I did have second thoughts about it myself, but it would be a breaking change. And I haven't made breaking changes in this library since ever, all changes have been incremental.\nI am considering a few breaking changes for version 2.0, but not sure when I will do it yet.\nBy the way, release 1.10.3 at least made queryResult read-only ;)\n. There is a standard way in Node JS to make it explicit that you are accessing a global variable, and that's by using global., example: global.queryResult.many.\nAnd we wouldn't want more than one definition of the same property to avoid ambiguity, I suppose. I will make the change in version 2.0, for sure.\nAnd, you are welcome!\nIt seemed finished in every way, till you raised that issue with promise.all, and I had to go back and write quite a bit more :) But, it was worth it! :) The new batch method is so much better than promise.all now, I'm glad I wrote it, and it was quite interesting also.\n. You are welcome!\nAre we good to close the issue? If so, please close it ;)\n. Just so, you have the complete set of methods that represent those flags, including all possible combinations: none, one, many, oneOrNone, manyOrNone=any ;)\n. P.S. Tip of the day : One of the most powerful features in the library - event extend, you can register whole object repositories for a high-level abstraction of business objects. If you are familiar with .NET, that's like doing a proper DAC (Data Access Component).\n. Your example is a little more complicated than the one I provided for the extend event, using just a simple object. I would probably do it in such a way as to avoid creating a new object every time also.\nTo answer your question, the db object that represents the database is always the same, it is extended only once when it is created.\nBut for calls like tx and task a new context is created every time, so you would end up allocating a new instance of your repository that way. That's why a little optimization here would make sense, especially for the code that heavily relies on tasks and transactions.\nBut I wouldn't be overly concerned about it, considering that 99% of time will go into query executions anyway ;)\n. @oliversalzburg\nFeel free to throw in suggestions here, if you have any: version 2.0 changes\n. @oliversalzburg  Thank you for your feedback! \nYou know, you can increase the pool size like so:\njavascript\npgp.pg.defaults.poolSize = 20;\nsee also: pool size\nAnd you are right, there isn't much for pg-promise to do in this instance.\nAs for the UUID usage for the tasks,...it is a step above what the library does at the moment, which is plenty already. See my notes for 1.9.1 release.\nThe logic that's there now: Check for the tag passed, and if it is not passed, then check for the default value associated with the task (property tag), and if that one not set, then use the function name, and if the function name doesn't exist, only then you get no tag.\nAlso, not to forget, the tag doesn't have to be a text string, you can pass anything. But if you want its name to be recognized and logged by pg-monitor automatically, just add method toString() into your custom tag object, and it will be logged properly ;)\nUPDATE: Almost forgot :) You can set the tag value while inside the task or transaction. They both have property this.ctx, which represents the task/transaction context. And you can do this: this.ctx.tag = value. The only issue, events task/start and tx/start won't log the name properly. After all, it was meant for read-only access initially ;)\n. By the way, all that tagging mechanism was described in detail here: Tags ;)\n. I've spent a weekend contemplating a proper definition of Data Throttling versus Load Balancing for my new library. I appreciate if you can tell me whether it makes good sense to you, and whether or not you agree with the definitions that I came up with. Thank you!!!\n. Thank you! It is good having a third-person perspective. I will try to improve on the definitions.\n. @oliversalzburg Interesting test: https://github.com/vitaly-t/spex/blob/master/docs/concept/sequencing.md\nI can't fathom how a sequence built on promises manages to outperform one that's calculated directly, but it is what's happening. Any idea? :)\n. @oliversalzburg yeah, it got me off guard, I can't understand this myself: http://stackoverflow.com/questions/32851905/promise-outperforms-direct-calculation\nNot that I'm not happy about it, more like the other way round, I just want to understand... :)\n. Cheers! :)\n. @oliversalzburg Just to poke you about the update - version 2.0 of the library has been released ;)\nLooking forward to hearing how you get along with the new version, if you decide to upgrade ;)\n. @oliversalzburg From what I can read in reduce documentation, it is a more specific function, while sequence is more generic.\n. @oliversalzburg Did you get a chance to migrate to the latest 2.x version?\nOne of the important things - nested transactions started working correctly only since version 2.2.x, in case you are planning to use those ;)\n. @oliversalzburg cool! 2.3 is out! :)\n. @oliversalzburg What do you think about this thing I just started?\nIt is already fully functional, by the way. I got tired of the almost non-existing support by the original author, who also banned my account, so I can no longer provide any support for users :-1: \nOnce it is all finished, I am considering to change pg-promise over to use this one instead.\n. @oliversalzburg \npg-promise 3.0 is now available. Please let me know what you think of this whole idea...\n. Thank you for coming back!\nAbout node-postgres, I believe the support provided there is awful, practically none in fact. I have been providing support there a lot to everybody, till about June this year when the author decided to ban my account, as a thanks for all my support. And when I asked him why he did it, no response ever came. This is one really bad thing here on Github when the owner is allowed to ban users without any reason.\nI'm shocked about the issue with transactions you describe. I've never seen those myself, yet. I would strongly suggest to use tasks for your read-only transactions, do not over-use the transactions. Other than that, the locking needs some research to see where the problem really lies and how to approach  fixing this.\nIf you can make time to investigate it further and get some results - please log it as a separate issue, and I will be sure to get into it.\nAnd pg-core does have the same interface. I have described the only differences right on the main page. Other than that it is all original code, just being re-factored for the better. Also it was packaged in the way that makes sense.\n. I would be interested to isolate the issue to the PostgeSQL level. I don't think that promises or lower-level drivers that we use are the reason. I am looking at the use of SERIALIZABLE and transaction isolation, but it would help to be able to replicate the issue right from pgAdmin UI.\nThere are also explicit locks to consider: http://www.postgresql.org/docs/9.4/static/explicit-locking.html\nThis is not an easy thing to investigate, but worth doing.\nAs before, I was suggesting to put all you can find that's relevant and open a separate issue for it.\n. @oliversalzburg have you made any progress investigating PostgreSQL transactions? It seems like a very important topic, given that you are running in transaction lock-ups. I really would like to know how to avoid it, and if possible, improve the library to that end. If you do have something to share, please open a new issue for that.\n. @oliversalzburg since you never replied... I have done some of my own research in this area, and added a new feature to the library with 2.5.0 release - Configurable Transactions.\n. @oliversalzburg About the first issue, how large a pool size are you using?\nAs for the second issue, have you seen this one? - 13.2.2. Repeatable Read Isolation Level\nIt explains the exact issue you are having and how to overcome it.\nAnd if you want to repeat a transaction in those cases, why not extend the protocol with a corresponding method, something like this - \n``` js\nvar options = {\n    extend: function (obj) {\n        obj.txLoop = function (p1, p2) {\n            function loop() {\n                return obj.tx(p1, p2)\n                    .then(function (data) {\n                        return data;\n                    })\n                    .catch(function (reason) {\n                        if (typeof reason === 'string' && reason.indexOf('could not serialize access due to concurrent update') >= 0) {\n                            // must repeat the attempt, consider an optional delay;\n                            return loop();\n                        } else {\n                            return promise.reject(reason);\n                        }\n                    });\n            }\n        return loop();\n    };\n}\n\n};\n```\nSee also Transaction Snapshots, as I mentioned it in Configurable Transactions ;)\n. > We have to repeat them ourselves. We don't even want to run into that situation. Who knows how often we have to retry them until they succeed? What do we do with the request that caused the transaction in the meanwhile? The situation must be avoided IMHO.\nI believe that to solve this is to consider: the request data, the changes needed in the database, the database structure, and transactions in the last. In all, this is an architectural solution, based on good understanding of the system and the requirements. I wouldn't expect a generic solution for this. This is the nitty-gritty of the software architecture.\n\nWhat would be great if \"the system\" could identify a transaction that would cause a conflict and queue it. Likewise, it should be able to identify transactions that won't cause conflicts and just send them out.\n\nHowever this is approached, it would be way outside of what this library can offer, it is a whole different level of problems. This library can only offer the low-level facility as supported by PostgreSQL. The rest is in your hands.\nP.S. Isn't programming fun? :)\n. @oliversalzburg just a heads-up, I'm biting the bullet on support for generators, planning to release 2.6 today with the initial generators support ;)\n. Are you using NodeJS prior to 4.x?\n. I see. Well, the good news is, generators within pg-promise are, of course, optional, which means you can write code with and without them at the same time. I'm publishing it is within minutes ;)\n. @oliversalzburg the new release with support for ES6 generators is out! :)\nCheck out for details: Generators.\n. @oliversalzburg if you have time later, I have published a blog on this: PostgreSQL with ES6 generators\n. The way that raw-text formatting works, as opposed to default variable formatting, it leaves the safety of it in your hands. It only cares that the value is not undefined and not null, because those may have any kind of interpretation as a raw text.\nIf you anticipate symbol ' as part of your filter, you would need to take care of it on your side. This is a fairly unique case after all.\nAnd you have two options here.\nFirst option, you can simply pass your filter value into a function that will fix ' for you. Here's an example of such function, as it is used inside the library itself when fixing quotes:\njavascript\n////////////////////////////////////////////\n// Replaces each single-quote symbol '\n// with two, for PostgreSQL compliance.\nfunction fixQuotes(text) {\n    return text.replace(/'/g, \"''\");\n}\nonce parsed through such function, you would use the ^ with your variable to indicate that you want raw-text insertion.\nAnother option - create your own type that represents a filter, as per Custom Type Formatting. The latter is more elegant, because you can just pass the object, and you don't even need to use ^ in the query, i.e. your custom type can decide whether it should be used as a raw-text variable or a regular one (the default).\nThis is what your type would look like:\njavascript\nfunction MyFilter(value) {\n    this.value = value;\n    this._rawDBType = true; // force raw-text output;\n    this.formatDBType = function () {\n        return \"'%\" + this.value.replace(/'/g, \"''\") + \"%'\";\n    }\n}\nAnd then you can use it like this...\njavascript\nvar filter = new MyFilter(\"Joe's\");\ndb.manyOrNone(\"select * from items where name like $1\", filter);\nP.S. To test your custom formatting quickly (avoid messing with the database), use pgp.as.format(query, values), as it is the one that query formatting uses ;)\n. Please follow up, and close the issue, if your question is answered.\n. Closing the question due to inactivity.\n. UPDATE\nThe approach suggested here is no longer necessary, since version 3.4.0, which added support for Open Values, using which we can write complex filters right in-line, without an additional type.\njs\ndb.manyOrNone(\"select * from items where name like '%$1#%'\", \"Joe's\")\nvariable $1# will properly escape the text, but won't add surrounding quotes.\n. F.Y.I. Important breaking change in v6.5.0.. I do not have access to a Russian keyboard at the moment. Will you be ok if I reply in English?\n. What you are executing looks like a transaction, except that you are not executing it as a transaction, which you absolutely should, because you are changing data. See transaction examples.\nThat's also why you are seeing constant connect/disconnect, as those refer to acquiring and releasing the connection from the pool, accordingly, which is normal for single independent requests.\nUse transactions when executing a chain containing data-changing queries, and use tasks when executing a chain of read-only requests.\n. \u043e\u043a, \u044f \u0441\u0435\u0439\u0447\u0430\u0441 \u0434\u043e\u0431\u0440\u0430\u043b\u0441\u044f \u0434\u043e \u0440\u0443\u0441\u0441\u043a\u043e\u0439 \u043a\u043b\u0430\u0432\u0438\u0430\u0442\u0443\u0440\u044b, \u0442\u0430\u043a-\u0447\u0442\u043e \u043f\u043e\u044f\u0441\u043d\u044e \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c...\n\u0421\u043f\u0435\u0440\u0432\u0430 \u043f\u043e \u043f\u043e\u0432\u043e\u0434\u0443 \u0441\u043e\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0439. \u0421\u043e\u0431\u044b\u0442\u0438\u044f connect/disconnect \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0442 \u0432\u0438\u0440\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0435 \u0441\u043e\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435 \u0440\u0430\u0431\u043e\u0442\u044b \u0441 \u043f\u0443\u043b\u043e\u043c \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438\u0445 \u0441\u043e\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 node-postgres. \u041e\u043d\u0438 \u043d\u0435 \u0438\u043c\u0435\u044e\u0442 \u043f\u0440\u044f\u043c\u043e\u0433\u043e \u043e\u0442\u043d\u043e\u0448\u0435\u043d\u0438\u044f \u043a \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u043e\u043c\u0443 \u0441\u043e\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044e, \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u043a\u0430\u043a \u043f\u0440\u0430\u0432\u0438\u043b\u043e \u0441\u043e\u0437\u0434\u0430\u0435\u0442\u0441\u044f \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u0438\u043d \u0440\u0430\u0437 \u043f\u0440\u0438 \u043f\u0435\u0440\u0432\u043e\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u0435. \u0425\u043e\u0442\u044f \u0435\u0441\u043b\u0438 \u0432\u0430\u0448\u0435 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0434\u043e\u043b\u0433\u043e \u0431\u0435\u0437\u0434\u0435\u0439\u0441\u0442\u0432\u0443\u0435\u0442 \u0442\u043e \u043f\u0443\u043b \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u0438 \u043e\u0442\u0441\u043e\u0435\u0434\u0435\u043d\u0438\u0442\u0441\u044f (\u0447\u0435\u0440\u0435\u0437 30 \u0441\u0435\u043a\u0443\u043d\u0434 \u043f\u043e \u0443\u043c\u043e\u043b\u0447\u0430\u043d\u0438\u044e - \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 poolIdleTimeout).\n\u0422\u043e \u0447\u0442\u043e \u044f \u043f\u044b\u0442\u0430\u043b\u0441\u044f \u043e\u0431\u044c\u044f\u0441\u043d\u0438\u0442\u044c \u0434\u043e \u044d\u0442\u043e\u0433\u043e, \u043d\u0430 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c, \u0447\u0442\u043e \u043f\u0440\u0438 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0435 \u0446\u0435\u043f\u043e\u0447\u0435\u043a \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u0437\u0430\u043f\u0440\u043e\u0441\u044b \u043d\u0430 \u0447\u0442\u0435\u043d\u0438\u0435 \u0432\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0438\u0445 \u043e\u0444\u043e\u0440\u043c\u043b\u044f\u0442\u044c \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0437\u0430\u0434\u0430\u0447 / task, \u0430 \u0435\u0441\u043b\u0438 \u0432 \u0446\u0435\u043f\u043e\u0447\u043a\u0435 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0437\u0430\u043f\u0440\u043e\u0441\u044b \u043f\u043e \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u044e \u0434\u0430\u043d\u043d\u044b\u0445, \u0442\u043e \u0442\u0430\u043a\u0438\u0435 \u043d\u0443\u0436\u043d\u043e \u043e\u0444\u043e\u0440\u043c\u043b\u044f\u0442\u044c \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0439 / transaction.\n\u041a\u0430\u043a \u0437\u0430\u0434\u0430\u0447\u0438 \u0442\u0430\u043a \u0438 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438 \u0438\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u0435\u0434\u0438\u043d\u043e\u0435 \u0432\u0438\u0440\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0435 \u0441\u043e\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435, \u0442\u0430\u043a-\u0447\u0442\u043e \u0432\u044b \u0443\u0432\u0438\u0434\u0435\u0442\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u043e\u0434\u0438\u043d connect/disconnect \u043d\u0430 \u0432\u0441\u044e \u0437\u0430\u0434\u0430\u0447\u0443 / \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u044e.\n. \u0422\u0435\u0445\u043d\u0438\u0447\u0435\u0441\u043a\u0438 - \u0434\u0430, \u0437\u0430\u0434\u0430\u0447\u0435 \u0431\u0435\u0437 \u0440\u0430\u0437\u043d\u0438\u0446\u0435 \u043a\u0430\u043a\u0438\u0435 \u0437\u0430\u043f\u0440\u043e\u0441\u044b \u043e\u043d\u0430 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u0442. \u041f\u0440\u043e\u0441\u0442\u043e \u044d\u0442\u043e \u043f\u0440\u043e\u0442\u0438\u0432\u043e\u0440\u0435\u0447\u0438\u0442 \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0430\u043c \u0446\u0435\u043b\u043e\u0441\u0442\u043d\u043e\u0441\u0442\u0438 \u0441\u0438\u0441\u0442\u0435\u043c\u044b - \u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0431\u0430\u0437\u044b \u0434\u0430\u043d\u043d\u044b\u0445, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043e\u0442\u043a\u0430\u0442\u0438\u0442 \u043d\u0435\u0443\u0441\u043f\u0435\u0448\u043d\u0443\u044e \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u044e, \u043d\u043e \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u0431\u0443\u0434\u0435\u0442 \u0434\u0435\u043b\u0430\u0442\u044c \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u043d\u0435\u0443\u0434\u0430\u0447\u043d\u044b\u0445 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u0432\u043d\u0435 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438.\n\u0412\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0432 \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0438 \u043e\u0441\u043e\u0437\u043d\u0430\u0432\u0430\u0442\u044c \u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0438 \u0438\u0437\u043c\u0435\u043d\u0435\u043d\u0438\u0439 \u0432\u043d\u0435 \u0442\u0440\u0430\u043d\u0437\u0430\u043a\u0446\u0438\u0438 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u0441\u0431\u043e\u044f \u043e\u0434\u043d\u043e\u0433\u043e \u0438\u0437 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 ;)\n. \u0417\u0430\u0434\u0430\u0447\u0430 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u0442 \u0441\u043e\u0431\u043e\u0439 \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u0439 \u043c\u043e\u0434\u0443\u043b\u044c, \u043f\u0440\u0435\u0434\u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e\u0449\u0438\u0439 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442 \u0441\u043e\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044f, \u0438 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0439 \u0432\u0441\u0435 \u043d\u0443\u0436\u043d\u044b\u0435 \u0437\u0430\u043f\u0440\u043e\u0441\u044b \u0434\u043b\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0438 \u0437\u0430\u0434\u0430\u0447\u0438. \u041d\u0435 \u0438\u043c\u0435\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u0438\u043b\u0438 \u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0446\u0435\u043f\u043e\u0447\u0435\u043a \u0432\u044b \u0445\u043e\u0442\u0438\u0442\u0435 \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u0432\u043d\u0443\u0442\u0440\u0438 \u043e\u0434\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438. \u0422\u0443\u0442 \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u0442 \u043b\u0438\u0448\u044c \u043f\u0440\u0438\u043d\u0446\u0438\u043f \u0443\u0434\u043e\u0431\u0441\u0442\u0432\u0430 \u043f\u0435\u0440\u0435\u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043c\u043e\u0434\u0443\u043b\u0435\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u044b.\n\u0418\u0437 \u0432\u0430\u0448\u0435\u0433\u043e \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u044f \u0440\u0435\u0447\u044c \u0438\u0434\u0435\u0442 \u043e\u0431 \u043e\u0434\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0435, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0435\u0439 \u0446\u0435\u043f\u043e\u0447\u043a\u0443 \u0438\u0437 4-\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432. \u0412 \u0442\u0430\u043a\u043e\u043c \u0432\u0438\u0434\u0435 \u0438 \u0440\u0435\u0430\u043b\u0438\u0437\u0443\u0439\u0442\u0435 \u0435\u0451. \u0412\u0430\u043c \u043f\u0440\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043d\u0435 \u043d\u0443\u0436\u043d\u043e \u043d\u0438\u0447\u0435\u0433\u043e \u043c\u0435\u043d\u044f\u0442\u044c \u0432 \u0443\u0436\u0435 \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u043d\u043e\u043c \u043a\u043e\u0434\u0435, \u043a\u0440\u043e\u043c\u0435 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0430 \u0432\u044b\u0437\u043e\u0432\u0430, \u0434\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u0430 \u0437\u0430\u0434\u0430\u0447\u0438.\n\u0412\u043e\u0442 \u0432\u0430\u0448 \u043a\u043e\u0434 \u043f\u0435\u0440\u0435\u0434\u0435\u043b\u0430\u043d\u043d\u044b\u0439 \u043d\u0430 \u0437\u0430\u0434\u0430\u0447\u0443:\n`` javascript\nfunction MyTask(t) {\n    return t.oneOrNone(\n            insert into\n            individual(first_name, surname, patronymic)\n            values($/individual_first_name/, $/individual_surname/, $/individual_patronymic/)\n            returning *\n        `, req.body)\n        .then(function (individual) {\n            return t.oneOrNone(\"select * from contractor where contractor_id = $/contractor_id/\", individual)\n                .then(function (contractor) {\n                    return promise.resolve({\n                        main: individual,\n                        contractor: contractor\n                    });\n                });\n        })\n        .then(function (individual) {\n            return t.oneOrNone(\"select * from document where document_id = $/document_id/\", individual.contractor)\n                .then(function (document) {\n                    individual.document = document;\n                    return promise.resolve(individual);\n                });\n        })\n        .then(function (individual) {\n        individual.document[\"notes\"] = req.body[\"document_notes\"];\n        individual.document[\"date_start\"] = req.body[\"document_date_start\"];\n        individual.document[\"number\"] = req.body[\"document_notes\"];\n\n        return t.none(`\n            update document set\n            notes = $/notes/,\n            date_start = $/date_start/,\n            number = $/number/\n            where document_id = $/document_id/\n        `, individual.document);\n    })\n    .then(function () {\n        res.send({\n            success: true,\n            data: req.body\n        });\n        return promise.resolve();\n    })\n\n}\n```\n\u0438 \u0442\u043e\u0433\u0434\u0430 \u0435\u0451 \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u0442\u044c:\njavascript\ndatabase.task(MyTask)\n    .then(function (data) {\n        console.log(\"DATA:\", data);\n    })\n    .catch(function (error) {\n        console.log(\"\u041f\u0440\u043e\u0438\u0437\u043e\u0448\u043b\u0430 \u043e\u0448\u0438\u0431\u043a\u0430 \u043f\u0440\u0438 \u0432\u0441\u0442\u0430\u0432\u043a\u0435 \u043d\u043e\u0432\u043e\u0433\u043e \u0444\u0438\u0437\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u043b\u0438\u0446\u0430:\", error);\n    });\n\u042f \u043f\u0435\u0440\u0435\u043d\u0430\u043f\u0440\u0430\u0432\u0438\u043b \u0432\u044b\u0437\u043e\u0432\u044b \u043a \u0431\u0430\u0437\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u043d\u0430 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442 \u0437\u0430\u0434\u0430\u0447\u0438 - \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 t, \u043f\u043b\u044e\u0441 \u044f \u0443\u043a\u0430\u0437\u0430\u043b \u0432 \u043a\u0430\u0436\u0434\u043e\u043c .then \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c promise, \u043d\u0435 \u0442\u043e \u0443 \u0432\u0430\u0441 \u0446\u0435\u043f\u043e\u0447\u043a\u0430 \u0440\u0432\u0435\u0442\u0441\u044f.\n. > \u0412\u0435\u0434\u044c \u0441\u043a\u0430\u0437\u0430\u043d\u043e \u0436\u0435, \u0447\u0442\u043e \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u043c\u043e\u0436\u043d\u043e \u043b\u0438\u0431\u043e promise \u043b\u0438\u0431\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440.\n\u044d\u0442\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u0438\u0437 \u043a\u0430\u043a\u043e\u0433\u043e \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430 \u0438 \u043f\u0440\u043e \u043a\u0430\u043a\u043e\u0439 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442?\n\n\u0418 \u043f\u043e\u0447\u0435\u043c\u0443 promise \u0441 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u0439 \u0431\u0443\u043a\u0432\u044b?\n\n\u044f \u043d\u0435 \u0437\u043d\u0430\u044e \u043a\u0430\u043a\u0443\u044e promise \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443 \u0432\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0435, \u044d\u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u043e\u0431\u0449\u0435\u0433\u043e \u043f\u043b\u0430\u043d\u0430 \u0441\u0441\u044b\u043b\u043a\u0430, \u0435\u0441\u043b\u0438 \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0432\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0435:\njavascript\nvar promise=require('bluebird');\n\u0415\u0441\u043b\u0438-\u0436\u0435 \u0432\u044b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0435 ES6 Promise, \u0442\u043e \u0442\u043e\u0433\u0434\u0430 \u043f\u043e\u043c\u0435\u043d\u044f\u0442\u044c \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u043d\u0430 Promise.\n. \u0424\u0443\u043d\u043a\u0446\u0438\u0438 task / tx \u0442\u0440\u0435\u0431\u0443\u044e\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0442\u0430 promise \u043e\u0431\u044a\u0435\u043a\u0442\u0430,\u0438\u043d\u0430\u0447\u0435 \u0431\u0443\u0434\u0435\u0442 \u043e\u0448\u0438\u0431\u043a\u0430 \u0432\u044b\u0431\u0440\u043e\u0448\u0435\u043d\u0430.\n. \u0415\u0441\u043b\u0438 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c - \u0442\u043e \u043b\u0443\u0447\u0448\u0435 \u043d\u0430 stackoverflow.ru\n\u0415\u0441\u043b\u0438 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u043e\u043c, \u0442\u043e \u043c\u043e\u0436\u043d\u043e \u0442\u0443\u0442 \u0437\u0430\u0434\u0430\u0432\u0430\u0442\u044c, \u043b\u0438\u0431\u043e \u043d\u0430 StackOverflow.com\n\u041d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c \u0442\u0443\u0442 \u043b\u0443\u0447\u0448\u0435 \u0442\u0443\u0442 \u043d\u0435 \u043f\u043e\u0441\u0442\u0438\u0442\u044c, \u044d\u0442\u043e \u0442\u0443\u0442 \u043d\u0435 \u043f\u0440\u0438\u043d\u044f\u0442\u043e.\n. It won't be addressed personally to me, rather to community. But I usually see questions on both StackOverflow websites when they contain a reference to pg-promise. This library is also a community one, not my personal ;) That's the way it works ;)\n. \u041f\u0435\u0440\u0435\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u044f \u0441\u0432\u043e\u0438 \u043e\u0442\u0432\u0435\u0442\u044b \u0441\u0435\u0433\u043e\u0434\u043d\u044f, \u043d\u0430 \u0441\u0432\u0435\u0436\u0443\u044e \u0433\u043e\u043b\u043e\u0432\u0443, \u0445\u043e\u0447\u0435\u0442\u0441\u044f \u0441\u043a\u0430\u0437\u0430\u0442\u044c \u0447\u0442\u043e \u043d\u0430\u0441\u0447\u0435\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0442\u0430 promise.resolve \u0432 \u0442\u0435\u0445 \u0441\u043b\u0443\u0447\u0430\u0435 \u0432\u044b \u043f\u0440\u0430\u0432\u044b, \u044d\u0442\u043e \u0438\u0437\u043b\u0438\u0448\u043d\u0435, \u043c\u043e\u0436\u043d\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c. \n\u0413\u043b\u0430\u0432\u043d\u043e\u0435 \u0442\u043e\u043b\u044c\u043a\u043e \u0447\u0442\u043e\u0431\u044b \u0441\u0430\u043c\u0430 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u0432\u0435\u0440\u043d\u0443\u043b\u0430 promise, \u0447\u0442\u043e \u043e\u043d\u0430 \u0438 \u0434\u0435\u043b\u0430\u0435\u0442.\n\u0418\u0437\u0432\u0438\u043d\u044f\u044e\u0441\u044c \u0437\u0430 \u043f\u0443\u0442\u0430\u043d\u0438\u0446\u0443.\n. I didn't remove anything: https://github.com/vitaly-t/pg-promise/issues/43\nI simply closed the question, as I thought it was fully answered. So, is this one, I believe, so please close it if so.\nP.S. Also, please do not open new issues in Russian language here. The issues discussed here are supposed to be readable by all members.\n. Cancelling queries isn't a frequent feature to be used by developers, for sure. In fact, you are the first one to ever ask about it.\nI personally have never used it. And now that you have asked, I just had a quick look at its implementation in file index.js\njavascript\n// cancel the query runned by the given client\nPG.prototype.cancel = function(config, client, query) {\n  if(client.native) {\n    return client.cancel(query);\n  }\n  var c = config;\n  //allow for no config to be passed\n  if(typeof c === 'function') {\n    c = defaults;\n  }\n  var cancellingClient = new this.Client(c);\n  cancellingClient.cancel(client, query);\n};\nI'm not sure how this function really works underneath - is it synchronous or asynchronous? If it is the latter, then what even does it provide? It would be interesting to find out.\nIn the meantime, you can access the pg instance via pgp.pg property, and call pgp.pg.cancel() the way you used to, I just don't know yet how it will work in this case. I think it was initially designed for use with the native client only, and since this library relies on connections from the pool,...\nTry and see if it works, and let us know ;)\n. @DomenicoColandrea86 any update there? ;)\n. Closing due to inactivity.\n. The locks on the database level exist so that the protocol extension via event extend is consistent:\n- Make sure you cannot extend the protocol outside of the extend event;\n- Make sure you cannot override any existing method within the protocol while extending it.\nSo, as you can see, it is actually a very useful feature. Switching it off would make it more error-prone.\nBut I can understand your problem with the test mocking...just thinking, if there is any other work-around for that...\nUPDATES:\nWe could add an environment variable that would explicitly disable all the locks. That would work fine for your test mode, I think.\nBetter yet, I could add property unlocked to the Initialization Options.\n. Ok, I will add noLocking as a new option within Initialization Options. Will let you know when it is ready ;)\n. @jniemin It is done in Release 1.11.0\nIf it all works for you - just close the issue ;)\n. It can be certainly considered for version 2.0 of the library, when all the breaking changes roll in.\nI've just updated the plan for 2.0.\nIn the meantime, you can use a more elegant way to check for the error message:\njavascript\n.catch(err)\n{\n    var msg = err.message || err;\n}\n. @madebyherzblut Reading it once more, I still do not see relevance to #37. That one is about checking the row being found or not, which isn't an error at all, and you were talking about the errors support here.\n. That's the thing, the issue you referred to isn't an error, and it doesn't reject. That was about use of method oneOrNone that resolves with null when no record is found. What you are talking about is different.\n. This has been implemented within the 2.0 branch (currently pre-release).\nType QueryResultError is returned when the returned dataset doesn't match the expected one according to the Query Result Mask.\n. @madebyherzblut In case you are still using the library, upgrading to the latest version will require a change in using type QueryResultError:\nIt is no longer available directly from the root, it is available from namespace errors, i.e. as errors.QueryResultError, as explained in the API ;)\n. Strings inside an array would be surrounded in quotes. Would you like to give an example of the code where it works otherwise?\nHere's an example:\njavascript\nvar pgp = require('pg-promise')();\nvar data = ['one', 'two'];\nvar s = pgp.as.format('test $1^', pgp.as.csv(data));\nconsole.log(s);\nOutputs:\njavascript\ntest 'one','two'\nTherefore, I'm not sure what you are talking about. Please clarify.\n. @RyanMcDonald Please follow up on your question, or close if it has been resolved.\n. This library formats and executes queries in their natural form. It does not provide any ORM facility beyond variable query formatting, since it is not an ORM.\nIt does however support Custom Types Formatting which allows complete freedom in formatting queries. You can implement your own type that represents SET ? and then inject it via your custom, pre-formatted type, which can achieve the same what you described.\n. @RyanMcDonald Did you get this working?\n. @RyanMcDonald The Custom Type Formatting is very flexible, and allows formatting override in many ways. I won't be going into every case possible. You should try it yourself first, and see how it works.\nI will give a quick example of what can be done, similar to what you described. But that's not the only approach, there can be many ways, depending on what you want precisely.\n``` javascript\nvar pgp = require('pg-promise');\n// sets all the object's properties as they are;\nfunction SetValues(obj) {\n    this.obj = obj;\n    this._rawDBType = true; // raw-text output override;\n    this.formatDBType = function () {\n        var props = Object.keys(this.obj);\n        var s = props.map(function (m) {\n            return m + '=${' + m + '}'; // creating the formatting parameters;\n        });\n        return pgp.as.format(s.join(\", \"), this.obj); // returning the formatted string;\n    }\n}\n```\nTest\n``` javascript\nvar testObj = {\n    first: 123,\n    last: \"text\"\n};\nvar result = pgp.as.format(\"UPDATE users SET $1\", new SetValues(testObj));\nconsole.log(result);\n```\nOutput\nUPDATE users SET first=123, last='text'\n. @RyanMcDonald Did this answer your question? If so, then please close the issue.\n. Closing due to inactivity.\n. @RyanMcDonald You are welcome! :)\n. There is a further progress with this in 4.0.6 update ;)\n. F.Y.I. Important breaking change in v6.5.0.. You need to use method sequence for that, either as part of a transaction or a task.\nAlso see Synchronous Transactions, which explains exactly the kind of problem you are having and how to solve it.\n. > does sequence() ensure that I create-destroy one promise before moving on to the next one?\nExactly!\n\nI also am not sure how to use sequence() in conjunction with a stream\n\nIt has a very simple interface, expecting requests one by one. Any API can be channelled into such logic easily ;) A stream also reads data one by one.\n\nI guess at this point, seems I should have gone to StackOverflow... I apologize.\n\nAnd then it will be myself again answering all the relevant questions on StackOverflow :)\nUPDATE: If you want more flexibility in using method sequence, then I suggest spex instead. PG-Promise 2.0 will be upgraded to use method sequence from spex when it is released.\nSee how pg-promise (2.0 branch) already redirects:\njavascript\n   this.sequence = function (source, dest, limit, track) {\n        return npm.spex.sequence.call(this, source, dest, limit, track);\n    };\nDo not mind the documentation there inside pg-promise 2.0 for the method, it hasn't been updated yet, but spex is fully documented.\n. @paulxtiseo Have you been able to sort out this issue?\n. You have all the right tools to implement proper throttling, methods sequence and page provided by the spex.\nI can't point you more precisely, because you are just describing the problem in general. But using those methods you shouldn't have a problem to solve it.\nIf you use method sequence, then your code would be something like this:\n``` javascript\nvar promise = require('bluebird');\nvar spex = require('spex')(Promise);\n// returns a promise with the next data read;\nfunction readNextData(index, data, delay) {\n// 1. Read next 1-row data from stream;\n// 2. Once read is finished, resolve with that row-data;\n\n// If failed to read, reject;\n// If no more data left, either return `undefined` or resolve with undefined;\n\n}\nfunction dest(index, data, delay) {\n    // process the next row of data;\n}\nspex.sequence(readNextData, dest)\n    .then(function (data) {\n        console.log(\"Read total:\", data.total);\n        console.log(\"Duration:\", data.duration);\n    }, function (reason) {\n        console.log(\"FAILED:\", reason);\n    });\n```\nAnd if you use method page, then your code would barely change:\n``` javascript\n// returns a promise with the next page of data read;\nfunction readNextPage(index, data, delay) {\n// 1. Read the next page of data from stream;\n// 2. Once read is finished, resolve with that data;\n\n// If failed to read, reject;\n// If no more data left, either return `undefined` or resolve with undefined;\n\n}\nfunction dest(index, data, delay) {\n    // process the next page of data;\n}\nspex.page(readNextPage, dest)\n    .then(function (data) {\n        console.log(\"Pages:\", data.pages);\n        console.log(\"Total:\", data.total);\n        console.log(\"Duration:\", data.duration);\n    }, function (reason) {\n        console.log(\"FAILED:\", reason);\n    });\n```\nIn fact, since you just want to insert data into the database, you may not even need to use dest function, but instead just generate your requests within the source function and return them to be resolved.\nAnyhow, it takes trying. And if you are stuck with a more specific issue, then I can help. Otherwise, that's all I can advise here.\nUPDATE: Please note that while methods sequence and batch of spex are now available right from any task or transaction within pg-promise, method page isn't. But use of spex from pg-promise is trivial, if you look at the single-line forwarding within pg-promise. In fact, I may add method page also.\n. Your task has just become even easier, because I released version 2.0.11 that adds support for spex.page natively.\nAt this point I suggest that you try and come up with a solution based on the methods available to you, which is more than enough. And if you are stuck along the way with something more specific, you can open an issue accordingly, and I will try to help you.\nIn the meantime, this issue here is too broad to make any progress, so I'm closing it.\n. @paulxtiseo If I did I would point you at it as the first thing instead of writing you a generic example of how to approach it. Sometimes we need to write things ourselves. Don't let the initial misunderstanding get in the way ;) Understanding promises properly does take time, while trying and writing your own code is the only way. Your code is 100% changeable to promises, you need to start rethinking it. And I gave you the initial ideas for it ;)\n. @paulxtiseo quite expectedly, I was the one to answer your question on StackOverlfow in the end :)\n. @paulxtiseo have you finished this research or are you still experimenting?\nI've been playing around with the idea of joining a readable stream with promises, and made some good progress there. Let me know if you are still interested.\n. Duplicate of #41\nThis has been already changed in 2.0 branch, which will be released soon. I'm just updating the documentation for it.\n. @kdex just so, 2.0.6 (pre-release) is available, and it is stable. And if you are stuck with that issue you described, you can try using that one instead. I just need to update lots of documentation before making an official release.\nFor the time being you can use 2.0 notes as a guidance.\n. 2.0 is a pre-release, so you can only install it with the version specified:\nnpm install pg-promise@2.x\n. That's strange, because those links are automatic, provided by GitHub, and they do not fail for me here.\n. You are right, this looks like a bug on GitHub, because those releases themselves were done correctly.\nAt least the npm command works as expected :)\n. UPDATE: I think I know what went wrong there. I forgot to change the destination branch to point to 2.0 when doing those pre-releases, and it pulled the code from the master branch. My bad. Will pay attention for the next update. Thank you for pointing this out!\n:+1: \n. @kdex pre-release 2.0.7 is now available, which no longer has the issues with those links. I also deleted the previous pre-releases, as they weren't pre-released correctly.\n. @kdex You are welcome! And version 2.0 is finally out! :)\n. @nickretallack Here: Prepared Statements\n. From the main page, Getting Started:\n\nTo get started quickly, see our Learn by Example tutorial.\n\nYou will find just about every example you need right there ;) Good luck!\n. Every query method that expects a query string as the first parameter, can also take a prepared statement instead.\n. If your question has been answered, please close the issue ;)\n. I was under the impression that the stream used by pg-query-stream was always set to the flowing mode...\nAt least, I haven't seen it working in any other way. Would you like to show an example where that library switches into the paused mode?\nI just want to be able to test the change properly.\n. Ok, just so unrelated to that, you know you can use pg-promise query formatting features directly. In your case it is by calling postgres.as.format(query, values). So when you are preparing a query for QueryStream you can use it ;)\nAlso see how filters can be created, in the end of this chapter.\n. I'm not sure what you mean by pre-compiled queries. When referring to passing parameters separately - there is Named Parameter formatting, so you can pass in just an object as the formatting parameter. There is also support for Prepared Statements, that's in case you are concerned about SQL injection.\n. @nyurik  in case you are interested, Learn by Example tutorial now also includes an example of streaming into the database ;)\n. Have a look at the most recent comparison charts, it might help ;)\n. You are mixing two things that are handled by two separate libraries.\nPrepared Statements are passed into node-postgres directly, which is the library that processes and executes them.\nAnd at the same time you are trying to use the syntax that's used within pg-promise for query formatting. \nThose are two separate things. Prepared Statements must adhere to their own syntax in node-postgres, because those are fillied with values within Postgres server. You will see from there supporting $1 syntax, while Named Parameter syntax isn't even supported there.\nJust so, there is little use for Prepared Statements, unless you are very concerned about SQL injection. And they are indeed a lot more awkward to use, and their syntax is non-extendible, since they are handled by the server.\n. That's not possible, because there is no conversion from Named Parameters into index-based parameters. It is an entirely different mapping logic.\n. Just to clarify a little...\nFirst of all, Named Parameter syntax is supported by pg-promise, and it implies a certain type of formatting, while node-postgres formats values a bit differently. This goes for Date, Number, Array, at the very least.\nSecond, Named Parameter syntax supports ^ for raw-text insertion, plus the entire Custom Type formatting, which node-postgres does not support.\nI understand that Prepared Statement formatting is awkward, but at least it is strictly defined, whereas if we try to mess with it, there is no clear path as to how it should work, will only cause more questions.\nI personally do not like Prepared Statements at all, and I think they are mostly a waste of time, because:\n- Their performance gain is only possible for very complex queries, while only slowing the simple ones. And even for the complex ones the gain is often negligible;\n- SQL Injections can by avoided differently, through the website security design. And it is also one of the least exploitable things, because it is very hard to make useful.\n. I've done testing in this area some tome ago, using massive insert queries. I saw something like 5% drop in performance with Prepared Statements.\nA regular INSERT is a simple operation, and its execution plan has no value on the server, it only gets in the way.\nExecution plan is only of value when there is an extra logic or processing within the query.\nI'm not sure if there is any article about it out there, it's just the result of some tests.\nI can see many developers go an extra mile to execute every query as a Prepared Statement, without understanding where it is needed and where it is of no use.\n. I don't think the inner transaction works the way you think it does.\nThis scenario has been covered by this test\nCOMMIT of the inner transaction is ignored when the parent transaction does ROLLBACK.\nBy the way, it would help a lot here if you were to tag your transactions, like this:\njavascript\ndb.tx(\"first\", function (t1) {\n    return this.none('update users set login=$1', ['External']).then(function () {\n        return t1.tx(\"second\", function (t2) {\n            return t2.none('update users set login=$1', ['External']);\n        });\n    })\n        .then(function () {\n            return t1.one('select * from unknowntable'); // emulating a bad query;\n        })\n});\nPlease provide an updated pg-monitor log.\n. Just on my previous response, please post an updated log, using tagged transactions, so we can see more clearly the execution chain, without making assumptions.\n. I have provided the code example for you above, already updated. You can see \"first\" and \"second\" injected tags before each transaction. This will change the log accordingly.\n. @vertiman I'm gonna look at it closely when I'm back in about 4 hours. Post any details in the meantime that you think are relevant ;)\n. @vertiman Thank you for pointing at this issue. As I had a chance to look at it in detail, I can see now that the problem is more serious than I thought, and your PR won't fix it.\nAccording to the official documentation about nested transactions, SAVEPOINT commands must be used, and not another BEGIN/COMMIT/ROLLBACK.\nI don't know how I've missed it, and what's worse, the tests didn't bring this up either. Perhaps it is because nested transactions aren't that popular to begin with.\nI have started working on making the necessary changes to address the issue, and will let you know when it is ready. But it looks like a major change/fix.\n. The official documentation says - you have got to use SAVEPOINT for inner transactions. I don't think there is any work-around for it. This is the only right way to do it, it seems, and I'm looking at implementing it. And I want the change to cover every possible situation, not to come back to it again.\n. Something along those lines, but I have to see everything working together first. And that includes proper execution of release savepoint and rollback to savepoint, which you didn't even include.\nAnyhow, i'm working on it, and I will need all the relevant tests in place before committing the changes.\n. @vertiman \nI have created a new branch nested_tx, and committed all the changes needed to fix the issue.\nI will do another review of it later and update all the tests, before merging it into the master branch.\nSo far it seems to be working in all the cases I tested it against. Since savepoints rely on the transaction level, I called all automatic labels level_1, level_2, etc, which corresponds to the transaction level.\nUPDATE: All tests added, the branch has been merged into master and then deleted.\n. @vertiman the issue has been fixed in v.2.2.0.\nPlease let me know if it all works for you now as expected.\nUPDATE: pg-monitor has been updated to recognize savepoint as special commands.\n. @vertiman did you have a chance to test the new release on your side?\n. Thanks for reporting back!\n. @vertiman Version 2.4.2 added an important note regarding the limitation of nested transactions.\n. @vertiman You are welcome! I have re-written that chapter several times since then, because it makes a crucial point about nested transactions.\n. @vertiman PostgreSQL official documentation doesn't state it clearly that they do not have proper support for nested transactions, call it a marketing step, so I had to explain it myself :)\n. @vertiman yep :)\n. Can't use this PR, sorry. The problem appears to be bigger, as per #57, requiring use of SAVEPOINT commands on the nested levels, which I've started working on.\n. Rework on nested transactions support.\n. Looking at the list of columns that you provided, they don't look dynamic, they look quite predefined, and if you try to treat known fields as generic ones, then you loose their type information, which already prevents you from doing any kind of intelligent query formatting. For example, date fields won't even work.\nYou shouldn't treat such list of fields as dynamic, and access fields normally, while converting to their appropriate types, so query formatting can convert data according to its type.\nAnd you shouldn't use this section within your post handler:\njavascript\n .done(function () {\n                        pgp.end(); // closing the connection pool, to exit immediately.\n                    })\nbecause this shuts down the library, and to be optionally used only when exiting the application.\n. As explained earlier, if you treat your fields as generic, you are missing their type information. Without type information your query formatting doesn't know how to format the values, and in fact neither do you. You cannot, for example, treat strings and date fields in the same way. Your fields aren't of the same type, they are of different types, so you have to treat them according to their type, you cannot do it in a generic way.\n\nIs there any way to dynamically map the req.body names with the column names in the insert sql?\n\nOnly when they are of the same known data type, which they aren't in your example.\n. @prajmehta if your question has been answered, then please close the issue.\n. We do have direct access to the node-postgres instance, from the library's root, it has property pg which represents the instance of the node-postgres module.\nAccordingly, pgp.pg.types - your node-pg-types interface;\n. @jcristovao please close the issue, if your questions has been answered ;)\n. Included this question into FAQ\n. There was a lot of debate over the new warning system added to Bluebird 3.0, which I personally hate, because every time I looked at it, it doesn't make sense. I believe it was just a bad idea.\nSee my arguments against it: https://github.com/petkaantonov/bluebird/issues/832\nThe best I could convince Petka was to restore the functionality of BLUEBIRD_DEBUG=0 that disabled those new warnings for the development mode. So if you set it, you won't see those warnings for Bluebird 3.0. That flag disabled the debug for development entirely.\nI was told it is an opinionated implementation of promises after all, so I guess it is ok to come up with your own standard from time to time. Totally not, if you ask me, but that's just my opinion.\n. In fact, I was so upset about that kind of change, I started writing my own promise library, I called it nitrous. It is at its very early stage of development.\nI called this way because I want to focus of performance, first of all, to outperform Bluebird, that's my target, and plus simplicity. It may be a while before I get there, but I have started :)\n. Thanks, @Cleod9 \nThe most troubling example I found is when we have return value;, but the value can be undefined, so we get the same warning again that no value was returned.\nAnd having promise.reject() should be treated the same as throw;, without any warning, it's just plain wrong.\nAnyhow, with 3.0 it feels Bluebird went a little sideways, trying to introduce invalid non-existing standards for code diagnostics. I think they will find many developers rejecting the concept.\n. Since this clearly has turned into a rant about Bluebird, it won't have any resolution here, so I'm closing the issue.\n. - Are you sure that you have the right record with the id that you specified?\n- Are you sure that _id you are passing is of the same type as the one used in the database? Check what's being executed.\nAnd why are you using such a complex approach to get your records? The following implementation would do just the same:\njavascript\nfunction findById(_id) {\n    return db.one(\"select * from location where id = $1\", _id);\n}\nI would suggest, start using pg-monitor, it will give you a clear idea of what is going on with your queries. Here's a complete example of using it.\n. You need to do some basic diagnostics on your side, as I don't have your database anyway.\nThis library doesn't post-process data from node-postgres, it returns it exactly as it was passed in. If something arrives in a strange format, I'd check what data types are being queried against.\n. @aHumanBeing is it sorted?\n. This is not a good approach, it has too many holes in it. If you were to single out an issue, I would be able to explain how to do it properly with pg-promise. Going back to the basic pg is only gonna cost you more time lost eventually. pg-promise can do everything pg does, and much more, and a lot easier, but not the other way round.\nThe last time I started by suggesting a very simply one-line implementation, asking what's wrong with that, you didn't return to me on that. Start with the basics, so you can understand better how it works.\n. What does this mean? An issue without a description at all... \nPlease provide a description of what it is you are logging next time.\n. You mean, delete your post?\n. You can't delete posts here :) Only replies can be deleted. I wouldn't worry about it though.\nI take it the issue you were having has been resolved - correct?\n. If you are referring to the difference between 2.2.4 and 2.2.5, then there wasn't any.\nAnd the error you referred to related to the fact that you used the library in Node JS prior to 0.12, while not specifying any promise library, which starting with version 2.0 wouldn't work, throwing that error, as explained in 2.0 Migration tutorial.\nAnd the code I saw in an email notification looked wrong also, It could have never worked.\nThis line:\njavascript\nvar pgp = require('pg-promise');\nis supposed to be:\njavascript\nvar pgp = require('pg-promise')(/*options*/);\nno matter which version of the library you are using.\n. If it is ES6 that you are using then ES6 promise should be sufficient. Otherwise, specify your promise library the way it is available from that Babel platform.\nOther than that, I know nothing about Babel. This library's official support is for Node JS 0.10 - 5.x.\n. @JaxCavalera Adding a link to a similar discussion: https://github.com/babel/babel-loader/issues/23#issuecomment-151498013\nAnd closing the issue here, because I can't really help with Babel promise usage, as I'm not familiar with it.\n. The library supports any Promises/A+ compliant library, as stated everywhere. And method done isn't part of Promises/A+ at all. It was used as an example for library Bluebird, which implements method done.\nAlso, the example you are referring to is long gone. See the updated examples. However, it was replaced with finally, which is also not part of Promises/A+, but it doesn't matter. The examples are not part of the library itself, they are just samples of the final application, and as such, do not need separate documentation, the API used is already mentioned there.\nAlso, I wouldn't even use method done, it is unnecessary.\n. @santanubasu I had a look at that old version of Q, and it is easy to see that it is absolutely not compliant with the Promises/A+ standard.\nHere's what the library's root looks like in 0.9.6:\njavascript\nfunction Q(value) {\n    return resolve(value);\n}\nthat looks very wrong. And here's what the latest looks like:\njavascript\nfunction promise(resolver) {\n    if (typeof resolver !== \"function\") {\n        throw new TypeError(\"resolver must be a function.\");\n    }\n    var deferred = defer();\n    try {\n        resolver(deferred.resolve, deferred.reject, deferred.notify);\n    } catch (reason) {\n        deferred.reject(reason);\n    }\n    return deferred.promise;\n}\nthat's what it should be to be compliant.\nI have no way of knowing from which version Q started supporting Promises/A+ standard, you can try to find out that yourself and let other people know here.\nIn the meantime, this isn't a problem for this library. Even though it does try to detect the compliance, it does it through the signature of the library, it doesn't run tests to determine compliance. I'm not sure if it should though.\n. I might consider improving the detection of the promise compliance, but it would be a bit awkward, because it requires running a series of tests during the library initialization, and while being a low priority, I'm not sure if it is a good idea to begin with.\nUPDATE:\nI addition, it would never guarantee that the library is in fact fully compliant with Promises/A+, because that requires a comprehensive test. So, I think it is a no go.\nYou just will need to check that the version of the promise library you are using is Promises/A+ compliant. There is no other way around.\nAnd this library cannot keep track of that, there are way too many promise libraries out there. Here're just some of them: awesome promises\n. @santanubasu thanks! :+1: \n. That sounds strange, something must be missing. I can't see the log to understand what exactly.\nAre you using pg-monitor? That would show you the exact queries being executed. Always helpful in nailing strange issues like this.\n. Ok. Here's an example for a quick start ;)\n. It doesn't really make sense. The only way such query would be formatted if you were to execute the following query:\njs\ndb.query('SELECT username FROM users WHERE username = $1^', uname)\nbut you aren't. When $1 variable is used for a text string, it is always wrapped into quotes. The only case it can avoid it is when uname is an object that supports custom formatting, which I presume you are not using - or are you?\nYou can check it yourself, output the following string:\njs\nvar s = pgp.as.format('SELECT username FROM users WHERE username = $1', 'jax');\nconsole.log(s);\nAccording to you, it would output:\nSELECT username FROM users WHERE username = jax\nwhich would be impossible, it is supposed to output:\nSELECT username FROM users WHERE username = 'jax'\n. How do you end up executing string like this: '\"jax\"'? The only way I can do it if I have uname='\"jax\"'.\nAre you doing some sort of pre-formatting somewhere?\nAgain, test this with direct value and the variable:\njs\nvar s = pgp.as.format('SELECT username FROM users WHERE username = $1', 'jax');\nconsole.log(s);\nand see the difference. something seems strange with that variable you are using.\nAs for the other question, you are not using the query result correctly. You call method query, which when doesn't find any record, resolves with []. If you always expect 1 record, use method one, if one or no records - method oneOrNone.\n. Hah! You shouldn't have used that function! And for the other part - I updated my previous reply ;)\nLet me know if this got it all sorted, and close the issue ;)\n. Because this is exactly what your code does:\njs\n  .then(function() {\n        console.log('Username is Available' + uname);\n        return true;\n    })\nyou do not even analyze what is being returned, just always return true.\n. There are no errors with that query, method query successfully returns 0 records, with data = []\nWhy would expect an error there?\n. Depends on what you app logic is there. If you are expecting any number of records, you can call query, or manyOrNone, or any, and then analyze the data:\njs\n .then(function (data) {\n    if (data.length) {\n        // have records;\n    }\n})\nif you are expecting 1 record always, use method one, if 1 or 0 records - method oneOrNone. See the methods documentation, it is the very basic thing about this library. Did you read Learn by Example? You really should ;)\nIt even starts with:\n\nQuery method names of the library denote the expected result from the query: query - generic, none, one, many, oneOrNone, manyOrNone = any and result.\nSee Queries and Parameters to understand the difference between them.\n. Yeah, have another look, it's really quite simple. I'm outa here for now, 2:30am here.\n. From chapter Connecting:\nThis library doesn't use any of the connection's details, it simply passes them on to PG when opening a connection. For more details see pg connection parameters in WiKi and implementation.\n\nSo if it works for you, it is great. And as far as pg-promise documentation goes, there is no denial of such functionality either. And there was never a goal to re-document functionality of the pg module.\nTo your question:\n\nShould the possibility to pass pool settings in connection options be a documented feature or is my code based on a bug?\n\nThe answer is: Neither. It is one of the pg functionalities not meant to be documented here. You should use pg documentation for reference instead.\n. Included this question into FAQ\n. I don't see why you think pg-promise and connect-pg-simple would use a different pg version:\nfrom pg-promise: \"pg\": \"4.x\"\nfrom connect-pg-simple: \"pg\": \">=2.7.0-0 <5.0.0-0\"\nboth will pull the same current version of pg - 4.4.3. Plus there is such thing in npm as the package version override, so you can easily force the 2 packages use the exact same version.\nAnd I would not expect any conflict, they should be able to work independently.\nAnd no, Configurable Transactions got absolutely nothing to do with it.\nOther than that, if somebody decides to implement a pg-promise version of express-session - that would be a welcome addition!\n. @JaxCavalera\nI have looked at couple framework-specific implementations: connect-session-sequelize and connect-session-knex, and frankly, I don't see much point in writing those in the first place.\nThe only thing that makes sense - they all were started at around the same time, spring 2014, so there was no usable implementation for PostgreSQL at all. But now that there 3 usable implementations already to work with PostgreSQL, I see no point adding one more, because they all do the same thing, just implement it differently.\nAnd in theory, all 3 should be able to work with pg-promise without issues, as they are all independent modules, but perhaps connect-pg-simple is the best or at least simplest choice.\n. @voxpelli Thank you! :+1: I believe this concludes the subject nicely :)\n. The only way this.batch(queries) would produce error TypeError: undefined is not a function is when this is not set, but it is set there for sure, and the fact that you use this successfully in the previous lines proves that.\nSomething else is missing, and it is not obvious from the code you provided. Try to narrow the error down to the exact place where the error is being thrown.\nYou have access to the call stack, and it is the kind of situation where you should use it to see where the problem is exactly.\n. I don't know which version of the library you are using, so the code line doesn't tell me anything. Line 443 in the current branch cannot throw such an exception. Try to find the exact line of code that's throwing the error.\nIt should be easy enough to debug on your side where it is happening ;) Also, please do tell me which version of the library you are using.\n. In terms of this library's lifespan, that version is ancient. Would you be able to upgrade to the latest 2.6.4 and see if the issue is there or gone?\nToo many changes happened since that version, 5,129 additions and 1,457 deletions, to be exact, which is equivalent of rewriting everything from scratch.\n. List of breaking changes between 1.x and 2.x: https://github.com/vitaly-t/pg-promise/wiki/2.0-Migration\n. Now you are showing an HTTP-related error, which got nothing to do with pg-promise. You will need to figure that out yourself.\nDo not use connect and finally at all, use task instead:\n``` js\nfunction myTask(t) {\n    return t.query(\"select count(*) from provider_type p\")\n        .then(function () {\n            recordsTotal = obj[0].count;\n            filteredQuery = \"select CASE when type = 0 then 'Triage / Screening' \" +\n                \"when type = 1 then 'Clinical' \" +\n                \"when type = 2 then 'Prescriber' END as type\" +\n                \", active from provider_type p where 1 = 1 \";\n        // provider id\n        filteredQuery += \" and p.provider_id = \" + pid;\n        filteredQuery += ' order by timestamp desc';\n        return t.query(filteredQuery);\n    });\n\n}\ndb.task(myTask)\n    .then(function (providerTypes) {\n        //socket.emit('insuranceProviders', insuranceProviders);\n        var returnData = undefined;\n        returnData = providerTypes.slice(parseInt(req.query.start), parseInt(req.query.start) + parseInt(req.query.length) + 1);\n        if (returnData !== undefined) {\n            res.send(JSON.stringify({\n                \"draw\": parseInt(req.query.draw),\n                \"recordsFiltered\": providerTypes === undefined ? 0 : providerTypes.length,\n                \"recordsTotal\": recordsTotal,\n                \"data\": returnData\n            }));\n        }\n    })\n    .catch(function (error) {\n        winston.log('error', 'providerTypeData'); // display reason why the call failed;\n        winston.log('error', error); // display reason why the call failed;\n        res.send(JSON.stringify({\n            \"draw\": parseInt(req.query.draw),\n            \"recordsFiltered\": 0,\n            \"recordsTotal\": 0,\n            \"data\": ''\n        }));\n    });\n```\nSee Tasks + examples.\n. @aHumanBeing any progress there? I'd like very much to get to the bottom of it :)\nI'm sure it's either a misunderstanding somewhere, or a very old bug that was fixed a while ago (the original post that is).\n. Both methods should work the same, but tasks were introduced later, making the connect approach kind of obsolete, even though it still works. I don't know why it didn't work for you. Something else must have been wrong there.\n. @aHumanBeing I believe the issue is resolved now.\n. You can use event extend to add a method that would inject the current user_id and request_id in every query.\nAlternatively, you can create your own custom type either to decorate every request with the additional data to be injected, or to work as a placeholder for the static values to be injected. Questions with examples of custom formatting: #42, #49 \n. @jmealo, you are welcome!\n. @jmealo out of curiosity, I just had a look at that Koa you mentioned, and I saw the very first example there made with ES6 generators. Just wanted to point out that pg-promise recently received proper support for ES6 generators, see generators ;)\n. Not sure I can wrap my brain around your implementation, it looks quite unconventional.\nPlus, you are fully relying on ES6 template formatting strings, as opposed to pg-promise query formatting. Be careful with that, because ES6 template strings do not know anything about PostgreSQL query formatting syntax, there is plenty of specific there, I wouldn't trust ES6 templates with that.\nIn addition, ES6 templates automatically devoid you of access to Custom Type Formatting, a very useful feature in the library. Just so, pg-promise query formatting is much more powerful and flexible than ES6 templates ;) aside from multi-line support.\n. That doesn't look right. You should use event extend, if your intent is to extend the protocol with your own methods.\nAnd pgp represents the library instance (initialized). I understand you want to use multiple db instances, based on connections?\n. If I understand it correctly, you have several databases, and you want specific user details to be used within your custom extension of each.\nIn this case you can always extend the protocol with an extra property that would represent details about the user for that database, and so you can use those details within your custom method.\nAs per documentation the event extent:\n\nThe extension thus becomes available across all access layers:\nWithin the root/default database protocol;\nInside transactions, including nested ones;\nInside tasks, including nested ones.\n. The method you introduce within event extend should have no problem accessing any settings outside of the protocol, or you can parametrize your custom method to take it as parameter. I don't understand the problem doing that, sorry.\n. @jmealo yes, you extend the protocol when initializing the library, and that's it. That shouldn't stop you from using your own dynamic context within the extensions.\n. @jmealo have you made any progress resolving this issue?\n. Closing, due to inactivity.\n. There was too much discussed here, and a while back, I'm no longer sure to what this pertains.\n\nIf there is another issue, it is probably best to open a new issue.\n. What do you mean? Please provide a description next time.\n. Every time I make a new release, I document what has changed. So if you just go through the releases, you will be able to see that.\nBeyond that, being a GitHub project, it is easy to see details of each commit.\nThere is also a very abbreviated project history, though it's not much of use.\nThat's all there is to it.\n. At the moment this project doesn't have a separate changelog file like that.\n. This library doesn't deal in terms of inserts or selects, only in terms of queries, in the most generic way.\nWhatever you ask it to execute, the library executes and returns the result, it doesn't offer any logic beyond that.\nI don't know what you were expecting that should have been generated, the library provides simple variable formatting, that's all.\n\nWhat was generated (and now what I need):\n\nAnd you showed only one line there,... where is the one that you need?\n. Looks like you are trying to mix up in-server query processing with the app formatting, which isn't possible in any library. And if it executes and inserts the data correctly, why would you want to split the insert statements?\n\nI tried using batch but this created a single insert with specified column as an array\n\nI don't know what you tried, to see what the issue was...\nBatch allows execution of any number of queries in parallel.\n. That looks fine. Are you saying that batch executes only one query? That would only happen if your variable stuff contained one element.\n. @aHumanBeing not a problem.\nOn another note, if you are executing multiple inserts, you may consider doing it as a transaction rather than a task ;)\n. UPDATE:\nThere is lots more the library offers now with the helpers namespace when it comes to automatic query generation for INSERT or UPDATE operations.\n. I don't know what Babel does to the stack tracing, if anything. In my tests when I wanted to see deep stack tracing I used Bluebird for that, which has the best stack tracing, not to mention it is the best promise library all around.\nIf Babel can work with the Bluebird, and you want deep stack tracking - that's your best option.\nAs far as pg-promise is concerned, it does nothing to the error stack, reporting errors exactly as they are.\n. That error can only be thrown if you pass invalid parameter for the query.\n\nif conditions 1 and 2 are false, an error is thrown: Parameter 'query' must be a non-empty text string.\n\nYou are not setting the query variable, hence the error. \nI do not know what that q(undefined) really does. It depends on what is it you are expecting from the task.\n. This library has no preference or expectations from any promise library, and as such it has no dependency on their versions or features.\nOnly your code can throw an error like that. And i'm guessing something goes wrong inside your Promise.all call, possibly inside getAsync(data).\nA good start would be posting the exact error you are getting.\nAnother example why you wouldn't get into .spread is if you are not configuring Bluebird as the promise library, in which case method .spread doesn't exist; check that: promiseLib\n. > It seems that db.result immediately returns an empty object which gets populated with the resolved value upon fulfillment.\ndb.result returns a promise immediately, because that's how promises work, they provide data upon fulfillment.\nNothing you described helps to identify the issue. My guess is, it is something very trivial on your side, or else it would have been reported long time ago.\nI tried to reproduce the issue in different ways, nothing like this happens.\nYou can try to replace the call to pgp with a simple promise result: Promise.resolve(data), and then chain the rest to see what happens.\n. Here's a complete test application. Are you getting any issue with it? If so, please let me know how, because I don't see any problems here.\n``` js\nvar promise = require('bluebird'); // v. 2.10.2\nvar pgp = require('pg-promise', { // v. 2.8.5\n    promiseLib: promise\n});\nvar db = pgp('postgres://postgres@localhost/pg_core_test');\ndb.one(\"select 123 as value\")\n    .then(function (data) {\n        return promise.all([\n            data.value\n        ]);\n    })\n    .spread(function (value) {\n        console.log(\"value:\", value);\n    });\n```\n. Glad you have sorted the issue. And yes, all static properties can be found on both the library instance and the initialized instance, which one of the changes introduced in version 2.x, as explained here.\nSee what else changed in the 2.0 Migration document.\n. js\ndb.tx(t=> {\n        return t.result(\"UPDATE table bla-bla\")\n            .then(result=> {\n                if (!result.rowCount) {\n                    // 1. throw new Error(\"Ops, not updated anything!\");\n                    // 2. return Promise.reject(\"Ops, not updated anything!\");\n                }\n            });\n    })\n    .catch(error=> {\n        // 1. error.message = \"Ops, not updated anything!\";\n        // 2. error = \"Ops, not updated anything!\";\n    });\n. You are welcome, and I have updated the answer for completeness ;)\n. Any query can be part of a batch. Can you give an example of what you mean exactly? I'm not quite clear there...\n. If you mean something like this, then certainly:\njs\ndb.tx(t=> {\n        var updates = [];\n        for (var i = 0; i < 100; i++) {\n            var query = t.result(\"UPDATE table bla-bla\")\n                .then(result=> {\n                    if (!result.rowCount) {\n                        return Promise.reject(\"Ops, not updated anything!\");\n                    }\n                });\n            updates.push(query);\n        }\n        return t.batch(updates);\n    })\n    .catch(e=> {\n        var errors = e.getErrors(); // to get only the text statements;\n    });\n. Doesn't my example above show exactly that? Unless you want it only to fail when the overall update from all requests resulted in 0 rows, then you would change it accordingly.\n. n.p.\nat this point it's mostly about how to use promises, not so much about pg-promise itself ;)\n. > You are a genius!! When you visit L.A., ping me, I owe you a beer!\nIf i'm there next time, it will be Florida. Other than that I'm in Dublin, Ireland, the beer is good here too :)\nCheers! :+1: \n. I wasn't considering adding that feature, no, but the possibility of implementing it, if necessary.\nThis is why version 2.8.0 introduced support for event receive, during which you can do whatever you like on the original data set before it reaches the client.\nAnd as an internal test, I did use that humps library to convert the column names. Read further.\n. I used the following implementation as a test, also optimized for performance:\n``` js\nvar options = {\n    receive: function (data / , result, e /) {\n        camelizeColumnNames(data);\n    }\n};\nvar humps = require('humps');\nvar pgp = require(\"pg-promise\")(options);\nfunction camelizeColumnNames(data) {\n    var names = Object.keys(data[0]);\n    var camels = names.map(n=> {\n        return humps.camelize(n);\n    });\n    data.forEach(d=> {\n        names.forEach((n, i)=> {\n            var c = camels[i];\n            if (!(c in d)) {\n                d[c] = d[n];\n                delete d[n];\n            }\n        });\n    });\n}\n```\nOne should always consider the performance implication such transformation may carry when pre-processing large data sets.\n. You are welcome!\nAnd here's a seriously optimized version of it, way better then my previous implementation:\njs\nfunction camelizeColumnNames(data) {\n    var template = data[0];\n    for (var prop in template) {\n        var camel = humps.camelize(prop);\n        if (!(camel in template)) {\n            for (var i = 0; i < data.length; i++) {\n                var d = data[i];\n                d[camel] = d[prop];\n                delete d[prop];\n            }\n        }\n    }\n}\nIn fact, this one has such a good performance, one can even use it in performance-critical stuff ;)\n. @jcristovao Just made the example part of the official documentation, see receive\n. @jcristovao Check out the new 2.9.0 feature - Query Files ;)\n. @jcristovao \nCheck out - my latest meditation about the query performance: Performance Boost ;)\n. @ferdinandsalis there is even some user's re-publishing of it:\nPg-promise and case sensitivity in column names :wink:\n. @arm5472 couple issues in your code...\nYour optimization is made in such a way that it will only be of benefit when some of the columns do not need to be camel-cased. I think if all columns need to be camel-cased, it is likely to become slower.\nSecond problem - you do not check for camel-case overrides, like I do here: http://vitaly-t.github.io/pg-promise/global.html#event:receive\njs\nif (!(camel in tmp)) {\nThis check prevents us from overriding an existing column when after caramelizing it matches another existing column. And you do not do this check in your code ;)\nExample: column _user-id can override column User ID, etc. Those are unique cases, but still ;)\n\nStill, it is a good effort, although I wouldn't use forEach , rather for for optimum performance ;)\nAnd I might even update my algorithm later on ;) I will post an update, if I do ;). @arm5472 which version of Node.js did you use?\nI've run tests against the latest, Node 8.6.0, and results are the opposite, your version is slower than mine by about 10%. Maybe you didn't measure it right? Here's the complete test.\n```js\nconst data = [];\nfor (var i = 0; i < 1000000; i++) {\n    data.push({\n        'first-col': i + 1,\n        'second-col': i * 2,\n        'third-col': i * 3,\n        'fourth-col': i * 4,\n        'fifth-col': i * 5\n    });\n}\nfunction cameliseColumnNames1(data) {\n    const tmp = data[0];\n    for (let prop in tmp) {\n        const camel = pgp.utils.camelize(prop);\n        if (!(camel in tmp)) {\n            for (let i = 0; i < data.length; i++) {\n                const d = data[i];\n                d[camel] = d[prop];\n                delete d[prop];\n            }\n        }\n    }\n}\nfunction cameliseColumnNames2(data) {\n    const firstRow = data[0];\n    const camelObj = {};\n    Object.keys(firstRow).forEach((prop) => {\n        const camel = pgp.utils.camelize(prop);\n        if (camel !== prop) {\n            camelObj[prop] = camel;\n        }\n    });\n    for (let i = 0; i < data.length; i++) {\n        const row = data[i];\n        Object.keys(camelObj).forEach((key) => {\n            row[camelObj[key]] = row[key];\n            delete row[key];\n        });\n    }\n}\nfunction test() {\n    const start = Date.now();\n    cameliseColumnNames2(data); // change it between 1 and 2\n    const duration = Date.now() - start;\n    console.log(duration);\n}\ntest();\n```\nAnd that's your original version, without the extra check even.. I have done a lot more tests, and can state conclusively, your tests are wrong. The implementation that I provided is as fast as can possibly be. You cannot do it faster. Something is wrong with your measurement.. I don't see how this is possible. Your algorithm is doing the same as mine, except that you are doing it in 2 steps what mine is doing in 1. I do not see how it can be faster.\nYour condition if (camel !== prop) is covered by mine if (!(camel in tmp)), i.e. I do all in one step, so your algorithm adds nothing extra, except it makes it longer.\n. I have increased the number of columns, and I still see your algorithm being slower than mine. I don't know what kind of tests you are running, but it doesn't seem right.\n. Again, what is your version of Node.js?. Tried that, the results are the same.\nOh, and your algorithm also will show very poor performance when only a subset of column needs to be changed. You may not know it, but delete is the heaviest operation in JavaScript, and you are using it for all the columns, even when it is not needed at all.\n. Anyway, I can see this is not going anywhere, so I want to park where it is ;). @Halynsky You cannot use this syntax in this case, as it is only suitable for simple cases, where conversion is not required. For more complex cases you would just need to manually specify the fields.\nThe comprehensive solution for inserts would be in using the helpers namespace. See this example: https://stackoverflow.com/questions/37300997/multi-row-insert-with-pg-promise\nhelpers:http://vitaly-t.github.io/pg-promise/helpers.html. See #39 (duplicate)\n. This issue isn't related to implementation of pg-promise.\nA quick search shows the following discussions, with all the pointers:\n- Heroku \u201cpsql: FATAL: remaining connection slots are reserved for non-replication superuser connections\u201d\n- Heroku \u201cpsql: FATAL: remaining connection slots are reserved for non-replication superuser connections\u201d\n- PostgreSQL ERROR: no more connections allowed\nYou should research this yourself. I'm not familiar with this Heroku-related issue, unfortunately.\n. Did you try this? - \n\nYou might be able to find why you have so many connections by inspecting view pg_stat_activity:\nSELECT * FROM pg_stat_activity\nMost likely, you have some stray loop that opens new connection(s) without closing it.\n\nfrom http://stackoverflow.com/questions/13640871/heroku-psql-fatal-remaining-connection-slots-are-reserved-for-non-replication\nAnd what did it say in the activity?\n. I wish I could help, but it does seem related to Heroku only. If you find out what it is, please let us know here.\n. @kimmobrunfeldt have you figured this one out?\n. They may have placed a micro-cap on the number of connections for free plans that it doesn't even work. Whatever it takes to get people to upgrade, is their moto :)\n. Check out - my latest meditation about the query performance: Performance Boost ;)\n. For that you should use connect and error events.\nSee how pg-promise does it when testing itself: \n- https://github.com/vitaly-t/pg-promise/blob/master/test/dbSpec.js#L171\n- https://github.com/vitaly-t/pg-promise/blob/master/test/dbSpec.js#L17\nWhen there is a connection-related error, your event error will have property cn set on the context parameter. On the context object.\nYou can also see how this works in action, if you use pg-monitor. Here's an example.\nBy the way, calling this:\njs\nvar db = pgp(\"postgres://this:is/just:999/silly\");\ndoesn't try to open a connection. You would have to call db.connect() to make it open the connection, if this is what you want to test.\n. Here's a complete application for testing an invalid connection:\n``` js\nvar options = {\n    error: function (error, e) {\n        if (e.cn) {\n            // A connection-related error;\n            console.log(\"CN:\", e.cn);\n            console.log(\"EVENT:\", error.message);\n        }\n    }\n};\nvar pgp = require(\"pg-promise\")(options);\nvar db = pgp('invalid connection string');\ndb.connect()\n    .then(function (obj) {\n        obj.done(); // success, release connection;\n    })\n    .catch(function (error) {\n        console.log(\"ERROR:\", error.message);\n    });\n```\nYou would see that the same error is reported within .catch and event error. In the event you also get e.cn - secure connection details (with password hashed, if present).\nFor me it outputs:\nCN: invalid connection string\nEVENT: password authentication failed for user \"Vitaly\"\nERROR: password authentication failed for user \"Vitaly\"\n. You are welcome!\nSee my latest meditation on the query performance: Performance Boost ;)\n. Answer to the question was later elaborated here: Verify database connection with pg-promise when starting an app\n. Thank you!\n. One can do this in 3 ways:\n- Event query - the standard way\n- Use pg-monitor, which does a comprehensive event logging\n- If you just want to check one particular query, you can call pgp.as.format, which is what the library uses to format queries.\nI answered first by reading the body of your message. Following its title though, there is event error for that also, just as there is an event for everything else there ;)\nA better documentation for event query is within the API.\n. > Is it possible to have the notice generated by one particular query ? \nWhat kind of notice are you talking about? You asked about a notice with the query details, and event query gives you all that detail.\nThis is a very unusual idea:\njs\nevent.client.on('notice', function (notice) { console.log(notice) })\nI'm not sure what is it you are trying to get the notice of with this... Events query and error already have everything in them, you don't need to set up anymore handlers.\n. Ok, I'm not sure how that really works under the hood. How did you set up the listeners before - on the library level or on the connection level?\nThis library uses the connection pool provided by node-postgres.\n. I remember somebody else tried to do something similar: https://github.com/mfrasca/ghini/blob/e1404e8c3e1392c32813b2d813bfd43866de39b4/web.js\nYou see, the event system in node-postgres works against its own concept of the connection pool, that's why if you want to user the old listeners, you'd have to connect to it using the old style.\nThat piece:\n``` js\nvar pgp = require('pg-promise')(/options/);\nvar db = pgp(dburl);\nvar client = new pgp.pg.Client(dburl);\nclient.connect();\nclient.query('LISTEN \"watchers\"');\nclient.on('notification', function(data) {\n    console.log(data.payload);\n});\n```\nNot a nice looking one, but it seems to be working. Other than that there is no connection within the library to attach to, it's all virtual, and everything is going through the dynamic connection pool.\nWith this approach you would effectively occupy one connection permanently.\nIt isn't as bad though, since what you are trying to achieve isn't standard, more like an exception, few people want that.\n. Exactly, they have to bypass the pool, because the listeners are supposed to be static, while the pool is dynamic.\n\nDoes this mean that they're bypassing the pool ? Are clients created in this way limited to one query at a time (I would expect it to be the case) ?\n\nI'm not quite sure, would need to test that... but most likely - yes.\n. @ceymard thank you!\n@ceymard just to keep in mind, fixing your code to a static connection isn't very good, even the author of node-postgres is trying to discourage developers from doing it, stating that the pool is the recommended approach. The global event listeners on another hand - they are awkward.\nI wrote many postgres apps, never bothered with those listeners.\nFor general error handling, event error is the best approach, it gives you great extent of details.\n. @ceymard , @jmealo \nRelease v.4.3.4 has some improvements in this area, to simplify the use of connections.\njs\n// this will get Client from the connection pool:\ndb.connect()\n    .then(function (obj) {\n        obj.client.query('LISTEN \"watchers\"');\n        obj.client.on('notification', function (data) {\n            console.log(data.payload);\n        });\n    });\njs\n// this will create a new Client outside of the pool:\ndb.connect({direct: true})\n    .then(function (obj) {\n        obj.client.query('LISTEN \"watchers\"');\n        obj.client.on('notification', function (data) {\n            console.log(data.payload);\n        });\n    });\nNote how we use obj.client.query instead of obj.query. We can use both, but if you use the database  object methods, those return promises, which means you would need to chain them.\nIn both cases, you can either call done() on obj when you want to release the connection, or never, which in this specific case is acceptable.\nSee the connect API for details.\n. This has been properly documented: LISTEN / NOTIFY\n. What migration? :)\nThis library doesn't even use any notion of a schema. This is not an ORM, it formats and executes generic queries, that's its whole purpose.\nData migration relates to the database architecture, which is outside of this library's purpose.\n. @leemhenson there are some packages out there that deal with the data migration, like this one: node-postgres-migrate.\n. I understand. And that demo only shows you the best way to organize all your queries and the application structure, if you want to develop a proper product.\nOther than that, this framework, as defined, is an advanced promise-based access layer on top of the very basic node-postgres.\nThere is plenty of documentation to help you understand it better. See it in WiKi.\n. How is it you are using await without Babel? await is part of ES7, it is not directly supported by Node.js\nIf you follow the example of using tasks and transactions, it gives you plenty of examples of how to use both, including the ES6 generators.\n. Well, that example doesn't use pg-promise, it uses something else.\n\nI just really want to be able try/catch async code.\n\n.then and .catch in promises is what already does it for you.\nAnd when you call method tx for transaction, it catches exceptions, and rolls back automatically.\n. As per my previous post, transactions in pg-promise are fully automatic, you do not need to do anything extra, just use method tx to execute them.\n. Here's a complete application for example: https://github.com/vitaly-t/pg-promise/blob/master/examples/transaction.js\n. I posted it above. Here again: https://github.com/vitaly-t/pg-promise/wiki/Learn-by-Example#tasks\n. N.P.\nJust to note that in this library transactions are tasks that automatically execute BEGIN/COMMIT/ROLLBACK.\n. And as for your misfortune with Babel, see this post: http://stackoverflow.com/questions/33624104/how-do-i-setup-babel-6-with-node-js-to-use-es6-in-my-server-side-code\n. If you are really using QueryFile like this:\njs\nreturn t.one(app.services.datastore.pg._sql('insertDeviceTokenRecord.sql'), paramObject);\nthat is wrong, because as per documentation here:\n\nYou should only create a single instance of QueryFile per file, and then use that instance throughout the application.\n\nand here:\n\nFor any given SQL file you should only create a single instance of this class throughout the application.\n\nAlso see pg-promise-demo for a good example of how it should be used, although it is mostly highlighted here as well.\n. All the different dialects is a matter of personal preference. I for one, never used TypeScript, always stick to the JavaScript.\nSome libraries do include more than one dialect in the same project, others publish them as separate repos. In this case, since I'm not a TypeScript developer at all, the second one seems more appropriate. I would be in no position to support it anyway.\nI will have no problem listing it on the project's main page.\n. Alright. I just went through the script, it looks definitely out of my experience, although I think it is still missing many things there...\nJust a few examples:\n- queryResult type\n- task/tx interface\n- QueryResultError type\n- TransactionMode type\n- The entire formatting namespace\nIt is easier to see by looking at the API.\n. @jcristovao I just noticed that the version you are trying to convert to TypeScript is quite old, and most importantly, it is prior to 3.0.5, which saw major code restructuring.\nI would suggest relying either on the very latest release or the current master branch.\n. @jcristovao any progress there? ;)\n. @jcristovao left a comment here: https://github.com/DefinitelyTyped/DefinitelyTyped/pull/8421\n. All query methods of the library resolve with the data that the query itself provides, unless the method name contradicts the expected data size.\nFor details see their API documentation, including method query.\n. See issue #49 for this.\n. In the latest version, one can improve that example with method as.name:\n``` js\nvar pgp = require('pg-promise');\n// sets all the object's properties as they are;\nfunction SetValues(obj) {\n    this._rawDBType = true; // raw-text output override;\n    this.formatDBType = function () {\n        var props = Object.keys(obj);\n        var s = props.map(function (p) {\n            return pgp.as.name(p) + ' = ${' + p + '}'; // creating the formatting parameters;\n        });\n        return pgp.as.format(s.join(\", \"), obj); // returning the formatted string;\n    }\n}\n```\nAlternatively, you can just return an object, if you don't like using new. Below is the complete example updated with ES6 syntax:\n``` js\nvar pgp = require('pg-promise');\nfunction setValues(obj) {\n    return {\n        _rawDBType: true, // raw-text output override;\n        formatDBType: () => pgp.as.format(\n            Object.keys(obj)\n                .map(p => pgp.as.name(p) + ' = ${' + p + '}')\n                .join(\", \"), obj)\n    };\n}\nvar test = {\n    first: 123,\n    last: \"text\"\n};\nvar result = pgp.as.format(\"UPDATE users SET $1\", setValues(test));\nconsole.log(result);\n```\nOutput:\nUPDATE users SET \"first\" = 123, \"last\" = 'text'\n. With WHERE it can be even simpler, if your columns are not dynamic.\nSee example here\n. F.Y.I. Important breaking change in v6.5.0.. Not directly, as this library is not an ORM.\nBut it has a very powerful query formatting engine, which can easily generate any kind of queries.\n``` js\nvar pgp = require('pg-promise');\n// Helper to generate an INSERT query from:\n// - table name\n// - object with values\nfunction insert(table, values) {\n    var keys = Object.keys(values);\n    return pgp.as.format('INSERT INTO $1~($2^) VALUES($3^)', [\n        table,\n        keys.map(k => pgp.as.name(k)).join(', '),\n        keys.map(k => '${' + k + '}').join(', ')\n    ]);\n}\nvar values = {\n    a: 1, b: true, c: 'hello'\n};\nvar query = insert('Table Name', values);\nconsole.log(query);\n```\nOutput:\nINSERT INTO \"Table Name\"(\"a\", \"b\", \"c\") VALUES(${a}, ${b}, ${c})\nSee also:\n- issue #49, issue #89 \n- SQL Names\n- Raw Text\n. > Why not to add this helper function to the library?\nToo many variations, it is not generic enough.\nExamples:\n- insert that returns a raw/value would end with something like RETURNING */RETURNING id\n- insert that uses an internal SELECT as the source of values\n- multi-value insert, see Performance Boost\n- Applying filters on column names\n. Those would have little value as separate functions.\nFor example:\njs\nfunction getKeyNames(values) {\n    return Object.keys(values).map(k => pgp.as.name(k)).join(', ');\n}\nBut the important part would be to use the same list of keys (sometimes you may want to filter them).\nAnyway, I'll think about it.\n. This library doesn't support syntax like @1~keys. It supports syntax $1~ for inserting an sql name/identifier. The current syntax allows easy custom formatting, see Custom Type Formatting.\nThe current formatting is already advanced enough, most users don't use all its nice features. I wouldn't consider advancing it further.\nIn addition, your idea goes against the custom formatting syntax, which allows any type of customization, while your syntax fixes it to a single possible outcome, while that also may need advanced formatting options, such as keys transformation/filtering.\nWith that insert I gave you the simplest approach on which you can easily build to fit your app's needs. That's the way it works best.\n. There is a further progress with this in 4.0.6 update ;)\n. UPDATE:\nThere is lots more the library offers now with the helpers namespace when it comes to automatic query generation for INSERT or UPDATE operations.\n. Syntax for SQL Names has been extended in v.5.2.1, to support enumeration of keys.\n. With the latest version of the library, filter :csv / :list will enumerate values. See CSV Filter.\nexamples\n```js\nconst obj = {first: 123, second: 'text'};\ndb.none('INSERT INTO table($1:name) VALUES($1:csv)', [obj])\n//=> INSERT INTO table(\"first\",\"second\") VALUES(123,'text')\ndb.none('INSERT INTO table(${this:name}) VALUES(${this:csv})', obj)\n//=> INSERT INTO table(\"first\",\"second\") VALUES(123,'text')\n```. See Tasks examples. Specifically the ES6 generator version is nice ;)\nAnd since you are using inserts, a transaction would be more appropriate ;)\n. P.S. Since you've just started looking through the library, have a look at pg-promise-demo ;)\n. What you are describing has been available for a while, as Custom Type Formatting.\nYou simply wrap your non-standard types into a self-formatting class or object.\nExample:\n``` js\nvar pgp = require('pg-promise');\nfunction MyJsonArray(data) {\n    this._rawDBType = true;\n    this.formatDBType = function () {\n        return pgp.as.json(data);\n    }\n}\nvar test = new MyJsonArray([1, 'two']);\nvar s = pgp.as.format(\"FORMAT: $1\", test);\nconsole.log(s);\n```\nOutput:\nFORMAT: '[1,\"two\"]'\nAlternative syntax:\n``` js\nfunction myJsonArray(data) {\n    return {\n        _rawDBType: true,\n        formatDBType: function () {\n            return pgp.as.json(data);\n        }\n    };\n}\nvar test = myJsonArray([1, 'two']);\n```\nWhat's more, since your array is already an object, it makes things even easier, because you can extend the object itself just as you get the data:\n``` js\nfunction asJsonArray(data) {\n    data._rawDBType = true;\n    data.formatDBType = function () {\n        return pgp.as.json(data);\n    }\n}\nvar test = [1, 'two'];\nasJsonArray(test); // extend the array for self-formatting;\n```\n. > Both require manual modification of the input parameters\nNo, see the code example again. The custom type formatting doesn't require any manual modification, its result is the output of the standard variables, as shown in the example:\njs\nvar s = pgp.as.format(\"FORMAT: $1\", test);\n. What you are looking for is exactly the manual mode, while what I suggested is the automatic mode hence the confusion...\nPerhaps in your case the manual call to as.json is the best option then. It is not likely that I would be extending the manual formatting overrides, too many variations. Custom Type Formatting is what's usable in most cases, yours is quite unique, if you can't simply extend the array object once you get the data, although I'm not sure why.\n. I know what you mean, but this isn't supported as of now, not sure if will ever be. It's all manual formatting, just from a different side.\n. I would do it myself (since I do everything with lots of tests + documentation), if I thought of it as a necessity, not quite sold on that one :) From the general usability point this is hardly ever needed, only in very special cases, and only as a minor convenience.\n. By any chance can this be remedied by the recent addition of the partial formatting support?\nIt was introduced in version 3.2.0.\nIf it is just a sub-parameter within your SQL file, then it is exactly what parameter params solves in the latest release of the QueryFile.\n. yeah, those are static ones, applied once, after loading the SQL.\n. Ok, maybe the partial formatting itself will be of help at least. You can pre-format the query dynamically now (in fact as many times as needs to be). That's the last ace I can throw back at ya :)\njs\npgp.as.format(query, values, {partial: true});\n. yep.\n. looks that way ;)\n. @nlf I have extended the formatting syntax with the following additions:\n- :raw = ^\n- :name = ~\n- :json - is what you were looking for.\nThe code has been checked in for 2.3.1, but it still needs a number of tests before it is released, plus many documentation updates.\n. I don't see why suddenly change other data types into something else. JSON is a special case, I think.\n. CSV - that is interesting...\n. This has been released as version 3.2.1\nBoth :json and :csv are supported.\nDo not mind the broken build status, Travis CI is down at the moment.\n. you are welcome!\n. @nlf I gave it some serious refactoring within version 3.2.2 ;)\n. @nlf how did you get along with the changes? Is everything working for you as expected?\n. @nlf that's good :) I went through all the recent changes you made but didn't see any use of this new syntax, that's why I asked. Did I somehow miss it or is it still in the pipeline?\n. @nlf I hope this new feature won't affect your library ;)\n. F.Y.I. Important breaking change in v6.5.0.. Node.js parallelism is achieved through IO. Other than that Node.js itself is synchronous.\nTranslated into the logic of the given example, even though we do not chain the promises, they cannot jump the sequence because they are executed within a transaction, and any transaction executes through a single connection / IO channel, so the sequence is enforced on the IO channel level.\nBut if it gives you more confidence, you can always promise-chain those requests, though it won't change the sequence. At least this is what the practice has proven.\nYou would need to execute requests on the root level, so the connection is allocated independently from the pool, in which case the actual parallelism is possible, if you end up with more than one connection allocated.\n. You are mostly correct, and the only part that you are missing is not even related to the library, it is related to the difference in PostgreSQL of presenting results for the two types of queries:\n- SELECT FunctionName(...)\n- SELECT * FROM FunctionName(...)\nYou need to use the latter to get what you are expecting. If you run the same commands against PGSQL, and you will see the difference.\nAlso, instead of * you can specify the exact columns, if you want.\n. N.P. ;)\n. P.S.\nAbout code like this one:\njs\nreturn db.one(`select get_account_details('${col_name}', '${col_value}'::text) as result`);\nyou should never format values like this, because es6 template strings know nothing about proper formatting for postgresql, only the library knows how to do it. You will end up breaking your queries very often otherwise.\nIt should be something like this:\njs\nreturn db.one(\"select * from get_account_details(${col_name}, ${col_value}::text)\", obj);\n. This might be of help also: Invalid query formatting with manual string concatenation and ES6 template strings\n. Is this one not good enough?\n;)\n. What is the point of such an example?\nYou use generators inside transaction to simplify the dependency chain. But when you need to execute a batch, you just execute it directly.\nIf you describe what it is you are trying to do, then I should be able to advise as to how to do this properly.\nPurely as an exercise, you could do:\n``` js\nfunction * a() {\n    return yield this.one(\"select 'A'\");\n}\nfunction * b() {\n    return yield this.one(\"select 'B'\");\n}\ndb.tx(t => {\n        return t.batch([a, b]);\n    })\n    .then(function (data) {\n        // success;\n        console.log(\"DATA:\", data);\n    })\n    .catch(function (error) {\n        // error;\n        console.log(\"ERROR:\", error);\n    });\n```\n. 1. No\n2. Not sure what you mean. If you mean just to pass parameters into generators - you can do so indirectly.\n3. yes, you can yield anything that returns a promise.\nGenerators support within pg-promise is limited to what you can run immediately inside a task or transaction.\nIf you want a wider support for the new async syntax, using async and wait, you should use babel or something like that. This is what people are using here mostly when they want to go for ES7.\nYou do not need generators at all, in order to implement a strict sequence.\nIs your sequence dynamic or pre-determined? Why can't you just pass queries into method batch? You can always put your query generation into separate functions, and they don't have to be generators for that.\n. Promises were created to solve the callback hell, and if you code it right, promises are  very easy to read and maintain. Take a shot at your project, and when you get it working, I will have a look to see if there is a better/simpler way ;)\n. Method as.csv escapes all the data correctly, according to its type. So no, it is not vulnerable in any way.\nAlso note the alternative syntax since version 3.2.1, which results in the exact same thing.\n. Have you seen this? - Performance Boost\n. You are welcome! :)\n. There is a further progress with this in 4.0.6 update ;)\n. Cheers! I keep working on documentation and tests. Any technical feedback would be nice ;)\n. The answer you are looking for isn't really about this library, it is about the repository pattern, and the way it applies to full-scale applications.\nYou start with table-level repositories that implement basic operations like shown in that demo app.\nThen you write higher-level repositories, so-called business-level/logic repositories that encompass the logic of your application and/or the table relationship.\nFor example, if your database has tables USERS->SALES->PRODUCTS, you can add repository Purchase that will use other repositories and perhaps have its own direct access to implement the process of selling products.\nThere are many resources that explain the repository pattern better that I can do here:\n- https://msdn.microsoft.com/en-us/library/ff649690.aspx\n- http://stackoverflow.com/questions/11985736/repository-pattern-step-by-step-explanation\n. The repositories are completely independent of each other, and you add all of them within event extend in the same way.\nAnd you do not need any relationship between repositories to implement a transaction.\n. I'm closing this issue now. If you still have questions, StackOverflow is a better place to ask them, as I may not always have the time, and this place should be used for reporting bugs or making change requests.\n. Directly related: #81\n. If it is ok for you to throw in an extra variable into each query, you can add your own Custom Formatting Type that would simplify the query addition, plus do the extra stuff you want.\nOr else, if you do not want to modify queries and want it to happen for every single query, you can use event extend to introduce your own query method that can pre-format each query.\nAnd this wouldn't be a feature request, you can accomplish it easily within the existing version of the library.\n. @jmealo Note to you, since we last discussed this, a new feature was added to the library, which may also come of help, depending on which way you will decide to do it: as.format has been extended with parameter options.\nP.S. Anyhow, I'm done for today, till tomorrow now)\n. @jmealo No feedback?\n. @jmealo Anything? You can re-open it when you have something.\n. I gave you 2 possible approaches earlier. Have you tried them?\n. > they lack the necessary scope to access the session object corresponding to the request\nWhen you add your own method via event extend it can take the session parameters and append/pre-prend the extra request you need.\nAnd such methods always execute within the current connection session.\n. Node.js enforces singularity of each modules loaded via require. It doesn't matter how many times you call it, you will end up with the same instance of the module. It is the db instance that would differ, if you are trying to represent different databases. And event extend will be called for each of the db instances that you have separately.\n. > If I could pass in the request context into each query method and then extend it to look for that property and append/prepend to the query that could work.\nSo add such method via event extend and use it. I don't see the problem or relevance of the HTTP that you describe. As far as pg-promise is concerned, this is all very generic.\n. Your 2 options:\njs\n// with extra parameter:\nvar options = {\n    extend: obj=> {\n        obj.myMethodWithParam = (query, values, context)=> {\n            // append/pre-pend query parameters, using the context;\n            return obj.query(query, values);\n        }\n    }\n};\njs\n// without extra parameter:\nvar options = {\n    extend: function(obj) {\n        obj.myMethodWithoutParameter = function(query, values) {\n            // append/pre-pend query parameters, using either\n            // a global context, or 'this' context.\n            return obj.query(query, values);\n        }\n    }\n};\n. If you want it to work through tasks and transactions, then perhaps relying on this parameter is better.\nAlternatively, you can pass the context into task/transaction easily as either tag or this context.\n. ``` js\n// passing context via tag:\ndb.task(myTag, function (t) {\n});\n// passing context via this:\ndb.call.task(myContext, function (t) {\n   // t.ctx.context = the calling context;\n    var context = t.ctx.context;\n});\n```\n. This looks quite inefficient, and won't work within tasks.\n\nI believe you said that if I use ES6 multi-line strings that I cannot use your custom goodies. \n\nI don't believe I ever said that, as it's not true.\n. This approach will not work with tasks and transactions. Event extend is the only correct way to extend the protocol.\nI never really understood the exact nature of the problem you were having in using event extend, and why the context didn't work for your there.\nIf you want my help in this, write a simplified app with a very simple concept of what you are doing and why it doesn't work the way you expected. Then I may be able to point you in the right solution.\nOtherwise, we won't make any progress here.\n. > In the documentation, the extend example defines an options object. I assume that this is to pass to the pg-promise constructor on a per-instance basis.\nNo. This refers to the global, once-off Initialization Options object for the entire library.\n\nHow do I use extend to override the built-in methods so that all use of pg-promise?\n\nYou do not override anything predefined in the protocol, you add new methods and properties that can extend the existing ones with extra or specific functionality.\n\nI also do not see how extend allows me to override the behavior of the built-in methods.\n\nI doesn't override anything, it extends.\n. @jmealo I had another look at how this library handles its own global instance, or to put otherwise - how to support more more one instance.\nI found some problems here, which may be in part what you were talking about. So, I have started working on it - to make it better, to support multiple instances properly.\nThis seems like a big change, but an important one.\nI have just finished doing it for spex, which is used by pg-promise. That library has (had) the same problems, being unable to properly handle its own multi-instance mode.\n. Update spex v. 0.4.2 has been released, with proper multi-instance support.\nNow I can start doing it for pg-promise, with the purpose to completely isolate separately initialized instances of the library.\n. @jmealo if you want to follow this up, I have created https://github.com/vitaly-t/pg-promise/issues/114\n. All updates are here: https://github.com/vitaly-t/pg-promise/issues/114\nCheers! :)\n. @jmealo this may or may not help you in your project, but version 3.8.0 received support for the database context, which further improves multi-database or status-full databases.\n\nextend is global though.\n\nIt is not that global anymore, as the database context is now available there, same as everywhere else.\n. There is only one solution to this. Since in such special case PostgreSQL parser cannot deduce type from the data, you have to cast it explicitly:\nsql\nINSERT INTO table (other_field, tags) VALUES('something', array[]::text[]);\ni.e. change your query to:\njs\nvar stmt = `INSERT INTO table (other_field, tags) VALUES($1, $2::text[])`;\n. Release v6.7.1 changes these things.. There are events connect and disconnect that represent virtual connections, i.e. from the connection pool.\nThis library works only via the connection pool, so you never physically/directly open or close connections, only virtually, which you can monitor through those events.\n. This is documented in Library de-initialization and in all examples.\n. I've never used it. If you can set it up, then please - go ahead.\n. @jgoux it is done: https://gitter.im/vitaly-t/pg-promise\n. @jgoux thank you for bringing this up, it does look very useful :)\n. Added through a manual merge instead.\n. My understanding of it is that there are multiple ways to insert a Buffer into the database, but only one way to read it. A hex string is just one presentation for the Buffer.\nI need to do a proper research here...\n. @nonplus If I find that there is one way to do it, I will implement it. But I won't be able to accept a PR on the formatting engine, for the following reasons:\n- I'm currently working on 3.4.0 with a few changes for the formatting engine, and do not want merging conflicts now.\n- $as.buffer(value, raw); doesn't adhere to the general logic anyway. First, all formatting methods are named to represent to destination, not the source, and perhaps it wouldn't make sense supporting raw presentation for the Buffer type.\n- I do not release the library without 100% tests and API documentation for the new methods.\nAnd the whole thing requires to think about.\n. Current findings...\n- Buffer requires 3 parameters: encoding, start + end. We would have the default to hex, 0, buffer length, which I'm not sure if a good thing.\n- Buffer may need to be stored as JSON\n- Buffer may need a different formatting for :json and :csv filters.\n. I still would have to cater for all the extra formatting overhead, like use of filters :json and :csv.\nFilter :json should convert Buffer into a JSON string, and filter :csv perhaps should convert it into CSV of byte values - i'm not even sure.\n. I've started making some changes, and changed as.csv to this:\njs\n    csv: function (values) {\n        values = resolveFunc(values);\n        if (values instanceof Buffer) {\n            return values.toJSON().data.join(',');\n        }\n        return formatCSV(values);\n    },\nThat will return something like: 1223,2143,4545, i.e. byte values separated by comma. Not sure there is any value in it, but at least it is something consistent.\n. This would mean that we format Buffer type differently from [Buffer1, Buffer2,...], which in turn would make it inconsistent with the rest of the formatting engine.\nThe question is - what good is binary conversion when inserting records? The whole point of the formatting engine is to generate something that can be used directly in queries. And I don't know how binary can be used in queries. Any example?\n. scrapped that :)\n. Frankly, none, perhaps. But if one calls and we do not support it - we need to throw an error of some sort. I think that somehow logical support is better than adding new types of errors in the midst :)\n. @nonplus initial implementation + 100% tests have been added: https://github.com/vitaly-t/pg-promise/commit/b76c85d2aea4b381898ecaee4484999ccc56f7e8\nThe way you showed in the example it should work correctly ;)\nYou can clone it and test it ;)\n. There was a build error, because as it turns out, Buffer was converted into JSON differently under Node.js 0.10.x. I have fixed that and checked it in again ;)\nNow there's lots of documentation to be updated before this all is released.\n. > I think the CSV case is inconsistent with the other types where someValue and [someValue] result in the same CSV representation.\nI believe it is consistent now. The library formats a single value the same way as [value].\n\nIn any case, you shouldhave a CSV test case where Buffer is an array element.\n\nI did include this into the tests, see https://github.com/vitaly-t/pg-promise/blob/master/test/formatSpec.js#L29\n\nIf it were me, I would make as.csv stricter and have it throw errors if caller doesn't pass in an array\n\nThat would make it a huge breaking change, because all function calls rely on as.csv that is often used with a single parameter.\n\nor if any of the array elements aren't primitive values (null, Number, String, Boolean, Date, Buffer) since CSV doesn't support structured values (arrays, objects)\n\nCan't do it, method as.csv is used to format values for function calls.\n\nRight now, (from your unit test) you convert [1, [2, 3], 4] to \"1,array[2,3],4\" which is actually the CSV representation of [1, \"array[2\", \"3]\", 4]. Was that your intention? I would either expect an error or 1,'[2,3]',4\n\nAs explained earlier, if of the main purposes of the method is to format parameters for function calls.\n\nBTW, CSV is encoding binary characters as as 3-digit octal sequence. The following function should convert a Buffer to its CSV representation:\n\nThe problem, as we discussed it earlier, the purpose of such formatting is unclear. I can't think of any practical example. It least it won't throw an error, and do something logical instead.\n. If I were to throw in that exact test, it would work like this:\njs\nexpect(pgp.as.csv([23, new Buffer([1, 2, 3]), \"Hello\"])).toBe(\"23,'\\\\x010203','Hello'\");\nI do it in parts instead, but I tested it, and that's how it works ;)\nOk, I will check that test in also ;) for the peace of mind :)\nBy the way, the following would produce the same results:\njs\nexpect(pgp.as.format(\"$1,$2,$3\", [23, new Buffer([1, 2, 3]), \"Hello\"])).toBe(\"23,'\\\\x010203','Hello'\");\nexpect(pgp.as.format(\"$1:csv\", [[23, new Buffer([1, 2, 3]), \"Hello\"]])).toBe(\"23,'\\\\x010203','Hello'\");\n. @nonplus here I have checked in the additional tests: https://github.com/vitaly-t/pg-promise/commit/30fe63ce697b99adb13297e80472cd0d32c98a6d\n. Version 3.4.0 has been released, with all the new features as planned.\nI also updated the example here for using type Buffer.\nIf you find any issues with support for Buffer, feel free to open a new issue, and reference this one. But for now the feature has been finished, and it seems to be working fine, with a comprehensive set of tests to support that.\n. If you look at the end of this post: https://github.com/vitaly-t/pg-promise/issues/105#issuecomment-198863916\n\nThe problem, as we discussed it earlier, the purpose of such formatting is unclear. I can't think of any practical example. It least it won't throw an error, and do something logical instead.\n\nWhat I meant by that, I was going to leave the initial implementation I did, converting Buffer as 1,2,3 when it is used as a value directly.\nThis is exactly what I did and how I tested it.\nThat's where the misunderstanding came in. That's why the actual result is what I was expecting.\n. I'm thinking it over now, and think you are right, that output should be changed, for consistency with simple values.\n. > Also, in the documentation, please make it clear that csv refers to a comma-separated list of PostgreSQL literal values and not Comma-separated values used in PostgreSQL's COPY FROM/TO CSV command. This had me confused while discussing #105.\nEven that very link says:\n\nIn popular usage, however, the term \"CSV\" may denote some closely related delimiter-separated  formats, which use a variety of different field-delimiters\n\nSo, the way it was phrased there was correct.\n. Corrected in v.3.4.1.\n. The most important error in the library, also one that is the most common to occur, is the mismatch between the type of method used and the number of rows returned, which indicates that whether you are handling invalid data set or there is an issue in the database.\nWhenever this happens, the library produces QueryResultError, which is referenced to from every query method in the API.\nSo there is your custom method, for the most frequent type of error.\nOther types of errors, created by an invalid query itself, are indeed reported via generic Error, but that's because this is how it arrives from node-postgres.\nAnd if you have a connection-related error, it is separated easily from the rest. See http://stackoverflow.com/questions/36120435/verify-database-connection-with-pg-promise-when-starting-an-app\n. Another very common example, with the use of external SQL files via QueryFile. When it fails to parse the SQL, it throws custom SQLParsingError.\nSo as you can see, custom errors are used a lot, in all important places in fact.\nIs there any particular type of error that gives you trouble?\n. This is exactly how such errors arrive from node-postgres, and pg-promise doesn't create any overhead on top of it.\nBut you can easily catch such errors. I'm seeing lots of developers use pg-error to simplify handling for such errors.\n. Cheers! :)\n. The type of error handling that you are looking for becomes so much easier if you just handle event error ;)\nThat way you can do it in just one place, while using individual .catch for generic error handling.\nNote to you, it is difficult to encompass everything within a DatabaseError. There are issue that relate to:\n- connectivity\n- query formatting\n- query running\n- data integrity violation\nThey are all quite different, and I wouldn't want to turn them all into a single error object. Such approach would be very questionable.\n. > So what is the way in your lib to hook up an errors and customize it?\nThere is no error customization in the library. The library provides you with all the error details. All you need is to parse is you you see fit.\n\nI don't find the way how to interface it with pg-promise\n\nI've just looked it it, and it seems kind of obsolete.\nYou can see how this guys just did it, and perhaps do something similar, for the special types of errors that you want to catch: https://github.com/evvvvr/q-and-a/blob/345476b509ad5ffb5e001d4428f583753b7c44d3/src/DbService.js\n. > Error thrown by pg-promise when an integrity constraint violation occurs\npg-promise doesn't throw those errors, node-postgres does, and pg-promises only passes them on.\nIf you want additional interpretation for such errors, you will have to implement it on your side. Parsing those error objects is simple enough.\n. Here's an example of how you can approach this:\n``` js\ndb.any(\"SELECT * FROM Users\")\n    .then(data=> {\n        // success\n    })\n    .catch(parseError)\n    .catch(error=> {\n        if (error instanceof MyCustomError) {\n            // do something...\n        }\n    })\n    .finally(pgp.end);\nfunction parseError(error) {\n    // parse 'error', and throw an error accordingly;\n    throw new MyCustomError(\"my custom error type\");\n}\n```\n. P.S. That's the best I can advise you. pg-promise won't do it for you, as it would completely break compatibility with all the previous versions. Developers are used to parsing node-postgres errors on their own.\n. > Thanks for your support and the time you took\nYou are welcome!\nIt is worth point out that while the raw errors may look like an inconvenience in some cases, they are consistent with how  node-postgres reports them, and many pg-promise users come from node-postgres, which makes the whole error layer transparent and 100% compatible, and those are good things ;)\n. @vieks \nNow that pg-promise supports native bindings, it will urk you more to know that errors are reported totally different under native bindings. This has been and continue to be a big pain for everyone who uses node-postgres, but no solution exists, as there is no alternative postgres driver.\nAs a hope, there is a discussion open lately about this, yet again: https://github.com/brianc/node-postgres/issues/972\n. The error is cryptic because what you were doing with the connection is wrong :)\nCorrected code:\njs\nlet pgp     = require('pg-promise')();\nlet cn = \"postgres://eddierozum:password@localhost:5432/ghSmart\";\nlet db = pgp(cn);\n. @eddievagabond did you get it working? ;)\n. no problem ;)\n. @eddievagabond recent version of pg-promise received an update that improve on such cases, to diagnose invalid library initialization and provide a more meaningful error about it ;)\n. Implemented on version 3.4.3.\n. From the Official Documentation:\n\nYou need only one database instance per connection details.\n\nYou have separate db instances for different connection objects/strings:\njs\nvar db = pgp(\"postgres://regular_user:pass123@host:port/database\");\nvar dbAdmin = pgp(\"postgres://admin:pass435@host:port/database\");\nThe same goes for connection objects.\n. > So, exactly as I did above, right?\nAhm, possibly, wasn't too clear to me :)\n. It still looks strange, as this:\njs\nvar pgp = new pgpLib();\nisn't used anywhere in the library...\nAnyway, the important thing is that my example is clear to you ;)\n. On an unrelated matter, you wanted to contribute into a TypeScript conversion of the library.\nIf you still want to do it, I'm looking at getting started with it, and I will make you a collaborator ;)\n. @jcristovao if you have time, we can chat here: https://gitter.im/vitaly-t/pg-promise?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n. > I was wondering if this can have a penalty due to lookup hash table used by JS engine \nNo, no penalty in such case.\n\nThe last one have more keys so must take more space\n\nMuch more space, because you are not considering the full syntax supported:\nfrom the comments there:\n\n... or multiple ones, if the order of executing such queries is unimportant\n\ni.e. you can write:\njs\n{\n   \"INSERT INTO Users(name, age) VALUES($1, $2)\": ['John', 123]\n   \"INSERT INTO Users(name, age) VALUES($1, $2)\": ['Mike', 456]\n   \"INSERT INTO Users(name, age) VALUES($1, $2)\": ['David', 789]\n}\n-for queries that can be executed in any specific order. For the ones where the order matters you put them in separate { ... }\nNot the best example, because for inserts you would concatenate them into a single insert, but you get the idea ;)\n. The bottom line, there is nothing complex there, you can tweak it any way you like, i.e. have completely your own version of joinQueries, if you like ;)\n. My version was the simplest and the shortest (to use). That's all ;)\n. @vieks Something that article doesn't mention yet...\nWhen you are designing the system yourself, much of the IO optimization can be done by moving query sequences into PostgreSQL functions ;)\n. Like I said, if you are designing the system ;) that implies you know what you are doing :)\n. @vieks I was thinking, that from the re-usability point of view, it would be better to have a version that uses an object property, like:\njs\n{query, values}\nbecause the example in the article is useless for dynamically loaded SQL ;)\n. @vieks with that in mind, I have re-implemented the example ;)\n. UPDATE:\nThere is lots more the library offers now with the helpers namespace when it comes to automatic query generation for INSERT or UPDATE operations.\n. This is a very standard JavaScript checking how the code was invoked - as a class or as a function.\nAnd if it was invoked as a function, it is automatically translated into a class call that way.\n. Enforce? You mean to throw an error? That wouldn't be friendly :)\nThat code makes it flexible ;) You can use it as a class or as a function ;)\n. n.p. ;)\n. F.Y.I. Important breaking change in v6.5.0.. to get summary message:\njs\ndata: error.message || error\nthis is not in any way specific to this library, this is just a generic error that arrives from PostgreSQL, and a generic way to get summary from any error context.\n. I have finished and checked in all the changes in the library necessary to isolate its initialization logic.\nBut then I ran into a problem that I logged here: https://github.com/brianc/node-postgres/issues/981\nWithout that problem solved, it is impossible to isolate initialization options regarding the use of Native Bindings. It needs to be fixed within the pg module first.\nWith the recent changes one can initialize pg-promise any number of times, using different promise libraries - all in parallel, and they all will work independently.\n. Released version 3.5.4 with the isolation done, pending on what will happen with this bug: https://github.com/brianc/node-postgres/issues/981\n. Release 3.5.5 finalizes the work in this direction, as there is nothing can be done till the issue with native bindings is resolved, which isn't likely to happen any time soon.\nTherefore, closing this for now.\n. That issue has been resolved, and version 3.6.3 has brought complete isolation for the first time.\n. This forum is mainly for reporting issues with the library, sometimes questions/suggestions about using it.\nYour questions is about writing SQL right, which is completely irrelevant to this library. You should ask it elsewhere, like on StackOverflow.com.\n. Your problem is that you do not know how to use INNER JOIN - which is what you need in your case. It is purely an SQL question, nothing to do with pg-promise as such.\n. > \u041d\u043e \u0434\u043e\u043b\u0436\u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u0442\u044c - \u043f\u0440\u0438\u044f\u0442\u043d\u043e \u0431\u044b\u043b\u043e \u0443\u0432\u0438\u0434\u0435\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u044d\u043a\u0440\u0430\u043d\u0438\u0440\u0443\u0435\u0442\u0435 \u043d\u0430 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0435, \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043a \u0434\u0435\u0442\u0430\u043b\u044f\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0443 \u0432\u0430\u0441 \u043d\u0435 \u043e\u0442\u043d\u044f\u0442\u044c.\n:)\n\u0412\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u0442\u044c\u0441\u044f \u0441 \u0442\u0435\u043c \u0447\u0442\u043e \u0442\u0430\u043a\u043e\u0435 INNER JOIN \u0438 \u043a\u0430\u043a \u0435\u0433\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c, \u0442.\u043a. \u0438\u043c\u0435\u043d\u043d\u043e \u0432 \u044d\u0442\u043e\u043c \u0443 \u0432\u0430\u0441 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430, \u0430 \u043d\u0435 \u0441 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u043e\u0439 ;)\n\u0415\u0441\u043b\u0438 \u0441 \u0430\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u043c \u043d\u0435 \u043e\u0447\u0435\u043d\u044c, \u0435\u0441\u0442\u044c StackOveflow.ru, Toster.ru \u0438 \u043f\u043e\u0434\u043e\u0431\u043d\u044b\u0435 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c.\n. \u0412\u043e\u043e\u0431\u0449\u0435-\u0442\u043e, \u0432\u0441\u0435 \u0434\u044b\u043d\u043d\u044b\u0435 \u0432\u0441\u0435\u0433\u0434\u0430 \u043f\u0440\u0438\u0445\u043e\u0434\u044f\u0442 \u043a\u0430\u043a JSON.\n. @sandrija see here: https://stackoverflow.com/questions/39805736/get-join-table-as-array-of-results-with-postgresql-nodejs. As a work-around, for the time being one can call pgp.pg.native.end() in order to terminate the connection pool for the Native Bindings.\n. It would appear that an important part of the protocol would have to be broken in order implement support for this new change:\nProperty pg and function end() would have to move into the post-initialization instance.\n. This has been resolved in version 3.7.0.\n. Thank you for your findings.\nI have spent so much time trying to resolve the issues with the declarations, it just doesn't work. And I'm out of time trying to resolve those issues, the seem to be without end.\nIn all fairness, I started to doubt the ability of TypeScript to properly describe libraries such as this one.\n. After much more effort and many questions asked on StackOverflow I was able to make some nice improvements.\nThe version currently checked in seems quite usable.\n. In the end, I have been able to solve all the problems, and made an official release.\nPlease get back to me on this one, so we can close this issue ;)\n. @awdogsgo2heaven You may find release 3.7.8 to be a good further improvement.\n. > Is there a way that I can make the count as a number without explicitly casting it using Number().\nNo.\nSee also in the demo: https://github.com/vitaly-t/pg-promise-demo/blob/master/JavaScript/db/repos/products.js\njs\n    total: () => rep.one(\"SELECT count(*) FROM Products\")\n            .then(data => parseInt(data.count))\nthat's the way to do it.\n. I honestly don't know. You can check that yourself. All type conversions are done by pg-types.\n. @OnionCoder did you get to the bottom of it?\n. Proper explanation: pg-promise returns integers as strings.. Writing TypeScript for pg-promise was my first TypeScript project ever. I was looking for contributors, but nobody wanted to help, so in the end I had to finish it all up by myself. \nThank you for taking time writing it all in detail, I will try to answer it item-by-item...\nFirst, I am surprised you couldn't get it to work, because not only the library itself has lots of tests included (folder test\\typescript), but pg-promise-demo uses it entirely, and no problem. Check it out, let me know if you have any problems running that demo.\nAre you using version 3.8.1 of pg-promise? If not, then it would be of no surprise, you must use that one ;)\nThe issue with pg.d.ts, its version on DefinitelyTyped is way more incomplete than mine. And as stated at the top of my version:\njs\n// Declaring only the part of the 'pg' module that's useful within pg-promise\nso I can't really use that one, I have to use my own version, that's why I wrote and included it.\nNext one...\n\nYou have to manually reference the typings, either using tsconfig.json or via  in a source file.\n\nThat's because this library doesn't come from Typings, it is distributed with pg-promise. Maybe someday it will.\n\nThis is prone to breaking (when refactoring) and generally not very nice.\n\nHow would you break it? I can't think of one way. The path with node_modules will remain the same always.\n\nYou should consider exporting some/all of the interfaces, so you could import them directly without having to use * as.\n\nOne of the ideas around ambient declarations is to replicate the existing interface precisely. When you start exporting things that are not available in the original library you are deviating from the official protocol, which isn't very nice.\n\nSome functions could be typed a bit better. For instance, the query methods in IBaseProtocol should take string | QueryFile | IPreparedStatement instead of any (you have to write an interface for the last one).\n\nFor one thing, there is no PreparedStatement in the library. Instead, there is only direct support for Prepared Statements, using {name, text, values}. Basically, there are so many variations, I simplified it for the time being. Possible to improve in the future.\n\nI really don't like the fact you have to edit a file in the module to change the type of the promise [provider]. You might be able to implement it better via generics, by having a type parameter TPromise extends Promise...\n\nNone of that would work, because we are supplying promise library dynamically, and it can be any promise library. You cannot extend a generic in TypeScript, I have researched this subject in detail, it is still in discussion to be implemented in the future, but until that happens, you have to patch the library with explicit promise type, or else you won't have access to its extended protocol.\n\nInstead of Array, you can use T[]\n\nHow is it any better?\n\nYou might want to use PascalCase for enums, as per the official TypeScript coding conventions.\n\nThe cases used for enum-s are exactly the same as in the original library. Changing them would break the protocol.\n. @paavohuhtala \nI would like to be able to address each issue in detail. There is way too much in both of our posts here.\nI would suggest opening separate issues for each problem, or else it becomes too difficult to follow, especially for other developers.\nPlease don't hesitate opening separate issues for each of the problems discussed here.\n. @paavohuhtala thank you, will look forward.\nAs for the promises, not to let you think that I haven't tried, a spent a lot of time trying to make it work without using the patch approach, but couldn't get it to work, always running into issues on the declarative level.\nHere's where I got stuck: http://stackoverflow.com/questions/36593087/using-a-custom-promise-as-a-generic-type\n. @paavohuhtala if you follow on that StackOverflow issue, it pretty much came to the expected conclusion, from some experts in that area, confirming that it is not possible in today's TypeScript.\nhttp://stackoverflow.com/questions/36593087/using-a-custom-promise-as-a-generic-type\nIt seems that the patching thing is the only one currently usable to inject a custom promise into the library's protocol.\n. @paavohuhtala In version 3.8.2 I renamed modules pg and promise into pg-subset and ext-promise, to avoid a conflict in case you already have a module with that name used somewhere.\n. @paavohuhtala will you get a chance to follow up with separate issues added or should we just close the issue?\n. @paavohuhtala those guidelines only apply to writing a TypeScript library, they do not apply to writing ambient declarations, it is a totally different story, because one is bound to the protocol implemented by the underlying library.\n. I did use .d.ts initially, but then renamed, because I couldn't find out what was that for... :)\n. @paavohuhtala there has been a number of improvements in TypeScript in 3.9.0, including:\n- Renamed all files back to contain .d.ts in the name\n- Strict query parameter for all query methods (as you asked earlier)\n- Support for the new Prepared Statements.\nSee 3.9.0 release notes ;)\n. All these requests are loose:\njs\nconst artworks = t.any('SELECT * FROM artworks WHERE ${id}', artist).return(artworks)\ni.e. they are not getting chained to anything. you cannot write promise code like that ;)\nalso sql WHERE ${id} isn't valid.\n. I'm not very familiar with method map of Bluebird. What are you trying to achieve there?\nPlus I don't know what your sql.all is.\n. The problem of your algorithm is the fact that method map returns an array full of promises, without binding them to the context of the current task.\nIf you look at all the examples of resolving an array of promises, we use method batch for that always. You are bypassing that, not resolving the inner promises, they become loose, hence the result.\nIn other words, your inner promises are trying to resolve after the task has finished.\nYou need to reorganize it, make sure any array of promises gets wrapped by method batch and that the result of such batch is chained to the task.\n. This simplified example would resolve with an array of results from the inner requests.\njs\nrep.task('artists with artworks', t =>\n    t.any(sql.all)\n        .then(artists=> {\n            return t.batch(artists.map(a=>{\n                return t.any('SELECT * FROM artworks WHERE id=${id}', a.id);\n            }));\n        }));\nyou would just add the additional mapping logic as you need.\n. Maybe there is a way to optimize it with Bluebird, somehow, I don't know.\nAt least this looks correct.\n. You can compress it to this:\njs\nrep.task('artists with artworks', t =>\n    t.any(sql.all)\n        .then(artists =>\n            t.batch(artists.map(artist =>\n                t.any('SELECT * FROM artworks WHERE artist_id=${id}', artist)\n                    .then(artworks => {\n                        artworks.length && (artist.artworks = artworks);\n                        return artist;\n                    })\n            ))\n        ));\nand to me it looks quite elegant, and readable ;)\n. pg-promise-demo is a good example to understand the pattern ;)\nAlso, I forgot about the second optional parameter in method batch, using which you can re-implement your code like this:\njs\nrep.task('artists with artworks', t =>\n    t.any(sql.all)\n        .then(artists =>\n            t.batch(artists, (_, _, artist)=>\n                t.any('SELECT * FROM artworks WHERE artist_id=${id}', artist)\n                    .then(artworks=> {\n                        artist.artworks = artworks;\n                    })\n            )\n        ));\nin case you think it looks any better to you ;)\n. A detailed example for this was added here: Get a parents + children tree with pg-promise\n. As per the sequence API, source is to generate and return new queries, till you got all of them.\n\nBut source is only a single query isn't it?\n\nNot, it is not, see the API for the method.\n. > So would I be calling sequence multiple times? Or would I be returning an array of queries from source?\nNeither. You call sequence only once, as shown in the example. And source doesn't return an array of queries, it is to return one query at a time, given the index parameter.\nsequence calls source repeatably, till one of the calls returns nothing (undefined), which then means the end of the sequence, as per the example.\n. I explained this in my previous post.\n\nsequence calls source repeatably, till one of the calls returns nothing (undefined), which then means the end of the sequence, as per the example.\n. Just change source to this:\n\njs\nfunction source(index) {\n    if (index < data.length) {\n        return this.any('UPDATE table SET colToUpdate = ${colToUpdate} WHERE id = ${id}', data[index]);\n    }\n}\nand you are done.\n. P.S. Setting { limit: data.length } and then skipping if (index < data.length) would produce the same result ;)\n. Just for completeness here, and to show how flexible method sequence is, for future references.\nIts optional parameter dest can also be used for resolving promises/queries:\n``` js\nconst data = [{id: 1, colToUpdate: 'newVal'}, {id: 2, colToUpdate: 'newVal'}];\nfunction dest(_, row) {\n    return this.none('UPDATE table SET colToUpdate = ${colToUpdate} WHERE id = ${id}', row);\n}\ndb.tx(t=>t.sequence(i=>data[i], dest, data.length))\n    .then(data=> {\n        // success;\n    })\n    .catch(error=> {\n        // error;\n    });\n```\n. > index will always start at 0?\nIt does. Documentation: http://vitaly-t.github.io/spex/global.html#sequence\nBut first, see this: http://stackoverflow.com/questions/39119922/postgresql-multi-row-updates-in-node-js\nIt is a lot more efficient that way :wink:\n. Example update for version 6.9.0 and later of pg-promise:\n```js\nconst data = [{id: 1, colToUpdate: 'newVal'}, {id: 2, colToUpdate: 'newVal'}];\nfunction dest(_, row) {\n    return this.none('UPDATE table SET colToUpdate = ${colToUpdate} WHERE id = ${id}', row);\n}\ndb.tx(t=>t.sequence(i=>data[i], {dest: dest, limit: data.length}))\n    .then(data=> {\n        // success;\n    })\n    .catch(error=> {\n        // error;\n    });\n```\ni.e. parameters for sequence changed.. pg-copy-streams was never supported, only pg-query-stream is supported.\nPrimarily because pg-copy-streams support is really bad, while having many issues.\n. If you are streaming from the database, pg-query-stream is sufficient.\nFor other cases see:\n- streaming into the database\n- Performance Boost\n. @nonplus you forget to release the connection there ;). @nonplus why not just use event extend and add whatever API you need for your project? :wink:\n. Yeah, I don't think you can, unless you allocate the connection right in the method, not sure it will work with tasks/transactions though.\nWhat you did there in your example, it will only fit into the existing architecture exactly as is, but it cannot fit in integrated, because we cannot both allocate a connection manually and propagate it through tasks/transactions at the same time.\nProbably best is to leave it as an example.. No, that would mean exposing client at all times, which would constitute a disaster eventually, because it is not supposed to be used normally, only in exceptional cases, like yours. It would be a breaking change in the concept of how one should use the library ;). I think best is to leave it as is. It would be too much work, for something of infrequent use, and to further bloat the existing protocol. The generic one is good as is ;)\n. Now that we finally have branch 6.x available, I might revisit supporting node-pg-copy-streams out of the box.\n. @nonplus please check out this new article Data Imports, and let us know how the performance figures given there compare to the ones you get by using the COPY query.. @goyoGit You are not writing names of the object anywhere, you are just declaring the object. I do not understand your concern.. > if we want to insert a million rows, the object called 'data' will have a million attributes called 'title'\nThe number of attributes is unrelated to the number of properties on the object. Something you are doing wrong. See Data Imports.\n. @goyoGit If you read through Data Import, you will see that you would never end up with 1 million rows. The article shows you how to page through inserts.\n. Duplicate of #49 and #90 \n. @L-u-k-e This is going to change soon. I have started working on 4.1.0 that will introduce helpers namespace with all the new useful functions ;)\n. @L-u-k-e I have done a lot of work for this here, but it turned out to be a lot more complicated than was initially thought.\nA truly generic approach requires so many extra considerations, it's crazy :)\nI've got the initial draft working, but there is still a lot of work before it can be released.\nBut I'm glad that I started. Once it is ready, it will be able to help a lot.\n. @L-u-k-e Would you like to have a look at what has been done so far? - release 4.0.6.\nIt would be nice to get some feedback ;)\n. @L-u-k-e any luck there? :)\n. > For the UPDATE and INSERT queries, you are currently requiring an array of column names when doing a multi values insert/update. What was the reasoning behind that decision?\nThe fact that the array may contain heterogeneous objects, you can't assume any of them to be a full object / template. Plus you will frequently need a more detailed configuration for columns, if you consider things like:\n- default values for non-existing properties;\n- value overrides\n- type modifiers (formatting-side)\n- type castings (sql server-side)\n. > The library function could just infer, then group together the objects have the same set of column names, no?\nIt would make the whole solution very inefficient performance-wise.\nColumnSet type is thoroughly optimized to generate queries from data as fast as possible.\n\nwhat about allowing a simple object to be passed in for the WHERE, whose property name/value relationship always translated to '=' and whose property/property relationship is always 'AND'. E.G. {ID: my_id}\n\nThat's a lot of assumption. Real use cases can include way more complex update conditions. I wanted to keep it as simple and as generic as possible. And besides, it is very easy to add the WHERE part to the query.\n. > Maybe I'll take a stab at doing a SELECT just to get the ball rolling\nDid you mean INSERT?\n. I was never even considering to create a SELECT helper. There is no point. What I created for INSERT and UPDATE is to simplify the complexity of generating all those queries, to get them formatted correctly, in a dynamic-object environment, and especially for multi-object operations.\nA SELECT operation does not need any of that. You were probably thinking about something ORM-related, which is beside the point of why I wrote those INSERT + UPDATE helpers, more along the line of why I wrote the Performance Boost article where I showed some basic grouping for multi-inserts, but that wasn't generic enough.\n. @L-u-k-e I didn't get anymore feedback from you, and I have released the new feature as v.4.1.0.\n. @L-u-k-e I have added an example for type ColumnSet: http://vitaly-t.github.io/pg-promise/helpers.ColumnSet.html\nthis should help understand better why using such an object is a good idea in general, and what kind of situations one can typically run into when automatically generating inserts or updates.\n. It is documented everywhere:\nFrom Named Parameters\n\nThe library supports named parameters in query formatting, with the syntax of $propName, where * is any of the following open-close pairs: {}, (), <>, [], //\n\nFrom Named Parameters examples\n\nNamed Parameters are defined using syntax $*propName*, where * is any of the following open-close pairs: {}, (), [], <>, //, so you can use one to your liking, but remember that ${} is reserved by ES6 template strings.\n\ni.e. you can use any of $(), $[], $<>, $// inside your ES6 template strings.\n. @leebenson look at this flame - https://github.com/vitaly-t/pg-promise/issues/204\nIt is a little idiotic, for which I apologize in advance, but you will find an answer to your question :wink:\n. @leebenson Your IO will always eat up most of the PC resources (think 90%+), not query formatting :wink:\nCheck out this: https://github.com/vitaly-t/pg-promise/wiki/Performance-Boost\n. @leebenson also consider this - any serious app should use external SQL files, via Query Files. And for those you need regular query formatting.\nSee also: pg-promise-demo.\n. UPDATE\nThe solution has been found. Now we just need it to make into a new release.\n. This has been resolved with 4.0.4 release.\nNode.js 6.x is now fully supported.\n. Thank you!\n. They shouldn't be used as an integer, because bigint is 64-bit, which has no native support in JavaScript.\nYou can override it within pg-types:\n``` js\n// numeric\npgp.pg.types.setTypeParser(1700, function (value) {\n    return parseFloat(value);\n});\n// bigint\npgp.pg.types.setTypeParser(20, function (value) {\n    return parseInt(value);\n});\n```\nBut that means you will be breaking numbers pass 53 bit.\nFor further information search issues within the pg library, and pg-types used by node-postgres for all the server-to-client data conversion.\nHere's a related discussion: https://github.com/brianc/node-pg-types/issues/39\n. Best explanation: pg-promise returns integers as strings.. @ForbesLindesay the link that was provided gives all the proper explanation.\n\nBest explanation: pg-promise returns integers as strings.\n. This is an excellent question! I haven't used that feature, and completely forgot it was there, which has backfired at me in version 4.x\n\nRight until version 4.0.0 Prepared Statements were only used directly, as a simple, object. As a result, setting rowMode: 'array' would have always worked, even though I never thought of it.\nBut version 4.x introduced new type PreparedStatement that proxies those objects, and it is missing support for parameter rowMode. As a result, version 4.x broke the support for rowMode.\nThank you so much for checking this back with me!\nI am elevating this to a bug, and will address this issue in the next release.\nIn the meantime, if you need that feature right away, you can start with any 3.x version , where it still worked correctly.\n. I was initially thinking about modifying the internal PreparedStatement object to allows those properties, but what you are suggesting refers to Parameterized Queries, which was never supported by pg-promise. It would have to be a new feature.\n. @matus123 I have created branch 4.1 which now contains the complete code with everything you need - support for all advanced properties (binary, portal, rowMode, rows, stream, types) within both Prepared Statements and Parameterized Queries.\nYou can use it for the time being. I will need to update a lot of documentation, plus add many new tests before I can merge it into the master and release.\n. > But wouldn't it be easier just to allow pass additionalProperties object to db.query() methods\nThat's what I implemented. What did you think was it? Just because I added an extra wrapper doesn't mean that you need to use it, it is optional, provided in place automatically when not used.\nYou still can write: db.query({text:'select...', rowMode: 'array'})\n. 4.0.5 release has it all implemented ;)\nAlso about the same again:\n\nBut wouldn't it be easier just to allow pass additionalProperties \n\nDirect objects are quite useless, compared to the classes that represent them, which can do this:\n- support QueryFile for text\n- report errors through promises\n- nice console formatting\n  etc...\n. Why not use the very solution that's already published there?\n``` js\nvar pgp = require('pg-promise')({\n    // initialization options\n});\npgp.pg.types.setTypeParser(1114, function (stringValue) {\n    return stringValue;\n});\n// In ES6 syntax:\n// pgp.pg.types.setTypeParser(1114, s=>s);\n```\n. FAQ: How to access the instance of node-postgres that's used?\nplus pg in API ;)\n. Some caching issues perhaps? Try Ctrl+0, and then Ctrl+F5 ;)\n. That is in interesting. I have here all browsers (Chrome, FireFox, IE), none of them show the problem your showed me. I wonder now how to reproduce this issue...\n. I have just updated it slightly. Does it look any better?\n. @ferdinandsalis thank you! so am I :) It's just that I do not see the problem here, which is what makes it harder to fix.\nThat API website is automatically generated, I am limited to a single CSS file that I'm tweaking.\nI have just updated it again, reduced the overall font size. Might look better now ;)\n. Should be better now ;)\n. You must be getting the path wrong. Relative path does work fine in QueryFile, there is plenty of evidence to that.\nWe've got all the tests rely on the relative path, we've got pg-promise-demo using only relative path here.\nAfter all, you should be able to see which wrong path it is trying to use - that's reported as part of the error.\n. @minipai check out the new addition to Query Files: https://github.com/vitaly-t/pg-promise/issues/153\n. > I finally find out it is relative to my project root.\nNot quite, only when your Node.js start-up file is in the root of your project, because the path is relative to your Node.js start-up file.\n. Yes, that was about Autonomous Transactions, supported by both MS-SQL and Oracle SQL.\nAlso, worth pointing out, that PostgreSQL does not support nested BEGIN->COMMIT, one has to use save points within the top-level BEGIN->COMMIT, which pg-promise does automatically.\n. I should say that when I used the word \"proper\", I meant that PostgreSQL does not support nested BEGIN->COMMIT, only save points below the the top-level transactions.\nWhen I started working on it initially, it all seemed quite awkward to automate, because save points require automatically generated names, but I got it all working eventually :)\n. I should have read this one before following up on the PR :)\n. Fixed in v.4.0.13\nLater on I will probably just remove that thing completely.\n. @djMax I was being careful about it, adding only $ + names. Did I break something or is it a hypothetical improvement?\nI can see your change is to make the extra properties non-enumerable, which does look like a good idea :)\n. I read the issue you logged after I wrote this here :)\n. There are 2 standards for property names in JavaScript:\n- variable notation\n- string notation\nThe one we are using is the variable notation, which includes everything that defines a valid variable in JavaScript. Symbol - isn't part of a valid JavaScript name.\nThe string notation supports all possible characters. It is not used because it eliminates the possibility to extend names via formatting modifiers and casting.\nFor example, all formatting modifiers that we support, as per method as.format: ^, ~, #, :csv, :json, name, raw, :value.\nWhat makes it even worse, defining a variable name with containers such as these: ${}, $(), etc wouldn't be accurate enough also.\nSo, the full string notation for properties is out of the question.\n\nThere are a few solutions to your problem. First of all, you do not need to camel-case the results of all queries. You can use event receive to do this automatically for everything, as shown in the example there.\nSecondly, there is a powerful new feature - helpers namespace, which is currently still in Alpha, but it allows automatic approach to generating INSERT + UPDATE queries for any column names.\n. But you would have to do that manually. Why not simply use the code shown in the receive event? It would camelize all incoming data automatically.\nBy the way, the helpers namespace just has been promoted to Beta, with 4.0.14 release ;)\n. > I was about to create a pull request to improve error handling when parsing the configuration\nWhat configuration parsing?\n\nI noticed that the tests are running against all database type: pg, mysql, sqlite3 and mongodb\n\nWhat are you talking about? This is a PostgreSQL-only library. It doesn't support anything else, it has no tests for anything else.\n\nIt's not very trivial for anyone to install and configure all these database\n\nYou've lost me completely here, I'm afraid.\n. Cheers! :)\n. From the Connection section:\n\nCreate a global/shared database object from the connection details:\njavascript\nvar db = pgp(cn);\nObject db represents the database protocol, with lazy database connection, i.e. only the actual query methods acquire and release the connection. Therefore, you should create only one global/shared db object per connection details.\n\ni.e. create one global db and reuse it everywhere (unless you use more than one database).\n. Check out pg-promise-demo, maybe that will make it clear ;)\n\ncall pgp(cn) with the same cn everywhere to get the same db?\n\nNO.\n\nhave to pass around a db to everything that uses the code?\n\nYES.\n\nmake no difference?\n\nIt does. The event system supported by the library will re-initialize all handlers if you waste your db object around.\n. > What you're doing is wrapping the handle in it's own package and not requiring pg-promise anywhere else.\nThat's what everybody is doing, yes. I thought it was clear. Maybe not so much :)\n. >  is that the connection pool is implemented by pg.Pool component from the base pg library, is this correct\nNo, this library uses version 5.1 of the driver that only uses the internal pool. See #206.\n. results is the array of fetched row objects.\n. Your question seems to pertain to how to use data returned from promises, which isn't relevant to this library. Your should read about how to use promises ;)\n. Method db.any resolves with an array that contains rows-objects ready to be used.\nThere is absolutely nothing else to it. There only way one may not understand this is when you don't understand how promises work, which you should learn then.\nThere is nothing crazy here, it is all very trivial.\n. There are no deadlocks in simple queries, and certainly not in the code that you showed.\n. @cybercoder \nDo you want to show a full example of what you were doing?\n. > Do Named Parameters use prepared statements under the hood?\nNo. Those are two completely separate things.\n\nThere is no mention of what is using prepared statements\n\nNothing is using them, except your own code, in the format of {name, text, values} (implicitely), or type PreparedStatement directly (explicitely).\n\nif to use them we need to do so explicitly?\n\nYou don't (see above).\n. That's not how pg library works. And I wasn't going to override what it does, only to make it easier to use.\nPrepared statements aren't that good really, they have their own problems.\nFirst of all, they offer very limited query formatting, a tiny fraction of what pg-promise can do.\nSecond, there are situations when they perform slower than simple queries.\n. All server-to-client data transformations are within node-postgres, which in turn uses pg-types, which is customizable.\n. I just did some testing, and I can see that for the proper float types we are getting Infinity back. Then I noticed you are using it for Date/Time types? Why? They do not need support for Infinity, they can accept NULL, which is sufficient.\n. pg-types in turn uses postgres-date to parse dates. I think if you try to change that, you will be running into an unholy mess. Date/Time coversion is the most complex part of types transformation.\nIf you want to manipulate dates, you can use something like:\njs\npgp.pg.types.setTypeParser(1114, function(value){\n   /// transform and return the value here...\n});\nwhich would be required for:\n``` js\nregister(1082, parseDate); // date\nregister(1114, parseDate); // timestamp without timezone\nregister(1184, parseDate); // timestamp\nregister(1115, parseDateArray); // timestamp without time zone[]\nregister(1182, parseDateArray); // _date\nregister(1185, parseDateArray); // timestamp with time zone[]\n```\nAnyway, since it is definitely not within the scope of this library, I am closing the issue.\n. @hannupekka This just had an unexpected follow up: https://github.com/bendrucker/postgres-date/issues/3\n. > I know node-postgres development isn't that speedy these days.\nThis would be an understatement. Brian C locked my account again, for the second time, after the insane amount of support I've been doing for his library. Something isn't right with this guy...\n. The limitation is meant for the number of columns in a single row, not on the number of parameters in general, because you are not supposed to even use it in that way, and if you are using it that way, that's a not a good approach at all.\nAs I understand, you are trying to do a bulk insert. Check out the latest question about that: Multi-row insert with pg-promise\nThat will guide you how to do it properly.\nOr if you don't care about performance optimization (though you should), the standard approach to this would be as shown here: Inserting multiple records with pg-promise. But that's way slower than via a single INSERT ;)\nSee also: Performance Boost.\n. I take it the issue has been resolved? ;)\n. @TazmanianDI it is a related problem, in a sense that you are also misusing the query-formatting engine. But the way you do it is also different.\nYou should never have things like this: IN (($1,$2,$3),($4,$5,$6),...,, that's the wrong query formatting approach.\nTypically, when one has so many parameters, you are trying to use static formatting instead of the dynamic one, i.e. you should use SELECT as the source for the parameters, rather than pull so many records from the memory.\nAnd in situations where you do have so many parameters in memory, have each object support Custom Type Formatting, and then inject an array of those using the :csv filter:\n```js\nfunction MyCustomType(){\n  this.propr1 = 1;\n  this.prop2 = 2;\n  this.prop3 = 3;\n// support for Custom Type Formatting:\n  this.formatDBType = function(){\n    return pgp.as.format('${prop1}, ${prop2}, ${prop3}', this);\n  };\n  this._rawDBType = true; // output as raw/pre-formatted string;\n}\n// This type you can use then as:\nvar a1 = new MyCustomType(), a2 = new MyCustomType(),...\ndb.any('...IN ($1:csv)', [a1, a2, ...]);\n```\nAnd you can also simply extend your data objects with the custom-type formatting, without creating any new types :wink:\nCustom Type Formatting:https://github.com/vitaly-t/pg-promise#custom-type-formatting. > The custom format would reduce the number of parameters I've got by a third\nYou misunderstand, it would reduce the number of formatting parameters to just the list of properties in a single object. And in your example that's just 3.\n. > $1 refers to the entire array?\nYes, precisely.\n\nWhat if I had some additional values that aren't part of that IN clause? Would it be...\n\nOf course!. Pardon, I've made a little mistake in the example earlier...\nYou need to pass it in like this:\njs\ndb.any('....IN($1) WHERE something = $2', [[a1, a2, ...], 123])\ni.e. not as an array directly, but as an array that's a parameter in the array of parameters, or via Named Parameters:\njs\ndb.any('....IN(${names:csv}) WHERE something = ${id}', {\n  names: [a1, a2, ...],\n  id: 123\n})\n. Yes, I can see where the problem comes form... replace this:\njs\nreturn pgPromise.as.format('${val1}, ${val2}', this);\nwith this:\njs\nreturn pgPromise.as.format('$1, $2', [this.val1, this.val2]);\n...to avoid recursive custom-formatting call. Then it should be fine ;)\n. If you were to use it as ($1) as was in the code example earlier, then you wouldn't need adding any parenthesis there ;)\n. Gotcha, you're right! :wink:. Version 6.0.16 of the library increased the limit to 99,999, for the reasons as explained in the release notes.\n. F.Y.I. Important breaking change in v6.5.0.. Why do you want to use connect for executing query anyway? It has been depreciated in that context. You should use Tasks instead, they are way better, and you don't need to care about the connection at all.\n. It is not possible, because Prepared Statements by definition are formatted by the database server, as part of the execution plan, while Named Parameters are part of this library's query formatting. You can't combine the two.\n. This was discussed in the past. It cannot, or rather shouldn't be done, because Named Parameters in this library support advanced syntax of type modifiers that cannot be translated into simple variables. Translating just a subset of Named Parameters into regular variables would only create more confusion. Therefore, it is not going to happen.\n. Object PreparedStatement is fully reusable. And the way you are using it seems correct.\n\nIt appears that not all values are updated in the DB.\n\nThat shouldn't be the case. I would suggest investigating further what is really happening.\nI know about projects that fully rely on reusable Prepared Statements exactly the way you do, and they always work. There must be something else wrong in your code and/or the database server, if you do not see the updates.\nExample of such a project: https://github.com/PCSTrac/napi\nHere he is even caching reusable PreparedStatements that use Query Files, and the whole thing works like a clock. Plus, we have plenty of tests for them.\n. Wait, I know what's wrong in your code! :) It is because you are executing each Prepared Statement on a separate session, which you should never do. It is bad in many ways. It will be depleting your connections, and it will be overriding your Prepared Statement values all the time.\nI could show the alternative way of doing it, but then using Prepared Statements like that to run multiple updates is wrong anyway. It would be a misuse of Prepared Statements.\nYou should execute all such updates as a single query, which will work 10 times faster. See helpers.update for that.\n. @ghusse Later on as I was doing some performance optimization within the query execution, your post helped me realize that even though the updates you showed isn't a good idea, they still should work anyway. So, I have reviewed the code and made some changes that I just released as version 4.2.4.\nThank you for the idea. Your code example that you gave now should work as you expect it. But I still strongly suggest that you do it differently, if you want to get good performance out of it ;)\n. > What do you suggest instead for the updates?\nNot instead of updates, I was suggesting that if you are running the same type of updates, you should do it via a single query, with the help of method update (plenty of examples there). You would generate the main body, and then append WHERE part with all your JOIN  logic as needed ;)\n. > The update utility generates a query including the FROM and WHERE parts.\nNo it does not, WHERE part is not included. Are you sure that data in your update comes from a table and not from memory? 'Cos that wouldn't be a typical update.\nAnyway, that update method can only help with data updated from memory, not from a table. If you need to update from  table, then you don't need it, you can update in a usual way.\n. Don't forget QueryFile ;) It is really good at loading and pre-formatting queries ;)\n. @ghusse There was an improvement in the new v.4.3.0.\nNow you can pass Prepared Statement parameters into query methods to set/override the values.\ni.e. you can simplify this:\njs\nBPromise.map(values, value => {\n  prepared.values = [value.value, value.id];\n  return db.none(prepared);\n});\ninto this:\njs\nBPromise.map(values, value => db.none(prepared, [value.value, value.id]));\n. @geonanorch that's the cost of reusing the same PreparedStatement object. Just like any object that can be modified asynchronously.\nPreparedStatement is only useful for large/complex queries that benefit from prepared execution on the server side. Inserts and updates really should be done via the helpers, especially repeated ones, that's singificantly more performance-efficient, as per the Performance Boost article ;)\n. - You have all the details reported within the error event, is where you should be logging it.\n- Standard rejects never modify the original reject data.\n. There is no other info there. Only the event error has all the context in it.\n. I'm not even sure what you mean by that. That's the global error handler for the entire library, that's all.\nEverything that goes wrong is reported there - is the perfect place to log your errors.\n. To make a better output string inside error(err, e):\n``` js\nvar EOL = require('os').EOL;\nvar errMsg = (err.message || err) + EOL + JSON.stringify(e);\n```\nBut if you want only error + the query, then:\njs\nvar errMsg = (err.message || err) + EOL + e.query;\n. I'm gonna do a bit of research on this first, to understand how this really works.\n. @jcristovao I still cannot make it work with a simple path, having this always:\njs\n/// <reference path='../typings/globals/pg-promise/pg-promise.d.ts' />\nIs there another change I need to make to make it pick up the library automatically?\n. I will try later again. In the meantime, it is all included in 4.3.2 - check it out ;)\n. > Did you install with --global and then included the index?\nWhich index file do you mean?\nSo far the only way I could include it is via a complete relative path:\njs\n/// <reference path='../typings/globals/pg-promise/index' />\nI must be missing something...\n. I missed the fact that you were including index, and not pg-promise\nUsing this minimum it actually works! :)\njs\n/// <reference path='../typings/index' />\n. @jcristovao I have updated the TypeScript documentation ;)\nDoes it look accurate to you? I'm not 100% sure about the global part :)\n. This is the limitation of the underlying driver node-postgres. There was a PR to get this implemented: https://github.com/brianc/node-postgres/pull/776\nBut like many others, it never made it into the release. Without it the results are merged into a single array, unfortunately.\n. @RyanMcDonald \nVersion 7.0.0 now can do all that :wink:\njs\ndb.tx(t=> {\n        return t.multi(joinQueries([\n            {query: \"INSERT INTO Users(name, age) VALUES($1, $2)\", values: ['John', 23]},\n            {query: \"DELETE FROM Log WHERE userName = $1\", values: 'John'},\n            \"SELECT count(*) FROM Users\"\n        ]));\n    })\n    .then(data => {\n        // data[0] = [];\n        // data[1] = [];\n        // data[2] = [{count: 123}];\n    });. This is a server-level error.\nThis means there is a problem with your SQL query, not with this library.\n. This is a query before formatting, which doesn't tell me anything. What is the final query?\n. You can use pgp.as.format(query, values) to get the final query string.\n. Works in console but not when executed from Node.js? I've never heard of such issues before. Are you sure about this?\n. Something else is wrong there. According to your error log, the query being executed is invalid.\n. The library would use whatever server you connect to.\nYou can check that:\njs\ndb.proc('version')\n    .then(function (data) {\n        // SUCCESS\n        // data.version =\n        // 'PostgreSQL 9.5.1, compiled by Visual C++ build 1800, 64-bit'\n    })\n    .catch(function (error) {\n        // connection-related error\n    });\n. You are welcome! ;)\n. According to the instructions, you are supposed to add:\njs\n{\n  \"globalDependencies\": {\n    \"pg-promise\": \"github:vitaly-t/pg-promise#a79eb4bade0493e1edfb03600f5bd73ade8e6f49\"\n  }\n}\nHave you done so? Then you would be able to get it installed simply by running typings install.\n. I am using typings 1.0.4, and yes, there are a few differences.\nregistry:dt/pg-promise indeed doesn't work as a reference, just use the one that's recommended, and you will be fine ;)\n. P.S. To install the same from the command line you would be using the following:\ntypings install --save --global  github:vitaly-t/pg-promise#a79eb4bade0493e1edfb03600f5bd73ade8e6f49\n. I have updated the installation instructions to be much simpler.\n. Initially released in v.4.3.6.\n. All the work for this feature has been finalized with the release of v.4.3.7.\n. Added new method buildSqlModule in v.4.3.8 release.\nNow the entire module can be generated automatically.\nExample:\njs\npgp.utils.buildSqlModule();\nGets us a very nice output module:\n``` js\n/////////////////////////////////////////////////////////////////////////\n// This file was automatically generated by pg-promise v.4.3.8\n//\n// Generated on: 6/2/2016, at 2:15:23 PM\n// Total files: 15\n//\n// API: http://vitaly-t.github.io/pg-promise/utils.html#.buildSqlModule\n/////////////////////////////////////////////////////////////////////////\nvar load = require('./loadSql');\nmodule.exports = {\n    events: {\n        add: load(\"../sql/events/add.sql\"),\n        delete: load(\"../sql/events/delete.sql\"),\n        find: load(\"../sql/events/find.sql\"),\n        update: load(\"../sql/events/update.sql\")\n    },\n    products: {\n        add: load(\"../sql/products/add.sql\"),\n        delete: load(\"../sql/products/delete.sql\"),\n        find: load(\"../sql/products/find.sql\"),\n        update: load(\"../sql/products/update.sql\")\n    },\n    users: {\n        add: load(\"../sql/users/add.sql\"),\n        delete: load(\"../sql/users/delete.sql\"),\n        find: load(\"../sql/users/find.sql\"),\n        update: load(\"../sql/users/update.sql\")\n    },\n    create: load(\"../sql/create.sql\"),\n    init: load(\"../sql/init.sql\"),\n    drop: load(\"../sql/drop.sql\")\n};\n``\n. Added a guideline: [SQL Files](https://github.com/vitaly-t/pg-promise/wiki/SQL-Files)\n. @msaron that's to be expected, since you are not re-generating the file on the deployment machine, which would be the alternative :wink:. You would have to override the default parser for that, usingpgp.pg.types`. See pg-types.\nI don't really know enough about the intervals to be more specific.\n. Knex and this library use the exact same driver underneath.\nYou can do the same with this library, but it is a terrible hack, which means you are executing a query on every single connection, slowing down your communications.\nAre you sure you want to do something like this?\nWhat's even worse, it would execute the command on every virtual connection, since both libraries use the connection pool.\nIt would be a lot more efficient if you needed it within a separate task.\n. A similar ugly hack via pg-promise would be:\njs\nvar pgOptions = {\n    connect: function(client){\n        client.query('SET intervalStyle = iso_8601');\n    }\n};\nI'm not sure it would work, but you can try.\nThe only proper way to do it is via the connection parameters: https://www.postgresql.org/docs/9.5/static/runtime-config-client.html, but their support by the driver is too limited.\n. That code in knex you showed me uses an exclusive Client instance. That cannot be a good idea, because clients outside of the pool do not scale well. One should always use the pool. Like I said, it is an ugly hack either way.\nCan you not use it within a separate task? Are you sure you need it for the entire application?\n. By the way, this gave me an idea about adding a flag into event connect to indicate when it is a fresh connection. That would help in your case a bit ;)\n. Yes. Can you try that hack in the meantime - to see if that even works?\n. Does this mean that it now works for you? This was the solution I suggested from the beginning ;)\n. COOL. I am working on it now ;)\n. This seems to be possible to implement only for the JS client, but not for the Native Bindings. This is quite a bummer.\n. I've got it working :)\n. @nareshbhatia released in v.4.4.3 ;)\n. I have figured out how to make this work for Native Bindings also. It ain't pretty, and not as efficient as for JS Bindings, but it works.\nI will release it later.\n. Ouch, in a rush, I forgot to update the typescript.\nreplace this line:\nts\nconnect?:(client:pg.Client, dc:any) => void;\nwith this one:\nts\nconnect?:(client:pg.Client, dc:any, fresh:boolean) => void;\nI will fix it later ;)\n. @nareshbhatia All done, see v.4.4.4\n. I have completely refactored it in 4.4.5, making it much simpler, and with much better performance.\n. You should follow the exact instructions.\nThey give you the right installation command:\ntypings install --save --global  github:vitaly-t/pg-promise\n. The library implements many custom errors to help in the error-handling logic, all of which you can see in the errors namespace.\nThe error handling can be simpler when used centralized, via the global error event.\nBy the way, have you tried pg-monitor yet?\n. What you are asking about, is an extra parser on top of the basic node-postgres driver, for errors reported by PostgreSQL. This library doesn't do it. There were numerous discussions about unifying error reporting within node-postgres, but none of them ever yielded much.\n. If it's any consolation, none of the existing PostgreSQL drivers or libraries do it.\nYou can have a look at node-pg-error though.\n. Two reasons:\n1. This kind of stuff should be done on the driver level - node-postgres, but its author never did it;\n2. It would be a major breaking change for all of the code that's out there.\n. @valeriangalliat \nWhen I was implementing it, I was considering to use process.mainModule.filename instead, but it seemed exactly the same.\nWould process.mainModule.filename work under the Node interpreter?\n. On second thought, if the interpreter doesn't supply an input file, that parameter wouldn't be there also.\nThe reason I'm asking, is because in normal Node.js cwd() isn't the same as the process file name.\n. The final code that was released in 4.5.0:\njs\n// istanbul ignore else\nif (process.argv[1]) {\n    module.exports.startDir = $path.dirname(process.argv[1]);\n} else {\n    module.exports.startDir = process.cwd();\n}\n. I wasn't aware somebody was using it with pgp('somedb'). This isn't really part of any documentation, if it worked, it worked unofficially.\nI think that supporting a configuration file is more useful than supporting a bare database name.\n. > But note this change also removes support from socket connection strings\nThat's a big Ops! :smile: \nI'm gonna be making some changes now for the next 4.5 release, which will also include node-postgres 5.0.0 dependency which was just released.\nThank you for the findings!\nI will post updates when I have them ;)\n. I am removing the entire Configuration File feature from 4.5.0\n. Fixed in 4.5.0.\nI completely removed the feature. There was no fix for it, it was a bad idea.\n. There is no special support for this. You can only do it as a separate test:\njs\nvar start = Date.now();\ndb.connect()\n    .then(obj=> {\n        var duration = Date.now() - start;\n        console.log(duration);\n        obj.done();\n    });\n. It is possible, but nobody asked for it yet.\nYou know, you can always hack into the API directly, by overriding pgp.pg.connect and provide your version that would log the duration ;)\n. @RyanMcDonald \nHere's how you can hack into the pg connection pool used by the library, to log the duration:\n``` js\nvar prevConnect = pgp.pg.connect;\npgp.pg.connect = function (cn, cb) {\n    var start = Date.now();\n    return prevConnect.call(this, cn, function (err, client, done) {\n        if (!err) {\n            var duration = Date.now() - start;\n            console.log(duration);\n        }\n        cb(err, client, done);\n    });\n};\n```\n. That, or with the connection parameters.\n. Problem 1:\nThe query formatting engine doesn't have any dependency, while QueryFile uses the formatting engine for partial query formatting. They cannot refer to each other. That's module cross-reference limitation.\nProblem 2:\nQuery formatting engine is highly optimized, and it is fully synchronous. Mixing it up with something like QueryFile would make a mess out of it.\nProblem 3:\nQueryFile takes many options, parameterizing which from a variable would be a mess.\nProblem 4:\nQuery formatting engine doesn't support the syntax you described for passing in variables. Changing the engine just for that,...I wouldn't do it. Implementing query formatting in QueryFile would be messy, plus again, passing all the options would make it bad.\nIn all, I don't think I would venture in something like this, sorry.\n. @tkellen it is worth adding, that if you need something like that, it is very easy to inject content of one SQL file into another, using QueryFile.\nHowever, it will be impossible to rely on query promises to reject when your QueryFile encounters a problem, and you won't be able to re-load such SQL files automatically, i.e. taking away all the good parts from QueryFile.\n. Here's an example of what you can do with the existing QueryFile and the formatting engine:\n``` js\nvar QueryFile = require('pg-promise').QueryFile;\n// file ./sql/inner.sql:\n// SELECT count(*) FROM table1\nvar innerQuery = new QueryFile('./sql/inner.sql', {minify: true});\n// file ./sql/full.sql:\n// SELECT * FROM table2 WHERE (${counter^}) > 0\nvar fullQuery = new QueryFile('./sql/full.sql', {\n    minify: true,\n    params: {\n        counter: getSql(innerQuery)\n    }\n});\nfunction getSql(qf) {\n    // wrapping in a function, to be called\n    // by the query formatting engine:\n    return function () {\n        if (qf.error) {\n            throw qf.error;\n        }\n        return qf.query;\n    }\n}\nconsole.log(fullQuery);\n//=>\n// QueryFile {\n//     file: \"./sql/full.sql\"\n//     options: {\"debug\":true,\"minify\":true,\"compress\":false,\"params\":{}}\n//     query: \"SELECT * FROM table2 WHERE (SELECT count(*) FROM table1) > 0\"\n// }\n```\nBut as previously stated, it won't update the inner SQL, if it changes during development, but at least you will be able to get the initial errors related to the inner SQL reported correctly ;)\nExample of an error for the inner SQL file, if we point it to non-existing file ./sql/invalid.sql:\nQueryFile {\n    file: \"./sql/full.sql\"\n    options: {\"debug\":true,\"minify\":true,\"compress\":false,\"params\":{}}\n    error: QueryFileError {\n        message: \"ENOENT: no such file or directory, open 'D:\\NodeJS\\tests\\sql\\invalid.sql'\"\n        options: {\"debug\":true,\"minify\":true,\"compress\":false,\"params\":{}}\n        file: \"./sql/full.sql\"\n    }\n}\n. Closing, due to inactivity.\n. The demo shown above there is now obsolete. It can be simplified to just the following:\n```js\nvar QueryFile = require('pg-promise').QueryFile;\n// file ./sql/inner.sql:\n// SELECT count(*) FROM table1\nvar innerQuery = new QueryFile('./sql/inner.sql', {minify: true});\n// file ./sql/full.sql:\n// SELECT * FROM table2 WHERE (${counter^}) > 0\nvar fullQuery = new QueryFile('./sql/full.sql', {\n    minify: true,\n    params: {\n        counter: innerQuery\n    }\n});\nconsole.log(fullQuery);\n//=>\n// QueryFile {\n//     file: \"./sql/full.sql\"\n//     options: {\"debug\":true,\"minify\":true,\"compress\":false,\"params\":{}}\n//     query: \"SELECT * FROM table2 WHERE (SELECT count(*) FROM table1) > 0\"\n// }\n```. Do I understand it right that you want to execute just one insert with two column values?\n. The tricky part of your query is the formatting for individual parameters, not for the overall query, because you are not doing a multi-row insert, it is just one insert.\nBecause of that, even though you still can use the helpers namespace for the query itself, it is not of much help.\nThe main thing that you need is the Custom Type Formatting in this case, which can let you format custom objects in any way you want.\nYou need one type for the first column, another one for the second one...\n. For example, for the first column type you might use:\n``` js\nfunction STPoint(x, y) {\n    this.x = x;\n    this.y = y;\nthis._rawDBType = true;\nthis.formatDBType = function () {\n    return pgp.as.format(\"ST_MakePoint(${x}, ${y})\", this);\n};\n\n}\n```\nSuch a type then can be used directly as a formatting parameter. You might already have a type similar to this, which you then can just extend with _rawDBType and formatDBType.\n. Accordingly, for the second type you can have this type:\n``` js\nfunction POI(id, element) {\n    this.id = id;\n    this.element = element;\nthis._rawDBType = true;\nthis.formatDBType = function () {\n    return pgp.as.format(\"poin(${id}, ${element})\", this);\n};\n\n}\n```\n. @kdex ignore the previous type examples I gave, use this one instead ;)\ncomplete example:\n``` js\nfunction STPoint(x, y) {\n    this.x = x;\n    this.y = y;\nthis._rawDBType = true;\nthis.formatDBType = function () {\n    return pgp.as.format(\"ST_MakePoint($1, $2)\", [this.x, this.y]);\n};\n\n}\nfunction POI(id, element) {\n    this.id = id;\n    this.element = element;\nthis._rawDBType = true;\nthis.formatDBType = function () {\n    return pgp.as.format(\"poi($1, $2)\", [this.id, this.element]);\n};\n\n}\nvar loc = new STPoint(50, 60);\nvar pois = [\n    new POI(2442522, 'node'),\n    new POI(359582, 'way')\n];\ndb.none(\"INSERT INTO $1~(location, pois) VALUES($2, $3::poi[])\", ['some-table', loc, pois]);\n```\nIt will execute:\nINSERT INTO \"some-table\"(location, pois) VALUES(ST_MakePoint(50, 60), array[poi(2442522, 'node'),poi(359582, 'way')]::poi[])\n. F.Y.I. Important breaking change in v6.5.0.. This part of code would need to be changed for v6.5.0 of the library:\n```js\nfunction STPoint(x, y) {\n    this.x = x;\n    this.y = y;\n    this._rawType = true;\n    this.toPostgres = a => pgp.as.format('ST_MakePoint($1, $2)', [a.x, a.y])\n}\nfunction POI(id, element) {\n    this.id = id;\n    this.element = element;\n    this._rawType = true;\n    this.toPostgres = a => pgp.as.format('poi($1, $2)', [a.id, a.element])\n}\n. Actually, you can make even much simpler:js\nfunction STPoint(x, y) {\n    this._rawType = true;\n    this.toPostgres = () => pgp.as.format('ST_MakePoint($1, $2)', [x, y])\n}\nfunction POI(id, element) {\n    this._rawType = true;\n    this.toPostgres = () => pgp.as.format('poi($1, $2)', [id, element])\n}\n``._rawType` isn't private, and cannot be, because it is accessed by the query-formatting engine, plus it can be set dynamically on an existing object.\nThe reason it uses an underscore in front is because it is meant to extend existing objects, and the underscore is there to minimize the chances of a conflict with an existing property in the object.\ntoPostgres is too unique a name, no need for an underscore, but rawType isn't, hence the underscore ;). This will not work on existing objects. It will only work on types that you declare in your code.. With the existing solution you can take any object that was created outside of your own code, and turn it into a self-formatting one, simply by setting the extra properties.\nWith what you suggest, you would have to wrap each external type into a local class before you can use Custom Type Formatting, that would be very awkward.\n. It is worth noting that syntax for Custom Type Formatting slightly changed in v6.5.0 :wink:. Version 7.2.0 changed it a bit.. I'm not quite clear on this. ORDER BY is part of your query string, so what stops you from using it there?\n. Ok, but in that example it is static, right? Which means you can just specify it in the query string.\nOr are you asking about some sort of a dynamic ORDER BY? If so, then please provide an example for that.\n. By the way, in your original example you are using the old syntax. Here's the new one:\njs\nconst data = await db.any(`SELECT * ... WHERE id IN ($1:csv)`, [ids.map(i => i.id)]);\n. According to the general syntax, there can be anything for the ORDER BY, it is impossible to generalize.\nFor your task you should just generate a string that will contain the list of all the WHEN/THEN statements, and then insert it.\nThere are many ways to do it, and they are all simple.\n. > resolve reject\nthere is no such thing :)\nAnd what you are trying to do there is very wrong to begin with. It should be:\nvar q = db.any(sql, params)\n.then(function(result) {\n    var a = {\n        status: 200,\n        result: result\n    };\n    console.log(a);\n})\n.catch(function(err){\n    var b = {\n        status: 500,\n        result: err\n    };\n    console.log(b);\n});\nYou should read about promises and how to use them ;)\n. > why does your documentation then have a reference to the Javascript Promise reference\nBecause it is fully based on promises.\n\nand as useful as the chaining is, it isn't intuitive...\n\nDo you mean the promises part? One should read about promises in other resources, this library simply uses the promise standard, it is not supposed to re-document promises.\n. There is nothing anonymous in the result, that's Node.js 6.x new specific for rendering objects into the console.\n. I don't, and Node.js objects presentation is not related to this library.\n. As per my earlier comments, there is no problem here of any kind.. But it is included! What are you talking about?\n. And how is this related to QueryFile?\n. I can't see what is going on from a piece of the error log that's not even referencing the library...\nCan you publish an example of what it is you are doing, so I can understand the problem?\n. The error you are seeing is not related to QueryFile at all.\nIt clearly states: error: column reference \"name\" is ambiguous\nmeaning your query is wrong.\n. It's all documented.\nhttps://github.com/vitaly-t/pg-promise/wiki/Learn-by-Example#where-col-in-values\n. > if the programmer misspells the QueryFile name, the resulting error is not descriptive. \nIf the programmer misspells a file name for QueryFile,  he gets a detailed error about file not found.\nTherefore, I'm not sure what this is for...\n. Can you point at a specific line?\n. Thank you. I now understand the nature of your problem with the query files.\nIt is indeed possible to improve this, but not in the way this PR does it. I will think about, and let you know when I have an update.\n. @shesko implemented and released in v.5.0.2.\n. As stated in the documentation:\n\nAvailable as pgp.end, after initializing the library.\n\nThis was a change in version 3.7.0. See accessisng node-postgres.\n. It is at the very beginning: https://github.com/vitaly-t/pg-promise#initialization\n. n.p. ;)\n. yeah, spex now ships with its own typescript, and the way I merged them - doesn't quite work automatically, for some reasons, I'm still trying to figure out how to improve this.\nIn the meantime, you can reference spex directly, as explained here: \nby running this script:\ntypings install --save --global github:vitaly-t/spex\n. P.S. I asked a question about this: A reusable library with typings from GitHub\n. there are several ways, here's just one of them:\njs\nvar data = [['John', 'Winningham'], ['Kate', 'Soromoth']];\nvar values = data.map(d=>'(' + pgp.as.csv(d) + ')').join();\nrep.many(sql.get_requested_users, values);\nand you would change your sql to this:\nsql\nSELECT *\nFROM user\nWHERE (user.forename, user.surname) IN ($1^)\nThe best alternative would be via Custom Type Formatting. It depends on whether the containing array needs to be an object in your app or not.\n. This would go against the logic of partial formatting where variables can be set in more than one step.\nSee parameter partial in the format method.\nQueryFile relies on it usage, for example.\n\nAs for another approach... when you are providing an object for named parameter formatting, it normally has all the properties in it, so if some of them are not set undefined, you would get your NULL-s.\n. In theory, we could have another parameter within the format function, say default, to provide default value for variables that do not exist.\nBut i'm not sure how much value it would add, since you do not have access to formatting options from query methods.\n. @tkellen any feedback on this?\n. Added option default in v.5.0.4\n. Have you seen the updated API? It is quite self-explanatory: as.format.\n. @tkellen many things have changed with the release of v.5.2.0, in relation to what was discussed here, making it way simpler to use external SQL as templates of any nesting level.\n. @tkellen you would be right to think of it in very simple terms, QueryFile is now a type that can be a formatting parameter, to inject sub-queries :) And you can use this type with method as.format directly.\nIf you try, I'm sure it will just work for right off the way you think it should ;) No hidden tricks there :)\n. @raine Can you clarify what kind of queries you are trying to build. Are those by any chance INSERT or UPDATE queries? If so, then you should be using the helpers, as they are the ones that support value overrides, etc.\nOption default is there to provide values when those are missing completely. Values undefined or null do not signify missing properties, they are just values. So you cannot use any options within as.format to replace values, it is not what that method is for.\nIf in some unique case you still need to inject DEFAULT directly with method as.format, you would have to provide the following for the corresponding value:\njs\nconst DEFAULT = {\n    rawType: true,\n    toPostgres: () => 'default'\n};\nThat uses Custom Type Formatting to inject default as a raw text string.\n. @raine In that case, as per my previous post...\n```js\nconst DEFAULT = {\n    rawType: true,\n    toPostgres: () => 'default'\n};\npgp.as.format('Values: ${first}, ${second}', {}, {default: DEFAULT});\n//=> Values: default, default\npgp.as.format('Values: $1, $2', [], {default: DEFAULT});\n//=> Values: default, default\n```\nSee Custom Type Formatting.\n. Duplicate of http://stackoverflow.com/questions/37566944/how-to-fix-pg-promise-error-promise-library-must-be-specified\n. You should create a single database object per connection details, and then reuse it.\nFrom the start-up instructions:\n\nYou should create only one global/shared db object per connection details.\n. See related questions:\n- http://stackoverflow.com/questions/34382796/where-should-i-initialize-pg-promise\n- http://stackoverflow.com/questions/36120435/verify-database-connection-with-pg-promise-when-starting-an-app\n\nOr the whole category: http://stackoverflow.com/questions/tagged/pg-promise\n. Oh, and complete forgot: WARNING: Creating a duplicate database object for the same connection.\nThat should do it :smile: \n. @CalebEverett \nSee the changes brought with v.5.1.0\n. So, your intergration test and the application are using separate configurators for the database, while connecting to the same database. Is it really necessary? Why not place it in a shared module? That would get rid of the warning :wink:\nOtherwise, in special cases you can disable all the warnings by using option noWarnings.\n. @MitMaro is this module supposed to return an initialized pgp variable or a new db instance?\nIf it is the former, it is fine, if it is the latter - it won't work, because the library's initialization call is missing, i.e. var pgp = require('pg-promise')(/*initialization options*/), not just var pgp = require('pg-promise').\nBy the look of it is, it is the latter, that's why I asked.\nAnd if you do have a special use-case, then as I commented earlier, you can use option noWarnings :wink:\njs\nvar pgp = require('pg-promise')({\n   noWarnings: true\n});\nvar db = pgp(connectionDetails);. pg-promise uses just one connection pool, as per node-postgres v5.1 used underneath.\n. this is designed to be called more than once:\njs\nexports.handler = function(event, context) {\n  const db = pgp(connParams);\nwhich results in a repeated creation of the same db object more than once, hence the warning.\nAlso, on another note, your code shouldn't have things like:\njs\n  return `\n      BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\n      -- 2 query...\n      COMMIT;`;\nThe library can do such things automatically, while guaranteeing correctness.\n. > in my case every 4 min\nthat's not only once :)\n. @benoittgt see this: http://stackoverflow.com/questions/34382796/where-should-i-initialize-pg-promise\n. @lakesare what is this about?\n. @RomeHein You would have to create and keep a separate db object, but that's not a great idea, because if it is the same server, the valuable IO connections would be consumed, and not reused across users.. This library formats queries before they are executed, not after that.\nIf you want to change schema inside a PostgreSQL function, that's your function's internal implementation, which got nothing to do with this library.\n\nI am also unable to find examples about using schema\n\nformatting + examples:\n- https://github.com/vitaly-t/pg-promise#sql-names\n- https://github.com/vitaly-t/pg-promise-demo\n. @digualu I don't know if using SET ROLE is the best thing in your case, but if it is, then the following question becomes relevant: https://github.com/vitaly-t/pg-promise/issues/154, the part of using event connect alongside parameter isFresh.\n. Closing due to inactivity.\n. The latest version of the library supports Initialization Option schema to dynamically set default schema(s).. Executing SET search_path before each query is very awkward, and performance inefficient. One should use option schema now, which executes automatically, and only once for every physical connection created.\n. That's not likely, there must be something else you are missing.\nPlus, this library does not interpret or change the data, you get it exactly as node-postgres provides it.\n. @mamobyz looks that way :+1: \n. Fixed in 5.1.2\n. This library makes no use of the connection parameters, it is the underlying node-postgres driver that handles all the connection details, including the SSL.\nYou should research related issues against node-postgres. There have been plenty logged in the past, and they all have answers either on the driver's website or on StackOverflow.\n. That's because this library uses its own, simplified TypeScript ambient declaration for pg, as I couldn't find any good one when I was adding it.\nTo my knowledge, it doesn't exist. Have you seen any good one? By good one, I mean a complete one. \n. @goenning thank you, I saw it earlier. The first difference I noticed - the following types missing altogether: IColumn and IResult.\nAlso, I tired to keep naming the same, only adding I in front. You changed some of them.\nSee my definitions.\nMaybe if we can merge the two together with everything, it will make a good solution ;)\n. This has been addressed in v.5.1.4.\n. Thank you for reporting this! Fixed in 5.1.3.\n. - FAQ: How to access the instance of node-postgres that's used?\n- API, property pg\n. This library does not do any data interpretation, it is done by node-postgres, which in turn uses pg-types to convert the data received, which you can access via pgp.pg.types.\nSee also: How to access the instance of node-postgres that's used?\n. As per documentation:\n\nPath to the SQL file with the query, either absolute or relative to the application's entry point file.\n\nIf you use a relative path, and then change the execution root file, that's the kind of trouble you would run into, naturally.\nChange the path to the absolute, by joining with the global __dirname variable, and that will solve it.\n. I really would like to understand how exactly you run into the issue you describe. I've tried some synthetic testing for this, but no luck.\nWhat is your use scenario?\n. @nlf thank you for the details! As you probably have seen, the patch for this was released. ;)\n. > but why would Promise.all() not settle promise?\nBecause it is not supposed to, as per the Promise spec. Once the first promise in the chain rejects, the whole promise.all rejects at once.\n\nDoes settling promises and settling queries have different meanings?\n\nNo, it is the same.\n\nThis shows a good example of how to do what you want: http://stackoverflow.com/questions/37664258/get-a-parents-children-tree-with-pg-promise\n. You are welcome. And just so, you definitely will not need more than one .task there :wink: \nAnd.. doing things like this is pointless:\njs\n                     if (queries.length === 0) {\n                            return [];\n                        }\n                        else {\n                            return t2.batch(queries);\n                        }\nbecause simply doing return t.batch(queries) will result in the same.\n. anyway, try to avoid manual loops through multiple query results. Instead, use methods map and each for that, they produce much shorter and more efficient code ;)\n. According to the error log your show here, you are trying to call a function on something that is undefined. Could be isDefaultValuePlaceHolder, or _.has, or seenDefaultValuePlaceHolders, I don't know, because your example doesn't include line numbers.\n. @vince166 \n\nI am now stuck at this error TypeError: undefined is not a function. Which seems to originate from the line queries.append (t.one(\" SELECT email FROM \\\"user\\\" WHERE user_id = $1 \", userId)).\n\nLOL. Arrays do not have method append, only push, hence the error.\n. yes, global event error is what can help you here ;)\n. I take it the original issue is settled then :)\n. Thank you!\n. If I understand right, you want to use a custom object that represents your query.\nThe only way you can do this is by introducing your own methods via event extend.\nThis library won't let you change the default/existing methods. But you can always add your own namespace that will repeat the standard query methods, with the extra support that you need.\n. Existing methods cannot be changed, as they are used by the library itself, in such cases as a transaction, for one thing.\nAnother reason - having the standard base protocol simplifies integration with third-party libraries.\n\nI lose the ability to use existing convenience methods like one, many, etc. without re-implementing them on top of my custom query event.\n\nRe-implementation would be formal, as we are talking only about forwarding into the existing methods, nothing else.\nFor example, adding namespace $:\njs\nvar pgOptions = {\n    extend: obj=> {\n        obj.$ = {\n            one: (query, values, cb, thisArg)=>obj.one(query.toString(), values, cb, thisArg),\n            many: (query, values)=>obj.many(query.toString(), values)\n        }\n    }\n};\n. Throwing overrides into separate namespaces as shown earlier would prevent such a confusion when the method outcome changes from the standard one. It is a safer approach ;)\n. Look at the ColumnSet API and the examples, you are using it wrong.\nThe correct call is: \njs\nvar cs = new pgp.helpers.ColumnSet(columns, {table: new pgp.helpers.TableName(table, schema)});\ni.e. table is passed in as an option, not as a direct parameter.\n. Why would you want to use partial query formatting by default?\nYou will end up with queries not fully formatted, so they cannot be executed.\nThe support for partial query formatting was added at the same time as QueryFile, because it was the first instance where partial formatting became important, to be able to pre-format SQL that's being loaded. You normally do not need partial formatting outside of QueryFile.\n. The right approach in this case is via helpers.insert.\nNot only it can generate a proper single-query insert for multiple objects, but also you can configure defaults for any column.\nFor your example:\n``` js\n// create these once, to be re-used:\nvar columns = ['name', {\n    name: 'currency',\n    def: null\n}];\nvar csCountries = new pgp.helpers.ColumnSet(columns, {table: 'countries'});\n// and then you can generate your insert query where needed:\nvar query = pgp.helpers.insert(data, csCountries);\n// data = a single object or an array of objects;\n// query = query string that you now can execute\n. See also: [Multi-row insert with pg-promise](http://stackoverflow.com/questions/37300997/multi-row-insert-with-pg-promise).\n. Closing for now. You can re-open it, if there are still issues with it ;)\n. @noinkling this depends on the type of the column data. If it is not a string, and can be injected as a [Raw Text], then you can just do:js\n{name: 'myColumn', def: 'DEFAULT', mod: ':raw'}\n```\nIf it can be a string, then you will need to create a more generic solution that uses Custom Type Formatting:\n```js\nvar setDefault = col => {\n    return col.exists ? col.value : {_rawDBType: true, formatDBType: () => 'DEFAULT'};\n}\n// which then can be used with the ColumnSet as:\n{name: 'myColumn', init: setDefault}\n```\n. F.Y.I. Important breaking change in v6.5.0.. If you wrap your tests into transactions, then all actual transactions will not be working as expected, as PostgreSQL doesn't support nested transactions.\nThe library itself supports nested transactions by having the actual transaction on the top level and partial nested transactions as save points on all nested levels.\nIn all, you cannot use transactions for tests without affecting the result of the transactions in your code that won't work as a result.\nSo, I think this idea is a no go.\n. UTC is the official default and recommended Date/Time presentation for PostgreSQL.\nIf you decide you don't like it, you can use any presentation you like, see Custom Type Formatting.\n. > my point is about milliseconds\nMy answer precisely covers that. Use of Custom Type Formatting allows for any type of Date/Time presentation.\n. > why choose toUTCString\nUTC is the official default and recommended Date/Time presentation for PostgreSQL.\n\nand omit milliseconds\n\nI do not omit anything, we use the standard UTC conversion provided by JavaScript.\n. As I explained earlier, you can easily switch over to toISOString, if you like ;)\nI'm not sure right now though if PostgreSQL will understand it by default or not.\n. @TazmanianD \nAs stated previously, if you do not like the default Date transformation, you can easily set your own. Therefore, this won't escalate into a  bug.\nFor that you can use Custom Type Formatting :wink:\n. > Imagine someone who does an upgrade from pg to pg-promise and suddenly they find the different behavior\nnode-postgres that you refer to has its own problems converting Date/Time properly. There is no ideal solution, per say, and what pg-promise uses is the recommended default for PostgreSQL.\nYou can change the query formatting easily, but changing the default now and breaking everyone's code isn't a good idea. The user base of this library is huge (over 300k downloads on npm this month), and I'd rather stay away from breaking things, unless really necessary.\n. I would need to look into it in detail, this weekend perhaps ;)\nIn the meantime, just use Custom Type Formatting. It is simple, and it works.\n. At the moment I'm considering to start forwarding Date/Time formatting into the driver.\nSee this commit where specific UTC flag was added.\n. Implemented in release 5.4.1.\n. There is nothing special about executing UPDATE queries, that's why it is not specifically highlighted.\nCan you post the code example that gives you the trouble?\n. js\n.query('DELETE FROM Users WHERE id IN ($1:csv)', ids)\nids in this case should be an array of integers.\nSee also: Pg-promise : Pass an array of integers in array of parameters\n. Looks like your query isn't even get formatted.\nCan you show me the full code piece of what you are doing there?\n. the extension is csv, not cvs\n. what is that 'client' object you are calling against?\n. This is a complete misuse of the library :)\nThis project offers a myriad of examples, and none of them do what you are doing there.\nDo not call method connect to execute queries, this is not what that method is there for.\nSee the examples that are everywhere and follow them...\n\nThis library hides the original pg and client objects, they are only used in special cases. You are supposed to usedb methods.\n. This is not a typo.\nColumns are pre-formatted within the object parameter there: columns: columns.map(pgp.as.name).join(), that's why we insert a raw string.\n. Use of sub-properties implies expression evaluation. This library doesn't do it.\n. This library converts type Date only in one place. Something to do either with your data presentation in the database or the way you create/insert it.\nThere is no bug in the library related to Date conversion, that's for sure.\n. Closing due to inactivity.\n. I agree on most of what's been said here, just wish I had the time to rewrite the main readme file...\n\nI would be more than happy to prepare a PR where I implement these changes in the README, if that is something you would be receptive to?\n\nPerhaps if you could do it in smaller parts :wink: \n. I have started making some improvements (all checked into the master):\n- removed the whole Connection section, not to get into the shared connection stuff at all, even though it is not quite obsolete yet, see example for LISTEN/NOTIFY.\n- simplified transaction chapters\n- Replaced all t = this notes with t and this here are the same\n. Closing it for now, due to inactivity.\n. @xiamx Thanks!\n. PostgreSQL supports many connection parameters, of which the underlying driver node-postgres supports only a sub-set, and as of today the default schema is not supported (not sure if even PostgreSQL supports it).\nOther than that, it is a matter for node-postgres, not this library, which only uses it.\nThe best support for a flexible schema support is available through Query Files. See also pg-promise-demo as an example.\n. See the following discussion: https://github.com/vitaly-t/pg-promise/issues/154\nThe same approach can solve the issue of setting the schema automatically, on every fresh connection.\n. Almost... as per the discussion that I gave a link previously...\njs\nvar options = {\n    promiseLib: promise,\n    connect: function (client, dc, isFresh) {\n        if (isFresh) {\n            client.query('SET search_path = cust');\n        }\n    }\n};\notherwise you will end up executing it non-stop for each connection request from the pool.\n. Version 8.3.0 simplifies this, with automatic support for option schema.\n. Version 8.4.0 affects the functionality described here.\nThe easiest is to avoid use of event connect for this, and just use option schema instead.\n. You should read this, and use it according to the instructions. pg-promise-demo shows that also.\n. @uhef The problem is, after many promises, the TypeScript still failed to deliver the support for generics as parameters. See this question: Using a custom promise as a generic type\n\nIt is knowingly bad, but what can we do.... So we are bound to continue hacking in order to make TypeScript work with a custom promise library. Only the explicit/manual type injection works.\n  . @uhef another approach would be, as suggested in file ext-promise.d.ts:\n\nif you do not want to get these settings overridden during an update or deployment,\n it may be a good idea to copy all of the *.ts files into your own project, and use them from there.\n\n...which considering that the definition don't change often, is perhaps the best solution ;). You can't use query-formatting features of the library within prepared statements or parameterized queries. The whole purpose of those is to be formatted by the server, which only supports the basic $1 variables.\n. It already works via Babel, plus the library supports ES6 generators. What else is missing? \ud83d\ude09 \nAs for node-pg-async, I don't see the point of it. It offers little from the vast variety of features this library has.\n. @langpavel two reasons why template strings are not suitable for query formatting:\n1. Most of data exchanged between the server and the client is in the form of arrays of values, objects and arrays of objects - all used implicitly, not explicitly like variables in the current scope.\nTemplate strings require formatted data to be always visible within the current scope, which requires additional code for injecting indirect data into the scope before the query can be formatted, resulting in a lot of duplicate code, and preventing the separation of concerns where a query can be fully detached from the data.\n2. Template strings are not customizable, they are limited to the syntax of ${variable}. Query formatting requires support for many formatting filters.\nThis library supports a number of formatting filters - all highly demanded and used everywhere. Examples: ^, ~, #, :name, :json, :csv, :value. These are essential, and one cannot write proper query formatting without such filters.\n. @langpavel Now that's just trolling. I'm locking the conversation, enough of your nonsense.\n. Thank you!\n. Another issue I stumbled upon in 6.x version of the driver - broken connection strings support: https://github.com/brianc/node-postgres/issues/1141\n. I keep going back and forth on this, considering to re-branch the entire node-postgres 5.1 as pg-core, just wish I had enough time to finish it all.\n. @abenhamdine thanks! It is interesting, academically. At this stage though, completely switching the driver would mean breaking compatibility all over the place. For example, I wouldn't even be able to implement such useful method as stream, and method result would be gone completely, these examples alone would break too much.\nI'm still hoping to make time and re-base node-postgres 5.1 as pg-core, so I can do the necessary fixes in the driver.\nThe main problem with that is the tests for node-postgres, I really don't like them, they are customized and unreliable (work only on Linux). And re-doing them requires a lot of time.\n. @devspacenine it is right in the first post...\n\nRelated issues raised:\n- Handling connection timeouts\n- Database disconnect handling\n. Presently, the library can work with the latest version of node-postgres, which you can force by overriding the dependency. For that, add file npm-shrinkwrap.json to your project's root, with the following content:\n\njs\n{\n  \"dependencies\": {\n    \"pg-promise\": {\n      \"version\": \"5.7\",\n      \"dependencies\": {\n        \"pg\": {\n          \"version\": \"6.x\"\n        }\n      }\n    }\n  }\n}\nThis will force the latest version of the driver, which will work, though not optimally, still using the traditional pg.connect for connections rather than the new pg-pool. For the latter, I have created new branch 6.x to start playing with.\n. ### Important Update\nNow there is full version of this library that uses the latest driver: Branch 6.x of the library, which automatically allocates a separate Pool object for each database.\nAlthough it is fully functional now, it is presently distributed as pgp package rather than pg-promise, until further notice. \n. If somebody wants to provide feedback on switching from pg-promise package to pgp, it would be much appreciated! :wink: Current version of pgp b.t.w., is 0.0.2.. @jmeas I continue improving branch 6.x right now, and just released 0.1.0. It is now an independent branch. At some point in the future the pgp package will start being released as pg-promise, just not yet.. pg-promise 6.0.0 Beta has been released.\nI am now tempted to close this issue, even though I think the issues highlighted about the connectivity may still be there.\n. Before closing this overly important issue...\nMany of the points that have been discussed here at the beginning are still as valid, as there haven't been any fixes in the driver to address them.\nHowever, pg-promise v6 (branch 6.x) has reached a point when it is as reliable as it can be, considering those issues in the driver (it is version 6.0.11 as of writing). Nonetheless, for the very same reasons, it is likely to stay as Beta for some time to come.\nFeel free to use it, and report any new issues, if found. I will continue using this issue for references later, but closing it now, as it's been here for too long.\n. @AlJohri we are way pass this... :smile:\nAre you using the latest pg-promise 7.x? It uses the very latest 7.x driver :wink: where those issues should be sorted long time ago.. Considering that the library has been in use on all existing OS-s for a very long time without any installation error, or any changes to the installation, I am inclined to think that something is broken in your local Node.js/Typings configuration.\nAccording to your error log, it bombs when trying to execute typings install. What is the version of typings that you got?\nDid you install it using npm install typings -g?\nThere is no logic why node install.js would fail there. Something is essentially broken in your local node.js environment.\n. What are the versions of your Node.js and typings?\nI would suggest reinstalling both, considering the error you are getting.\n. > My node.js version is 3.5.2 \nThere is no such version of Node.js, and never existed.\nTo check the version, run:\n$ node -v\n. Great! :)\n. Either of your approaches are wrong for this sort of thing. Never use Promise.all with this library, it is guaranteed to deplete your connection pool in no time and then go berserk.\nThe second one is not usable, as you will run into the IO limit on a single channel.\nThe only right approach is as follows...\nSplit your rows into multi-insert queries, with 1,000 - 10,000 rows per query, depending on size of each row, and then execute each query.\nI recommend doing it within a transaction.\nYou definitely should read this: Performance Boost.\nAnd you definitely should use helpers.insert to generate your multi-row inserts.\n\nIf you do it right, it is realistic to insert 20mln rows in about 1 minute, depending on the size of rows, hardware performance, Node.js process load balancing.\nAlso consider using a sequence. It will relieve your CPU load at the same time. See Massive Transactions.\n. This library returns exactly what the driver sends back, unchanged. If there is any issue like that, it should be logged against the driver.\n. It is a feature. See http://stackoverflow.com/questions/37782423/query-formatting-for-parameterized-queries/37790918#37790918\nYou have no access to the library's query formatting from inside Prepared Statements or Parameterized Queries, because those are formatted by the server.\n. Interesting... I will check it later this evening.\n\nPg-promise 1.4.4\n\n@boris-hocde That can't be right... the current version is 5.3.3\n. I did some testing....\nThought you were kidding quoting pg-promise version as 1.4.4, but you weren't. That version did indeed have the issue you described.\nThe real question - why are you using such a prehistoric version of the library? The current version is 5.3.3.\nAnd that issue was fixed ages ago. Just upgrade the library.\n. See Functions and Procedures.\nNo other feature is needed, rows are all fetched automatically, if the function returns a recordset.\n. See Functions and Procedures.\n. This is the only safe way to do it within a JavaScript environment, so this is how pg-types does it. Basically, it is necessary because direct conversion into a number cannot guarantee precision.\nSee also pg-promise returns integers as strings. For numeric typeid is 1700, but the idea is exactly the same.\n. For that there is pg-monitor.\nOr you can log it yourself, by handling event query:\njs\n// library initialization options:\nvar options = {\n    query: function (e) {\n        console.log('QUERY:', e.query);\n        if (e.params) {\n            console.log('PARAMS:', e.params);\n        }\n    }\n};\n. Did you use the basic monitor.attach(options)?\n. that should work. see what's happening... maybe you are doing something else there that breaks it...\n. > Ok i see it working but it is duplicating the log.\nthat's what you code does, you either log through monitor or event query, not both.\n\ndoes pg-promise makes any parsing on dates?\n\nNo. The underlying driver does, converting Date/Time into a  Javascript Date object.\n. You do as you like.\nWhat's wrong with the current Date presentation?\n. pg-types does all the conversion, which can be accessed via pgp.pg.types, if you want to change the formatter.\n. That's actually the standard way to add your own type parsers within that library. It is not in any way cumbersome, but it is global.\n. > his is printing all the queries, is there a way to just print when I want it?\nFormat the query via pap.as.format, and print the result.\n. Your link is invalid, and if it were valid, you would see that the file there says:\n// deprecated. see https://github.com/vitaly-t/pg-promise/tree/master/typescript.\n. No, no such plans. The TypeScripts will remain shipping with the library, for now.\n. @gkiely there is method helpers.insert for it.\n. Define a function to get your first-property value:\njs\nvar scalar = a => a[Object.keys(a)[0]];\nthen you can write your queries like this:\njs\ndb.one(query, values, scalar);\nIt is better, being explicit, and lets you do more specific stuff, like defining this:\njs\nvar scalarInt = a => +a[Object.keys(a)[0]];\nAs you can see, value-transformation parameter that comes with query methods is more flexible, it doesn't fix you to a specific transformation, you can do anything with the value - all through a single method parameter.\nThis is why value-transformation was added in the first place - to simplify any specific value transformation that's needed.\n\n@BrianRosamilia \nAnother typical example of using the transformation parameter:\njs\ndb.one('SELECT count(*) FROM table', [], a => +a.count);\nExtracting the right property + converting it into integer in one operation, is as simple as it gets \ud83d\ude09 \n. > objects are the minimum a user is able to work with\nRow objects are the basic elements that any application needs.\n\nAlso your library already coerces integers, no need to have a scalarInt function\n\nNot quite. All 64-bit integers arrive as strings (see why), like in that count example I showed earlier.\n. The bottom line is, this is not an ORM, it is lower-level, to work with queries directly, to allow access to complete underlying structure, for maximum flexibility. In that purpose it succeeds.\n. From documentation:\n\nTasks are for executing multiple queries against the same connection. Also see Chaining Queries.\n\nEach query acquires its own connection from the pool, and then releases it back when the query is finished. Tasks make it possible to use the same connection for multiple queries.\n. > like node-Postgres does by default\nWhat do you call the default? There is a recommended approach to query execution within node-postgres, using the connection pool, which is what this library is built upon. It is best in terms of performance and predictable connection management.\n\nDoesn't the current implementation encourage opening a new db connection for every http request\n\nIt does, and this is what this library encourages also. You should use direct query execution when only one query needs to be executed, and you should use tasks when executing more than one query.\n\nthe only other way is to basically wrap your whole application in a db.task\n\nNot the whole application, only those http handlers that require execution of more than one query.\nYou can, of course, wrap all http handlers into tasks, if you want, it won't affect the performance.\n. You are correct. The pool uses a configurable delay for live connections, which defaults to 30 seconds.\nNote to you, when we get a connection from the pool, it is virtualized, and never guaranteed to be the same as before. This is what tasks do, they only release connections back to the pool after the task is complete, so you are guaranteed the same physical connection.\n. They presumably will. Note, presumably, because we indeed have finished the first query, but due asynchronous nature of Node.js IO and promises, there can be a query elsewhere that will acquire the connection in between, pending a racing condition, so there is never any guarantee.\nIn fact, if you do asynchronous testing, you will run into it more often than you think, when a single chain of queries is suddenly using more than one connection.\nAnd in a highly asynchronous application it will be happening all the time.\n\nso shouldn't the client from the first one be idle and waiting in the pool?\n\nNo. Just as the query finishes, it releases the connection back to the pool, making it available to whatever next query asks for it.\n. The issue is not with performance, but with the fact that you get very limited number of physical connections that you have to use concurrently. For that one needs a reliable mechanism for controlling the number of connection being used, trying to keep them to the minimum.\nAnother side of the coin - transactions, which require use of the same physical connection.\nIf you do not make proper considerations for how your connections are used, your app may deplete them quickly, and then it will be sitting idle on requests, waiting for a connection to become available in the pool, now that's really bad for performance.\n. All these considerations are not some sort of invention in this library, they are inherent from node-postgres, which in turn inherits them from what Node.js can do in terms of concurrent IO + PostgreSQL limits.\nYou can set the number of connections of up to 100, but at that point you will start seeing Node.js choking trying to process so many physical connections in a single process. I'd say, 20 is a resonable maximum.\n. There is no TypeScript 2.0 support yet. I can't say at this point when it will be added. It may require a lot of re-work, for which I don't have the kind of time at present.\nAny contribution for this would be welcome. :wink:\n. @mauriciovigolo do you use a custom promise library or the standard ES6 Promise?\nThe reason we have an external promise module is to overcome a shortcoming in TypeScript 1.x - lack of parameterized template support. I heard they have fixed it in 2.0.\nThis is the first thing that will need to be fixed in typescripts. After that, I will consider moving it away from the project and into the definitely typed repository.\nI just don't have time for any of that at present. I'm not sure about accepting an incomplete change in TypeScript at the moment.\n. @blakeembrey Could you, please, add what you think should be done, if anything, as opposed to what's not to do.\nI haven't had a chance to get into TypeScript 2.0 specifics yet.\n. Initial changes have been made (thanks to @xiamx), and released in 5.4.3.\nOther things that need to be looked at, in order to finalize the changes:\n- Update pg-minify in the same way as spex, re-release and then update pg-promise\n- Re-test and update TypeScript installation instructions for the library.\n- Redo the custom promises support fully, since TypeScript 2.0 supports custom promises\nPR-s are welcome! :wink:\n. @xiamx Also, should we now just remove file install.js and command \"postinstall\": \"node install.js\" from package.json?\n. @xiamx cheers, all removed in https://github.com/vitaly-t/pg-promise/commit/4a91ca9b04dae44f6a8ed7c644dfe0457d0542f3\n. > We still need to add a reference here\n@xiamx if I understand right, this is optional, just to make the package searchable - right?. Although there are still small details to look into, like updating TypeScript for pg-minify and then updating pg-promise accordingly, the main objective here for the library itself has been achieved, so I'm closing the issue.\n@xiamx if you can help me with the same type of PR for pg-minify as you did for spex, that'd much appreciated!\nThanks everybody!\n. @xiamx and finished, as far as pg-minify goes.\nNow the only outstanding piece is [Custom Promises Support].\nIf you know how to do it - please help out ;)\n[Custom Promises Support]:https://github.com/vitaly-t/pg-promise/blob/master/typescript/ext-promise.d.ts. @mauriciovigolo I'm not so sure.\nIt is just about how to add a custom promise library, but more importantly - how to parameterize the library correctly to use the custom promise library.\nIf you think you know how to do it right - please, add a PR ;)\n. I'm re-opening it in the meantime, for easier access to this discussion ;)\n. @bIgBV are you using TypeScript 2.x?\n. @mauriciovigolo would you be able to comment on this? I'm a little lost as to what it wouldn't work as reported above...\n@sbusch99 were you able t sort this out eventually?\n\nIdeally, if there are still issues, it would be better to close this one and open a new one specific to the problem.\n. @bIgBV and @sbusch99 \nGuys, if you are still experiencing problems with the TypeScript support, please open a separate issue specific to your problem, it will be easier to track.\nI'm closing this one, because general TypeScript support is finished, and I see no problems there as of yet.\n. Are you trying to create a JSON object that represents the entire array or you want an array where each value is turned into a JSON object? Sounds like the latter, but to be sure...\nI'm trying to understand the expected output format, before making a suggestion.\n\nConsider the following example:\n``` js\nvar toArrayOfJSON = arr => ({\n    _rawDBType: true,\n    formatDBType: () => arr.map(pgp.as.json).join()\n});\nvar data = ['one', 123, {val: 'text'}];\nvar s = pgp.as.format('Result: $1', [toArrayOfJSON(data)]);\nconsole.log(s);\n//=> Result: '\"one\"',123,{\"val\":\"text\"}\n```\n@kay999 Is that what you are trying to achieve?\n. First, remove your Array override, this is though possible, also dangerous, as it affects too much.\nSecond, use :json override, like this:\njs\ndb.query('update table set data=$2:json where id=$1', [id, data]);\nor in case you prefer to use objects, then:\njs\ndb.query('update table set data=$[data:json] where id=$[id]', { id, data });\n. So what's wrong then with the original solution that I gave you? It does exactly what you are asking for.\nWait,...I will update it...\nThis works just fine:\n``` js\nvar asJson = value => ({\n    _rawDBType: true,\n    formatDBType: () => pgp.as.json(value)\n});\nvar data = ['one', 123, {val: 'text'}];\nvar s = pgp.as.format('Result: $1', [asJson(data)]);\nconsole.log(s);\n//=> Result: '[\"one\",123,{\"val\":\"text\"}]'\n```\n\nI suspect that this behavior is easily patchable just by using a \"map\" on the values array instead of translating the whole array from top-level recursively.\nThis is IMO quite unexpected so It took me some time to figure out why positional parameters doesn't work. If you don't intend to change this, please add a small note in the docs about this behavior.\n\nActually, everything works exactly as expected. You simply ran into the situation that's documented in Custom Type Formatting:\n\nPlease note that the return result from formatDBType may even affect the formatting syntax expected within parameter query, as explained below.\nIf you pass in values as an object that has function formatDBType, and that function returns an array, then your query is expected to use $1, $2 as the formatting syntax.\nAnd if formatDBType in that case returns a custom-type object that doesn't support custom formatting, then query will be expected to use $propName as the formatting syntax.\n\nIn other words, your Array override affects more than what you think, and as a side effect to this it only works via the objects, while the array of parameters mutates / receives a different interpretation.\n. > instead of '[1,2]', 'x'\nx is not a valid JSON literal presentation, it is \"x\".\nYour expectation is an invalid JSON.\nIf you want JSON for just one field, then use it accordingly:\njs\npgp.as.format('Result: $1, $2', [asJson([1, 2]), 'x']\n//=> '[1,2]', 'x'\n. You can wrap into a Custom-Type function any type, and format it in any way you like, as shown with function asJson.\nI'm not sure though why one would need it. My feeling is telling me there is something you are trying to achieve that can already be done automatically, but you do not use it. This is the most common case with this library :) Reading documentation and many examples usually helps ;)\n\nBut if I convert all input data this way normal text fields will get still the wrong input.\n\nThe wrong input? There is only one presentation for strings, which this library supports automatically. I don't understand what wrong input you are getting...\n. It was a standard solution anyway, not the Array extension. So I think it is sorted now.\n. F.Y.I. Important breaking change in v6.5.0.. postgres read replicas - I never came across such a thing, no idea what that is.\n. Do I understand you right, that you want to set aside a single dedicated connection for some special read-only operations?\n. Normally, all connections are automatically acquired from the connection pool and then released back.\nThere can be however some very special cases when one might want to create a connection separately from the pool, like in case of a permanent event listener LISTEN / NOTIFY, as I show here: LISTEN / NOTIFY.\nFrom what you told me, it sounds like another special case when you want to declare a global connection to be used for some special cases.\nThis means you can create it by calling db.connect({direct: true}). This creates a new connection outside of the pool, see the API for method connect.\nBut be careful, because:\n1. It is not meant to be scalable, you cannot just keep creating and releasing such direct connections;\n2. Since it is out of the connection pool's control, such connection is vulnerable to unreliable database connections, meaning that if you lose your database connection, your object becomes dead, and you would need to pick it up yourself, whereas the connection pool would do it for you automatically.\n3. You should not use it to execute queries that can change data (it is for read-only queries).\n. You can either use prepared statements, or if you only send in dynamic column names, then you should use SQL Names to escape them properly.\n. Not quite. As I said earlier, you should use SQL Names, do not the table name directly.\nBetter still, using helpers.insert is so much better ;)\n. > I can't see how to make result sets from a prior operation available to the next operation\nThis is a generic promise question, not exactly related to this library. See how it is done with promises in general.\n. See http://stackoverflow.com/questions/28250680/how-do-i-access-previous-promise-results-in-a-then-chain\n. The problem is, I went quite a length with this project to make sure every promise library is treated the same.\nAlso to consider, that Bluebird today is not even the most commonly used library with pg-promise, ES6 Promise is, because it is the default. I know it because I keep looking through the projects that are hosted on GitHub, and this is what I see.\nI am really not comfortable with introducing something specific to one promise library.\n. > There are valid user-parameterised queries, that can take longer (5 - 10 minutes)\n@boromisp \nSuch queries are implemented differently, never directly...\nYou send a query that triggers a job on the server, sets a notification and returns immediately. Once the job has finished, it provides a notification back, at which point the client can either receive the simple signal that the job has finished, or re-queries data from a temporary/prepared table, and cancels the notification.\nSee also: LISTEN/NOTIFY.\n. Type ColumnSet is used for generating queries. If you have a column that's never to be generated, then why have it in the ColumnSet in the first place?\nAre you trying to reuse a ColumnSet object for both UPDATE and INSERT?\n. This code:\n``` js\nclass Persona(){\n    constructor(){\n        this.InsertColumnSet = pgp.helpers.ColumnSet([\n            {\n                name:'name',\n                skip:function (name){\n                    return !this.hasOwnProperty(name);\n                }\n            },\n            {\n                name:'surname',\n                skip:function (name){\n                    return !this.hasOwnProperty(name);\n                }\n            },\n            {\n                name:'city',\n                skip:function (name){\n                    return !this.hasOwnProperty(name);\n                }\n            }          \n        ], {table:{table:'person', schema:'public'}});\n    this.ColumnSet = this.InsertColumnSet.extend(['?idPerson']);\n}\n\n}\n```\ncan be reduced to:\njs\nclass Persona(){\n    constructor(){\n        this.table = pgp.helpers.TableName('person', 'public');\n        this.csInsert = pgp.helpers.ColumnSet(['name', 'surname', 'city'], {table: this.table});\n        this.csUpdate = this.csInsert.extend(['?idPerson']);\n    }\n}\nI don't understand why you do this:\njs\n                skip:function (name){\n                    return !this.hasOwnProperty(name);\n                }\nas this is the default behavior already, i.e. option inherit for ColumnSet is false by default.\nIf you wanted to do the opposite:\n``` js\n                skip:function (name){\n                    return this.hasOwnProperty(name);\n                }\n```\nthen you would simply add option inherit: true when constructing your ColumnSet object.\n. Uh, ok, I got you now, I wasn't looking at the code close enough to see you were checking for the object properties and not columns :)\nIn this case I would just use something like this:\n``` js\nfunction prepareColumns(cols) {\n    return cols.map(function (c) {\n        return {\n            name: c,\n            skip: function (name) {\n                return !this.hasOwnProperty(name);\n            }\n        };\n    });\n}\nclass Persona {\n    constructor() {\n        this.table = pgp.helpers.TableName('person', 'public');\n        this.csInsert = pgp.helpers.ColumnSet(prepareColumns(['name', 'surname', 'city']), {table: this.table});\n        this.csUpdate = this.csInsert.extend(['?idPerson']);\n    }\n}\n```\n. n.p. ;)\n. @jmpmscorp This story continued as feature request #239 \n. By any chance this one is related? - https://github.com/vitaly-t/pg-promise/issues/207\n. The same way as for other methods:\njs\ndb.proc('foo', [], a => a.returned_num)\nAnd yes, it is all by design.\nIn PostgreSQL, a procedure is like a function that returns 1 or no values. That's why it already returns just one object, as opposed to an array, like method func does (by default).\nIf you want a scalar function for your query, you should use method one with transformation + conversion:\njs\ngetCount => db.one('SELECT count(*) FROM table', [], a => +a.count);\nIn most cases you need not only the value, but also a conversion, like in this example (count returns a 64-bit number as a string, as it does for all 64-bit numbers), so a dedicated scalar function would be of very little use.\n. > I hadn't realized Postgres actually treats the return value of foo as a \"result-set\" like object itself\nWell, PostgreSQL iteself doesn't, it returns just raw data, but the underlying driver node-postgres does, converting all returned rows into objects.\n. > I'm closing all connections after all DB queries are executed.\nYou should absolutely NOT do that. Calling pgp.end() shuts down the connection pool, and effectively the entire library. It is only to be called when implementing a command-line utility that needs to exit right after it is finished.\n\nPlease, correct me if I'm wrong. But I still see idle connections in DB after pgp.end(). Why?\n\nI'm not sure what kind of side effect one might run into after trying to call pgp.end() repetitiously - something it was never designed for.\nAlso, you should not use Promise.all with the library when resolving queries.\n. Sorry, I know nothing about  AWS Lambda.\n. > i'm running out of all available pool connections\nThat means there is an issue in how you use your connections, which is what should be fixed. You are probably executing queries in a way that depletes your connections.\n\nCan I use pg-promise without pool connection\n\nSuch a solution cannot scale well.\n. I'm afraid you are looking at it from the wrong angle. If you use the connection pool correctly, it will give you no trouble. You just need to understand better what that correct pool usage actually means.\nIs your code published on GitHub?\nYou can always of course just create and use one global connection outside of the pool, but it has too many downsides:\n1. Such approach won't allow automatically restore connection when it is lost\n2. It won't let you use transactions, as each transaction requires its own connection.\n. Those issues relate to invalid use of the pool.\n. > What am I doing bad?\nSpelling. It is ParameterizedQuery.\nAlso, simply doing console.log(pgp) would show you what's defined there.\n. See my comments in the other PR: https://github.com/vitaly-t/pg-promise/pull/225\n. As per my comments for this PR - bluebird 3.x cancellation support for db.query, a proper solution for long queries is via LISTEN/NOTIFY, not through cancelling them, which is basically a hack, as far as the client->server relation goes.\nSorry, but I cannot accept this PR, I see it as a bad pattern for writing database services in general.\nAnyway, I really appreciate the effort and time you put into this.\n. Does this PR simply create a copy of the existing .ts file?\nI'm trying to understand what else, if anything, it does...\n. @blakeembrey my hesitation to merge this comes from being not familiar what TypeScript 2.0 really brings different, and the lack of time at present to get into all that.\nI appreciate someone with good understanding of the changes making suggestions \ud83d\ude09 \n. @mauriciovigolo Thank you for coming back and closing it. I hope the comments made by the author of typings are usable for you :wink:\n. Thanks!\n. This warning relates to the underlying driver, and not immediately to this library. This will get resolved when the time is right. At the moment it is not, as the driver got a number of its own problems, and not really suitable for an upgrade. By the time that \"soon\" becomes \"now\" it will be resolved, somehow.\n. Version 7.2.1 of Node.js removed both the warning and the plans for making the change in 7.x :smile:\nProblem solved! \ud83d\ude38 . How is it that you are using .then for the result from the connect that does not return any promise?\nSomething in your execution path is wrong. And method oneOrNone always returns a promise. I'm just not sure you even get there.\n. Can you even see method oneOrNone being executed? You can always add some logging to see whether you are getting inside that method at all.\nIt would be easier on your side to diagnose what is going on in your code than me guessing it. I know that oneOrNone reliably returns a promise.\n. Method oneOrNone uses generic execution logic which works always, with exhaustive tests to prove that, and long usage history without anyone reporting any issue there. After all, this is the core functionality of the library.\nYour description of the issue seems a bit odd. I would suggest debugging it further to see what really is going there underneath.\n. no, that looks exactly like an unresolved Promise object returned by bluebird.\ni.e. the same as:\n``` js\nvar Promise = require('bluebird');\nconsole.log(new Promise(function (res) {\n}));\n/=>\nPromise {\n  _bitField: 0,\n  _fulfillmentHandler0: undefined,\n  _rejectionHandler0: undefined,\n  _promise0: undefined,\n  _receiver0: undefined,\n  _trace:\n/\n```\n. Try to track the execution path inside the method. Could there be something that suddenly interrupts the execution?\nI mean, there shouldn't be, because the method blocks them all, but I don't know where else to start...\n@paulxtiseo please let us know about your findings ;)\n. @paulxtiseo have you made any progress with it? It would be nice to close the issue knowing what was causing it :wink: \n. Thank you for coming back on this! I will re-open it, if needed later.\nMy only guess so far - some call gets interrupted by Mocha, so it doesn't give that promise a chance to settle.\n. This is an SQL-level error, not relevant to this library. Try StackOverflow :wink:\nClearly, you are passing in something that's not expected, possibly due to not using the right type cast.\n. @xiamx spex has been merged and released as version 1.1.\nDo you need to make some changes here before I accept the merge? (that's what you said above).\n. @xiamx I see the change doesn't build under Node.js 4, 6 and 7, due to an issue with the TypeScript test:\nFailures:\n  1) Typescript build must build without error\n   Message:\n     Expected { killed : false, code : 127, signal : null, cmd : 'tsc' } to be null.\n   Stacktrace:\n     Error: Expected { killed : false, code : 127, signal : null, cmd : 'tsc' } to be null.\n    at /home/travis/build/vitaly-t/pg-promise/test/typescriptSpec.js:8:31\n    at ChildProcess.exithandler (child_process.js:218:5)\n    at emitTwo (events.js:106:13)\n    at ChildProcess.emit (events.js:191:7)\n    at maybeClose (internal/child_process.js:885:16)\n    at Socket.<anonymous> (internal/child_process.js:334:11)\n    at emitOne (events.js:96:13)\n    at Socket.emit (events.js:188:7)\n    at Pipe._handle.close [as _onclose] (net.js:501:12)\n. @xiamx Thank you! \ud83d\udc4d \nWe now need to update the following:\nhttps://github.com/vitaly-t/pg-promise/tree/master/typescript (documentation)\nhttps://github.com/vitaly-t/pg-promise/blob/master/typescript/ext-promise.d.ts (approach to the use of custom promises, since TypeScript 2.0 now supports it, presumably)\n. I have published a lot about it in the project's chat, and just to reiterate that here...\nThe SQL syntax doesn't allow for skipping values for multi-row inserts, only for single-row inserts.\nFor single-row inserts, property cnd (conditional column) cannot be used, because there are no conditions that can be appended to the insert query.\nThis leaves only one scenario: use of skip callback for single-row inserts. And I'm still considering the real value of adding support for this one case.\nThe original idea of adding it was so that we could re-use ColumnSet object between updates and inserts. However, I expect that the logic for skipping columns would be in most cases different, which is what makes the whole idea dubious.\n. Only when I start thinking about this in the context of adding a single record, so we can simplify building an insert, then I can see some benefit in supporting it, but perhaps not that much...\nHowever, what I don't like is that not using cnd makes it a little inconsistent with updates, while using it is pointless, because there is no scenario where inserts could use it.\n. @skatcat31 you haven't come back on this ever since, so I'm closing it for now, due to inactivity.. @skatcat31 you are talking about use of skip logic for single-row inserts, specifically. Even cnd rule cannot be applied there as being pointless.\nThis is why I questioned the need of supporting it.. @skatcat31 \nyes, that, plus the useful methods extend and merge is what makes it easy configuring/mutating ColumnSet objects ;)\n. @erndob Cheers! \ud83d\ude38 \n. After a careful consideration I've decided against this feature, as I remembered why it wasn't done in the first place, as explained below.\nWhen I introduced methods map and each, we got a clear approach as to what kind of transformation is to be applied: \n\nmethod map returns a new array with transformed values\nmethod each returns the same array with transformed values\n\nIf we add the transformation logic into methods many, manyOrNone or any, we would make such calls ambiguous as to the transformation logic - should it be the map or each logic. And adding an extra flag for it would make the protocol too convoluted, which nobody wants.\nThis is why it's a no go. Better stick with calls that are explicit in their intention, makes it easier to read the database logic.\n. @MitMaro thanks, it is a duplicate indeed.\n@paleite it will get attention by the time Node.js 7.x becomes an LTS ;)\nGranted, the driver's support is less than good these days, so I'm looking at alternatives.. Version 7.2.1 of Node.js removed both the warning and the plans for making the change in 7.x :smile:\nProblem solved! \ud83d\ude38 . @josefzamrzla Please follow up with an explanation of why you think this is needed.\nThis library already includes event query, which let's you know when every query is about to execute, and it has event receive, which tells you when you get any data, so effectively whenever a query finishes.\nSo why would you need a separate finish event? Also consider that when any query request resolves, you also get your event finish point.\nPlease add a task example (description) that you think you cannot do with the currently supported list of events, so I can understand the need better, or see if something is being missed.. @josefzamrzla Thanks!\nI'll have another look at it later in the week. It will require writing tests before it can be submitted.\n. @josefzamrzla this library isn't the best way to approach query performance optimization, you should rely on the internal database performance analysis, using query EXPLAIN.. I understand your reasoning for doing this, but im trying to figure out just how generic this is for anybody else to need it... this is the first time for this kind of request, and i like avoiding bloating the library when possible ;). @josefzamrzla sorry for the delay with decision on this, but I'm still not convinced that anybody else would need this feature. It would only make a tiny helper for diagnosing performance, and not a very good one at that.\n. Closing, for the lack of value to the community. It is more important to keep the code clean than to keep extending it with something of little value.\n. Re-asked in https://github.com/vitaly-t/pg-promise/issues/283. Yes, via method connect with option direct.\nPlease use StackOverflow for questions, here's just for issues reporting.. There is more than one way to implement UPSERT logic within PostgreSQL today. Which one do you refer to as the newish?\n. The existing helpers.insert already allows you to do that. You generate the insert, and then simply append the ON CONFLICT clause.. May I assume this has been answered? Closing, since there was no reply.. @davegri The supported syntax for upsert operations in PostgreSQL is too verbose to be able to generate them automatically. They are extensible too much, and I do not see value in generating an SQL that represents only a small sub-syntax of what is possible, while supporting the full syntax is unrealistic.\nHere're just couple scenarious:\n\nhttp://stackoverflow.com/questions/15939902/is-select-or-insert-in-a-function-prone-to-race-conditions\nhttp://stackoverflow.com/questions/36083669/get-id-from-a-conditional-insert\n\nThe upsert logic can be infinitly complex, that's why this library does not generate any upsert stuff.\n\nSee also: SELECT=>INSERT example.. > Could we have an option for generating the update clause without the table name?\nOf course, this is why we have all the low-level methods available: helpers.sets, helpers.values, ColumnSet.names, ColumnSet.values, etc.\nIt is all there so you can combine them into a custom and more complex insert or update scenario.\nSee helpers.. @AlJohri this is one of those examples of going about it in the wrong way.\nThe rule of the thumb - over-complicating queries construction doesn't pay. All complex queries should instead reside in their own SQL files, see Query Files and pg-promise-demo.\nAnd if you need to run a multi-insert or multi-update, you inject the result of the corresponding helpers.insert and helpers.update methods into the SQL via a raw-text variable.. @AlJohri there are only so many basic methods we can think of, while the rest should be easy to compile from those. The full syntax for inserts and updates can be too verbose sometimes to try covering everything.\nIn your example you've got close enough to doing what you need. Couple of notes though:\n\nSince you access the column names directly, you need to escape those correctly\nYou can use join() instead of join(', '), as the result is the same\n\nIn all, change you code to this, and it'll be fine:\njs\nfunction upsertReplaceQuery(data, cs) {\n    return pgp.helpers.insert(data, cs) +\n        ' ON CONFLICT(id) DO UPDATE SET ' +\n        cs.columns.map(x => {\n            var col = pgp.as.name(x.name);\n            return col + ' = EXCLUDED.' + col;\n        }).join();\n}\nUPDATE\nA more recent version of the library started supporting method ColumnSet.assignColumns, so the last part can be simplified:\njs\nfunction upsertReplaceQuery(data, cs) {\n    return pgp.helpers.insert(data, cs) +\n        ' ON CONFLICT(id) DO UPDATE SET ' +\n                cs.assignColumns({from: 'EXCLUDED', skip: 'id'});\n}\nNote: I've added the skip part because you typically would want to skip updating the key column(s) on which the conflict occurred.\n. @MahimaSrikanta The ON CONFLICT part is simply appended to the insert that's generated. If it is not supported, you simply do not append it, and use one of the older solutions that worked in PostgreSQL v9.4. You can find those on StackOverflow.\n. Only noticed it now :)\nThe maximum payload size for NOTIFY in PostgreSQL is 8000 bytes. Passing in a longer string will result in an error :wink:. It is only as strange as your query returning no data from time to time. Otherwise the library wouldn't report this to you. You need to analyze your data logic and/or changes to see where the problem lies. It is certainly not with the library.\n. If you simply switch to method any and log whenever 0 records are returned, you will get the same results, guaranteed. So again, this is either an issue with your database logic or the parameter values you are passing in, not with the library, that's why I'm closing it.. @mathisonian were you able to get to the bottom of it? :wink:\n. It is not realistic, considering the features QueryFile has that are specific to this library:\n\nuse of pg-minify for compressing SQL (designed for PostgreSQL only)\nreliance on the library's query formatting engine to format the SQL after loading (for two-step query formatting)\ncustom protocol + promise logic for detecting issues: if (qf.error) {reject(qf.error)} else {resolve(qf.query)} + qf.prepare()\n\nAs you can see, QueryFile is very tightly integrated with the rest of the library, and trying to separate it would require a lot of extra work, writing substantial dependency replacements, which in my mind isn't worth doing.. Implemented in version 5.5.0.\n. Implemented in version 5.5.0.. Spreading values isn't very useful, because way too often you would need to provide certain modifiers for your values.\nFor fully automatic query generation see the helpers namespace :wink:\n. Did you see this - WHERE col IN (values)?\n:csv filter is what you should be using, not :raw\n. Closing, since there was no reply.. @popara you are welcome! \ud83d\ude38 \nAnd CSV stands for comma-separated values, which is exactly what you are getting :wink:. @abenhamdine yes, I noticed that :wink:\nIt is one of the commits I'm planning for pg-core when I get around to finishing it.\n. > what is pg-core?\n@AlJohri an unfinished attempt at branching out an independent version of the node-postgres driver.\n. @AlJohri The things is, some time ago I did that, created an independent module form node-postgres, and it all worked fine.\nThe reason I eventually axed it was that tests in the existing node-portgres are horrific.... they are very much custom-written, and they do not work on Windows at all.\nI had to rewrite a substential part of tests and dump many of them just to make the test work on on all platforms. I wasn't impressed by the reasult, and getting it to 100% didn't seem realistic at the time.\nI still have it backed up in a private branch, so may revive it later on.\nThe bottom line is, I found everything that occurred with node-postgres v5.1 a bit of a disappointment, and mostly compromizing on its quality rather than improvement. But I'm back to ranting about it :)\nSo, pg-core is still in the plans.\n. Yes, because it is not desperate at present, the node-postgrs driver didn't get any critical updates since v5.1, which remains to be its best version. I think we are good for some time forward :wink:\nAnd the fix that corresonds to this issue as was open originally is a very minor one, i.e. there is very little chance to run into it.\n. Much less, by my account, about 3 that are minor improvements, the rest is unimportant.\nI'm planning to have this pg-core thing working for version 6.0.0 of the library.\nThe thing is, I would have no problem simply upgrading to the latest they got, but this thing stops me.\nP.S. For version 6 I'm also planning to dump support for Node.js 0.10.x and 0.12.x, and start requiring 4.x as the minimum, and use ES6 syntax everywhere.\n. Time is everything. I will post an update here when I get around to pg-core again. Like i said, its biggest issue will be rewriting most of the tests.\nThere is also, of course, the other option - fix the connectivity in the current version of the driver and then upgrade to it, but I don't know if it is feasible or if it is a good idea in general. The way this library now is 100% robust, and trading it for something illusive may not be a good idea.\n. I'm closing it now, because there is no point keeping just one minor issue here. There are several minor issues that have been fixed in the driver since version 5.1, and those all will need to be addressed, somehow.\nI am particularly concerned about this fresh one: https://github.com/brianc/node-postgres/issues/1286\nSome sort of a merge / branching is unavoidable later on, that's for sure. I just need to make time for it.\n. There is branch 6.x available now, for anyone who is interested :wink:\n. Closing, because it would require that QueryFile type is available only after the library's initialization, as it would require access to option noWarnings.\n. I'm not familar with the WebPack, never used it.\nIsn't WebPack primarily a tool for the front-end? Why are you packaging it?\nAnd you didn't even describe the problem that you were having.. Thank you for the clarification. Still, I know nothing about Webpack to advise you.\n. Closing it, because I don't think it will receive any attention here. Maybe it is relevant to the driver, or maybe there is an issue with how it is used in the Webpack.\nI personally won't be investigating it, since I've never used Webpack, and I'm not sure about its value for this library either, since it is a 100% server-side module.\n. The first call is with initialization options, the second one with the database connection, as per documentation.. This means you are using either ParameterizedQuery or PreparedStatement, documentation for both states that you only have access to the basic $1, $2 formatting, because those are formatted by the server.\n. It is the wrong approach. I will detail when back from holidays.. Duplicate of #143 . Can you include the code of the call you are making into the function?\n. I'm not sure what that trigger is expected to return back to you.\nI can only recommend - try use function result instead, to see if the returned object gives you anything extra on the second call.\nIf you can't find out the difference, it may be a good question to ask on StackOverflow, there may be something special regarding return of triggers, something that's outside of this library.\nAlso, you should not use ES6 strings to format queries, they will end up broken. Always use the library's own query formatting engine.\n. That is quite interesting. You are the first one to ever ask this question :smile:\nI'm thinking now whether such a notification should be a resolve or reject...\nIt kind of intercepts the response in the middle, providing extra details, regardless of whether the result is success or failure, or so it seems...\n. I'm not sure there can be an unequivocal approach to treating such notices as success or failure.\nIt is possible to handle those globally though, if you need it just for logging.... >what would be the best approach to inform a user that the user they are trying to add to teh table does not exist\nTypically, you execute something like this:\njs\ndb.oneOrNone('SELECT * FROM Users WHERE name=$1', [userName])\n    .then(user => {\n        if (user) {\n            // user found\n        } else {\n            // user not found\n        }\n    });\nsee oneOrNone.\n. At the moment you can intercept those notifications globally, like this:\n```js\nvar options = {\n    connect: function (client, dc, isFresh) {\n        if (isFresh) {\n            client.on('notice', function (msg) {\n                console.log(\"notice: %j\", msg);\n            });\n        }\n    }\n};\nvar pgp = require('pg-promise')(options);\n```\nSee event connect.\nBut it won't affect the result of your query calls.. > I think I was trying to over-engineer this problem\nLooks that way, since no-one ever pointed at this before, and the user base for this library is huge today.\nBut it was an interesting question nonetheless :wink:\nBy the way, when it comes to handling the commonly used LISTEN/NOTIFY, this library is very easy to use: https://github.com/vitaly-t/pg-promise/wiki/Learn-by-Example#listen--notify\n. I have done some estimates for adding support for custom exceptions.\nThe main trouble is the concept: The idea of non-critical errors (like warnings) is alien to promises, which means it requires a separate/dedicated method perhaps for executing queries, while also providing information about those notices.\nWe definitely cannot simply reject in the existing methods when a notice event occurs, because they are all considered non-critical.\nWhat makes matters worse, there can be any number of notices generated before the query resolves successfully.. Initial implementation to support non-critical messages has been committed.\nIt will be released, pending a few tests + documentation updates.\n\nCurrent approach issues:\n\nrepeated listener is set during the same connection\ninvalid association with the original query, results in event being sent into the wrong handler\n. It looks like there is no solution to the issues highlighted earlier, except to add the missing functionality into the driver :(\n\nI've logged the issue here: https://github.com/brianc/node-postgres/issues/1226\nI fear without that feature properly supported there is no point having it half-way, i.e. not supporting multiple queries with notifications.\nI may remove the previous commits because of that.\n. Closing again, once I've realized notifications cannot be linked to queries, makes them so useless.\n. So your question translates into how to get an object property in JavaScript...\nI don't want to sound arrogant, but this is not the place to ask such questions.\nClosing and locking.\n. This library is built on a connection pool, you should not allocate connections manually, they are all managed automatically.\nAnd pgp.end doesn't close a connection to the database, it closes all the connections and shuts down the library's connection pool. It should only be used when exiting the application.\nAnd this is the wrong idea:\njs\n    connect(uri) {\n        this.client = this.pgp(uri);\n        return this;\n    }\nIt doesn't connect to anything, see this: http://stackoverflow.com/questions/36120435/verify-database-connection-with-pg-promise-when-starting-an-app\n. > I was wondering if there is a way to close the single connection pool\nNot at present.\n\nIs there an easy way you know of to extend your Database class.\n\nSee event extend and pg-promise-demo.\n. I'm not sure that's even possible at the moment.. I wouldn't keep your hopes up on this, even most PR-s are stale there for many months.. It would require a comprehensive testing. And considering that you are doing it not for a real project, rather another framework on top of it, it probably diminishes the value of the change. I'm trying to avoid at this point changes that carry little benefit to developers who use the library directly.. If you use db.connect directly, such solution will not scale. Any application should rely on the connection pool, which works as the default. See method connect documentation.\n. Then it should be fine :wink: I was concerned that you wanted to use .connect for executing queries, but I guess I misunderstood.\nAny client application should have direct access to the db object, in order to be able to implement scalable code.. * What error are you getting back?\n* Does configuration object work for you?\n. There must be something else you are doing, as it works for me perfectly, and works for lots of other developers out there.. Try to debug it. After all a connection string is converted into a connection object, so you can see what kind of connection object you are getting and then compare it to your manually defined connection object.. @goncalopereira any update there?\n. Cheers!. Use of helpers is the best way to generate queries dynamically.\nOther than that, the library doesn't have anything like this:values, and there is little point in having it either, you should use helpers instead, because only helpers give you the flexibility to configure the values (see type Column), which cannot be done with a syntax like this:values, i.e. you have: formatting modifiers, SQL casting, value overrides, default values, etc.\n. > The solution that I used was to provide a key order to JSON.stringify, however this would require a change to node-postgres\nI don't see why this would require a change in node-postgres. It can be easily fixed inside pg-promise itself, by reordering the keys alphabetically.\nI would consider it being a very minor bug though.\n\nwhere the order of the configuration keys are different does not result in the reuse of the connection pool\n\nThis is not a correct statement. The connection pool is a singleton, it is always just one that's used. In the future though, if upgraded to the driver with support for multiple pools it may change.\n. Fixed in v.5.5.1.\nThank you for reporting the issue.\n. @MitMaro could you confirm the issue as resolved, please?\n. As per the Official Documentation:\nstep1:\njs\nvar pgp = require('pg-promise')({\n    // Initialization Options\n});\nstep2:\njs\nvar db = pgp(connection);\nCheck your code, it's not what you are doing :wink:\n. @dyllandry It is the first thing that's explained: http://vitaly-t.github.io/pg-promise/\n. You may be misunderstanding the purpose of method query.\nIt is the generic query method that can mutate the result based on parameter qrm, as per the API. This is why it cannot have any specific type pre-declared.\nFor a client application you should be using result-specific methods instead. Method query is mostly useful when writing reusable database frameworks and utilities.\nHave you seen the Learn by Example tutorial? It starts with that very explanation.. The main Readme file is the one that was written initially, and even though I keep updating it, it is still not the best reference. The API and Wiki though make up for it :wink:\nMethod any is result-specific, it means anything can be returned, while method query expects the data according to its third parameter, which specifies how to treat the result.\nCheck out some good examples, like pg-promise-demo :wink:. The duplicate connection is not so much an issue in itself, as being a bad development pattern, so no, it shouldn't try to outsmart you, it should tell straight up you that what you are doing looks wrong.\nYou should have the connection object in its own shared module, and that's it, then you won't have any warnings. See Where should I initialize pg-promise.\n. I hope this answers it ;)\n. Those should be: ${sortKey~} ${sortDir^}, which is the same as ${sortKey:name} ${sortDir:raw}, i.e. the first one is an SQL name, and the second one is just an open-text variable.\nSee also:\n\nSQL Names\nRaw Text\n\n. See the links below:\n\nhttp://stackoverflow.com/questions/20878932/are-postgresql-column-names-case-sensitive\n\ni.e. changing UserId to \"UserId\" does the trick ;)\n\nhttps://github.com/brianc/node-postgres/issues/1062\n\ni.e. that's a Node.js 6.x onward feature, showing anonymous in the console - just ignore it.\n. See  the following links:\n\nEvent receive\nArticle: https://coderwall.com/p/irklcq/pg-promise-and-case-sensitivity-in-column-names\n\n. js\nreturn db.tx(function (t) {\n    var queries = event.tables_names.map(function (name) {\n        return t.none(`REFRESH MATERIALIZED VIEW CONCURRENTLY $1~`, name);\n    });\n    return t.batch(queries);\n});. Wow! This looks like a bug in PostgreSQL...\nIt is supposed to understand the old/string array notation of '{1,2,3}' and the new one of array[1,2,3] in the exact same way, but in this particular example (for an empty array) it fails.\ni.e. if instead of passing in array[] we pass in '{}', which is the old presentation of an empty array, it succeeds. This doesn't make sense. It does look like a bug in PostgreSQL.. I've asked about it on StackOverflow: PostgreSQL fails on empty constructor arrays.\nAnd following the answers, I can see that the issue is as follows...\nThe forward type casting within multi-row updates doesn't really work, or maybe it works sometimes, but not consistently...\nThe solution is to change from the forward type casting to in-line / value type casting.\nA corresponding change is required in the library.\n. Fixed in v.5.5.3\n@fend25 Thanks for reporting! \ud83d\udc4d . @jorisdegeringel Thank you for your donation! \ud83d\udc4d . What do you intend to re-try - query execution? Then just do it as you would with any promise-based interface, there is nothing extra to consider here.\nOtherwise, please formulate more specific questions, as what you published here is just too broad.\nAlso, I'm not really sure about the DB fallback part...\n. @haysclark feel free to re-open, if you have a clarification.. You can only make a limited use of type ColumnSet, by combining methods pgp.helpers.values and pgp.as.format, because your case is too unique, you are addressing values outside of the available data.\n```js\nvar cs = new pgp.helpers.ColumnSet(['id', 'delta']);\nvar data = [{id: 1, delta: 1}, {id: 2, delta: 2}];\nvar values = pgp.helpers.values(data, cs);\n//=> (1,1),(2,2)\nvar update = pgp.as.format('UPDATE users SET balance = balance + v.delta FROM(VALUES$1^) AS v(id, delta) WHERE v.id = users.id', values);\n//=> UPDATE users SET balance = balance + v.delta FROM(VALUES(1,1),(2,2)) AS v(id, delta) WHERE v.id = users.id\n```\nAlthough the use of pgp.as.format in this specific case is optional, you can just concatenate the strings, it makes a good example in general, of proper query formatting :wink:\n. @phiresky this was a question worthy of StackOverflow :wink:\n. Re-opening it for now, because this question gave me an idea of an improvement, to be able to get the list of prepared/escaped column names for unique cases such as this one.\nTo that end, I have done an initial commit, and will re-release the library later today.\nThe idea is to use cs.names to get \"id\",\"delta\", with reference to our example, so we can use it as part of more intelligent query formatting.\nThat property .names is already there, but currently works a bit differently and marked as private. I will update its functionality and make it public for the next release.. Release v5.5.5 adds new properties names and variables to class ColumnSet.\n@phiresky now the same example can be updated to a little more civilized version:\n```js\nvar cs = new pgp.helpers.ColumnSet(['id', 'delta']);\nvar data = [{id: 1, delta: 1}, {id: 2, delta: 2}];\nvar values = pgp.helpers.values(data, cs);\n//=> (1,1),(2,2)\nvar update = pgp.as.format(UPDATE users SET balance = balance + v.delta FROM (VALUES $1^)\n                            AS v($2^) WHERE v.id = users.id, [values, cs.names]);\n//=> UPDATE users SET balance = balance + v.delta FROM (VALUES (1,1),(2,2))\n//=> AS v(\"id\", \"delta\") WHERE v.id = users.id\n``\ni.e. we can now usecs.names` to get the list of escaped column names.\n. Are you asking about how to monitor queries being executed by the library?\nThere is event query for that, and there is pg-monitor module.\nSee also this example.\n. Number of rows inserted? You've lost me. Whatever rows you insert, that's the number.\nWhat do you want the library to tell you and in what particular case?\nAnd method .batch resolves with whatever the set of promises/values you provide there.\n. How is this relevant to this library? This is a pure SQL question.\n. If you are inserting inside a transaction, then your .catch will contain the error details. But if you are inserting from another select that fails for one specific value - i'm not even sure how to debug it, rather than use pgAdmin and see where the problem is, since it is all just one query.\nAnyhow, the question is not relevant to this library, so i'm closing it.. That means your insert is successful, but perhaps your select is simply returning less data than you hoped for, which is a data/business problem, that needs database logic analysis of what is going on there.. This library does not do any formatting for the values that arrive from the server, it is all done by the node-postgres driver, which in turn uses its pg-types sub-module, which you can access from pg-promise via pgp.pg.types.\nSee also: How to access the instance of node-postgres that's used?.\n. Please provide the following...\n\nThe complete SQL file that you have\nThe exact values for timezone and s3path variables that you are passing in\n\nThen I will be able to test it.\n. @benoittgt are you saying that you followed the test steps and they didn't work?\n. I've got the very latest version of Chrome here, but I do not see the problem. Before changing anything, I'd like to be able to reproduce the problem, so I can understand the fix.\nPlease provide details about the device + DPI + resolution where you're experiencing the problem.\n. @benoittgt updated documentation has been re-deployed. Thank you for the PR!\n. Such questions are best asked on StackOverflow.\nYou have 2 places where you create promises that are not chained, and thus become loose:\n1\njs\nvar lId = t.one('SELECT list_id from Lists WHERE list_title = $1', [lName]);\n````\n**2**js\nt.none('INSERT INTO ListWords (word_id, list_id) '\n                                      + 'VALUES ($1, $2);', [wId.word_id, lId])\n```\nYou can't do it, it goes against the promises theory, they all must be chained.\n\nI would advise you how to fix it, but it is not clear why you are calling this\njs\nvar lId = t.one('SELECT list_id from Lists WHERE list_title = $1', [lName]);\nas you are not using its results in yor example.\n. See how files are organized in pg-promise-demo.\nIn the demo you can see each SQL using variable ${schema~} to inject a schema.\nUsing the same approach you can link multiple SQL files into a single file, although you would inject them as raw text (:raw / ^), like this: ${externalFile^}, then parameterize the constructor accordingly.\nIn that demo we use a plain approach to declaring all SQL files at once. You can deviate from it, by declaring first the ones that have no dependency, and then inject ones with dependencies, while paramerizing them with already created QueryFile objects using option params used by the constructor.. Here, I've come up with a simple example of it...\n```js\nvar QueryFile = require('pg-promise').QueryFile;\nvar path = require('path');\n// independent queries:\nvar queries = {\n    one: sql('sql/one.sql')\n};\n// adding queries with dependencies:\nqueries.two = sql('sql/two.sql', queries.one);\nfunction sql(file, external) {\n    var fullPath = path.join(__dirname, file); // generating full path;\n    var options = {\n        params: {\n            external: external\n        }\n    };\n    return new QueryFile(fullPath, options);\n}\nmodule.exports = queries;\n**sql/one.sql**sql\nFIRST QUERY\n**sql/two.sql**sql\nSECOND QUERY\n${external^}\n```\nYou can obviously modify it to support a whole array of dependencies that way :wink:\n. Closing now, for the lack of feedback.. If you make the initialization sequence correct, then it will work exactly as in the example, which by the way was tested.\nThe issue you refer to is about not being able to use null/undefined values for a raw-text variable, as interpretation of those becomes ambiguous. It is not relevant to query files, it is a feature of the library's query formatting engine.\nAlternatively, you can just skip setting params when it is not needed :wink:\nAnyhow, glad it worked for you! :smile:\n. Something to keep in mind... the very useful option debug of type QueryFile will not work for nested SQL files, obviously :wink: See http://vitaly-t.github.io/pg-promise/QueryFile.html#.Options\n. @allthesignals \n```js\nconst paginateQuery = new pgp.QueryFile(path, options);\nfunction paginate(values) {\n    return {\n        toPostgres: () => pgp.as.format(paginateQuery, values),\n        rawType: true\n    };\n}\n``\n. You should never use.toPostgres, which defeats the idea of [custom type formatting](https://github.com/vitaly-t/pg-promise#custom-type-formatting), which can only work as an object, like I showed in the code above. Otherwise, it will ignore flagrawType`, and try re-format SQL that's pre-formatted.\n. Ops, yes, I've corrected the code ;). @allthesignals \nA small correction, after reviewing the code. I forgot that I implemented auto-update within the regular query formatting, so using this approach will guarantee auto-update automatically:\n```js\nconst paginateQuery = new pgp.QueryFile(path, options);\nconst paginate = values => ({\n        toPostgres: () => pgp.as.format(paginateQuery, values),\n        rawType: true\n});\n```\nNo extra provision needed.\nAnd if you need to inject a QueryFile without formatting parameters, then you can pass it in directly.\n. Event query is reported for every query executed, which happens just before it executes.\nMany requests provide additional details about the duration, but that's only available locally, not globally.\nAlso see: https://github.com/vitaly-t/pg-promise/pull/243, I wonder if it is worth reviving now... :)\n. > and receive doesn't seem to fire all the time\nAs per the API, it only fires when there is data received, to allow post-processing.. As I mentioned earlier, most of methods expose property duration on the result, like any, many, manyOrNone, result, stream, etc, though it is not available for such methods as one, none and oneOrNone. In other words, those resolving with an array have each array extended with invisible property duration, plus the ones that are custom.\nP.S. I've just checked - the API doesn't quite say it in documentation, so I'm gonna update it right now :wink:. @djMax documentation has been updated to mention whenever property duration is available :wink:\n. Release 5.5.8 included updates for better support of property duration everywhere.\nAnd presently I am not planning any extension for centralized support of it. That's why I am closing this question for now.\n. pgp.end() is all that's needed to kill all connections within the client.. Since you are using a schema, the value for parameter table in your case must be of type TableName:\njs\nvar table = new pgp.helpers.TableName(table, schema);\nThe method can also convert an object into TableName implicitely, i.e. you can do:\njs\nvar insert = pgp.helpers.insert(data, null, {table, schema});\n. > should we have to release every connection?\nNo. It is all done automatically. There are some very unique cases when method connect is used, to be released when no longer needed, but again, you would barely ever need it.\n\nor are they released automatically after every query?\n\nThey are indeed, released automatically.\nAnd when you need to execute more than one query at a time against the same connection, you use either tasks (method task) or transactions (method tx).\n\nwhat happens if the pool is exhausted ? Is an error sent back ? Or the query is queued until a connexion is available ? Is it possible to set a timeout for that?\n\nIt will be waiting till a connection becomes available, no error is thrown. The whole pg-promise architecture is however built around the idea of efficient connection use, so it would never happen, if you design your database layer correctly.\n\nwhat are the differences with pg on this?\n\nI think the above answers just about cover it :wink: But beside the connections management, the differences are endless, I wouldn't know where to start :) For one thing, it has 10x more powerful query formatting engine, best support for external SQL files, really powerful query generators for complex cases, like multi-row inserts and multi-row updates. etc... too many to list here, you should just read the docs ;)\nHave a look at the Wiki pages, and of course at the pg-promise-demo :wink:\n. That's because you are passing it in as an array. How do you think the formatter would know you want it formatted as JSON instead? :smile:\nYou need to tell the query-formatting engine of the library that your parameter is JSON.\nFor that change $/val/ to $/val:json/\n. Duplicate.\nSearch for existing questions, please: https://github.com/vitaly-t/pg-promise/issues?utf8=%E2%9C%93&q=named%20parameter%20server\nQuote:\n\nIt cannot, or rather shouldn't be done, because Named Parameters in this library support advanced syntax of type modifiers that cannot be translated into simple variables. Translating just a subset of Named Parameters into regular variables would only create more confusion. \n. >whats the correct way to do this?\n\nvia helpers.update.\nsee also: skip update columns with pg-promise\n. Interesting, I've never come across those objects.\n. Ok, the new release 5.6.3 already has the fix :wink:. Looks like an issue in the errors declaration.\n@akdor1154 you are welcome to offer a fix, if you have one :wink:. I have merged it, will release an update shortly.. This has been fixed and released in v5.8.2.\n@akdor1154  Thank you for the PR, and apologies it too me this long to finally apply the fix.\n. You test it the same as you would any promise-based interface, nothing special there.\nSome useful links:\n\nhttp://stackoverflow.com/questions/37368261/how-should-i-do-asynchronous-unit-testing\nhttps://coderwall.com/p/axugwa/cleaning-the-database-in-between-mocha-tests-with-pg-promise\n. It is doable, but in my experience it is too limiting.\n\nBest is to use something like Travis CI where you get all PostgreSQL versions for granted. This is how this library tests itself also.\n. Thank you for the PR, I will review and test it just as I get a chance ;). > There're some useful settings that aren't currently configurable from pg-promise\nWhat do you mean not configurable? You can easily do:\njs\npgp.pg.defaults.myOption = value;\nAnd all you change is to be able to do var pgpOptions: {defaults: {myOption: value}} instead of just pgp.pg.defaults.myOption = value.\nI'm not sure I see the value in this, more like the other was round - trying to mix the driver's protocol with this library's, which creates more dependency, dual protocol, plus still requires changes for the TypeScripts.\nBut I appreciate the effort :wink:\n@dmfay I've just realized you are the owner of massive.js \ud83d\ude03. @dmfay something important to know: node-postgres fixed @5.1. In my mind, the driver went a bit sideways after 5.1, which was the last solid version.\nI can offer many useful insights on the real situation with the driver, in case you need it ;)\n. Yeah, It is one of the worst problems, because if you invest a lot of time creating a product, and only after unleashing full load on it you find out it bombs every so often on broken connections. Worst - because at that point it is too late jumping to a different version/framework. The connection dies, and there is nothing you can do to fix it, which really sucks.\nI developed this framework because I needed to use it, and I still use it in a number of projects, to know how important it is to count on reliable database connections. That's why I'm not upgrading from 5.1\n. You do not open connections, the library does it automatically.\nWhen executing more than one query within a request-response cycle you should use tasks (or transactions when needed).\nSee:\n\nTasks\nChaining Queries\n. The library automatically releases connections back into the pool, those close after a non-use timeout.\n\nYou will only use pgp.end() when implementing a run-through module / program that once finished needs to exit the process immediately. Other than that, you never use it.\nSee examples\n. A correct query example:\njs\ndb.any('SELECT * FROM tt ORDER BY $1~ $2^ LIMIT $3 OFFSET $4', ['colName', 'DESC', 10, 100]);\nYou need to understand the formatting differences, as just using :raw everywhere isn't a good approach ;)\n. Method query doesn't parse the query for sub-queries, neither does anything else in this library for that matter.\nSo, naturally, yes, of course everything in the query string executes against the same connection.\nIf however you want to reuse a connection between individual queries, you should use tasks (see Chaining Queries).\n. Your query formatting is wrong. You are trying to inject a fully-escaped string into another pre-formatted string.\nYou need to use one of the following three solutions, whichever you like better:\njs\ndb.any(\"SELECT * FROM dictionary WHERE word ~ $1\", '^(' + req.body.word + ').*')\njs\ndb.any(\"SELECT * FROM dictionary WHERE word ~ '^($1#).*'\", req.body.word)\njs\ndb.any(\"SELECT * FROM dictionary WHERE word ~ '^($1:value).*'\", req.body.word)\nSee Open Values.. @allenyin55 check what you are getting in the other ones and publish here :wink:\nFor that use this approach:\n```js\nvar format = pgp.as.format; // the formatting method as used by all the queries ;)\nvar s1 = format(\"SELECT * FROM dictionary WHERE word ~ '^($1#).*'\", req.body.word);\nconsole.log('s1:', s1);\nvar s2 = format(\"SELECT * FROM dictionary WHERE word ~ '^($1:value).*'\", req.body.word);\nconsole.log('s2:', s2);\n```\nThen we can see what is wrong there ;)\n. @blendsdk you should follow this pattern: http://stackoverflow.com/questions/34382796/where-should-i-initialize-pg-promise\nand initialize your database object only once per connection, and then share it.\n. When you run the pg directly, are you still using version 5.1 or a different version?. In that case it is hardly possible. This library passes the connection string directly into the pg without changes.  See https://github.com/vitaly-t/pg-promise/blob/master/lib/connect.js#L11\nSomething else must be missing that you didn't disclose.\nI can't think of single reason to even start looking into it. And it is certainly not reproducible.\nYou will have to diagnose the issue on your side a little better to see what is really going on.\n. I would start by adding:\njs\nconsole.log('CONNECTION:', ctx.cn);\nright above this line: https://github.com/vitaly-t/pg-promise/blob/master/lib/connect.js#L11\n...to see that the library executes precisely in the same way as you do separately with the pg module.\n. Oh for cry-sake...\njs\nconst config = { \n  host: 'my.database.hostname.com',\n  port: 5432,\n  database: 'mydb',\n  user: 'myuser',\n  pass: 'myuserpass',\n  poolSize: 10 \n}\nThe config expects password, not pass. Picks up correctly from the connection string, obviously, and ignores when connecting to the localhost, if it is not needed.\n. helpers were built entirely on the query formatting engine that's inseparable from the library. They also do benefit from the global options passed into the library itself, such as capSQL.\n. > a reference to helpers to the connection object\nI don't understand what that means. What connection object?. > const query =\n      require('pg-promise')().helpers\nYou should never do such coding. Variable pgp must be created only once and then exported around. See: http://stackoverflow.com/questions/34382796/where-should-i-initialize-pg-promise\n\ndb.helpers\n\nThis is not a good idea, as it pollutes an already extensible protocol - event extend, which you can use to add anything you want into the protocol.\nI think you should read more of the documentation, to understand better how this library works and what one can do with it :wink:\n. It is already there, see Database.$config.\nWhenever you need access to the helpers namespace where only db object is available, you can do:\njs\nvar helpers = db.$config.pgp.helpers;\nAs for the protocol extensibility, it has always been fully extensible, via event extend.\nSee also pg-promise-demo to understand better how a good database project should be organized.\n. I have updated the answer here: Where should I initialize pg-promise :wink:\n. Why would you expect any side effects? Lots of people are using it in the same way, without any problems.\nP.S. There's method .tx, only, there is no method .transaction.. All 3 approaches work as native with this library for methods task and tx:\n\nregular promise chains\nES6 generators\nES7 await clause\n\nSee examples: https://github.com/vitaly-t/pg-promise/blob/master/examples/select-insert.md\n. I suppose, why not, but note that all examples I provided do not require any additional setup, while ES7 await implies there is a full Babel setup in the background.\nI myself haven't done a project with Babel yet, but I get an impression there is quite a bit to it: http://stackoverflow.com/questions/33624104/how-do-i-setup-babel-6-with-node-js-to-use-es6-in-my-server-side-code\nThen again this can be just referenced to, so go ahead with the PR :wink:. Yeah, I just did a local ES7 await test here, using Node.js 7.7.3\n$ node --harmony-async-await test.js\nand it worked fine:\n```js\nvar pgp = require('pg-promise')();\nvar config = {\n    database: 'pg_promise_test'\n};\nvar db = pgp(config);\ndb.task(async (t) => {\n    return await t.any('select * from users');\n})\n    .then(data => {\n        console.log('DATA:', data);\n        pgp.end();\n    })\n    .catch(error => {\n        console.log(error);\n    });\n```. My first test threw compilation errors, so I thought that was the reason, but now without the parameter it works, not sure what happened earlier :)\nI haven't gotten into the ES7 stuff yet. Most of the time I develop Angular.js projects using ES5 \ud83d\ude22 \n. @rafaelkallis By the way, it is worth noting that ES7 await offers no advantage over ES6 generators, at least as far as this library is concerned, because the syntax is almost identical, you just use yield instead of await, as per the examples.. Note, you should never use Promise.all on queries ;) See: https://github.com/vitaly-t/pg-promise/wiki/Common-Mistakes#tasks-versus-rootdirect-queries\n. One query, if it is first one, that's it's fine, but it is hardly ever the case in practice ;)\nBest is to be consistent, and use t.batch instead ;)\nMethod batch offers a consistent way to settle all the promises, and is way more flexible than Promise.all.. Actually, I thought that inside the task was sufficient, as the rest is beside the point?\njs\nfunction getInsertUserId(name) {\n    return db.task(async (t) => {\n        let userId = await t.oneOrNone('SELECT id FROM Users WHERE name = $1', name, u => u && u.id);\n        return userId || await t.one('INSERT INTO Users(name) VALUES($1) RETURNING id', name, u => u.id);\n    });\n}. It is not critical, let's leave it as is ;) After all, we are not changing functionality, this is just some example :)\n. This library doesn't generate any of these. I don't know where you got them from. It is not in anyway related to this library.. @jmeas a little googling took me to this page: https://www.postgresql.org/message-id/BANLkTimQ_mqPq0iFc-Xqzubzz81AtvGrLA@mail.gmail.com\n. @jmeas did you get to the bottom of it?\n. What you are suggesting would only work for a single-row insert, but not for multi-row inserts, and thus offer little value. See #239. > This seems like quite a messy and complex workaround\nWhy? You can define that logic only once, in a nice, reusable way:\n```js\nvar rawText = text => ({_rawDBType: true, formatDBType: () => text});\nvar optCol = name => ({name, init: c => c.exists ? c.value : rawText('DEFAULT')});\nand then you can declare your optional columns like this:js\nnew pgp.helpers.ColumnSet(['title', optCol('author')], { table: 'books' });\n```\nThis solution will work for both single-row inserts and multi-row inserts, and it is very extensible. And as I said earlier, it would offer little value trying to add something that will only work in one specific scenario. It is best to keep the protocol to the minimum / to what's necessary, imo.\nAnd again, what you suggested isn't flexible enough, because when a property does exist, we need full control over the value, which is what property init gives us.\n@noinkling back at ya :wink:\n. The example for optCol is just one possible variation. There are many other properties supported by type ColumnConfig, and I'd rather not bottle up those into a limited-purpose function, giving you the complete freedom of how the type is used.\nI think this issue has been discussed at length ;)\n. @noinkling after a bit more thinking I've realized that I've missed something trivial in the example that I provided earlier...\nThis logic that we do: init: c => c.exists ? c.value : rawText('DEFAULT'), it exactly replicates the logic for property def, i.e. your type optCol can be defined in a much simpler way:\njs\nvar optCol = name => ({name, def: rawText('DEFAULT')});\n. @noinkling a related question was published on StackOverflow today. It wasn't you, right? \ud83d\ude04 . F.Y.I. Important breaking change in v6.5.0.. This makes a good reusable example of an optional column that's skipped for updates and provides DEFAULT for inserts:\njs\nconst optional = name => ({\n    name: Array.isArray(name) ? name[0] : name,    // Allows use as a template string tag\n    def: {toPostgres: () => 'DEFAULT', _rawType: true},\n    skip: c => !c.exists\n});\nUsing it we can define optional columns in two ways:\n``js\noptionalname` // ES6 syntax\noptional('name') // ES5 syntax\n```\n@noinkling I published it here to be used as a reference for later :smile:. Version 7.2.0 changed it a bit.. The library with its promises executes the requests asynchronously, because IO works asynchronously in Node.js\nHowever, since the queries within one transaction execute against the same IO channel, they become synchronized (by the server) with respect to each other, i.e. the sequence of queries executed will be exactly as you defined it.\nAlso, one usually places then/catch outside of the transaction:\njs\ndb.tx(t => {\n        let deleteQuery = t.none(`DELETE FROM TEST_TABLE WHERE MY_ID = ${myId}`)\n        let cleanUpQuery = t.none(`UPDATE TEST_TABLE SET RANK = RANK-1 WHERE MY_ID = ${myId} AND RANK > ${index}`)\n        return t.batch([deleteQuery, cleanUpQuery])\n})\n    .then(() => {\n        // ...\n     })\n     .catch(error => {\n            // ...\n     });. @ferlopez94 the easiest way is to patch your values (array of objects) with the generated user_id, right after you get it. Then you can do helpers.insert to generate the complete insert query.\nNot only it is ok, but you should do it inside the transaction, i.e. only the creation of the ColumnSet should be external. The reason is to let the transaction callback handle exceptions, in case helpers.insert generates one, thus adhereing to the transaction logic.\n\nDo you recommend to use pgp.helpers.insert even when I have to insert 2 records?\n\nIt still will be twice as fast, that's for sure ;)\nUPDATE\n@ferlopez94 As an alternative, you can add column user_id into your ColumnSet object, with property def set to a function, which would return the correct user_id that was generated :wink:\n. @darrinholst This is odd, because inside a transaction you have just one IO channel, so the first query of DELETE cannot be outrun by queries that follow, as it will definitely arrive into the server first.\nAnyways, with the new ES6 Generators or ES7 async/await you no longer need to care about it, as all operations are strictly sequenced, unlike the traditional promises. So if you upgrade up, you won't have the problem for sure.\n. @darrinholst That is the most typical mistake, happens to people all the time. Here's the most recent one: Why does insert query results remain in database after transaction rollback?.\n\ud83d\ude04 \nAnd as I commented there...\n\nuse of the wrong connection context inside transactions can lead to some messy results. You must always use the right connection context provided by the transaction\n. This library does not create any extra blank spaces in SQL. You need to learn PostgreSQL  and SQL in general, to understand what type char(x) is and how it works. It is documented everywhere that the type is filled with spaces by the server, which is standard, because it is a static-length type.\n. You didn't provide enough detail about how it works otherwise.\n\nAnyway, this type of question is better suited for StackOverflow. Publish it there with all the details, and either I or somebody else will answer it.\n. There is no practical need to ever create the same database connection after shutting down the connection pool via pgp.end().\nYou only call pgp.end() just before exiting a run-through application. Otherwise, you are using it wrong.\n. Again, why do you want do call pgp.end(), if you are not exiting the test process? There's no point doing it then.\nYou should call it only when you want to exit the test.\nFor example, pg-promise in its own tests does this:\n```js\nif (jasmine.Runner) {\n    var _finishCallback = jasmine.Runner.prototype.finishCallback;\n    jasmine.Runner.prototype.finishCallback = function () {\n        // Run the old finishCallback:\n        _finishCallback.bind(this)();\n    pgp.end(); // closing pg database application pool;\n};\n\n}\n```\nso the pool is shut down after the test has finished.\nAnd if in your tests, for some reasons, you want to keep re-creating the same database object, then you can suppress all warnings:\njs\nvar pgInitializationOptions = {\n    noWarnings: true\n};\nJust don't do it in the actual application :wink:\n. @Michal-dia I've never tried, need to look into that...\nMaybe it is possible to get the pool instance from the connection object...\n. @Michal-dia I had a quick look at it, and see that pgp.pg.pools.all gives you access to the object in which every property is a Pool object from the generic-pool module (v.2.4.2)\nIn that object there is property _count that I believe what you were looking for.\nSo you can do:\njs\nvar pools = pgp.pg.pools.all;\nvar firstPoolName = Object.keys(pools)[0];\nconsole.log('First Pool Size:', pools[firstPoolName]._count);\nAnyway, as you can access the original pool object (generic-pool), you should have no problem figuring out how to diagnose it :wink:. Just log the resulting query into the console, and then you can see where you make a formatting error.\njs\nvar s = pgp.as.format(query, values);\nconsole.log(s);\nAlso see pg-monitor.\nThis part looks definitely wrong:\njs\n\\'${website}\\'\nyou are passing in a string, which generates containing quotes already, so you will end up with too many quotes. You should try and avoid using any quotes in the query string at all.\n. Yep, you got it right! :wink:. Please check the API documentation before opening an issue. Method connect has always been there, which does exactly that.\n\nUsing ph-promise#task for this is not an option since task initiates a transaction (CONNECT->COMMIT/ROLLBACK).\n\n@gajus update...\nA task (method task) does not initiate any transaction. Only method tx does that.\n. I'm open to PR-s that would improve the documentation :wink:\nThe Database type is the one that encapsulates all the available operations, which is further extended while inside tasks or transactions :wink:\n. By using the right method + RETURNING * you can monitor how many records are being deleted, as you would receive back the rows that just have been deleted:\n\ndb.one('DELETE FROM table_name WHERE id = $1 RETURNING *', 123) - will reject, if it deletes anything other than one record.\ndb.many('DELETE FROM table_name WHERE id = $1 RETURNING *', 123) - will reject, if it deletes less than 1 record.\n\nand so on.\nAlternatively, use the result method, which resolves with verbose Result object, which includes, among other things, the number of rows affected (deleted in your case) - property rowCount:\ndb.result('DELETE FROM table_name WHERE id = $1', 123, a => a.rowCount) - will resolve with the number of rows deleted.\nThe last approach is best-performing, as it doesn't request any rows back, only the counter.\n. Your question is too broad. And there is nothing lambda-specific in this library.\n\nI observed that connections stay open for few minutes in DB even though I use pgp.end()\n\npgp.end cannot terminate connections that haven't been released yet, hence the delay.\n\nI get warning \"creating a duplicate database object for the same connection\".\n\nSee: http://stackoverflow.com/questions/34382796/where-should-i-initialize-pg-promise\n\nUse the existing list of issues + StackOverflow for more guidance :wink:\n. From your log it says clearly: Node: 1.2.0.\nWhat are you getting when you run command node --version?. I've closely looked into this, and there is no issue with the library. You are trying to run it against an ancient version of Node.js, which is evident not only from your log saying Node: 1.2.0, but also all the other version details - all point at the internals of a very old Node.js version.\nBy the way, I also have Mac OS X 10.x here, and I've never seen any problem like that. You need to fix your Node.js installation.\n. Dude, if you install Node.js 7.8.0, and output process.versions, you will get the following precisely:\njs\n{\n  http_parser: '2.7.0',\n  node: '7.8.0',\n  v8: '5.5.372.43',\n  uv: '1.11.0',\n  zlib: '1.2.11',\n  ares: '1.10.1-DEV',\n  modules: '51',\n  openssl: '1.0.2k',\n  icu: '58.2',\n  unicode: '9.0',\n  cldr: '30.0.3',\n  tz: '2016j'\n}\nNow compare it with what you have. Either your installation is broken, or you've got a ghost Node.js installed somewhere in addition, which is the one that tries to run the library.\nEither way, this library determines the Node.js version correctly, there is no two ways about it.\n. > Hmm, does it affect if it's runned by the karma testing framework?\nIt does, if you set it up to run as client-side, which implies the use of the old ES5, not real Node.js environment, which would explain what you are getting. If you want to test this library, you should run it as a server-side module only.\n\nFunny thing is it was working... until i hit npm update, had to revert to previous version of this package.\n\nOlder version of pg-promise? It was version 5.6.0 that dumped the old Node.js versions.\n. That must be the cause, yes. I believe this is just a matter of correct configuration, to specify the right Node.js version.\n. Use either event query to log queries globally, or pgp.as.format(query, values), to check locally what you're getting on the output.. There is no problem using it this way, the code looks correct. And as for property ColumnSet.names being for internal use, it used to be that way, then later on became exposed. Perhaps I should remove that internal use note, to avoid confusion.\nType ColumnSet represents most value for multi-row inserts and updates. When you plan only on single-row inserts and updates, its value isn't that great, unless you have lots of columns, and/or need to implement additional logic for manipulating values. For sure it is always cleaner to use it.\nWill your code work though for multi-row operations? I'm not sure it will, as the sets would need to be prefixed with something like excluded.. Actually, I did consider adding such a feature, to provide a prefix automatically, just haven't done it yet.\nAnyway, you are on the right track! :wink:\n. I've never heard of joi validator before. But I suspect, what you do with it, you can do via ColumnSet natively, as it uses very flexible column configuration. See the Column syntax for details, plus ColumnSet code example ;). Methods like .insert and .update are expected to generate the final SQL, to be executed directly, not another formatting template. You are doing the same mistake as this guy.\nIf your formatting parameter is as simple as an integer Id to be appended, you can do:\njs\nvar update = pgp.helpers.update({content: 'earn $8.00 now'}, null, 'table') + ' WHERE id=' + id;\ndb.one(update)\nAnd if your condition does require proper escaping, then you should format it separately, and then append the result:\njs\nvar update = pgp.helpers.update({content: 'earn $8.00 now'}, null, 'table');\nvar where = pgp.as.format(' WHERE id = $1 AND name = $2', [123, 'John']);\ndb.one(update + where)\n. P.S. Doing single-row updates like this is of little value. It would offer you a cleaner approach keeping the SQL externally, via Query Files. See also: pg-promise-demo.. Check out node-postgres, as it is the underlying driver. If it is not there, then it is not supported.\nThis library does not deal with the authentication details, it simply forwards them into the driver.\n. Best is to rely on EXPLAIN.\nAlternatively, you can throw each query in question into its own task, and use pg-monitor to see how long each task takes. Don't forget to tag your tasks also, see Tags.\nIn addition, many methods in the library add property .duration to the result. See the API ;)\nP.S. Why are you using things like fs.readFileSync('query1.sql');? The native Query Files support is so much better.\n. I hope this answers it ;)\n. That means your query fails, and you are not providing any .catch handler (regular catch in case of ES7 await/async).\nSee also: http://stackoverflow.com/questions/40500490/what-is-unhandled-promise-rejection\nBasically, this is a generic promise-misuse issue, not an issue with this library ;). > I used try catch and still doesn't \"work\", I traced the exception and it is unhandled because you have a line on your code that is preventing of throwing.\nYou provided none of the details to that end. And I gave you the right answer for the details that you did provide.\n\nI think you should be more polite when talking to people who uses your library. Or maybe don't answer if you don't want to help\n\nReally? I fail to see how. But If you feel that way, then seek help elsewhere.\nP.S. Your code looks wrong also, instead of return db.func( there should be return await db.func(.. @pshemk the code you showed cannot produce that type of problem. I have tried, but cannot reproduce, the rejection is always handled by .catch() correctly.\njs\nconst test = async () => {\n    try {\n        const res = (await db.oneOrNone('select * from users where id > $1', [1])).id;\n    } catch (error) {\n        console.log('CAUGHT:', error); // always gets here\n    }\n};\nWhat is your complete environment? - \n\nNode.js version?\npg-promise version?\nOS?\n. @pshemk ok, if you can get the steps for this, get back to me. Otherwise, I cannot reproduce it from the example given earlier.\n. @pshemk Try proper error stack tracing, so that way you at least can post it here, if you ever reproduce the issue:\n\n```js\nconst promise = require('bluebird');\npromise.config({\n    longStackTraces: true // enable Long-Stack Traces\n});\nconst pgp = require('pg-promise')({\n    promiseLib: promise\n});\n```. Not presently.\nAnd nobody ever asked about it before. I'm not sure about its value though, considering that you are passing the connection into the database object yourself anyway.\nDo you have a special use-case that would need it?\n. >  if you make reusable library wich accepts db as a parameter and needs information about connection itself\nValid point, marking it as a feature request.\n. No worries, I will implement it soon. Will require tests + documentation updates, so I better do it myself ;)\n. On second thought thought, it cannot be something like db.$config.connection, because $config represents the state of the initialized library, and there can be multiple db objects, all using the same $config object.\nCurrently, I'm thinking something like db.$cn, perhaps...\n. Implemented in v5.6.8. See the release notes for details.\n. In version 6.x of the library, Database also exposes property $pool, which represents the connection pool used by the object.\n. @jmeas everything is explained here.\nAnd pg.connect is the only connection method that exists in v5.1.\n\nSwitching to the pg-pool API would have additional benefits, too\n\nActually, none, unless you are using 2 or more database servers concurrently, and with heavy load.\nIf you use multiple databases from the same server, you will get no benefit, as all queries will be going through the same IO, for which one pool is absolutely sufficient.\n\nIn my view, the driver went sideways after version 5.1:\n\nReplacing the true-and-tried shared pool with untested pg-pool that had and still has problems.\nThrowing in a very limited, half-baked promise support, that brought a new set of bugs\nBroken connection string support\n\nThat's why the upgrade still remains questionable.. @jmeas \nThere is branch 6.x available now, for anyone who is interested :wink:\n. You need to use :csv (comma-separated values) modifier for that:\nsql\nSELECT * FROM tags WHERE title LIKE $1 AND id NOT IN ($2:csv) ORDER BY title ASC\nSee also: How to execute WHERE col IN.. The best / professional approach of working with SQL is via external files / Query Files, as also shown in pg-promise-demo. In that context the syntax of ${} appears to be the friendliest, and the most logical.\nIt is considered the default syntax, from which you can deviate as you wish, depending on the situation.\nWith that in view, I see no reason for changing the documentation to an alternative syntax.\nAnd the alternative syntax is documented in every section related to the Named Parameters:\n\nIn WiKi\nOn the main page\n\nSo I don't see any issue with the documentation.\n. @darknoon How to include external files into Webpack - that's a trivial task, if you research it, lots of people are doing it. I won't even go there, as it is way outside of this library topic.\nIt seems this discussion has been concluded, so I'm closing the issue.\n. You have many problems in your code:\n\nDo not use db.connect() like that, it is not what the method is there for, see the API\nNot only you use method db.connect(), but you also do even use the connection, and later do not release it, two errors in one\nWhen executing queries in that manner, you are supposed to use a transaction, see Chaining Queries\nFor the problem you asked about, read this: https://github.com/vitaly-t/pg-promise/issues/318\n\nIn all, do not do something like this VALUES ($1, $2, \"$3.00\"), place your \"$3.00\" into a formatting value.. > thou shalt never put values directly in a query\" really the stance that pg-promise is taking\nIf you follow the link that I gave you,...you have a formatting template + values, that's what you use to format the final query. You cannot take the resulting query, and then re-use it as a formatting template yet again. It is not about what this library does, it is the common sense - a formatting template cannot contain values already.\n. This library does not do any authentication, it passes all authentication details into the driver, which in turn manages it. You should address this question to the driver, and probably best is to ask it on StackOverflow, if it is not there yet.\n. ```js\nfunction setupFreshConnection(client) {\n    var sets = [\n        'SET bla-1',\n        'SET bla-2',\n        'SET bla-3'\n    ];\n    client.query(sets.join(';'));\n}\nvar pgOptions = {\n    connect: (client, dc, isFresh) => {\n        if (isFresh) {\n            setupFreshConnection(client);\n        }\n    }\n};\nvar pgp = require('pg-promise')(pgOptions);\n```\nAvoid executing multiple queries at that point, for performance sake, that's why we join them into just one query there.\n. > would it be possible for queries to be executed before setupFreshConnection runs\nExecuted against what? The connection just has been allocated there.\n\nThough I am a bit concerned about surpressing errors part.\n\nWhat we do there is basically a hack, it was never meant to be a feature, the event is provided primarily for logging. That's why you cannot throw any error there. But then again, you should not execute there any queries, apart from simple SET-s, those should not error out.\nP.S. You can, of course, provide your own try/catch in there, just in case ;). @elmigranto  I trust the question has been answered then ;). This approach guarantees execution of queries before anything else for fresh connections :wink:\n. @danihodovic   You can't use timeout in that case, that's what breaks your sequence.\n\nThe use case I need this for is initializing custom type parsers before performing any queries in my application\n\nSo what's stopping you? Just initialize them in the beginning, before doing any queries. I don't understand why you are first executing queries, and only then set up type parsers.. > Since the function is asynchronous\nQueries execute in the same order in which they get into the IO. The one in onConnect will always be the first one.\n@elmigranto you should never use any Promise.all like that ;)\n. It depends on the execution pattern, really. If you want to be sure your queries in onConnect are executed first always, you should do something like this:\njs\nconnect: (client, dc, isFresh) => {\n   if(isFresh) {\n       client.query('SET 1...;SET 2...;ETC.'); // do fail-proof concatenated queries only!!!! to guarantee their all preceed everything else.\n   }\n}. 1) You can NOT use ES6 generators within connect\n2) You should call setTypeParser in the beginning, not during queries.. Version 8.4.0 affects the functionality described here.. What is the final SQL string that you are expecting? I'm asking, because this syntax (${obj~}) seems unusual for an update. Also, you cannot do ${obj.values}, it is not a supported syntax, and the library does not do variable evaluation.. > update tablename set (\"name\",\"picture\") = ('$1','$2') where id = myid returning *\n\nupdate tablename set (\"firstname\",\"lastname\",\"picture\",\"email\",\"phone\") = ('$1','$2','$3','$4','$5') where id = myid returning *\n\nThose two represent your final query? I don't get it. Are you trying to insert variable names into the database?\nAgain, what is your final query? Give an example. From what I've seen so far, nothing makes sense.. Maybe you are having some misunderstanding similar to this: #318 . The simplest is to throw your myid into the same object:\njs\nobj.id  = 123;\nThen you can write your query:\njs\ndb.one('UPDATE tablename SET firstname = ${firstname}, lastname = ${lastname}, picture = ${picture}, email = ${email}, phone = ${phone} WHERE id = ${id} RETURNING *', obj);\nAlternatively, you can have the method resolve with the number of rows updated:\njs\ndb.result('UPDATE tablename SET firstname = ${firstname}, lastname = ${lastname}, picture = ${picture}, email = ${email}, phone = ${phone} WHERE id = ${id}', obj, r => +r.rowCount);\nthen you don't need to return updated rows at all ;). I never heard of such a thing before. You will have to escape such special cases manually.\nBut if you have a special column within ColumnSet, you can define such column as:\n```js\n// reusable escape function:\nvar escapeBackSlash = a => a.replace(/\\/g, '\\\\');\n// define special columns like this:\n{name: 'columnName', init: c => escapeBackSlash(c.value)}\n```\n. Related: https://stackoverflow.com/questions/44389063/postgresql-returning-fails-with-regexp-replace\n@AriaFallah you can see from my answer there what it is all about ;)\n. When you create one db per database connection, you are not going to get the warning. So what is really the question/issue then that you are having?\nThe dc purpose is documented within the constructor: http://vitaly-t.github.io/pg-promise/Database.html\n. I don't know if a wrapper is necessary.\nSee:\n\npg-promise-demo\nwhere to initialize\n\nP.S. I'm shocked you would want to use pgFormatting: true, that's basically switching off all the nice query formatting provided by this library, which is way more advanced than the base driver does.\n@aberenyi This isn't gonna work:\njs\nobj.close = () => obj.$config.pgp.end()\nbecause object $config is only available from the root db object. It is generally a bad idea, trying to introduce a method on all levels that would kill off the library's pool.. This explains it: https://stackoverflow.com/questions/39168501/pg-promise-returns-integers-as-strings\n. Your UPDATE has invalid SQL syntax.. What error are you getting?. @jdalrymple Very good, I thought it was something like that :smile:. This forum is primarily for reporting issues.\nHave you tried to execute those queries? Was there any issue executing them?\n. As per the API, you are only supposed to use a transaction to enforce consistency when changing the data.\nVACUMM doesn't change data in your database, plus the operation is irreversible, i.e. you cannot apply ROLLBACK to it in case of failure.\nSimply switch from the tx method to task, and that is all.. In pg-promise, isolation level can only be set for a transaction, see Configurable Transactions. I don't think this will help you though with this situation. Simply follow what I wrote just above, as that is all that's needed.\n. See this: https://stackoverflow.com/questions/41764182/node-with-async-await-how-to-get-specific-line-where-error-happened\n. > Are there plans to actually handle this with native Promises\nNative promises, aka ES6 Promises are part of Node.js since v0.11, not this library.\n\nso I don't have to pull in another Promise lib just to get tack traces out of pg-promise\n\nBluebird isn't just another promise library. It is The promise library that should be used today, also the only one that does stack tracing so well.\nThe standard ES6 Promise is too basic anyway, don't expect much from it.\n. You understand this almost correctly, except the part where you're seemingly blaming this library for not supporting or documenting it.\nStack tracing for promises is an area that's supposed to be addressed by the promise library, that's the way it's always been. I guess it takes a little practice to figure out those kind of basics. I can't really cover all of them in my documentation.\nAs far as this library is concerned, it supports all existing promise libraries, which is well-documented. And that's the main thing.\n. There may be other promise libraries beside Bluebird today that support long stack traces, I don't know, I haven't looked into it in a while. But I wouldn't suggest any alternatives anyway.\nAnd I do accept PR-s that improve documentation :wink:. > Happy to PR, so long as you can drop the condescending comments\nI'll do my best \ud83d\ude04 . @euforic you are 7 hours too late, this was the PR.. @tonylukasavage to make you feel better, following this issue I brought this up within the Massive.js that just migrated to pg-promise: New driver considerations.\nSo the word is spreading, even if it costs you some hair :smile:\n. @psypersky \n\nI didn't liked just to extend the stack using an external lib\n\nThat is the only practical way today. And what is there not to like? It works perfectly.\n\ndo you see any potencial problem in doing this?\n\nA few:\n\nYou are limiting yourself to just ES7, while this library support extends to ES5.\nYou cannot build complex logic based on many query methods, it will be too verbose/unreadable\nIt cannot work within a transaction context\n\nBeside this, you also should use the library's query formatting, and not ES6 template strings, i.e. SELECT ${memberFields} should rely on use of filter :name instead.\n. Well, they all generate poor stack traces (except Bluebird), not just ES6 Promise :). I've just updated the section: https://github.com/vitaly-t/pg-promise#promiselib\nthe change:\n\nBluebird - best alternative all around, which includes the very important Long Stack Traces;\n\nPerhaps should suffice? ;)\n. > It's up to you whether you care how that affects users.\nI did reply to all your questions, and I did an update in documentation, which means that I do care. Beyond that, I'm now too focused on version 6.x of the library. Legacy documentation isn't a priority for me right now.\n. It is difficult to see any issue from the sheer amount of code that you have provided.\nIt would have been much easier to diagnose simply by logging all the queries, because it looks like at some point the wrong query is generated, one where there is a conflict in data types.\nUse pg-monitor to see all the queries being executed, then you will see which query is causing the error. And come back with that ;)\n. It should be very simple. See this example: https://github.com/vitaly-t/pg-promise/blob/master/examples/monitor.js\n. Both the API and the example show you to attach pg-monitor to the pg-promise options object that you use to initialize it. And you instead create a new separate object and attach to it, which will of course do nothing.\n```ts\nimport * as Bluebird from 'bluebird';\nvar options = {\n    promiseLib: Bluebird\n};\nconst PostgresPromise = require('pg-promise')(options);\nconst PGMonitor       = require('pg-monitor');\nPGMonitor.attach(options);\n``. @sustained have you made any progress?. @sustained did you getpg-monitor` to work for you?\n. This is a lot of assumptions, and as it happens, they are not correct :smile:\nAt the center of this library is the Database object, which is what meant for integration with other libraries. And specifically for that purpose it exposes property $config that gives you access to everything in the library, including its initialized root instance - $config.pgp.\nSee also the integration in wiki ;)\n. What you are suggesting isn't possible, the library needs to initialize itself before it can start creating database objects.\nSome of the most important global initialization parameters are promiseLib and pgNative that affect how everything else works after initialization.\nThe library was implemented this way for a good reason, considering the logic of the existing driver, its connection pool, promises and other parameters.\nWhat you call awkward, isn't really. The awkward piece here is the underlying driver, and this library goes a mile to hide all that, while doing it in the way that would be usable and fully automatic.\n\nI currently have to initialize it at some point in time. Is it correct?\n\nSee this: https://stackoverflow.com/questions/34382796/where-should-i-initialize-pg-promise\nAnd this gives a good approach to building proper database applications - pg-promise-demo.\n. > Why wouldn't a similar approach be possible for this library? With defaults using the system Promise and pgNative being false?\nThis library was designed and implemented when such thing as ES6 Promise didn't exist, and required a promise library to be explicitly specified always.\nSome other of its aspects also refer to the legacy design aspects. It was only recently that the library ceased support of Node.js 0.10 and 0.12, for example.\nI did consider changing the initialization approach, but it would always result in a major breaking change, and I never liked the idea for a library with such a huge user base. May be some day though. There are more important things presently, like what is to become of the 6.x branch.\nThe way it is, nothing really gets in the way of using it properly. You should use the pg-promise-demo as a reference point.\n. > but would you be interested by a PR bringing this feature?\nSorry, but presently I cannot accept a PR with such breaking changes. It would have to be a new major version, with some major changes in functionality, documentation and tests, for which I do not have the time now.\nI will wait for matters with the 6.x branch to finalize before looking at those things again.\nAnd thank you for all other considerations you've made here, I'll surely use them as a reference for later.\n\ud83d\udc4d \n. I take it this subjected has exhausted itself, so I'm closing it for now.\nB.t.w., if you want the best form this library, I suggest that you use pg-monitor from the beginning :wink:\n. Cheers!. ES7 async/await supports promises automatically, out of the box.\nLots of developers have been using it via Babel for a long time. And since Node.js 7.6 the async/await is available by default.\nWith the current Node 8.1.2 not only it all works perfectly, but 8.x shows outstanding performance improvements over the previous versions.\n\nIf not, I will try by myself and publish an article about this topic I can share.\n\nIt wouldn't be of value just in the context of pg-promise. It's a general trait of using promises.\n\nAre there some examples online?\n\nhttps://hackernoon.com/6-reasons-why-javascripts-async-await-blows-promises-away-tutorial-c7ec10518dd9. On the final note, the real value of the new async/await syntax is when executing more than one query at a time.\nWhen executing just one query, you typically use the root db object, with one .then and .catch, which isn't any worse or better than using a try / catch syntax that you have otherwise.\nBut when executing more than one query at a time, you are supposed to use tasks (see Chaining Queries), which automatically gives you access to the callback that can use the new syntax:\njs\ndb.task(async (t) => {\n    let a = await t.any('SELECT...1');\n    let b = await t.any('SELECT...2');\n    return {a, b};\n})\n   .then(data => {\n        // data = {a, b}\n   })\n    .catch(error => {\n    });\nAnd this is how the new syntax should be used :wink:\n. It is documented everywhere:\n\nMain documentation\nAPI documentation\nExamples\nquery\ntransaction\n\n\nI'd like to clarify what actually happens if we do specify it.\n\nAs per the API documentation:\n\nShuts down the entire library, as well as all the connections + pools currently allocated.\n\nYou never use it inside an HTTP service, only inside a run-through application, at the very end.\n. > when i user oneOrNone and the query returns no data in the then promise data is empty\nIt is not empty, it is null, as per the API documentation: http://vitaly-t.github.io/pg-promise/Database.html#.oneOrNone\n\nwhen i use manyOrNone or any data is an empty object\n\nIt is not an empty object, it is an empty array, again, as per the API: http://vitaly-t.github.io/pg-promise/Database.html#.manyOrNone\nSo what is your question?\n. All methods return data according to their naming semantics, and as documented in the API.\nFrom the Learn by Example:\n\nQuery method names reflect how many rows the query is expected to return: none, one, many, oneOrNone, manyOrNone = any. Do not confuse it with the number of rows affected by the query, which is a different thing.\n\nPerformance Boost:https://github.com/vitaly-t/pg-promise/wiki/Performance-Boost. > why SELECT * only returns null\nMethod any never resolves with null. Even when there is no data, it resolves with an empty array. What are you really getting? And what query are you using when specifying the columns? Give us a full example, please.\n. Let me get this right, your question is why uid gets the value without the star and not with?\nThat's because you've got overlapping column names, which creates a conflict, i.e. the driver cannot tell the difference between u.uid and s.uid.\nThe driver does not support join results through * when the tables being joined have columns with the same name. You have to specify the columns explicitly. And in order to show different columns with the same name, you must give them aliases, like u.id as userId, s.uid as sessionId.\nAlso, this is all how the underlying driver works, not relevant to this library. If you want to research this further, open the issue against the driver.\n. Closing it here, because this library doesn't change the data served by the driver, which is what should be researched against. I suggest that you either ask it against the driver or on StackOverflow, while you can use this issue as a reference :wink:\n. What user name are you passing in? Can you show the actual connection string that you are using? (You can obscure the password).. I don't know, the error comes from your azure setup. Try changing it to configuration object, i.e. from \npostgres://user:password@pgsvr-cvrmtoolbox.postgres.database.azure.com/dbname\nto\njs\nvar config = {\n  server: 'pgsvr-cvrmtoolbox.postgres.database.azure.com',\n  database: 'dbname',\n  user: 'user',\n  password: 'password'\n};\nAnyhow, the issue you are having is outside of this library's scope.\n. Any of those errors that you are getting are outside of this library.\nWhen you are using a connection string, it is parsed via pg-connection-string by the driver, which converts into into the config object. And the config object is processed by the underlying driver.\nInvalid Username specified you get from the hosting provider (azure), maybe your user name contains invalid characters in it, or maybe something else.\nEither way, it is better if you log it against the driver, if you can't find the answer, or on StackOverflow.. Cool. You can use this issue for reference, though I'm closing it here, because as stated earlier, it is out of scope for this library.\n. @retorquere Only through a manual connection, method connect, should be used with care, and only in special cases. Other than that, you should not use it.\nWhy do you need the connection object anyway?\n\nOops sorry, I see it now, it's pgp.pg\n\nThat's not the client connection, that's the whole driver. Again, what are you trying to accomplish?\n. If you have any azure-specific problems, then all the errors you get from the driver are the same errors you will get from pg-promise, so going lower isn't strictly necessary ;)\n. @retorquere as for checking connectivity via pg-promise, see: https://stackoverflow.com/questions/36120435/verify-database-connection-with-pg-promise-when-starting-an-app\n. It does, if escaped correctly. But this is a discussion for the underlying driver that processes connection details, not this library, which simply passes the details into the driver.\n. I don't understand how this change will help anything. The way that QueryFile works,...\nFirst, happens the static query formatting, based on parameter params. The type of the pattern to be used depends on the value being passed in:\n\n$1, $2, ..., if you set params to an array\n$1, if you set params to anything other than an object or an array (excluding Date, Buffer, null)\n$*propName* (* = any of {}, //, <>, [], ()), if you set params to a regular object\n\nThe static pre-formatting happens with the use of option partial, see method format, which in turn may leave other parameters there, for dynamic formatting.\nAnd when it is time to use SQL from the query file, one provides values that are once again interpreted based on the same logic as described above.\nSo, it is a two-step process, which allows the correct use of both $1, $2 variables and Named Parameters in query files at the same time, some are for static pre-formatting during loading, others when formatting queries.\nAll things considered, I'm having difficulty understanding what the suggested change accomplished exactly, though I keep re-reading it over and over again...\nMaybe if you rephrase the issue you are are having with it, or provide an example, it will help me understand it, and possibly even suggest something better.\nTypical QueryFile example that mixes Named Parameters and variables:\nsql\nSELECT * FROM ${schema~}.products WHERE price > $1 AND name = $2\nUPDATE\n\nto tell whether a single object passed to a QueryFile invocation was intended to be a parameter map or query options\n\nI recognize there may be an issue, but this change is an attempt to change the formatting logic from the declarative protocol to content-based. Will it not create more confusion for developers? There must be a way to keep it on the declarative level. I believe this to be a dangerous approach, to change how the formatting works - from how it is parameterized to what the content has.\n. Following this PR, I refactored the code related to the use of regular expressions and released v5.9.5.\nNow you can access those 3 key regular expressions like this:\n```js\nconst patterns = require('pg-promise/lib/patterns');\nconst namedParametersPattern = patterns.namedParameters;\nconst multipleValuesPattern = patterns.multipleValues;\nconst singleValuePattern = patterns.singleValue;\n```\n...and more.\nSo if in the end you still decide to follow the design you wanted, you can implement that property paramCount on your side, though instead of .sql property you should use .formatDBType() function.\nStill, I would suggest that a solution on declarative level would be a better one.\nAs for this PR, I hope you are ok with it being closed....\n. > It makes sense to have this information exposed in the QueryFile itself\nI understand what yo are trying to achieve, but I do not see a justification for adding this new property into the library. I see it as an edge case, and it won't be needed outside of your library. Maybe it will change over time, but for now this is how I see it.\nThe important thing, I have done everything this library needs to let you easily add this on your end, without the need to do it in this library:\n```js\nvar QueryFile = require('pg-promise').QueryFile;\nvar patterns = require('pg-promise/lib/patterns');\nQueryFile.prototype.getParamCount = function () {\n    const self = this;\n    const names = ['namedParameters', 'multipleValues', 'singleValue'];\n    return names.reduce((count, p) => {\n        const match = self.formatDBType().match(patterns[p]);\n        return count + (match ? match.length : 0);\n    }, 0)\n};\n```\nThat's simply translating what you did to comply with the latest v5.9.5 syntax, though I'm not sure if the result is 100% correct, you need to throw in some testing there ;)\nI think it can be easily refactored into something better :wink:\n\nI'd appreciate you holding off for a bit\n\nWhat I meant was - it's in the conflict already anyway, after the changes I made ;)\nI hope you are happy to proceed with changes on your side at this point, then we can just close it here.\n. @dmfay you are welcome, and thank you for contributing!. @dmfay a little correction, singleValue shouldn't have been used there:\n```js\nvar QueryFile = require('pg-promise').QueryFile;\nvar patterns = require('pg-promise/lib/patterns');\nQueryFile.prototype.getParamCount = function () {\n    const sql = this.formatDBType();\n    const names = ['namedParameters', 'multipleValues'];\n    return names.reduce((count, p) => {\n        const match = sql.match(patterns[p]);\n        return count + (match ? match.length : 0);\n    }, 0);\n};\n```\nNote however, if your QueryFile object doesn't use option minify: true, and your SQL contains a comment that mentions a variable, then the count will be inaccurate.\n. @dmfay my last take at it, for the most efficient and clear implementation :wink:\n```js\nvar QueryFile = require('pg-promise').QueryFile;\nvar patterns = require('pg-promise/lib/patterns');\nQueryFile.prototype.getParamCount = function () {\n    const sql = this.formatDBType();\n    const m1 = sql.match(patterns.namedParameters);\n    const m2 = sql.match(patterns.multipleValues);\n    return (m1 && m1.length || 0) + (m2 && m2.length || 0);\n};\n```\nIt is worth noting that the method returns the number of variable occurrences, not how many actual variables are used, i.e. see the example:\nsql\nSELECT * FROM table WHERE a = $1 OR b = $1 OR c = $1\nFor such an sql it will return 3, not 1.\n. Too many things have changed around the use of options between v5.3.5 and the current v5.9.5\nYou need to upgrade and re-test. If it still happens after the upgrade, we will investigate further.\n@aahoughton just so, your error Stack doesn't look right for v5.3.5 of the library. In that version there is nothing on line 1267, or any near it, only large comment blocks.\nGet back with an update please ;)\n. @aahoughton I'm closing it for now, will reopen if/when you come back with more details.. @aahoughton That is interesting. I wonder, if you would still get the same error with pg-promise 6.0.22.\nAny chance you could try? That version represents the very latest of this library and the driver at the same time.. I thought for a moment that was something to do with this library, but now I'm not sure anymore. Still, on the off-chance that you are doing something wrong with the driver, it would be no harm trying to use pg-promise 6.0.22.\n\nStill want me to try with latest pg-promise? It's pretty clear this isn't a pg-promise issue. :)\n\nAh, I was just answering that :) I don't know, this is the best I can suggest right now.. I think I have an idea of what is going on there,...\nAs per my comment here: http://vitaly-t.github.io/pg-promise/Database.html#.connect\n\nDo not use this option for regular query execution, because it exclusively occupies one physical connection, and therefore cannot scale. This option is only suitable for global connection usage, such as event listeners.\n\nAlso, another reason I put that note there is that repeated use of Client.connect does fail quickly. I think this is what you are running into.\nWhen in pg-promise you do db.connect({direct: true}) it is the same as with the driver directly: client.connet(), both are bad, and cannot scale.\nStill I think this is an old problem in the driver, and very much worth reporting afresh :wink:. > es, but this use-case is exactly where I do want to use a single connection -- I need to verify the DB is up for a regular health check, and I don't want to get stuck behind other queries in a pool\nStill, you've got to use the connection pool.\n\nI haven't looked at the latest pg code -- there may be a way to force a connection from a pool even if the pool is considered full (which would be incredibly dangerous, clearly).\n\nNo, it is not possible, and shouldn't be either.\n. > I need to verify the DB is up for a regular health check\nCheck out: https://stackoverflow.com/questions/36120435/verify-database-connection-with-pg-promise-when-starting-an-app. Closing, as version 6 has been officially released.\n. @RomeHein You should never create a table with open name like Projects. You should either create it as projects or as \"Projects\". And then query against it in the same way.\nAnyway, the error is database-level, not library-level.\n. @joux3 you are right. This was the only thing on my mind, to document that connection strings do not quite work in 6.x, I just didn't realize they do not work at all.\nI will re-test it and post an update.\n. According to this: https://node-postgres.com/features/connecting#connection-uri\nYou can do pgp({conectionString: 'postgresql://...'}) and it will work well with connection strings.\nSomehow I've missed that in my tests and documentation. Will be updating it shortly.\nP.S. I might just automatically do the conversion of a string into such an object.. Fixed in v6.1.1.\nNow connection strings work in 6.x the same as they did in 5.x:\njs\nconst db = pgp('postgres://user:password@host:port/database');\n. @juanpaco Please follow the getting started steps, you first initialize the library, then connect. In your code you skip the initialization step, hence the error.\n. The warning comes from the underlying driver, and only if you misuse it directly somehow. \nThis library itself doesn't use any depreciated methods of the driver. You are probably using something directly, bypassing it?\nCan you locate and show the code that produces the warning?\n. Seems like it is this issue: https://github.com/brianc/node-postgres/issues/1332\nBut it was fixed a week ago, so only some pre-release 6.x versions were exposed to the issue.\nWhich pg-promise 6.x version are you using?\n. The last version where those warnings were possible was 6.0.19 pre-release, fixed in 6.0.20.\nI am confident there are no such warnings in any of the release 6.x versions. If you find otherwise, please come back with details.\nIn the meantime, I am closing it.\n. There is nothing in your code that would help me reproduce the issue. I never see any warnings.\nCould you, please try to narrow it down to something more precise?\nI don't think that there is anything wrong with the Express. I've just updated pg-promise-demo to the latest version of pg-promise, and no warnings ever come up.\n. @phips28  any chance there is still a dependency issue, using the older version of the driver?\nI would suggest to delete the entire node_modules folder and then run npm install afresh.\n. Well, that's it then. That relic package must be trying to attach itself to the driver through events that the driver considers deprecated.\n. I believe the issue isn't with the driver and isn't with the relic package either.\nThe relic package must be hacking into other packages through events, and it cannot know when some package decides to make those events obsolete.\nSo it's like no-one to blame, really, just an awkward situation.\nMaybe there is a way to configure relic to avoid this for some specific package.\n. b.t.w., best for event monitoring is to use pg-monitor :wink:\n. The same issue reported against the driver: https://github.com/brianc/node-postgres/issues/1348\n. You are missing the library's initialization step. As per the Official Documentation:\njs\nconst initOptions = {/* initialization options */};\nconst pgp = require('pg-promise')(initOptions); // load and initialize the library\nconst db = pgp(connection); // create the database object. > do they use the connection obtained by the parent task\nThey do. The same connection, but different task context.\nA task within task makes sense from the tracking/logging point of view. For example, in pg-monitor you can see which task is executing queries.\nAnother reason - task re-usability, when existing smaller tasks are combined into a bigger/composite task.\nPerhaps the composite tasks make for the best example ;). Updated API documentation for methods task and tx to explicitly mention how the connection is used.\n. @OrKoN you are welcome!\nAnd changes in Release 6.2.0 are very much relevant to your question, as now you can change your task/transaction logic based on its nested level of execution.\nMore so in Release 6.2.1 ;)\nYou can now even do if(t.ctx.connected) to check directly whether the task acquired the connection on its own.\n. Could you, please clarify that? I'm not clear on what it is exactly you are suggesting and why.\nAn example might be useful also.. To avoid the naming confusion, improving the example from @elmigranto:\n```js\nconst pgp = require('pg-promise')();\nconst db = pgp('postgres://localhost:5432/database-name');\npgp.end(); // shuts down all connections pools\ndb.$pool.end(); // shuts down one specific pool (6.x feature)\n// In tests this should work:\nafterAll(db.$pool.end); // 6.x feature\n```\nAPI references:\n\npgp.end\n$pool\n\n@olalonde Does this help you or you still have concerns?\n. @olalonde I don't see the unref logic applicable to this database driver. The only thing we do or can do - to shut down the connection pool. Our only choice is to shut down all pools or just one.\nAs for your example, I don't think it will work with the connection pool at all.\nAnyhow, there is no real need for this, and sorry for saying so, but it is easy to see when something is definitely a no-go.. Sorry, I don't see it happen in a foreseeable future, as I do not recognize a real need for it.\nFor that reason, and since the original question about releasing the connection pool has been answered, I am closing the issue.\n. How do you get this type of error?\nYou can't get it simply by executing that query. Need to see more context.\nAlso, which pg-promise version are you using?\n. @anonrig not sure what you mean by that.... @anonrig it works either way.. @kmarchenko I can execute the same type of query, with all possible result/error types, but I never get the type of error you described. There must be something else going. Can you show the code that calls it?\nAlso, when you get the error, try to log the following:\njs\nconsole.log(error.code);. When I run exactly this code, I'm getting:\n[INFO] Successful database connection!\n0\nYou are going to need to debug this a little further yourself, as I cannot see how this type of error is even possible. I can't reproduce it, even with your own code.\n. @kmarchenko the only way such an error can happen, is if we try to hack the error class toString method, by doing this:\njs\nerror.toString.call({});\nthe code above makes direct string conversion on the invalid context.\nThe thing is, there is no such situation inside pg-promise anywhere, so I cannot fathom how this can possibly happen.\nI'm beginning to assume something else is wrong with your app, like maybe there is some dumb library, like newrelic, trying to hack into toString or Error implementations.\nI have no other explanation. You will need to investigate this on your end, as I have done everything I could think of at this point.\n. @kmarchenko what version of Node.js are you using?. @kmarchenko also, could you, please try 2 specific versions: 6.0.24 and 6.0.25. There was a change between those two, which I suspect may somehow contribute to the problem. If that's the case, then it will work for you with v6.0.24, but not with v6.0.25. That's just the best guess right now.\nAlso, I wonder what happens when you create such an error object yourself, directly:\njs\nconst error = new pgp.errors.QueryResultError(0, {rows: []}, 'select', 1);\nconsole.log(error);\nbecause that's what seemingly results in creation of an invalid error object, which is very strange.\n. A related question has been asked on StackOverflow: https://stackoverflow.com/questions/44850862/custom-es6-javascript-error-issue\n@kmarchenko created branch error-test specially for you to try it, and see if the issue is gone.\nReally need some feedback from you, man!\n. @kmarchenko thank you! Than means most likely if you pull it from branch error-test, it should work. Have you tried though?. Ok, cool, we are getting somewhere. I'm preparing one last test for you to see if it works, and then I can apply the fix to the master. Give me 10 mins, please ;). @kmarchenko could you, please, run your test against branch error-test-2, and tell me if it works!\nThank you!!!\nP.S. This is the last one, I promise! ;)\n. @kmarchenko Thank you!\nI honestly still do not understand how the error you are getting is even possible. But still, based on your results, I'm going to revert the master to the version that's in error-test, and re-release it shortly (within an hour).\nThank you for finding and reporting the issue, and if you somehow stumble upon the reason of why this was happening, please let me know.\n. @kmarchenko the issue has been fixed in Version 6.2.3. Please re-test, and if all is good - close the issue.\nThank you for your help!\n. @kmarchenko Is this what was happening? - \n\nYou get the error bellow if you are using bluebird somewhere in the chain and have not set pg-promise to use bluebrid\n\n@redben thank you!\n. @sachinavinaw the code that you showed wouldn't create the kind of problem. Can you show the code that produces the error?\nAlso, what version of the library are you using?\n@anonrig that's not a good example. For one thing, use of method connect is discouraged, unless needed for special cases. And if one really has to use it, must always take care of releasing the connection:\njs\ndb.connect()\n    .then(obj => {\n        obj.done(); // release the connection\n    })\n    .catch(error => {\n        console.error('ERROR:', error.message);\n    });. @sachinavinaw please get back with more details. Closing until then.\n. Methods in namespace helpers all generate queries, that's their whole purpose, they do not execute them, the main library does that.\nWhen you call [helpers.insert], it generates and returns the complete INSERT query, and you do not execute it anywhere, that's why nothing is ever inserted.\nIn addition to it, you should do the following:\n\ncreate the ColumnSet statically, outside, so it can be reused\ngenerate everything either inside a task or try/catch, so that way any query-generating error can be reported the same as the query itself.\n\nWith all being said, yous code would be something like:\n```js\nconst helpers = db.$config.pgp.helpers;\nconst cs = new helpers.ColumnSet(['STEAM_GROUP_ID_64', 'STEAM_ID_64'], {table: 'CSGO_USERS'});\nstatic saveUsers(users) {\n    return db.task(t => {\n        const query = helpers.insert(users, cs);\n        return t.none(query);\n    });\n}\n// or, if you are confident that your insert generation is always correct, then you can just do:\nstatic saveUsers(users) {\n    return db.none(helpers.insert(users, cs));\n}\n```\n[helpers.insert]:http://vitaly-t.github.io/pg-promise/helpers.html#.insert. You should research it against promises in general, not this library...\n\nhttps://stackoverflow.com/questions/42013104/placement-of-catch-before-and-after-then\n\n. Interesting, I will look into it. Do you have any specific test I can use to test it?\n\nDo I need to start closing connection pools explicitly?\n\nNo, this should happen automatically.. @dmfay the only way I can reproduce the issue is when I set poolSize > max_connections. But that's the expected behavior.\nOtherwise it always works in all versions of pg-promise. However, if you are creating multiple db objects, each gets its own pool, then it can become a problem. But it would be a test-related problem, not an actual application problem.\nIf you create some temporary db objects, you might try and force the pool destruction when done with them, by calling db.$pool.end(). Otherwise, you will be exceeding your connection quota.\n. There is nothing odd with that, version 5.x was using the driver with a single global connection pool. And now in v6.x you get one pool per database object, hence the test-specific problem.\nAs I was suggesting earlier, at the point when your db isn't used anymore, do db.$pool.end() in your tests, it should take care of the unused connection pools.. Also, you probably used option noWarnings in your tests, which hid away the warnings, which is bad. b.t.w. if you dispose of the pools then since v6.3.1 you won't even get those warnings, as per the release notes: https://github.com/vitaly-t/pg-promise/releases/tag/v.6.3.1. Explicit cast to what? JavaScript doesn't have native presentation for 64-bit integers. There is nothing that operand :csv can do there for you. You will need to convert those and/or recast on your side.\nUse $1^ for the variable, and values.map(a => pgp.as.text(a) + '::bigint').join() for the value.. I'm having difficulty reproducing it, since cnOptions is always set: https://github.com/vitaly-t/pg-promise/blob/master/lib/database.js#L218\nCould you add more details to it, please, possibly with the usage example.\n. I have reproduced the issue. It comes from the fact that only a direct-connection method would provide a non-null connection options, while top-level queries and tasks would not, hence the crash.\nI will be fixing it asap.\n. Fixed in v6.3.5.\n@calibermind Thank you for reporting!\n. @brianc Thank you for following this up!\n\nI don't recommend pipelining multiple queries in a single query text unless you have a specific edge type of case\n\nMost of users that I came across were interested in executing multiple queries just for the sake of maximizing the performance. When you have multiple queries and all can be executed at once, it makes sense to simply concatenate them. Previously developers did this type of optimization only when just one of the queries returned a result. But now that you can get all of them, I think many will want to do just that.\n. After a careful consideration...\nThe kind of changes that node-postgres v7 brings to the table are impossible to accommodate within pg-promise, not without a major breaking change of the library. This is all about supporting multi-result-sets.\nThis all is now being done within branch v7, which will be released as pg-promise v7.x, with substantial breaking changes for the first time ever.\nThe new release will also bring other long-outstanding breaking changes, to improve the overall architecture.\n. Branch v7 has been finished, and it all works well. But there will be a lot of test refactoring + documentation updates before it can be merged into the master branch.\nIn the meantime, anyone can start playing with the new version 7 of the library.\nSince documentation for v7 is being written, in the meantime you can use the list of changes within the v7 milestone, as a reference for what was changed.\n. Updated documentation for version 7 is temporarily published here.\n. Entire version 7 is likely to be scrapped, and re-done from scratch, as something simpler, because the initial approach seems to make the whole layer too complex. I will try to re-do it in such a way to keep everything simple, and hopefully without breaking changes.. Version 6.7 has been released that now uses the latest 7.x driver, while disabling the breaking change of the 7.x driver. See the Release Notes.\nNow any work toward the actual version 7.x of pg-promise will be a breaking change in the protocol to support multiple results. This will happen at a later stage, as it is no longer a priority.\nFor now when getting multiple results the library will only return the last result on the list, thus providing compatibility with all the previous versions.\n. Replacing this issue with #405, with focus on what's remaining.\n. Updated type in branch driver-7.x:\n``js\n/**\n * @enum {number}\n * @alias queryResult\n * @readonly\n * @description\n * _Query Result Mask._\n *\n * Binary mask that represents the result expected from queries.\n * It is used by generic {@link Database#query query} method, as well as method {@link Database#func func}.\n *\n * The mask is always the last optional parameter, which defaults toqueryResult.any.\n *\n * Any combination of flags is supported.\n *\n * The type is available from the library's root:pgp.queryResult`.\n \n * @see {@link Database#query Database.query}, {@link Database#func Database.func}\n /\nconst queryResult = {\n    /* Expecting a single result-set, with a single row in it. /\n    one: 1,\n/** Expecting a single result-set, with one or more rows in it. */\nmany: 2,\n\n/** Expecting a single result-set, with no rows in it. */\nnone: 4,\n\n/** Expecting multiple result-sets. */\nmulti: 8,\n\n/** No expectation as to the data returned. */\nany: 15 //= one | many | none | multi\n\n};\n```\nThis is a major breaking change, as it completely changes the logic for the query-result mask.\n. ## Masks vs Result\nAll combinations of masks as can be passed into methods query and result, versus what is expected back:\n| mask/rows             | 0            | 1            |   > 1        | multi-result       |\n|:---------------------:|:------------:|:------------:|:------------:|:------------------:|\n|   none              | null       | error      | error      | error            |\n|   one               | error      | {}         | error      | error            |\n|   many              | error      | [{}]       | [{}...]    | error            |\n|   multi             | [[{}..]..] | [[{}..]..] | [[{}..]..] | [[{}..]..]       |\n|   any               | null       | {}         | [{}...]    | [[{}..]..]       |\n| none&one          | null       | {}         | error      | error            |\n| none&many         | null       | [{}]       | [{}...]    | error            |\n| none&multi        | null       | [[{}]]     | [[{}...]]  | [[{}..]..]       |\n| none&one&many   | null       | {}         | [{}...]    | error            |\n| none&one&multi  | null       | {}         | error      | [[{}..]..]       |\n| none&many&multi | null       | [{}]       | [{}...]    | [[{}..]..]       |\n| one&many          | error      | {}         | [{}...]    | error            |\n| one&multi         | [[]]       | {}         | [[{}...]]  | [[{}..]..]       |\n| one&many&multi  | [[]]       | {}         | [{}...]    | [[{}..]..]       |\n| many&multi        | [[]]       | [{}]       | [{}...]    | [[{}..]..]       |\nNote that any = none & one & many & multi.\nMethods vs Result\nWe only have methods that use result mask combinations that make practical sense, even though you are free to use any combination via methods query and func.\n| method/rows   | 0                  | 1                  | > 1                | multi-result       |\n|:-------------:|:------------------:|:------------------:|:------------------:|:------------------:|\n| none        | null             | error            | error            | error            |\n| one         | error            | {}               | error            | error            |\n| oneOrNone   | null             | {}               | error            | error            |\n| many        | error            | [{}]             | [{}...]          | error            |\n| manyOrNone  | null             | [{}]             | [{}...]          | error            |\n| any         | null             | {}               | [{}...]          | [[{}..]..]       |\n| rows        | []               | [{}]             | [{}...]          | error            |\n| map         | []               | [{}]             | [{}...]          | error            |\n| each        | []               | [{}]             | [{}...]          | error            |\n| result      | Result{}         | Result{}         | Result{}         | error            |\n| proc        | null             | {}               | error            | error            |\n| multi       | [[{}..]..]       | [[{}..]..]       | [[{}..]..]       | [[{}..]..]       |\n| multiResult | [[Result{}..]..] | [[Result{}..]..] | [[Result{}..]..] | [[Result{}..]..] |\nThe only methods not listed here are query and func that take the result mask as a parameter, and expect the data according to the mask.\n. Implemented, pending tests + documentation.. @dmfay The reason I did it this way is to have methods that can access the original data sets either as arrays or as Result. That's how methods multi and multiResult work accordingly.\nIt is of a compromise. I was thinking - how to get all the basics available and not to over-design this thing. We already have so many methods for the mask, even though it is all logical, I'd love keeping it simple.\nAnyhow, any suggestions are welcome! :wink:\n. I agree, the previous method any was way more useful. But if I were to keep it that way, it would be a problem, because method any is expected to return anything, but it couldn't for multi-result sets.\nAlso method manyOrNone becomes less useful, for sure.\nI just want a set of methods that are logical, consistent with their naming and the logical mask.\nThis is somewhat challenging. May be I should reduce the methods to a bare minimum... somehow.\n@dmfay I am open to suggestions :wink:\nMy biggest concern right now - the default mask for method query. It used to be any, but now it would become quite useless to use any as the default.. * multiple renamed into multiRow\n* added new multiResult\nsee below:\n```js\nconst queryResultErrorCode = {\n    /* No data returned from the query. /\n    noData: 0,\n/** No return data was expected. */\nnotEmpty: 1,\n\n/** Multiple rows were not expected. */\nmultiRow: 2,\n\n/** Multiple results were not expected. */\nmultiResult: 3\n\n};\nconst errorMessages = [\n    {name: 'noData', message: 'No data returned from the query.'},\n    {name: 'notEmpty', message: 'No return data was expected.'},\n    {name: 'multiRow', message: 'Multiple rows were not expected.'},\n    {name: 'multiResult', message: 'Multiple results were not expected.'}\n];\n``. Implemented, pending tests + documentation.. Implemented, pending tests + documentation.. Implemented.. Version [7.2.0](https://github.com/vitaly-t/pg-promise/releases/tag/v.7.2.0) change it again.. CON: questionable improvement.\n. Won't be doing it, no real benefit.. Closing, because this may not happen for some time. And the branch may need to be redone from scratch.. I don't know what type you are using in your table, but you can always re-cast the type. Use the column like this:{name: 'state_date', cast: 'date'}`, or use whatever type you need casting into (see Column).\nAlso, do not create ColumnType dynamically, they are heavy static structures, as documented everywhere, and meant to be created once.\nAnd lastly, your call return t.batch(results); is pointless, it is never even reached.\nColumn:http://vitaly-t.github.io/pg-promise/helpers.Column.html. You are welcome! :). Wouldn't it be better not to break the existing protocol, and leave a version that doesn't require any type, i.e. one that defaults to any? Then you could write it both ways.\n. That's not always a good thing. Plus you'd be breaking all the code out there that works fine. Best is to make it optional, not required, imo.\n. @rafaelkallis i've just added an update to your code, check it out. Would that work for you?. And how about dual declaration instead? - would it be any better?\n```js\ntask()...\ntask()...\n```\nI'm just thinking from the point of view of avoiding breaking any existing code...\n. Also, worth mentioning, you can use your own methods task/tx via the protocol extension ;)\nAre you extending the protocol, b.t.w.?. Native extensions are much better, as they are available from inside tasks/tx, so you can implement complex scenarios. See pg-promise-demo.\nYou cannot access your extensions correctly from a task/tx.. @rafaelkallis version 6.3.6 also received the same type of update for the entire protocol, i.e. each method supports templating now.\n. I don't see how pg-promise update from 6.3.6. to 6.3.7 could break anything.\nOnly the update of QueryStream from 1.0.0 to 1.1.0 could have done it.\nI suppose you can continue using version 1.0.0 of it, till pg-promise is updated to support the latest version?\n. I can see that QueryStream 1.1.0 is a complete breaking change from 1.0.0, should have been 2.0.0 instead.\n. I have released v6.3.8 just to avoid the dependency issue.\n@dmfay you can set it as the new dependency ;)\nI've tried to do a quick fix on the new version of pg-query-stream, but it didn't work. I will need to spend a bit more time with it.\n. There some issues there: https://github.com/brianc/node-pg-query-stream/issues/28\nSo I wouldn't rush trying to support the latest, especially considering that version 1.1 doesn't fix any issues, just refactors for ES6, while breaking some existing functionality, as evident.\n. I'm going to wait till Brian follows up on this: https://github.com/brianc/node-pg-query-stream/issues/30, before making any changes that is.. More to the fire: https://github.com/brianc/node-pg-query-stream/issues/32\n. Replacing this issue with #383.\n. You get exactly what you query for when you do SELECT (col1, col2 ....) instead of SELECT col1, col2,.... Check PostgreSQL documentation. That's how data is supposed to be returned in this case.\nSyntax SELECT (col1, col2) is for selecting each row as a tuple.\n. Release 6.8.0 did it.\n. Looks like it only created problems, arh, because of this: https://github.com/brianc/node-pg-query-stream/issues/37. Method func is there to simplify regular function calls, i.e. the ones where default type casting is sufficient, which isn't your case, so the method isn't suitable there.\nThe simplest solution is to generate the query yourself:\njs\ndb.any('SELECT * FROM \"createUser\"($1, $2::text[])', [username, tenants])\n. @MadsRC yes, I didn't notice at first the use of the camel case in the name. I have corrected my example by using double-quotes for the function name.. Release v6.7.1 changes these things.\n. Cheers! I will up the dependency in the next update ;)\n. Updated in v6.4.0.\n. > Which way is the best\nWhichever you like, the result is the same.\n. @LoiKos Note that your example is generally a bad one, because you use oneOrNone first, which can resolve with 1 row or null, as per the API, and you use it without checking if it is null.. ok, but now you show bad code style for ES6 generators.\nWhen inside generators, you should use throw instead of return Promise.reject :wink:\nI will work either way, because pg-promise is so smart, but in general it is considered a bad style ;)\n. js\nif(!product){\n    throw ApiError.notFound();\n}. This is how the driver works presently. You can try logging it against the driver or on StackOverflow.\nThere have been many issues logged against timestamp in the past, and more than one solution proposed.\nI won't be going into more details, since it is outside of the scope for this library. See the driver.\n . Note that if you are formatting it on your side, and simply passing in a string, then it is the server that applies the timezone, not the driver.. You will get a better answer on StackOverflow for this. Issues related to Date/Time conversion are outside of this library, which doesn't convert them in any way. This library uses exactly what the driver provides.\n. Do you mean how to access the driver? See How to access the instance of node-postgres that's used?. It is important to know that this behavior changed in v5.4.1.\n. Your usage pattern is correct, but the error message tells you that your provider refuses to allocate enough connections for you.\nYou need to find out how many connections are allowed, and set it within the connection config, using parameter poolSize:\njs\nconst db = pgp({\n    poolSize: 5,\n   // other connection parameters\n});\nThe default is: https://github.com/brianc/node-postgres/blob/master/lib/defaults.js#L41\n\nNote that using fewer connections than needed by your code will inevitably result in a slow-down.\n. The following query is not a valid SQL:\nsql\nSELECT * FROM table WHERE id IN ()\nwhich means you are not supposed to even try to execute it.\n\nShould pg-promise replace [] with something like select 1 from false so the user gets what they expect?\n\nNo, the formatting engine only formats the values, it doesn't generate verbose SQL statements on its own.\n\nYou should check if the array is empty, and then do not execute the query.\n. You need to add the Node.js types, like here: https://github.com/vitaly-t/pg-promise-demo/blob/master/package.json#L35, with the version according to your Node.js version.\nIt includes all the standard types, including Buffer.\nThis is generic to all TypeScript libraries for Node.js, i.e. not specific to pg-promise.. @Brizak What doesn't work?\nThe current TypeScript version of the library has been used by developers for a very long time. You are the first one to log this kind of issue, which most likely means you are doing something wrong there.\nPlease be more specific why can't you install and use the Node.js TypeScript definitions, or best yet - ask it on StackOverflow, as it is likely you will find an answer there much quicker, since you issue is most likely generic.\n. Looks like postgresMapper.Registry.TransactionRequest.insert is not really a QueryFile object.. Must be something trivial, but it takes debugging the code, which I cannot do for you.. I don't follow. How is this a problem?. Which version of the library are you using?\nIf this code - if (query instanceof $npm.QueryFile) doesn't pass, means this is not a properly created QueryFile. I can't say more, not without running the code.\nThis is the first time someone has this issue, so must be something either very special or trivial.\n. Possibly, your code if (PostgresMapperModel.queryFiles.filePath == null) doesn't pass the first time, since you do not set it to null, so your query becomes undefined, and hence the error.\n. I don't think it is relevant, but I still would suggest an upgrade to the latest v6.5.1. Did you figure out the problem?. I'd need to to be able to reproduce the issue, in order to recognize that there is a problem, and to understand the value of any change. Right now I do not understand how to reproduce the problem you are having.. I don't think that Node.js version or NPM got anything to do with your issue.\nYour best shot is to create a full reproducible example that I could run and see what the issue is.. > may be i should create a demonstrating example and share with you?\nIsn't it exactly what I asked you just prior?\n. @keerthivasan-r did u make no progress with the issue?\n. npm loads the version according to the package.json configuration, so multiple versions can be loaded.\nNewer versions of NPM do it better though.\nYou were probably creating QueryFile using one version of the library, and then executing them using a different version. That would explain the mess.\n. @keerthivasan-r I've added an answer here: https://stackoverflow.com/questions/45954954/pg-promise-query-formatting-issue-in-queryfile\n. This happens when you mismatch the name case, i.e. your product_bundleId in the database most likely does not have a capital in it, while you use capital I in the Id. The two need to match exactly for the column name to be recognized.\nAnd for that you do not need to rename anything, you can simply use prop for the ColumnSet, see Column properties.\n. Your definition is invalid: product_bundleId INT REFERENCES product_bundles(id) ON DELETE CASCADE, because it uses capital I in it, without quotes. Read about use of capitals in PostgreSQL and how the server handles them.\nYou should either use it without capital, or put into quotes for the server to be able to tell the difference.\n. To explain it better, when you create product_bundleId without quotes, it automatically creates it with all small letters. You would have to place it into quotes in order to create it in the right case.\nYou need to read about how PostgreSQL treats camel cases.\nThat's why if you replace in your code with \"product_bundleid\", i.e. without capital, it will work, because that's what you actually have.\n. @Tobbeman any feedback?. This issues keeps coming up all the time. Here's the most recent duplicate: https://stackoverflow.com/questions/46062417/how-to-declare-a-string-with-double-quotes-in-query-config-object-in-node-postgr\nYou cannot use variable column/table names inside a Parameterized Query.. That means you are not escaping it right: username is an SQL Name, while password is a string value.\nYour query must be:\njs\ndb.none('CREATE USER $1:name WITH PASSWORD $2', ['myUserName', 'password']);\n// OR\ndb.none('CREATE USER $1~ WITH PASSWORD $2', ['myUserName', 'password']);\nwhichever you like :wink:\n. Duplicate of #354 \nYou should upgrade to the latest version ;)\n. @cryptcoin-junkey can you come up with a usable reproducible example?\nThen I would be able to have a look at it.. @cryptcoin-junkey Update v6.5.2 includes the change, though I will be surprised if it makes a difference.\nCould you come back on this one, please!\n. @cryptcoin-junkey it contained the change you were looking for.\nLooks you are gonna need to come up with a usable example that reproduces the issue. Otherwise I wouldn't even know where to look at.\n. Issue  brianc/node-postgres#1334 was merged there on June 20, and it is included into v6.4.2 that's used by this library.\n. @ubreddy as per the message above, you should follow this issue: https://github.com/brianc/node-postgres/issues/1344\nFrom what I can see, it is still not resolved within the driver.\n. @ubreddy not that I know of, sorry.. @ubreddy Current dependency is set as \"pg\": \"~7.4.0\", which means it will install driver 7.4.1, if you re-install the dependencies :wink:\n  . Both versions work the same in 6.x, and only poolSize works in 5.x, no need for an update ;)\n. If you defined \\d in JavaScript, then it is not valid for SQL, as it is a special symbol, and you should use \\\\d for that. But I'm only guessing, since you didn't show the actual code + value.\nAlso, it is important to understand that when you use a ParameterizedQuery, then pg-promise doesn't do any query-value formatting, as it is passed into the server directly.\n\nMaybe a brief note about naked backslash in the docs?\n\nThere is nothing really else to say that, if you simply misused the ParameterizedQuery type, or didn't use \\\\d within JavaScript.\nTherefore, I'm closing this issue. If it is something else - please let us know, and we can re-open it ;)\n. The first message - Abnormal client.end() is spot on, it is there by design, and it is always displayed because the loss of connection is considered the most severe type of error in the library. And the other possible reason - invalid code should definitely be reported. Those two manifest themselves in the same way, unfortunately - client.end() is invoked directly, and no way of telling which one happened at that point.\nThe second one - TypeError: Cannot read property 'client' of null is due to uncertain state of the task/transaction in that case, and depends on what is happening inside those. Why that specific error happens in your case - I'm not sure, would need to debug it. But in general, as long as both reject, it is not a big issue, imo.\nI will look at the second message later on and see if I can reproduce it.\n. A new release v6.5.3 addresses the issue with the second error type, making it consistent with how query methods fail in case they are executed against a released connection, producing an error such as Error: Unexpected call outside of task/transaction.\nThis is the best the library can deduct for the situations when connection is lost while inside a task or transaction. It is impossible to detect it any better, as it is impossible to determine whether the disconnection occurred before or after running the last query within it, which makes it uncertain whether the task should be considered a success or failure.\nAt least the error that's reported is now consistent, and not something caused by the task engine internally as it was before.\nAs for the first error type, as I wrote earlier, it is there by design.\n@boromisp Thank you for reporting the issue! I hope you will find the fix satisfying.\n. Release 6.5.4 is a further improvement on this.\n@boromisp any feedback?\n. I was able to reproduce the problem, and I have found the cause, which is with the driver - node-postgres.\n. Here I have reported the issue: https://github.com/brianc/node-postgres/issues/1454\n. Reopening as an existing issue, although it seems to be only relevant to transactions, but not tasks.\n. @boromisp congrats, mate! you've raised a deado! \ud83d\udc80\ud83e\udd23  \n...a very old issue that got 3 dupes at this point.\n. After reviewing the issue, I've decided to close it, at least for now, for the following reasons:\n\nThe main issue that was reported here was fixed and re-released\nThe secondary issue here is strictly a bug in the node-postgres dependency, which was reported, and is unknown how long it is gonna take BrianC to fix, as it was reported in duplicated or directly related issues a number of times.\n\nThe bottom line is, there is nothing actionable for this module at this point. When the driver resolves the issue, it will be automatically resolved for pg-promise.\n. Re-opening the issue, to start working on a work-around, since there is still no solution to this on the driver side. Removed label low frequency, since the issue also manifests itself when dealing with a slow network.\n. I'm looking for some feedback, of whether this PR solves the issue, as a work-around: https://github.com/vitaly-t/pg-promise/pull/456\n@boromisp @Raphyyy. Release 7.5.4 should fix the secondary issue that was reported in the beginning.\n. {} is considered a legacy syntax, while array[] is the latest, and also the recommended one.\nThere are two ways to provide for an empty array of values:\n\nCheck for the array length, and not execute the query that will knowingly produce no result, see #391\nUse type casing ::text[] that will always inform the server of the type before parsing the array.\n\nI don't see switching to the old syntax being a good solution at this point, or necessary for that matter. You should use one of the solutions above, depending on the situation, your output SQL will be more consistent.\n\nimo your tests very weak at this point.\n\nHow do you figure that? What tests am I missing?\n. I'm not aware of any such issue within Massive.js. Perhaps it would have been wise to chase it from the beginning, against Massive.js, to log it there with all the details, and then we could discuss the best solution to this and on which side it should be. Having a PR here for this at the start seems a little premature ;). @btb, by the way, so you are not stuck for the moment, the flexibility of pg-promise allows you to get what you want without any change in the library's source, by using Custom Type Formatting:\n```js\nconst asArray = require('pg-promise').as.array;\nfunction wrap(a) {\n    return {\n        _rawType: true,\n        toPostgres: () => a.length ? asArray(a) : '\\'{}\\''\n    }\n}\n```\nThen instead of passing an array variable as myArray, you pass in wrap(myArray), and get what you want:\njs\nconst myArray = [1, 2, 3];\ndb.any('SELECT $1', [wrap(myArray)]);\nThe effect will be the same as what your PR does :wink:\nUPDATE\n@btd  P.S. Actually, for use from within Massive.js, you would replace this line:\njs\nconst asArray = require('pg-promise').as.array;\nwith this one:\njs\nconst asArray = db.pgp.as.array;\nsee: https://github.com/dmfay/massive-js#accessing-the-driver. @btd now you can open an issue against Massive.js, and chase it from there, without any rush ;)\nAnd the solution you used will work always, it is the power of the Custom Type Formatting \ud83d\udcaa . I don't know why I didn't realize the obvious the first time, but that wrap function can be just this:\njs\nfunction wrap(a) {\n    return a.length ? a : '{}';\n}\nThe result is the same.\n. Closing, as there is no real need for this PR anymore. See https://github.com/dmfay/massive-js/issues/468. After a careful consideration and manual tests with the server, I am now implementing this feature for pg-promise 6.7.1.\nJust note that this PR was neither correct no sufficient. It would not work for nested empty arrays, and missing some key tests for it.\n. UPDATE\nAfter a bit more testing... It will never work right for nested empty arrays, explicit type casting is required anyway.\n. After much more testing, version 6.7.1 is finally available.\n. Looks like a bug in the new pg-query-stream, it stopped supporting readable event.\n. > Thank you. Should I open an issue there?\nI already have: readable event no longer works.\n. @pdanpdan but if you want, you can open it against spex as the one that cannot handle a stream that doesn't support readable events.\nAnd then close this one, because this issue isn't really in pg-promise.\n. @pdanpdan Version 6.9.0 got it all resolved.\nFor pg-query-stream you need to use option readChunks: true (see stream.read), as it presently only supports event data, but not readable.\n. >  it's not documented anywhere\nIt is documented plenty. There are tons of duplicate issues others opened in the past, and there are issues on StackOverflow, like this one: Loose request outside of an expired connection.\nPlease see Contribution Notes.\n\nYour code is simply invalid. The error you are getting is because you create queries inside the transaction, and you fail to chain them to the transaction's callback logic:\njs\ninnerFunc(tx, count);\ninnerFunc(tx, count);\nThis creates loose promises. The correct call to chain the result must be:\njs\nawait innerFunc(tx, count);\nawait innerFunc(tx, count);\nAnd this is standard for ES7 async functionality, that's why there is no specific documentation to cover that.\nAnother issue:\njs\ndb.tx(cb);\nwhere you again create a loose promise. You must always provide at least a catch handler:\njs\ndb.tx(cb)\n    .catch(error => {\n    });\n. P.S. Note that the error itself Loose request outside an expired connection was refactored in version 6.5.4.. Version 7.0.0 is out.\n. > In this configuration does it make sense to create a pool in pg-promise\nThe connection pool is at the very core of each Database object, you cannot not create a pool.\n\nshould I disable connection pooling in pg-promise?\n\nYou can't.\n\nIf I set pool size to 1 and idle timeout to 0, will it disable connection pooling so every request creates a connection to pgBouncer and disconnects.\n\nIt is possible, but I won't speculate, as I don't know how this all will work in combination with pgBouncer. You might want to address a wider audience for this by asking on StackOverflow.\nAnd if it is at all possible, then you would be closer to the truth by investigating it against the driver directly, or its connection pool, because those are the ones used within pg-promise.\n. I have done a little more investigation into this. Here's one of the related issues: https://github.com/brianc/node-postgres/issues/975\nFrom what I've seen, you can create a pool of size 1 within this library, and use it that way, but I'm not sure how this going to affect the scalability of the app. You see, the connection pool is what provides the two key features here:\n\nscalability, through dynamically allocated physical connections, according to the maximum number of parallel requests, and capped by the maximum pool size\nautomatic restoration of lost connections\n\nIf the double-pooling compromises any of those two key functions, the end result will be bad. You cannot use an API that doesn't scale its connectivity, and you cannot rely on something that cannot auto-fix broken connections.\n\nOther than that, there are lots of issues pertaining to pgBouncer logged against the driver: https://github.com/brianc/node-postgres/issues?utf8=%E2%9C%93&q=pgBouncer\n. I trust you will continue the search against the links provided above. So I'm closing it here ;). It was discussed here: using named parameters with nested objects.\n. Yes, you missing the part where it says:\n\nThe library only formats Named Parameters, it doesn't evaluate them.\n\nYour approach requires evaluation, which in itself brings huge performance hit.\nAnd another reason - variables here support rich syntax for value transformations, like ${varName:json}, etc, so the full variable syntax is impossible, or would be problematic.\n. > As for performance, do you mean the performance of obj[v.name] vs. get(obj, v.name)? That seems like not a huge problem to me because people that are needing to use nested values are already having to resort to hacks around that (like flattening) that probably incur similar performance loss. \nNot quite. For once, it would be replacing direct JavaScript with two third-party calls - one for has, one for get, and those would have to be called for everything, thus dropping the performance a lot for the sake of 1% of code out there.\nMost developers do not need nested access. In practice, and especially when implementing large database services, it is best to have all your SQL in external files, which is when nobody really needs nested variable access anymore.\n. Anyhow, your use case is too unique to be making such a change in the library at this point. And it would affect much more in this library than you think, like the entire helpers namespace for one thing.\nI will take it under a consideration, but I don't know if I will ever get around to it, or even if it is worth it.\n. ^ or ~ were long time ego made interchangeable with :raw and :name, which surely is simple to remember ;)\nthis case is fore very unique cases, one usually doesn't need it.\nAnyhow, it is all very well documented within the helpers :wink:\nAnd there are many examples on StackOverflow also.\n. @ianstormtaylor the question did intrigue me, so I had a look at how methods has and get work in lodash, and start to believe there is a way to implement those without affecting the performance.\n. \nI did a research for performance of split versus indexOf, finding that the latter is significantly faster. So I came up with this implementation for the two methods:\njs\n// doesn't assume validity of the property path\nfunction has(obj, prop) {\n    if (prop.indexOf('.') === -1) {\n        return prop in obj;\n    }\n    const names = prop.split('.');\n    for (let i = 0; i < names.length; i++) {\n        const n = names[i];\n        if (obj && typeof obj === 'object' && n in obj) {\n            obj = obj[n];\n        } else {\n            return false;\n        }\n    }\n    return true;\n}\njs\n// assumes validity of the property path\nfunction get(obj, prop) {\n    if (prop.indexOf('.') === -1) {\n        return obj[prop];\n    }\n    const names = prop.split('.');\n    for (let i = 0; i < names.length; i++) {\n        obj = obj[names[i]];\n    }\n    return obj;\n}\nSince get would only be called after a successful has, it can assume validity of the property path, and never verify it, to improve performance.\n\nThe slow-down with this approach would only occur when you actually do use nested names, because when you don't, it only executes the additional indexOf, and that's it.\nI believe it is possible to optimize this code even further, by making method get even more dependent on method has, so it wouldn't need to do a second indexOf / split at all, and reuse their results from has.\nI'm leaving it for now, in case I do decide to add it all later on.\n. Here, I have come up with something that would offer the maximum performance:\njs\nfunction getIfHas(obj, prop) {\n    const result = {};\n    if (prop.indexOf('.') === -1) {\n        result.has = prop in obj;\n        result.value = obj[prop];\n    } else {\n        const names = prop.split('.');\n        for (let i = 0; i < names.length; i++) {\n            const n = names[i];\n            if (obj && typeof obj === 'object' && n in obj) {\n                obj = obj[n];\n            } else {\n                result.has = false;\n                return result;\n            }\n        }\n        result.has = true;\n        result.value = obj;\n    }\n    return result;\n}\nThe method will always return an object like {has: true, value: 123}, which should be the fastest, because:\n\nit executes indexOf only once, and split when needed, also only once\nit gets the value at the same time, so the get verifications are no longer needed\n. I have done a comprehensive performance test of getIfHas versus calling _.has and _.get from lodash, and I am getting a consistent 300% performance advantage with getIfHas when using nested property names, and 500% advantage when using regular (non-nested) property names.\n\nTests were done on Windows 10, under Node.js 8.5.0\n. I just did a post here for lodash: https://github.com/lodash/lodash/issues/3401, to share some thoughts on my findings.\nThe author (@jdalton) immediately marked it invalid, with comment Oh boy, closed it and locked access, so I can't do anything there anymore.\nThis is by far the most rude/obnoxious response I have seen from any public library support.\nIndeed,... Oh boy... \ud83d\ude20 \n . @jdalton well, I work on GitHub projects full time, and fast. I saw your invalid label attached, no comment, so I politely asked why, and you just locked the issue, so I can no longer follow up.\n. @jdalton Issues are mediated through edits and comments. GitHub is all about contribution and code sharing. Locking people out because you think no need to follow up in the issue is the wrong sentiment, imo, and a discouraging one at that.\nAnd the guidelines for use of locks on GitHub is to deal with conversations that stretch too far (outside of the subject) or got too many comments.\n. @ianstormtaylor I bet you didn't expect this much thrust in this issue :smile:\nAny thoughts are welcome! :wink:\n. Created brunch nested-names to prepare a PR for the feature.. Now I'm trying to figure out how to amend the RegEx to be able correctly identify valid variables: https://stackoverflow.com/questions/46442884/regex-for-javascript-variables-with-nested-names\nIt seems the RegEx update would be more complex than I initially thought.\n. The feature has been finished in brunch nested-names, pending test coverage to be added + documentation updates.\nIn the end I went for a much simpler and elegant solution that didn't require any RegEx updates, also one that performs best. The RegEx blindly accepts . in the name, but the post-match parser can detect and report any anomaly in the syntax, which is so much faster, simpler, and better - because you get an error thrown when the syntax is wrong.\n\nUPDATE: Actually, there will be a number of updates for the helpers namespace before it is really ready.. After much trying I am about to give up on the helpers namespace, as not only it is a huge overhead, but it creates logical anomalies that do not have a clear resolution.\nHere's an example of such an anomaly...\n```js\nconst cs = new pgp.helpers.ColumnSet(['one.two']);\nconst data = {\n    one: 123\n};\nconst insert = pgp.helpers.insert(data, cs);\n```\nThe problem is that property one in this example exists, but since its is not an object, name one.two cannot be defaulted to anything without overwriting the value of one. \nSo, executing either init or def logic will result in lost values because of the type contradiction. And that's a whole new ball game. I have no idea what to do with it.\nI am tempted to do the limited feature within the formatting engine only, leaving out the helpers namespace entirely.\n. @skatcat31 Yes, it is true. By allowing implicit name path resolutions, we are exposing ourselves to a situation when a property exists only partially, and that would provide poor diagnostics.\nFor example, we have property one.two.three, and only three doesn't exist, but the error from the parser will always be that property one.two.three doesn't exist.\nAlthough I could push it further, and report partial-path issues, I'm not sure it's all a good idea.\n\nAnyhow, I will be releasing the feature for the formatting engine only, but not for the helpers namespace, because of the next-level logical problems it creates, as described earlier.. @skatcat31 As I wrote in the chat earlier, I do not want to be the judge of this anymore, let developers take the responsibility.\nMy objective was to make sure that performance is not affected, which I did achieve nicely. Regular variable references are not affected. It is only if you are using nested properties, the performance will be slightly worse, but only by a tiny margin, negligible, really.\n. Released the feature as v6.10.0\n. @ianstormtaylor Not a single comment? You asked for this feature.\n. @ianstormtaylor well, good luck!\nJust so, promises aren't gonna help you manage connections or do proper transactions or high-performance query generation, etc... If you think this library just adds promises to the driver, you haven't really used it then. Not to mention the promise-related issues in the driver.\n. Fixed in v6.10.1.\n. It can be done by making the query via JSON functions. See example with json_build_object here: https://stackoverflow.com/questions/39805736/get-join-table-as-array-of-results-with-postgresql-nodejs. * If the functionality restores shortly after, that means you are misusing connections. See Chaining Queries\n* If functionality isn't restored, that means you are leaking connections somewhere.\nI cannot say much more without seeing the code.\n\nEven after 30 munites, the 20 entries are still there and the api is KO and I have to restart the node server to cleaning up connections.\n\nSounds like the 2nd cause is more likely, but it is worth looking at both ;)\n. Ok, so you are absolutely NOT following the considerations outlined in Chaining Queries. That's something you need to consider seriously.\nYour code is full of promise anti-patterns. You are creating promises + do coroutines in places where none needed, which only convolutes the code, making it much harder to read.\nAnd in some places your code is totally wrong, like in function create_user where you are executing function update_metrics against the root connection, which is completely invalid. You are inside a transaction, you can only use the transaction connection there, i.e. while inside a transaction you cannot execute queries against some random connections.\nThose are just a few problems I've spotted at first sight.\n. Why are you mixing transaction logic with database calls from another connection? It doesn't makes sense, looks bad and achieves nothing, except opens a pandora box to connectivity issues. You should never write code like this.\nThose are two completely independent things, and must be executed independently of each other.\n. > If query 1 fails don't execute query 2;\nYou do not do that in your example. You showed me the code that executes those queries independently.\n\nIf query 2 fails roolback query 1; if query 2 is ok commit query 1;\n\nCOMMIT/ROLLBACK does not work across databases. Neither do transactions.\n. That code looks bad in a number of ways...\nThere is absolutely no point in using the sequence there and there is no need for an extra promise, or to re-run the same query against more than one context, or dynamically resolving the static connection.\nHere's the same method create_user done right:\njs\nfunction create_user(physical_shard_id, user, metricdata) {\n    const sharding = get_stat_sharding(),\n        ctx = physical_shard_id !== sharding.physical_shard_id && physical_shards_connections[sharding.physical_shard_id];\n    return physical_shards_connections[physical_shard_id].tx(t => {\n        return t.func('register_mobile_user', [user])\n            .then(() => (ctx || t).func('update_metrics', [JSON.stringify(metricdata)]));\n    });\n}\nOnce you review all your code, and reduce to what is sensible and readable, then you should have no problem to spot any issue related to connectivity, if there is still one.\nThat's the best I can help you with presently.\n. I cannot reproduce the issue.\nWhat version of pg-promise are you using? To be absolutely sure, do this:\njs\nconsole.log(db.$config.version);\nAlso, please tell me your Node.js version, just in case.. Something doesn't add up. When it comes to supporting camel cases, the library has a very comprehensive set of tests. There must be something important missing in your example, is what I suspect.\n. authUserId != authUserID \ud83d\ude04 \nLate-night coding? :smile:. No worries, it happens! :smile:. This is not related to this library. See Node.js 6 and anonymous objects.. It can very easily be done within SQL files...\nsql\nSELECT ${columns:name} FROM table...\nINNER JOIN...\nin JavaScript:\n```js\nconst values = {\n    columns: ['column1']\n};\nif (/need to add columns/) {\n    values.columns.push('column2', 'column3');\n}\n```\nFilter :name, for SQL Names, has a special logic to it, and it can be:\n\na simple text string, which is normal, to represent a single column name\nstring * (asterisks), which is detected automatically\narray of strings, in which case it is considered a list of column names\nany object, to pull all property names automatically.\n\nIn our example we use the latter to provide a list of columns.\n\nHere's another example, when by default we would select all columns, and only on a condition we would select specific columns:\n```js\nconst values = {\n    columns: '*' // all columns\n};\nif (/need specific columns/) {\n    values.columns = ['column1', 'column2', 'column3'];\n}\n```\n. Following this question, I did a complete rewrite of the SQL Names documentation, which now explains everything nicely, with examples.\n. > Regarding adding addtional JOINS and other parts of the query dynamically . Would I have to use \"Raw Text\" then?\nYes, you can use Raw Text for that.\nAlternatively, if other parts are too complex, you can always have your query file use another query file, i.e. as a composite query file.\n. Do I understand it right that you want to extend the protocol according to what database it is?\nIf so, then you should use the Database Context, see the Database constructor.\nvar options = {\n    extend: (obj, dc) => {\n        // check the `dc` and apply the extension according to the context\n    }\n};\nThe dc is available everywhere in the protocol ;)\n\ndoes it 'extend' the base methods?\n\nYou add new methods that can call an existing method + do something else.\n. I just did a test against the latest pg-promise v7.0.1 + PostgreSQL 10, and it works as expected:\n```js\nconst pgp = require('pg-promise')();\nconst appName = 'my-app';\nconst db = pgp({\n    database: 'pg_promise_test',\n    port: 5432,\n    user: 'postgres',\n    application_name: appName\n});\ndb.any('SELECT application_name FROM pg_stat_activity WHERE application_name = $1', appName)\n    .then(data => {\n        console.log(data);\n        pgp.end();\n    });\n```\nOutputs:\n[ anonymous { application_name: 'my-app' } ]\n. You want the interactive user from the request? That's quite unrelated to all of this. If you have the user from the request, you log right there, so what's the problem?\nAnd your question in this case is completely misleading. It got nothing to do with event extend or the application_name. It's just basic logging logic that you implement yourself.\n. Also, what do you think your method myMethodWithParam should do? You cannot amend query like that.\nYou can either execute two queries, like log insert + the actual query, or you can use a template for the query, like from the external SQL file that would expect the extra parameters, so it can amend it.\n. I cannot advise you on how to template it into a single query, as it depends on the database architecture, and what it is exactly you are expecting.\nAt best, I can show you how you can do it via 2 queries and event extend:\njs\nvar options = {\n    extend: obj => {\n        obj.logContext = context => {\n            return obj.none('INSERT INTO logs(context) VALUES($1)', context);\n        };\n        obj.queryWithContext = (query, values, context) => {\n            return obj.logContext(context).then(() => {\n                return obj.query(query, values);\n            });\n        }\n    }\n};. @jmealo you want to execute SET application_name on every HTTP request? That doesn't sound right, because due to parallel HTTP on top of parallel query execution you are likely to keep overriding it, thus ending up with invalid current application.\nThe only way it won't override the setting is by using a transaction. But using a transaction for every HTTP request is a horrible idea, because transactions are blocking operations.\nSo I don't think you'd want to do it :wink:. Anyhow, as a pure exercise, you can play with it. One approach is to extend the protocol with your custom task method that will execute SET application_name = ... in the beginning of every task:\njs\nconst initOptions = {\n    extend: obj => {\n        obj.appTask = (appName, cb) => {\n            return obj.task('app-task', t => {\n                cb = cb.bind(t, t);\n                return t.none('SET application_name = $1', appName).then(cb);\n            });\n        }\n    }\n};\nThen you can call:\njs\ndb.appTask('app-name', t => {\n    // do the task....\n}).then().catch();. @jmealo The example I showed above is a task, not a transaction.\nAnd of course you can alternatively prepend a set to each query either manually or by extending the protocol.\nHere, for example, we introduce a new special appQuery method that extends the existing query method by prepending each query with SET application_name = :\njs\nconst initOptions = {\n    extend: obj => {\n        obj.appQuery = (appName, query, values, qrm) => {\n            const sql = pgp.helpers.concat([\n                {query: 'SET application_name = $1', values: appName},\n                {query, values}\n            ]);\n            return obj.query(sql, [], qrm);\n        }\n    }\n};\nIt would be well-performing, as the query is concatenated and executed only once.\nSee also method helper.concat ;)\n. Another thought, if the number of apps is very limited, you can throw each app-specific stuff into its own schema, and then use separate database objects ;)\n. I'm a bit divided about this. Those errors were never designed to be thrown by the client. Only pg-promise knows internally when and how exactly to raise those errors.. > I am testing that if a pg-promise function raises a QueryResultError, it is properly processed by the rest of the application.\nThe right way to do it is by executing a query that will throw one:\nexample 1\njs\ndb.none('SELECT 1')\n// will always throw QueryResultError = {code: queryResultErrorCode.notEmpty, ...}\nexample 2\njs\ndb.one('select a from (values(1),(2)) s(a)')\n// will always throw QueryResultError = {code: queryResultErrorCode.multiple, ...}\n. Then you are doing tests that aren't real, but synthetic, something that's recommended to avoid when possible. And since for any database-related tests you do need to execute queries via a live connection, synthetic tests don't really make much sense.\nProper tests are quite easy to write. Here's an example: https://coderwall.com/p/axugwa/cleaning-the-database-in-between-mocha-tests-with-pg-promise\n. Also think about this - even this library itself tests all those errors only by executing queries, and you want to avoid that in your database service? I don't think it's a good idea.\n. > Also for the sake of consistency & accuracy - the error types are exported, looking as if their constructor is a normal error constructor, but it is not.\nThey are exported only to be able to write code like this:\njs\nif (error instanceof errors.QueryFileError) {\n}\n...as an example.\nAnd yes, the error constructors are completely proprietary, hence my reluctance in declaring those in TypeScript, only to give the wrong idea of trying to instantiate those.\n. What you are doing seems quite unique, and doesn't follow the good practice for testing database-related code, which should be done by executing queries. If you really want to make an exception for your case, you can always either clone the repo, or just use a modified version of the TypeScript locally within your special test.\nPerhaps the best yet is to declare your own types derived from those errors, with the constructors :). This is an interesting question :smile:\nThe best way is to extend the protocol with your own transaction method (let's call it myTx for simplicity):\n```js\nconst pgpLib = require('pg-promise');\nconst TransactionMode = pgpLib.txMode.TransactionMode;\nconst isolationLevel = pgpLib.txMode.isolationLevel;\nconst txMode = new TransactionMode({\n    tiLevel: isolationLevel.serializable\n});\nconst pgp = pgpLib({\n    extend: obj => {\n        obj.myTx = (tag, cb) => {\n            cb = cb === undefined ? tag : cb;\n            cb.txMode = txMode; // assigning custom transaction mode\n            return obj.tx(tag, cb);\n        }\n    },\n    capSQL: true // if you want all generated SQL capitalized\n});\n```\nMethod myTx fully replicates the tx input parameters, while making all transactions open with:\nBEGIN ISOLATION LEVEL SERIALIZABLE\nSee also:\n event extend\n TransactionMode\n* txMode namespace\n. extend is actually an event, not a method ;)\nAnd you are welcome!. The approach needs to be modified for version 8.0.0 and later, as the transaction mode can only be set as option mode on the method.\n```js\nconst pgpLib = require('pg-promise');\nconst TransactionMode = pgpLib.txMode.TransactionMode;\nconst isolationLevel = pgpLib.txMode.isolationLevel;\nconst mode = new TransactionMode({\n    tiLevel: isolationLevel.serializable\n});\nconst pgp = pgpLib({\n    extend: obj => {\n        obj.myTx = (options, cb) => {\n            options = (typeof options === 'object' && options) || {};\n            options.mode = mode;\n            cb = cb === undefined ? options : cb;\n            return obj.tx(options, cb);\n        }\n    },\n    capSQL: true // if you want all generated SQL capitalized\n});\n```\nHowever, in general, I would advise to just specify the transaction mode where needed, as the code is very simple and readable for that:\njs\ndb.tx({mode}, t => {});\nIn fact, the example I showed isn't great, as it cannot handle all use cases for the exsiting method, because the way parameters options and cb and processed is a bit more complicated, and you would have to replicate all that in your custom method.\n. Opened issue #470, specifically to improve this type of use case.\n. Version 8.1.0 added function utils.taskArgs, to simplify extension of tasks and transactions.\nExample:\n```js\n// Registering a custom transaction method that assigns a default Transaction Mode:\nconst initOptions = {\n    extend: obj => {\n        obj.myTx = function(options, cb) {\n            const args = pgp.utils.taskArgs(arguments); // prepare arguments\n        if (!args.options.mode) {\n            // set default transaction mode, if none was specified:\n            args.options.mode = myTxModeObject; // of type pgp.txMode.TransactionMode\n        }\n\n        return obj.tx.apply(this, args);\n        // or explicitly, if needed:\n        // return obj.tx.call(this, args.options, args.cb);\n    }\n}\n\n};\n```\nWithout utils.taskArgs, it was almost impossible to extend those methods correctly with version 8.0.0\n. Cheers! :smile:. About the issue.\nI will be able to test it in about 10 hours from now.\n\nAbout the protocol.\nIt was originally designed to control results of a single query, up until v7 of the driver that suddenly started supporting multiple results. The library was updated in v6 to process only the last query result, which is consistent with the driver, pgAdmin, and the previous versions of pg-promise.\nAnd in v7.0.0 of the library I added new methods multi and multiResult to execute multi-query statements and get all the results at once. Their logic does not include any result control, they just return what is there. And I'm not planning to change that, to avoid ambiguities in such restrictions.\nIf you want a method that deals with mult-query results while controlling it somehow, just use event extend to extend the protocol with your own method.\nHere's an example of adding custom method multiCheck:\njs\nconst initOptions = {\n  extend: obj => {\n    obj.multiCheck = (query, values) => {\n      return obj.multi(query, values).then(data => {\n        if (/*I don't like something in the data*/) {\n          throw new Error('I don\\'t like the data!');\n        }\n        return data;\n      });\n    }\n  }\n};\n\nand are using a modified version of this library to apply result checking to arbitrary results instead of only checking the last result.\n\nThe library's protocol is fully extensible by design, no point in modifying the library itself.\n. I can confirm that the issue is definitely there, and needs to be fixed.\n. Fixed in v7.2.1.. > Is there any way to use the func() method with named parameters?\nNo. The method takes an array of dynamic parameters, which normally doesn't go along with named parameters that are all known in advance.\nBut you can just execute the function yourself, since func doesn't do anything magic there:\njs\ndb.any('SELECT * FROM my_func_name(${param1}, ${param2})', {param1, param2})\n. You are not supposed to call it directly, ever! That's why it is not there.\nCalling db.done() will internally call it correctly when it is the right time.\nAnd if you ever call it in a JavaScript app, it will throw this error:\nAbnormal client.end() call, due to invalid code or failed server connection.. @pyramation what do you mean the client is not disconnecting?\nBest is to provide an example of what you are doing ;). @pyramation You cannot call client.end() against connections managed by the pool, it is against the API of the underlying driver node-postgres, and against the overall concept of automatic connection management within this library.\nThere are two types of connections - default/automatic (via the pool), that are released automatically by the connection pool, and independent connections - when you call connect, passing it {direct: true}. For the latter, calling .done() will result in immediate physical disconnection. See the API :wink:\n. @pyramation did you get this working as per my earlier comments?. QueryFile formats the query with the variables as you provide those. So what stops you from applying those in the way that you want? What is the problem?\n\nIs there a way to load a common query by QueryFile and then append to it conditionally?\n\nThat's pretty much the general idea around using the type. Perhaps it would be helpful if you described something you tried that didn't work for you? Then I could advise better.\n. Or are you talking about a single generic query file that needs to be pre-formatted differently upon loading? If the difference inside the SQL, without many variations, then perhaps best is to use separate SQL files. Otherwise, you can provide details during the query formatting.\n. You can never do things like this: sqlByQueryFile.query. In fact, in the latest version of the library it won't even let you.\nYou must add a variable in your generic query and then inject the variable part, through, ahem a variable, obviously.\n. Your generic SQL query:\n```slq\n-- some generic SQL here\n-- dynamic SQL:\n${condition:raw}\n-- more generic SQL\n```\nAnd then you inject the raw part with the condition from JavaScript.\n```js\nconst condition = /isNullCase/ ? 'AND col1 IS NULL' : pgp.as.format('AND col1 = $1', targetCol1Value);\ndb.any(sqlByQueryFile, {condition, etc...});\n```\n. First of all, see Contributing Notes. And you should at least format the code in your question.\nExamples of such transactions can be found everywhere, if you searched for those...\njs\ndb.tx(async t => {\n    // await statements\n}).then().catch();\nSee also Chaining Queries. Syntax for using tasks and transactions is identical.\n. This library does named interpolation under the Named Parameters, which is far more advanced, as it supports both Formatting Filters and Nested Named Parameters.\n\nuse psql style variables in files parsed by QueryImport\n\nDid you mean QueryFile?\n. The ${name:filter} syntax is more verbose, as it supports filters, so it cannot map back to :name syntax. And for the opposite direction, you'd have to do the transformation yourself, i'm afraid.. @bsouthga You are welcome!\nJust worth saying that once you start using the full syntax of the Named Parameters, with use of Formatting Filters, Nested Named Parameters and Custom Type Formatting you will see that whatever psql can support is just way too basic and of little help in projects that do need the flexibility :wink:\n. > After I call this function pgp.end(), how can I check connection is dead or alive?\nAfter you call pgp.end(), all connections are dead, there is nothing to check. Although technically, you can check property db.$pool.ending that would be true, but i generally would not recommend, because it may change in the future.\nFor one thing, you should not use pgp.end(), it is just bad coding. I should update both library and documentation for that. That call kills all of the connection pools in the library.\nIf you want to shut down a connection pool on the database object that you allocated, use db.$pool.end() instead. But even then, it is normally not needed, except in some special cases, like to exit a process without delay.\nSo, you need to understand the implications of that call.\n\nWhat's different after pgp.end(). And how could I test it ?\n\nAfter you kill the pool, all database methods against that pool will fail with error: Connection pool of the database object has been destroyed.. You are welcome! Though I have no idea what your _client represents, you didn't show that.\nAnd db.$pool.end() cannot be true, only db.$pool.ending flag can be, after the pool shuts down.\n. Library de-initialization has been fully updated (documentation).\n. Your variable syntax is incorrect, it is not $foo, it is ${foo}, see Named Parameters, so the full formatting variable in your case is ${foo:csv}.\nPlease next time format your code examples properly ;)\n. ${foo}:csv is also incorrect, see Formatting Filters.. For your ColumnSet, you can define the column like this:\njs\n{name: 'product_id', mod: ':raw', init: () => 'DEFAULT'}\nSee Column.\n. You are passing in data:\njs\ndata = req.body.systemTypes.map(type => ({\n        type: type, \n        submissionid: 123123123\n    }));\nwhich doesn't have any systemtype_id, hence the error.\n. Duplicate of #396 . When you are getting Querying against a released or lost connection., it means you are executing a query after the task/transaction has finished.\nHere's a more detailed example of working with streams: Data Imports.\nAlso, you do not need to import spex externally, this library exposes it as pgp.spex.\n\nOther than that, I can do only very limited support while away on holidays till Dec 18th.\n. You misunderstood Prepared Statements that are executed on the server side, while :csv is a client-side filter, can only be used within the default query-value formatting.\nAll the rich query-formatting syntax implemented by this library isn't available from within Prepared Statements and Parameterized Queries, as those are formatted on the server-side by definition, so they limit you to the basic $1, $2,... syntax that the PostgreSQL server supports.\n. The same error? That's hardly possible, unless you set flag pgFormatting during the library's initialization. Did you? Otherwise, that query cannot produce the  same error.. Why are you using triple `````? That's what's causing it.\nSee:\njs\nconsole.log(``````);\nthe same output.\nAnd this got nothing to do with this library.. P.S. I have just asked a related question on StackOverflow: Nested template string error in Node.js\n. According to your own error log, it is pointing at line 293, which isn't the code you are showing here.\nFirst, make sure you are looking at the right code that's causing it :wink:\nAnd the code you are showing doesn't seem to have any problem.\n. It may or may not be one of those stupid cases where Bluebird wants you to return the promise even when it is handled locally, i.e. replacing db.tx() with return db.tx(). You can try it ;)\nI mean, it generally doesn't complain like that, but there are special cases when it fails to identify that the promise is already handled. This could be one of those cases.\n. > Is it possible to to initialize pg-promise with a decorated version of pg?\nNo idea, needs to be looked into. As of now, there is no provision for that. Could you detail, please on what tracing information you are looking for, and what else that decoration offers.\n\nAllowing users to provider a version of pg probably opens up a raft of compatibility issues.\n\nIt definitely would, the node-postgres driver had plenty of breaking changes from one version to the next, which were accommodated through changes within this library.\n. I have just read the documentation and examples of using aws-xray-sdk-postgres, and from what I could see, everything you can do with that module is already available within pg-promise, simply start using pg-monitor, and you will get the best logging for everything.\nAlternatively, there are events, such as query, etc, which pg-monitor simply handles in its own way, but you can use those yourself, directly, if you want.\n. @timReynolds why not just use pg-monitor? Seems like the simplest option to me, and this is what it was written for, to trace queries and all events supported by the library.\n. I take it the subject has been exhausted? I'm closing it then..... Duplicate of #354 + #396 \nYou should upgrade to the latest version, for starters. And if that doesn't help - see through the related issues.. Read Data Imports, and follow the example. Inserting 5 million rows should only take around 1 minute, unless you are doing it wrong.\nThe work-around that you are suggesting is way more complicated, and it wouldn't even be discussed here, as it is completely outside the database patterns supported by this library.\nAnd you cannot share a real transaction context across processes, at least not in Node.js\n. js\ndb.query('SELECT * FROM books WHERE title IN ($<titles:csv>)', { titles });\nSee CSV Filter.\n. Problems with authentication is best to investigate on StackOverflow, as they are generic, always outside of this library, and that's why we do not address them here.. > This is the error forwarded from the underlying pg library, but perhaps pg-promise could provide a better error for this scenario?\nIt can't, because filter :csv is generic, and it doesn't know in which context you are using it.\nYou should instead check titles.length, and when it is 0, do not execute such invalid query at all, i.e. having IN () is invalid in queries, and that's what you are trying to execute, hence the error.\n. > If there are cases in which :csv is valid without any elements then my suggestion will not work.\nThere are plenty, as the filter is generic, it is used in many places, not just for IN () statements.\n. Related issue on StackOverflow: https://stackoverflow.com/questions/48344888/best-practice-for-creating-sql-select-queries-while-handling-potential-undefined\n. More on this question: WHERE col IN Query with empty array as parameter\n. When you pass [] into your query method, you tell it that your query uses syntax $1, $2,... to format the query. The method enumerates all formatting variables, and finds one that points outside the range of parameters. So, the error reported is accurate.\n\nI would expect the helper classes to properly escape reserved characters.\n\nExcept, you are not using any :) You are generating a query without any ColumnSet, based just on the object itself.\n\nWhat is the proper way to use the helpers/work around this issue?\n\nThe following solutions depend on other requirements about the source object and/or the project...\nThe simplest one, is to remove [] when you call the query method, to avoid the parameter formatting, i.e. just call:\njs\ndb.one(query)\nThe more generic one is to define a reusable ColumnSet, and use it when generating the query.\nEither way, you should avoid formatting queries more than once, or it will always create problems. In your case this is exactly what you are doing. Your query already contains values, which means it has been formatted already, and then you are trying to format it again by passing in [], that will inevitably create problems :wink:\nThis mistake is commonly known as Double-Formatting Issue :wink:, when the first formatting introduces values into the query which may contain text that looks like formatting variables, and then the second formatting naturally trips over that. Formatting a query must always be done in one step, unless you make use of partial formatting, but that's a separate topic.\n. @ethanresnick  This happens only when you mistakenly double-format queries, so the error would be on your side, and not in the library.\n\nDouble-formatting - formatting a query that already has been formatted and contains values.\n. This query template is invalid - \nINSERT into some_table (\"name\", \"amount\") VALUES ($1, '$10')\n\nThere can be 3 types of valid queries:\n\nWith formatting variables (not formatted yet)\nWith values in it (after formatting)\nMixed query that has variables, plus non-conflicting values.\n\nYour query doesn't fit into any of those. It contains both variables and variable-conflicting values. You are not supposed to have such a query to begin with.\nYou should do something like this instead:\njs\nformat('INSERT into some_table (\"name\", \"amount\") VALUES ($1, $2)',  ['My Name', '$10'])\n. > The promise that's returned by none() doesn't appear to be waiting for the transaction to be committed\nOnly if you fail to chain it to the transaction. Other than that, all methods are the same underneath ;)\nThe issue is most likely with the way you do the transaction, so I need to see the whole transaction code.\n. @daveisfera closing, due to the lack of the example. If you come back with an example, I will re-open the issue.. Interesting, that you use variables $2, $3, $5, but not $1, $4, that normally wouldn't work at all...\nFrom the short piece of code that you showed, the transaction code looks ok. Please show it with the error handler + what error you are getting. I'd like to see the whole code, not just an abbreviated version of it, because, as they say, the devil is in the detail :)\n. > Sorry, that's a mistake on my part. I stripped down our actual code a bit to make that example.\nAre you going to show the complete version, as per my previous post? I can't help any further without it.\n. > Your cb function is probably going to be asynchronous in 99%\nIt doesn't matter what cb does, the result is always treated as asyncronous.\nOn the overall, I agree, it should be:\nts\ntask<T=any>(cb: (t: ITask<Ext> & Ext) => XPromise<T>): XPromise<T>\nIt's what the library does anyway, just the TypeScript is inaccurate in this case.\nThe same for transactions.\n\nI will address this in the next PR :wink:\n. Fixed, and will be available in the next release.\n. @sgronblo Release v7.4.0 now has the update :wink:\n. > However it seems this interface doesn't seem to accept the connection configuration as a connection string?\nIt accepts both connection string and a connection config object. What is connection configuration as a connection string - there is just a connection string. \ud83d\ude15 \n\nHow is the poolSize used to specify max and min when using pg-promise?\n\nIn the exact same way as for the pg-pool - min, max, poolSize. What seems to be the issue?\n. > I was just wondering if it was possible to use both a connection string AND a config at the same time.\nNo, it is not. Either one or the other.\n. You can either set the pool size to 1, which will prevent creation of more than one connection, or you can make use of the global connection. However, for the latter you would have to monitor yourself, in case the connection is lost.. > the pool will keep track of whether the connection has been disconnected and re-connect it if necessary?\nYes, as its one of the most important functions. With the manual connection you would have to do it yourself, which is, by the way, quite possible ;)\n. @sgronblo In version 8.0.0 it is now possible to use both connection string and the config:\njs\nconst connection = {\n   connectionString: 'postgres://user...',\n  ...\n  all other properties\n};\ni.e. the same way that connectionString was intended in the driver:\n\n// a Postgres connection string to be used instead of setting individual connection items\n  // NOTE:  Setting this value will cause it to override any other value (such as database or user) defined\n  // in the defaults object.\n. * Returning a rejected promise or throwing an error from the transaction callback will result in ROLLBACK.\n* Returning nothing / a value / a resolved promise from the transaction callback will result in COMMIT.\n\nGiven that you are using promises, it is implied everywhere. I suppose this could be emphasized in docs.\n. This commit adds this onto the main page also :wink:. Couldn't figure out a proper fix for this, so posted a question on StackOverflow: Matching exclusive search groups in a string.\nSo nice having the community helping out like this! \ud83d\ude04 Now it is fixed properly.\n. Released with version 7.4.0.. Released with version 7.4.0.. Released with version 7.4.0.\n. This is one of the most duplicated questions. See Multi-row insert with pg-promise\n. It has always been supported. Have you even tried?\nJust pass statement_timeout within your connection parameters. \nSee also: Connection Syntax, it is listed there.\n. > It wasn't listed in the official docs\nYou are pointing at the library's initialization options, not database connection parameters :wink:\n. Version 8.5.3 started also supporting query_timeout.\n. Implemented in version 7.4.1, see new property inTransaction in TaskContext.\n. Can you do this using the base driver node-postgres?\nIf yes, then please show an example. Then I could figure out how to do it within this module.\nAnd if you cannot do this in the base driver, then you won't be able to do it here either.\n. Closing due to non-response. Will re-open, if there is any.\n. Implemented and merged in branch v8.. Implemented in branch v8.. Patching Result object with duration was removed when new method multiResult was introduced, which would create ambiguity, as the array would contain Result objects without any duration.\nThis library now provides duration only within its own API, such as tasks + transactions, i.e. you can handle events task and transact and log duration from those.\n. * Event receive is called for all data reads, including the data coming from a stream, in which case there can be no duration applicable.\n* Event receive is presently not called for queries that return no data, which makes it useless for logging any duration.\ni.e. I would have to change the way receive works completely, and it would be a breaking change.. I had similar requests before, but to change the receive event, and I'd rather be inclined to do just that, for version 8.x, as a breaking change.. I have opened a new issue for this - #461, and closing this one. If you have more comments, please add them to the new issue :wink:. The feature is now available in Version 8.0.0.. See Multi-row insert with pg-promise.. You can replace Object.keys(obj[0]) with just obj[0], as that's the default.\nAnd ColumnSet should be created once, not re-created all over again.. Implemented within branch v8.. Implemented and merged into branch v8.. Implemented in version 8.1.0.. I have done the same test, and cannot reproduce it, the code always works no matter where you place %, in front or in the end, which is what expected ;)\nPlus, the error that you see comes from the PostgreSQL server, and not this library.\nI would suggest taking the generated SQL and executing it directly against pgAdmin, then you should be able to see the problem.\njs\nconst sql = pgp.as.format('SELECT table_name FROM information_schema.tables WHERE table_name LIKE $1', ['%string']);\nconsole.log('SQL:', sql);\n. Then it should work through pg-promise also.\n. Since no other details are provided, I cannot help you. And besides, as I wrote earlier, your query error comes from the server, which is a bit outside of this library.. This is due to the namespace being read-only.\nI'm going to revise the use of read-only namespaces, and probably remove them all, to avoid issues with tests like that.\n. @nazarhussain The fix is now in master, and will become available with the next release.\nFor that particular case, option noLocking was supposed to work, but it didn't. It will now ;)\nSee Initialization Options.\n. Now available in version 8.1.1.\n. I think it was copied from another context where + was used for converting a string that always contained an integer into type number (SELECT count(*)). In that context + was indeed just a shorter version of parseInt. But in general, you are correct.. I have committed the change to use parseInt instead. Thank you for the PR!\n. Shouldn't we just remove this one? - \njs\n(cn: string | TConfig, dc?: any): IDatabase<IEmptyExt>\nI'm not sure why I needed it to begin with. Maybe something worked differently in older TypeScript.\n. @alex-sherwin Thank you for the PR!. pg-promise doesn't control the number of connections, the underlying driver node-postgres does it via its connection pool parameters, which you simply pass through with pg-promise.\nNeither library can control the number of connections across processes.. See related issue: #406\n. I take it the question has been answered? :wink:. > So logically t1 and t2 should refer to same transaction\nNo. Every task and transaction create their own connection context object with the protocol.\nAnd the default logic for method [txIf] is to create a new task when already in transaction.\nYour list of arguments isn't really valid...\n\nWriting unit tests related to transaction\n\nNothing stops you from doing so.\n\nExtending active transaction to be used later used in repo method\n\nIt is by design that you cannot use task/tx context outside of the callback. Also, creating the new context for each task/tx is what makes it possible to do independent extension of the protocol and/or implementation logic.\n\nSpying/Stubbing transactions\n\nYou still can do so, just not for nested context.\n[txIf]:http://vitaly-t.github.io/pg-promise/Database.html#txIf. > This test failed, as we spy t2 and none was called on t1 whereas both don't refer to same transaction/task object.\nAnd where is the t1 in your example? :smile:\nThis code doesn't have it. And from the look of it, the example should work - \n```js\ndb.tx('my-out-side-tx', function *(t2) {\nsinonSandbox.spy(t2, 'none'); \nyield t2.DBRepo.action();\nexpect(t2.none).to.be.calledOnce; \n});\n``\nbecause this is no other task/tx context created there.. Following this discussion, plus outside in Slack, opened new issue #480 as an improvement.\n. The syntax is IN ($(ids:csv)), not IN ($(ids):csv)`. See the documentation:\n\nFilters use the same syntax for Index Variables and Named Parameters, following immediately the variable name...\n\nExamples are provided there below ;)\n. This one still needs a bit of review before merging, and likely extra tests.. This one still needs documentation updates.. You can define your ColumnSet like this:\njs\nconst cs = new pgp.helpers.ColumnSet([\n    'user',\n    'group',\n    {name: 'points', init: c => c.value + 10;}\n], {\n  table: 'users'\n});\nSee flexible type Column for details.\n. Oh dear lord, that was such a stupid mistake that crawled into v8.2.0! I am fixing it immediately!\nThank you for reporting it!. Fixed in v8.2.1 \ud83d\udd28 . This is all because of the #488 mistake.. Duplicate of #488 . Implemented.. db.stream provides a callback function in which you are expected to initiate streaming, as shown in the example.\nMethod read within spex is to read data from a generic stream. Why you passed read into db.stream and expected it somehow to work - I don't know :smile: Those are two separate things.\nThe middle ground between them is that you can pass your qs into method read, and have it read the data, although I don't remember if I ever tested it, as those are parts of 2 separate libraries.\n. This is by design, see the API: http://vitaly-t.github.io/pg-promise/helpers.Column.html\n\nDestination column name + source property name (if prop is skipped). The name must adhere to JavaScript syntax for variables, unless prop is specified, in which case name represents only the column name, and therefore can be any string.\n. Are you using string presentation for your source property names? If not, then what is your source property name?. I understand that, I asked about the name of the property in the source object that contains the value.. > I'm using string presentation for property names.\n\nIf property names in your source object do not satisfy the open-variable syntax for JavaScript, then type Column cannot directly map to such properties.\nThis is because the whole idea of the helpers namespace is around preparing high-performance formatting templates that use Named Parameters, which in turn can only use open-variable syntax for properties.\nWhat you are doing is unusual, and can only happen when trying to work-around some legacy bad written database. New databases should never use names like column-bla-name, that's a horrible idea. The standard is column_bla_name, which then can be auto-camelized with pg-promise, if you want.\nIf you still have such awkward columns to deal with, you would have to provide a value override, i.e. your column would be:\njs\n{name: 'column-layer', prop: 'unused', init: c => c.source['column-layer']}\ni.e. you'd have to provide any prop just to indicate that name no longer represents the source property, only the column name, and thus can be any string, as explained in the API.\nJust shorter alternative:\njs\n{name: 'column-layer', prop: '_', init: c => c.source['column-layer']}\n. Look like the use of Custom Type Formatting will benefit you most. I cannot be more specific, unless you show an example of a valid SQL that works, so I would know what format is expected exactly.. Again, can you show an example of the resulting SQL that does work?\nAnd what you show isn\u2019t custom type formatting, see the link I provided.. If I understand your SQL example right, you are trying to create an array that contains the object keys as the values. In this case your Custom Type Formatting can be done like this:\njs\nfunction toTypes(types) {\n    return {\n        toPostgres: () => pgp.as.format('\\'{$1:raw}\\'::FACILITY_TYPE[]', Object.keys(types).join()),\n        rawType: true\n    };\n}\nAnd then you can use it like this:\njs\ndb.one('INSERT INTO \"Facility\"(name, types) VALUES($1, $2) RETURNING *', [createObj.name, toTypes(createObj.types)])\nor like this:\njs\nconst params = {name: createObj.name, types: toTypes(createObj.types)};\ndb.one('INSERT INTO \"Facility\"(name, types) VALUES(${name}, ${types}) RETURNING *', params)\n. @tjPalSingh \nEven shorter syntax for this:\njs\nconst params = {name: createObj.name, types: toTypes(createObj.types)};\ndb.one('INSERT INTO \"Facility\"(name, types) VALUES(${name}, ${types}) RETURNING *', params)\nwould be:\njs\nconst params = {name: createObj.name, types: toTypes(createObj.types)};\ndb.one('INSERT INTO \"Facility\"(${this:name}) VALUES(${this:csv}) RETURNING *', params)\nor inline:\njs\ndb.one('INSERT INTO \"Facility\"(${this:name}) VALUES(${this:csv}) RETURNING *', {\n    name: createObj.name,\n    types: toTypes(createObj.types)\n})\n. > callstack output with error messages is separate from the callstack context where the bug exists.\nThat's what you normally get when using the default promises, as they are quite inept.\nUse Bluebird with Long Stack Traces instead, and you will get proper stack tracing...\n```js\nconst promise = require('bluebird');\npromise.config({\n    longStackTraces: true // WARNING: Setting this options in production may impact performance.\n});\nconst initOptions = {\n    promiseLib: promise\n};\nconst pgp = require('pg-promise')(initOptions);\n```\nSee also: Initialization Options.\n. @ericandrewlewis It's been 3 days, and no response, so I'm closing it.. Thank you for reporting it! It is now fixed.\n. Thank you for the PR! I have changed the link to a relative one instead.. See Verify database connection with pg-promise when starting an app.. Transactions in this library are automatic, one never needs to do COMMIT or ROLLBACK manually.\n. > access a transaction from outside of the callback for a specific implementation\nSuch ideas typically point at misunderstanding of how automatic transactions work, and then looking for a solution to an non-existing problem.\nIf you can provide an explanation of your specific case, then I should be able to point you at how to approach it within the existing protocol.\n. > I would like to execute it this way because at the time of the beginning the transaction all the queries that I would like to run within that transaction are not known yet.\nSo here's the misunderstanding: The callback function returns a promise, so you can chain any dynamic context/queries into it.\nThe whole protocol is fully dynamic, as it is based on promises.\n. Are you asking to how to chain promises? That's all it takes for your dynamic queries to work, but it is not a question to be asking here.\nIf you are not familiar with promises, you should read about those. I wouldn't be getting into such basics of asynchronous programming here.\n. No, both examples you provided are wrong...\nDepending on asynchronous pattern required, you change the implementation, but always inside the callback function:\njs\ndb.tx(t => {\n    /*\n      either chain you queries one by one, or via batch, or via ES6 generator\n   */\n})\n.then(data => {\n    // data = what transaction returned\n})\n.catch(error => {\n    // error\n});\nBoth this website and StackOverflow has hundreds of examples.\n. Please format the code further on. I have done it for the question there.\nFor such an issue you need to provide:\n\nExpected query syntax\nGenerated query syntax\n\nAlso, which of the parameters gives you the problem?\nAnd it is pointless using syntax like columns: columnsFiles.map(this.pgp.as.name).join(),. Instead, you can just do columns:name in the template, which will be formatted automatically.\n. Slightly changed code should rectify the immediate issue:\n```js\nfunction insertAtomic(file: any): Promise {\nlet insertStatement = 'INSERT INTO $[schema:name].$[table:name]($[columns:name]) VALUES($[values:csv])';\n\nreturn this.db.tx(t => {\n\n    return t.one(insertStatement + ' RETURNING ger_idfile', {\n        schema: this.schemaName,\n        table: this.tableName,\n        columns: columnsFiles,\n        values: valuesFiles\n    }).then(result => {\n        let queries = [];\n\n        let columnsFiles: Array<string> = [\n            'ger_id',\n            'arq_file'\n        ];\n        let valuesFiles = [\n            file_id,\n            file\n        ];\n\n        queries.push(t.none(insertStatement, {\n            schema: this.schemaName,\n            table: 'tb_files',\n            columns: columnsFiles,\n            values: valuesFiles\n        }));\n\n        let source = (index, data, delay) => {\n            return queries[index];\n        };\n\n        return t.sequence(source);\n    });\n});\n\n}\n```\nI don't know why you were trying to use the CSV formatting for each individual value, hence the problem, it seems. The CSV should be used for the entire array of parameters, and that's it.\nAnd I changed it to the automatic syntax, for simplicity.\nOther than that, I do not understand why you are using sequence to execute a single query there. I understand if you removed some code there for simpicity, because the way the example is, it makes no sense.\n. From the lack of responses, I conclude the issue is no longer of interest, so I'm closing it.. What is the version of pg-query-stream that you are using?. Cheers, that all seems fine, at first sight. I will only be able to run some tests on this about 10 hours from now. I will post results here, or maybe you will find something in the meantime :wink:. I was able to reproduce the issue. It seems that all the work-around I did following this issue wasn't quite enough.\nUnfortunately, that library's author is barely ever seen doing anything for his libraries, which is very frustrating. And unless that module is fixed to support readable streams, I cannot do anymore work-around on my end, because this time I'm not sure it is even possible.\n. @shaunakv1 I wouldn't be upset about this whole issue, because that stream library isn't great anyway, because it soaks everything in before it starts streaming, so it is not a real streaming.\nIf you've got too much data to get out of the database, you can always page your requests, using OFFSET + LIMIT :wink:. This is a nice attempt ;)\nOf course there is always room for improvements. For example, if your onUsersAvailable returns a promise, then you can simplify the chain, and not create the extra promise inside getUserInBatch.\nAlso, method sequence is mainly valuable for unlimited/dynamic sequences. When you specify explicit limit, it means your sequence can be simplified to a regular while/loop.\n\nI'm closing the issue in the meantime, because there not going to be any progress in a foreseeable future, until the day pg-query-stream is fixed, which I do not expect any time soon, having followed its progress, or the lack of it, to be exact.\n. Interesting,... I will have a look later this evening. Thank you for supporting the project! :wink:. Fixed in v8.2.3.\n@OrKoN Thank you for reporting the issue!. > I do not see any easy way to fix this without manually going through the large SQL-file\nThe error tells you exactly the line and column where the problem is. Should be easy to locate it and fix.\nposition: {line: 39139, col: 299}. There are some apparent issues with the format of the SQL file that you generate.\npsql is known to pre-parse and work-around some formatting issues, while neither this library no its underlying driver can, i.e. the file is either valid SQL or it is not.\nI would suggest looking into how you end up getting quoted multi-line identifiers there, so may be you can avoid it when generating such SQL files. Normally, one never uses multi-line quoted identifiers, so it looks like an anomaly in the output file, like a formatting issue.\n\nDo you think the same problem is the origin of that error as well?\n\nQuite possibly, but I can only speculate there, since I do not have the SQL file.\n. @ferdizz I understand your SQL file is huge, but I still recommend that you upload it somewhere and post the link here, or else I won't be able to help any further with this.\n. @amenadiel Thank you for the follow-ups!\npg-promise wasn't designed to handle large SQL dumps, and especially that use non-standard SQL syntax. The library was designed to handle SQL files that fit well in memory and can be processed as a single string. Anything else is out of the scope, and underlying driver node-postgres doesn't support it either.\n. As per the Column syntax, you can define your 'count' column any way you want, including:\njs\n{\n    name: 'count',\n    init: c => 'count + ' + c.value,\n    mod: ':raw'\n}\nwhich will give the result you want. However, the thing to note, such syntax will work only for single-row updates, but not for multi-row updates, because there is no such thing as increment syntax within multi-row updates.\nP.S. You should start with upgrading your pg-promise version, as 5.9.7 is now ancient.\nColumn:http://vitaly-t.github.io/pg-promise/helpers.Column.html. Ouch, how did I forget.... :smile:. Re-opening after auto-close. I usually close issues after a new release that contains the fix.. Fixed in v8.3.1.\n@Vinnl Thank you for reporting it! \ud83d\udc4d . Version 8.3.2 further improves on this feature, with support of callbacks.\n. You should upgrade to the latest version of the library, and then research it against the driver. Here's a related issue.\nMaybe adding ?ssl=1 to the connection string, or switching over to the connection object will help in your environment, as explained there. But either way, if there is an issue, it is the driver + environment issue, and not in this library.\n. I would say that 99% chance it is an issue with your PostgreSQL configuration. The thing is, PSQL can access the server in its own way. You need to sort out the connection configuration, to make sure the server is accessible from an app :wink:\nThere are many questions on StackOverflow related to the issue of the connection hanging for PostgreSQL, and they all relate to issues with the database server configuration. You just need to find the one that fits your scenario best, and follow the fix recommendations ;)\nAnd as far as this library is concerned, it is a little out of scope ;)\n. The default examples remain using ${prop} syntax because this is the syntax you'd want when building a proper database layer, one where all your SQL is in external SQL files.\nSee Lisk database layer as an example of such a project, or pg-promise-demo as a concept.\nIt is generally not good to mix SQL with your code. So if you do it right from start, syntax ${prop} is what you will use inside your SQL files.\n. Because ${prop} is already standard in JavaScript, and in many other systems, in fact, so the same is advocated here as standard for SQL files. Syntax ${prop} is fairly universal, and one of the easiest to read and differentiate from everything else.\nAlso, originally the library supported only ${prop} syntax, at the time when Node.js 4.x didn't exist yet. So it will remain to be the default.\n. Thank you for the suggestion, but I won't be making the change, as it is too much to change and for little-to-none benefit.\n. @ericandrewlewis \nI have added a new important consideration for SQL compression, which has direct bearing on what was discussed here, also reassuring why syntax ${prop} must remain to be the default one:\n\nSQL Compression\n\nIn this library, type QueryFile uses pg-minify underneath to optionally minify and compress SQL.. It will work, if you use query formatting correctly, i.e. you cannot use ES6 templates or manual formatting that way, it must be:\njs\nreturn db.one('INSERT INTO users(user_name, email, password) VALUES(${user_name}, ${email}, ${password}) RETURNING *')\nFor ES6 templates you'd need a different format.\n. P.S. Please post future questions on StackOverflow. And see CONTRIBUTING notes.\n. Connection strings use URL-based syntax, and as such they expect all its parts to be URL-encoded.\nFor symbol ?, for example, it is %3F. See also: Question mark in the middle of a url variable?\n\nIf you want to see what your password should look like inside a connection string, use encodeURIComponent. And for the opposite, to see what your password will be used as, use decodeURIComponent.\nYou can also make use of the generic connection-string, which I wrote some time ago, trying to improve the core library, which never got integrated, although the package itself was fully finished, and can be used independently.\nOr, you can just use a connection object instead, which is friendlier to special symbols, as you won't need to do any URL encoding.. Note that after some testing, I found an issue in that module, which I'm going to fix soon: https://github.com/vitaly-t/connection-string/issues/2\n. That issue with connection string module has been fixed.\nAnd closing this one, due to non-response.\n. I've just noticed this post, since you closed it quickly :smile:\nSo basically yes, use of cast within the column definition is all you needed in your case, since you are producing an empty object there, which is a special case for object serialization.\nAs for Custom Type Formatting, you can do anything with it, but in your specific case it is not needed.\n. @benhjames  b.t.w., you do not have to use type Column explicitly, it can be used implicitly:\njs\nconst keys = ['id', { name: 'myField', cast: 'jsonb' }];. No, no such thing. What the error contains is what the server provides, and that is it.\n. This is something to do with the specifics of using SSL under AWS, and not with this library.\nAnd the Bluebird library got nothing to do with it either.\n. May have something to do with the query you are executing, if the query itself hangs. Sometimes the issue is with the hosting environment, when connection hangs.\nThis library would not hang on its own.\n. If the pool is very limited on your server, or completely depleted due to all connections being busy or not released correctly, then yes, you can run into the situation when a query method hangs as it is waiting for next available connection that never becomes available.\nThis points at issues within implementation of your app. Maybe you are allocating a connection by mistake and/or forget to release it.\n. > I have just realised that you can get the number of rows affected with RETURNING field\nThat is not a good way of doing it, if you do not need the data, as you end up transmitting a lot of useless data across the IO from the server.\nThe right approach is via method Database.result that resolves with the original Result object, and then use property rowCount from it, which gives you what you want.\njs\ndb.result(query, values, r => r.rowCount)\n.then(affectedRowsCount => {\n})\n.catch(error => {\n});\n. The error you are getting is perfectly correct, and there by design.\ntitles[1] as title isn't a valid column name. Property name expects the name of the column, as documented, and not a whole SQL statement.\n\nI've tried lots of different options and CFT formatting here as well but Column still tries to format the name key it seems.\n\nYou must be approaching it the wrong way. Give us a complete example of what you are trying to achieve, then I may be able to tell you what you are doing wrong and how to do it right :wink:. ColumnSet is there to automatically generate inserts and updates, so when it comes to multi-row inserts and updates, they can work in the same way. Selects were never in the scope, as they do not need type ColumnSet at all.\nIn your example, if you select dynamic columns with aliases then you would use ${cols:raw} indeed. For columns without aliases you would simply use ${cols:name}.\nAnd in case of ${criteria.field:raw} you should be using ${criteria.field:name} instead, passing it either an object or array of names, to generate SQL Names automatically.\n. A better question - why would you want to do this? It would only make sense if the columns were dynamic, but in your example it looks static. What are you trying to achieve, on higher level?. After your update, ok, it is possible to automate what you are doing, for a much nicer syntax...\njs\n// removed the code as incorrect.\nActually, no, hold on that,.... I will need to review this...\n. A simpler syntax for sure would be to create your own Custom Type formatting class, to encapsulate list of columns, and optional aliased ones, and then format the set automatically, so you do not have to use as.name or as.format where columns are declared.\nThere are many ways to approach this, as the formatting engine is very flexible. You even can extend Array to inject your custom names.. Just one crazy example to show that everything is possible, including the standard types override:\njs\nArray.prototype.asNames = function() {\n    return {\n        toPostgres: () => this.map(pgp.as.name).join(),\n        rawType: true\n    };\n}\n...which you can extend to also support something like {name, alias} inside the array of names.\nAnd then inside your SQL, with the help of nested named parameters:\nsql\nSELECT ${cols.asNames}\ncols must be simply an array for this to work.\n. > Can you clarify our point about extending it?\nYou can check for each individual array element, if {name, alias} found, format it accordingly.\nExample\njs\nArray.prototype.asNames = values => ({\n    toPostgres: () => values.map(a => {\n       if(typeof a === 'object' && 'name' in a) {\n           return pgp.as.format('${name:name} as ${alias:alias}', a);\n       }\n       return pgp.as.name(a);\n    }).join(),\n    rawType: true\n});. I think this discussion has come to a happy end :smile:. 1. Include the full error that you are getting.\n2. I do not do support for such ancient versions of the library as 5.9.7; You need to upgrade first, and then update the issue.\n. @pavan6cs But you haven't upgraded the library as I asked you to.\nAlso, please format the code that you paste there.\nAnd your error syntax error at or near \"as\" looks like SQL error, and not with this library.\n. After another look, you definitely got the SQL syntax wrong there, so it's got nothing to do with this library, so I'm closing this issue as unrelated.\n. You need to find where the syntax is broken in your SQL, and fix it. I can't help you with that, as it is not related to this library.\n. As explained in that issue, parsing an array like that can be locale-dependent, and that's why it doesn't have embedded support. It is not always as easy as you think ;)\n. This library doesn't validate or parse any data that arrives. All that is handled within node-pg-types, so I'm not sure what you are suggesting for pg-promise. Can you clarify that?. As per my previous answer, parsing data is outside of this library's scope.\nAnd why can't you use that approach presently? What is the issue with that? From what I know, you can set any parser there you want.. What 10 lines of code? And you didn't explain what stops you from using that 1 line of code.\nAnd I've mentioned twice that data parsing is out of scope here.. The underlying driver generally does not support parsing arrays of custom types, so it is unavoidable that the client needs to provide its own parser. And this library cannot help you with that, as it does not parse any data coming from the driver, so it is out of scope here.. I know nothing about Azure dock container to make any suggestions, sorry. You should instead research why connections in a Node.js app are not released on Azure as expected, and how to resolve it.\n. pg-monitor only logs query-related things, plus the virtual (pool-level) connections.\nI suspect it is something to do with the environment configuration, and definitely not on pg-promise level, so debugging it is a waste of time.\nYou can even use node-postgres to write a simple connect-disconnect test, and I believe it will show the connection isn't released as expected.. @shaunakv1 \nFound it! - https://github.com/brianc/node-postgres/issues/1627\nSo apparently, the issue was fixed since version 7.4.2 of the node-postgres driver, which means if you use the latest pg-promise (8.4.4 currently), it should just work.\n. I cannot reproduce the issue, the library works as documented. Perhaps you are doing something else that you did not enclose here.\nMy guess, you probably initialize the library in more than one place, and in one of those places you do not use the flag. This would be the wrong way to use the library, see Where should I initialize pg-promise.. Have you seen the example for event receive? It shows you how to do it ;)\nSee also: Pg-promise and case sensitivity in column names.\n. COPY operation isn't supported, not by this library, not by its node-postgres driver, because it requires specific protocol of communicating with the server, which the driver doesn't provide on its own.\nThis is why there is node-pg-copy-streams out there, which one has to use, unfortunately.\nIn general, COPY operation is very auxiliary, you don't have to use it, as it can be replaced with regular inserts that's just as fast, while significantly more flexible.\n. @ThomWright Those were spot on, thank you! :smile:. Dumbest idea - attach correlationID to each query string/object:\njs\nconst query = 'SELECT * FROM table';\nquery.correlationID = 123;\nThat way inside the query event you can access e.query.correlationID.\nAfter all, it is JavaScript :wink:\nOther than that, the library automatically provides context for tasks and transactions, see Tags.. And if you are using Prepared Statements or Parameterized Queries, things get even simpler:\njs\nfunction createParameterized(query, values, correlationID) {\n    const ps = new PreparedStatement(query, values);\n    ps.correlationID = correlationID;\n    return ps;\n}\nThen again you can extract it inside the query event from e.query.correlationID.\n\nP.S. Although if you are using TypeScript, you would be out of luck on any of these solutions, sadly.\n. Wrapping the protocol like this isn't practical, as it wouldn't be reusable from inside tasks and transactions. Event extend should be used when extending the protocol. See also pg-promise-demo.\n\nThe Tags feature is useful, is there a reason there couldn't be a context for a single query outside of a task/tx?\n\nTasks and transactions provide context to easily locate any specific query in the code, since those become reusable and can be called from anywhere. You are trying to accomplish a different task, injecting traceable parameters into queries that are not part of the query.\nYou are trying to join HTTP request context with the queries context, and as those are two completely separate things, I wouldn't consider it a shortage on the pg-promise part, it's kind-of unrelated. Given the unusual nature of what you want, a work-around would be the best to hope for.\n. What's stopping you from doing so? Why did you open an issue here? See Contributing.. I cannot reproduce the error.\nFor starters, what you are showing passed as options into the QueryFile isn't even a valid JavaScript.\nAnd your CREATE TABLE SQL is invalid, as you are trying to create a table without fields, which in itslef would throw an error.\nBut none of them will throw the error you're describing. Something else is wrong in your code, and you need to find out what else you are may be doing wrong there.\nOnce I fixed those obvious mistakes in your example, the code just worked.\nP.S. Best is to pass the schema in as ${schema:name}, not as raw text. And even better still not to use schemas at all, but that's not relevant to the issue ;)\n. Closing due to non-response, and as being non-reproducible.\n. Yes, it does.\nPlease read CONTRIBUTING before opening a new issue.. I can see a number of problems in your code:\n\nreturn yield req.body.lineitems.map(line => createSalesOrderLine(line, orderId, t)) - here you are generating an array, which you cannot just yield like this, it is not a single promise anymore.\nYou fail to chain the task in createSalesOrderLine, so worse than that, you end up with an array of undefined when you use it\nreturn yield line.options.map(char => updateOptions(char, 12, lineId, task)); - again, you can't just yield an array of promises\n\nSo all your problems are promise-related.\nFor example, this line: return yield line.options.map(char => updateOptions(char, 12, lineId, task)); should be replaced with this:\njs\nreturn yield task.batch(line.options.map(char => updateOptions(char, 12, lineId, task)));\nSimilar for the other case, but there you need to chain the inner task first ;). Your code should be something like this:\n```js\nfunction addSalesOrder(req, res, next) {\n    const customer = req.body.customer;\ndb.tx('addSalesOrder', function* (t) {\n    const orderId = yield createSalesOrderHeader(customer, t);\n    return yield t.batch(req.body.lineitems.map(line => createSalesOrderLine(line, orderId, t)));\n})\n    .then(data => {\n        res.status(200)\n            .json({\n                data: `Sales Order successfully created.`\n            })\n    })\n    .catch(error => {\n        res.status(401)\n            .json({\n                data: `Sales Order creation failed.`\n            })\n    });\n\n}\nfunction createSalesOrderHeader(customer, t) {\n    t = t || db;\n    return t.one('SELECT createsalesorder($1) AS id', customer.toUpperCase(), o => o.id);\n}\nfunction createSalesOrderLine(line, orderId, t) {\n    t = t || db;\nreturn t.task('createSalesOrderLine', function* (task) {\n    lineId = yield task.one('SELECT createsalesorderline($1, $2, $3, $4, $5, $6) as id',\n        [line.item,\n            orderId,\n        line.duedate,\n        line.duedate,\n        line.quantity,\n        line.price], l => l.id);\n    console.log(`LineId ${lineId}`);\n\n    return yield task.batch(line.options.map(char => updateOptions(char, 12, lineId, task)));\n});\n\n}\nfunction updateOptions(char, charId, lineId, t) { \n    t = t || db;\n    return t.one('SELECT updateOptions($1, $2, $3, $4) as id', ['C', lineId, charId, char.value], a => a.id);\n}\n```\nAlso, things like a => a && a.id apply to the oneOrNone logic, but not to one logic when you always expect one object, that's why I changed those as well.. Not just missing. If you have .catch there, then you need to re-throw when chaining, i.e.\njs\n.catch(error => {\n    console.log(error);\n    throw error;\n})\nBut you are better off not doing things like that at all. Instead, use pg-monitor :wink:\n. I would guess that perhaps formatting item.pageNumbers as JSON isn't sufficient for tuples?\nAnd you absolutely should use pgp.as.format to format the string, never do manual formatting, i.e. this part:\njs\noutput += `\"(`;\n      output += `${item.text},`;\n      output += `${item.signerName},`;\n      output += `${item.signerEmail},`;\n      output += `${item.coordinateX},`;\n      output += `${item.coordinateY},`;\n      output += `${item.width},`;\n      output += `${item.height},`;\n      output += `${pgp.as.json(item.pageNumbers)},`;\n      output += `${item.fontSize},`;\n      output += `${item.fontFamily},`;\n      output += `${item.required},`;\n      output += `${item.documentId},`;\n      output += `${item.userId}`;\n      output += `)\"`;\nshould be:\njs\noutput += pgp.as.format('(${text},${singerName},${coordinateX},${coordinateY},${width},${height},${pageNumbers:json},${fontSize},${fontFamily},${required},${documentId},${userId})', item);\nWhether this will be enough or not - I don't know. I would suggest to proceed with this on StackOverflow, as general tuples formatting is something a bit on the outside here.\n. The example pushes all queries as far as the IO where Node.js parallel thread will be processing them. That's how Node.js works.\nUsing queries as functions will only delay it, but not solve your problem. It sounds like you have a racing condition, which you should resolve by properly chaining the dependency, according to the logic of your queries.\nBy the way, have you thought of using helpers instead? - https://stackoverflow.com/questions/37300997/multi-row-insert-with-pg-promise\nThey are much more efficient ;) Even if you use only helpers.concat.\n. batch settles promises, means it makes sure each promise to its end-of-life. The way queries go through IO is another matter. If you have a dependency, you should enforce it explicitly, by not creating queries till the point where it is possible.\nMakes you easier to understand when you use ES7 async/await, or at least ES6 generators ;)\n. ES6 version:\njs\ndb.tx(function * (t) {\n    for(let i = 0; i < queries.length; i ++) {\n        yield t.none(<an insert with a foreign key constraint>, queries[i]);\n    }\n}). > In your ES6 snippet above, where would you create the queries variable?\nqueries in that context in fact would be an array of objects with parameters, so really, anywhere. Just poorly named :)\n. More like this:\njs\ndb.tx(function * (t) {\n    yield t.none(<an insert that the foreign keys referenced>);\n    for(let i = 0; i < queries.length; i ++) {\n        yield t.none(<an insert with a foreign key constraint>, queries[i]);\n    }\n}). This is not related to this library. It doesn't do anything JSON-specific, it just executes queries as they are.\n. > When I provide an object\nObjects are formatted as JSON, which is the standard. So i'm not sure what you mean by unexpected bugs :)\nAnd if you use undefined as a formatting parameter, it is formatted as null.\n\nError: Property 'member_id' doesn't exist\n\nThat's an error on your side, missing the property in the object.\n\nit would be less error-prone to have an option to tolerate these values as Null\n\nYou can implement it on your side either by using Custom Type Formatting or helpers.\nAlso there is option default for as.format method, but that's a low level.\n. A transaction in this library manages its integrity by fully encapsulating the connection inside it, as well as result of the operation. What you are asking, is to compromise both of those things, by exposing the connection outside of the transaction, and its ability to handle errors correctly.\nFor any application it should be strictly a no-no.\nBut if you want to hack your tests that way, you can manage the connection manually all the way, but be careful not to leak the connections, and to handle errors properly...\n```js\nlet cn;\ntry {\n    cn = await db.connect(); // manually created connection;   \nawait cn.query('BEGIN');\n\n/* and so on */\n\nawait cn.query('COMMIT'); // or `ROLLBACK`\n\n} catch(error) {\n// handle errors\n// like, you can do your ROLLBACK here\n} finally() {\n    if(cn) {\n        cn.done(); // must release the connection in the end\n    }\n}\n```\nYou can do all that, but it is generally not a good idea.\n\npg-promise doesn't support explicit rollbacks\n\nIt does. If a transaction throws an error or returns a rejected promise, it will ROLLBACK right away.. You are looking at testing database integrity the wrong way. The right way is to test for data that was either rolled back or committed. Then you do not need to reinvent those dangerous things here.\nThere is no scenario that would require breaking a transaction, the way you are trying to break it.\nI know it, as I helped along with a number of large projects at this point, which involved testing hugely complex transactions.. I take it the issue is resolved, since there was no response, so closing it.. What you are doing here is looks fundamentally wrong, I'm not even sure where to start...\nFor one thing, one does not run queries like this in a loop, and without tasks, as it will always be of very poor performance - too many queries and without even sharing a connection. For another, a selection like this using either WHERE IN or a block SQL that includes everything. You should end up with just a single SQL query, like INSERT... FROM (SELECT ....).\nAs per the CONTRUBUTING notes, you should ask such things on StackOverflow.\n. I'm not sure what you are really asking here...\n@hawkeye64 can you clarify your question, please?. Closing, since no clarification provided after 3 days.. Are you asking why Python receives data differently from Node.js?\nI don't know what code you are using in Pyton :)\nWithin Node.js driver used here, you can request data as an array, via option rowMode within ParameterizedQuery:\n```js\nconst PQ = require('pg-promise').ParameterizedQuery;\nconst query = new PQ({\n   text: 'some query',\n   values: / some values /,\n   rowMode: 'array'\n});\n```\nIt is possible that it is what your Python code is doing.\n. Some of that code was to work-around issues within pg-query-stream, for which I spent quite a bit of time. If you can find a way to improve it, PR-s are welcome!\n. In my view, the biggest issue with that driver is that it fails in its very basic purpose - actually streaming data. It only makes the query interface compatible with the streaming (and not even fully at that), but the actual data is consumed as a whole at once, which defeats the very purpose of streaming large data, making it quite useless.\nTypically, developers expect to process data record-by-record, and with that driver they cannot.\n. After a long wait to get the PR fixed, it was closed unmerged, due to non-response. Therefore, closing this issue as well.. @jamo You simply append the ON CONFLICT part to your query, and that's it.\nFor example, if you are getting a conflict on columns col1 + col2, you would often use:\njs\nconst insert = pgp.helpers.insert(data, cs) + ' ON CONFLICT(col1, col2) DO UPDATE SET ' +\n    cs.assignColumns({from: 'EXCLUDED', skip: ['col1', 'col2']});\n. @bradleykirwan Could you, please review why the change resulted in the test coverage drop from 100% to 68%? Looks like something is broken there.. @bradleykirwan Thank you! To run the tests locally, see Testing.. @bradleykirwan any update? :wink:. @bradleykirwan If you can't get back at issues with the tests it is ok, but I can't merge such PR, and will have to close it then. I don't want it hanging here forever.\nAlso note the update with streaming that was done in 8.5.0 release.. @bradleykirwan I gave you plenty of time, and made a few requests to fix the tests issue, but got no response, I even have tried to fix it myself, but could not find the issue. Some of the changes appears to be invalid.\nAt this point I have no choice but to close the PR. Feel free to re-do it later, if you can make time.\n. Inserts and updates are special cases that require providing repeated series of data that needs to be escaped automatically. That's why this library supports it. Any other part of a query you can generate yourself. And this library is NOT an ORM, it is not supposed to generate all your queries dynamically, its purpose is to execute pre-generated queries.\n. Your question was answered there long before you re-published it here. So why you re-published it here?\nAlso see CONTRIBUTING.. What feature request?. I believe I explained you well in my answer on StackOverflow that type casting has no relation to this library, as it simply executes queries. There is no feature request to be considered here whatsoever.\n. I suggest a bit more diagnostics on your side, as transactions and QueryFile are unrelated, i.e. the latter doesn't care in which context it is used.\n\nOn a side note, such queries are best to be generated. And your query isn't formatted well, it should be using the EXCLUDED values within the ON CONFLICT section, and not re-insert values.\nHere's a good example for generating such queries:\n```js\n// declare the ColumnSet only once, statically:\nconst cs = new pgp.helpers.ColumnSet(['id', 'creator_id', 'name', 'description', 'game_system_id', 'currency_id', 'dates', 'latitude', 'longitude', 'location_name', 'postcode', 'address'], {table: 'tournaments'});\n// data = your object with data, or an array of such objects;\nconst insert = pgp.helpers.insert(data, cs) +\n    ' ON CONFLICT ON CONSTRAINT tournaments_pkey DO UPDATE SET date_edited=CURRENT_TIMESTAMP,' +\n    cs.assignColumns({from: 'EXCLUDED', skip: 'id'});\ndb.none(insert).then().catch();\n```\nSee helpers.\n. I'm sure the issue you had was an overlap of sorts, because the behavior you referred to is fundamental, and if there were an issue, it would have been reported long time ago, considering how large the user base is.\n. helpers.concat is supposed to work with either regular flattened SQL or SQL files via Query Files. The latter can remove comments and flatten the SQL, by passing in option minify, so it is never an issue, if you are using external SQL correctly.\n```js\nconst f = new pgp.QueryFile('./my_query.sql', {minify: true});\nconst q = pgp.helpers.concat([\n   f, / etc... /\n]);\n```\nOther than that, helpers.concat does not work with SQL that contains comments, and it is not supposed to, as Query Files are supposed to take care of it.\nSo everything works as expected.\n. I don't see how this is related to this library. It is a server-side error, and should be addressed accordingly.\n. There are so many duplicates about this already, and they all boil down to just appending the correct SQL clause to your query. Here's just one of them: #542\n. What you describe here has no relevance to this library. It is the PostgreSQL server that auto-converts the string type into the expected date type, and then returns the stored value. This library only returns the data exactly as the server provides it, without any conversion.. See the following questions:\n\nNested query object mapping with pg-promise\nCombine nested loop queries to parent array result - pg-promise\n\nget JOIN table as array of results with PostgreSQL - this is probably the best one for joins.\n\nOther than that, please follow CONTRIBUTING notes. Here's just for issues, not for questions anymore.. In this case you would have to take formatting outside for the WHERE clause.\nThe best + safe way to do it is like this:\n```js\nconst query = () => pgp.helpers.update(data, null, 'some_table') +\n                    pgp.as.format(' WHERE id = ${id} RETURNING *', data);\nreturn db.one(query);\n```\n\nIt is better to use named parameters where possible, as we use ${id}\nIt is better to pass in a function rather than a string, when you are formatting it outside, so in case of a formatting issue it will be wrapped by the query method, and not thrown outside (which would need to be handled).\n. This is strictly a server-side error, and as such, it is not related to this driver.\n. There also must be a way to configure the server to do the same. Support for option schema is there for cases when it is somehow not possible, or creates a lot of inconvenience.\n. @akhchan99  You are mistakenly executing queries against the root context db, while you need to execute them against the transaction context t:\n\nHere's your code corrected and simplified, using ES7 syntax:\njs\ndb.tx(async t => {\n   await t.none('UPDATE userTable SET first_name = $2 WHERE user_id = $1', [1, 'A']);\n   await t.none('UPDATE userTable SET last_name = $2 WHERE user_id = $1', [1, 'B']);\n   await t.none('UPDATE userTable SET first_name = $2 WHERE user_id = $1', ['XXX', 'C']);\n})\n  .catch(console.error);\nThe same, with ES6 generators syntax:\njs\ndb.tx(function * (t) {\n   yield t.none('UPDATE userTable SET first_name = $2 WHERE user_id = $1', [1, 'A']);\n   yield t.none('UPDATE userTable SET last_name = $2 WHERE user_id = $1', [1, 'B']);\n   yield t.none('UPDATE userTable SET first_name = $2 WHERE user_id = $1', ['XXX', 'C']);\n})\n  .catch(console.error);\n. @isikfsc Could you, please provide examples? I'm not clear on what you are talking about, especially for to change and use a different query file for payloads with different keys.\n. Closing, as the author didn't clarify what he wanted, and it is not clear from the initial context without examples.\n. You are supposed to be using parameter dc - Database Context, that's it primary purpose, to differentiate between different databases. See the constructor.\nAlso, upgrade the library to the latest, and use initialization option schema, setting it based on the dc. \nThat is all that you need.\n```js\nconst initOptions = {\n   // use the syntax with a function that takes dc as parameter:\n    schema: dc => return 'my_dc_schema_bla_123' // set schema, based on the Database Context\n};\nconst pgp = require('pg-promise')(options);\n```\nAnd then you create a separate db for each connection, passing in dc that can be identified when setting the schema:\njs\nconst db1 = pgp(/*connection details*/, 'any unique string/number/object - anything');\nconst db2 = pgp(/*connection details*/, 'any unique string/number/object - anything');\nconst db3 = pgp(/*connection details*/, 'any unique string/number/object - anything');\n. Which version of TypeScript are you using?. Modern TypeScript has so many special options these days, that some of them indeed may result in pg-promise script not transpiling correctly. This however does not merit documenting all such cases.\nThe important thing is, examples like pg-promise-demo builds correctly, without issues, while relying on the default TypeScript settings, i.e. it builds fully without any tsconfig.json.\nIf something does not build using the default TypeScript profile, it is worth documenting, otherwise it is not.\n. As in turns out, the issue was with broken compatibility with pg-query-stream, which was reigning havoc on the test results.\nThe updated version 1.1.2 of pg-query-stream fixed the issue.\n. Resolved in v8.5.0.\n. Are you asking how to update JSON fields from SQL or how to construct the exact query you seek?\nIf it is the former, then it is not applicable to be asking here.\nIf it is the latter, then you can change the column definition to this:\njs\n{\n    name: 'metadata',\n    mod: ':raw',\n    init: c => pgp.as.format('$1:name || $2', [c.name, {custProp: c.source.custProp}])\n}\nor this (just simpler):\njs\n{\n    name: 'metadata',\n    mod: ':raw',\n    init: c => 'metadata || ' + pgp.as.json({custProp: c.source.custProp})\n}. > how is sequence or page performed\nhttp://vitaly-t.github.io/spex/global.html#page\nI might look at your code later, currently down with flu and fever, can't read the damn code right. \ud83d\ude37\n. @kristjank still bad here :)\nHave you made any progress in the meantime? I gave the link to method page documentation earlier, and as you can see, it supports option limit there.\n. @kristjank The example in Data Imports already shows the use of paging within the streaming context. Have you tried that?\nOther than that, everything is promised-based, so everything is doable/chainable :wink:, just a matter of trying. If you stumble upon a more specific issue, I might help you, otherwise your question is too broad for any more specific suggestions.\n. Yeah, that QueryStream is a third-party type, so it can't handle type QueryFile.\nType QueryFile implements Custom Type Formatting (the Symbolic one), so you can do:\n```js\n// qf = your QueryFile object\nconst ctf = pgp.as.ctf;\n// gets the most current query, according to the internal debug option:\nconst query = qfctf.toPostgres;\n```\nYou should treat it as an exception, because all query methods handle it automatically, and with more intelligence. For example, if the object is in error state, all query methods know how to handle it, while the query would be null, i.e. - \njs\nif(qf.error) {\n    // the object is in error state\n    // qf.error = error details\n} else {\n    // only when not in error state we can get the query through CTF:\n    query = qf[ctf.toPostgres]();\n}\nSo to wrap this up, you can do something like this, internally:\njs\nQueryFile.prototype.getQuery = function() {\n    if(this.error) {\n        throw this.error;\n    }\n    return this[pgp.as.ctf.toPostgres]();\n}\nAnd then you can use qf.getQuery() in those special cases ;)\n. On second thought though, it would be a much better use of the library to rely on the internal query-formatting engine manually in this situation (even if you do not have any params):\njs\nconst query = pgp.as.format(qf);\n// OR:\nconst query = pgp.as.format(qf, params);\nThat way you will automatically get the query + format it using pg-promise native syntax, so for example you can use Named Parameters, i.e. full formatting syntax supported by the library, and not just the very basic $1, $2... as supported by that QueryStream type.\n. I take it, this has been resolved.\n. Thank you for reporting!\nI have updated the Connection Syntax page to mention type TSSLConfig that's within TypeScript.\n. Why would it be ignored when the official documentation clearly explains that it is the very feature of the library to provide the event for both regular queries and streaming?\n\nGlobal notification of any data received from the database, coming from a regular query or from a stream.\n\n\n\nAccording to docs, this should be ignored when using stream export.\n\nWhat docs?\n. The receive handler receives all records. Why your code doesn't change all of them - I don't know, depends on what that code is doing. If you can reproduce it in a simplified form - post it here, and I will have a look.. That code was an attempt to mediate a long-standing issue in the driver. And as such, it is, of course, not perfect, I only provided for the most common cases of losing a connection during a long transaction. If you have a different case, and can offer an improvement over this work-around, a PR is welcome.\nIdeally, we all want to see the issue fixed in the driver, but I'm tired of chasing it, without result.\n. P.S. There is a chance that this PR resolved the original issue, but it needs a thorough testing.\n. @joux3 as per my comments earlier, there is a chance that the whole issue was resolved in the latest driver.\nSo please try this first:\njs\nfunction isConnectivityError(err) {\n    return false;\n}\nand see if your error still happens or not.\n. @joux3  After applying all the latest updates, some tests and considerations, I ended up implementing the very same change as was originally in your PR.\nThe reason is, even though the library now can work without function isConnectivityError, it works better/smarter with that verification. So I left it, amended with the query cancellation exception.\nIt was all released in version 8.5.3.\n. This is consistent with how Configurable Transactions are meant to be used.\n. TypeScript declarations circumvent the JavaScript implementation of the library, which is never ideal, from the declaration point of view. And because of that, it has been tweaked many times.. You should import those definitions like this:\nts\nimport {txMode} from 'pg-promise';\nThen you can access the required definitions like this:\nts\ntxMode.isolationLevel;\ntxMode.TransactionMode;. I would suggest first to try this:\njs\nfunction isConnectivityError(err) {\n    return false;\n}\nBecause there is a chance the whole thing has been resolved within the latest version of the underlying driver. I just haven't tested it yet.. Do you want to shared your code? Otherwise I cannot advise any better than it is done within Data Imports.\n. You should read data in chunks that can be kept in memory, then insert those, and only then read the next one. Instead, you are reading everything in, which overloads the memory.. It needs to be debugged to see the exact issue. I cannot do with a piece of code, neither I'd have the time. I suggest that if you still have this problem, try to narrow it down, and ask it on StackOverflow.\n. I don't mean to be rude, but this is absurd to even suggest, for a library used in thousands of projects.\nYou must be doing something wrong, and you should investigate what it is on your side, and avoid opening such issues prematurely.\n. I would need to include some tests for this. So how can this be tested? And what kind of errors are you getting?\nAlso, this library does not use callbacks, only promises.\n. @tolgaakyuz Can you provide more details on this? - \n\nWhere in the pool code the connection becomes stale, if error is not passed within done?\nHow did you test this?\n\nFrom what I've seen so far, the pool doesn't care about any parameters passed within method done. Where do you see it being used?\n. @johanneswuerbach Ok, that makes sense, cheers, merged.. This forum is for rising issues. For questions please use StackOverflow in the future.\n\nDoes this waiting time increase event queue delay on app side or the waiting happens on postgres side\n\nIt is on the app side. If you want it to be on the server side, then execute it all as one query. For an example, see this recent question: Returning different queries together as a JSON object vs. multiple queries.\n. Example of how to format such a query:\njs\nconst q = pgp.as.format(`INSERT INTO public.users AS t($1:raw) VALUES $2:raw ON CONFLICT(email) \n                         DO UPDATE SET $3:raw WHERE t.name IS NOT NULL`,\n    [cs.names,\n    pgp.helpers.values(data, cs),\n    cs.assignColumns({from: 'EXCLUDED', to: 't', skip: 'email'})]);\nwhere cs is your ColumnSet, and data is the array of insert objects. And you can pass in the template + formatting parameters directly into your query method, to be formatted by it.\nSince your query is so special, you can use such approach, but in general, the library doesn't really need it. It is flexible enough to allow for any unusual syntax.\n. format method is just an example, you do not need to use it in your case, you can just pass the query template + formatting parameters directly into your query method.\n\nAnd you can pass in the template + formatting parameters directly into your query method, to be formatted by it.\n. If values are inserted correctly, then it works, no?\n\nI can't see from your example where the error is coming from.\nYou should publish such questions on StackOverflow.. @johanneswuerbach As you can see, your changes result in errors. Do you want to re-do it?. @johanneswuerbach I had to comment out most of the test, as it is flimsy, works only sometimes.\nIf you get a chance to rewrite the test, please do another PR.. Formatting filters are there only to cover the most common scenarios.\nFor any other custom formatting, there is Custom Type Formatting.\n. Because you are not returning anything from the transaction callback function. You are running an asynchronous code block that's completely detached from the transaction callback. As a result, nothing is chained to the transaction, not results, not errors.. It is not outdated. What makes you hope it would be? :) You are probably confused by word \u2018processed\u2019, which in this context means it resolves with the last value.. multi + multiResult.. > looking at the docs for node-postgres it seems that they already provide a promise interface out of the box (in addition to supporting callbacks). Am I missing something here?\nYes, looking at the docs of this library. Start from the very beginning, the About section.\nAnd today it has even more features, like query generation for complex multi-row inserts and updates.\nAs for the selling point, there isn't much going on, this library has always been free :)\n. Your question pertains to SQL syntax, and not this library usage.. What have you tried so far?. Good! :). @lzambarda Please follow the new issue template, and provide a usable example / reproduction steps, and other details. There is nothing obvious in the code to start with.. I have tried to reproduce the issue on my Windows 10 machine, but without success. Any other detail you might add for this?\nMaybe it is something specific to your OS, app, configuration, or the way you terminate the connection?\nOr more likely, it is something specific to SSH tunneling, which in all fairness, I'm not well familiar with.. @lzambarda Can you provide steps for reproducing it on Windows, including configuration and termination of SSH tunneling? Then we might be able to make progress on this.\n. From what I wrote earlier, I don't know how to set up SSH tunneling on Windows. Can you provide the steps? Also, I do not have access to any remote PostgreSQL server here, only local ones. Will that be a problem?\nAlternatively, you can try and diagnose the issue on your side, since you can replicate the issue :wink:\n. @lzambarda Any chance you will attempt a fix PR yourself? I cannot reproduce it, so can't make any progress on this issue.. Unfortunately, I keep getting an error in the console, every time I'm trying to connect to the tunnel:\n\nAny idea what I might be missing?\n. I saw a suggestion to add -vvv key for diagnostics, which produced mind-boggling level of detail, which doesn't really tell me much, as I am not a networking expert:\n```\nC:\\WINDOWS\\system32>ssh -vvv -N -L 2222:127.0.0.1:5432 localhost\nOpenSSH_for_Windows_7.6p1, LibreSSL 2.6.4\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/config error:2\ndebug3: Failed to open file:C:\\ProgramData\\ssh/ssh_config error:2\ndebug2: resolving \"localhost\" port 22\ndebug2: ssh_connect_direct: needpriv 0\ndebug1: Connecting to localhost [::1] port 22.\ndebug1: Connection established.\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_rsa error:2\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_rsa.pub error:2\ndebug1: key_load_public: No such file or directory\ndebug1: identity file C:\\Users\\Vitaly/.ssh/id_rsa type -1\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_rsa-cert error:2\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_rsa-cert.pub error:2\ndebug1: key_load_public: No such file or directory\ndebug1: identity file C:\\Users\\Vitaly/.ssh/id_rsa-cert type -1\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_dsa error:2\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_dsa.pub error:2\ndebug1: key_load_public: No such file or directory\ndebug1: identity file C:\\Users\\Vitaly/.ssh/id_dsa type -1\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_dsa-cert error:2\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_dsa-cert.pub error:2\ndebug1: key_load_public: No such file or directory\ndebug1: identity file C:\\Users\\Vitaly/.ssh/id_dsa-cert type -1\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_ecdsa error:2\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_ecdsa.pub error:2\ndebug1: key_load_public: No such file or directory\ndebug1: identity file C:\\Users\\Vitaly/.ssh/id_ecdsa type -1\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_ecdsa-cert error:2\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_ecdsa-cert.pub error:2\ndebug1: key_load_public: No such file or directory\ndebug1: identity file C:\\Users\\Vitaly/.ssh/id_ecdsa-cert type -1\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_ed25519 error:2\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_ed25519.pub error:2\ndebug1: key_load_public: No such file or directory\ndebug1: identity file C:\\Users\\Vitaly/.ssh/id_ed25519 type -1\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_ed25519-cert error:2\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/id_ed25519-cert.pub error:2\ndebug1: key_load_public: No such file or directory\ndebug1: identity file C:\\Users\\Vitaly/.ssh/id_ed25519-cert type -1\ndebug1: Local version string SSH-2.0-OpenSSH_for_Windows_7.6\ndebug1: Remote protocol version 2.0, remote software version MS_1.100\ndebug1: no match: MS_1.100\ndebug2: fd 3 setting O_NONBLOCK\ndebug1: Authenticating to localhost:22 as 'vitaly'\ndebug3: hostkeys_foreach: reading file \"C:\\Users\\Vitaly/.ssh/known_hosts\"\ndebug3: record_hostkey: found key type RSA in file C:\\Users\\Vitaly/.ssh/known_hosts:3\ndebug3: load_hostkeys: loaded 1 keys from localhost\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/known_hosts2 error:2\ndebug3: Failed to open file:C:\\ProgramData\\ssh/ssh_known_hosts error:2\ndebug3: Failed to open file:C:\\ProgramData\\ssh/ssh_known_hosts2 error:2\ndebug3: order_hostkeyalgs: prefer hostkeyalgs: ssh-rsa-cert-v01@openssh.com,rsa-sha2-512,rsa-sha2-256,ssh-rsa\ndebug3: send packet: type 20\ndebug1: SSH2_MSG_KEXINIT sent\ndebug3: receive packet: type 20\ndebug1: SSH2_MSG_KEXINIT received\ndebug2: local client KEXINIT proposal\ndebug2: KEX algorithms: curve25519-sha256,curve25519-sha256@libssh.org,ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group16-sha512,diffie-hellman-group18-sha512,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha256,diffie-hellman-group14-sha1,ext-info-c\ndebug2: host key algorithms: ssh-rsa-cert-v01@openssh.com,rsa-sha2-512,rsa-sha2-256,ssh-rsa,ecdsa-sha2-nistp256-cert-v01@openssh.com,ecdsa-sha2-nistp384-cert-v01@openssh.com,ecdsa-sha2-nistp521-cert-v01@openssh.com,ssh-ed25519-cert-v01@openssh.com,ecdsa-sha2-nistp256,ecdsa-sha2-nistp384,ecdsa-sha2-nistp521,ssh-ed25519\ndebug2: ciphers ctos: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr\ndebug2: ciphers stoc: chacha20-poly1305@openssh.com,aes128-ctr,aes192-ctr,aes256-ctr\ndebug2: MACs ctos: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1\ndebug2: MACs stoc: umac-64-etm@openssh.com,umac-128-etm@openssh.com,hmac-sha2-256-etm@openssh.com,hmac-sha2-512-etm@openssh.com,hmac-sha1-etm@openssh.com,umac-64@openssh.com,umac-128@openssh.com,hmac-sha2-256,hmac-sha2-512,hmac-sha1\ndebug2: compression ctos: none\ndebug2: compression stoc: none\ndebug2: languages ctos:\ndebug2: languages stoc:\ndebug2: first_kex_follows 0\ndebug2: reserved 0\ndebug2: peer server KEXINIT proposal\ndebug2: KEX algorithms: ecdh-sha2-nistp256,ecdh-sha2-nistp384,ecdh-sha2-nistp521,diffie-hellman-group-exchange-sha256,diffie-hellman-group-exchange-sha1,diffie-hellman-group14-sha1\ndebug2: host key algorithms: ssh-rsa,ssh-dss\ndebug2: ciphers ctos: aes128-ctr,aes192-ctr,aes256-ctr,3des-cbc,aes128-cbc,aes192-cbc,aes256-cbc\ndebug2: ciphers stoc: aes128-ctr,aes192-ctr,aes256-ctr,3des-cbc,aes128-cbc,aes192-cbc,aes256-cbc\ndebug2: MACs ctos: hmac-sha2-256,hmac-sha2-512,hmac-sha1\ndebug2: MACs stoc: hmac-sha2-256,hmac-sha2-512,hmac-sha1\ndebug2: compression ctos: none\ndebug2: compression stoc: none\ndebug2: languages ctos:\ndebug2: languages stoc:\ndebug2: first_kex_follows 0\ndebug2: reserved 0\ndebug1: kex: algorithm: ecdh-sha2-nistp256\ndebug1: kex: host key algorithm: ssh-rsa\ndebug1: kex: server->client cipher: aes128-ctr MAC: hmac-sha2-256 compression: none\ndebug1: kex: client->server cipher: aes128-ctr MAC: hmac-sha2-256 compression: none\ndebug3: send packet: type 30\ndebug1: sending SSH2_MSG_KEX_ECDH_INIT\ndebug1: expecting SSH2_MSG_KEX_ECDH_REPLY\ndebug3: receive packet: type 31\ndebug1: Server host key: ssh-rsa SHA256:KjCodpv/BcMCF5qlNCS2HbYApJ3lEhFHEeHlka9nIVI\ndebug3: hostkeys_foreach: reading file \"C:\\Users\\Vitaly/.ssh/known_hosts\"\ndebug3: record_hostkey: found key type RSA in file C:\\Users\\Vitaly/.ssh/known_hosts:3\ndebug3: load_hostkeys: loaded 1 keys from localhost\ndebug3: Failed to open file:C:\\Users\\Vitaly/.ssh/known_hosts2 error:2\ndebug3: Failed to open file:C:\\ProgramData\\ssh/ssh_known_hosts error:2\ndebug3: Failed to open file:C:\\ProgramData\\ssh/ssh_known_hosts2 error:2\ndebug1: Host 'localhost' is known and matches the RSA host key.\ndebug1: Found key in C:\\Users\\Vitaly/.ssh/known_hosts:3\ndebug3: send packet: type 21\ndebug2: set_newkeys: mode 1\ndebug1: rekey after 4294967296 blocks\ndebug1: SSH2_MSG_NEWKEYS sent\ndebug1: expecting SSH2_MSG_NEWKEYS\ndebug3: receive packet: type 21\ndebug1: SSH2_MSG_NEWKEYS received\ndebug2: set_newkeys: mode 0\ndebug1: rekey after 4294967296 blocks\ndebug3: unable to connect to pipe \\\\.\\pipe\\openssh-ssh-agent, error: 2\ndebug1: pubkey_prepare: ssh_get_authentication_socket: No such file or directory\ndebug2: key: C:\\Users\\Vitaly/.ssh/id_rsa (0000000000000000)\ndebug2: key: C:\\Users\\Vitaly/.ssh/id_dsa (0000000000000000)\ndebug2: key: C:\\Users\\Vitaly/.ssh/id_ecdsa (0000000000000000)\ndebug2: key: C:\\Users\\Vitaly/.ssh/id_ed25519 (0000000000000000)\ndebug3: send packet: type 5\ndebug3: receive packet: type 6\ndebug2: service_accept: ssh-userauth\ndebug1: SSH2_MSG_SERVICE_ACCEPT received\ndebug3: send packet: type 50\ndebug3: receive packet: type 51\ndebug1: Authentications that can continue: gssapi-with-mic,password\ndebug3: start over, passed a different list gssapi-with-mic,password\ndebug3: preferred publickey,keyboard-interactive,password\ndebug3: authmethod_lookup password\ndebug3: remaining preferred: ,keyboard-interactive,password\ndebug3: authmethod_is_enabled password\ndebug1: Next authentication method: password\ndebug3: failed to open file:/dev/tty error:3\ndebug1: read_passphrase: can't open /dev/tty: No such file or directory\nvitaly@localhost's password:\ndebug3: send packet: type 50\ndebug2: we sent a password packet, wait for reply\ndebug3: receive packet: type 2\ndebug3: Received SSH2_MSG_IGNORE\ndebug3: receive packet: type 52\ndebug1: Authentication succeeded (password).\nAuthenticated to localhost ([::1]:22).\ndebug1: Local connections to LOCALHOST:2222 forwarded to remote address 127.0.0.1:5432\ndebug3: channel_setup_fwd_listener_tcpip: type 2 wildcard 0 addr NULL\ndebug3: sock_set_v6only: set socket 4 IPV6_V6ONLY\ndebug1: Local forwarding listening on ::1 port 2222.\ndebug2: fd 4 setting O_NONBLOCK\ndebug3: fd 4 is O_NONBLOCK\ndebug1: channel 0: new [port listener]\ndebug1: Local forwarding listening on 127.0.0.1 port 2222.\ndebug2: fd 5 setting O_NONBLOCK\ndebug3: fd 5 is O_NONBLOCK\ndebug1: channel 1: new [port listener]\ndebug2: fd 3 setting TCP_NODELAY\ndebug1: Entering interactive session.\ndebug1: pledge: network\ndebug3: receive packet: type 2\ndebug3: Received SSH2_MSG_IGNORE\ndebug3: receive packet: type 2\ndebug3: Received SSH2_MSG_IGNORE\ndebug1: Connection to port 2222 forwarding to 127.0.0.1 port 5432 requested.\ndebug2: fd 6 setting TCP_NODELAY\ndebug2: fd 6 setting O_NONBLOCK\ndebug3: fd 6 is O_NONBLOCK\ndebug1: channel 2: new [direct-tcpip]\ndebug3: send packet: type 90\ndebug3: receive packet: type 92\nchannel 2: open failed: unknown channel type:\ndebug2: channel 2: zombie\ndebug2: channel 2: garbage collecting\ndebug1: channel 2: free: direct-tcpip: listening port 2222 for 127.0.0.1 port 5432, connect from 127.0.0.1 port 57017 to 127.0.0.1 port 2222, nchannels 3\ndebug3: channel 2: status: The following connections are open:\ndebug3: receive packet: type 2\ndebug3: Received SSH2_MSG_IGNORE\ndebug3: receive packet: type 2\ndebug3: Received SSH2_MSG_IGNORE\ndebug1: Connection to port 2222 forwarding to 127.0.0.1 port 5432 requested.\ndebug2: fd 6 setting TCP_NODELAY\ndebug2: fd 6 setting O_NONBLOCK\ndebug3: fd 6 is O_NONBLOCK\ndebug1: channel 2: new [direct-tcpip]\ndebug3: send packet: type 90\ndebug3: receive packet: type 92\nchannel 2: open failed: unknown channel type:\ndebug2: channel 2: zombie\ndebug2: channel 2: garbage collecting\ndebug1: channel 2: free: direct-tcpip: listening port 2222 for 127.0.0.1 port 5432, connect from 127.0.0.1 port 57077 to 127.0.0.1 port 2222, nchannels 3\ndebug3: channel 2: status: The following connections are open:\ndebug3: receive packet: type 2\ndebug3: Received SSH2_MSG_IGNORE\ndebug3: send packet: type 1\ndebug1: channel 0: free: port listener, nchannels 2\ndebug3: channel 0: status: The following connections are open:\ndebug1: channel 1: free: port listener, nchannels 1\ndebug3: channel 1: status: The following connections are open:\nTransferred: sent 1736, received 1128 bytes, in 72.0 seconds\nBytes per second: sent 24.1, received 15.7\ndebug1: Exit status -1\n```\nSince you cannot help solving the issue, I will have to close it, since I can't reproduce it, and it requires a set of knowledge that I do not have, about proper SSH tunneling on Windows 10 for PostgreSQL. And I will not have time learning and researching this subject.\n. On second thought, leaving it open, at least for now.. @pennyandsean We still need steps for reproducing this - \n\nHowever, at random intervals, the connection drops on its own.. This library doesn't impose any limitation on transactions, so you can do whatever you like with them.\n\nOther than that, as per Contributing document, you should post your questions on StackOverflow. Here's just for issues reporting.\n. You should never concatenate queries manually. And for table names there are SQL Names.\nAlso, we no longer support pg-promise as ancient as v5.\n. No thank you. The query-formatting engine in this library is way more powerful and flexible.\nAnd when it comes to external SQL files, which is how any serious database module is written, those ES6 template strings become useless.\n. This library is for server-side only.\nThere is no such thing as PostgreSQL JavaScript driver for the client-side, and never will be.. @MichaelOdumosu57 The right direction is not to do it. The reason it can only be a server-side:\n\nIt requires tight integration with the hosting OS, which only Node.js can provide\nMany security reasons\n. @MichaelOdumosu57 There are good reasons why such thing doesn't exist, and nobody is looking for it. Maybe you should start with that in your research :wink:\n\nThere are many confused questions on StackOverflow about it, like this one.. > We have pinned to an exact version for now\nThis is the right approach to using public repositories today. And this is not just my perspective, but other packages, here starting with pg, keep doping the same on minor versions. Practice shows it is too dangerous not to fix versions. This is why all my libraries and projects are all version-fixed, which is what I advise you to do also :wink: Many dependent packages also followed the suit, like Massive.js, they now all use fixed versions only.\n\nPlease can you follow semver.\n\nThis only works in theory. I did follow it for a few years, then gave up. And in case with pg-promise, if I did follow it, it would be over v100.0.0 by now, and I'm not cool with that. I only increment the high version over major breaking changes.\n. Fair enough, I will keep that in mind for the next breaking changes.\nI am also locking this conversation, because pouring your negativity on Twitter to have this support forum bombarded by your Twitter followers is low. Please do not do this again, or I will ban your account here.\n. Nice one! Re-released with this patch.\n. AWS has been the culprit when it comes to connectivity. They are awful in their connection management, and require a lot of extra work.\n. And yet, all the problems related to dropped connections ever reported were around AWS. The library works fine with all other hosting providers.. You fail to initialize the library in accordance with the instructions:\njs\nvar pgp = require('pg-promise')(/* initialization options */);\nIt should be something like this:\n```js\nconst pgp = require('pg-promise')();\nconst DEFAULT = {rawType: true, toPostgres: ()=> 'DEFAULT'};\nlet addTopicsToUser = function (userid, allSubscriptions, callback) {\nconst cs = new pgp.helpers.ColumnSet([\n    {name: 'useridcode', init: ()=> userid},\n    {name: 'strcode', prop: 'item', def: DEFAULT}\n    ], {table: 'subscriptions'});\nconst query = ()=> db.helpers.insert(allSubscriptions.subscriptions, cs);\n\ndb.none(query)\n    .then(data => {\n        logThisDev(\"Success inserting, %j\", data);\n        return callback(null, {success: true});\n    })\n    .catch(error => {\n        logThisDev(\"Success inserting, %j\", data);\n        return callback(error);\n    });\n\n}\n``. So what do you expect to happen when youritemisnull`?\nAnd I do not see your actual input data, to say whether the mapping is correct.\nAlso, Failing row contains is a server-side error, not one in this library.\n. I understand that | 0 is indeed the shorter version, but in this case I'd rather use parseInt in documentation, not to confuse developers with some strange-looking syntax :smile:. ",
    "emilioplatzer": "Thanks for the fast answer. \nI apologize for my poor English (I'm Spanish skpeaker). \nFor pg-promise I can wait the next version (0.2). I'm very interested in this module. I thought that have something like queryResult is a great feature. \nI have some skills in testing and coverage. I start writing Node modules recently, the last ones with 100% test coverage. \nI'm using Mocha, Travis-CI and coveralls.io \nThanks\nEmilio\n. if you let me make a suggestion: \n- you can evaluate to change the C-style form of passing options (queryResult.none | queryResult.many) with JS-object-style form {none:true, manu:true} \n- you can evaluate to prepare the module to have full functionality in the future. In the mean time thats means:\n  - you should permit pass parameters to the query function:\njs\n    query('select country from countrys where population>$1' , [10000]).then(...\n. I'm happy with your quick responses. Thanks. \nI like the feature of control the number of records in the result (none, one and many). Buy I need the parameter of client.query that pg have (at the moment I'm using parameters in my queries). \nI'll try with pg-bluebird. May be I add a filter that controls de recordCount. Something like: \njs\nclient.query(\"select $1::integer\", [1]).then(pgControls.OnlyOne).then(function(data){\n  // do something usefull with data!\n});\n. It's a pqlib feature. It's referred in pg readme.md in client.query example. \nTry this:\njs\nvar pg = new PGBlueBird();\npg.connect(config.db).then(function(conn){\n    console.log('connected!',connection);\n    return conn.client.query(\"SELECT 2 + $1\", [5]);\n}).then(function(data){\n    console.log('data recived',data);\n});\n. Hi. I'm sorry about not send replies in the weekend. \nI think that changing the order of parameters of query (sql, values, mask) is a good thing. Your module names pg-promise, and like fs-promise should have the same functionality of pg changing the callback with a returned promise. And like fs-promise will add new features. \nI think you are in the right way. \nA year ago I write a module https://github.com/emilioplatzer/motor-db that haves similar features. The main problem with it is that the test are not complete. Others problems:\n- The new features, the documentation and the test are in Spanish (I'm from Argentine)\n- Thera are a mix of features of diferent levels in the same level: \n  - pg functions returning promises (ES6-promises)\n  - multi database (pg, sqlite and mysql)\n  - special functions that control counts like yours none, one, many (in my module: ejecutar, dato or fila, todo)\n  - logical nested transactions\n  - named parameters (like \"SELECT * FROM t1 WHERE year = :year\", {year: 2014})\n  - group parameters (like \"insert into t1 (???FIELDS) values (???VALUES)\", {year: 2014, alpha:1, betha:2} insted of write \"insert into t1(year, alpha, betha) values ($1, $2, $3)\", [2014, 1, 2])\nWhat I learn in that moment?\n- I need solid tests. Otherwise, after a while, I begin to distrust my own code\n- I need to split the features in several levels and use more standars \n- I need to colaborate with others\nWhat can I suggest to you\n- If you need you can add a method ctx.oneValue(\"SELECT one_field FROM ... \") that returns a value insted of a row. \n- Travis-CI have a way to test in postgres databases (I don't knew this a year ago). Brings to your code a empty database for each test build. \n- If you want you can use Mocks to test without a real db. \n- If you want to test with a real db and want to know if the calls to certain pg modue are like you expect you can use https://github.com/emilioplatzer/expect-called\nI think that I would to enhance pg-bluebird by adding some like one, many, all, value, etc. But I only begin if I write the test first. If I do it I notice you. \nRegards\n. ",
    "lalitkapoor": "What does the done method have to do with the promise library? That's just the function that's returned when opening a connection. Sorry, I don't quite follow.\n. Oh, I see. Bluebird does have a done as well https://github.com/petkaantonov/bluebird/blob/master/API.md#donefunction-fulfilledhandler--function-rejectedhandler----void\n. Also, you can see more about speed and memory usage across various implementaitons and how they compare here - https://github.com/petkaantonov/bluebird/tree/master/benchmark\n. ",
    "jcristovao": "No problem! Thanks for such a cool library, I'm liking it so far! Cheers\n2015-03-18 16:34 GMT+00:00 Vitaly Tomilov notifications@github.com:\n\nThank you!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/pull/7#issuecomment-83049112.\n. Nevermind... it is as easy as going through the array backwards:\nfor (var i = values.length - 1 ; i >= 0; i--) {\n\nDo you want me to send a pull request nevertheless?\n. Eh eh, thanks :) It might come handy indeed!\n. Exactly! That's it.\nJust to be clear, I'm talking about assigning an array to a single $1 value.\n. >  you should be able to do an easy work-around by converting such an array into a text string yourself.\nOk, fair enough, I shall give it a try! Thanks!\n. No, I think that's perfectly fine - one must know the limitations of postgres arrays.\nThanks!\n. Yes it is, tks! :)\n. Thanks! It works perfectly!\nYes, I am aware of it, I'm considering for special cases to have two separate pgp instances, one 'camelCasing' for non-performance-critical stuff, and another one for critical stuff.\n. Alright! I'll let you know as soon as I publish it.\nThanks\n. Yes, I am quite aware it is missing some stuff, that's part of the reason I am open sourcing it - I don't need those features right now, but perhaps someone else might want to help :)\nRegarding the 3.0.5, yes, you are right, and I had forgot about that... oh well, perhaps next week I'll have some time to take a new look at it!\n. Not really, I think it will still take me somedays till I look at this again :(\n. So, exactly as I did above, right?\nThanks!\n. Ok, just updated the example :)\n. Ah, that... Just non standard naming on my part ;)\nconst pgpLib    = require('pg-promise');\n. Did you install with --global and then included the index?\n/// <reference path='../typings/index.d.ts' />\n. Thanks :)\n. ",
    "joeandaverde": "There's a lot to be desired in the regular expression that's breaking up the SQL statement. Currently it's simply using '.split(/(:\\w+)/)' which would work in the cases you mentioned. \n``` javascript\n\n'SELECT * FROM users where id = :id and idiom = :idiom'.split(/(:\\w+)/)\n[ 'SELECT * FROM users where id = ',\n  ':id',\n  ' and idiom = ',\n  ':idiom',\n  '' ]\n```\n\nWhere it would fail is if you wanted to use the full range of valid Ecmascript variable names. This is because Ecmascript allows for unicode variable names (see: https://mathiasbynens.be/notes/javascript-identifiers-es6). Another possible issue is that object keys can be any arbitrary string. This would also fail to match :\\w+.\nI'm currently using [A-Za-z0-9_]+ (aka \\w+ see: http://www.w3schools.com/jsref/jsref_regexp_wordchar.asp) which handles most sane variable names.\n. I'm also up to find a more resilient way of representing variable insertion.\n. Another issue with my solution is that it doesn't account for postgres type casts. However, this can probably be fixed with a negative look behind for :. However, that's not supported in JavaScript's RegExp.\nsql\nselect 1::int\n. In regards to your previous message about enumerating object properties:\nhttps://github.com/joeandaverde/tinypg/blob/8cab154a9652fe15ba087688171ebc3f5afa02d3/index.js#L60-L62\nThis particular implementation pulls a mapping of $1 to variable name. It iterates those instead of the object parameter to extract values into an array.\n. It would be ideal not to collide with ES6 template strings to avoid needing to escape:\nhttps://babeljs.io/docs/learn-es6/#template-strings\n. I originally started with regex exec. When I got to replacement's I found it was easier using the split method on string (using a capture group in order to not lose variable name). This made it easier to map Postgres vars to property names. \n. Can't edit from phone or link to line. But you can see an example here in the function parseSql: https://github.com/joeandaverde/tinypg/blob/master/index.js \n. I think we were talking about something completely different. What you implemented was formatted strings. My original request was to implement a feature to allow for parameterized SQL queries using named parameters. In order to accomplish this safely (not using string interpolation as this opens a gaping hole for SQL injection)  there needs to be a way to map $1, $2 etc to array elements.\nIt's worth mentioning again that the implementation as is condones unsanitized SQL queries.\n. I'm not suggesting named variables IN the final SQL query.\njavascript\ndb.query('SELECT * FROM users WHERE name = :name and age > :min_age', { name: 'joe', min_age: 30 })\nwould get transformed by your library to be\njavascript\ndb.query('SELECT * FROM users WHERE name = $1 and age > $2', ['joe', 30])\nThis type of query can be sanitized by the postgres database engine whereas one which allows for string substitution would not. This is bad:\njavascript\ndb.query(\"SELECT * FROM users WHERE name = 'joe' and age > 30\")\n. The parameters passed to Postgres can be injected into the query in a safe manor for example:\njavascript\nas.format('SELECT * FROM users WHERE name = ${name} and age > ${min_age}', {name: '\\'1\\'; DROP TABLE users;--', min_age: 30 })\nwould become \njavascript\n'SELECT * FROM users WHERE name = \\'1\\'; DROP TABLE users;-- and age > 30'\nIf instead those parameters are passed to the Postgres engine ($1, $2 => ['Joe', 30]), it can do the appropriate escaping of malicious characters.\n. Here's an example of the Postgres log of similar query.\nLOG:  statement: ROLLBACK\nLOG:  statement: BEGIN\nLOG:  execute <unnamed>: INSERT INTO bottle.inbound_message (source, raw_text)\n    VALUES ($1, $2)\n    RETURNING *\nDETAIL:  parameters: $1 = 'xxx', $2 = 'TEST MESSAGE'\n. It does. That's what you're seeing above. Directly from the Postgres logs.\n. http://www.postgresql.org/docs/9.2/static/plpgsql-statements.html\nscroll down half way (39.5.4. Executing Dynamic Commands)\n. ",
    "chrisvariety": "Are you sure the alternate syntax array[] works with strings?\nQuick POC:\n``` js\npgp = require('pg-promise')();\u2028\u2028\ndb = pgp('postgresql://localhost/whatever');\u2028\ndb.none('create temporary table foobar ( test text[])').then(function() {\u2028\n  return db.none('insert into foobar values ($1)', pgp.as.array(['biz']));\u2028\n}).then(function() {\n  console.log('done', arguments);\n}).catch(function() {\u2028\n  console.log('blew up', arguments);\u2028\n});\n```\nFor me, this logs:\n[error: malformed array literal: \"array['biz']\"] Array value must start with \"{\" or dimension information.\npg-promise 1.4.0, psql 9.4.1, iojs 2.0.2\naccording to this comment PostgreSQL will not accept ARRAY[] syntax in a prepared insert statement but I wasn't able to find any more information.\nChanging the definition of formatArray to the below seems to work on the above test case, what cases were you seeing the {} syntax not work?\n``` js\nfunction formatArray(arr) {\n    function loop(a) {\n        return '{' + a.map(function (v) {\n                return (v instanceof Array) ? loop(v) : formatValue(v);\n            }).join(',') + '}';\n    }\nreturn loop(arr);\n\n}\n```\n. @vitaly-t Got it! thanks so much for your quick reply-- \n. ",
    "ForbesLindesay": "Are you using domains?\n. domains are a feature of node.js that allows you to handle certain errors even when they are thrown into the global context.  The latest promise update removed support for using domains with promises, so it could cause your bug.  In general, domains are not recommended as they tend to leave applications in a really inconsistent state.\nThe only other breaking change I'm aware of is that .then is no-longer automatically bound to the promise in the newest version.  e.g.\njs\nvar foo = new Promise(function (resolve) {\n  setTimeout(function () {\n    resolve(42);\n  }, 100);\n});\nvar then = foo.then;\nthen(function (result) {// this throws an error, because `then` is not bound to `foo`.\n  console.log(result);\n});\n. That is definitely interesting.  I'd love to see an example that reproduces the error, then I could try and fix it, but without that I really can't do much.  Promise passes the full Promises/A+ test suite, as well as a number of its own tests, some of which test some pretty stringent edge cases.\nMy guess is that @parashar is depending on some unspecified ordering/race condition somewhere, and that the two different promises fire the handlers in a subtly different order.\n. @parashar The problem is that .then(null, function (err) { cb(reason, null); }) is always going to result in a reference error on reason whenever any other error is thrown.  If you are using then/promise, you should be able to just use the nodeify api to interface with promises.  So I would re-write your code as:\njs\nexports.query = function (query, q_params, cb) {\n    var params;\n    if (arguments.length == 3) {\n        params = q_params;\n    }\n    if (arguments.length == 2) {\n        cb = q_params;\n    }\n    db.connect()\n        .then(function (conn) {\n            return conn.query(query, params);\n        })\n        .nodeify(cb);\n};\nThis will still leak db connections as @vitaly-t says.  If you want to learn how to use promises properly, and why they are important, try reading https://www.promisejs.org/ (or alternatively watch https://www.youtube.com/watch?v=qbKWsbJ76-s if you'd prefer it given as a talk).\n. This answer isn't quite right.  It will actually be fine until 2\u2075\u00b3 - 1 because JavaScript number become doubles once they exceed the 32-bit range, giving them 53 bits.. This is the second time I've seen a semver break affecting my production code in 7 years of working with node.js on a daily basis. The problem results from a tiny number of package maintainers ignoring semver, which usually comes from the belief that integers are somehow in short supply.\nIt is possible that by now you might be on version 100, but more likely it would be something like version 20. Even if you were on version 100, which would be fine, it would take at least 40 years to get to version 1000, and well over 400 years to get to version 10000.\nThe cost of a larger version number for the major version is essentially 0. How many people have chosen not to use jest just because their major version number is now 24? The cost of not being able to tell when a version will be a breaking change is incalculable.. ",
    "kcparashar": "@vitaly-t It is indeed an issue with the Promise module (I believe thru the asap module). I tried Bluebird and an older version of Promise and they both work. Thanks for your help and for forwarding the issue on to the Promise developer.\n@ForbesLindesay, sorry I am not sure what you mean by \"using domains\". Could you please elaborate a little bit? \n. @vitaly-t, @ForbesLindesay \nThanks for trying to resolve this. Since I am quite new to using promises in general, I have used very sparingly; in fact this is really the only spot where there is any sort of complexity:\nJavascript\nexports.query = function (query, q_params, cb) {\n    var params;\n    if (arguments.length == 3) {\n        params = q_params;\n    }\n    if (arguments.length == 2) {\n        cb = q_params;\n    }\n    db.connect()\n        .then(function (conn) {\n            return conn.query(query, params);\n        },\n        function (err) {\n            cb(reason, null);\n        })\n        .then(function (data) {\n            cb(null, data);\n        },\n        function (err) {\n            cb(err, null);\n        })\n        .done();\n};\nI don't think I know enough about this subject to aptly contribute to this discussion. But perhaps if their are particular queries I may be able to answer let me know!\n. @ForbesLindesay, @vitaly-t, Thank you! You all are awesome!\n. @andrewbrinker\n. ",
    "gniquil": "!awesome! Thanks for response.\n. aweomse! :+1: \n. ",
    "mdvorak": "There are currently no unit tests for this change. If you will be considering merging this, I'll write them (or you can of course).\n. You're right, since rows.length should always match rowCount (if there are data actually), providing way for a rowCount to be accessed should be enough.\n. MyBatis solves this by having methods like selectOne, selectList, selectMap for data retrieval\nand delete, update, insert for modifications. It is explicit, but i thing actually method update with rowCount as result would suffice.\nMy original solution provides you access to full result object, which allows future users to get any possible future properties available there :) Also might allow easier transition from pure pg/other libraries to this one. Therefore, I would still include it.\n. My PR wasn't breaking, it just added new queryResult. But having .update(...).then(function(rowCount) {}); would serve as well. I can't think of case where you would need both data and rowCount.\nIf you don't want to have generic passthru method, this is my suggestion.\n. Well, that still produces rows array, therefore you have its length property.\nBut I was also originally thinking about raw as queryResult, so rawQuery or queryRaw sounds fine to me. queryRaw fits me better, since you are executing query and consuming raw result. While rawQuery would make me think i'm bypassing pg-promise query formatting (pgFormatting=true).\n. Thats great news, thanks!\n. ",
    "miraage": "https://github.com/vitaly-t/pg-promise/wiki/Learn-by-Example#affected-rows\nbroken. could you please update wiki?\n. ",
    "paulovieira": "\nCalling exec implies none of that, so cannot be considered a semantic alias.\n\nMy idea was more in the fact that \"exec\" is a verb, which is the natural choice for methods. By \"db.exec\" it is implict (for me) that we are executing a pg function. But this is a matter of opinion in the naming, of course.\nHere's an analogy: if the library had a sugar method for select * from users, a natural choice for me would be db.readAll(\"users\"), not db.table(\"users\") (or even db.all(\"users\"))\n\njust add it to your application; event extend allows you to do that easily:\n\nThanks for the tip! Will do it.\nI will try your other library. Looks interesting!\n. @vitaly-t I just tried pg-monitor. It's a very cool addition to pg-promise! \n. Consider adding it to pg-promise directly. It would definitely add value. Maybe something like:\njavascript\nvar pgp = pgpLib({\n    monitor: true  // call monitor.attach with the default options (monitor all events)\n});\njavascript\nvar pgp = pgpLib({\n    monitor: [\"query\", \"error\"]  // monitor only some events (with the default message)\n});\njavascript\nvar pgp = pgpLib({\n    monitor: {  // monitor some events using a custom message\n        query: function (e) {\n            /* do some of your own processing, if needed */\n            monitor.query(e); // monitor the event;\n        },\n        error: function (err, e) {\n            /* do some of your own processing, if needed */\n            monitor.error(err, e); // monitor the event;\n        }\n    }\n});\n. ",
    "vastus": "That's why I asked b/c that example didn't work for me. Still hangs that 30s or so.\n. When using promises on cmd line the connection stays open (what I think) and doesn't close before the set time out.\nSo this https://gist.github.com/vastus/02e27b9b954d08d4d1e3 works nicely and closes the connection when the query is run. But when using pg-promise it hangs til the time out.\n. Yeah that's exactly what I was looking for, thanks :) Didn't just catch it in the docs. Thanks for this btw, I started doing my own little lib but luckily found yours.\n. ",
    "devspacenine": "Sorry, should have clarified. I was in a rush when posting. When executing a function that returns multiple rows with arbitrary columns (not based on a table), you need to be able to define column aliases. For Example, using this test function:\nsql\nCREATE OR REPLACE FUNCTION test()\n     RETURNS SETOF record AS\n$func$\n     VALUES (1, 2), (3, 4);  -- 2 rows with 2 integer columns\n$func$ LANGUAGE sql;\nCalling it like this with pg-promise:\njavascript\ndb.func('test', []) \n    .then(function(data) {\n        console.log('Data: ', data);\n    }, function (err) {\n        console.log('Error: ', err);\n    });\nWill execute:\nsql\nSELECT * FROM test()\nPostgres will respond with ERROR:  a column definition list is required for functions returning \"record\"\nTo successfully execute the function you would select with column aliases:\nsql\nSELECT * FROM test() AS f(a int, b int)\n. What is the relevent issue on the node-postgres project?. ",
    "Aysnine": "I also encountered this problem.\nsome data:\nsql\ncreate table foo (\n    my_array text[]\n);\ninsert into foo (my_array) values (ARRAY['hello', 'world']::text[]);\nif select like: \nsql\nselect my_array from foo; -- output string: '{hello,world}' \nbut try this:\nsql\nselect my_array::text[] from foo; -- output array: ['hello', world]\nsimple and nice :). ",
    "knexp": "Thanks @vitaly-t .\n. ",
    "woolfi182": "thanks @vitaly-t, it work for me. I tried to set this parameter in \"cn\" option but it not worked.\n. ",
    "pkoretic": "var test = \"test\" \nconsole.log(`this is a ${test}`)\nthis is one of the features of es6 template strings, so you can see why using 'your' syntax makes a problem, since template strings expect that variable to exists, but you use the same syntax and actually bind the variable later.\nSo when you declare your sql using template strings and your named parameters es6 template strings fail because there is no variable by that name that es6 syntax expects.\n. @vitaly-t yeah, changed it in the meantime ;)\n. @vitaly-t I don't think that is relevant. You shouldn't use standard language keywords for custom functionality which obviously collides. This is why sequelize for example use '?' and ':' for position and named binds.\n. @vitaly-t  template strings are in the release candidate stage implemented in all major software http://kangax.github.io/compat-table/es6/.\nIt's better to avoid it sooner than later since this is not backward compatible change for your library.\n. Ok, in general whatever suits you best. I'm for one that's fastest to parse so I'll do some tests :) Thanks.\n. Ok, $(var) of maybe even ?{var}. Seems easiest to write (on english keyboard at least)\n. Ok, looks great.\n. Looks great! We will then switch to using it from monday if released.\n. ",
    "waynehoover": "Yes, thanks.\n. ",
    "nevir": "Nevermind, misread the docs.\n. ",
    "macprog-guy": "Apologies. I had not noticed that I was running an older version. Thanks.\n. ",
    "ZeusIV": "many apologies vitally\nI didn't test it because I switched to pgFormatting: true when I was getting problems and came back to see if it had been fixed. not fluent in regex it looked like a fix for this problem and a bit finite :)\nwhy an object?\nwell pg accepts an object and if you promisify pg you could pass it the object, add the .then, continue coding. if later you want to switch to pg-promise it is a lot more coding. a similar calling interface would make it easier. that's all.\nsorry to go on about squel but it returns this object because a few sql interfaces use it like that. it would be nice to change database engine without having to rewrite gobs of code. however the extend framework looks like it might do the job :)\nRegards Zeus\np.s. I dunno how to like, unless this is it :+1: \n. ",
    "dothebart": "yes, the new version indeed works without issues. You've been very busy doing new releases ;-)\n. ",
    "Maximization": "Made a typo there, shouldn't be a function. Original post edited.\n. None as a matter of fact. The rows are inserted fine in the db but the transaction as a whole never resolves. Neither of these execute\n.then(function(data) {\n    // Success, do something with data...\n    return res.status(200).json(data);\n  }, function(reason) {\n    // Error\n    return res.json(reason);\n  });\nDoes it have anything to do with the fact that I have those two queries inside the first one?\n. You're right, the success promise does execute.\nHowever, I get undefined for console.log(data). That's preventing returning a success status back to the client. I was manipulating the data which I didn't have. Do you know how I could get the result from the first query passed all the way to the end? \n. It works indeed, thanks a lot.\nYou're right, in the end it was a promise related issue rather than anything with this library. Thank you for you quick response. Keep up the good work.\n. @vitaly-t Thank you for the heads up. I read through the discussion and it can indeed be quite dangerous in certain cases. I've updated to latest and started using batch in all new transactions. Will refactor old transactions asap too.\nKeep up with the good work!\n. I double checked that the library was initialised with Bluebird. I hadn't made any changes there. I did a console.log of one of the query methods both using Bluebird 2.x and 3.x. Bluebird 3.x would return an unfulfilled Promise and 2.x would return an empty object.\nI looked at my Promise.all code and it seemed fine. Nothing changed there either. Promise.resolve() would error too as in the following example:\n```\n40 db.result(query, values).then(function(result) {\n41     return Promise.resolve([]);\n42 }).spread(function(result, replies) {\n43     return res.status(200).end();\n44 }).catch(next);\nTypeError: undefined is not a function\n        at result (/Users/maxim/Code/backend-v2/server/controllers/news.controller.js:42:4)\n```\nI use pg-monitor and the error is returned before the query is executed. It seems that db.result immediately returns an empty object which gets populated with the resolved value upon fulfillment.\n. After many hours of debugging I was able to trace the cause of this strange behaviour in my app. Turned out it had nothing to do with Bluebird. I was initialising the pgp library a second time in one of my code files. Here's a reproducible snippet:\n```\nvar Promise = require('bluebird'); // v. 2.10.2\nvar pgp = require('pg-promise')({ // v. 2.8.5\n    promiseLib: Promise\n});\nvar pgp2 = require('pg-promise')();\nvar db = pgp('postgres://mystyl@localhost/mystyl');\ndb.result('SELECT 123 AS value').then(function(data) {\n  return Promise.all([ data.rows[0].value ]);\n}).spread(function(value) {\n  console.log('value:', value);\n});\n```\nThe reason I was initialising it twice is because I needed one of the conversion helpers, pgp.as.csv. I didn't want to export the pgp library on top of the db connection in my db.js file. So instead of module.exports = db having to do something like this:\nmodule.exports = {\n    db: db,\n    pgp: pgp\n};\nThat would mean I'll have to change the way I require my db connection in all places of the app. Then I found out that I can have access to the conversion helpers simply by requiring pg-promise, without initialising it.\nAnother thing learned! Glad I was able to solve the issue. Strangely enough this didn't give me a problem with pg-promise 1.x.\n. Thank you for your help. I'll mark the issue as closed.\n. @sefo user is a reserved word in postgres. It's an alias for current_user function so when you do select * from user; internally it translates to select current_user; which, as you may have guessed, it returns the DB user used in the connection.\nTry to avoid reserved names when naming your tables, columns, functions or anything else. users or user_accounts might be a better choice here. \n. ",
    "BeeDi": "I would like to thank you both, @vitaly-t for this great library & @Maximization for his question. \nI have found in your conversation very good answers to my own questions and I just found the learn by example section of the Doc. Indeed a must read \ud83d\udc4d\n. ",
    "wprudencio": "End if I have a loop?\n```javascript\nfor (let city in fileContent.Sheets) {\n    if (fileContent.Sheets.hasOwnProperty(city)) { // This kind of check must be executed as good practice\n        let citySheet = fileContent.Sheets[city];\n    db.one(\"insert into common_city(name, state_id, time_zone_id) values($1, $2, $3) returning id\", [city, 5, 2])\n        .then(function(data) {\n            console.log(`--> Created city \"${city}\";`);\n\n            for (let row in citySheet) {\n                if (citySheet.hasOwnProperty(row) && citySheet[row].hasOwnProperty(\"v\") && citySheet[row].v !== city) {\n                    let neighborhood = citySheet[row].v;\n\n                    db.none(\"insert into common_neighborhood(city_id, name) values($1, $2)\", [data.id, neighborhood])\n                        .then(function() {\n                            console.log(`----> Created neighborhood \"${neighborhood}\" from city \"${city}\";`);\n                        })\n                        .catch(function(error) {\n                            console.log(`Error creating \"${neighborhood}\" from \"${city}\": ${error.message || error};`);\n                        });\n                }\n            }\n        })\n        .catch(function(error) {\n            console.log(`Error creating \"${city}\": ${error.message || error};`);\n        });\n}\n\n}\n```\nShould I put 'db.tx(function (t)' inside loop?\n. ",
    "ghost": "You are spot on, using the correct connection context and resolving with the data instead of simply returning it is exactly what I needed to do. The transaction behaves as expected now.\nPerhaps you'll consider adding an example of a transaction like this to the documentation. It seems like a relatively common and simple use case that emphasizes key ideas (such as using the proper connection context) :)\nAnyway, I appreciate your prompt help, thank you!\n. ",
    "andrei-cocorean": "In some cases you might want the boost in performance from the native bindings and it's easier to make the switch if pg-promise supports it.\n. Thanks for looking into this. I'll give it a try when I have some time.\n. ",
    "clicktravel-pavel": "Thanks for the advice!\nUsing var date = new Date(Date.UTC(year, month, day)); works correctly.\nThanks for the support!\n. ",
    "jllodra": "I should RTFM more carefully.\nFinally figured it out, thanks for answering.\n. ",
    "RichardJECooke": "It's weird that you have to call pgp() twice. I just had the same issue. There should probably be a line in the manual clarifying it a little so others don't struggle too.\n. ",
    "tjmcewan": "@vitaly-t because 'options' is in comments I didn't \"see\" the extra function call.  I don't know if it's the same reason for the others.\n. I'm not commenting on the api, just the way I read the docs. :)\nMaybe pass in an actual options object with some defaults instead of the\ncommented text? Just to make it clearer that it's not something that can be\nignored.\nThanks!\nOn Wed, 20 Jan 2016 at 15:56, Vitaly Tomilov notifications@github.com\nwrote:\n\n@tjmcewan https://github.com/tjmcewan Thank you! I wasn't sure how to\nupdate the docs to make it more clear. Plus all the examples show the same\nthing.\nI can't say this is ideal, no, but this is the way it has been implemented\nfrom day one, so for the sake of compatibility... The bottom line, once you\nfigure that one out once, you won't make the same mistake again ;)\nI will consider an update, should I start on version 3.0.0\nCheers!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/39#issuecomment-173087827.\n. \n",
    "kaizokuace": "I also ran into this problem, this issue thread helped solve it for me.  What made it hard for me to notice was that I am using ES6 import statements instead of require. The docs go from the require description to connection settings. \n. ",
    "oliversalzburg": "Wow, I was on 1.8.3! I just upgraded and will report back ;)\n. Well, no issues so far. I'll open another ticket if necessary :) Cheers!\n. I ran into this issue again, even after the upgrade. I'll read you replies and get back into the details tomorrow.\n. Well, in general, this happens when some query doesn't go as planned during a transaction (transaction.one() returned multiple rows). I'm still not quite sure why it happens, as it doesn't happen everytime there is an error.\nThe full stack is:\nc:\\Users\\Oliver\\core\\node_modules\\pg-promise\\lib\\index.js:619\n                    client: ctx.db.client,\n                                  ^\nTypeError: Cannot read property 'client' of null\n    at notifyReject (c:\\Users\\Oliver\\core\\node_modules\\pg-promise\\lib\\index.js:619:35)\n    at null.callback (c:\\Users\\Oliver\\core\\node_modules\\pg-promise\\lib\\index.js:605:22)\n    at Query.handleError (c:\\Users\\Oliver\\core\\node_modules\\pg\\lib\\query.js:106:17)\n    at null.<anonymous> (c:\\Users\\Oliver\\core\\node_modules\\pg\\lib\\client.js:171:26)\n    at emit (events.js:107:17)\n    at Socket.<anonymous> (c:\\Users\\Oliver\\core\\node_modules\\pg\\lib\\connection.js:107:12)\n    at Socket.emit (events.js:107:17)\n    at readableAddChunk (_stream_readable.js:163:16)\n    at Socket.Readable.push (_stream_readable.js:126:10)\n    at TCP.onread (net.js:538:20)\nI'm currently debugging an issue in our code where a large amount of requests to the API generate race conditions with our database transactions. The failures usually end in the error above being logged and Node exiting.\nDue to the nature of this issue, requiring large amounts of correctly timed queries, and the asynchronicity of the code, it is really hard to see what the underlying cause is.\nIf you have any suggestions regarding what I should look into, I'd be happy to investigate.\n. Hey @vitaly-t. Sorry for not providing better feedback earlier. It was quite the busy weekend.\nI just installed 1.9.8 and am still seeing the same error. I'm using bluebird 2.9.34.\nRegarding the transaction code. The code is rather long and I would prefer not to share it publicly. I could send you a copy via email if you like.\n. It seems to me like the rejection handler from $connect (line 483) is first invoked, then the callback for query (line 568) is.\nI'll send you the code shortly.\n. Do you think there is a possible relation to https://github.com/brianc/node-postgres/issues/746? Because I've been banging into that left and right. First an error event is handled, but then an identical message is received on the data stream and handled as the same error again.\n. Good point. I'll look into reducing the number of transactions.\nThe point where the code is failing originates from the query in updatePersonTags. But this could be coincidental.\nIn my test scenario, I'm creating 100 person records. When I run the necessary queries in succession, everything completes successfully without errors.  \nWhen I issue the queries to the API as fast as possible, many transactions are being generated and sent to Postgres at the same time. The client pool will fill up with connections and a lot of failures start to happen due to table locking conflicts. With the current state of the code, this is expected.\nHowever, at some point the access to ctx.db.client happens and db is null, even though the same line of code was hit many times before and ctx.db wasn't null :\\\nI'll have a look at pg-monitor. Thanks for the suggestion.\n. Ah, I see the point of pg-monitor. We're already logging excessively (In fact, I'm pretty sure our postgres logging is inspired by pg-monitor. The format looks very similar) and I got all the \"details\" at hand. But the log for a single test run is close to 8k lines. It would probably help if I tagged all transactions with a unique ID, but even then it's a lot of log.\n. I'm pretty sure this problem is not related to the actual queries. It seems like a lot of things are going wrong once we're hitting the connection limit of postgres or the connection pool size limit of node-postgres (https://github.com/coopernurse/node-pool/issues/99).\nMaybe what I'm seeing in pg-promise is just a side-effect of a larger underlying issue :P\n. Nope, not related to the issue mentioned above. Working on a different issue at home, I run into this error much earlier, given invalid input for our application. I run a single createWith for a person and end up with the uncaught error.\nI took the liberty of emailing you the log, which shows the issued queries. The API failure is expected, the TypeError obviously is not :(\nGiven that this is a much simpler case than before, I'll try to see if the debugger brings up any new information.\n. I've reduced the code that creates the problem to this:\n``` js\nreturn self.db.tx( function tx( transaction ) {\n    return Promise.all( [ foo(), fails() ] );\nfunction foo() {\n    return succeeds()\n        .then( succeeds )\n        .then( fails );\n\n    function succeeds() {\n        return transaction.one( \"SELECT 1 AS id\" );\n    }\n}\n\nfunction fails() {\n    return transaction.one( \"SELECT id FROM person WHERE(FALSE)\" );\n}\n\n} );\n```\nInterestingly, if I specify an invalid table name instead of person, I get a different, handled, error and execution continues just fine.\n. At home, I replaced the content of createWith with the code I posted. That method is the one against I'm primarily running the tests. So the SET TRANSACTION wasn't used in my test case last night (either way, thanks for catching that loose call :+1:).\nOf course, that method is called from our API, so there is more stuff around it that could explain why I get different results. I was able to reproduce the issue with that code here at work as well, but it's a lot harder. So I was also able to check how 1.9.9 impacts the situation and I'm happy to say that the problem is gone and I now get the Loose request outside a connection that has expired. error.\nI'm going to try to find out why this happens. What's confusing me the most, is that the problem only happens on my work desktop when I put a lot of load on the system. When I run the same code at home, the first query produced the issue instantly.\nThanks for your help! I'll report back when I have any new insights.\n. So, looking at it some more, it kinda makes sense, but I still don't see how this should be prevented (except for restructuring our queries, which is what I'm going to do).\nLooking at the log output from the sample:\n```\n2015-09-08 10:08:32.469 [DEBUG ] (       postgres.js) tx(tx)/start\n2015-09-08 10:08:32.469 [DEBUG ] (       postgres.js) tx(tx): begin\n2015-09-08 10:08:32.470 [DEBUG ] (       postgres.js) tx(tx): SELECT 1 AS id\n2015-09-08 10:08:32.470 [DEBUG ] (       postgres.js) tx(tx): SELECT id FROM person WHERE(FALSE)\n2015-09-08 10:08:32.470 [DEBUG ] (       postgres.js) tx(tx): SELECT 1 AS id\n2015-09-08 10:08:32.471 [ERROR ] (       postgres.js) tx(tx)/error on database transaction: No data returned from the query.\n2015-09-08 10:08:32.471 [INFO  ] (       postgres.js)   SELECT id FROM person WHERE(FALSE)\n2015-09-08 10:08:32.477 [DEBUG ] (       postgres.js) tx(tx): rollback\n2015-09-08 10:08:32.478 [DEBUG ] (       postgres.js) tx(tx): SELECT id FROM person WHERE(FALSE)\n2015-09-08 10:08:32.478 [DEBUG ] (       postgres.js) tx(tx)/endduration: 9, success: false\n2015-09-08 10:08:32.481 [DEBUG ] (           pg:pool) timeout: 1441699742481 verbose\n2015-09-08 10:08:32.481 [DEBUG ] (           pg:pool) dispense() clients=0 available=2 info\n2015-09-08 10:08:32.481 [DEBUG ] (       postgres.js) disconnect(postgres@fairmanager)\n2015-09-08 10:08:32.489 [INFO  ] (     apiHandler.js) api(create person)/fail\n2015-09-08 10:08:32.489 [ERROR ] (     apiHandler.js) No data returned from the query.\n2015-09-08 10:08:32.490 [DEBUG ] (              HTTP) POST /api/people 500 42.691 ms - 34\n2015-09-08 10:08:32.490 [ERROR ] (       postgres.js) tx(tx)/error on database transaction: Loose request outside a connection that has expired.\n2015-09-08 10:08:32.490 [INFO  ] (       postgres.js)   SELECT id FROM person WHERE(FALSE)\n```\nI see that the query, which is being detected as a loose call, is SELECT id FROM person WHERE(FALSE). But I don't see how the sample code resulted in that. The promise chain looks perfectly valid to me. Except that there are probably multiple chains, due to the use of Promise.all and only the top most promise ultimately rejects when the nested chains completed.\nSo I'm asking myself if this can actually be prevented and if it needs to be prevented. Especially since it seems like the client reference was still valid when the query was issued, but only became invalid when it's time to handle the resulting error.\nEither way, the core issue was the uncaught TypeError, which is now gone. So I can focus on the other issues related to this code :)\n. Well, maybe I'm misinterpreting how the code actually works, but this is how it plays out in my mind.\nThe rollback is issued at line 795, but the state of the transaction/client is not changed at that point. The success is determined in another then handler. This is handled asynchronously again. So there is an opportunity for other code to run. Only after the then handler, will the transaction promise be rejected and future calls would fail.\n. Humm, you're probably right. It fits the observed error.\nI guess the promise that causes the transaction to be rejected, is the one that is constructed with Promise.all. This will be rejected as soon as any contained promise rejects. But that doesn't stop the other contained promises to still be executed.\nI adjusted my sample code:\n``` js\nreturn self.db.tx( function tx( transaction ) {\n    return Promise.all( [ foo(), failsFirst() ] )\n        .catch( function( error ) {\n            log.debug( \"Promise.all rejected\" );\n            throw error;\n        } );\nfunction foo() {\n    return succeeds()\n        .then( succeeds )\n        .then( failsSecond );\n\n    function succeeds() {\n        return transaction.one( \"SELECT 1 AS id\" );\n    }\n}\n\nfunction failsFirst() {\n    return transaction.one( \"SELECT id AS first FROM person WHERE(FALSE)\" );\n}\n\nfunction failsSecond() {\n    return transaction.one( \"SELECT id AS second FROM person WHERE(FALSE)\" );\n}\n\n} );\n```\nNow the log shows this:\n2015-09-08 11:03:44.819 [DEBUG ] (       postgres.js) tx(tx)/start\n2015-09-08 11:03:44.819 [DEBUG ] (       postgres.js) tx(tx): begin\n2015-09-08 11:03:44.820 [DEBUG ] (       postgres.js) tx(tx): SELECT 1 AS id\n2015-09-08 11:03:44.820 [DEBUG ] (       postgres.js) tx(tx): SELECT id AS first FROM person WHERE(FALSE)\n2015-09-08 11:03:44.821 [DEBUG ] (       postgres.js) tx(tx): SELECT 1 AS id\n2015-09-08 11:03:44.821 [ERROR ] (       postgres.js) tx(tx)/error on database transaction: No data returned from the query.\n2015-09-08 11:03:44.821 [INFO  ] (       postgres.js)   SELECT id AS first FROM person WHERE(FALSE)\n2015-09-08 11:03:44.826 [DEBUG ] (  personManager.js) Promise.all rejected\n2015-09-08 11:03:44.827 [DEBUG ] (       postgres.js) tx(tx): rollback\n2015-09-08 11:03:44.828 [DEBUG ] (       postgres.js) tx(tx): SELECT id AS second FROM person WHERE(FALSE)\n2015-09-08 11:03:44.828 [DEBUG ] (       postgres.js) tx(tx)/end duration: 9, success: false\n. If I take out one succeeds(), the behavior changes. I'm not sure if the length of the promise chain is relevant here or if this just affects the way the resulting queries are interleaved.\n. Yes, the problem persists. The full log is:\n2015-09-08 11:03:44.819 [DEBUG ] (       postgres.js) tx(tx)/start\n2015-09-08 11:03:44.819 [DEBUG ] (       postgres.js) tx(tx): begin\n2015-09-08 11:03:44.820 [DEBUG ] (       postgres.js) tx(tx): SELECT 1 AS id\n2015-09-08 11:03:44.820 [DEBUG ] (       postgres.js) tx(tx): SELECT id AS first FROM person WHERE(FALSE)\n2015-09-08 11:03:44.821 [DEBUG ] (       postgres.js) tx(tx): SELECT 1 AS id\n2015-09-08 11:03:44.821 [ERROR ] (       postgres.js) tx(tx)/error on database transaction: No data returned from the query.\n2015-09-08 11:03:44.821 [INFO  ] (       postgres.js)   SELECT id AS first FROM person WHERE(FALSE)\n2015-09-08 11:03:44.826 [DEBUG ] (  personManager.js) Promise.all rejected\n2015-09-08 11:03:44.827 [DEBUG ] (       postgres.js) tx(tx): rollback\n2015-09-08 11:03:44.828 [DEBUG ] (       postgres.js) tx(tx): SELECT id AS second FROM person WHERE(FALSE)\n2015-09-08 11:03:44.828 [DEBUG ] (       postgres.js) tx(tx)/end duration: 9, success: false\n2015-09-08 11:03:44.830 [DEBUG ] (           pg:pool) timeout: 1441703054830 verbose\n2015-09-08 11:03:44.830 [DEBUG ] (           pg:pool) dispense() clients=0 available=2 info\n2015-09-08 11:03:44.830 [DEBUG ] (       postgres.js) disconnect(postgres@fairmanager)\n2015-09-08 11:03:44.838 [INFO  ] (     apiHandler.js) api(create person)/fail\n2015-09-08 11:03:44.838 [ERROR ] (     apiHandler.js) No data returned from the query.\n2015-09-08 11:03:44.840 [DEBUG ] (              HTTP) POST /api/people 500 39.770 ms - 34\n2015-09-08 11:03:44.840 [ERROR ] (       postgres.js) tx(tx)/error on database transaction: Loose request outside a connection that has expired.\n2015-09-08 11:03:44.840 [INFO  ] (       postgres.js)   SELECT id AS second FROM person WHERE(FALSE)\n. Very interesting! I had also noticed that the problem went away when using Promise.settle instead of Promise.all. Obviously, because it waits for all promises to be resolved/rejected. I just didn't go for it because I wasn't sure what else it implies in this context.\nIn case it still matters, my work workstation is Windows 7 x64 Node 0.12.7. My home workstation is Windows 10 x64 Node 0.12.7.\n. Yes, I guess, everything is working as it should or as well as it possibly can. The problem is now that our code is producing this situation, but I'm working on resolving that :)\n. I had actually looked at sequence but decided that I wouldn't need it, because I do want parallelization. But I'll rethink this once more :)\nThanks for the help! \n. @vitaly-t Wow, excellent work! I'll try to test this soon, but I have to work on progressing our project further right now. I spent a lot of time on load and concurrency issues the past few days and people are waiting for a merge :P\n. I'll definitely upgrade ASAP, but I'll be on a short trip until the end of the week. I'll report back next week :)\n. @vitaly-t Not yet. This is still on my radar though :) In fact, I'll switch out our .all() calls right now.\n. All tests are passing and I can see the new failing behavior in edge cases. I'll just have to adjust our error handling because it doesn't expect the resolution array we're getting now.\n2015-09-16 09:49:10.000 [ERROR ] (     apiHandler.js) [\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": true\n2015-09-16 09:49:10.000                                 },\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": true\n2015-09-16 09:49:10.000                                 },\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": false,\n2015-09-16 09:49:10.000                                   \"result\": \"Single row was expected from the query, but multiple returned.\"\n2015-09-16 09:49:10.000                                 },\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": false,\n2015-09-16 09:49:10.000                                   \"result\": \"Single row was expected from the query, but multiple returned.\"\n2015-09-16 09:49:10.000                                 },\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": false,\n2015-09-16 09:49:10.000                                   \"result\": \"Single row was expected from the query, but multiple returned.\"\n2015-09-16 09:49:10.000                                 },\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": false,\n2015-09-16 09:49:10.000                                   \"result\": \"Single row was expected from the query, but multiple returned.\"\n2015-09-16 09:49:10.000                                 },\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": false,\n2015-09-16 09:49:10.000                                   \"result\": \"Single row was expected from the query, but multiple returned.\"\n2015-09-16 09:49:10.000                                 },\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": true\n2015-09-16 09:49:10.000                                 },\n2015-09-16 09:49:10.000                                 {\n2015-09-16 09:49:10.000                                   \"success\": true\n2015-09-16 09:49:10.000                                 }\n2015-09-16 09:49:10.000                               ]\n. Nice catch! Yeah, we're using JSON.stringify there. Should be fixed now (not optimal, but it'll do for now).\nWe're on 1.10.5 since yesterday :)\n. @vitaly-t Sure, we're still using it ;) I'll check out the article in more detail later. But I don't think it's relevant for us at this time.\n. I'm confused. I'm not seeing it being attached to $main or module.exports. But I also just noticed the // No 'use strict' here, to let queryResult export into the global namespace.. Well, I have to get back to this after my trip ;D\n. Why is it exported into the global namespace anyway? Wouldn't it be much cleaner to export it from the module?\nIf I just use queryResult globally in my code, I feel like that would be confusing, because people might not be aware where it is exported from.\n. Would you consider exporting queryResult in addition from the module? That would make it more obvious where the symbol is coming from when we use it in our code.\nThank you very much btw for your continued work on this library. It's much appreciated!\n. Good point about using global.. Thanks!\n. Yeah, sorry. I'd be very happy to see this changed in 2.0, but I know how to work with this in the meantime :) Cheers\n. I know. But I wrote an abstraction layer that has a listBy and readBy method, which both bind to a generic selectBy method. They only differ in what they expect selectBy to return; a single item or many. That's where I wanted to utilize query() with queryResult. I'm already using the methods you mentioned anywhere else.\nThanks though :)\n. We're already making use of extend ;) However, I might actually have a question for you regarding that part.\nWe're using it like this:\njs\nextend     : function extend( target ) {\n    target.person   = Person.manager( target );\n}\nAnd Person.manager will then be:\njs\nmodule.exports.manager = function initManager( db ) {\n    return new PersonManager( db );\n};\nNow, what bothers me slightly about this is, every time there's database interaction, a new PersonManager instance is created. If possible, I would prefer to only construct it once and re-use it. But I'm not sure if db is always the same object.\nOf couse, I could also refactor our code so that all our methods in PersonManager just expect db to be passed, instead of relying on a previously stored member variable. But I haven't gotten to optimizing all of this yet. For now, I'm just working on getting stuff to run ;D\n. Awesome. That's already very helpful advise. Thanks again.\n. @vitaly-t Heh, I was just drafting a very long email to you, because I thought I had found another issue with pg-promise. But the rubber-duck-debugging while writing the email lead me to the solution.\nI really wish something in our postgres stack would detect/warn about connection pool exhaustion (which was the cause for our issue). But I don't think pg-promise is the right place for this.\nThe draft looks really nice. One thing we're using in our code, which I could see as beneficial in pg-promise is the automatic generation of task/transaction IDs. We assign a UUID to every task/transaction, which makes it slightly easier to track them in our logs.\nOther than that, I've really not found anything missing from pg-promise.\n. Yeah, I figured out a way to increase the pool size shortly after I realized what my problem was (which took me hours!)\nI came by that auto-tagging code today, while debugging, and was kind of surprised to see those capabilities :D I might actually take another look at our tagging mechanism again and see if I can utilize the auto-tagging.\nThanks again :)\n. I always interpreted load balancing as the process of distributing tasks among multiple, usually equally capable, resources. As in, we're balancing the load of multiple resources on the same level. So, either my understanding of the term is not correct or I'm not seeing that in your definition.\nThe data throttling definition sounds spot on from my point of view.\n. @vitaly-t From a brief look, I would expect that the promise case allows for superior optimization for some reason. Maybe V8 can guess the final size of result in the promise case and saves reallocs. But that's just a wild guess.\n. Join us in the StackOverflow JavaScript chat room if you don't get any good answers ;D\n. @vitaly-t Oh, sweet. I'll check it out ASAP :)\nJust read the migration guide. Sounds great! I'll definitely try to work this in before our next major release.\nI wonder if we can use spex in places where we used to rely on _.reduce or Array.prototype.reduce to sequence promises. I'll definitely have to give that library a closer look.\nThanks for the heads-up :)\n. Hah, I didn't even know about Promise.reduce in bluebird! :D I was thinking about code like this:\n``` js\nreturn _.reduce( selection, function populateForeignKey( previous, foreignKey ) {\n    var foreignType = self.foreignKeys[ foreignKey ];\nif( !foreignType ) {\n    log.warn( \"Invalid foreign key '\" + foreignKey + \"' in selection!\" );\n    return previous;\n}\n\nreturn previous\n    .then( _.bind( self.populate, self, record, foreignKey, foreignType ) );\n\n}, Promise.resolve( record ) );\n```\nI guess Promise.reduce would be even better, if I understood it correctly. And spex.sequence could be even more betterer ;D\nSo much to learn still\u2026\n. @vitaly-t We're running on 2.2.5. No issues :)\n. I generally like the idea. I don't want to complain about node-postgres as it is very helpful in our project. However, I also noticed that the author isn't able to take care of support requests. Which is understandable, but disappointing nevertheless.\nWe currently have a pretty stable setup without any issues, so there isn't really any incentive to migrate to a different library.\nWhat would really help us a lot, is a scheduling/queuing approach that solves resource conflicts caused by transactions that both want to lock the same tables. We currently only allow a single transaction at a time to work around the issue, but this is rather disappointing because it doesn't allow us to have any concurrency :( Our current system isn't smart at all and doesn't even allow concurrent read-only transactions :P\nIf pg-core has the same interface as node-postgres, I can definitely see us moving there as I definitely enjoy your responsiveness and the care you take of your projects :+1: \n. We are actually using tasks for read-only transactions. I was just using the term transaction broadly.\nLet me give an example of what we're dealing with. We have an entity \"person\". All properties that are relevant to a person are not stored within the same table, they require several in fact.\nHowever, we have a REST API that pretends like all the data stored in all those tables are a single entity. So when we receive a new entity through the API, we need to update all the tables in a transaction. Because we had even more trouble with resource conflicts in the past, we're now setting every transaction as SERIALIZABLE, even though it doesn't really matter, as we now avoid all concurrency anyway.\nNow, due to the nature of promises and how queries are dispatched, if there are a lot of API requests, it is possible for multiple of these transactions to lock the same resource and transactions get rejected. Retrying transactions is rather complicated and we decided to take the easy way out and simply queue all transactions.\nThis queuing is already in place at the API level, so it doesn't know if the data layer would only dispatch read-only tasks.\nSo, yeah, we still have a lot of work to do :P\n. @vitaly-t Sorry, I wasn't able to look into the topic again and I don't know when I can allocate time to investigate this any further.\nWe're generally dealing with 2 types of concurrency issues in our codebase:\n1. Connection pool exhaustion - Caused by running too many concurrent, read-only queries.\nWe solve this with Promise.map( foo, { concurrency : n } ) or similar approaches.\n2. Resource conflicts - Cause by running concurrent transactions that write to the same tables.\nWe solve this by only allowing a single request, which can cause these types of transactions, to enter into the data abstraction layer.\nThe only thing that I could see as beneficial in the library would be a mechanism that can retry transactions if ERROR:  could not serialize access due to concurrent update occurs. But I'm not sure if that would be a smart move in production. I prefer to control the behavior at a higher level, where we have full understanding of all the entities we're manipulating, how they affect each other and what measures should be taken to resolve conflicts.\n. It really doesn't matter how large of a pool size we choose. If we don't limit our concurrency, we'll always have actions that will exceed it. Since we changed our approach, we're fine the default pool size, whatever that is.\nAnd, yeah, I know about repeatable transactions. The point is this:\n\nWhen an application receives this error message, it should abort the current transaction and retry the whole transaction from the beginning.\n\nWe have to repeat them ourselves. We don't even want to run into that situation. Who knows how often we have to retry them until they succeed? What do we do with the request that caused the transaction in the meanwhile? The situation must be avoided IMHO.\nWhat would be great if \"the system\" could identify a transaction that would cause a conflict and queue it. Likewise, it should be able to identify transactions that won't cause conflicts and just send them out.\nSending out as many transaction as you want and retrying the ones that failed until they succeed does not seem like a clean approach.\n. Heheh, I totally agree :)\n. Our whole codebase is still on ES5 :( Thanks though.\n. No, we're using 4.2. But a lot of our code is for the browser and mobile devices. If we want to move to ES6, we have to introduce every developer to it and allow it in all areas of our code base. And then you have potential conflicts with dependencies or development tools\u2026\nYou might not believe it, but it was already quite the challenge to introduce promises into our code base.\n. Ah, I see. Nevermind then :)\n. ",
    "fundon": "see bluebird.reduce\n. @vitaly-t  I mean that we need a changlog or history file to record changes.\n. Likes https://github.com/nodejs/node/blob/master/CHANGELOG.md https://github.com/npm/npm/blob/master/CHANGELOG.md.\n. Yes, I saw the release notes.\nBut a changelog file that make us clear.\nAfter installed the package, we just need to open the changelog file, open node_modules/pg-promise/CHANGELOG.md.\n. > There is also a very abbreviated project history, though it's not much of use.\nOkay, I saw.\n. ",
    "gilly3": "Thanks for your detailed response.  Very helpful.  pgp.as.format() is particularly useful.\n. ",
    "khusamov": "Oh, how ... I thought you were in Russian can speak. On the toaster you wrote in Russian)))\n. Yes, please write in English. Anyway already got.\n. > Use transactions when executing a chain containing data-changing queries, and use tasks when executing a chain of read-only requests.\n\u041d\u0435 \u044f\u0441\u043d\u043e \u043a\u0430\u043a \u0431\u044b\u0442\u044c, \u043a\u043e\u0433\u0434\u0430 \u0443 \u043c\u0435\u043d\u044f \u0438 data-changing queries \u0438 read-only requests \u0432\u043c\u0435\u0441\u0442\u0435. \u0423 \u043c\u0435\u043d\u044f \u0442\u0430\u043c \u0441\u043d\u0430\u0447\u0430\u043b\u0430 insert, \u043f\u043e\u0442\u043e\u043c select, \u043f\u043e\u0442\u043e\u043c update...\n. \u041f\u043e\u0445\u043e\u0436\u0435 \u043c\u043e\u044e \u0437\u0430\u0434\u0430\u0447\u0443 \u043d\u0435 \u0440\u0435\u0448\u0438\u0442\u044c. \u0423 \u043c\u0435\u043d\u044f \u0442\u0430\u043c \u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0437\u0430\u043f\u0440\u043e\u0441 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u043f\u0440\u0435\u0434\u044b\u0434\u0443\u0449\u0435\u0433\u043e... \u041d\u0443 \u0440\u0430\u0437 \u0434\u043b\u044f \u043e\u0434\u0438\u043d\u043e\u0447\u043d\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043f\u043e\u0441\u0442\u043e\u044f\u043d\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 \u044d\u0442\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e, \u0437\u043d\u0430\u0447\u0438\u0442 \u0432\u0441\u0435 \u0432 \u043f\u043e\u0440\u044f\u0434\u043a\u0435. \u042f \u0434\u0443\u043c\u0430\u043b \u0447\u0442\u043e \u043e\u0434\u0438\u043d \u0440\u0430\u0437 \u0432\u043a\u043b\u044e\u0447\u0438\u043b \u0441\u043e\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435, \u0430 \u043f\u043e\u0442\u043e\u043c, \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0447\u0435\u0440\u0435\u0437 10 \u0434\u043d\u0435\u0439, \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u0440\u0443\u0431\u0438\u0442\u044c \u0441\u043e\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u0435, \u043a\u043e\u0433\u0434\u0430 \u043e\u043d\u043e \u0443\u0436\u0435 \u043d\u0435 \u043d\u0443\u0436\u043d\u043e\u0435.\n. \u0432 \u0437\u0430\u0434\u0430\u0447\u0443 \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043c\u0435\u0441\u0442\u0438\u0442\u044c insert, update?\n. \u042f \u043f\u043e\u0445\u043e\u0436\u0435 \u0447\u0442\u043e-\u0442\u043e \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u044e. \n\u0423 \u043c\u0435\u043d\u044f \u043f\u0435\u0440\u0432\u044b\u0439 \u0437\u0430\u043f\u0440\u043e\u0441 \u044d\u0442\u043e insert. \n\u0412\u0442\u043e\u0440\u043e\u0439 \u0438 \u0442\u0440\u0435\u0442\u0438\u0439 select, \u0437\u0430\u0432\u0438\u0441\u044f\u0442 \u043e\u0442 \u043f\u0435\u0440\u0432\u043e\u0433\u043e, \u0442\u0430\u043a \u043a\u0430\u043a \u043d\u0443\u0436\u043d\u043e \u0437\u043d\u0430\u0442\u044c id \u043d\u043e\u0432\u043e\u0439 \u0441\u0442\u0440\u043e\u043a\u0438. \n\u0414\u0430\u043b\u0435\u0435 update, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 select-\u043e\u0432 (\u0442\u043e\u043b\u044c\u043a\u043e \u0438\u0437 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0433\u043e select \u043c\u043e\u0436\u043d\u043e \u0443\u0437\u043d\u0430\u0442\u044c id \u0437\u0430\u043f\u0438\u0441\u0438, \u043a\u043e\u0442\u043e\u0440\u0443\u044e \u043d\u0430\u0434\u043e \u043e\u0431\u043d\u043e\u0432\u0438\u0442\u044c.).\n\u041c\u043e\u0436\u043d\u043e \u0432\u0441\u0435 \u044d\u0442\u043e \u0432 \u043e\u0434\u043d\u0443 \u0437\u0430\u0434\u0430\u0447\u0443 \u0437\u0430\u043f\u0438\u0445\u0430\u0442\u044c? \u0418\u043b\u0438 \u044d\u0442\u043e \u0442\u0440\u0438 \u0437\u0430\u0434\u0430\u0447\u0438?\n. 1) \u041d\u0435 \u043f\u043e\u043d\u044f\u043b, \u0437\u0430\u0447\u0435\u043c \u043d\u0430\u0434\u043e \u0442\u0430\u043a \u043f\u0438\u0441\u0430\u0442\u044c:  return promise.resolve(individual); \u0432\u043c\u0435\u0441\u0442\u043e return individual?\n\u0412\u0435\u0434\u044c \u0441\u043a\u0430\u0437\u0430\u043d\u043e \u0436\u0435, \u0447\u0442\u043e \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0442\u044c \u043c\u043e\u0436\u043d\u043e \u043b\u0438\u0431\u043e promise \u043b\u0438\u0431\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440.\n2) \u0418 \u043f\u043e\u0447\u0435\u043c\u0443 promise \u0441 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u043e\u0439 \u0431\u0443\u043a\u0432\u044b? \u042d\u0442\u043e \u043e\u043f\u0435\u0447\u0430\u0442\u043a\u0430 \u0438\u043b\u0438 \u044f \u0441\u043d\u043e\u0432\u0430 \u0447\u0435\u0433\u043e-\u0442\u043e \u043d\u0435 \u0437\u043d\u0430\u044e? \u0412\u0440\u043e\u0434\u0435 \u043a\u043b\u0430\u0441\u0441 \u0441 \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0431\u0443\u043a\u0432\u044b \u043d\u0430\u0447\u0438\u043d\u0430\u0435\u0442\u0441\u044f. \u041d\u0443 \u043f\u043e \u043a\u0440\u0430\u0439\u043d\u0435\u0439 \u043c\u0435\u0440\u0435 \u0432 \u0443\u0447\u0435\u0431\u043d\u0438\u043a\u0435.\n. > \u044d\u0442\u043e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u0438\u0437 \u043a\u0430\u043a\u043e\u0433\u043e \u0438\u0441\u0442\u043e\u0447\u043d\u0438\u043a\u0430 \u0438 \u043f\u0440\u043e \u043a\u0430\u043a\u043e\u0439 \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442?\n\nhttp://habrahabr.ru/post/209662/\n\n```\nvar promise = new Promise(function(resolve, reject) {\n  resolve(1);\n});\npromise.then(function(val) {\n  console.log(val); // 1\n  return val + 2;\n}).then(function(val) {\n  console.log(val); // 3\n});\n```\n. \u042f\u0441\u043d\u043e. \u041d\u043e \u043f\u043e\u043a\u0430 \u043e\u0448\u0438\u0431\u043e\u043a \u043d\u0435 \u0431\u044b\u043b\u043e.\n\u0412 \u043e\u0431\u0449\u0435\u043c \u044f \u043f\u0440\u0438\u0448\u0435\u043b \u043a \u0432\u044b\u0432\u043e\u0434\u0443, \u0447\u0442\u043e \u0432\u0441\u044e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u044e \u043d\u0430\u0434\u043e \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a (\u0442\u0430\u043a \u043a\u0430\u043a \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0441 \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u0435\u043c \u0441\u0430\u043c\u043e\u0439 \u043a\u043e\u043d\u0446\u0435\u043f\u0446\u0438\u0438) \u0438 \u0434\u043e\u043b\u0433\u043e \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c. \u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0437\u0430 \u043e\u0442\u0432\u0435\u0442\u044b! \u0415\u0441\u043b\u0438 \u0447\u0442\u043e - \u043a\u0443\u0434\u0430 \u0441\u043b\u0430\u0442\u044c \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u043f\u043e \u0432\u0430\u0448\u0435\u0439 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0435?\n. > \u0415\u0441\u043b\u0438 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u043e\u043c - \u0442\u043e \u043b\u0443\u0447\u0448\u0435 \u043d\u0430 stackoverflow.ru\n\u043a\u0430\u043a\u0438\u0435 \u043e\u0431\u0440\u0430\u0437\u043e\u043c \u0442\u0430\u043c \u043c\u043e\u0436\u043d\u043e \u0432\u043e\u043f\u0440\u043e\u0441 \u0437\u0430\u0434\u0430\u0442\u044c \u0430\u0434\u0440\u0435\u0441\u043d\u043e, \u0438\u043c\u0435\u043d\u043d\u043e \u043a \u0432\u0430\u043c?\n. \u042f\u0441\u043d\u043e)\n. I got it! Thank you!\n. ",
    "jniemin": "Maybe could be something that would be on by default but could be turned off for testing purposes?\n. I totally get it that having those Locks are important to not let people do mistakes by accident. \nI like the idea of using initialization option as then people don't have to play and set env variables while running tests but just initialize it with different options.\n. Work nicely in 1.11.0, thanks. I'll close the case\n. ",
    "madebyherzblut": "For me both issues are about the same problem: I consider everything that rejects my promise as an error and right now checking for an error is awkward because sometimes  you get an Error object and sometimes a string:\n\n\u2026if it's a real syntax error I get a complex object, but for no data found it's just a string saying \"No data returned from the query.\". It would be nicer if there were a error object with a code and message for no data found.\n\nThis is how I do it now:\njavascript\ndb.one(query).catch(err => {\n  if (typeof err === 'string') {\n    // This must be a pgp error caused by a mismatched query result mask\n    return next(new error.NotFoundError(err));\n  }\n  // This is likely a database error\n  next(new error.InternalServerError(err));\n});\n. Ah, sorry you are right. I somehow had one instead of oneOrNone stuck in my head. \n. ",
    "RyanMcDonald": "Thanks for the quick responses @vitaly-t! It's working for me now; it was a different issue that was causing my strings not to be surrounded in quotes.\n. @vitaly-t I'm not sure I understand how to apply the custom formatting in this scenario. Could you provide  an example?\n. Sorry, I just got around to this. Thanks so much, @vitaly-t, this was exactly what I needed!\n. Ah, yeah that's what I was doing in the meantime. Would this be difficult to implement? Possible feature request for a future version?\n. @vitaly-t Beautiful, thank you! \ud83d\ude04 \n. @vitaly-t Thanks for the info. It was worth a shot. \ud83d\ude04 \n. ",
    "paulxtiseo": "I did look at using task() (don't need a transaction) and sequence(), but unless I am mistaken, that simply helps me run the promises sequentially inside a shared connection. How do I control the amount of promises being created, or does sequence() ensure that I create-destroy one promise before moving on to the next one?\nI also am not sure how to use sequence() in conjunction with a stream. (You seem to allude to a  benchmark that would maybe shed some light on it, but not sure where it is.)\nPS: I guess at this point, seems  I should have gone to StackOverflow... I apologize.\nPPS: I'm looking for the promise-y version of this: https://bassnutz.wordpress.com/2012/09/09/processing-large-files-with-nodejs/\n. Not quite.\nFrom my original code, I have an array (mappedFiles) that has multiple large files (paths, really). I bluebird.map() the array to a function that creates a stream and processes it. The data being read must be stored to a DB. The function doing this is the none() function from your library that returns a promise. So, at my first pass above, I keep generating a ton of pending DB operations that eventually would resolve, but also cause tremendous memory pressure, oftentimes causing an abort by Node.\nI can see that the burden of throttling resides entirely in processCsvStream(). I was about to bring in async and use queue to do this. Thanks for the ref to spex. I am looking at page() (instead of sequence()) as a way to batch the data into the DB. Haven't done it just yet due to a support issue in my work day and having a hard time wrapping my head around streams-mixed-with-promises. :)\n. One last question, do you have or know of any real, working example that processes a stream using these methods()? \n. Actually, the connect being called is:\n```\nDatabaseManager.prototype.connect = function(nameOrAlias) {\n  let self = this;\n  return new promise(function(resolve, reject) {\n    let conn;\n    try {\n      conn = self.getConnectionByNameOrAlias(nameOrAlias);\n  if(!conn) {\n    reject(new verror('No match on name/alias.'));\n  }\n\n  if(!conn.adapter) {\n    switch(conn.settings.dialect) {\n      case 'postgres':\n        resolve(providers.postgres.connect(conn));\n        break;\n      default:\n        reject(new verror('Not implemented!'));\n        break;\n    }\n  }\n}\ncatch(e) {\n  reject(new verror(e, 'Could not connect.'));\n}\n\nresolve(conn);\n\n});\n};\n```\n(Eventually, there would be more dialects handled, hence the layers; postgres is my first one, via pg-promise.)\n. I actually added a console.log in the library code for oneOrNone(), and it was triggered/displayed. The odd thing is that it doesn't seem to be percolating back up properly. Neither the then nor the catch in initMigrate are being hit.\nPS: My code above is a bit truncated. Here's the actual whole function:\nproviders.postgres.initMigrate = function(connection) {\n  console.log('providers.postgres.initMigrate', connection);\n  //return 'yay';\n  //console.log(promise.Promise.resolve('yay'));\n  //return promise.resolve('yay');\n  //console.log(connection.adapter.oneOrNone('SELECT table_name FROM information_schema.tables WHERE table_name=\\'migration\\';'));\n  console.log(connection.adapter);\n  return connection.adapter.oneOrNone('SELECT table_name FROM information_schema.tables WHERE table_name=\\'migration\\';')\n    .then(data => {\n      console.log('OneOrNone data',data);\n      return 'yay';\n    })\n    .catch(err => {\n      console.log('OneOrNone err',err);\n      return 'nay';\n    });\n};\n. I agree that it's very odd. I've used this library many times with great success, thank you! :)\nIf I have a console.log(connection.adapter.oneOrNone('SELECT table_name FROM information_schema.tables WHERE table_name=\\'migration\\';'));, I do get a Promise object back: \nPromise {\n  _bitField: 0,\n  _fulfillmentHandler0: undefined,\n  _rejectionHandler0: undefined,\n  _promise0: undefined,\n  _receiver0: undefined }\nDoes anything look suspicious to your better eye?\n. The issue happens only in the tests under the Mocha-Chai harness. If I use the same codebase without Mocha overlayed over it, I don't get the strange behavior.\nYou are welcome to close the ticket. I will likely drill down on why, and can always post here for posterity once I find a better answer.\n. ",
    "kdex": "Ah, that's great news. Much obliged!\n. Thanks for the update, @vitaly-t. But out of curiosity, are you certain that 2.0.6 (pre-release) has been shipped correctly? For me, after inspecting the archive, the package.json still shows:\njson\n    \"version\": \"1.11.0\",\nand the implicit global queryResults is still being used in lib/index.js. Is this intended? Both files look okay remotely, but accidental in the released archives.\n. Thanks a lot for the pointer, but I was actually referring to the downloadable zip/tar archives that this repository links to on the bottom this page which makes a package.json that uses it in its devDependencies fail (since the archives contain 1.11.0 instead):\njson\n    \"pg-promise\": \"https://github.com/vitaly-t/pg-promise/archive/v.2.0.6.tar.gz\",\nAnyway, I just ran babel against 2.0.6, and it worked flawlessly. Thank you!\n. Oh, let me rephrase: By failing, I was referring to them pointing to the contents of the wrong version. So what has been released as v.2.0.6.tar.gz really contains v.1.11.0 right now:\nbash\n$ wget https://github.com/vitaly-t/pg-promise/archive/v.2.0.6.tar.gz\n$ tar -xf v.2.0.6.tar.gz \n$ cat pg-promise-v.2.0.6/package.json | grep version\n  \"version\": \"1.11.0\",\n$\n. @vitaly-t Ah, neat! Thanks a lot for all of your effort! Everything works as expected. :smirk_cat: \n. I've already had a glance at the errors namespace, but I couldn't see a helper that could simplify the task of figuring out which value broke a unique constraint. I found that error.detail gives me\ndetail: 'Key (account_name)=(kdex) already exists.',\nwhich seems \"simple enough\" to be parsed for the column's name, but I was really wondering if the column name is directly exposed somewhere in the error I haven't looked yet.\nAlso, is there a way of detecting a uniqueness violation without using the \"magic number\" 23505 anywhere in the code?\nI feel tempted to have a separate file that only maps all of PostgreSQL's \"error codes\" to something human-readable like:\njs\nexport const UNIQUENESS_VIOLATION = \"23505\";\nexport const NO_DATABASE = \"3D000\";\n// etc.\n. Huh, so I suppose I'll have to write a custom error parser. What a bummer. :)\nThanks a lot for helping out!\n. Why don't they? Is the error reporting something that PostgreSQL might completely overhaul at some point then? And thank you for the pointer; looks very promising. :+1: \n. That makes sense; but as for breaking changes: that should absolutely not be an issue with close adherence to semver/npm; that's what it's for. I'll have a look at node-pg-error, thanks. :)\n. Yes. There's three columns in the original source, but I've simplified the scenario for GitHub.\n. Oh, that looks very promising. I'll give it a try. Thanks!\n. Works exactly as advertised! Thanks so much! :smile:\n. Thank a lot for saving me the trouble, much obliged! \ud83d\ude03. Looking at how one would have to set _rawType to true, I'm actually a little confused: Traditionally, ECMAScript, for the lack of a real private fields, has been using leading underscores to denote that API users aren't supposed to change a field, as it might come with unexpected side effects.\nIf, at some point, you decide to transform the codebase to a newer ECMAScript version (for which there are plans of supporting true, private fields), users wouldn't be able to change _rawType anymore.\nWhy is it denoted like a private variable, although based on the advice you're giving, it seems like people are supposed to use it?. Feel free to correct me if I'm wrong, but to me, this sounds like you might benefit from creating a prototype that users are supposed to extend. E.g. something akin to:\njs\nclass POI extends RawType {\n  toPostgres() {\n      // \u2026\n  }\n}\nThis would even get rid of the need for a _rawType property altogether, as internally, you could rely on x instanceof RawType instead.\n(Also feel free to transform this example to ES5 if you don't like syntactical classes.). I don't think I'm following you; could you elaborate on that?. Ah, yes, that makes sense. Though wrapping it isn't really that involved:\njs\nObject.setPrototypeOf(obj, RawType.prototype);\nBut before I go as far and recommend you mess with prototypes at runtime, which will hurt performance in most current engines, I'll stop. \ud83d\ude04 Thanks for the explanation!. ",
    "nickretallack": "Oops, should have checked the wiki.  I guess I expected to see a link from the README file, since it's what shows up on the NPM page.  Oh, I'm wrong, that page is linked from there.  I feel silly.\n. Yeah I'm blind I guess.  What lead me astray actually was this description of the query function which doesn't seem to leave room for passing in an object.\n. I realize that's how it's set up now.  However, I'm not sure what's stopping you from translating the arguments from one style into the other.\n. ",
    "nyurik": "My code requires an iterator function (created by the promistreamus npm lib). Calling the iterator function returns a promise of a value once that value becomes available. This way i can process one/several values at a time from a very large stream of values in a pull fashion, e.g. from a database query.  See code that exposes it.\n. Thanks for the suggestion about query formatting. Does postgres client lib support pre-compiled queries?  In which case I suspect that it is better to always use the same query string, and pass parameters separately?\n. Thanks!  Reading is good for now ))  We are still debating between Cassandra and postgres, so i wouldn't want to spend too much effort on building a perfect map tiles backend yet ))\n. We are running Cassandra, but i have heard that Postgres will outperform it. The thing is - there are more pressing issues to deal with - once they are done, we could start optimizing )\n. ",
    "destructive-dragon": "\nTheir performance gain is only possible for very complex queries, while only slowing the simple ones. And even for the complex ones the gain is often negligible;\n\ncould you please elaborate on that topic? I couldn't find any sources so far that prepared statements slow down anything? Maybe a link or something?\nThanks!\n. ",
    "vertiman": "I don't think that is true, commit of the inner transaction is not ignored when the parent does a rollback.  There is no such thing as an inner transaction in PG.  That test you referred to passes because the inner transaction fails, not the outer transaction.\nHere's a full test showing what I mean.  I'm working on a strategy to patch this now... we'll see if you like it.  This test fails right now, with all the db entries being set to \"Internal\" even though they should remain as \"Before\" since the outer tx rolls back.\n```\ndescribe(\"When an outer transaction fails that has a nested transaction\", function () {\n    it(\"both transactions must rollback\", function () {\n        var result, context1;\n    db.none('update users set login = $1', 'Before').then(function () {\n        return db.tx(function (t1) {\n            context1 = this;\n            return this.none('update users set login=$1', 'External').then(function () {\n                return context1.tx(function (t2) {\n                    return t2.none('update users set login=$1', 'Internal')\n                });\n            }).then(function () {\n                return context1.one('select * from unknowntable') // emulating a bad query;\n            })\n        })\n            .then(function () {\n                result = null; // must not get here;\n            }, function () {\n                return promise.all([\n                    db.one('select count(*) from users where login=$1', 'External'), // 0 is expected;\n                    db.one('select count(*) from users where login=$1', 'Internal'), // 0 is expected;\n                    db.one('select count(*) from users where login=$1', 'Before') // > 0 is expected;\n                ]);\n            })\n            .then(function (data) {\n                result = data;\n            });\n    });\n\n\n    waitsFor(function () {\n        return result !== undefined;\n    }, \"Query timed out\", 5000);\n\n    runs(function () {\n        expect(result).toBeTruthy();\n        console.log(result[0], result[1], result[2]);\n        expect(result[0].count == 0).toBe(true);\n        expect(result[1].count == 0).toBe(true);\n        expect(result[2].count > 0).toBe(true);\n    });\n});\n\n});\n```\n. Even more importantly, that test could be structured to show that if the statement that updated values to External wasn't completely overwritten by the statement that sets values to internal (if they used different data), both of those statements would be committed, even though the outer TX rolls back.\n. Sorry, I'm not sure what you're asking me to do?  Happy to help, just unsure.\nHow do I use tagged transactions?\n. ---------------------------15:47:03 connect(xxx@pg_promise_test)\n15:47:03 update users set login = 'Before'\n15:47:03 disconnect(xxx@pg_promise_test)\n15:47:03 connect(xxx@pg_promise_test)\n15:47:03 tx(outer)/start\n_15:47:03 tx(outer): begin\n15:47:03 tx(outer): update users set login='External'\n15:47:03 tx(inner)/start\n_15:47:03 tx(inner): begin\n15:47:03 tx(inner): update users set login='Internal'\n_15:47:03 tx(inner): commit\n15:47:03 tx(inner)/end; duration: .002, success: true\n15:47:03 tx(outer): select * from unknowntable\n15:47:03 error: relation \"unknowntable\" does not exist\n         tx(outer): select * from unknowntable\n_15:47:03 tx(outer): rollback\n15:47:03 tx(outer)/end; duration: .007, success: false\nHere you go.  Per PG spec, when the second BEGIN is issued it will issue a warning, and notify that no new txn is created, then when the first COMMIT issued, it commits the entire transaction, before it then fails and issues a ROLLBACK with no active transaction.\n. I actually have a fix for this behavior, I just tested it.  Let me make a PR and let me know if you think it's in the right spot, or if there is a better spot for it.\n. @vitaly-t Thanks for being so responsive.  PR will be up in a few minutes.  Your library rocks, I'm happy I can contribute!\n. What is your intended behaviour though?  Given the nature of promises and rejection, I don't know if SAVEPOINTS is the right approach.  The PR essentially limited any db connection to a single nesting of transaction (only 1 BEGIN/ COMMIT / ROLLBACK would occur per nested set), and if an error bubbled up to the outer transaction it rolled back the whole thing.  I figured that would be the desired behaviour.\nThe only situation you'd want to use SAVEPOINT for would be continuing the outer txn in the event an inner TXN failed.  It seems like potentially a valid case, but would be quite complex to model effectively. \n. PS - at a minimum cherry pick in the unit test I added in the PR.  It will fail for improperly behaving nested transactions.\n. What about this - https://github.com/vertiman/pg-promise/commit/ca40929e2ea84cdc9ccf0e1ae95d59ce3d59b617\nThe design makes it so if you catch the error off the inner .tx() it will allow the outer to commit, even though it did the savepoint rollback, but if you don't catch it it will throw the rejection all the way up and roll back the whole transaction.  I think it covers all the cases now?\nLet me know and I can send a PR.\n. Just finalizing it all now.  There might be a minor issue with pg-monitor not showing rollback, but otherwise I think the behaviour is pretty clean.  Thanks for the quick turnaround.\nWill update you on the pg-monitor thing in a few minutes.\n. Never mind - it's all good.  Thanks again!\n. @vitaly-t thanks for the update, looks good!\n. @vitaly-t you do a good job explaining it, but why not just defer to the postgres documents (or at least link to them?).  I think the most important part of your new chapter is the very last line, outlining how pg-promise behaves with regards to nested transaction blocks.\n. Wow, you're right.  It's pretty subtle...\n\nIssuing BEGIN when already inside a transaction block will provoke a warning message. The state of the transaction is not affected. To nest transactions within a transaction block, use savepoints (see SAVEPOINT).\n. \n",
    "pmehtaaustin": "Thanks for your quick comment.  And thanks for pointing out the pgp.end mistake.  Noob here ;-)\nIs there any way to dynamically map the req.body names with the column names in the insert sql?\n. ",
    "Cleod9": "@vitaly-t I just ran into this thread when trying to figure out why the latest bluebird was so noisy about warnings by default. I just wanted to let you know that I'm completely on your side, so you're not alone! Some of ben's comments in the other issue thread definitely came off as a bit sarcastic and rude :/\nAnyway, there are two things that really bother me about that warning message:\n1) A developer writing proper promise rejection handling will have more than enough insight to debug. I don't see any anti-patterns with not returning a promise, so it doesn't make sense to me to warn the user about anything like that (and if they did mean to return it but forgot, they'd get an obvious synchronous error when trying to call then() anyway)\n2) I feel that a function from a library shouldn't be poking around to figure out if its instance was returned or not, leave that up to the compiler/linter to catch. Is it examining the stack or something to even determine this warning in the first place? Just seems strange to me, and this is my first time ever seeing something this progressive in terms of default warnings in a JS library\nI hope at some point that warning in particular is reconsidered, but for now I suppose I'll just adjust my config or downgrade\n. ",
    "aHumanBeing": "I'm sure for the first question and will double check the 2nd.  In using the simple approach, I don't have the location object data but something else:\n2015-11-09T15:43:19.442Z - debug:  _41=0, _86=null, _17=[]\nThis result appears no matter what id I use.\n. My apologies for the late response.  Yes, I was able to get over this hurdle but not as I wished.   No need to reopen. This code works:\n```\nvar client = new pg.Client(conString);\n    client.connect(function (err) {\n        if (err) {\n        next(new Error('could not connect to database'));\n    }\n    var _id = req.params.id;\n    client.query(\"select * from table where id = $1\", [_id], function (err, result) {\n        if (err) {\n\n            next(new Error('error running query'));\n        }\n        if (result === undefined || result.rows[0] === undefined) {\n            winston.log('error', 'Unknown provider with id: ' + _id);\n            client.end();\n            res.redirect('/aroute');\n        }\n        req.provider = result.rows[0];\n        client.end();\n\n        res.render('routeView', {\n            title: 'Route View'\n        });\n    });\n})\n\n```\n. I see.  Well, I'll try to take some time over the next few weeks and get back to you.  I'm willing to understand better and appreciate the help thus far!\n. The stack points to the pg-promise library. Of course I'll see if there's something else I did:\n2015-12-02T21:15:57.652Z - error:  TypeError: undefined is not a function\n    at Transaction. (/home/user/IdeaProjects/OASIS/app.js:203:21)\n    at invoke (/home/user/IdeaProjects/OASIS/node_modules/pg-promise/lib/index.js:443:29)\n    at /home/user/IdeaProjects/OASIS/node_modules/pg-promise/lib/index.js:478:20\n    at tryCallOne (/home/user/IdeaProjects/OASIS/node_modules/pg-promise/node_modules/promise/lib/core.js:37:12)\n    at /home/user/IdeaProjects/OASIS/node_modules/pg-promise/node_modules/promise/lib/core.js:103:15\n    at flush (/home/user/IdeaProjects/OASIS/node_modules/pg-promise/node_modules/promise/node_modules/asap/raw.js:50:29)\n    at process._tickCallback (node.js:355:11)\n. It's version 1.7.7.\njavascript\nresult = ctx.cb.call(obj, obj); // invoking the callback function;\nof function $transact(ctx, obj)\n. Not a problem lol. I'll let you know how it goes.\n. I believe upgrading helps but I'm struggling to get around a particular issue after modifying the code based on the new library:\n``` javascript\ndb.connect()\n        .then(function (obj) {\n            sco = obj; // save the connection object;\n            return sco.query(\"select count(*) from provider_type p \"\n            );\n        })\n        .then(function (obj) {\n            recordsTotal = obj[0].count;\n            filteredQuery = \"select CASE when type = 0 then 'Triage / Screening' \" +\n                            \"when type = 1 then 'Clinical' \" +\n                            \"when type = 2 then 'Prescriber' END as type\" +\n                            \", active from provider_type p where 1 = 1 \";\n        // provider id\n        filteredQuery += \" and p.provider_id = \" + pid;\n        filteredQuery += ' order by timestamp desc';\n        return sco.query(filteredQuery);\n    })\n    .then(function (providerTypes) {\n        //socket.emit('insuranceProviders', insuranceProviders);\n        var returnData = undefined;\n        returnData = providerTypes.slice(parseInt(req.query.start),parseInt(req.query.start) + parseInt(req.query.length) + 1);\n        if (returnData !== undefined) {\n            res.send(JSON.stringify({\n                \"draw\": parseInt(req.query.draw),\n                \"recordsFiltered\": providerTypes === undefined ? 0 : providerTypes.length,\n                \"recordsTotal\": recordsTotal,\n                \"data\": returnData\n            }));\n        }\n    })\n    .catch(function (error) {\n        winston.log('error', 'providerTypeData'); // display reason why the call failed;\n        winston.log('error', error); // display reason why the call failed;\n        res.send(JSON.stringify({\n            \"draw\": parseInt(req.query.draw),\n            \"recordsFiltered\": 0,\n            \"recordsTotal\": 0,\n            \"data\": ''\n        }));\n    })\n    .finally(function () {\n        if (sco) {\n            sco.done();\n        }\n    });\n\n```\nIf I remove the .finally block, I don't get the error but of course I wouldn't do that.\nError: Can't set headers after they are sent.\n    at ServerResponse.OutgoingMessage.setHeader (_http_outgoing.js:335:11)\n    at ServerResponse.header (/home/user/IdeaProjects/OASIS/node_modules/express/lib/response.js:700:10)\n    at ServerResponse.send (/home/user/IdeaProjects/OASIS/node_modules/express/lib/response.js:154:12)\n    at /home/user/IdeaProjects/OASIS/routes/providerTypeData.js:53:21\n    at runMicrotasksCallback (node.js:337:7)\n    at process._tickCallback (node.js:355:11)\n. What you have provided worked perfect actually, thanks!  I was confused because I saw both tasks and connect mentioned in the docs.  I figured I could still use connect but I suppose not.\n. I think I'm following.  Basically, I have a select list on the client side and based on the number of options selected, should result in the same amount of records (inserts) generated and executed. I updated my description (there was a typo).  The 2nd insert is not what I need but was generated as a result of t.batch.\n. This is what I tried:\njavascript\nfunction myTask(t) {\n        var queries = [];\n        stuff.forEach(function (val) {\n            queries.push(t.any(\"insert into user_location (id,active,timestamp,location_id,provider_id) \" +\n                \" select nextval ('user_location_id_seq'),'t',now(),$1,$2 \" +\n                \" where not exists (select * from user_location where location_id = $1 and provider_id = $2) RETURNING id\",\n                [val, req.body.providerid]));\n        });\n        return t.batch(queries);\n    }\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n...\njavascript\ndb.task(myTask)\n        .then(function (data) {\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n})\n        .catch(function (error) {\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n});\u0000\u0000\u0000\n. Yes it only executes one query.  I'll look again at what I have.\n. I think it's my fault with the 'stuff' array.  Please disregard.  Sorry. \n. Believe I found a solution:\nreturn (condition1 === 'true' || condition2 === 'true' ? t.query(query) : q(undefined))\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n. q is from https://github.com/kriskowal/q\nA tool for creating and composing asynchronous promises in JavaScript\nq(undefined) allows me to create a promise that does nothing really yet allows me to continue with my business logic and not throw an exception if neither condition is true.\n. ",
    "Rabinzon": "I don't know how i can delete this  issue. \n. Yes:) \n. Yes. \nI had this error \"Promise library must be specified\". It's appeared with version 2.5. and I didnt know what to do, because it was not in 2.4.\nI added, bluebird library. Now all works.\nI hope you understand me:) Thank\n. ",
    "santanubasu": "I probably should have provided a minimal test case to demonstrate the issue, here it is:\nvar promise = require('q');\nvar options = {\n    promiseLib: promise\n};\nvar pgp = require('../lib/index.js')(options);\nvar db = pgp(\"postgres://postgres:postgres@localhost/postgres\");\ndb.query(\"select 1;\")\n    .then(function (data) {\n        console.log(\"DATA:\", data);\n    })\n    .catch(function (error) {\n        console.log(\"ERROR:\", error);\n    })\nAs you can see there is no done() or finally() here.\nThe code above reaches then() when Q 1.4.1 is used.  It reaches the catch() when Q 0.9.6 is used.  Is that something you would expect?\n. Yes, agreed, looks like the issue is with Q's compliance then.  For those viewing this ticket, the version bump that takes Q from breaking to working in the above test case is 1.0.0 to 1.0.1.\n. ",
    "JaxCavalera": "i'm not i'll grab pg-monitor now\nand report back what's actually being queried\n. ok this is what it returns in the log with monitor running :\n11:49:14 connect(postgres@tagged)\n11:49:14 SELECT username FROM users WHERE username = jax\n11:49:14 error: column \"jax\" does not exist\n         query: SELECT username FROM users WHERE username = jax\n11:49:14 disconnect(postgres@tagged)\nChoose a New Username or Login { [error: column \"jax\" does not exist]\n  name: 'error',\n  length: 96,\n  severity: 'ERROR',\n  code: '42703',\n  detail: undefined,\n  hint: undefined,\n  position: '45',\n  internalPosition: undefined,\n  internalQuery: undefined,\n  where: undefined,\n  schema: undefined,\n  table: undefined,\n  column: undefined,\n  dataType: undefined,\n  constraint: undefined,\n  file: 'parse_relation.c',\n  line: '3087',\n  routine: 'errorMissingColumn' }\nand even if i put the field value in quotations :\n11:51:59 connect(postgres@tagged)\n11:51:59 SELECT username FROM users WHERE username = \"jax\"\n11:51:59 error: column \"jax\" does not exist\n         query: SELECT username FROM users WHERE username = \"jax\"\n11:51:59 disconnect(postgres@tagged)\nChoose a New Username or Login { [error: column \"jax\" does not exist]\n. adjusted the query and am using the live variable again (tried hard coding a string value of \"jax\" n just to check) same result though :\ndb.query('SELECT * FROM users WHERE username = $1^', uname)\n11:57:53 connect(postgres@tagged)\n11:57:53 SELECT * FROM users WHERE username = \"jax\"\n11:57:53 error: column \"jax\" does not exist\n         query: SELECT * FROM users WHERE username = \"jax\"\n11:57:53 disconnect(postgres@tagged)\nChoose a New Username or Login { [error: column \"jax\" does not exist]\nif I change the query to : \njavascript\nexport function regAble(uname) {\n    db.query('SELECT * FROM users WHERE username = ${name}', {\n        name: uname,\n    })\nit returns the following based on what I set \"uname\" to be\n12:00:51 connect(postgres@tagged)\n12:00:51 SELECT * FROM users WHERE username = '\"jax\"'\n12:00:51 disconnect(postgres@tagged)\nUsername is Available\n12:01:00 connect(postgres@tagged)\n12:01:00 SELECT * FROM users WHERE username = '\"jaxa\"'\n12:01:00 disconnect(postgres@tagged)\nUsername is Available\n12:01:05 connect(postgres@tagged)\n12:01:05 SELECT * FROM users WHERE username = '\"jaasdfadsfaxa\"'\n12:01:05 disconnect(postgres@tagged)\nUsername is Available\nall of those shouldn't be returning as true I would have thought since there is only 1 entry in my db and it's for lowercase \"jax\"\n. hmm well i did find something, I think I'm getting closer to the problem here, when i remove the \"JSON.stringify\" from the \"uname\" variable it comes back as follows :\n12:07:48 connect(postgres@tagged)\n12:07:48 SELECT * FROM users WHERE username = 'jax'\n12:07:48 disconnect(postgres@tagged)\nUsername is Available [object Object]\n12:07:58 connect(postgres@tagged)\n12:07:58 SELECT * FROM users WHERE username = 'somerand'\n12:07:58 disconnect(postgres@tagged)\n. I just don't understand why it returns \"available\" a.k.a true no matter what I enter.. \nthe query should be false if there is no match right?\n. but shouldn't that only happen if there are no errors with the query?\n. O.o ehm  but then how do i do a simple do this if no results are found.. do that if a result is found?\n. Ohhh so Catch is only catching errors with the actual query being executed.. not errors like.. no results found\nso all my logic has to be applied to the returned value (data) and processed inside the \".then(function)\"\nI think i misunderstood the content in the Learn by Example area\nThank you for helping resolve this and the extra bonus round of explaining how to handle results from a query with pg-promise.\n. no worries thanks again for your help and time with this, appreciated greatly.\n. Hi guys sorry I didn't get back to you in time, thank you for confirming this and looking into the situation further on my behalf.  I'll go with the advise provided and should there be any problems, I'll contact the correct people regarding.\n. ",
    "voxpelli": "The connect-pg-simple module should absolutely work with this module. If it for some strange reason doesn't, then I would be happy to take bug reports for it.\nconnect-pg-simple is meant to be the smallest and simplest session library for Postgres so that it can be compatible with and usable for as many Postgres based apps as possible.\n. ",
    "jmealo": "@vitaly-t thanks for the quick response. I knew you'd have built-in a way to do this before marking things as read-only :-).\nKudos on pg-promise, it really is a pleasure to use, especially with Koa!\n. I'm using an older version of Koa as most of the middleware isn't ready. I don't use too much third party middleware so your support of generators is getting me closer.\nThis is the middleware that I use to add a pgp to each Koa request. I do not create a new instance for each one, so I don't think that I can use your suggestions as my session information is only available on each Koa request (shown as this below).\nMy nginx load balancer authenticates the user and tells me which remote databases I need to connect to. I use this to decide which pgp instance to stick on the connection.\nIf I wrap a query in this.guc everything works as expected and I get the username and request_id in my postgresql.log and GUC settings.\n``` javascript\nfunction pgp(options) {\n    return function *pgp(next) {\n        var schema = this.header['x-nginx-mysql-schema'];\n    global.pgpConnections || (global.pgpConnections = initializePgp(options.config, options.slateConfig));\n\n    if (schema) {\n        this.pgp = global.pgpConnections[schema];\n    } else if (this.request.path === '/healthcheck') {\n        this.pgp = global.pgpConnections.shared;\n    } else {\n        this.throw(new Error('If you are not behind a load balancer; you must pretend to be. See README.md.'), 400);\n    }\n\n    this.sharedPgp = global.pgpConnections.shared;\n\n    this.guc = function(query) {\n        return `\n            SET spark.user_id = ${this.userId};\n            SET spark.role = ${this.role};\n            SET spark.request_id = ${this.requestId};\n            SET application_name = 'spark-api_${this.username}_${this.requestId}';\n            ${query}\n        `;\n    }\n\n    yield next;\n};\n\n}\n```\nHere is my test endpoint using this.guc:\njavascript\nfunction *getHandler() {\n    this.body = {\n        session: this.session,\n        headers: this.headers,\n        query: this.query,\n        body: this.body,\n        guc: yield this.pgp.one(this.guc(`\n            SELECT current_setting('spark.user_id') AS user_id,\n                   current_setting('spark.request_id') AS request_id,\n                   current_setting('spark.role') AS role,\n                   current_setting('application_name') AS application_name\n        `))\n    };\n}\nGiven this use case, should I just wrap each of the public methods on this.pgp to wrap the query parameter using this.guc? I'm not seeing any other immediately obvious solutions.\n. I'd agree with that, however, I believe the GUC decorating use case will take off with adoption of RLS.\nThe reason we're not using the built-in facilities for escaping is:\n1. We use those when using pgp, we want to decorate the queries we're generating automatically, not pass in this information with every query. If we use pgp's escaping here, it's harder to use it upstream.\n1. The data is trusted and cannot be tampered with\nI was just trying to see if you could confirm the only way to do this using a shared pgp object between requests would be to wrap each of the pgp methods \n``` javascript\n// this = koa request\nvar self = this;\nthis.pgp = {};\nfor (var method in pgpInstance) {\n   // not sure if I can set this correctly, I tried this already and ran into a problem with that\n   this.pgp[method] = function () {\n      // augment the query using \"self\" which is a reference to the koa request this pgp wrapper will be bound to\n      yield pgpInstance[method].apply(meh, arguments);\n   };\n}\n```\n. @vitaly-t I'm not instantiating a new pgp instance with each request, it is shared between requests. if I keep extending the same object (which I couldn't seem to do after instantiating) then every request will be executed on behalf of the most recent incoming request.\nMaybe I'm not understanding what you're suggesting.\nIt seems like extend can only be passed to options in the pgp constructor.\n. I'm just worried how this will work in transactions and tasks, it almost seems like I'd need to patch $query to make sure that the correct user is always set.\n. Thanks for sticking with me, the multiple databases isn't necessarily relevant here. The client is a school, and if Johnny, Suzy and Stanley all are in School XYZ, they'd all share a pgp instance.\nWorkflow:\n1. Incoming request (this) is supplemented with a shared pgp instance (this.pgp)\n2. Calling this.pgp[any-method] should prepend the query with SET statements populated from this.session populating relevant GUC variables for use in audit logging and RLS.\nI'm unsure of how to do this using extend as I'm not instantiating new PGP objects with each request (this).\nI'm of the impression that the only way to do this without instantiating a new pgp object per-request would be to wrap the methods outside of pgp so that this is in scope and can be used.\n. @vitaly-t I thought that you can only extend pgp once and only through the constructor?\n. @vitaly-t: Does this make the use case anymore clear? https://gist.github.com/jmealo/4dc1c59b8d31009f9262\nI would want a way to wrap each query issued by pg-promise with a BEFORE/AFTER statement that would set/clear the GUC explicitly.\nGoing off the documentation (didn't dig into the code), this might be similar to the transaction mode.\n. @vitaly-t Good idea! I've created #100 \n. @ceymard: If you're not afraid of abusing the GUC, you can set your application_name to match up with the user's session/request and then use syslog or otherwise stream your PostgreSQL log and match the log output back to a query/session/request.\n(application_name is the only GUC that you can set which is available for logging)\nYou can use NGINX/OpenResty to assign a random request id and include it in the HTTP headers. When Node gets it, it can append it to any logging it does. If you then pass that request_id usign SET application_name = request_id; in your SQL you'll accomplish unified logging HTTP, Node, SQL. I use full SQL logging and am able to see all queries issued in a request and see all logging made during a request through my entire stack.\nIf you don't find a work around and your use of NOTIFY and LISTEN are connection related, you may want to take a look at my project that allows live streaming of database changes in Node or as a standalone service: https://github.com/JarvusInnovations/lapidus\n. I'll check this out tomorrow... Thanks!\n. @vitaly-t I do not want to add a query parameter to each query.\nMy issue is, on each incoming request to my API, I attach a pg-promise insance. I will need each request to prepend/append session data related to the request object. I need to have that in scope and continue to share the connection pool.\nIs this possible?\nJust know that this is a pattern in Koa, not something exotic that I'm doing in express or with bare the core http server.\n. @vitaly-t I cannot re-open an issue in this repo; I can just comment.\nAdditionally, I do recall that I tried to \"fix\" this by wrapping all the pg-promise methods to perform this and ran into  an issue that the object did not allow modifications to be made.\nFor performance reasons, I'd really prefer not to wrap this in a Proxy, but it would be a work around.\n. Yes, I tried them last time that we discussed this, they lack the necessary scope to access the session object corresponding to the request. I sunk a few days into this the last time we discussed it couldn't find a way to do it via monkey patching; I would need to fork the library and make changes directly.\nI was hoping now that I introduced a concrete use case that shows the state-fulness and needed context.\n. extend is global though.\nMy node app has multiple pg-promise instances which maintain their own connection pool. Each instance connects  as a different db role with a SEARCH_PATH and permissions clamped down to that tenant/schema.\nIncoming HTTP requests are handled asyncronously, so even if I could extend and unextend the session context, the context sent to the service would always be the most recent request, not necessarily the one making the queries.\nDoes that make sense?\nIt looks like it may be possible within a task or transaction.\n. I use generators that sleep when waiting on IO. Imagine an API endpoint that makes a SQL query, then a REST request to an external HTTP API followed by a second SQL query.\nWhile that external API response is pending, other incoming HTTP requests to my API will use that pg-promise instance while the generator is sleeping because the SQL connection has been returned to the pool. (Correct?)\nHow I picture working around this:\nThe session data would be  SET to a custom GUC variable in the prepend and that GUC would be cleared/reset in the append. This is just for safety, it should be fine to only prepend.\nIn summary:\nMultiple requests share the pg-promise instance at the same time. It is not an instance per request. It is an instance per schema/tenant. The same pg-promise instance can be returned to the connection pool multiple times during a request.\nThat's why I need a way to always pass in the session context with each query, or somehow wrap it, so the context.pgp object which points to the pg-promise instance that request should be using will always set the GUC variables for that user to make sure audit logging, query logs, and row-level security rules match that user's web application session.\nI'm not opposed to a hack, but I'm unsure where I can fit it in.\n. Some possible solutions:\n- [ ] If I could pass in the request context into each query method and then extend it to look for that property and append/prepend to the query that could work.\n- [ ] I can make some kind of WrappedQuery object that when passed into any of the query methods accomplishes the same goal.\n- [ ] I could possibly override the way that query parameters are parsed to check if the first/last item passed is a session object. I can remove that before passing it for SQL formatting/placeholder replacement and modify the query accordingly.\n- [ ] Create a tiny wrapper in my sql provider module that could support prepend/append templates with static and dynamic variables by taking the ctx into the query. \nEager to understand from someone who understands the internals the best way to accomplish this as it will literally impact every call to pg-promise in my app.\nThe query event seems to late in the pipeline as the final SQL string is already ready to process, so there would be no pass in the session.\n. How would I extend the event get the entire request object if I don't want to pass the request object as a query parameter (not a parameter to a query method, that would be fine, I don't want to pass it as an actual value that would go in as $1 in the SQL)?\n\nAll pre-defined methods and properties are read-only, so you will get an error, if you try overriding them.\n\nIf I can't redefine the extend before every single query that I execute, multiple times per request, I don't see how extend lets me do what I want to do? I don't see how I can keep scope to the request object since multiple requests use the pg-promise instance concurrently.\nThe lifecycle of my application is not one pg-promise instance per request.\nRequest middleware supplements each request context with a pre-initialized pg-promise instance. One is not created for each request.\n. It looks like I could do it with QueryFile but it's locked down for internal use by the library. That's how most of my ideas/hacks have been thwarted.\n. @vitaly-t: Thank you for the example. I don't think that I can do it without the extra parameter because we're re-using the instance and cannot call extend multiple times, but the extra parameter should work if I do this.\nI'm going to try wrapping every query method and that should work... not sure if it'll work with tasks and transactions though.\nLet me try that and I'll report back.\n. Sorry, I don't follow, what would that look like?\n. @vitaly-t Okay, that last bit makes sense. I'll give this a shot. Thanks\n. I ended up just wrapping the basic query methods for now. I'm using my own transactions now (I hope that's sanctioned)... so I haven't had to use any of the built-ins for tx/tasks.\n``` javascript\n'use strict';\nfunction PgpWrapper(pgp, ctx) {\n    this.pgp = pgp;\n    this.ctx = ctx;\n}\n/ This wrapper is verbose and minimalistic in order to optimise for:\n    1. Garbage collection [each instance is just two references with methods on prototype, not each instance]\n    2. Execution speed / V8 Optimization [ES6/ES5 meta-programming features are slow. V8 can optimise simple objects.\n    3. Code auto-completion [explicitly defining the methods should work with most IDEs, looping over an array may not]\n/\nPgpWrapper.prototype.none = function(query, values) {\n    return this.pgp.none(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.one = function(query, values) {\n    return this.pgp.one(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.many = function(query, values) {\n    return this.pgp.many(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.oneOrNone = function(query, values) {\n    return this.pgp.oneOrNone(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.manyOrNone = function(query, values) {\n    return this.pgp.manyOrNone(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.any = function(query, values) {\n    return this.pgp.any(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.result = function(query, values) {\n    return this.pgp.result(this.ctx.guc() + query, values);\n};\nmodule.exports = PgpWrapper;\n``\n. @vitaly-t: I'm fairly certain that I need to wrap pg-promise for this to work as a shared instance of pg-promise between requests is not concurrency safe (if multiple requests areyield`ing).\nThe only way I can see getting around that is to pass state in somehow using custom formatting, which I'm still trying to wrap my head around.\nIf I want to do this 100% transparently, I believe wrapping is the way to go.\nTo make this work inside of tasks, could I just need to wrap this inside the task to also use the wrapped methods? I haven't read the code yet.\nThere's a low-level query method that'd be ideal to wrap but monkey patching isn't concurrency safe. \nFrom a meta-programming approach, you'd use a Proxy that traps all function calls, swaps out query with a patched version that prepends the query no matter how pg-promise uses it (this would work within tasks). I think this could be concurrency safe because we're guaranteed run to completion within the trap but I haven't given it careful consideration as I don't think it would be performant/advisable.\n. @vitaly-t:\nI was able to wrap the task method, however, as you mentioned, it doesn't look like tx will be wrappable.\nMy testing included the queries being wrapped. I'm not sure if task behaves as documented when wrapped like this.\nIt might be a good idea for me to try to modify the unit tests to run with my wrapped version to check for regressions possibly.\n``` javascript\n'use strict';\nrequire('generator-bind').polyfill();\nfunction PgpWrapper(pgp, ctx) {\n    this.pgp = pgp;\n    this.ctx = ctx;\n}\n/ This wrapper is verbose and minimalistic in order to optimise for:\n    1. Garbage collection [each instance is just two references with methods on prototype, not each instance]\n    2. Execution speed / V8 Optimization [ES6/ES5 meta-programming features are slow. V8 can optimise simple objects.\n    3. Code auto-completion [explicitly defining the methods should work with most IDEs, looping over an array may not]\n/\nPgpWrapper.prototype.none = function(query, values) {\n    return this.pgp.none(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.one = function(query, values) {\n    return this.pgp.one(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.many = function(query, values) {\n    return this.pgp.many(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.oneOrNone = function(query, values) {\n    return this.pgp.oneOrNone(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.manyOrNone = function(query, values) {\n    return this.pgp.manyOrNone(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.any = function(query, values) {\n    return this.pgp.any(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.result = function(query, values) {\n    return this.pgp.result(this.ctx.guc() + query, values);\n};\nPgpWrapper.prototype.task = function (p1, p2) {\n    p1 = p1.bind({}, this);\nif (p2) {\n    p2 = p2.bind({}, this);\n}\n\nreturn this.pgp.task(p1, p2);\n\n};\nmodule.exports = PgpWrapper;\n``\n. I'm totally for making a sample application, however, I don't have a way of even moving forward ifextend` is the only correct way to do this.\nI do not want to extend the protocol, I need every query to at least once (more than once is ok) issue a SET statement. The contents of the SET statement is dependent on the incoming HTTP request's session information, which must be in scope. In order to be concurrency safe we cannot mutate a single shared copy of pg-promise.\nQuestions about extend:\n1. In the documentation, the extend example defines an options object. I assume that this is to pass to the pg-promise constructor on a per-instance basis. In which case, I would need a new pg-promise instance per incoming request because I need to be bound to the context of my request.\n1. How do I use extend to override the built-in methods so that all use of pg-promise? I'm not seeing how this would work.\nI also do not see how extend allows me to override the behavior of the built-in methods.\nThe only thing I see that seems even remotely possible would be to make an SqlQuery type with a String prototype to keep a reference to the request. Then a custom formatter could pull the request off and prepend/prefix the query?\n. @vitaly-t: I'm interested in getting this to work. I'm not sure if multiple instances would help me personally.\nI would like to commend you for being so diligent in following up and trying to understand my use case. You're a real standup guy :+1:.\n. @vitaly-t: Hello again! Since this keeps coming up, for the record: what's the recommend way to prepend/append a single SET statement with a per-request unique identifier*?\n*Allowing re-use of connections/pg-instances between users/requests and use of pg-monitor.\nsql\nSET application_name = <per-request-unique-uidentifier>\nI've been version locked with my nasty hack and am just now trying to fix this the correct way :-). @vitaly-t Use case: if you set the application name GUC to a combination of a session id and correlation id  you can match any PostgreSQL log output to a given correlation id or web application session.  The application_name GUC is the only way to influence the CSV logging at run-time. Extending Postgres to do this would be an issue for those using hosted solutions.\nSo to be clear, I do not need a transaction per query, I need a SET statement prepended to allow queries issued by pg-promise so that the query is attributed to the correct correlation id. It's strictly for correlating logs and can be used in RLS rules to reference the web application user.\nIf there is something I misunderstand about the internals of pg-promise that would break the above use case please let me know, I've had a shim in place in production for 2 years but I'd really like to get rid of it and use the latest version of pg-promise. The shim wraps each method and munges the input strings. I only have very limited use of the library because of this.. @amenadiel @vitaly-t Thank you for your suggestions, however, they do not accomplish the stated goal, just to re-iterate:\n\nThe app name can be included in PostgreSQL logging.\nApp name is the only value that can be output via standard logging AND set per-query via GUC.\nSetting the app name enables us to output an arbitrary value that will be associated with a given query in the logging facilities provided by PostgreSQL.\nIf we prepend a SET statement to each query that sets the application name to a correlation/request id/web application user name we're able to do application-level/user-level tracing without adding logging facilities to our web application.\nTo make absolutely sure that every query issued by pg-promise on behalf of a given request (or given user), we'd want to inject values into a SET statement along with each query.\n\nEvery suggestion that I've received does not accomplish the goals above. I had to wrap pg-promise and am locked into an older version which allowed such monkey patching.\nWhen using something like Koa where we can have a middleware that presents pg-promise to the request context it would be hepful to set these state variables without entirely wrapping pg-promise.. @vitaly-t: This works with older versions of pg-promise but I cannot upgrade, hopefully this makes my user case clear: https://gist.github.com/jmealo/58c96de7add64ebbab86a637a46d3417\njavascript\nguc = {\n    'spark.user_id': ctx.userId,\n    'spark.role': ctx.role,\n    'spark.request_id': requestId,\n    application_name: `spark-api_${ctx.username}_${requestId}`\n};\nThis populates a set of GUC variables that can be used both for logging and row-level security.\nThe goal here is to enforce that a given set of GUC variables be SET before issuing every query leaving pg-promise. The GUC variables depend on some state, so there would need to be a way to bind that state.\npg-promise has tons of facilities for modifying the query, however, I have been unable to find one that will work appropriately within a request/response workflow where we want to use state from the request in the GUC variables (see example above).. ",
    "dorongrinstein": "amazing response time. THANK YOU!!!\n. sorry, one more question -- can this be rolled into a batch (multiple part) transaction?\n. var r = db.tx(function(t) {\n            t.batch([\n                t.none('update foo set field1 = ${f1} where id = ${id}, {f1: 10, id: 1}),\n                t.one(\"insert into foo_audit values ('1', 'abc')\")\n            ]);\n        });\nI want the transaction to fail if the update did not affect any rows. thanks\n. I'll check into it in a few. In a meeting now. THANK YOU again for the super prompt response!\n. I see what you did there. Perfect! thank you!!\n. You are a genius!! When you visit L.A., ping me, I owe you a beer!\n. ",
    "ferdinandsalis": "Was just wondering if this is possible. You got me covered! Thanks.. How would I chain them?\nAmazing support by the way.\n. I want to retrieve all artists with their related artworks.\n. Sorry for not providing more detail. I feel inadequate to go more into depth.\n. Thanks, you have been very helpful.\n. This is what I ended up with. Doesnt feel too elegant though \u263a\ufe0f.\nrep.task('artists with artworks', t =>\n  t.any(sql.all).then(artists => {\n    return t.batch(artists.map(artist => {\n      return t.any('SELECT * FROM artworks WHERE artist_id=${id}', artist)\n        .then(artworks => {\n          artworks.length && (artist.artworks = artworks)\n          return artist\n        });\n    }))\n  }))\n. Yup! \ud83d\udd96 Now figuring out where that would go in the repository pattern. Thanks again. I am enjoying learning sql through your library.\n. I see the same issue with the website. Hard refresh renders no change.\n. Yes it does. No more overlapping. \ud83d\udc4d \n. @vitaly-t if you want I could tackle that next week for you. I am a front end dev by trade ;)\n. Yes better. Though I still can\u2019t scroll the list on the left when my window size is not heigh enough. Giving the .nav container a fixed height, e.g. position: fixed; top: 0; bottom: 0; and setting the overflow-y: scroll should do the job.\n. I did \u2014 here https://github.com/graphql/graphql-js \u263a\ufe0f. \ud83d\udc4d. ",
    "arm5472": "Thank you all. This is really helpful.\nI believe the code below will be more optimised, especially when a lot of rows are returned. (in my test, 204,000 rows were camelised in 3019ms compare to 5600ms using the function above)\njavascript\nfunction cameliseColumnNames(data) {\n    const firstRow = data[0];\n    const camelObj = {};\n    Object.keys(firstRow).forEach((prop) => {\n        const camel = pgp.utils.camelize(prop);\n        if (camel !== prop) {\n            camelObj[prop] = camel;\n        }\n    });\n    for (let i = 0; i < data.length; i++) {\n        const row = data[i];\n        Object.keys(camelObj).forEach((key) => {\n            row[camelObj[key]] = row[key];\n            delete row[key];\n        });\n    }\n}. @vitaly-t many thanks for your reply and pointing out the matching issue.\nI added the if condition which you mentioned and also only selected columns which required camelisation (38 columns in my new test). The result still shows a big gap with the same number of records; 1933ms vs 3529ms.\nTry it for yourself, if you have a moment, please.. Thanks for your test. \nIt turns out that the more columns you have, the more beneficial my code would be compare to yours.\nYou have only 5 columns in your test.  If you have 10 or 20 (or in my test 38), you see the gap in performance widens hugely. (make 30 columns and it runs 3 times faster)\nI guess it's now down to users as to which one to pick, based on their situation. In my case, the loss with small number of columns is little, but the gain with more columns is substantial. But I understand this is not the case for everyone.\nThanks again @vitaly-t . The main difference is I don't loop through all rows for every camelised column. I only go through each row once, replacing all necessary columns. You can see the performance difference if you add more columns to your test.\nAs I mentioned before, it's a matter of choice. I'm writing an API which returns a lot of data (both columns and rows). But for the calls that return small data, the difference is negligible anyway. . I'm using node 8.3.0.. This is very helpful. Thanks a lot @vitaly-t .. ",
    "Halynsky": "Hello. First of all, thank you @vitaly-t for replay! =)\nI tried your solution and it works nice.\nI have one more additional question to this case.\nWhat if after retrieving an object from the database I want to modify it in code and update it in the database with 'this'  syntax. Or for example, I have an object with typescript interface based on camel case and I want to insert it into the databesa.\n'this'  syntax.\njavascript\nconst obj = {\n    one: 1,\n    two: 2\n};\ndb.query('INSERT INTO table(${this:name}) VALUES(${this:csv})', obj);\n//=> INSERT INTO table(\"one\",\"two\") VALUES(1, 2)\nI will try to explain with code what I want to do\n```typescript\nexport class User {\n    name?: string = null;\n    password?: string = null;\n    registrationDate?: Moment = moment.utc();\n}\nlet user: User = new User();\nuser.name = \"Taras\"\nuser.password = 'pass'\ndb.none('INSERT INTO users (${this:name}) VALUES (${this:csv})', user)\n```\nSo now I have backward problem. I have camel cased property names but in the database, they are underscore cased. Is there some similar solution for it?\nBest regards =). ",
    "kimmobrunfeldt": "I have researched and there's nothing more to research on Heroku's part. Was it Heroku or not, I need to somehow see how many Postgres connections this library is creating. So my question is, how can I debug the library's internal behavior? \nI started with the highest level library instead of opening issue to node-postgres or pool because I was assuming you have had to debug the connection pooling when developing this lib.\n. The activity seems normal when running the query. I might need to open a Heroku support ticket after all.  Our endpoint is as simple as it could and there are no loops or other behavior which could cause this inside our app. Sorry for bothering and thanks for your help!\n. We ended up upgrading our Heroku Postgres plan from free to paid one: Hobby basic. At least we haven't got any errors after the upgrade. We are still monitoring the situation but I have pretty high confidence that it fixed the issue.\n. Yep seems like it. That was quite disappointing because in the database management UI, they stated that we were using 1/20 connections when using the free plan. So I assumed we would have 20 connections available.\n. ",
    "reinvantveer": "Yes, I'm seeing it thrown. Thanks for the elaborate answer and example!\n. Thanks!\nDen m\u00e5n 21 mar 2016 06:35Vitaly Tomilov notifications@github.com skrev:\n\nAnswer to the question was later elaborated here:\nhttp://stackoverflow.com/questions/36120435/how-to-ensure-database-connection-before-starting-app-server-with-pg-promise\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly or view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/81#issuecomment-199135113\n. \n",
    "ceymard": "Ah found a way ; using event.client.on('notice', ...)\nFor future people who would be wondering ;\njavascript\nrequire('pg-promise')({\n   /* ... */\n  query: function (event) {\n    event.client.on('notice', function (notice) { console.log(notice) })\n  }\n}\nIs it possible to have the notice generated by one particular query ? Using the above code attaches a listener every time I make a query to seemingly the same client object...\nI do not see how pgp.as.format can help though...\nAs for pg-monitor, it looks like it is a command line tool which is no use for me since I actually want to handle the notice in my application (and thus send them to a client, etc..)\n. I use plv8, with which I use the function plv8.elog(NOTICE, 'my message')\nIt is very helpful to me as I use it to keep track of what's happening if very long running functions (30 minute update processes, some whacky algorithms that run entirely inside the database, etc..)\nI am thus interested in seeing messages generated by a particular query (like SELECT my_func() which could produce a couple hundred of logs over the course of a long time). As the functions are called by a nodejs server and not manually inside pgadmin, I still am looking for a way to get those logs and do stuff with it.\n. Ah yes, they're working with pg's pub/sub implementation.\nDoes this mean that they're bypassing the pool ? Are clients created in this way limited to one query at a time (I would expect it to be the case) ?\n. @jmealo and @vitaly-t, thanks for all the input, I'll try to figure out how I want to do it.\nI'll probably try to stay clear of the GUC though, seems like a recipe for headaches :)\n. ",
    "leemhenson": "Well, I suppose I raise this question because I'm learning about the library in conjunction with looking at vitaly-t/pg-promise-demo.\nIn that project, you do interact with a schema:\nhttps://github.com/vitaly-t/pg-promise-demo/blob/master/db/sql/products/create.sql\nI'm just thinking about the use of this library inside a real-world project, and a natural first step is to think about that sort of stuff.\n. Yep, makes sense.\n. ",
    "jadencarver": "I'm using Google's traceur package: https://github.com/google/traceur-compiler - If you look at the index.es5.js file it shows what it transpiles to.  It might be out of date.  I'll update it.  I haven't tried it using Babel, but a friend of mine has and had no luck either.  I understand if it's not supported, I just really want to be able try/catch async code.\n. Yes.  I just updated the compiled version.  It was out of date from my earlier attempt.  The .then call works, which is how I'm proving to myself that I didn't do anything wrong - so it's just await/async not working for SQL.  Experimental features are experimental I guess, but it would do wonders to clean up transactions in Node.js.\n. Do you have an example that uses a generator?  I think that will help me figure out what's wrong.  Technically, I haven't even gotten to the transaction part, because I can't get await/async to work for even a single query.\n. Got it, thank you!\n. ",
    "jordansnyder": "Stupid mistake on my part, I was calling\nreturn t.one(app.services.datastore.pg._sql('insertDeviceTokenRecord.sql', paramObject));\nwhen it should be \nreturn t.one(app.services.datastore.pg._sql('insertDeviceTokenRecord.sql'), paramObject);\n. Thanks I appreciate the advice...I have seen that, but this is development code just to get things working at a basic level. I will be refactoring today and building a module to load the files, similar to what your examples show.\njs\n\nOn Feb 5, 2016, at 3:22 AM, Vitaly Tomilov notifications@github.com wrote:\nIf you are really using QueryFile like this:\nreturn t.one(app.services.datastore.pg._sql('insertDeviceTokenRecord.sql'), paramObject);\nthat is wrong, because as per documentation here:\nYou should only create a single instance of QueryFile per file, and then use that instance throughout the application.\nand here:\nFor any given SQL file you should only create a single instance of this class throughout the application.\nAlso see pg-promise-demo for a good example of how it should be used, although it is mostly highlighted here as well.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "milabs": "Thanks for the explanation. It's really powerful. Why not to add this helper function to the library? Don't think that my case is unique...\n. OK, but wouldn't it useful to have something like pgp.as.keys() and pgp.as.vals()? Anyway, as your answer has the solution it would be nice to have faq/wiki reference for that.\n. Just for the possible supposition of query formatting it may be useful to have something like:\ndb.query(\"INSERT INTO table(@1~keys) VALUES(@1~vals)\", obj)\nWhere @1~keys and @1~vals is the first object's keys and their values. Strictly speaking, ~keys and ~vals are not needed there, but check for paired @1 / @1 tokens required.\n. Ok\n. The question is no longer actual for me as I've found the workflows wiki page. Tasks are great.\n. Thanks. Great library.\n. ",
    "nlf": "I don't see how extending the object is any better than running JSON.stringify on it. Both require manual modification of the input parameters. What I'm talking about is enabling the definition of that format in the query instead. \n. I mean the parameters being input, in your example 'test'. Doing asJsonArray(test) is no different than doing JSON.stringify(test). I don't always have control over the input parameter, but I do have control over the query. If I can set the format type in the query it would make things much simpler\n. The current manual mode operates on the input, I want to define the formatting in the query. For example:\nInstead of\nquery('INSERT INTO \"table\" (\"column\") VALUES ($[value])', { value: pgp.as.json(input) })\nnote that the formatting is done in the input parameters\nI want to do\nquery('INSERT INTO \"table\" (\"column\") VALUES ($[value:json])', { value: input })\nnote how in this case I'm specifying the format for the column as part of the query rather than manipulating the input parameters.\n. would you accept a pull request adding this feature?\n. where this came up for me was in using the SQL file feature. i have several SQL files that do an INSERT (very similar to the example above, where i have a JSON column that i'm storing an array in). I'm passing user input straight to these SQL files with a simple method.\nwithout the ability to specify the format of a parameter in the query itself, i have to document and inform users that they must stringify or otherwise format their input before passing it to the method. this is incredibly unfriendly for the user, so i was hoping to come up with a solution at the pg-promise level that would resolve my concerns as well as hopefully helping someone else out further down the road.\nif you think it's not worth adding, i'll have to implement it on my end and wrap all the calls to the SQL files with another method to adjust the formatting.\n. the partial formatting is applied before the user's input though, isn't it? the documentation says it's for static parameters. the parameters i need to apply specific formatting to are dynamic and passed in by the user\n. yeah, so that wouldn't work for my use case then\n. but doing so i still need to manually specify the format for the user's input, don't i?\n. so that still leaves me in the same position, having to alter the user's input or require them to do it themselves. doesn't really help.\n. Wow, thanks! I thought I was going to have to do it in my own code this morning. This is great!\nCurious what your thoughts are about having one of these options for every format? Seems like if one is supported might as well support them all\n. I agree, it just seemed nice for consistency purposes. I do have a use case for a :csv but everything else is simple enough it's not needed\n. i use it for IN queries, like:\nquery('SELECT * FROM \"table\" WHERE \"some_column\" IN ($[items])', { items: pgp.as.csv(['one', 'two', 'three']) })\n. awesome! thank you!\n. Yes, it's working perfectly. Thanks again\n. it's being used in raw queries in a private project that consumes muckraker, so yeah you won't be able to see it here. i can definitely verify it's working perfectly though\n. we're using hapi and passing request.query directly as the argument to a query file. i'm not sure where or why it's happening, but something is causing request.query to be a plain object. something roughly like this should reproduce it:\njs\nvar query = new QueryFile('./query.sql');\nvar param = Object.create(null);\nparam.name = 'test';\ndb.query(query, param).then((res) => {\n  // never reached\n});\n. i did see, thanks for getting it released so quickly :)\n. ",
    "WestleyArgentum": "Thank you both so much for going through all of this -- just ran into the need for :json, super pleased it exists now\n. ",
    "jl-": "New to PostgreSQL, Long way to go..\nYou are right, SELECT * FROM ... works as expected, and I see the difference now. \nThanks so much! :+1: \n. Thanks for reminding me of this, I'll definitely keep it in mind for any value formatting. :clap: \n. After going through pg.as.* and some of the source code in formatting.js, now I have a clearer view of the great formatting support with this lib. \nTips listed in Common Mistakes are really helpful, I should definitely try out pg-monitor next. :100: \n. ",
    "esetnik": "Hi, I was hoping for one using db.tx in conjunction with this.batch and two generator functions.\n```\nfunction* a(data) {\n  yield this.none(\"something\", data);\n}\nfunction* b(data) {\n  yield this.none(\"b thing\", data);\n}\nfunction doit(data) {\n  db.tx(t => {\n        return this.batch([a(data), b(data)]);\n    })\n    .then(function (data) {\n        // success;\n    })\n    .catch(function (error) {\n        // error;\n    });\n}\n```\n. The example above was definitely contrived. My use case is to ensure that a sequence of inserts occurs in entirety or not at all. I'd prefer to use several functions to separate out the complicated logic and processing required to prepare for the inserts. I was hoping to do as you've demonstrated above.\nA few questions:\n1. Is it required to return yield this.any instead of this.none when using generators even if there is no result?\n2. is it possible to pass data into a() and b() by capture?\n3. Is it possible to yield the entire transaction so that a generator-based framework like koa can easily   initiate the transaction without consuming the promised result?\n. Yeah it's totally true that I can accomplish this without the use of generators I was just hoping to use the cleaner syntax to avoid promise hell.\n. ",
    "tkellen": "Ah! No, I haven't. This seems like exactly what I was looking for. Thanks!\n. Radddddd. Are you on twiter? I just posted this: https://twitter.com/tkellen/status/728670878989422592\n. thanks @vitaly-t, that's a super helpful example--I may play with that a bit in the future!\n. Awesome!. hey @vitaly-t, you are too fast! thank you so much! can you share an example usage?\n. Indeed it is, thanks again @vitaly-t!\n. That's awesome @vitaly-t! Will you be spinning up any examples for this? I'm pretty sure I understand what you're describing based on the release notes but I haven't had a chance to try it yet.\n. ",
    "Ashtonian": "Thanks for the response! Based on that information and the advance demo app you provided the basic repo pattern, I have some questions on the implementation.\nWhere you would expose that higher level repository - In that example would you inject dependent repositories into the other ones?\njavascript\n extend: obj => {\n        obj.childRepo = repos.child(obj)\n        obj.parentRepo= repos.parent(obj,repos.child(obj));\n    }\nor would you just do it at a higher level? \njavascript\n extend: obj => {\n        obj.ChildRepo = repos.child(obj)\n        obj.parentRepo= repos.products(obj);\n    }\nModel.js:\njavascript\nvar parentRepo= require('parentRepo.js')\nvar childRepo = require('childRepo.js')\nmodule.exports = {\n    Add: item => parentRepo.add(item).then(childRepo.add(item.children)\n // TODO: somehow convert to batch or .tx\n};\nI'd like to be able to keep the table/relationship code in one place and use a transaction when inserting into multiple tables, and I'm having a hard time of seeing what an actual implementation of that would look like in javascript as this is all very new to me. \nOn a similar note - what would be the best practice for exposing the pgp library to one of the repos as in that example project, without adding circular dependencies or adding another  require('pg-promise') statement to the repo itself. Adding another require statement would work but seems like there is a better way. \n``` javascript\nvar options = {\n    promiseLib: promise,\nextend: obj => {\n   // TODO: injecting it here is circular\n    obj.users = repos.users(obj,pgp);\n    obj.products = repos.products(obj);\n}\n\n};\nvar config = {\n    host: 'localhost',\n    port: 5432,\n    database: 'pg-promise-demo',\n    user: 'postgres'\n};\nvar pgp = require('pg-promise')(options);\n```\n. ",
    "AriaFallah": "Needed to call done()\njavascript\ndb.connect()\n  .then((con) => console.log('db connection successful') ? null : con.done())\n  .finally(pgp.end);\n. @vitaly-t why not just enforce always using the new keyword instead of doing this?\n. Thank you! I appreciate how well maintained this library is. Finding it, and seeing 0 issues was really reassuring.\n. So would I be calling sequence multiple times? Or would I be returning an array of queries from source?\n. Okay so this is where my confusion comes in.\nIf source only returns a single query, and you only call sequence once, how does sequence execute multiple queries?\n. So given what I know now, my call would look something like this\n``` js\nconst data = [{ id: 1, colToUpdate: 'newVal' }, { id:2, colToUpdate: 'newVal' }];\nfunction source(index) {\n    const row = data[index];\n    // create and return a promise object dynamically,\n    // based on the index passed;\n    if (index < 100000) {\n        return this.any('UPDATE table SET colToUpdate=${colToUpdate} WHERE id=${id}', row);\n    }\n    // returning or resolving with undefined ends the sequence;\n    // throwing an error will result in a reject;\n}\ndb.tx(function (t) {\n    // t = this;\n    return this.sequence(source);\n})\n    .then(function (data) {\n        // success;\n    })\n    .catch(function (error) {\n        // error;\n    });\n```\nI have two questions about this.\n1. When does the this.any update call return undefined?\n2. Eventually, in this case, when index = 2, it'll be out of bounds, and it'll return an error right? Should I opt to use { limit: data.length } to avoid this?\n. @vitaly-t ah alright. So is it different from the error object I posted here, or is there specific reason why I'm not getting the info here?\nThis is what caused it:\njs\nreturn db.tx(function(t) {\n  return t.batch([\n    t.none('CREATE TABLE IF NOT EXISTS $1:value($2:value)', [name, types.join(',\\n')]),\n    t.none('INSERT INTO $2:value($3:value) VALUES $1', [data, name, cols.join(',')]),\n    t.none('GRANT SELECT ON $1:value to readonly', name)\n  ]);\n});\n. And is it only available inside of the pg initialization or is there somewhere else I can access it?\n. @vitaly-t oh nevermind \ud83d\ude05  thank you!\n. This is what mine ended up looking like:\njs\n// Configure the database connection\nexport const pgp = postgres({\n  promiseLib: Promise,\n  error(err, e) {\n    if (process.env.NODE_ENV === 'production') {\n      reportError(`${err}\\n${JSON.stringify(e, null, 2)}`);\n    } else {\n      console.log(err, e);\n    }\n  }\n});\n. @vitaly-t \nAlright, thank you!\nAlso I'm not surprised you haven't heard of it, because if you ctrl-f for standard_conforming_strings here you'll see that since 9.1, ~6 years ago, it's defaulted to on instead of off.\n\nThis controls whether ordinary string literals ('...') treat backslashes literally, as specified in the SQL standard. Beginning in PostgreSQL 9.1, the default is on (prior releases defaulted to off). Applications can check this parameter to determine how string literals will be processed. The presence of this parameter can also be taken as an indication that the escape string syntax (E'...') is supported. Escape string syntax (Section 4.1.2.2) should be used if an application desires backslashes to be treated as escape characters.\n\nAlso as a side note, perhaps dollar-quoting could be the solution to escaping issues.. ",
    "cortopy": "That's what I thought from reading the docs. However, the problem is that after my program finishes, it still hangs until connection is dropped. \nIf there was a way to close the connection, the app would be able to exit when it's all done. \n. That's what I was looking for! Thanks\n. ",
    "jgoux": "You need to be the owner of the repository to create a room for the project.\nYou just have to connect via Github and create a room (you will be able to pick your repo) :+1: \n. Thank you ! :sparkler: \n. ",
    "coveralls": "\nCoverage remained the same at 100.0% when pulling fcc05b0e9316b0b26d1b1e13f25c3c0e14aaed34 on gitter-badger:gitter-badge into 01e07e144368df0be71af7314f8e4266f795ceb3 on vitaly-t:master.\n. \nCoverage remained the same at 100.0% when pulling 4764c4556b180c7afa05df4be7ad84e50139e429 on warrenseymour:patch-2 into b3f3694a6fd787c8a4b5635c90a0b20a7691f069 on vitaly-t:master.\n. \nCoverage remained the same at 100.0% when pulling 6220d09ff5bd46ff25712e346bcd236ec53e8942 on djMax:master into b6259e6a190b431179360d3872dfcc0bc63e991c on vitaly-t:master.\n. \nCoverage remained the same at 99.934% when pulling c4b72092cff0ec0fbe068932684dca36bafe1b58 on jcristovao:typings into a5e0bcd605d93dc367005e9c90e9815bd757c1a2 on vitaly-t:master.\n. \nCoverage remained the same at 99.94% when pulling baab052f4899e4effe7d85f378f35bc34de390f1 on valeriangalliat:hotfix/regression-node-interpreter into 7b889619e05e0895d8711e34b652e6aac12c4997 on vitaly-t:master.\n. \nCoverage remained the same at 99.94% when pulling 86f6631d54cc9b7afc1effa86d879ae5ff19c941 on valeriangalliat:hotfix/regression-node-interpreter into 7b889619e05e0895d8711e34b652e6aac12c4997 on vitaly-t:master.\n. \nCoverage remained the same at 99.94% when pulling 3b6c2fadecc1ff725a90dc7d9ed1bf274240e52a on valeriangalliat:hotfix/regression-node-interpreter into 7b889619e05e0895d8711e34b652e6aac12c4997 on vitaly-t:master.\n. \nCoverage remained the same at 99.883% when pulling 0933ae814693bd3b4c707fc65d2546e32fcb4a2e on 5.0-beta into 96108b13e862b7564b05dcf37ab0a87fa122a5ca on master.\n. \nCoverage remained the same at 99.886% when pulling d8187982105e63c201097003c22bd1c01c6aad37 on nlf:patch-1 into 4f8ae4edd934e5ae60493fc9ed1726db7abf3a4a on vitaly-t:master.\n. \nCoverage remained the same at 99.886% when pulling def2c0080b0cea994bb68eb5b11a63240c60a2d9 on SweetIQ:master into 5f1f454fdf70efe0b715e247ffa0237ca77dbb6a on vitaly-t:master.\n. \nCoverage remained the same at 99.886% when pulling f6bbc68dfdcf018858d32767a8bc8b26354dee98 on andrewharvey:patch-1 into a50ed0c4be259d51e5907db1754c0e53773a0f58 on vitaly-t:master.\n. \nCoverage remained the same at 99.887% when pulling acc4a2c0b2a28773c75378af52a0bfd57db7afe7 on oliversalzburg:patch-1 into 7ef32f6baf69366d9e6ad20b54da4fa094c165ea on vitaly-t:master.\n. \nCoverage remained the same at 99.887% when pulling 4400fcfe42e366ccea7691fd1d0a16f09541a62d on SweetIQ:master into 7ef32f6baf69366d9e6ad20b54da4fa094c165ea on vitaly-t:master.\n. \nCoverage remained the same at 99.887% when pulling 5492816afbfc5b857da83c8a80121338fa42f29c on jcristovao:patch-2 into f21e072046c0bcc29b68c6844b9bc87d9424b625 on vitaly-t:master.\n. \nCoverage decreased (-0.2%) to 99.717% when pulling 9db9f5877e90f317b130047cc919619df1f17ace on boromisp:bluebird-3-cancel into 5ed380e2e1a34877be5d72038a832428710004d5 on vitaly-t:master.\n. \nCoverage increased (+0.0003%) to 99.887% when pulling 311d19d50bd16521151823401126d0294781af40 on boromisp:bluebird-3-cancel into 5ed380e2e1a34877be5d72038a832428710004d5 on vitaly-t:master.\n. \nCoverage decreased (-42.6%) to 57.239% when pulling 094abe2c476a30ab19b4e9007d1f0a7b5e7587ef on boromisp:arg-cancel into 5beb5565b1c7a8544433db2da3ca798070b625f6 on vitaly-t:master.\n. \nCoverage remained the same at 99.887% when pulling 9009fee694c7b04a11088ffe88da1f3db99a28e4 on mauriciovigolo:master into 5beb5565b1c7a8544433db2da3ca798070b625f6 on vitaly-t:master.\n. \nCoverage remained the same at 99.887% when pulling 06b6eaea8d572a85f531c7dcee7645287ed0e30f on cmelone:patch-1 into 5beb5565b1c7a8544433db2da3ca798070b625f6 on vitaly-t:master.\n. \nCoverage remained the same at 99.887% when pulling aaccdf446ae2d00c8b770b668bfa335b9098933b on SweetIQ:master into 997ee14bbf18a6d3adf986b76a82af370b486880 on vitaly-t:master.\n. \nCoverage remained the same at 99.886% when pulling 601ae6a4d2d003a3b3bed8609a2136a4b24d410c on SweetIQ:master into 269f647bb0453af7d71d80f4040b7548861ddf75 on vitaly-t:master.\n. \nCoverage remained the same at 99.886% when pulling 57a34f856f0852b996e72d3b9e877eba73ab20c1 on erndob:patch-1 into 03666a845d15f4e443f1129a83f508551b3c6a32 on vitaly-t:master.\n. \n\nCoverage decreased (-0.2%) to 99.716% when pulling aa71417011df422d3f19d189b2636036372f352b on josefzamrzla:query_finished_event into 0283b7eb719b3fc47100fdd9348d9a418c5c6270 on vitaly-t:master.\n. \n\nCoverage remained the same at 99.943% when pulling 963660aa09e79e4c188ce06d4cd3c0c3cc9ba537 on benoittgt:More_marging_in_doc into 3b36f9e12f1790d00c584ac1e8e6015da2255c02 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 05aceee23a4854c867366e48ddd12d22b322a4ca on ferdinandsalis:patch-1 into 90cde12818ae33f6282527a39b0bec69900406e0 on vitaly-t:master.\n. \n\nCoverage remained the same at 99.152% when pulling 0e424e2b7e46dc84f96861ebbacd5018d404139b on akdor1154:errorsDecl into 2c195d0be531ac0ca6b02bd024c409bab8ff9d91 on vitaly-t:master.\n. \n\nCoverage remained the same at 99.152% when pulling 17f3e7f8e2922d248e96b7e28f0e9761b72c043d on akdor1154:errorsDecl into 2c195d0be531ac0ca6b02bd024c409bab8ff9d91 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 40edbb33039d025eda9d9a935064999223428fcc on akdor1154:errorsDecl into 7e8d00239d7fda61ea3b7b61c7bc5a6234bc2aec on vitaly-t:master.\n. \n\nCoverage increased (+0.03%) to 99.178% when pulling f19df2aeed58a8403b0c961089d7aaa9765cac30 on dmfay:pg-defaults into 2c195d0be531ac0ca6b02bd024c409bab8ff9d91 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 959b97b680386b4e4c6bee97ad9c97251ac7d793 on rafaelkallis:master into 8475ab30e6efe306598a7c4a4021bac8f2ac77c7 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 8757ea796f80f1e417c9c456b7477d864c41bf9f on 6.x into d538a6d96eacf56b64f834f6a189fbd2da618ded on master.\n. \n\nCoverage remained the same at 100.0% when pulling 3f486b39941e8d591ec2212476324943a53b1554 on 6.x into d538a6d96eacf56b64f834f6a189fbd2da618ded on master.\n. \n\nCoverage remained the same at 100.0% when pulling b5738dd3f91be15e97c9bd22db395ace4037b31b on tonylukasavage:patch-1 into 1af68101132fa8b32de95129f59b4ce7a689e8aa on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling ad050f4d709e45a0cd61005a500e0968a47d233a on demurgos:nested-tx-doc into 9ce1f87a684c75f9515cd85befed8cde5efc8e33 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling cb868634cf78588a22b2401f2268cfd660095fce on dmfay:file-param-counts into 6ec25502375de827aeb0c02e9a4ed1d61ae243a6 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 229efb3b68cd428f922b946a58f5b5cada6b1906 on direct-connections into 665beeff1d077d8b0cb96db53ae45845c9725e6b on master.\n. \n\nCoverage remained the same at 100.0% when pulling 8b6e348aa6942a686ab81104e793cdacabe89360 on rafaelkallis:master into df4de10eb0f42ec506ab16be443553398027157c on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling b64b8cbd381116144de3c3c3be3d5995f3a68b6d on rafaelkallis:master into df4de10eb0f42ec506ab16be443553398027157c on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 9d3cbc5ab1d5b4e2271b8fac86777be38973410d on batch into d0d42c0744ce9b427e9a6d50799fdf4a8d3c3dcc on master.\n. \n\nCoverage remained the same at 100.0% when pulling 9d3cbc5ab1d5b4e2271b8fac86777be38973410d on batch into d0d42c0744ce9b427e9a6d50799fdf4a8d3c3dcc on master.\n. \n\nCoverage remained the same at 100.0% when pulling 828a71eeb49d6fdf6380cef760d7255d8d5795f0 on btd:master into ede77b39c2d7cb1951cee72775cc6f5fa61071c9 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 2e0123f514bd37ef4321d3690510f3a161cecad1 on nested-names into 285bdc0c353f065e186f663191c0225fc85301dc on master.\n. \n\nCoverage remained the same at 100.0% when pulling 5cd855f0b548c11585f8f1fb4acc86948ccf37d7 on pavelkucera:error-constructor into 2035a82ab7bf212c37ec335a3129be6d56338996 on vitaly-t:master.\n. \n\nCoverage decreased (-0.05%) to 99.947% when pulling 128e3fdddec43b2839a5825799100076f918bf57 on broken-network-fix into 5a43da886b8fac0ac4f74ee21a98a3b1fdfc2382 on master.\n. \n\nCoverage decreased (-0.05%) to 99.947% when pulling 128e3fdddec43b2839a5825799100076f918bf57 on broken-network-fix into 5a43da886b8fac0ac4f74ee21a98a3b1fdfc2382 on master.\n. \n\nCoverage decreased (-0.05%) to 99.947% when pulling 128e3fdddec43b2839a5825799100076f918bf57 on broken-network-fix into 5a43da886b8fac0ac4f74ee21a98a3b1fdfc2382 on master.\n. \n\nCoverage remained the same at 100.0% when pulling b98380177c84760c2c894f97c4bf01cc54867732 on broken-network-fix into 5a43da886b8fac0ac4f74ee21a98a3b1fdfc2382 on master.\n. \n\nCoverage remained the same at 100.0% when pulling 69f1d184aef09dcd0538a4fbab528b51199b66cb on nidin:nidin-patch-1 into 122ecec195d768acf4ed4b6e4a5d0807601ce493 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 69f1d184aef09dcd0538a4fbab528b51199b66cb on nidin:nidin-patch-1 into 122ecec195d768acf4ed4b6e4a5d0807601ce493 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 9118de2fe636a1f3b8d50308e0bc91eb6d8170a9 on alex-sherwin:master into c81b0481a5de39926fd8db3c4d3caf2eeb4028b2 on vitaly-t:master.\n. \n\nCoverage decreased (-0.05%) to 99.948% when pulling d4b49e5422255c77f56fcf8b538cdd1c66db7c33 on 480-reusable-tx into f494793a5254c429da0cd60db2d2bc4aefc7ad7a on master.\n. \n\nCoverage remained the same at 100.0% when pulling 35b58d6f9bfa010eea5aa51b0529c0eac1806cef on 480-reusable-tx into f494793a5254c429da0cd60db2d2bc4aefc7ad7a on master.\n. \n\nCoverage decreased (-0.05%) to 99.948% when pulling d4b49e5422255c77f56fcf8b538cdd1c66db7c33 on 480-reusable-tx into f494793a5254c429da0cd60db2d2bc4aefc7ad7a on master.\n. \n\nCoverage remained the same at 100.0% when pulling 2a6b6262ba90de07ea9229833c850ff9157a14dc on 483-query-functions into f494793a5254c429da0cd60db2d2bc4aefc7ad7a on master.\n. \n\nCoverage remained the same at 100.0% when pulling ddee5cdec32ee66255e92aa6fda162221d6ceef0 on 481-taskIf into e7987e342f220a5c9da7ed18921d59b16e6faee4 on master.\n. \n\nCoverage remained the same at 100.0% when pulling edbba9f45eb50dac8bf523d635ca3fc03e67c720 on tobymurray:407-fix-link-to-contributing-doc into fdfd44cfbfb648e0f87fc643b79284e28a5b8af5 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling edbba9f45eb50dac8bf523d635ca3fc03e67c720 on tobymurray:407-fix-link-to-contributing-doc into fdfd44cfbfb648e0f87fc643b79284e28a5b8af5 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling edbba9f45eb50dac8bf523d635ca3fc03e67c720 on tobymurray:407-fix-link-to-contributing-doc into fdfd44cfbfb648e0f87fc643b79284e28a5b8af5 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling edbba9f45eb50dac8bf523d635ca3fc03e67c720 on tobymurray:407-fix-link-to-contributing-doc into fdfd44cfbfb648e0f87fc643b79284e28a5b8af5 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling eafbf20c2eae5f0d20c9fd91c2dd9dd2b07375e1 on Vinnl:patch-1 into 892bd52ecd57e40c7ec8eff931041d34f833e859 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling eafbf20c2eae5f0d20c9fd91c2dd9dd2b07375e1 on Vinnl:patch-1 into 892bd52ecd57e40c7ec8eff931041d34f833e859 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling dd4af9c5f864631071343a159e551db2b24fcc71 on ThomWright:type-improvements into 8cf9d2a989593f4bbb9a53faabc14e13a9cccf89 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling dd4af9c5f864631071343a159e551db2b24fcc71 on ThomWright:type-improvements into 8cf9d2a989593f4bbb9a53faabc14e13a9cccf89 on vitaly-t:master.\n. \n\nCoverage decreased (-31.5%) to 68.481% when pulling a3427c8403e261b4aa3cfc468bbbd42e872a0251 on bradleykirwan:dont_attach_data_listener_to_query_stream into 283231392d9e70906849eded443c3ce27348063a on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 6906728421a3167e0eea06dd7cf9be2c0aa9a3c8 on benhjames:patch-1 into 3c355ffd94a5dac41eb046d5722b5772a7de02ef on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 6906728421a3167e0eea06dd7cf9be2c0aa9a3c8 on benhjames:patch-1 into 3c355ffd94a5dac41eb046d5722b5772a7de02ef on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling 07111e3ebfad07e765a47d6900594eef80314f98 on joux3:rollback-on-statement-timeout into 47924505105466abe8d34eaee9f49b01212386cd on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling e4163a792c5c498cd5ca34214485de2b87ef17f7 on tolgaakyuz:propagate-err into eacc2716fe9312f7350bf45d958dec7f76372298 on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling f04b2295a87dc4426693fa15f90800c3b7ec433a on johanneswuerbach:fixup-error-handling into eb771b98a08b5a56a447408eb15b696291973c4e on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling f04b2295a87dc4426693fa15f90800c3b7ec433a on johanneswuerbach:fixup-error-handling into eb771b98a08b5a56a447408eb15b696291973c4e on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling f5bbbec57fc2441de8c854e3a776d37399b0a1c1 on ForbesLindesay:patch-1 into 4fbdd6a5e45592342a661419a28186c2aff7900d on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling f5bbbec57fc2441de8c854e3a776d37399b0a1c1 on ForbesLindesay:patch-1 into 4fbdd6a5e45592342a661419a28186c2aff7900d on vitaly-t:master.\n. \n\nCoverage remained the same at 100.0% when pulling f5bbbec57fc2441de8c854e3a776d37399b0a1c1 on ForbesLindesay:patch-1 into 4fbdd6a5e45592342a661419a28186c2aff7900d on vitaly-t:master.\n. ",
    "nonplus": "Sorry about the ambiguous case title (I've updated the title) I am actually interested in fixing support for saving Buffers to the database. FWIW, reading them is handled by node-postgres and works just fine.\nAccording to PostgreSQL, the bytea Hex Format:\n\nThe \"hex\" format encodes binary data as 2 hexadecimal digits per byte [...] The entire string is preceded by the sequence \\x [...] The hex format is compatible with a wide range of external applications and protocols, and it tends to be faster to convert than the escape format, so its use is preferred.\n\nThis is exactly what my suggested $as.buffer() implementation does.\nNote that all parameters to Buffer.toString() are optional (see documentation) and the default start and end values are what we need.\n. The existing $as.json implementation works acceptably with Buffers since it ends up using Buffer.toJSON().  But, I would not expect someone to store a Buffer value in an JSON (or JSONB) column, since it's not a native JSON type and you have to jump through extra hoops when your JSON.parse() it.\nLooks like CSV uses backslash characters for encoding binary data, so $as.csv would have to account for that (the current implementation converts a Buffer value to its JSON representation).  Though, frankly, trying to use binary data in CSV is not recommended.\nFor now, I'm not going to worry about these edge cases and fix the Buffer formatting in my fork.\nThanks for the feedback.\n. That could work for a single value, but not for an array that contains Buffer(s).  If you look at what Postgres does when it dumps a BYTEA column (e.g. COPY table TO '/tmp/test.csv' WITH CSV HEADER) you'll see how it encodes the binary values as a string with backslashes.\nSo you'll probably also want to handle this in formatCSV:\njavascript\nfunction formatCSV(values) {\n    if (!values instanceof Array) {\n        values = [values];\n    }\n    return values.map(function (v) {\n        if (Buffer.isBuffer(v)) {\n            return JSON.stringify(v.toString(\"binary\"));\n        }\n        return formatValue(v);\n    }).join(',');\n}\n. Well, I probably wouldn't use the binary in a filter, but definitely when inserting/updating data:\n```\n-- Schema\nCREATE TABLE public.person\n(\n    id SERIAL PRIMARY KEY NOT NULL,\n    first VARCHAR(32),\n    last VARCHAR(32),\n    picture BYTEA\n);\n// Insert a person with an image\nctx.none(\"INSERT INTO person SET (first, last, picture) VALUES (${first}, ${last}, ${picture})\", {\n   first: \"Georger\",\n   last: \"Washington\",\n   picture: new Buffer(...)\n});\n``\n. BTW, what _is_ the use case for the CSV formatting?  I'm familiar with bulk imports/exports (COPY FROM/TO), but I've only used those with CSV files.\n. Looks good.  I think the CSV case is inconsistent with the other types wheresomeValueand[someValue]` result in the same CSV representation.  In any case, you shouldhave a CSV test case where Buffer is an array element.\nexpect(pgp.as.csv([23, new Buffer([1, 2, 3]), \"Hello\"])).toBe(???);\nIf it were me, I would make as.csv stricter and have it throw errors if caller doesn't pass in an array, or if any of the array elements aren't primitive values (null, Number, String, Boolean, Date, Buffer) since CSV doesn't support structured values (arrays, objects).  Alternatively, you could JSON.stringify() the structured values but store them as properly escaped strings.\nRight now, (from your unit test) you convert [1, [2, 3], 4] to \"1,array[2,3],4\" which is actually the CSV representation of [1, \"array[2\", \"3]\", 4]. Was that your intention? I would either expect an error or 1,'[2,3]',4\nBTW, CSV is encoding binary characters as as 3-digit octal sequence.  The following function should convert a Buffer to its CSV representation:\n```\nfunction bufferToCsv(buf) {\n    return \"'\" + buf.toJSON().data.map((b) => {\n        var c = String.fromCharCode(b);\n        if (b >= 32 && b < 127 && c != \"'\" && c != \"\\\"\" && c != \",\") {\n            return c;\n        }\n        return \"\\\" + (\"00\" + b.toString(8)).substr(-3);\n    }).join(\"\") + \"'\";\n}\n// Buffer test\nexpect(pgp.as.csv([\n    23,\n    new Buffer([ 0, 1, 72, 101, 108, 108, 111, 255 ]),\n    \"World\"\n])).toBe(\"23,'\\000\\001Hello\\377',World\");\n. Works great!\n. Thanks for the fix.  As far as CSV goes, I missed your comments in the discussion of #105 (middle of my night and I was skimming...). \n. FYI, you can use pg-copy-stream, with pg-promise, but you have to use the connect() method to get at a client instance (I'm using Q promises in this example):\nvar db = require('./db');\nvar copyTo = require('pg-copy-streams').to;\nvar ct = copyTo('COPY (SELECT * FROM users) TO STDOUT');\ndb.connect()\n    .then(function (con) {\n        var client = con.client;\n        var dfd = Q.defer();\n    var stream = client.query(ct, done);\n    stream.pipe(process.stdout);\n    stream.on('end', done);\n    stream.on('error', done);\n\n    function done(err) {\n        con.done();\n        if (err) {\n            dfd.reject(err);\n        } else {\n            dfd.resolve();\n        }\n    }\n    return dfd.promise();\n});\n\n```\nFWIW, I use streams when doing bulk inserts - COPY FROM STDIN is way faster than doing inserts. Even when compared to the helpers.insert approach.. Fixed it. ;-). @vitaly-t, would you be open for a feature request/PR for adding support for the COPY FROM STDIN and COPY TO STDOUT commands?  Basically node-pg-copy-streams functionality but integrated into pg-promise (hence it could be used in a pg-promise task or transaction).  The commands would return a promise that resolves when the stream is done.  I would envision the API like this:\ncopyTo(query: string, values: any, stream: NodeJS.WritableStream): XPromise<any>;\ncopyFrom(query: string, values: any, stream: NodeJS.ReadableStream): XPromise<any>;\nFWIW, I'm not sure when you'd need bound parameters for \"copyFrom\", but it probably makes sense for the APIs to be consistent.\nThe following is an example of usage:\n```\nvar db = pgp(connection);\n// piping from db to a stream\ndb.copyTo(\"COPY (SELECT * FROM user WHERE last_name = $name) TO STDOUT\",\n    { name: \"Smith\" }, process.stdout\n)\n    .then(...);\n// piping stream into db\nvar fileStream = fs.createReadStream('users.csv')\ndb.copyFrom(\"COPY users FROM STDIN WITH CSV HEADERS\", null, fileStream)\n    .then(...);\n``\n. Can I get my hands on theclientin the extension method?  If so, how?. No chance of always exposing the client as actxproperty the waydatabase.connect()does?. I figured as much and can't really argue with that.  Hence my original suggestion to havecopyFromandcopyTobe part ofIBaseProtocol`.\nAnyway, in my project I don't need my bulk uploads to be inside a transaction, so the workaround with db.connect() and node-pg-copy-streams works for me.. ",
    "vieks": "Indeed it's right I have not notice them so far. I was simply not confront to that kind of error.\nIt seem I was unlucky because the only one I encounter is with an integrity constraint violation and in this case there is not custom error, just a plain Error object with that following shape:\njavascript\n{\n        \"message\": \"duplicate key value violates unique constraint \\\"users_name_key\\\"\",\n        \"name\": \"error\",\n        \"length\": 224,\n        \"severity\": \"ERROR\",\n        \"code\": \"23505\",\n        \"detail\": \"Key (name)=(fogogdgfsdfgzgifsdgfgngfgfgggfgfgdggggdsad) already exists.\",\n        \"schema\": \"public\",\n        \"table\": \"users\",\n        \"constraint\": \"users_name_key\",\n        \"file\": \"nbtinsert.c\",\n        \"line\": \"406\",\n        \"routine\": \"_bt_check_unique\",\n}\n. Yes node-pg-error-lib is what I need to use in response to my issue or I can implement my own wrapper it's not a problem.\nOn the other hand, like many of us in my humble opinion, no one want to manage with this low level of code. Your lib is the perfect abstract for Postgres, we just want install it and use it to manage the database without the need to code our own or plug to it other small libs to compensate a very small but important functionality. Pg-promisy to rule them all.\nSo I can understand parsing and filtering at your level of all errors that Postgres driver pulls up with node-postgres will be a pain and not the purpose of your lib. That's right and it's a good design choice.\nBut in that case, an elegant and a quick fix is that you only implement one global custom parent Error for your lib (wrapping around node-postgres raw Error whatever their shape don't care about it), so now at our code level we can check easily if your lib has thrown an exception at runtime among other libs.\nSo please implement just a single DatabaseError (or of course whatever name you want :) ) because at our level coding we can't detect and and wrap it easily (or in an elegant way without touching pgp underlying lib): Our app code <-- pg-promise <-- node-postgres (=Who want manipule that ???) \nAnd for those that want an advanced Postgresql errors mechanism with custom classes for all possible error cases, they install pg-error.\n. The goal is to have something like that:\n``` javascript\n      try{\n        // app code throwing many kind of exceptions\n      }catch(error){\n        if(error instanceof ValidationError){\n          // ...\n        }else if(error instanceof BcryptError){\n          // ...\n        }else if(error instanceof DatabaseError){/ pg-promise easy error detection with a single/parent error /\n      // want advanced customized errors based on low level database parsing use an add-on lib like pg-error\n      const errorName = error.constructor;\n\n      switch(errorName){\n        case 'DatabaseConstraintViolationError':\n          // ...\n        break;\n        case 'DatabaseQueryError':\n          // ...\n        break;\n        // etc...\n      }\n\n    }else{\n      // ...\n    }\n  }\n\n```\n. Whatever you choose to do, big thanks for the hard work complete so far. Your lib is really nice and keep up the good work ;)\n. I can't use pg-promise global error event handler since there is now way within it to return a new custom error object, append a property or throwing from that place.\nThe only thing I need in my code is having the possibility to use the instanceof operator (SO no getting a generic Error but a custom one) in a catch block in an async ES7 function.\nThis is not the case actually with pg-promise, I get a simple Error instance (but with nice properties). I just need an error instance with the proper 'constructor' property in order to use the instanceof operator. But I can't there is now way.\n\nNote to you, it is difficult to encompass everything within a DatabaseError.\n\nAll my code stands in the frontend, where validation occurs. If the client by-pass something or/and an error occurs on the backend, I merely send him back an error. All pretty output and details is no need on backend, only type error dectection. Or at least a custom constraint integrity error violation, since there is no way to check it on frontend stage.\nSo what is the way in your lib to hook up an error and customize it ?\nOtherwise you advise me to use pg-error but here I don't find the way how to interface it with pg-promise: pg-error-pg-postgres (It seems the object shape is not the same I have no access to connection prop on pgp.pg from pg-promise instance)\n. > [...] The library provides you with all the error details. All you need is to parse is you you see fit.\nYes details are nice, but the error instance have the type Error not a type inherited from Error that permit to discern it with an error instanceof ... check.\n\nYou can see how this guys just did it, and perhaps do something similar, for the special types of errors that you want to catch: https://github.com/evvvvr/q-and-a/blob/345476b509ad5ffb5e001d4428f583753b7c44d3/src/DbService.js\n\nError thrown by pg-promise when an integrity constraint violation occurs have the code 23505so that guy makes:\njavascript\n.catch((err) => {\n  if (err.code === PgErrorCodes.FOREIGN_KEY_VIOLATION /*23503 for foreign key*/) {\n      throw new QuestionNotFoundError();\n  } else {\n      throw err;\n  }\n} );\nso transposed to my code:\n``` javascript\n} catch(error) {\nif(error instanceof ValidationError){\n    // ...\n  }else if(error instanceof BcryptError){\n    // ...\n  / We can't use instanceof operator here/\n  }else if(error.code === db.INTEGRITY_CONSTRAINT_VIOLATION){\n}\n```\nIT'S QUITE UGLY ! Ok for checking status error code inside the clause but not at this level of check, it will be way better like that:\n``` javascript\n} catch(error) {\nif(error instanceof ValidationError){\n    // ...\n  }else if(error instanceof BcryptError){\n    // ...\n  }else if(error instanceof DatabaseError){ // or PgError, whatever name\n    const res = handleDatabaseCode(error.code);\n    // ... or use a switch statement here\n    return res;\n  }\n}\n```\nNow IT'S NICE ERROR CODE handling :) , but we can't do it actually only because pg-promise throws raw Error and not a single one which is inherited from Error. Due to that, we lose the power to use instanceof operator. \n. It's bad node-postgres does not implement its own Error class. So I thought that pg-promise as a wrapper abstraction layer was a good opportunity to correct this behavior. But as you said:\n\nit would completely break compatibility with all the previous versions\n\nYour code to correct this issue:\n``` javascript\ndb.any(\"SELECT * FROM Users\")\n    .then(data=> {\n        // success\n    })\n    .catch(parseError)\n    .catch(error=> {\n        if (error instanceof MyCustomError) {\n            // do something...\n        }\n    })\n    .finally(pgp.end);\nfunction parseError(error) {\n    // parse 'error', and throw an error accordingly;\n    throw new MyCustomError(\"my custom error type\");\n}\n```\nIs pretty nice for an ES6 promise style.\nBut with ES7 async syntactic sugar we can't chaining catch clause like that, so finally I create a little wrapper on it to prevent code bloating and handle all exceptions under the hood. The result IMHO is really nice.\nCode snippets for those reading this topic later:\nHandle user input request (no more try/catch needed)\n``` javascript\n  async mutateAndGetPayload(payload, { rootValue: { mutation, conf } }) {\n    d(payload);\nvalidator.check(payload);\n\npayload.role       = role.BASIC;\npayload.status     = status.UNREGISTERED;\npayload.created_at = payload.updated_at = +new Date;\npayload.pass       = await bcrypt.hashAsync(payload.pass,\n                                            conf.BCRYPT_ROUNDS);\n\nreturn await mutation('user.create', payload);\n\n},\n```\nPostgreSQL raw error codes mapping to javascript exception classes\n``` javascript\nimport * as errors from '../../errors';\nexport default {\n/ INTEGRITY_CONSTRAINT_VIOLATION /\n  '23505': errors.DatabaseIntegrityConstraintError,\n// [...]\n};\n```\nThe interesting part which hiding the raw exceptions pulls up by pg-promise/node-postgres\n``` javascript\nimport {\n  queries,\n  mutations,\n} from './actions';\nimport {\n  DatabaseError,\n} from '../errors';\nimport errorCodes from './postgres/error-codes';\n// [...]\nasync function mutation(\n  action: string,\n  payload: Object,\n): Promise<?Object | DatabaseError> {\n  return await mutationsaction.catch(error => {\n    const Exception = errorCodes[error.code];\nif(Exception)\n  throw new Exception;\n\nthrow new DatabaseError;\n\n});\n}\nexport { query, mutation };\n```\nDone, it works like a charm.\nThanks for your support and the time you took @vitaly-t :)\n. Thanks @vitaly-t for relaying the information. Indeed it seems being messy.\n. Ok thanks a lot I get it.\n\nNo, no penalty in such case.\n\nDo you know when it starts to be ?\n\nThe bottom line, there is nothing complex there, you can tweak it any way you like, i.e. have completely your own version of joinQueries, if you like ;)\n\nThis is exactly what I was starting to do but it's obvious you are not doing things without a good reason :) that's why I wanted to know more about your choice of of { ['SQL']: params } which is not a usual case (long string goes normally as value not key). \n. Understood Sir ;)\n. Stored procedures at Postgres server so the client send only parameters to them ?\nIt needs to know PL/SQL so having DBA skill, it's more advanced/specific stuff for a full stack web dev I think.\nOr I mislead and you speak about other thing ?\n. Lol yeah ok :)\n(sorry I don't grab all sentence meaning directly I am sh*t in english )\n. But thanks for all your advises (here from issues and wiki+page lib)\n. Nice thank you.\n. ",
    "eddievagabond": "@vitaly-t Hey I did get it working. I must have miss read the documentation over and over again. Thanks for the help.\n. ",
    "4ertovo4ka": "Thanks. But i cant do this query with your library. And i ask help to work with pg-promise. \nThanks for you attention. On stack you answer more thoroughly and everywhere where they could tell about the library . And in fact , when it had a question that I 'm trying to solve a few days - can not wait for help . OK.\n. \u041d\u043e \u0434\u043e\u043b\u0436\u043d\u0430 \u043f\u0440\u0438\u0437\u043d\u0430\u0442\u044c - \u043f\u0440\u0438\u044f\u0442\u043d\u043e \u0431\u044b\u043b\u043e \u0443\u0432\u0438\u0434\u0435\u0442\u044c, \u0447\u0442\u043e \u0441\u0438\u043c\u0432\u043e\u043b\u044b \u044d\u043a\u0440\u0430\u043d\u0438\u0440\u0443\u0435\u0442\u0435 \u043d\u0430 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0435, \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u043a \u0434\u0435\u0442\u0430\u043b\u044f\u043c \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438 \u0443 \u0432\u0430\u0441 \u043d\u0435 \u043e\u0442\u043d\u044f\u0442\u044c.\n. \u0421 inner join \u0442\u043e\u0436\u0435 \u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u0430, \u043d\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u0441\u0442\u0440\u0430\u043d\u043d\u044b\u0439. \u041f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u0442\u0430\u043a\u043e\u0439 \u0436\u0435 \u043a\u0430\u043a \u0438 \u0432 \u043e\u043f\u0438\u0441\u0430\u043d\u043d\u043e\u043c \u043c\u043d\u043e\u0439 \u0441\u043b\u0443\u0447\u0430\u0435. \u0418\u0437\u0432\u0438\u043d\u0438\u0442\u0435, \u0441\u0443\u0442\u043a\u0438 \u0431\u0435\u0437 \u0441\u043d\u0430 \u0438 \u043d\u0435 \u0441\u043e\u0432\u0441\u0435\u043c \u0442\u043e\u0447\u043d\u043e \u043e\u043f\u0438\u0441\u0430\u043b\u0430 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443. \u0421\u0435\u0439\u0447\u0430\u0441 \u043d\u0430\u0448\u043b\u0430 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u0442\u043e\u043c \u043e row_to_json. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u044e \u0442\u0430\u043a, \u043c\u043e\u0436\u0435\u0442 \u044d\u0442\u043e \u0440\u0435\u0448\u0438\u0442 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 \u0432\u043b\u043e\u0436\u0435\u043d\u043d\u044b\u0445 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u0432. \u041d\u0430 \u0441\u0442\u0430\u043a\u0435 \u0438 \u0432 \u0442\u043e\u0441\u0442\u0435\u0440\u0435 \u043d\u0435 \u043f\u0438\u0441\u0430\u043b\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \"\u043b\u0438\u0431\u043e \u043b\u044b\u0436\u0438 \u043d\u0435 \u0435\u0434\u0443\u0442, \u043b\u0438\u0431\u043e \u044f \u0438\u0434\u0438\u043e\u0442\" \u0438 \u0442\u0430\u043a \u043a\u0430\u043a \u0431\u044b\u043b\u043e \u0434\u0432\u0430 \u0432\u0430\u0440\u0438\u0430\u043d\u0442\u0430 - \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0430 \u0441 \u043c\u043e\u0438\u043c \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u043c \u0438\u043b\u0438 \u044f \u043d\u0435\u0432\u0435\u0440\u043d\u043e \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0443. \u0420\u0435\u0448\u0438\u043b\u0430, \u0447\u0442\u043e \u0440\u0430\u0437 \u0432\u044b \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0430\u043a\u0442\u0438\u0432\u043d\u043e \u043e\u0442\u0432\u0435\u0447\u0430\u0435\u0442\u0435 \u0442\u0443\u0442 - \u0445\u043e\u0442\u0435\u043b\u0430 \u0441\u043f\u0440\u043e\u0441\u0438\u0442\u044c \u0442\u0443\u0442. \u0418 \u0438\u0441\u043a\u043b\u044e\u0447\u0438\u0442\u044c \u043f\u043e \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u0432\u0430\u0440\u0438\u0430\u043d\u0442 \u043d\u0435\u0432\u0435\u0440\u043d\u043e\u0433\u043e \u043f\u043e\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438. \n\u0418\u0437\u0432\u0438\u043d\u0438\u0442\u0435. \u041f\u0440\u043e\u0441\u0442\u043e \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0435\u0440\u0435\u043f\u0440\u043e\u0431\u043e\u0432\u0430\u043b\u0430 \u0438 \u0438\u043d\u043d\u0435\u0440\u044b \u0438 \u0432\u0441\u0435 - \u043d\u0435 \u0435\u0434\u0443\u0442 \u043b\u044b\u0436\u0438. \u0421 \u0432\u0441\u0442\u0430\u0432\u043a\u043e\u0439 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u044f\u043c \u0440\u0430\u0437\u043e\u0431\u0440\u0430\u043b\u0430\u0441\u044c, \u0430 \u0432\u043e\u0442 \u0441 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u044b\u0445 \u043d\u0435 \u043c\u043e\u0433\u0443. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u044e \u043d\u0430 \u0441\u0432\u0435\u0436\u0443\u044e \u0433\u043e\u043b\u043e\u0432\u0443. \u0421\u043f\u0430\u0441\u0438\u0431\u043e, \u0447\u0442\u043e \u0441\u0436\u0430\u043b\u0438\u043b\u0438\u0441\u044c \u0438 \u043e\u0442\u0432\u0435\u0442\u0438\u043b\u0438 \u0432 \u043a\u0430\u043a\u043e\u043c \u043d\u0430\u043f\u0440\u0430\u0432\u043b\u0435\u043d\u0438\u0438 \u043a\u043e\u043f\u0430\u0442\u044c. \u0418\u0437 \u0432\u0441\u0435\u0433\u043e, \u0447\u0442\u043e \u044f \u0443\u0432\u0438\u0434\u0435\u043b\u0430 \u043d\u0430 \u043f\u0440\u043e\u0441\u0442\u043e\u0440\u0430\u0445 \u0443 \u0432\u0430\u0441 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0441\u0430\u043c\u0430\u044f \u043b\u0443\u0447\u0448\u0430\u044f \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0430. \n. SELECT\n  p.,\n  row_to_json(e.) as employee\nFROM project p\nINNER JOIN employee e USING(employee_id)\n\u0415\u0441\u043b\u0438 \u043a\u043e\u0440\u043e\u0442\u043a\u043e, \u0442\u043e \u0432\u043e\u0442 \u043e\u043d\u043e - \u0440\u0435\u0448\u0435\u043d\u0438\u0435 :) \u0422\u0435\u043f\u0435\u0440\u044c \u043b\u044b\u0436\u0438 \u043f\u043e\u0435\u0445\u0430\u043b\u0438\n. \u0425\u043c... \u0417\u043d\u0430\u0447\u0438\u0442 \u0447\u0442\u043e-\u0442\u043e \u044f \u043d\u0435 \u0442\u0430\u043a \u0434\u0435\u043b\u0430\u044e. \u041b\u0430\u0434\u043d\u043e, \u0431\u0443\u0434\u0443 \u0434\u0430\u043b\u044c\u0448\u0435 \u043a\u043e\u0432\u044b\u0440\u044f\u0442\u044c. \u0421\u043f\u0430\u0441\u0438\u0431\u043e \u0431\u043e\u043b\u044c\u0448\u043e\u0435 :) \n. ",
    "sandrija": "I know this is and old question and is not related about bug issue, but vitaly-t, if you have time, can you look at my question on stack overflow?\nhttps://stackoverflow.com/questions/46886955/pg-promise-nest-tables-results-when-selecting-from-multiple-tables. ",
    "ryeice289": "how do I display the results of the code in postman?  answer no 9\n. ",
    "awdogsgo2heaven": "@vitaly-t Looks like I was heading in the right direction with the function and namespace, glad you were able to finish this! I'll check it out tonight and get back with you.\n. Looks good @vitaly-t! Thanks.\n. ",
    "Onionz": "I see pg-promise uses node-postgres, is it the implementation of node-postgres or it is just the natural behavior of postgres itself?\n. Thanks, I will check it out there. :smiley: \n. When I call count(*) in postgres it returns a 64-bit bigint type number. But Javascirpt does not support 64-bit integers (which is also mentioned in pg-types). So it will be converted to string.\n. ",
    "paavohuhtala": "Thanks for your feedback. I don't have the time to write a full response right now, but I can confirm that I've managed to get your typings to work :+1:. It was caused by a stupid mistake on my part; the node-postgres declerations from typings conflicted with the ones in your library.\nI'll review my issues when I have the time, and promote them to full issues when applicable.\n. You can close this issue for now. The most important thing you should do is to follow this guide so that the TypeScript compiler can discover the typings for the library automatically.\nI'll open more issues and/or pull requests should the need arise.\n. ????\nThe link is about adding a property to package.json, so that the compiler can discover the typings by itself. It's not about naming conventions or anything related to the protocol. Additionally, I don't think it's limited to TypeScript projects; any project with included typings should work.\nAlso the typings should use the file ending d.ts. Currently, if you include the typings using tsconfig.json, the compiler will compile the typings into .js-files, instead of just using them for type resolution. This will result in unnecessary files.\n. ",
    "ferlopezm94": "Thanks for the examples! Just one doubt: inside source, index will always start at 0? I couldn't find this information in the API, maybe it's obvious but I just want to be sure about it.. Hi @vitaly-t, what would be the difference between using the dest function as you showed above and using source? Also in what situation could we use both source and dest? I'm a little confused since I can't see the different usage with your example.\nAlso, is it ok to use a sequence inside a batch? Or do I have to use it separately?. Hi!\nI'm learning how to use pg-promise and I have the situation when I need to insert like 50 records to populate my test database. Researching I came with batch and pgp.helpers but the problem is that I also need to insert some info that depends on the generated id of another insert. The solution is:\n```\nlet populateDatabase = () => {\n   let cs = new pgp.helpers.ColumnSet(['col_a', 'col_b'], {table: 'table1'});\n   let valuesInsert = pgp.helpers.insert(values, cs);\ndb.tx(t => {\n      return t.one('INSERT INTO user (first_name, last_name) VALUES (${first_name}, ${last_name}) RETURNING id', user)\n         .then(data => {          \n            user.id = Number(data.id);\n            personalMeasurements[0].id_user = user.id;\n            personalMeasurements[1].id_user = user.id;\n        const q1 = t.none('INSERT INTO device (model) VALUES (${model})', device);\n        const q2 = t.none(valuesInsert);\n        const q3 = t.one('INSERT INTO personal_measurement (personal_measurement_date, height, id_user) VALUES (${personal_measurement_date}, ${height}, ${id_user}) RETURNING id', personalMeasurements[0]);\n        const q4 = t.one('INSERT INTO personal_measurement (personal_measurement_date, height, id_user) VALUES (${personal_measurement_date}, ${height}, ${id_user}) RETURNING id', personalMeasurements[1]);\n\n        return t.batch([q1, q2, q3, q4]);   \n     });\n\n}).then(data => {\n      console.log('populateDatabase success', data);    \n   }).catch(error => {\n      console.log('populateDatabase error', error);\n   });\n};\npopulateDatabase();\n```\nI wanted to ask you:\n1. Is it ok to make a simple insert and inside the then clause have the batch which includes a pgp.helpers.insert? I just want to make sure I'm not implementing it with a bad pattern. I saw some examples but there are with different situations.\n2. Do you recommend to use pgp.helpers.insert even when I have to insert 2 records? So instead of q3 and q4 have pgp.helpers.insert(valuesPersonalMeasurements, csPersonalMeasurements).\nThanks!\n. Thank you so much for the feedback @vitaly-t ! I took them into consideration and after reading more about ColumnSet and ColumnConfig, I changed the code to the following:\n```\nlet populateDatabase2 = () => {\n   let csValues = new pgp.helpers.ColumnSet(['col_a', 'col_b'], {table: 'table1'});\nlet csDevice = new pgp.helpers.ColumnSet([{name: 'model'}],\n                                           {table: 'device'});\nlet csUser = new pgp.helpers.ColumnSet([{name: 'first_name', prop: 'firstName'}, \n                                           {name: 'last_name', prop: 'lastName'}],\n                                           {table: 'user'});\nlet csPersonalMeasurement = new pgp.helpers.ColumnSet([{name: 'personal_measurement_date', prop: 'personalMeasurementDate'},\n                                                          {name: 'height'}, \n                                                          {name: 'id_patient', def: () => user.id}],\n                                                          {table: 'personal_measurement'});                                          \ndb.tx(t => {\n      let userInsert = pgp.helpers.insert(user, csUser) + 'RETURNING id';\n  return t.one(userInsert)\n     .then(data => {            \n        user.id = Number(data.id);\n\n        let deviceInsert = pgp.helpers.insert(device, csDevice);\n        let valuesInsert = pgp.helpers.insert(values, csValues);\n        let personalMeasurementInsert = pgp.helpers.insert(personalMeasurements, csPersonalMeasurement) + 'RETURNING id';\n\n        const q1 = t.none(deviceInsert);\n        const q2 = t.none(valuesInsert);\n        const q3 = t.many(personalMeasurementInsert);\n\n        return t.batch([q1, q2, q3]);   \n     });\n\n}).then(data => {\n      console.log('populateDatabase success', data); \n      personalMeasurements.forEach((personalMeasurement, index) => personalMeasurement.id = data[2][index].id);   \n   }).catch(error => {\n      console.log('populateDatabase error', error);\n   });\n};\npopulateDatabase();\n```\nThe code works perfectly and  I can see clearly the benefits of pgp.herlpers :D \nPD: In the then clause, I implement a forEach in order to add the generated id for personalMeasurements, maybe this can be a problem where multiple generated id return, but because I'm not inserting more than 5 I don't think it's a problem.\nYou mentioned here that doing this way is \"completely safe, as all data types are going through the library's query formatting engine to make sure everything is formatted and escaped correctly.\", so does it prevent us from SQL injection?. ",
    "rdmurphy": "Aw, bummer. I'll probably just defer to using pg to do COPY calls for now, although I am weary of those issues. Unless there's any other suggestion you or others may have! I really like the promise based workflow afforded here. I actually left out a part of that query \u2014 the end goal is tack on a WITH CSV HEADER too and save the CSV dump to a file. The potential size of the output though makes the ability to stream appealing.\nThanks for the speedy reply!\n. Thanks for the links! (I'm not trying to insert in this scenario, only need to pull from the database.)\nIt's not so much that streaming from the database isn't sufficient \u2014 it's the format of the data in that stream that's not ideal. My end goal is to produce a CSV. That's the benefit of being able to send a query using COPY TO ... WITH CSV HEADER \u2014 Postgres sends it over ready to go. I could try to do something via pg-query-stream to a Node.js CSV writer, but it'd be nice to skip that step and let Postgres handle it, which was the original appeal of pg-copy-streams.\n. ",
    "goyoGit": "@vitaly-t, I couldn't compare performance of 'pgp.helpers.insert' and 'COPY' yet but I see an issue with the first method as it is necesary to write the name of the column for each value to be inserted, which increases the size of the object. I.E:\nconst data = [\n    { title: 'red apples', cost: 2.35, units: 1000 },\n    { title: 'large oranges', cost: 4.50 },\n];\nIs there any way to avoid writing the name of the columns?\nThanks!\n. Hi @vitaly-t and thanks for your response,\nMy concern is the size of the object to be inserted, in the previous sample if we want to insert a million rows, the object called 'data' will have a million attributes called 'title'. I was wondering if it si possible to avoid this, maybe using an array for each row or a 'csv' format string. . @vitaly-t ,\nI think I didn't explain myself. Considering the example in Data Imports, let say that I want to insert 1 million rows, then the data object will be:\nconst data = [\n    { title: 'red apples', cost: 2.35, units: 1000 },\n    { title: 'large oranges', cost: 4.50 },\n    ...\n    { title: 'bla', cost: 25, units: 22 },\n    ...\n    { title: 'blabla', cost: 5, units: 22 }\n    ...\n];\n// Aray of 1 million rows, each row is an object with 3 attributes\nMy question is if there is a chance to use a smaller object for the data, for example\nconst dataArray = [\n    ['red apples', 2.35, 1000 ],\n    ['large oranges', 4.50 ],\n    ...\n    ['bla', 25, 22 ],\n    ...\n    ['blabla', 5, 2999 ]\n    ...\n];\n// Aray of 1 million rows, each row is an array\nI think dataArray needs less memory than data, and my question is if 'pgp.helpers.insert' can handle data in this format or if it is planned.\nThanks a lot for you time.. ",
    "Vivek0829": "@vitaly-t I went through your docs, but I find it hard to stream JSON data into the function for massive inserts. I created a post to describe the issue in the below link. \nhttps://stackoverflow.com/questions/50206942/massive-insert-from-a-json-file-using-pg-promise. ",
    "L-u-k-e": "Thanks!\nI understand not wanting to turn this library into an ORM.\n. Awesome!\n. @vitaly-t Definitely. I have a ton of work to do on a project tonight and I will be receiving my bachelors degree tomorrow, so this weekend might not be so productive on that front, but Monday after work  I will definitely get to this. \n. Sorry. I've been busy. I'm actually at work right now. I won't be able to look deeper until later tonight. I was looking at the documentation though and one question I have is as follows:\nFor the UPDATE and INSERT queries, you are currently requiring an array of column names when doing a multi values insert/update. What was the reasoning behind that decision?\n. Regarding the heterogeneous objects: The library function could just infer, then group together the objects have the same set of column names, no? The point about needing more information for those more detailed types of queries is definitely valid though.\nAdditionally, I see that we are requiring users to add their own WHERE's to the update statement. I was thinking about ways to do that on my own and I also concluded that you can't offer the same about of expressiveness without making the syntax more pedantic than it already is. However, since the goal is to provide convenience methods, not necessarily the entire SQL functionality, what about allowing a simple object to be passed in for the WHERE, whose property name/value relationship always translated to '=' and whose property/property relationship is always 'AND'. E.G. {ID: my_id}.\nI know that's a super simple use case, but it's also a super common one. \n. Tonight I'll actually dive into the code. Maybe I'll take a stab at doing a SELECT just to get the ball rolling, but more likely I'll spend the time just reading the code getting a better feel for things. \n. Actually, I meant that I would branch and attempt to write a SELECT helper. However, since you've expressed a desire to keep the WHERE clause generation separate from these helpers, that pretty much also throws GROUP BY, HAVING etc. out the window, so would you agree that it would be in best interest not to create a helper for SELECT statements?\n. Check out the helpers namespace:  http://vitaly-t.github.io/pg-promise/helpers.html\ndb.insert(post, null, 'posts')\n. ",
    "cymen": "@vitaly-t My apologies for overlooking the documentation. Thank you very much!\n. To be clear, opening here first just to confirm no length limits in pg-promise!. I figured it out -- I'm playing with NOTIFY too and it's related to the payload size for NOTIFY.. ",
    "leebenson": "Just to follow-on from this, are there any plans to allow template literal strings, a la:\njs\ndb.query(sql`SELECT * FROM table WHERE id=${id}`);\nThe benefits being:\n\nNo need to pass in an object/array/scalar as a second value - the value, type-checking and escaping is evaluated inline\nDedicated template literal syntax, making it clear that it's a query being passed\nNo need to check for $(), $[], $<>, $// - one syntax to rule them all (possibly faster because of that?)\nIt's clean/modern/simple, and works on every version since Node 4\n\nThoughts?. Thanks @vitaly-t. I'm new to this lib, so I apologise in advance for over-looking obvious considerations you no doubt already made. Whenever I see uber-flexible regex, I'm always concerned that will translate to unnecessary CPU cycles, when I myself am sticking to writing in just one style.\nI can see that doesn't work in this case. Thanks!. Helpful, thanks @vitaly-t . I'd vote always. I usually have a lot of environments that are 'devvy', but not strictly development - e.g. staging, testing, etc. I'd like for warnings to display in all of those.\nFor production, I generally monkey patch logging to push messages via Winston to some kind of third-party tool, so warnings/errors are relevant too.\nI'm not a fan of hiding useful messages just for the sake of a 'clean' console. I'd rather know about them and do something about them, than risk them slipping through the cracks!. Thanks @vitaly-t. Hopefully that concludes my newbie questions for the day \ud83d\ude04 . ",
    "matus123": "I was thinking if it possible to pass object, which would contains query and rowMode similar to this example in pure PG\n```\npg.connect(conString, function(err, client, done) {\n  if(err) {\n    return console.error('error fetching client from pool', err);\n  }\nclient.query({text: SELECT 'name1' as name, 'name2' as name, rowMode: 'array'}, function(err, result) {\n    //call done() to release the client back to the pool\n    done();\nif(err) {\n  return console.error('error running query', err);\n}\nconsole.log(result);\n\n});\n});\n```\nIf it is possible to write something similar with pgp library, for example using db.query(), where i would pass object containing query, values and rowMode, without using PreparedStatement\n``\nconst query = {\n    text:Select 'name1' as name, 'name2' as name`,\n    rowMode: 'array'\n}\ndb.result(query)\n.then((result) => {\n  console.log(util.inspect(result,{depth:null}));\n  pgp.end();\n})\n.catch((err) => {\n  console.log(err);\n});\n```\n. Thanks a lot for fast reply, i will give it a try.\nBut wouldn't it be easier just to allow pass additionalProperties object to db.query() methods, so you wouldn't need to support pg parameterized queries, because you already have your own implementation.\nBecause main reason i asked the question about rowMode was that i have use case where i want to get result from db in form of arrays, because if query contains same column names, then the columns are discarded and i lost data. \nBut, thanks a lot for fast implementation, i really appreciate it.\n. ",
    "OrKoN": "Thanks, the solution is fine. Now I know how to access the pg instance.\n. Some minor issue with the website, bwt\n\n. Yes, no overlapping. I am on Chrome Version 50.0.2661.94 (64-bit), Mac version. \nBut still the font size is too big and the entire left menu does not fit the screen and it is not scrollable.\n\n. That it is ugly in PostgreSQL. See http://stackoverflow.com/questions/6822651/postgresql-order-by-values-in-in-clause\n. Yes, it would be great if the library provided a shortcut for this:\nORDER BY\n     CASE id\n      ${ids.map((item, i) => 'WHEN \\'' + item.id + '\\' THEN ' + i).join('\\n')}\n     END\nThis kind of ORDER BY clause is always based on what is passed as $1^. \nI am not sure how this could look like in the library though.\n. Perhaps?\nconst data = await db.any(`SELECT * ... WHERE id IN ($1:csv) ORDER BY $1:orderBy`, [ids.map(i => i.id)]);\n. Thanks for the prompt response!. Thanks for the quick fix! I will upgrade soon. ",
    "minipai": "Thanks for quick response. If I use\n```\nfunction sql(file) {\n    var path = __dirname + '/sql/' + file;\nvar options = {\n    minify: true,\n};\n\nreturn new QueryFile(path, options);\n\n}\n```\nResult is\nQueryFile {\n   file: \"/Users/Art/Sites/bbs/server/db/sql/findPostsByBoard.sql\"\n   options: {\"debug\":false,\"minify\":true,\"compress\":false}\n   query: \"\"\n}\nBut, if I use\n```\nfunction sql(file) {\n  console.log('sql')\n    var path = './sql/' + file;\nvar options = {\n    minify: true,\n};\n\nreturn new QueryFile(path, options);\n\n}\n```\nResult is \nQueryFile {\n   file: \"./sql/findPostsByBoard.sql\"\n   options: {\"debug\":false,\"minify\":true,\"compress\":false}\n   error: QueryFileError {\n       message: \"ENOENT: no such file or directory, open './sql/findPostsByBoard.sql'\"\n       options: {\"debug\":false,\"minify\":true,\"compress\":false}\n       file: \"./sql/findPostsByBoard.sql\"\n   }\n}\nI think __dirname should be same as ./  as a relative path ?\nReally confused.\n. I finally find out it is relative to my project root. Thanks.\n. Wonderful ! \n. ",
    "ypercube": "Thank you.\nI didn't intend to submit this a bug, maybe just a glitch in the docs, but I couldn't add the \"doc\" label.\nI think only Oracle and DB2 has true autonomous transactions. SQL Server, no: Connect titem\n. As I found our from a SQL Server expert, SQL Server uses them internally (for example in sequence/identity columns and for page splits not to roll back) but they are not exposed to the users.\n. ",
    "djMax": "Modifying Array.prototype is a very bad practice. That's happening here:\nhttps://github.com/vitaly-t/pg-promise/blob/master/lib/array.js#L90\nNot sure that's what's blowing things up, but it's definitely not something a module should do.\n. I kind of agree with the sentiment that #243 was doing too much (e.g. the stats). But I do think there has to be a reliable query completion event of some sort, and receive doesn't seem to fire all the time. In my case I have a layer I can just insert this into by overriding our relatively thin method usage (one, many, oneOrNone, etc). But I'd be happy to avoid that if there was a finish type event. It should include some access to something that was passed in the query event to allow linking it all back together.. ",
    "boromisp": "I see your point.\nI ended up doing something like this:\njavascript\n// field could be eg. 'arrived-timestamp'\nconst paramName = filter.field.replace(/[^a-z0-9\\$_]/g, '_'); \n// field isn't user input, so this should be safe\nquery += ` WHERE table.column=$<${paramName}>`;\nparams[paramName] = filter.value;\nThe helpers namespace looks interesting, but for my use case not really applicable.\n. Because the receive event happens after the query.\nWhat I would need, is preprocessing before query parameter name parsing. I could possibly do that in the query event handler, but I rather use supported parameter naming in the first place.\nI could solve this much cleaner, then I first thought it would be possible, so this turned out to be a non-issue.\n. An other option (that doesn't require support from the promise library) is to pass in a kind of event emitter to the query functions as an argument.\nWhen this object emits a cancel event (but only if it happens before the query promise has been resolved or rejected), the query would be dequeued or cancelled.\nIf the query is not active yet, the the promise will have to be rejected with a custom error (cancelled active queries emit error).\n``` js\nvar cancelEmitter = new EventEmitter();\ndb.manyOrNone('SELECT pg_sleep($);', { t: 10 }, cancelEmitter).then(function () {\n}).catch(function () {\n});\n// ...\ncancelEmitter.emit('cancel');\n```\nOf course it doesn't have to be an EventEmitter, it could be something lightweight, like this, because we only need a single subscriber:\njs\nfunction createCancelEmitter() {\n    var emitter = function (cb) {\n        emitter.cb = cb;\n    };\n    emitter.cancel = function () {\n        if (emitter.cb) { emitter.cb(); }\n    };\n    return emitter;\n}\nThe callback function implementation would be similar to the onCancel in this PR, but with lower level node-postgres API calls, so we could reject the cancelled not-yet-active queries.\nA big downside of this implementation is that every function calling the generic query (or at least db.query) needs an additional argument (but I cannot see a way around that without promise library support).\n. Thanks for your quick response!\nI haven't looked at 6.5.4 yet, but the changes in 6.5.3 addressed my original concern.\nWhen I started testing 6.5.3, found a case, where a transaction rejection got lost with nodejs native Promise.\nIf during a transaction (db.tx(...)) the connection breaks while a query is being executed (eg. SELECT pg_sleep(15)), the inner promise gets rejected correctly, but when the execution reaches the error handler of the db.tx, and it tries to rollback the change, the promise returned from query never resolves.\nThe stack trace of the query in question:\nTrace: return\n    at $p ([...]\\test\\node_modules\\pg-promise\\lib\\query.js:229:17)\n    at Promise (<anonymous>)\n    at promise ([...]\\test\\node_modules\\pg-promise\\lib\\promise.js:23:20)\n    at Task.$query ([...]\\test\\node_modules\\pg-promise\\lib\\query.js:129:12)\n    at Task.<anonymous> ([...]\\test\\node_modules\\pg-promise\\lib\\query.js:235:23)\n    at Task.query ([...]\\test\\node_modules\\pg-promise\\lib\\task.js:106:36)\n    at Task.obj.none ([...]\\test\\node_modules\\pg-promise\\lib\\database.js:473:30)\n    at exec ([...]\\test\\node_modules\\pg-promise\\lib\\task.js:301:22)\n    at rollback ([...]\\test\\node_modules\\pg-promise\\lib\\task.js:292:16)\n    at callback.then.reason ([...]\\test\\node_modules\\pg-promise\\lib\\task.js:241:32)\nThe test code:\njavascript\npgp(cn).tx(t => t.any('SELECT pg_sleep(15)')\n    .catch(e => {\n      console.error('inside', e);   // error: terminating connection due to administrator command\n      throw e;\n    }))\n  .then(r => console.log('outside', r))  // never reaches neither this line\n  .catch(e => console.error('outside', e))  // nor this one\n  .then(() => console.log('tx finished'));   // nor this one\nInterestingly, when using bluebird as promiseLib, I got the expected 'Error: Unexpected call outside of transaction' error on the outside.\nUpdate\nIt is probably some kind of timing issue, because with native Promise at this line: https://github.com/vitaly-t/pg-promise/blob/ede77b39c2d7cb1951cee72775cc6f5fa61071c9/lib/task.js#L101 ctx.db is still not null.\nUpdate 2\nI experimented a bit with travis, if you want to, you can restart postgres during tests like this:\n```javascript\nconst { exec } = require('child_process');\nexec('sudo service postgresql restart', function (err, stdout, stderr) {\n  if (err) {\n    console.error(err); // node couldn't execute the command\n    return;\n  }\n// the entire stdout and stderr (buffered)\n  console.log(stdout: ${stdout});\n  console.log(stderr: ${stderr});\n});\n```\n. In their redshift documentation they seem to imply it is your local network, VPN provider, or other intermediaries who drop the connection. But, because of the scale of their operation, I'm sure they are aggressively pruning idle connections. I just wish they would document it somewhere.\nThe brianc/node-postgres#1847 PR adds two new connection parameters to address these issues:\njs\nconst db = pgp({\n    keepAlive: true,\n    keepAliveIdleMillis: 200 * 1000,\n    connectTimeoutMillis: 10 * 1000,\n})\nThe manual heartbeat would still be required of course, but I couldn't find a general solution for that.. ",
    "Maxwell2022": "Another solution would be to mock all the database drivers.\n. Sorry mate, wrong repo... I was meant to post this on db-migrate... too many tabs open :/\n. Awesome work by the way ;) Thank you!\n. ",
    "EvanCarroll": "Still not sure.\n\nCreate a global/shared database object\n\nWhat makes it global? That's a database handle, is it using the global. object to store the details? I would assume so from that.\n\nyou should create only one global/shared db object per connection details.\n\nBut, why? If it's using the global object to store the pool it should realize the connection details are the same and return the same handle?\nSo again my question is how do we wire an app with this internal pool? Do we,\n- call pgp(cn) with the same cn everywhere to get the same db?\n- have to pass around a db to everything that uses the code?\n- make no difference?\n. Perl provides both of these for a reference through connect_cached and connect.\n. Yep, that makes it clear! That example is great. What you're doing is wrapping the handle in it's own package and not requiring pg-promise anywhere else. This makes sense and is clean, but it's not what the docs would have guided me into doing without that example. Well done.\n. Why not make everything use them? That's how other languages do it, afaik. It's the default in Perl for SELECT, INSERT, UPDATE, and DELETE?\n\nSetting pg_server_prepare to \"1\" means that prepared statements should be used whenever possible. This is the default when connected to Postgres servers version 8.0 or higher.\n. \n",
    "luthv": "sorry for resurrecting an old thread, I was trying the new managed Azure Postgres as a service offering from Azure, and I noticed that the idle connections was not released properly and I keep getting an exception stating that my application have reach the max connection limit set by Azure.\nMy understanding is that the connection pool is implemented by pg.Pool component from the base pg library, is this correct?\nIf so then the problem should be on Azure side that doesn't manage the connection properly. The Azure managed postgres service is still in preview release and I also have logged a question in their feedback forum, I just want to clarify my side of the infrastructure first.. I see, thanks for clarifying that up.\nDo you have any suggestion on how to investigate this problem with azure connection?. ",
    "cybercoder": "and can access from outside of function?\n. I just want to fetch a postgresql table data into an nodejs array, but i can't. It's craziness!\n. I used massive.js and it solved my problem, with that can fetch data into array, simply.\nThe craziness i mentioned was not about your promise, it was about i tried to fetch but i got deadlock.\nRegards.\n. ",
    "hannupekka": "I guess the issue is not in this projects scope - tried also with node-postgres and got same results.\nIf I cast those fields to text in the select, they came out correcly:\ndb.oneOrNone('SELECT *, time_from::text, time_to::text FROM rules WHERE ... LIMIT 1', [...])\n. ",
    "bendrucker": "\ud83d\udc4d  I know node-postgres development isn't that speedy these days. The beauty of npm/modularization is that we can do much quicker patch releases on the crazy parsers!\n. It's hard to understand each other online. I've only blocked someone once for opening a couple of issues all at once and being a bit combative with me. I ended up undoing it after a bit and forgetting about it. I know Brian personally and am more than willing to give him the benefit of the doubt here. \nIf you'd like, email me (listed on my profile) with any PRs or issues you're stuck on them and I can have a look. I'd rather not continue on this thread in public.\n. ",
    "TazmanianDI": "@vitaly-t I think I may have hit this problem as well except for me it's on a large IN clause with a SELECT query. The alternatives for doing large INSERTs and UPDATEs don't apply here so I'm not really sure what to do.\nI have a query that basically looks like this:\nSELECT * FROM \"myTable\" WHERE (\"colOne\", \"colTwo\", \"colThree\") IN (($1,$2,$3),($4,$5,$6),...,,($125698,$125699,$125700))\nAnd the array I'm passing to the database call has 125700 elements in it. But I get the there is no parameter $10000 error out of this. I'm not really sure what to do about this. I could probably split the query into smaller pieces but that seems less than desirable.. The custom format would reduce the number of parameters I've got by a third but that's just a slight shift in the amount of data I've got to process. I think I might like doing that anyway as flattening out the parameters is a bit of an annoyance in our code.\nThis is a bit of an edge case for us; this occurs during some rare \"first time\" data initializations so it's not like this is a regular query (I'd be an idiot to frequently execute a query with 125k parameters ;-). Will the custom format do the right thing when used as a compound value in an IN clause? I.e., the query does need the extra parens around each value:\nIN ((val1, val2), (val3, value4))\nThe values required in the query really are being computed in memory (it's looking for sets of missing data). I'm not sure I understand your recommendation \"SELECT as the source for the parameters\". If you mean use a sub-query to get the parameters, that's not so easy to do in our usage as determining what needs to be queried for is complicated.\nI think I am going to try and rework this anyway as 125k really is quite a lot. Maybe I can break down the data into ranges instead of discreet points. But is there a reason you put {0,3} in the query replace regexp? Is there some specific limitation you're avoiding by putting the limit in there. I understand it's not smart to query with 125k params but seems like the library should allow me to be stupid if it's something Postgres can actually live with.\nI do appreciate the quick responses from you; you're pretty good about that :-).. Oh, so in \ndb.any('...IN ($1:csv)', [a1, a2, ...]);\n$1 refers to the entire array? [a1, a2, ...]? What if I had some additional values that aren't part of that IN clause? Would it be \ndb.any('...IN ($2:csv) AND col = $1', [b1, [a1, a2, ...]]);\n. I hope I'm not doing something stupid but there's something not quite right here: \n```\nfunction MyType(val1, val2) {\n  this.val1 = val1;\n  this.val2 = val2;\n  this._rawDBType = true;\nthis.formatDBType = function() {\n    return pgPromise.as.format('${val1}, ${val2}', this);\n  }\n}\nconst values = [\n  new MyType('00000000-0000-0000-0000-000000000000', '00000000-0000-0000-0000-000000000000'),\n];\nconst sql = 'SELECT * FROM \"myTable\" WHERE (\"val1\", \"val2\") IN ($1:csv)';\ndatabase.any(sql, [values])\n```\nSpits out this:\nRangeError: Maximum call stack size exceeded\n    at MyType.formatDBType (.\\test\\sandbox.js:30:25)\n    at resolveFunc (.\\node_modules\\pg-promise\\lib\\formatting.js:205:29)\n    at $formatQuery (.\\node_modules\\pg-promise\\lib\\formatting.js:234:40)\n    at Object.format (.\\node_modules\\pg-promise\\lib\\formatting.js:686:16)\n    at MyType.formatDBType (.\\test\\sandbox.js:30:25)\n    at resolveFunc (.\\node_modules\\pg-promise\\lib\\formatting.js:205:29)\n    at $formatQuery (.\\node_modules\\pg-promise\\lib\\formatting.js:234:40)\n    at Object.format (.\\node_modules\\pg-promise\\lib\\formatting.js:686:16)\n    at MyType.formatDBType (.\\test\\sandbox.js:30:25)\n    at resolveFunc (.\\node_modules\\pg-promise\\lib\\formatting.js:205:29)\nThe formatDBType seems to be getting stuck in a loop.. Okay, that does seem to work with one more tweak. I did have to put parens around the values in formatDBType in order to get them to appear properly as a composite IN clause. That does mean this formatting does need to know something about how it's used (specifically being used in an IN clause). Are there other circumstances you had in mind for supporting this kind of formatting where the parens would actually be bad and this formatting function wouldn't work (I guess I'd have to create a new type for that unless I can specify the formatters per request).\nreturn pgPromise.as.format('($1, $2)', [this.val1, this.val2]);. You might be missing that I'm executing a query with an IN clause using composite values (i.e. more than one column). If I leave out the parens from the formatDBType function:\n```\nfunction MyType(val1, val2) {\n  this.val1 = val1;\n  this.val2 = val2;\n  this._rawDBType = true;\nthis.formatDBType = function() {\n    return pgPromise.as.format('$1, $2', [this.val1, this.val2]);\n  }\n}\nconst values = [\n  new MyType('1', '2'),\n  new MyType('3', '4'),\n];\nconst sql = 'SELECT * FROM \"myTable\" WHERE (\"col1\", \"col2\") IN ($1:csv)';\ndatabase.any(sql, [values]);\n```\nThis generates the query: \nSELECT * FROM \"myTable\" WHERE (\"col1\", \"col2\") IN ('1', '2','3', '4')\nThe parens are missing inside the IN clause. This query should be:\nSELECT * FROM \"myTable\" WHERE (\"col1\", \"col2\") IN (('1', '2'),('3', '4')). ",
    "LordZardeck": "Nvrm, forgot to specify the bluebird library, noticed the native Library doesn't have .finally()\n. ",
    "ghusse": "OK, I understand.\nI suppose it would be possible for the JS code to transform queries with named parameters to a query with indexed parameters. But for now it's not the case.\n. I figured (looking at the source code) I needed to create an object for each query. pg-promise seem to handle the creation only once by session.\nThe API is not very clear on this point. I assumed I could reuse the same object (in a same way the prepared statement reuses a declaration on the server).\nThanks for the utility\n. Hi,\nThanks for the update, I will definitely test it again with the new version.\nWhat do you suggest instead for the updates? In my case I have to run\nmultiple update queries that use another table for the condition (like a\njoin but there is no join condition in updates in of)\nLe 21 mai 2016 6:24 AM, \"Vitaly Tomilov\" notifications@github.com a\n\u00e9crit :\n\n@ghusse https://github.com/ghusse Later on as I was doing some\nperformance optimization within the query execution, your post helped me\nrealize that even though execution updates isn't such a good idea, it still\nshould work regardless. So, I have reviewed the code and made some changes\nthat I just released as version 4.2.4\nhttps://github.com/vitaly-t/pg-promise/releases/tag/v.4.2.4.\nThank you for the idea. Your code example that you gave now should work as\nyou expect it. But I still strongly suggest that you do it differently, if\nyou want to get good performance out of it ;)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/146#issuecomment-220757724\n. Thanks for the info & the docs. I did not know about this utility.\n\nI'm not sure to see how to use it in my case, because I have to use a query like this one:\nUPDATE table1 set table1.val = $1 \nFROM table2\nWHERE table1.foo = table2.foo\nAND table1.id = $2\nThe update utility generates a query including the FROM and WHERE parts. If I want to reuse that, I need to either parse the generated query to insert my second table after the FROM or generate a query by myself with the same 'trick' (listing values in the query).\n. Thanks for the help.\nI went to this kind of solution, I just used as.format to generate values. I have an utility for that in our code, and this way I have the whole query in a file.\n. That's exactly what I did :)\n. Great news! That is exactly what I wanted to use :)\n. ",
    "geonanorch": "I was about to open an issue when I found this one. I just experienced the same problem: using a prepared statement repeatedly with separate queries, something like:\nconst ps = new PS('my-insert', 'INSERT INTO mytable(id) SELECT $1');\nmyarray.map( (id) => {\n  ps.values = [ id ];\n  pgp.none(ps);\n})\nArguably and as I learned from this thread that is not the proper way to chain inserts, but that is not the point: the fact that the prepared statement values are not copied during the call to pgp.none() was counter-intuitive for me (like for @ghusse). Currently if one wants to play it safe a prepared statement should only be used in one place of the code \u2013and even then paying attention not to loop.\nIt seems to me that ps.values should be copied synchronously. Or be locked read-only while a query is using it. Or at least have a big fat warning in the doc that it should not be modified until said query resolved.\nThis being said, I am very grateful for this great library and its excellent documentation. And I shall use helpers.update()!  ;-). ",
    "VersBinarii": "Apologies for hijacking but is there a way to aggregate the error info available in the error event, package it up in a custom my_error object that can be sent to client?\n. Very good. Thanks guys!\n. Understood. I suspected this could be the case after looking at the code but was hoping that i'm missing something :)\nThanks for great library!. ",
    "zzeng4": "INSERT INTO listing (unit_number) VALUES ('123123') ON CONFLICT (mls_id) DO UPDATE SET (unit_number) = (${unit_number}) WHERE listing.mls_id = ${mls_id};\nthis is my query.\n. INSERT INTO listing (unit_number) VALUES ('123123') ON CONFLICT (mls_id) DO UPDATE SET (unit_number) = (null) WHERE listing.mls_id = 'W3123123';\nthis is the final string,\nand it works when i test it in console\n. it might be the version misuse. How can I test whether the library is using the 9.5.3 version? as the ON CONFLICT clause is only released after 9.5.0+.\n. this is my bad, i test it on my local server, which is the 9.5.3 and the one i am using is the remote server, which is 9.4.7.\nthat's why it doesn't work..\nThanks for help me......\n. ",
    "nareshbhatia": "When I run the following command:\ntypings install --save --global  dt~pg-promise\nit automatically add the following to typings.json:\n{\n  \"globalDependencies\": {\n    \"pg-promise\": \"registry:dt/pg-promise#0.0.0+20160417141255\"\n  }\n}\nThis is different from what you are showing. If I switch to your version, everything works fine.\nI think the issue is due to the new version of typings (v 1.0.4). Older version (0.8.x) used to work differently. What version of typings are you using?\n. Ok, will do. Thanks for the quick response.\n. Ah, good to know!\n. Like it a lot! Thanks.\nI am really enjoying using pg-promise.\n. Hmmm, isn't there any way to just issue a SET intervalStyle to Postgres and have it return the desired format instead of converting using pgp.pg.types?\nI am switching over from knex, and there I used to be able to do this:\n```\nvar pool = {\n    // Ask Postgres to return intervals as ISO 8601 strings\n    afterCreate: function(connection, callback) {\n        Promise.promisify(connection.query, connection)('SET intervalStyle = iso_8601;', [])\n            .then(function() {\n                callback(null, connection);\n            });\n    }\n};\nvar dbConfig = {\n    client: 'postgresql',\n    connection: {\n        host: 'localhost',\n        user: '',\n        password: '',\n        database: 'staffer',\n        charset: 'utf8'\n    },\n    pool: pool\n};\nvar knex = require('knex')(dbConfig);\n```\nI was assuming that pool.afterCreate() is a feature at node-postgres level and I should be able to reuse it over here.\n. Thanks, Vitaly. I see that the feature described above is at the knex level - see here.\nHowever, can you please clarify why this will slow down the communications? If I understand it correctly, SET intervalStyle will be issued only once per connection in the pool - not every time we are executing a general query.\nAnother consideration is that by default Postgres is sending intervals like this: { \"minutes\": 15 }. Writing a parser to convert this to ISO 8601 seems like needless effort when Postgres can do it for me.\nWould love to hear your thoughts.\n. You just read my mind :-). I was going to ask you how connection pooling works. From your answer, I am assuming that a connection obtained from the pool can either be a fresh connection or a previously used connection. I would have to run SET intervalStyle only on fresh connections. Correct?\n. Just tried it. It is not working. Investigating.\nHere's my code:\nvar pgpOptions = {\n    promiseLib: Promise,\n    extend: (obj: any) => {\n        ...\n    },\n    connect: (client) => {\n        // Ask Postgres to return intervals as ISO 8601 strings\n        client.query('SET intervalStyle = iso_8601');\n    }\n};\nWhen I console.log the interval property it prints this:\nduration: PostgresInterval {},\nHowever the same query in psql returns correctly:\n```\nSET intervalStyle = iso_8601;\nselect duration from projects where id = 40;\n| duration |\n| PT1H     |\n```\nAlso used to work in knex.\n. Ah, I think I know what it is. In my knex implementation, I had to override pg-type for this.\n```\n// Keep interval as an ISO 8601 string. By default pg converts it to an object.\nvar parseInterval = function(val) {\n    return val;\n};\nvar INTERVAL_OID = 1186;\ntypes.setTypeParser(INTERVAL_OID, parseInterval);\n``\n. It is working now, but with both pieces of code applied (which is my intent):\n1. First tell Postgres to send intervals in ISO format:SET intervalStyle = iso_8601`\n2. Then tell pg-types to not do any conversion on the returned value:\nvar parseInterval = function(val) {\n       return val;\n   };\nThe combination works like a charm! Now if I could only have the fresh connection flag :-)\n. It is a bummer. I am using the JS client though.\n. @vitaly-t, thanks so much!\nI am having bit of an issue compiling with TypeScript:\nconnect: (client, dc, fresh) => {\n    // For fresh connections, ask Postgres to return intervals as ISO 8601 strings\n    if (fresh === true || typeof fresh === undefined) {\n        client.query('SET intervalStyle = iso_8601');\n    }\n}\nI am getting this error:\nerror TS2345: Argument of type '{ promiseLib: PromiseConstructor; extend: (obj: any) => void; connect: (client: any, dc: any, fre...' is not assignable to parameter of type 'IOptions<{}>'.\n  Types of property 'connect' are incompatible.\n    Type '(client: any, dc: any, fresh: any) => void' is not assignable to type '(client: Client, dc: any) => void'.\nAre the type definitions updated? I tried installing fresh type definitions, but I get this error:\n$ typings install --save --global  github:vitaly-t/pg-promise\ntypings WARN badlocation \"github:vitaly-t/pg-promise\" is mutable and may change, consider specifying a commit hash\ntypings INFO reference Stripped reference \"https://raw.githubusercontent.com/vitaly-t/pg-promise/master/typescript/pg-subset\" during installation from \"pg-promise\" (main)\ntypings INFO reference Stripped reference \"https://raw.githubusercontent.com/vitaly-t/pg-promise/master/typescript/pg-minify\" during installation from \"pg-promise\" (main)\ntypings INFO reference Stripped reference \"https://raw.githubusercontent.com/vitaly-t/pg-promise/master/typescript/ext-promise\" during installation from \"pg-promise\" (main)\n. No worries.\n. @vitaly-t, thanks so much for all your help on this issue. All working smoothly now.\n. ",
    "msaron": "One problem I had was I was developing on Windows and deploying on linux. The filenames generated have windows back slashes. I had to use npm module normalize-path to generate unix paths. . ",
    "bali182": "My bad then, lesson learned: when there is a readme, always read it :smile: Thank you!\n. ",
    "valeriangalliat": "process.mainModule is not defined when run from Node interpreter.\nI just updated my PR, was wrong passing cwd() to dirname, should be instead of dirname when process.argv[1] is not defined.\nIt's still not the same as the process filename, but there's no process filename when run from interpreter, so cwd() is probably close enough.\nBut to have it deterministic between interpreter and regular file, it would be better not supporting relative configuration file from the CLI I guess.\n. pg-connection-string parses it as database name:\njs\nrequire('pg-connection-string').parse('foo')\n{ port: null,\n  host: null,\n  database: 'foo',\n  user: '',\n  password: '' }\nThat's because it's treated as a URL and foo is an URL with just a pathname as parsed by url.parse. It's not explicitly documented but that's a natural consequence of the connection string being treated as an URL.\nI always used this when using pg and pg-promise before, like I would do psql foo to connect to local foo database (I found it intuitive at least, but if I'm the only one doing this that's probably not worth supporting).\nBut note this change also removes support from socket connection strings documented here.\nHaving different connection interface with pg, even though it's documented, can break the user expectation, especially when moving from pg to pg-promise. While the connection parameter is properly documented by pg-promise, I'd think it's better for an API design point of view to have pg-promise always pass the connection string/object raw to pg and let it handle it.\nFeel free to close this issue if you don't want to maintain 1:1 compatibility with pg for the connection parameter, I just wanted to make sure it's intended. \ud83d\udc4d \n. Thanks a lot!\n. ",
    "Iazzetta": "Hmmm work for me using this:\npgp.pg.defaults.ssl = true;\n. ",
    "jamesvanleuven": "ok, so I noticed it just returns. I don't have to actually deal with the Promise.resolve/reject/define aspects huh? I didn't realize that, I just did a test and it returns it anyway..\nI wasn't aware of it. All the examples have the console.log(something); which is fine, but nobody on earth returns the console log and that's it... lol \nI'm coming from an AngularJS viewpoint so I'm thinking\nvar deferred = $q.defer(); and then the return deferred.resolve(value); and return deferred.reject(err);\nbut I just doesn't require it... i'm still reading the docs and am realizing this is ES6 not JS so ...\n. um, why does your documentation then have a reference to the Javascript Promise reference?\nhttps://developer.mozilla.org/en/docs/Web/JavaScript/Reference/Global_Objects/Promise\non this page as an external promise? http://vitaly-t.github.io/pg-promise/external-Promise.html\nI am reading your docs dude. I've been reading it all day, and as useful as the chaining is, it isn't intuitive... \n. ",
    "und3fined": "Thanks reply.\nDo you have any way to fix this problem?\n. ",
    "dugouchet": "So I have a problem to JSON.parse my object ... DO someone had this problem ?. ok thank you for your quick response @vitaly-t . ",
    "cowboy": "Hmmn.. this is the error I just saw:\nerror: column reference \"name\" is ambiguous\n    at [object Object].Connection.parseE (/Users/cowboy/repos/bocoup/expertisebot/node_modules/pg/lib/connection.js:539:11)\n    at [object Object].Connection.parseMessage (/Users/cowboy/repos/bocoup/expertisebot/node_modules/pg/lib/connection.js:366:17)\n    at Socket.<anonymous> (/Users/cowboy/repos/bocoup/expertisebot/node_modules/pg/lib/connection.js:105:22)\n    at emitOne (events.js:77:13)\n    at Socket.emit (events.js:169:7)\n    at readableAddChunk (_stream_readable.js:146:16)\n    at Socket.Readable.push (_stream_readable.js:110:10)\n    at TCP.onread (net.js:523:20)\n. That was the exception I got when I made a query using the QueryFile object. I'm not seeing any errors when creating the QueryFile object, but I am when querying the db using that object.\n. Sorry, I apologize for not being clear, let me try again. When I do this, I see that stack trace because of an error in my_query.sql.\njs\nconst pgp = pgPromise(options);\nconst db = pgp(config);\nconst queries = utils.enumSql(__dirname, {}, file => {\n  return QueryFile(file, {\n    minify: true,\n    debug: true,\n  });\n});\ndb.query(queries.myQuery).then(r => console.log(r));\nIs there any way for the error message to include my_query.sql somewhere, to aid in debugging?\n. Ok.\n. ",
    "shesko": "oh, must have missed that section. Thank you!\n. Have you checked the demo code I shared on my comment?\nHere is the link: https://github.com/shesko/pg-promise-error-handling\n. In the github repository that I linked you, I explain on the README file that I used your demo code to demonstrate the issue I am concerned with. In the db/repos/users.js file I unpurposely put a typo in the first argument of the function called in line 16. \nThe function I call is:\nrep.one(sql.adds, name, user => user.id)\nbut the sql file was actually defined as sql.add.\nWith this typo in place the resulting error is:\n{\"success\":false,\"error\":\"Invalid query format.\"}\nTo replicate the error just clone that repository, install the dependencies and run node index.js. Once the server application is running, you can insert the following link in your browser: \nhttp://localhost:3000/users/add/john\n. Ok, your solution makes more sense :)\n. thnx!\n. ",
    "jonathan-stone": "Thanks for your response!\nMaybe I don't understand what \"initializing the library\" means. I thought this line did that:\nvar db = pgp()(dbConfig);\nWhat is your definition of initializing the library? And what is \"the library?\" These probably sound like silly questions to you, but I can't tell from reading the docs.\n. Oh, I think I get it. So I need to change my require call to:\nvar pgp = require('pg-promise')();\nTested it and no longer got the error. Thanks!\n. ",
    "sbusch99": "Thanks\n. Thank you! I will try to test this out this weekend.. I am - I brought up the issue because of this. Due to business\ncircumstances, I no longer needed to use pg-promise. I can resurrect the\ncode if you wish.\nSteve Buschman\n978-254-1114\nOn Dec 30, 2016 7:06 PM, \"Vitaly Tomilov\" notifications@github.com wrote:\n\n@bIgBV https://github.com/bIgBV are you using TypeScript 2?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/220#issuecomment-269837047,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ANRyxzXNP5IWb0n186ngbgUhfjoq8P5qks5rNZyRgaJpZM4KWffE\n.\n. \n",
    "raine": "How does one pass postgres keyword default with { partial: true } or possibly { default: <something> }?\nWhen I use pgp.as.format(query, { location: undefined }, { partial: true }) it leaves undefined values unchanged in the query, causing syntax error:\nVALUES ('blah', $[location])\nerror: syntax error at or near \"$\"\nTo clarify, the use case is that I'm making a parameterized query and the passed values object may not contain all the parameterized values, in which case I'd like postgres to use the default value set in the table for that column. default keyword in postgres seems to allow that.. Oh yeah, the properties are actually missing, not set as undefined. That was just the example which I thought would clarify but I guess it had the opposite effect. \ud83d\ude01. ",
    "CalebEverett": "I was going off of those, but missed a module I was pulling into one of my route files. Thanks again\n. ok, thanks\nsomebody just posted a configuration that works:\nhttps://github.com/brianc/node-postgres/pull/1072/commits/38c6a8d53e748641a024241cba774b3b354c426f\n. ",
    "thenikso": "I've a followup on this. I'm using pg-promise both in my app and on an integration test wrapper that initialize the test database to then import and call the app.\nI get this warning which I believe it's not a problem and I can safely ignore it. However do you have any further best practice regarding this kind of scenarios?. ",
    "MitMaro": "We have a distributed microservice like system, that sometimes runs several independant services within the same node process, with an unknown set of database connections. This meant that explicitly using a shared module wasn't a viable solution. Instead we ended up creating a factory that would use a hash of the configuration to return a cached instance. Looks something like this:\njavascript\nconst pgPromise = require('pg-promise')(/*initialization options*/);\nconst cache = new Map();\nmodule.exports = function  databaseConfig) {\n    const dbKey = JSON.stringify(databaseConfig, Object.keys(databaseConfig).sort());\n    if (cache.has(dbKey)) {\n        return cache.get(dbKey);\n    }\n    const instance = pgPromise(databaseConfig);\n    cache.set(dbKey, instance);\n    return instance;\n};\nedit: Fixed a mistake I made while breaking out the example. Thanks @vitaly-t for pointing it out.. I made a mistake when I pulled out the example from the codebase. It's a database connection, not the pg-promise instance. Our actual code is a bit more complex. I've edited the example.\nI was initially suppressing the warning but wanted to use a shared connection pool when possible. I didn't think to check to see if pg-promise reused an existing connection pool on the same connection parameters, but my guess is that it does not.. Interesting. I think I found a bug in either pg-promise or node-postgres then. I will do up a test case and report it.. Looks like this won't be an issue for at least node 7.\nhttps://github.com/nodejs/node/commit/4fffe32a4b3a93548c4d39b1c2531aa38403b5e6. I believe this is a duplicate of #234 . The warning message issue is resolved but I am still seeing a set of duplicate connections. I was hoping to do some further testing on this last week, but was unable to find time.. ",
    "benoittgt": "Hello\nThanks for pg-promise ! Love using it.\nI have very simple code (that run on an AWS lambda) but I get THE warning :\n\nundefined WARNING: Creating a duplicate database object for the same connection.\nat exports.handler (/var/task/index.js:20:14)\n\nHere is my code :\n``javascript\n'use strict';\nconst config = require('./config_from_env');\nconst connParams =pg://${config.user}${config.password}@${config.host}/${config.database}`;\nconst pgp = require('pg-promise')();\nconst moveTravels = function(travelType) {\n  return BEGIN TRANSACTION ISOLATION LEVEL REPEATABLE READ;\n      -- 2 query...\n      COMMIT;;\n};\nexports.handler = function(event, context) {\n  const db = pgp(connParams);\nreturn db.tx(function (t) {\n    return t.batch([\n      t.none(moveTravels('arrivals')),\n      t.none(moveTravels('departures'))\n    ]);\n  })\n    .then(function () {\n      return context.succeed(Successfully moved);\n    })\n    .catch(function (error) {\n      return context.fail(Failed to run queries : ${JSON.stringify(error)});\n    });\n};\n```\nWhy I have the warning ? I know it's friday but I don't see duplicated database object.. Thanks a lot with your comment I just discover : http://vitaly-t.github.io/pg-promise/txMode.TransactionMode.html\n\nthis is designed to be called more than once:\nBut this handler is called only once (in my case every 4 min)\n\nThanks again. Thanks a lot again @vitaly-t for the fast and correct answer. \ud83c\udf89. Not exactly. I would like to get the numbers of rows inserted in case of an INSERT TO.\nGood job for pg-monitor. Didn't know it exists.. I'm sorry. For example if I have this query : \nsql\nINSERT INTO sensor_log_b (location, reading, reading_date)\n  SELECT id, location, reading, reading_date\n  FROM sensor_log_a\n  WHERE reading = 22;\nI would like to log the answer (for example: INSERT 0 5230). Yes you are probably right. \nIn my case I have an AWS lambda that runs an INSERT INTO query often against Redshift database and sometimes (around 1%)  rows are not inserted. I thought displaying this log could have help me find the issue.\nThat's a very specific case.. Thanks a lot @vitaly-t for your answer and your time. \n.catch is implemented but didn't catch any errors.. Thanks @vitaly-t \nI was wrong. Query was correct but not properly reloaded. After adding debug: true. It's correct. \nSorry\nPS: I ran my own test using a fork of pg-promise and a lot's of tests were failing because of wrong database setup. Do you think a CONTRIBUTING.md shouldn't be useful to help to run project locally?. @vitaly-t My bad... I scroll down the Readme looking for those infos and didn't see them in the middle. I was wrong again. I promise that I would read the doc 3 times before opening an issue the next time. Thanks again for your patience.. ",
    "lakesare": "these winky emojies are awful, I can't think of anything more pseudo-friendly and condescending.. ",
    "RomeHein": "Hello!\nI'm currently trying to implement authentification on my API. I was wondering how could I manage different user connections to the db?\nThis is how I currently create the connection:\n```\nconst pgp = require('pg-promise')();\nlet instance = null;\nmodule.exports = class Connector {\nconstructor(user,psw) {\nif (!instance || (user && psw)) {\n  instance = this;\n\n  this.config = {\n    user: user || process.env.boUser,\n    database:process.env.boDatabase, \n    password: psw || process.env.boUserPassword, \n    host:process.env.boHost, \n    port:process.env.boPort,\n  };\n\n  this.db = pgp(this.config);\n}\n\nreturn instance;\n\n}\n}\n```\nAnd I use it like so (Connector is a global variable):\nmodule.exports = class Pack {\n    ...\n    static all() {\n        return Connector.db.tx(t => {\n            return t.batch([t.manyOrNone(sql.pack.findAll)]);\n        })\n        .then(data => {\n            if (data[0].constructor === Array && data[0].length > 0) {\n                return Pack.parseArray(data[0]);\n            }\n            return null;\n        });\n    }\n}\nI'm a bit lost and don't get how I can create multiple connections.\nLet's say my API receive 10 requests from 10 different users, with different access rights on the db, how can I manage that on the API side?\nI feel like my pattern is not right.. Ok thanks for your quick answer!\nI'll move the authentification logic to the API, and keep only one user to perform the IO connections to the db.\nThx again. Hey @vitaly-t,\nI'm having an issue to move from 5.9.7 (last of the 5.x) to v6.\nWhen I run my code with the v5, everything works fine. But when I update to v6, I got this kind of issues:\n\nRelation \"project\" does not exist error\n\nHere is an exemple of where I got that issue:\nstatic find(projectIdentifier) {\n        if (!projectIdentifier) {\n            return Promise.reject(new Error('Project informations needed to connect to the db'));\n        }\n        return connector.db.task(t => {\n            return t.batch([\n                t.oneOrNone(sql.project.findById, projectIdentifier),\n                t.manyOrNone(sql.project.participantsByProjectId, projectIdentifier)\n            ]);\n        })\n            .then(data => {\n                var project = null;\n                if (data[1]) {\n                    project = new Project(data[1]);\n                    if (data[2] && data[2].length > 0) {\n                        data[2].forEach(function (participant) {\n                            project.participants.push(new User(participant));\n                        })\n                    }\n                }\n                return project;\n            })\n            .catch(error => {\n                console.log(error);\n                return error;\n            });\n    }\nand here is the sql file for sql.project.findById:\nSELECT project_id,project_name,project_description,project_creator as creator_id,user_name as creator_name,project_channel,project_state AS state_id,state_name FROM Projects \nINNER JOIN Users ON Users.user_id = Projects.project_creator \nINNER JOIN States ON project_state = state_id\nWHERE Projects.project_id = $1;\nI'm aware that this issue is due to the case sensitiveness of pg, but I'm not sure the way I'm creating my tables is wrong: \nCREATE TABLE if not exists Projects (...);\nNote that I'm not using the double quote around the table name.\nIs there something I'm missing to update to v6? Is it the new node-postgres driver?\nBtw I always wanted to say how I found your job amazing. Honestly, if I could be half as good as you are one day, it would be great!. Ok, it's a bit weird though, cause when I run the sql command directly against the db it works fine, same when using v5, but not with the v6.\nAnyway, you are right, I should change for projects. \nI've read couple posts on stackoverfow saying that using double quotes was a bad idea.\nThanks for your answer, much appreciated.. ",
    "rbhughes": "Hi @digualu, I had this requirement earlier this week and found that altering the login role (in pgadmin3  or psql or whatever) was the most pragmatic solution. For example, my schema is ppdm, so this worked:\nALTER ROLE ppdm_user SET search_path TO ppdm\n. ",
    "digualu": "Hi rbhughes, if I understand correctly, your solution requires to append SET ROLE before issuing each query since it uses connection pool, we can't tell whether that connection has the correct role unless we explicitly set it up. But I guess that's the best we can do with the connection pool. Thanks For the advice!\n. ",
    "amenadiel": "However... he could execute SET search_path TO schema1 and then call the stored procedure as two consecutive queries inside a transaction.\nOR\nHe could pass the schema as a parameter to the function\n```sql\nCREATE OR REPLACE FUNCTION public.foo(the_schema text) RETURNS TEXT AS\n$$\n...\nEXECUTE 'SET search path to $1' using the_schema;\n...\n$$\n``\n. As a matter of fact, for common scenarios you can just prepend the schema name to the table or function and never change the schema frompublic. You can also set thePGAPPNAMEenvironment variable. If you are usingdotenv` in yout project, then it will be sent by default when opening the connection. This also works natively on Heroku.. It depends on the contents of the SQL file. If your files contains COPY statements, linebreaks instead of semicolons, tabs instead of commas and  \\N instead of null statements (typical copy dump format) then you won't be able to parse it with pg-promise nor any pg syntax interpreter. \nIn postgres tools such as pgAdmin (or phpPgAdmin) you don't load the dumps in the same panel you execute queries. You have to upload such scripts as attachments which will in turn execute through an asynchronous task.\nThis means you don't execute such a file. You copy it from STDIN. \nExecuting such scripts is completely out of the scope of syntactic sugar provided by pg-promise. Well, I'm the current mantainer of https://github.com/HuasoFoundries/phpPgAdmin6 which is a fork of a long abandoned project. \nThe part to process such a file is a very, very convoluted method taken from the very source code of bin/psql/mainloop.c lexer.\nSince the driver won't provide a lexer, and pg-promise is a superset of a driver, the only approach I see is to build a lexer on JS.. Even if the issue is showing up just in one server, there should be some kind of trace.\nHow are you wrapping the SQL operation? Sometimes I see that using try/catch blocks inside async / await sequences does enter the catch block but the error doesn't show up unless you specifically log err.stack.. ",
    "goenning": "@vitaly-t Have you tried https://github.com/typed-typings/typed-pg? I'm trying to keep it up to date as much as possible.\n. Not really. \nBut I found the answer here https://github.com/npm/npm/issues/11100\nMy docker host is a Digital Ocean 512mb server which is causing trouble due to its low memory. Just tried yarn instead of npm. It worked \ud83d\ude04\nAnyway, it's not pg-promise related.\n. ",
    "jpdesigndev": "Oh, wow. I suppose I was just too deep to see what was right in front of me. Thanks!\n. ",
    "vince166": "Thank you for the lightning fast reply! The example you gave seems to be the Explicit pass-through approach in the stackoverflow link that I mentioned. I will give it a try. \nBest regards.\n. Thanks a lot for the tips. \nWhat is the effective way to debug? The exceptions seem to be caught internally by pg-promise. The final error caught by the expressjs error handler doesn't contain much useful information. Is adding print statements or adding more catch() the only way to go? \nI am now stuck at this error  TypeError: undefined is not a function. Which seems to originate from the line queries.append (t.one(\" SELECT email FROM \\\"user\\\" WHERE user_id = $1 \", userId)). \n```\n        return db.many(\" SELECT s.name AS name, s.description AS description, \" +\n                \"        s.default_value AS default_value, us.user_value AS user_value \" +\n                \" FROM \" +\n                \" (SELECT setting_id, name, description, default_value, $1::uuid AS user_id FROM setting) AS s \" +\n                \" LEFT JOIN user_setting us ON s.setting_id = us.setting_id AND s.user_id = us.user_id \" +\n                \" WHERE s.user_id = $1\", userId)\n            .then(function (userSettingRecs) {\n                return db.task(function (t) {\n                    logger.debug(\"Got userSettingRecs: \" + JSON.stringify(userSettingRecs));\n                    var queries = [];\n                    var seenDefaultValuePlaceHolders = {};\n                    logger.debug(\"userSettingRecs: count=%d\", userSettingRecs.length);\n                    for (var i = 0; i < userSettingRecs.length; i++) {\n                        logger.debug(\"looping\");\n                        var userSettingRec = userSettingRecs[i];\n                        var defaultValue = userSettingRec.default_value;\n                        logger.debug(\"default_value type is: \" + typeof(defaultValue));\n                        if (userSettingRec.user_value === null &&\n                            isDefaultValuePlaceHolder(defaultValue) &&\n                            !_.has(seenDefaultValuePlaceHolders, defaultValue) ) {\n                            logger.debug(\"appending \");\n                            queries.append(\n                               t.one(\" SELECT email FROM \\\"user\\\" WHERE user_id = $1 \", userId)\n                            );\n                        // Never reach here...\n\n                        logger.debug(\"after appending \");\n                        // getUserSpecificDefaultValue(t, defaultValue, userId));\n                        _.set(seenDefaultValuePlaceHolders, defaultValue, true);\n                    }\n                }\n                //logger.debug(\"Making batch queries for defaultValuePlaceHolders: count=%d\", queries.length);\n                logger.debug(\"Making batch queries for defaultValuePlaceHolders: \");\n                return t.batch(queries);\n            })\n            .then(function (results) {\n                logger.debug(\"Got defaultValuePlaceHolder map: \" + JSON.stringify(results));\n                var defaultValues = {};\n                for (var i = 0; i < results.length; i++) {\n                    defaultValues[results[i].name] = results[i].value;\n                }\n                return [userSettingRecs, defaultValues];\n            })\n            .catch(function (error) {\n                logger.debug(error);\n            });\n        })\n        .then(function (results) {\n            logger.debug(\"Got both userSettingRecs and defaultValue map: \" + JSON.stringify(results));\n            var userSettingRecs = results[0];\n            var userSpecificDefaultValues = results[1];\n\n```\n2016-07-20T01:23:39.180 - debug:  TypeError: undefined is not a function\n    at Task. (index.js:118:37)\n    at callback (\\node_modules\\pg-promise\\lib\\task.js:191:25)\n    at Function.Task.exec (\\node_modules\\pg-promise\\lib\\task.js:291:12)\n    at \\node_modules\\pg-promise\\lib\\database.js:1177:45\n    at tryCallOne (\\node_modules\\promise\\lib\\core.js:37:12)\n    at \\node_modules\\promise\\lib\\core.js:123:15\n    at flush (\\node_modules\\promise\\node_modules\\asap\\raw.js:50:29)\n    at process._tickCallback (node.js:355:11)\n. Thank you so much! Theappend is exactly the cause. Is there any suggestion to catch this kind of error early? In Expressjs, I usually do not catch errors in the intermediate steps and instead depend on the final error handler. With pg-promise this seems to result in the hard-to-find errors. For example in this case when I added the catch() to the wrong place I got theLoose request outside an expired connection error instead. Any suggestion on debugging and error handling when using pg-promise? \n. Sorry... I should have read the documentation more thoroughly: \nhttp://vitaly-t.github.io/pg-promise/global.html\n``` js\nvar options = {\n// pg-promise initialization options...\n\nerror: function (err, e) {\n\n    // e.dc = Database Context\n\n    if (e.cn) {\n        // this is a connection-related error\n        // cn = safe connection details passed into the library:\n        //      if password is present, it is masked by #\n    }\n\n    if (e.query) {\n        // query string is available\n        if (e.params) {\n            // query parameters are available\n        }\n    }\n\n    if (e.ctx) {\n        // occurred inside a task or transaction\n    }\n  }\n\n};\n``\n. Thanks for quick response. I was under the impression that a query, once loaded by QueryFile, can no longer be appended to.  Please let me know if I was wrong. For example:  two complex queries only vary by one predicate: \"col1 = $1\" or \"col1 IS NULL\".  We want to take advantage of QueryFile to maintain the query in a file while still want to be able to append to the query conditionally (so that we don't have to create two sql files that are almost identical). It would obviously work if the common part of the query is maintained as a string in code, but I am not sure how it would work with QueryFile. . Now I think the return from QueryFile is just a string. So there should be no problem modifying it further. I will try this. Thanks. . Hmm. The return from QueryFile is an object with a 'query' getter. But I'm not sure if it makes sense to modify a \"prepared\" query, and then prepare it again:db.any(sqlByQueryFile.query + (isNullCase ? 'AND col1 IS NULL' : 'AND col1 = $1), targetCol1Value); . Aha, that's how we use:raw`. Thank you so much!. ",
    "dzaman": "The downside of solving this problem by introducing custom methods via the extend event is that I lose the ability to use existing convenience methods like one, many, etc. without re-implementing them on top of my custom query event. Those methods were 1/2 of the reason I adopted this library.\nIt might be useful to have a query-preparation event called before \"Invalid query format\" error is thrown or to allow the existing methods (like query) to be redefined. (Why are they read-only?)\n. That's fair, I can see cases when people might accidentally get themselves into trouble by overriding these behaviors in a way that isn't backwards compatible.\nI think it's valuable to let users easily customize modules, but the solutions available (namespacing, forking) are reasonable alternatives.\nThanks for your time!\n. ",
    "Richie765": "Thanks for the fast response. Perhaps I misunderstood the function of it, but what I'm looking for is this. I have an array of items I'd like to insert, but not all fields are set. For instance:\njavascript\nvar countries = [\n { name: 'Canada', currency: 'CAD' },\n { name: 'Australia' },  // Currency not yet set\n];\nThen I iterate each item and insert the record in the db:\njavascript\n// loop countries\ndb.none('INSERT INTO Country(name, currency) VALUES(${name}, ${currency})', country);\nThat would succeed for the first row, but fail for the second: Property 'currency' doesn't exist. So I have to put something in like this:\njavascript\nif(! ('currency' in country)) country.currency = undefined;\nI've quite a bit of data like that, and it would be nice it it would just insert NULL values where the property doesn't exist.\nI suppose it could be done like this, but a default partial = true somewhere would make it easier.\njavascript\ndb.none(pgp.as.format('INSERT INTO Country(name, currency) VALUES(${name}, ${currency})', country, { partial: true }));\n. That looks good, I'm going to try it after the weekend! Thank you!\n. ",
    "noinkling": "How do you let the database determine a default for the column?\nWhen feeding columns to helpers.update at least I can specify skip: ({exists}) => !exists to omit a column that doesn't exist on the object (even if it feels clunky), but helpers.insert ignores that property. def: null and def: undefined both set NULL regardless of any default set on the column in the db itself, and def: 'DEFAULT' just sets that string.. > What you are suggesting would only work for a single-row insert, but not for multi-row inserts, and thus offer little value. See #239\nAs you show in your next post, if you're generating a multi-row insert, it could just set DEFAULT for the relevant records instead of omitting the column, same effect right? I mean, even for single-row insert that would be fine too if it makes the implementation cleaner/easier.\n\nWhy? You can define that logic only once, in a nice, reusable way:\n\nI do like this (thanks for making it look pretty), but it really is just a matter of convenience and having something obvious instead of the developer having to spend time working out and implementing something like this themselves. I mean, even if you just included that optCol function on helpers (maybe with a different name) I would be happy:\n```js\nvar pgp = require(...);\nvar { insert, update, optional } = pgp.helpers;\nvar query = insert(req.params, ['title', optional('author')], 'books');\n```\nIt could maybe even be made into a tag function: optional`author`.. You'd still have that freedom though right? This would just be convenience for a relatively common use-case.\nAnyway I respect your decision, thanks for the well-maintained project.. Thanks \ud83d\udc4d For some dumb reason it didn't occur to me that def would respect the custom type (maybe because I was too busy seeing if I could make the mod option work dynamically).\nFor reference my pared-down implementation looks something like this now, in case it's useful to someone else, or you want to add an example in the docs or something:\n```js\nconst DEFAULT = { formatDBType: () => 'DEFAULT', _rawDBType: true };\nconst skip = ({exists}) => !exists;\nexports.optionalColumn = (name) => ({\n  name: Array.isArray(name) ? name[0] : name,    // Allows use as a template string tag\n  def: DEFAULT,\n  skip: skip\n});\nUsage:js\nconst { db, pgp } = require('./db');\nconst optional = require('./db/utils').optionalColumn;\nconst columns = new pgp.helpers.ColumnSet([\n  title,\n  optional author    // Really easy to add/remove optional status with this syntax\n]);\n``\nI includedskipso it also works with single-objecthelpers.update. The only thing I have to watch out for is that I don't use it with the multiple-object form, because that doesn't work withskip(for obvious syntactical reasons) and behaves more like an INSERT, falling back toDEFAULT` on omitted properties, which probably isn't desirable.. @vitaly-t Nope.. Thanks for the heads-up.. ",
    "nkzawa": "Thanks for your reply, but my point is about milliseconds, not about if it's UTC or not though.\n. toISOString is also UTC, so I don't follow your answer. I understood we can change the default behaviour, but why you choose toUTCString and omit milliseconds as the default behaviour? \n. toISOString is also UTC but with milliseconds though.\nActually I'm not sure if toUTCString is the only \"standard\" when you need UTC.\nAnyway, thanks for your prompt answer!\n. I wonder when the change of the default behavior would actually break someone's code.\nAs SQL, 2016-11-16T06:25:38.182Z (toISOString) works fine as far as I tested.\n. ",
    "TazmanianD": "I just hit this as well and I'm going to push back and argue strongly this really is a bug. Just because the Javascript Date.toUTCString function doesn't include milliseconds doesn't mean a UTC time can't include milliseconds. The wiki article (https://en.wikipedia.org/wiki/Coordinated_Universal_Time) on UTC does make mention of milliseconds. The Date.UTC function accepts milliseconds as an argument and the Date object has a Date.getUTCMilliseconds function.\nYou say that Postgres uses UTC, which is correct, but that doesn't mean that Date.toUTCString is the correct formatting mechanism for it when sending dates from Javascript to Postgres. Postgres clearly supports milliseconds (https://www.postgresql.org/docs/9.1/static/datatype-datetime.html) (in fact it supports microseconds). You also said you don't know what formats Postgres supports that we could use as an alternative for toUTCString. The document says that Postgres is pretty lenient (and the ISO format is explicitly mentioned): \"Date and time input is accepted in almost any reasonable format, including ISO 8601\".\nI've just moved from a project that used Java against Postgres to one that's using Javascript and I was quite surprised by the difference in behavior. With the Java libraries, when you pass a Date object, the database does indeed get and store the milliseconds. Furthermore, if you test with the base pg Node module, it does handle the dates with milliseconds. So you would have to argue that your module is correct and 'pg' is not.\nHere's a specific test case:\n```\nconst myDate = new Date('2016-11-08T08:55:15.804Z');\ndatabase.any('DROP TABLE IF EXISTS time_test')\n  .then(() => {\n    return database.any(CREATE TABLE time_test (time TIMESTAMP WITH TIME ZONE));\n  })\n  .then(() => {\n    return database.any(INSERT INTO  time_test values ($1), myDate);\n  })\n  .then(() => {\n    return database.any(INSERT INTO  time_test values ($1), myDate.toISOString());\n  })\n  .then(() => {\n    return database.any('SELECT * FROM  time_test');\n  })\n  .then((rows) => {\n    for (const row of rows) {\n      console.log(row.time);\n    }\n  });\n```\nThis  outputs:\n2016-11-08T08:55:15.000Z\n2016-11-08T08:55:15.804Z\nYou can see that when passing in a Date, the milliseconds are lost. When converting to an ISO string first, the milliseconds are preserved. So I think the toISOString is the correct mechanism here and not toUTCString.\nI really really think this is a bug here.\n. I obviously can't force you to change it but I do think this is a bug and your workaround really isn't the best solution. Your module is not behaving as I suspect most users would expect and it's not behaving like the pg module does either. Imagine someone who does an upgrade from pg to pg-promise and suddenly they find the different behavior. Having to modify the prototype for a built-in datatype isn't ideal either. I suppose we can just wait and see if anyone else finds this issue and votes on it but the more users who use this library, the more people any such change would affect since it wouldn't be backwards compatible.\n. ",
    "netaisllc": "No worries.  I was confused in trying to compose the query string itself.  Found the helpers which worked for me.  Thanks for your efforts.\n. ",
    "sveip": "Thanks, but I get { error: syntax error at or near \":\"?\n. Hi, here is the exact query:\n.query(DELETE FROM Users WHERE id IN ($1:csv), [ids]) where ids is an array of ints.\nPassing just one int (id) to .query(DELETE FROM Users WHERE id = $1, [id] works fine.\n. Sorry, just a typo here in my post. It's correct in the code. Still same error.\n. pg.connect(connStr).then(result => {\n    const client = result.client;\nI'm on my phone until Monday, sorry for the missing formatting.\n. Thanks! I will for sure :)\n. ",
    "jabooth": "Sure thing! I'll just chip away at little corners of it when I get chance \ud83d\udc4d \n. ",
    "rendongsc": "@vitaly-t  Query Files. can't meet our needs.\nthank you!\n. var options = {\n    promiseLib: promise,\n    connect: function(client){\n        client.query('SET search_path = cust');\n    } \n};\ncan be used.\nthanks very much\uff01\n. ",
    "uhef": "I've read the links provided and still having hard time figuring out how to configure pg-promise so that typescript handles promises returned from pg-promise functions as bluebird promises instead of native promises. Closest I've come to success is to manually configure ext-promise.d.ts as explained here within node_modules. But surely the answer is not to install pg-promise as dependency and then modify pg-promise code within node_modules. Somehow I suppose the configuration should be achievable within the application depending on pg-promise.\n  . I see.\nOne way to improve the current situation would be to remove typescript type definitions from pg-promise and host them as a separate project. Currently pg-promise comes with type definitions which results in a situation where user needs to make modifications to pg-promise project in node_modules. If pg-promise would ship without type definitions and type definitions would be hosted in a separate project user could install pg-promise in to node_modules and clone pg-promise type definitions to a separate folder within the using project (lets say types). types folder could be added to version control and user could make required modifications there to take bluebird into use. In order to read type definitions from types one would just need to modify the tsconfig.json file and add types folder under typeRoots - key like this:\n\"typeRoots\": [ \"node_modules/@types\", \"types\" ]. ",
    "langpavel": "Haha.. My library is tiny but powerful. It offers you template strings. Difference is, you escaping input variables but I'm passing them to pg library. It is better do less.. \n. You are not right. Look at my code and then you can make assumptions. My escaping is better then yours.\n. Btw escaping inside library is reason why I don't use your library and write my own. I know that my library is used by real projects with thousands of requests and it works well because it is tiny and just works\n. ",
    "abenhamdine": "Hi, Vitaly,\nI'm concerned about all this stuff, since we heavily use node-postgres in our HR application and don't want to get stucked with pg 5.1 for years.\nI was wondering if this project would be interesting for you : https://github.com/mscdex/pg2\nThe author is a core contributor of nodejs.. This has been fixed in pg@6.1.1. > So, pg-core is still in the plans.\nGlad to read this \ud83d\ude03 \nBecause no activity in https://github.com/brianc/node-postgres since Dec 16th... \ud83d\ude1e . yes, 42 commits since 5.1 : https://github.com/brianc/node-postgres/compare/v5.1.0...master\nincluding ~20 readme or changelog changes, and several commits related to the new pool.\nSo probably less than 10 commits to backport if you are willing to... I would be glad to help on this, unfortunately I have no spare time at the moment...\nPerhaps in a few weeks, when I will have a new developper in the team.. Thx a lot for this detailled answer !\nIMHO this should be more highlighted in the docs, it's useful to know for a newcomer.. ",
    "jamesplease": "Nice @vitaly-t ! Were there any upstream changes around this, or are you just allowing this option in the meantime?. Thanks for the confirmation that it isn't in this lib, @vitaly-t . I'll google around a little more to see if i can find a way to disable it on the DB side of things. If I find a solution, i'll post it here. I'll close this out for now, since it's unrelated to this project \u270c\ufe0f. I did not, but I also didn't spend too much more time on it since it was just an inconvenience. Very helpful and informative! Thanks @vitaly-t !. ",
    "AlJohri": "Just wanted to note that it seems like https://github.com/brianc/node-postgres/issues/1075 is resolved as of 6.2.4. https://github.com/brianc/node-postgres/issues/1060 may still exist. Haha, I'm just catching up and updating old code. Still on 5.x right now \ud83d\ude04 . I think it would be great if the helper could handle the most basic case of replacing all (or some defined subset) of the columns without any constraints. Either that or perhaps some additional examples of using the low level methods to achieve this in the documentation.\nFor example:\nI've defined a ColumnSet like thus:\njavascript\nconst cs = new pgp.helpers.ColumnSet([\n  'article_id',\n  'topic_id',\n  'potomac_pos_score',\n  'potomac_neg_score',\n  'potomac_decision_score',\n  'potomac_class',\n  'patuxent_score',\n  'ensemble_score',\n  'ensemble_class',\n], {table: 'article_topics'});\nAnd I'm trying to basically do a full upsert of the document:\njavascript\nconst query = pgp.helpers.insert(articleTopics, cs) + \" ON CONFLICT (article_id, topic_id) DO UPDATE \" +\n    \"SET potomac_pos_score = EXCLUDED.potomac_pos_score, \" +\n        \"potomac_neg_score = EXCLUDED.potomac_neg_score, \" +\n        \"potomac_class = EXCLUDED.potomac_class, \" +\n        \"potomac_decision_score = EXCLUDED.potomac_decision_score, \" +\n        \"patuxent_score = EXCLUDED.patuxent_score, \" +\n        \"ensemble_score = EXCLUDED.ensemble_score, \" +\n        \"ensemble_class = EXCLUDED.ensemble_class\";\nSeems like a pretty common use case?. @vitaly-t thanks for the feedback, I'll definitely check that out.\nComing from an ORM world, it just sort of seemed like doing a CREATE OR REPLACE operation (I'm replacing the entire row here) wouldn't require a custom query.. For anyone else that comes to this issue from Google, I found this case didn't warrant custom SQL since I was doing the same thing for a few different tables. I settled on this function instead:\njavascript\nfunction upsertReplaceQuery(data, cs) {\n  return pgp.helpers.insert(data, cs) + \n    \" ON CONFLICT (id) DO UPDATE SET \" + \n    cs.columns.map(x => `${x.name} = EXCLUDED.${x.name}`).join(', ');   \n}\nI couldn't find a method or attribute on ColumnSet that returned an array of column names without traversing the .columns attribute, so if there's a better way of doing the above, please let me know.. @vitaly-t what is pg-core?. Cool, thanks I'll give this a shot. Mongo also similarly maintains a connection pool and tests the connection on the first .connect.\n1) I was wondering if there is a way to close the single connection pool (all connections in a single pool) instead of doing pgp.end?\n2) Is there an easy way you know of to extend your Database class? This question is more general as I'm failing to understand how to extend classes in general in Node.\nThanks for your help!\n. Awesome. Would you accept a PR to close all the connections in a single pool?. @vitaly-t It seems there is no public API that makes this possible at the moment. I made a request upstream: https://github.com/brianc/node-postgres/issues/1193. Ok, in light of the slow changing dependency, if you would accept a PR that uses the node-postgres internal API, I'd be happy to make one. The implementation would look pretty much like this:\njavascript\nconst poolName = JSON.stringify(config);\nconst pool = config.pgp.pg._pools[poolName];\ndelete config.pgp.pg._pools[poolName];\npool.end(function() {\n  if (Object.keys(config.pgp.pg._pools).length == 0) {\n    config.pgp.pg.emit('end');\n  }\n});\nThe most likely location would be in connect.js as function poolDisconnect(config) and exposed in database.js via a close function.. Yeah, that makes sense.\nSince my use case involves only one postgres connection pool per application I think this abstraction works for me:\nindex.js\n```javascript\nimport { MongoClient } from 'mongodb';\nimport { PostgresClient } from './pg-wrapper';\n(async () => {\n  const mongoDb = await MongoClient.connect(process.env.MONGO_URL);\n  const postgresDb = await PostgresClient.connect(process.env.POSTGRES_URL);\nconst docs = await mongoDb.collection('Articles').find().toArray(); // {date: { $gt: }}\n  console.log(docs);\nmongoDb.close();\n  postgresDb.close();\n})().catch(e => console.error(e.message, e.stack))\n```\npg-wrapper.js\n```javascript\nimport pgPromise from 'pg-promise';\nexport const PostgresClient = {\n  async connect(uri) {\n    const pgp = pgPromise({\n      extend: function (obj, dc) {\n          obj.close = () => { obj.$config.pgp.end() }\n        }\n    });\n    const db = pgp(uri);\n    const obj = await db.connect();\n    obj.done();\n    return db;\n  }\n}\n```\nAdapted from the stackoverflow for checking the connection and pg-promise-demo for using extend.\nI appreciate your help and responsiveness!\nPlease let me know if you find anything wrong in the example above.. @vitaly-t Maybe you can take another look? I do believe I'm using the connection pool properly. My wrapper PostgresClient.connect function does a return db; where db is the connection pool.\nThe db.connect is just called once at app start up to check if the database connection works (and is promptly released via .done()) as you suggested in the StackOverflow.. ",
    "taylorknopp": "I believe my typings version is 1.3.3. My node.js version is 3.5.2 if I'm looking at the right thing. I made sure typings is installed and still get the same errors. I have tried several npm installs but, none of which have worked.\n. I have version 4.2.6 of Node.js  I just tried reinstalling it, though, and for some reason, this time, that fixed it.  Thanks for the help and patients with my noobishness in this area.\n. ",
    "zevanb": "I grouped my queries, and used sequence instead of promise.all in a single transaction. It works pretty well in few minutes.\nThank you for your help !\n. ",
    "luchillo17": "I've tried the monitor lib, however haven't been able to make it work, maybe i lacked some sort of config, it worked with the connections but query events were not working, dunno why.\nEnded up using the manual options config.\n. Yup, like this:\njs\n    pgPromiseOptions = {\n        query: (e) => {\n            console.log('QUERY: ', e.query);\n            if (e.params) {\n                console.log('PARAMS:', e.params);\n            }\n        }\n    }\n    pgPromise = require('pg-promise')(pgPromiseOptions);\n    pgMonitor = require('pg-monitor');\n    pgMonitor.attach(pgPromiseOptions, ['query', 'error']);\n. Ok i see it working but it is duplicating the log.\nAlso the error i'm seeing is that the query itself returns a date like 2016-01-01 in PgAdmin 3 while through pg-promise returns like Thu, 01 Sep 2016 05:00:00 GMT, does pg-promise makes any parsing on dates?\n. > that's what you code does, you either log through monitor or event query, not both.\nThen i pass options only to monitor?\n\nNo. The underlying driver does, converting Date/Time into a Javascript object.\n\nHow do i stop it from converting it? do i need to format in JS to string then?\n. I use it as param to another query and is messing up my query.\n. That's somewhat cumbersome, i'll just parse the JS Date object as needed. Thanks.\nBetter yet, i use moment elsewhere, so i'll just tell moment to parse it for me.\n. Dude... if you look closely at the image in the bottom it shows the output of the node -v command, it says 7.8.0.. Hmm, does it affect if it's runned by the karma testing framework?\nFunny thing is it was working... until i hit npm update, had to revert to previous version of this package.. Soo i think i now know where's the issue, i'm using a package karma-nodewebkit-launcher to be able to run testing in karma, it uses NW.js to simulate node code.\nI suspect such package either uses an old version of NW.js that uses an old Node.js version, or NW.js itself has never updated it's Node version, i'll try and change to an Electron based karma launcher instead, though fixing the version of pg-promise to one before 5.6.0 should still work ok.. Tested with electron, it runs actually faster and doesn't complain for the node version, thanks for taking the time to answer.. ",
    "dfloresgonz": "this is printing all the queries, is there a way to just print when I want it? thanks. @mythical-programmer How exactly did you managed to do it? I create a connection in each call but I don't end() them, what's the correct way to handle connections and lambdas?. I get the same error with minify : false It's because I have a double quota in my data \"\nHow can I solve this? don't tell me to modify data please.. What I am trying to achieve is to dump a .sql file to create the whole database (part of an automated process) I don't do it by hand (CLI or using pgadmin). Other counter part is that I run it in serverless I don't have a CLI where I can use pg_restore. Is there a way to generate the backup with this characters you mentioned scaped in order to pg-promise to parse it?\nOther solution I tried was using pg-minify but it parses strings not files.. ",
    "Retro64": "Therefore I asked if there will be any update\n. ",
    "gkiely": "Awesome, thanks guys.\n. ",
    "BrianRosamilia": "I just thought it would be convenient for \ndb.scalar('select count(*) from users') \nto support returning a single value, not an object.  Of course this can be done in numerous ways,  I'm surprised a simple way to get a scalar value back is not supported and that objects are the minimum a user is able to work with.\nAlso your library already coerces integers, no need to have a scalarInt function.\n. > Row objects are the basic elements that any application needs.\nMany database libraries would disagree with you that the user needs to always be dealing with a complex object\n- http://docs.sqlalchemy.org/en/latest/orm/tutorial.html#returning-lists-and-scalars\n- https://github.com/StackExchange/dapper-dot-net/blob/61e965eed900355e0dbd27771d6469248d798293/Dapper.Tests/Tests.Async.cs#L406\n- http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/reference/dql-doctrine-query-language.html#query-result-formats\nIt doesn't matter much to me though, the library is pluggable so this isn't a burden for me.  I honestly thought it was just an oversight not offering this.  You seem to really be against it so feel free to close.\n. ",
    "markasoftware": "Is there any reason that it does not automatically put queries on the same connection when possible, like node-Postgres does by default? Doesn't the current implementation encourage opening a new db connection for every http request because the only other way is to basically wrap your whole application in a db.task?\n. Correct me if I'm wrong, but doesn't the pool keep connections open, even when you're not using them, for up to 30 seconds, and will automatically put new queries onto the existing connections if they're idle?\n. So, to be clear, if I do something like this:\ndb.query('query here').then(r => db.query('another query'));\nWill they use the same connection? The second query does not start until the first is completed, so shouldn't the client from the first one be idle and waiting in the pool?\n. Ok, I think I understand now, thank you for all your help. But when exactly would somebody want to use a task? Shouldn't the connection pool put queries onto already-open connections if possible, meaning that a task doesn't have any real performance benefit?\n. Doesn't pg pool support a connections limit so that you can't ever get too many connections, task or no task?\n. Ok, I think I get it. Thanks!\n. ",
    "mauriciovigolo": "just did a pull request with the @types support -> typescript 2 support - @types #232. We have been using it internally and it's okay. \nTks Vitaly.\n. @vitaly-t, this new version of Typescript really provides support for es2015 declarations as mentioned at the Typescript 2 release notes. \nI did in that way for backward compatibility, however if you desire to move on, I also believe that it would be a great idea. In this case, we just have to insert the following in the tsconfig.json: \n\"compilerOptions\": {\n    \"lib\": [\"es2015.promise\"]\n}\nWe will have to update the Typescript pg-promise documentation. If it's okay I could help you to update it and publish to the definitely typed repository.\nRegards,\n. @vitaly-t, this new version of typescript has a list of built-in API declarations, including es2015.promise - 2.0 release notes. \nFor using it, the pg-promise user should insert in the project's tsconfig.json file, the required libraries. If the project is using bluebird instead of ES2015 promise library, it would be necessary to install the bluebird definitions, ie.: npm i --save-dev @types/bluebird .\nIn my opinion you may only need to update your typescript docs about this.. sure @vitaly-t. Thanks @blakeembrey!\n. ",
    "blakeembrey": "Please don't move it away from the repo and please do not use @types. This repo could work today by using the native TypeScript node module resolution that's been supported since TypeScript 1.5 and wouldn't require users to do anything (either using Typings or NPM @types). See https://www.typescriptlang.org/docs/handbook/declaration-files/publishing.html.\n. @vitaly-t None of it is TypeScript 2.0 specific. All you need to do is add typings or types to package.json and point it to your main .d.ts file while using external modules, and it will \"just work\". No need to maintain @types or typings.json.\n. :-1: Using global are a bad idea, you should write definitions in external module format. Globals are how you create conflicts. It looks like this PR is mostly done because of a misunderstanding in how TypeScript works. See my additional comments in https://github.com/vitaly-t/pg-promise/issues/220#issuecomment-260165225.\n. @vitaly-t In terms of being a definition author, TypeScript 2.0 only brings module resolution for non-native TypeScript definitions (using node_modules/@types) and null | undefined types. Nothing else is too relevant with 2.0, but there are some new type constructs (even better ones on the way). I'm only suggesting what is usually the best approach - don't maintain @types, it benefits no one when the author is already willing to provide the best experience over third-parties. Use typings in package.json to point to the entry definition file (e.g. https://github.com/blakeembrey/free-style/blob/master/package.json#L6) and TypeScript will pick it up automatically.\n. Yep, that should be all that's needed.\n. The only thing that would need to be fixed is the dependency (spex) if that is used. If it is also updated to the same format, you'll be able to have everything working in TypeScript for the beginning without any external tools needed.\n. You can continue using typings for development locally with those. I'm not 100% sure what spex does though, but if that can be turned into an external module it'd be good.\n. ",
    "xiamx": "\nshould we now just remove file install.js and command \"postinstall\": \"node install.js\" from package.json?\n\nyup :). Cheers\uff01\nAccroding to this\n\nPublicize your declaration file\nAfter publishing your declaration file with your package, make sure to add a reference to it in the DefinitelyTyped repo external package list. Adding this will allow search tools to know that your package provides its own declarations.\n\nWe still need to add a reference here. Yup, though it will help people notice that we distribute our own definition along with the package. \nQuite a few still follow the tsd, typings guide and will assume that we don't have type definitions if it can't be typings install-ed. @vitaly-t looks like you are already started :) . Would love to see this merged \ud83d\udc4d \n. @blakeembrey how should dependencies on global node definition (https://github.com/vitaly-t/pg-promise/blob/master/typings.json#L11) be handled?\n. Thanks @blakeembrey \n\nPlease don't merge this yet. Once https://github.com/vitaly-t/spex/pull/6 is merged, I'll update this pull-request with changes related to spex\n. Hi @vitaly-t \nI updated this pull request with more changes. Since turning ambient definition into module definition unindents the d.ts files by a level, there are a lot of whitespace changes. For your reviewing pleasure, it might better to review them with whitespace changes off (https://github.com/vitaly-t/pg-promise/pull/238/files?w=1)\nThe general format of the patch is almost the same as https://github.com/vitaly-t/spex/pull/6\n1. typings key is introduced to package.json pointing to typescript/pg-promise.d.ts\n2. Then applying Blake's suggestion, I turned typescript/*.d.ts from ambient definitions to module definitions. This step involves removing module declaration and unindenting the d.ts file.\n3. I also updated test/typescript/* to reflect the changes made in 2.\n4. A typescriptSpec.js is added so travis can test for the build process\n5. And finally, update of README to reflect new changes\n. I forgot to add typescript in as a devdependency, maybe that's why it failed. Looks like everything is building fine now :)\n. awesome thanks for the help :)\nI'm going to look into using pg-monitor and task tags.\nWe do use Query Files internally ;) don't know why I chose to use fs in my example :joy:. ",
    "bIgBV": "Hi, I am trying to use pg-promise in my own typescript project for creating an API. Reading the documentation around typescript says I don't have to install anything else. I figured that VS Code will automatically get the type definitions.\nBut after installing the latest stable release of the library, I'm still getting the following error:\n[ts] Cannot find module 'pg-promise'.\n\nI've tried to manually install types by using npm install --save @types/pg-promise but that seems to be deprecated. I even tried downloading the definition files and using the /// <reference path=\"\"> tag to include them, but that too didn't work.\nI'm sure I'm doing something boneheaded here, but I'm not sure how to proceed from here.\nThanks!. ",
    "kay999": "I just want to insert arrays as json by default. So I tried to override formatting by using Array.prototype.formatDBType. \nThis works perfectly as long as I supply the query parameters as an object, for example\ndb.query('update table set data=$[data] where id=$[id]', { id:id, data:data });\nBut if I use positional parameters, it doesn't work anymore:\ndb.query('update table set data=$2 where id=$1', [id, data]);\nThe reason is that the whole [id, data] is now converted to a json-string and inserted as \"$1\" in the query.\n. My problem with your solution is that using \":json\" only works if I know that \"data\" expects json. This is easy to do in hand-written code but it doesn't work for me because it's auto-generated based on the column names (without type informations). \nNow everything works fine as long I don't try to insert array-values in json-columns. Everything else works: Strings, Numbers, Objects. But because pg-promise converts arrays to \"Array[]\" syntax inserting array-valued data doesn't work for json columns.\nUsing Array.prototype.formatDBType works fine as long parameter-values are supplied as Object. Only with positional parameter it doesn't work because pg-promise translates the whole value-array instead each value of it's own. I suspect that this behaviour is easily patchable just by using a \"map\" on the values array instead of translating the whole array from top-level recursively.\nThis is IMO quite unexpected so It took me some time to figure out why positional parameters doesn't work. If you don't intend to change this, please add a small note in the docs about this behaviour.\n. This solution doesn't work directly. For example\nPgp.as.format('Result: $1, $2', [asJson([1, 2]), asJson('x')]\ngives \n'[1,2]', '\"x\"'\ninstead of \n'[1,2]', 'x'\nBut if Array override is to dangerous, I can use map/mapValues/isArray to find and map all arrays in the value-array to JSON. It just looked as a simple and direct solution to use formatDBType on arrays...\n. You're right. My code only uses JSON arrays and objects as values so I never ran into this problem.\nBut if I convert all input data this way normal text fields will get still the wrong input. So I need to do a selective transformation of the input data anyway. Not really a big deal but always needs a wrapper-call.\n. I have code which creates sql-statements like insert/update based on a table and column-names. The data are simply arrays of data-elements which should be inserted via those statements. The data elements are basic values (string, number, bool, null) but also objects and arrays. Objects and arrays should both written into JSONB-type columns. The type of the columns is not known to the statement-generator so it can't create something like \"$1:json\".\nWith objects this is no problem, because pg-promise automatically converts objects to JSON, but with arrays this doesn't work automatically because pg-promise converts to array(...) notation (which is no valid JSON and thus rejected from Postgresql).\nI've solved it now by using your code to selectively wrap arrays in the the input data via checking by Array.isArray.\n. ",
    "akshayp": "@vitaly-t It's essentially what AWS calls a database slave except it's primary function is meant for read only queries\n. Correct, non mission-critical read only operations would be routed to a database slave\n. ",
    "cmelone": "If I created my query with something like this:\n`` javascript\nfunction insert(table, values) {\n  const keys = Object.keys(values),\n  columns = keys.join(', '),\n  placeholders = keys.map((k, i) => '$' + (i + 1)).join(', '),\n  query =INSERT INTO ${(table)} (${columns}) VALUES (${placeholders})`;\nreturn db.none(query, keys.map(k => values[k]));\n}\n...\nconst data = {\n  name: req.body.name,\n  email: req.body.email,\n  phone: req.body.phone\n}\n...\ninsert('table_name', data)\n  .then(function() {\n    console.log('Success');\n  })\n  .catch(function(err) {\n    console.log('Error: ' + err);\n  })\n```\nWould it be protected against SQL injection?\n. ",
    "jdabbs003": "Thank you for this.  With the promise chain, the underlying PosgreSQL transaction wraps the outermost Promise inside the tx object?\n. ",
    "jmpmscorp": "Thank you for reply.\nAs you supposed, I'm trying to reuse same ColumnSet on UPDATE and INSERT queries. I found an approach like:\n```\nclass Persona(){\n    constructor(){\n        this.InsertColumnSet = pgp.helpers.ColumnSet([\n            {\n                name:'name',\n                skip:function (name){\n                    return !this.hasOwnProperty(name);\n                }\n            },\n            {\n                name:'surname',\n                skip:function (name){\n                    return !this.hasOwnProperty(name);\n                }\n            },\n            {\n                name:'city',\n                skip:function (name){\n                    return !this.hasOwnProperty(name);\n                }\n            }          \n        ], {table:{table:'person', schema:'public'}});\n    this.ColumnSet = this.InsertColumnSet.extend(['?idPerson']);\n}\n\n}\n```\nThen, I use InsertColumnSet on INSERT queries and  ColumnSet on UPDATE. Is there better approach?\nAnother question: Is there anyway to get only name columns in Array? I was looking for in API but I didn't find this helper or method. I use:\n```\nthis.getNameColumnsAsArray()\n{\n    let columns = [];\nfor (let i in this.ColumnSet.columns)\n    columns.push(this.ColumnSet.columns[i].name);\n\nreturn columns\n\n}\n```\nBye : )\n. Hi again.\nI use \nskip:function (name){\n       return !this.hasOwnProperty(name);\n}\nbecause f I put 'def:value' on a column and I do UPGRADE query, if property is not in JSON object passed, default value is set and I don't want this. I upgrade fields through PATCH verb and request JSON object hasn't all properties than ColumnSet has. Due to this, defs values are set and I want to skip this default value because I needn't. If I only want to upgrade a field, I don't need pass any other value... (I don't know if I explain well, i wish you can understand me...).\nMaybe, there is a better way to attach this behavior.\nThank you.\n. Thank you very much for your help.\nI was working in a rigth way more or less. I'll change code like you advised me.\nThank you again.\n. ",
    "scharris": "Thanks for your comments, that makes sense.\nI hadn't realized Postgres actually treats the return value of foo as a \"result-set\" like object itself also, at least in psql,  and not just a simple scalar, because in psql it shows the result with label \"1 row(s)\" and with a column name \"foo\".  So the library is just being consistent to map those to one or more js objects in the usual way.  So in that light it's not surprising behavior at all.\n. Libraries are not involved in this point I was trying to make - I think what I was trying to get at could be summarized as:  functions can only be evaluated by the SQL engine (even for local assignments within functions involving function call expression the SQL engine is invoked), and that the SQL engine will only give back relations not scalars to the outside world.\n```\npostgres=# select  foo();\n foo\n\n22\n(1 row)\n```\n(Note row count and column name - this is relation not scalar).\nAnd I guess that \"foo\" is what becomes the key in the result object when the libraries receive the result.  Looked at in that way, the library behavior seems very natural.\n. ",
    "lexakozakov": "I'm using pg-promise in AWS Lambda. You might be aware about the issues with AWS Lambda. How can I control the connections?\n. AWS Lambda is a microservices architecture. Multiple services (AWS Lambda) speak to my PostgresSQL and i'm running out of all available pool connections. \nCan I use pg-promise without pool connection, but just general connection - connect to DB, trigger a query, close connection? \n. I just read some docs. We have to stop using connection pooling in AWS Lambda. Amazon handles this very badly.\nHow can I terminate any connection manually? What function do I have to call?\n. I set poolSize option to 0 and I see that connections are still alive until I terminate the script. How can I release connection even when script is alive? \n. Sorry, I used wrong terminology. I have to terminate the pool connections, not just to release a connection to pool. \n. Unfortunately, no. But I'm not the one who have a such issue. \nQuite fresh comments can be found here and here. \n. Definitely, the issue is not related to connection pool itself. \nThank you. I appreciate your help. \n. ",
    "alpertuna": "I supposed Buffer is used by this library since I didn't look into source, that's why I just wanted to report it. Of course time is not problem as nodejs 7 is very new.\nI would move the issue to main library but there is already reported one; https://github.com/brianc/node-postgres/issues/1163\nBtw, thanks for quick reply.\n. ",
    "DwGr": "Probably, I found problem. I've installed node as \"nodejs\", but any command with \"node\" fails. Fix it runing\nsudo apt install nodejs-legacy \n(don't know exactly what does it mean)\n. ",
    "sunilgupta123": "Thanks for reply Vitaly, shall hook on Stackoverflow.\ni thought its driver issue cos array: function (arr) in formatting.js returns string and postgres cant digest it. If array is nested in object then it works smoothly e.g. \"MyPref\" : \"{Same Object array as above}\"\nHere MyPref gets through in column \"FoodPref\" without any issue. \n. ",
    "skatcat31": "Works for me. If it's not useful for multi inserts or it requires a lot of odd work around then it isn't really a useful piece of code. I'll just deal with using more than one columnset. Honestly it sounds like there isn't a need. Having multiple columnsets doesn't take up much memory, and it's not hard making multiple and an if statements so if it isn't useful in any other case then it isn't a useful feature.. I feel like this is attempting to remove a developers responsibility from verifying their own objects. I may be wrong there, but think about enforcing a developer to actually verify and then transform their object to a proper form.\nBy requiring the developer to do their transformation it means less maintenance since where they verify and update to de-nest objects is where they need to change things when the form changes, NOT the query or query formatting, where it is much more prone to problems in the first place and can only support a SINGLE version.\n```js\n// inside your repo or route handler\nfunction versionOne(obj){...} // format input object to be the same format always used by the query\nfunction versionTwo(obj){...} // these will always return either an Object will all required fields or a false-y\nfunction versionThree(obj){...} // by returning a false a test can be done that will basically say input invalid\nfunction formatQueryObject(obj){\n  switch obj.version\n    case 'v1' :\n      return versionOne(obj);\n    case 'v2':\n      return versionTwo(obj);\n    case 'v3':\n      return versionThree(obj);\n    default:\n      return false;\n}\n// inside your route handler\nconst queryObj = formatQueryObj(req.body);\nif (!queryObj) return res.sendStatus(400);\nreq.app.locals.db.repo.method(queryObj)\n  .then(res.sendStatus.bind(res,200))\n  .catch(logErrorAndRespon(res))\n```\nBy relying on the library to verify your object you're locking yourself to a single version. Proper design means handling ALL version separately and sending a single format that makes sense to your DB interaction layer. After all the DB interaction layer needs to be as fast as possible and as stable as possible. By not verifying your input and relying on the DB interaction layer to do such you remove yourself from a good design pattern and will run into more error prone design in the future.. As a follow up when looking at nested object versus single layer objects, you're still locked into a single query which means for maintenance reasons you'd then be making a transformer into a then nested object. So either way there is going to be a transformer for your object before it goes into your query if it's a complex object, so why complicate the library more? It's all dependent on the developer in either case.. @vitaly-t I feel like this is going to lead to a bad maintenance paradigm though for some people who just keep changing the query over and over again instead of the logic before the query, but TBH that's a problem that can exist with named values anyways.\nName spacing is a helpful enoguh feature for remember where it came from and that may be enough to warrant the inclusion in the formatting engine. After all if your variable name is long enough that an indexof check on the string causes a performance hit.... ",
    "paleite": "@vitaly-t That's what I call effective problem solving! \ud83d\ude04 . ",
    "josefzamrzla": "@vitaly-t  I'd like to have an event like that to be able to track/log all executed queries with their duration and number of fetched/affected rows to help me with query optimizations. I've seen an event 'receive' but this event is triggered only when a query returns some data, it is not triggered for data changing queries without RETURNING clause.. javascript\noptions.finish = (stats, e) => {\n    let color = stats.duration && stats.duration < 500 ? 'green' : 'red'\n    console.log(chalk[color](\n        `\\n${e.query.replace(/\\t/g, '').trim()}\\nFetched ${stats.rowCount} rows in: ${stats.duration} ms\\n`\n    ))\n}. @vitaly-t sure, but this event can notify me about all (successfully) finished queries. @vitaly-t to better explain my motivation - I have an app that already uses pg-promise and I want to be able to detect probably non-optimized queries for further investigation. I know that the 'duration' time is not a 'query time' but some kind of 'fetch time' but that's ok in development mode.. Ok :-). ",
    "peyoh": "Thank you.\n. ",
    "seanlindo": "To be more specific, I'm looking for a performant way to perform a bulk INSERT INTO...ON CONFLICT DO UPDATE operation. . Yes, it can be closed. Solution was super easy and performance is outstanding. \nlet query = this.pgp.helpers.insert(collection, col_set) + ' ON CONFLICT ON CONSTRAINT constraint_name_goes_here DO UPDATE SET modified_date = now()'. ",
    "davegri": "It would be nice if there was a way to combine the insert and update helpers. I would like to do:\nconst upsertContact = (contact) => {\n  const insert = pgp.helpers.insert(contact, null, 'contacts');\n  const update = pgp.helpers.update(contact);\n  db.none(`${insert} ON CONFLICT (sf_id) DO ${update}`);\n};\nEven better would be \nconst upsertContact = (contact) => {\n  const upsert = pgp.helpers.upsert(contact, null, 'contacts', 'sf_id');\n  db.none(upsert);\n};\nthe first generates \nINSERT INTO \"contacts\"(\"email\",\"password\",\"sfId\",\"orgId\") values('emailvalue','passwordvalue','sfIdValue',1) ON CONFLICT DO update \"contacts\" set \"email\"='emailvalue',\"password\"='passwordvalue',\"sfId\"='sfIdValue',\"orgId\"=1\nwhich doesn't work because it needs to be\nINSERT INTO \"contacts\"(\"email\",\"password\",\"sfId\",\"orgId\") values('emailvalue','passwordvalue','sfIdValue',1) ON CONFLICT DO update set \"email\"='emailvalue',\"password\"='passwordvalue',\"sfId\"='sfIdValue',\"orgId\"=1. Could we have an option for generating the update clause without the table name? then I would be able to combine them. ",
    "kissmygritts": "@vitaly-t, I love this library, and the amount of work you put into it. Every problem I've had you already have a solution some where. I just need to be pointed in the right direction occasionally.. ",
    "MahimaSrikanta": "Can we do similar kind of UPSERT for the PostgreSQL with version 9.4. where we do not have 'ON CONFLICT'. ",
    "mathisonian": "Thanks @vitaly-t, that is helpful. I'll try with any and see whats going on at the database layer. . ",
    "popara": "Oh, sorry, yes this makes sense. \nI did not see this piece of code! \n Maybe csv name is a bit missleading but except that, all is good!\nthank you very much! \n:)\nSent from my iPhone\n\nOn Dec 10, 2016, at 11:59 PM, Vitaly Tomilov notifications@github.com wrote:\nClosed #252.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "hassankhan": "Sorry, here's the actual error I'm getting now:\nTypeError: Path must be a string. Received undefined.\nWe're using Webpack as a bundler only for our Node projects, as bundling/minifying improves performance on Lambda.. Fair enough, I was hoping someone else might have tried the same and gotten somewhere.. ",
    "lsimmons2": "Ahh gotcha, I missed that you still have to call the initialization method even if you're not using initialization options.\nThanks!. ",
    "MagnumDopus": "```\nCREATE OR REPLACE FUNCTION new_user()\nRETURNS TRIGGER AS $$\nDECLARE\n    key TEXT;\n    emailQry TEXT;\n    userQry TEXT;\n    found TEXT;\nBEGIN\nemailQry := 'SELECT 1 FROM ' || quote_ident(TG_TABLE_NAME) || ' WHERE email = ' || quote_literal(NEW.email);\nuserQry := 'SELECT 1 FROM ' || quote_ident(TG_TABLE_NAME) || ' WHERE username = ' || quote_literal(NEW.username);\n\nEXECUTE emailQry INTO found;\nIF found IS NOT NULL\nTHEN\n    RAISE NOTICE 'Email % already in use.', NEW.email\n        USING HINT = 'Do you already have an account?';\n    RETURN NULL;\nEND IF;\n\nEXECUTE userQry INTO found;\nIF found IS NOT NULL\nTHEN\n    RAISE NOTICE 'Username % taken.', NEW.username\n        USING HINT = 'Try a different name';\n    RETURN NULL;\nEND IF;\n\nRETURN NEW;\n\nEND;\n$$ language 'plpgsql';\nCREATE TRIGGER new_user BEFORE INSERT ON users FOR EACH ROW EXECUTE PROCEDURE new_user();\n```\nAnd then the test case where I am just trying to catch the notice in some way is very simple:\n```\n...[setup db using appropriate settings]\nlet username = process.argv[1];\nlet email= process.argv[2];\ndb.any(INSERT INTO\n        users (\n            username,\n            email,\n            salt,\n            password\n        )\n    VALUES (\n        '${username}',\n        '${email}',\n        'salt',\n        'pass'\n    )\n    ON CONFLICT\n        DO NOTHING\n    RETURNING\n        confirmation_link AS link, email)\n```\nOn the first call it should be successful, then redoing my test it should throw a notice due to duplicate names/emails.  On the first call it returns the returning poriton of hte insert as expected but on the second call where it throws the notice it returns an empty set.  I need to see the raised notice but it doesn;t seem to be returned anywhere.. Well with node-postgres which this is built upon you can listen for notices: https://github.com/brianc/node-postgres/wiki/Client#notice--object-notice\nI'm assuming this behavior isn't extended into pg-promise?  Also about the es6 strings, I am only doing that for testing, not a production solutions!\nAfter testing result I can verify it too does not return any information of the notice.. Typically in postgres the raise levels are: DEBUG, LOG, INFO, NOTICE, WARNING, and EXCEPTION\nOnly exception raises an error and typically rollsback a transaction.  In that regard I would imagine everyhting except exception would resolve and exception would reject.  \nAs per the Postgres documentation: https://www.postgresql.org/docs/current/static/plpgsql-errors-and-messages.html. Well, and perhaps I am approaching this the wrong way, but what would be the best approach to inform a user that the user they are trying to add to teh table already exists?  Should I abandon a plpgsql command and instead just perform two statements in a transaction, one query the database for emails/usernames, and one then performing the insert?  I suppose that might be the more appropriate solution, I felt like using notices might have been the most appropriate response but maybe not.... I would assume since this has barely been requested that I am approaching this improperly from a sql perspective.  I will just use a transaction block with a query followed by the insert.  Since the table has appropriate constraints the insert will fail if the user/email is present anyway.  I think I was trying to over-engineer this problem.  \nIntercepting globally is nice but it breaks the simplicity of a promise chain.. Indeed, its a super rad library.  I will just go about and do it the non-ridiculous way.  I'll just get rid of the trigger functions and do it the proper way.  Thanks for the swift responses!. ",
    "goncalopereira": "Hello,\nI'm getting the 'relation does not exist' error from postgres but only because it's not setting up the 'postgres' user and failing back to my own.\nConfiguration object works without a password field.. Might be from https://github.com/iceddev/pg-connection-string/blob/master/index.js\nWhen there is no password and cannot split auth.\n. Fair enough. Feel free to close this issue then, was just letting you know in case it was an actual issue.\nRegards. Okay let me get around to it and I'll give some feedback tomorrow.. No updates I'm afraid - I tried to replicate the issue by getting 'current_user' with different connection strings but it was fine on my machine. I only saw this at a colleague's machine so cannot reproduce it at the moment.\nI'm sure you can close this for now. Thank you.\n. ",
    "dyllandry": "For those like me who have trouble finding the difference: Double parenthesis are necessary while requiring pg-promise because it exports a function. The first parenthesis accepts the module name while the second is for the exported function's parameters.\njavascript\nvar pgp = require('pg-promise')( /*in this case we pass no options );. I see that now. I just got to the point of following documentation instead of solely tutorials, so I'm still trying to tell what's important and what's not. I'll add 'getting started' sections to the important list. I'll delete the comments if you like.. ",
    "ubershmekel": "Now I notice that it's just the pg query result. So the only possible improvement would be to change it from any to any[].. I understand. Thank you for clearing that up. I did read the beginning of \"Learn by Example\" before posting. But I did not immediately see the value or intent behind using a method called any to query the db. Also on the github repo readme the first thing to show up under Usage is the function query so I assumed that's what I should be using.. ",
    "nokizorque": "Turns out I wasn't using it correctly and I just needed to remove ^ and place :csv inside the curly braces.\njavascript\ndb.query(\"select ${columns^} from tracks where uniqueid in (${ids:csv})\", {ids: ids, columns: columns.map(pgp.as.name).join()}, qrm.any). ",
    "brandonros": "I found :raw, thanks!\nExample: ${sortDir:raw}. ",
    "jgamio": "vitaly-t  thanks for your response\n1 That work when I use a field on the database but when i try to create a alias the return is a low case name. \nAlready make the change to \n'select *, id as \"UserId\" from user '\nbut the return value is userid \n2 Thanks you I didn't know that.\n. ",
    "jorisdegeringel": "ahem, minute I posted it realised it was duplicate column names, pretty lame ;). as attunement i will donate aomething to this great lib\ud83d\udc4d  . ",
    "phiresky": "Thank you!. ",
    "FrankV01": "I suspect you'd be better served closing this and posting it on Stack overflow... . Ok, thanks. . For what it's worth, I noticed this on my Mac yesterday day as well. It might have to do with resolution or screen size as I have a 13\" Mac.... . In an attempt to help: Each call to one or none returns a Promise. That means that you can use then on each call to get the results or process them. The result is given in the first parameter. \nSo: \n// Intentionally incomplete. \n.one(...).then(function(my_result){\n   //do something with result.... could be another query\n})\nYou're gonna want to learn about promises before trying to leverage this library in it's most efficient form. Both it and promises are very powerful but promises take a bit of experimenting to learn and understand. . ",
    "nosracq": "How do i chain them? You're right, I should have asked on stack overflow. Thanks for your help!. Thanks again for your help, i got it working, but definitely need to do some reading up on promises. . ",
    "saidkholov": "Well, I am in a different timezone. I can't provide feedback straight away . Thanks for your example. Looks neat. I will try it tonight. \n. It worked very well thanks. The only thing i had to change is that parameter external to default to empty string. QueryFile does not accept null or undefined apparently. \nES6:\nfunction sql(file, external = '') {\n // ....\n}\n. Yep my bad. It worked just fine. The problem was in my query. Cheers. Thanks, I'll bear that in mind. ",
    "allthesignals": "Very interested in this - have you found queries to be more composable using this method? For example, maybe I would like to split out a query and pass through limits or offsets using the external query file variable so that some queries aren't limit-able. The idea is that we'd have less duplication but risk abstracting the wrong way. . For example, I would like to be able to setup a \"paginate\" helper like so:\nconst projects =\n      await db.any(listProjectsQuery, {\n        communityDistrict,\n        itemsPerPage,\n        dcp_publicstatus,\n        paginate,\n        offset: (page - 1) * itemsPerPage,\n      });\n... where paginate is a query file with named parameters. The problem is I don't know how to setup the paginate SQL fragment with the parameters. I wonder if I can render the parameters directly before sending it to db.any?\n. \ud83d\ude4c thank you, @vitaly-t - you're a saint! I was digging through source and was JUST about to look up the docs for using functions as values. \nOnly difference is I needed to use toPostgres directly when passing my paginate value:\njavascript\n    const projects =\n      await db.any(listProjectsQuery, {\n        communityDistrict,\n        itemsPerPage,\n        dcp_publicstatus,\n        paginate: paginate({ itemsPerPage, offset: (page - 1) * itemsPerPage }).toPostgres,\n      });\nWhich seems wrong to me, but it works. @vitaly-t I agree, it seems wrong - I think am not using the proper syntax for CTF in my query file for the paginate value.. js\nfunction paginate(values) {\n  return {\n    toPostgres: () => pgp.as.format(paginateQuery, values),\n    rawType: true\n  };\n}\ntoPostgres probably needs to be a method not a property\n. ",
    "msjoshi": "Perfect! Thanks @vitaly-t . @mythical-programmer \nWe ended up using a very small connection pool (3-4) with low idle timeout (5 secs). This way if the request comes to the same lambda container, it uses connection from the pool. Ours was a high concurrency requests to Lambda functions so we got good mileage of this strategy. . Thanks @vitaly-t for the investigation. The post in node-postures was very helpful. . ",
    "NobleUplift": "I had the same issue but I just changed my Error inheritance and now it works.. ",
    "JacobSoderblom": "Thank you for your answer!\nI'm familiar with how to test promise-based interfaces, but what I want to achieve is to remove the database connection when running my tests. Maybe sinon will make this possible.\nBtw, great library!. ",
    "akdor1154": "No worries! Thanks for maintaining this project!\nOn 7 Mar 2017 9:05 PM, \"Vitaly Tomilov\" notifications@github.com wrote:\n\nThank you for the PR, I will review and test it just as I get a chance ;)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/pull/293#issuecomment-284677158,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGa8H6Pe5citPI0fLpx04p8Isdw1K65gks5rjSvygaJpZM4MU51w\n.\n. \n",
    "dmfay": "Yeah, I've been working on the long-awaited promise conversion for a while. I had everything going off a kludgey wrapper for pg.Client temporarily -- this is much nicer!\nFor some reason I had thought the driver defaults needed to be set prior to connection. Looks like I was wrong, but I suppose that's one less thing to worry about.. I did notice that! I hadn't realized there were problems -- most of our production instances are connecting to a Postgres server on the same machine, so it hasn't really had a chance to affect us.. Massive loads query files as invocable functions on the connected db, and I want to be able to leverage pg-promise's named parameter support like so:\ndb.myQueryFile({param: 'value'}).then(resultArray => ...);\n\nIt also allows the use of query options such as single or stream:\ndb.myQueryFile({param: 'value'}, {stream: true}).then(stream => ...);\n\nSince both the parameter object and the options object are simple maps, there can be some confusion:\ndb.myOtherQueryFile({stream: true}).then(/* was 'stream' a parameter or an option? */);\n\nIn this situation, the only way I can tell for sure whether stream was a parameter or an option is to see whether the QueryFile expects parameters. If it does, I know that the object contains parameters; if it does not, I know that it's setting query options.\nIt makes sense to have this information exposed in the QueryFile itself, since that's where it's relevant and its utility isn't necessarily limited to Massive. I'm not quite following you on the distinction between declarative and content-based formatting logic; all the paramCount getter does is test whether any of the replace calls would actually have an effect. Since the pre-formatting has already happened by the time it's possible to use paramCount, it should only find actual prepared statement parameters unless I'm missing something.\n\nAs for this PR, I hope you are ok with it being closed....\n\nIt's your house! I'd appreciate you holding off for a bit so we can go over ramifications/improvements/etc -- I'm not given to dropping pull requests and running, I'm happy to discuss things like this -- but if we arrive at a good solution on this side of things then it's easy enough to send another your way.. Fair enough. Thanks for exposing the regexes!. oh thanks, I'll be getting to that later. I'm already minifying them precisely because of variables in comments though :). The Massive tests do a lot of connecting and reconnecting to apply different schemas and so forth. With max_connections = 100 there are more of those happening than the server will support. So there isn't a single specific case to point to, just the act of running all Massive's tests with pg-promise upgraded.\nI should be able to lean on Massive's reload to do less connecting, but there'll still be some amount. It is kind of odd that this didn't come up at all with pg-promise 5.x, though.. Guilty as charged :) Destroying the pool after tests works well though!. I'm not sure about the multi mask results... for 0, 1, >1 I would expect [[]], [[{}]], [[{}...]] respectively. That looks like it could just be copy & paste getting you though, and it's the only thing that jumps out at me.. ah, I got you. It is a bit up for debate whether a constraint is on one or all resultsets when you bring multi into the mix.\nLooking a bit closer at the methods, I'm more of the 'if a result can ever iterate, it should always iterate' school of thought. I'd never use manyOrNone when rows exists, for example. With the former I have to make an explicit check for null on every result. With the latter I only have to check for zero length when it matters that there are precisely no records, and if I'm performing an iterative task on the results then I don't have to check anything (no iterations = no problem). Likewise with the new any, but I guess it's probably a safe bet that someone somewhere is going to need it for something I'm not thinking of at the moment.. As far as reducing the method count: out of the first seven (none through rows in the table) I think the essential set is oneOrNone and rows, with any as the odd one out. The rest of that set don't really add a lot of value imo; sure, none and one save you from having to test for null yourself, but at the expense of having a generic exception instead of being able to write your own more actionable error message if you do get an unexpected result back. many and manyOrNone are similar with respect to the more useful rows as I outlined earlier.\nI'm not sure query really has a place anymore. rows does what it would do if multiple resultsets weren't a concern, which is in turn covered by multi, and I don't know that they need to be mixed. Especially given @brianc's note:\n\nThe change also doesn't impact prepared statements at all because postgres doesn't allow more than 1 prepared statement to be executed at a time.\n\nBut fundamentally, I have a hard time envisioning a use case in which I'm emitting a query and don't know up front whether it comprises one or several individual statements. If I did have that kind of situation, I think it'd be reasonable to just use multi or multiResult and work with the nested output.. The 6.3.7 upgrade itself didn't break anything -- the build passed on the dependency update. It's just pg-query-stream. I expect it'll be breaking your build soon too though, since you depend on 1.x :). Thanks! :). ",
    "blendsdk": "Thank you. Does this also mean that I don\u2019t need to close the connection because it will automagically close due timeout?\n\nOn 12 Mar 2017, at 13:26, Vitaly Tomilov notifications@github.com wrote:\nYou do not open connections, the library does it automatically.\nWhen executing more than one query within a request-response cycle you should use tasks, or transactions when needed.\nSee: Tasks https://github.com/vitaly-t/pg-promise/wiki/Learn-by-Example#tasks.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/vitaly-t/pg-promise/issues/295#issuecomment-285941237, or mute the thread https://github.com/notifications/unsubscribe-auth/AFRMc2wWYdoLjUEjWlOg72EhtSJBIREZks5rk-SAgaJpZM4MahZw.\n\n\n. ",
    "rochapablo": "ORDER BY posts.created $(order^) LIMIT $(limit) OFFSET $(offset)\n@vitaly-t, much better now.\nThank you! . ",
    "allenyin55": "Thanks! I should mention only the first one works for me. Not sure why. . ",
    "aarontropy": "Same version.\n. OK, I will dig a little deeper.  Thanks.. GAH! apologies. Thank you for taking a look.. ",
    "skylize": "Ok, that makes sense. How about adding a reference to helpers to the connection object?. What I mean is that since db connection pool is already carrying around the overhead of the entire pgp module, there is no significant change if you just assign helpers to the connection pool object. But providing that reference makes it so using helpers is easier and potentially reduces overhead significantly in some common design patterns.\nReturning to an altered version of the previous example with lots more comments, here is what I mean:\nExample useage: \napp.js\njavascript\n//...\n// instantiate connection pool\nconst db = require('pg-promise')(opts)(config)\n// insert some data through a data model from another file\nrequire('./model').insert(db, data, table)\n//...\nmodel.js (currently working, but too much overhead)\njavascript\n// Model would normally have some business logic and/or validation,\n//   but for simplicity of example we just do the insert.\nmodule.exports = {\n   // Even though we already have the connection pool at hand,\n    //    we must also pass around pgp object to access helpers \n    //    or, as done here, include & instantitate an entirely new copy of pgp.\n  insert (data, db, table){\n    // build query using helpers\n    const query =\n      require('pg-promise')().helpers\n      .insert(data, Object.keys(data), table)\n    // insert and return result\n    return db.query(query)\n  }\n}\nmodel.js (better solution, but currently won't work)\njavascript\n// Model would normally have some business logic and/or validation,\n//   but for simplicity of example we just do the insert.\nmodule.exports = {\n      // This would be much better, but fails because the connection \n    //    pool has no reference to helpers\n  insert (data, db, table){\n    // build query using helpers\n    const query = \n      // Error: cannot read property 'insert' of undefined\n      db.helpers.insert(data, Object.keys(data), table) \n    // insert and return result\n    return db.query(query)\n  }\n}\nFix needs to be done internally as shown by this attempted patch\napp.js (failed patch)\n```javascript\n//...\n// instantiate connection pool\nconst pgp =require('pg-promise')(opts) \nconst db = pgp(config)\n// Attempt to attach a helpers reference to our connection pool\n//    before passing it to the model.\n//    TypeError: Can't add property helpers, object is not extensible\ndb.helpers = pgp.helpers\n// insert some data through a data model from another file\nrequire('./model').insert(db, data, table)\n//...\n``\n. This issue is appropriately closed. So I am going to reopen a new issue with the new request and copy this example to it.. >const query = require('pg-promise')().helpers`\n\nYou should never do such coding. Variable pgp must be created only once and then exported around. See: http://stackoverflow.com/questions/34382796/where-should-i-initialize-pg-promise\n\nYes. I absolutely agree. That's why I came here. Lol\n\nvar helpers = db.$config.pgp.helpers;\n\nThat's perfect! Thanks. Don't think I would have found that without your help.\n\nI think you should read more of the documentation, to understand better how this library works and what one can do with it \ud83d\ude09\n\nYep, working on it.  \ud83d\ude0f . \ud83d\udc4d . ",
    "rafaelkallis": "Great, thanks for clarifying!. @vitaly-t mind If I make a pull request with an additional ES7 await clause example for the select-insert you referenced?. @vitaly-t will do, even though I have used babel in bigger projects, I currently explicitly use Typescript for all my code, which supports and transpiles the ES7 async clause. \nAdditionally if using latest node, async await is natively supported.\nWill create a PR with extended documentation later. Thanks again.. @vitaly-t you don't have to use --harmony-async-await since Node.js 7.6. @vitaly-t I am aware that anything done with await can be done using yield. I just find it more pleasing working with promises instead of generators. \nIf I remember correctly, the only difference in theory is that you can use arrow functions with an async clause, which is not possible using generators. (haven't tested it)\nEDIT: I'm also not sure how error handling works with generator functions. With an async clause I can catch the error inside an try catch clause, or with a promise styled asyncFn.catch(e => { ... }).\n```\n// async await\n        ...\n        await this.database.tx(async tx => {\n            if (await this.personModel.loginExists(login, tx)) {\n                throw new LoginExistsError();\n            }\n            const [personId, hash]: [number, string] = await Promise.all([\n                this.personModel.create(login, firstName, lastName, tx),\n                BCrypt.hash(password, 12)\n            ]);\n        await this.authModel.create(personId, hash, tx);\n        return personId;\n    });\n\n```\n```\n// generators\n        ...\n        const self = this;\n        yield this.database.tx(function* (tx) {\n            if (yield self.personModel.loginExists(login, tx);) {\n                throw new LoginExistsError();\n            }\n            const [personId, hash]: [number, string] = yield Promise.all([\n                self.personModel.create(login, firstName, lastName, tx),\n                BCrypt.hash(password, 12)\n            ]);\n        yield self.authModel.create(personId, hash, tx);\n        return personId;\n    });\n\n``.this.personModel.create(login, firstName, lastName, tx)executes one query.BCrypt.hash(password, 12)` executes no queries.\nIf Promise.all executes only one query, just like in the example above, shouldn't there be no issues at all?\nThat would be the equivalent of:\nconst result = await Promise.all([\n    t.any('select * from users')\n]);. Both are equivalent. I just believe also declaring getInsertUserId as async improves readability, because it allows the user to know that the function returns a promise.\nIf you disagree I can remove the async from the function definition.. On the other hand, if the user's build breaks, wouldn't that imply a type error in the user's codebase?. I have probably expressed myself in the wrong way in the previous message, allow me to explain it in a different way.\nI claim that, if the user's return type of any task() or tx() statement is in compliance with the expected return type, there should be no breaking change. \nEDIT: In other words, if there are no runtime errors, there should be no compile time errors.\n```ts\n// All good!\nconst good: number[] = db.task(async t => { return [1, 2, 3]; });\ngood.forEach( ... );\n// Breaks! (error during compile time instead of runtime)\nconst bad: number[] = db.task(async t => { return 1; });\nbad.forEach( ... ); // would result in runtime error\n```\nIf the return type T defaults to any, runtime errors like the one demonstrated above will continue to happen, AFAIK.. Let's not overcomplicate it. The default declaration (your last commit) is the best of both worlds.. Protocol extensions look damn good. PR can be merged from my side.. I don't use native protocol extensions, instead I roll my own modular wrappers of the pg-promise database object.\n\n. @vitaly-t that looks really good! Glad this library started avoiding the dreadful any!. ",
    "nilspreusker": "Excellent, thanks for the quick answer and keep up the great work!. ",
    "darrinholst": "\nHowever, since the queries within one transaction execute against the same IO channel, they become synchronized (by the server) with respect to each other, i.e. the sequence of queries executed will be exactly as you defined it.\n\nThis no longer appears to be the case. I had a list of 1 delete and 7 inserts and the delete would consistently be run after 1 or 2 inserts. Switching to sequence fixed it for me...\njs\n  return db.tx(tx => tx.batch(commands));\njs\n  return db.tx(tx => tx.sequence(i => commands[i]));\nNot sure what changed since the batch used to work. Running against heroku postgres 9.6.8 if it matters.. @vitaly-t yep, my bad. I was creating the queries off of db and not the tx. False alarm.. ",
    "Michal-dia": "As I said - I'm using it in unit tests while testing different scenarios with the same connection. Is there a problem is creating the same database connection after shutting down the connection pool in unit tests?. Thank you!\nOne more question - is there a way to know how many connections (from the connection pool) are used at a specific time?. ok thanks!\n. ",
    "socketwiz": "Ah thats what I needed, thanks!  It looks like if your parameter is a string you don't need to wrap it in quotes like I was doing.  Thanks for the quick response.. ",
    "gajus": "\nPlease check the API documentation before opening an issue. Method connect has always been there, which does exactly that.\n\nSorry. I've scanned through the docs but could not find it. To be specific, I only found the event \"connect\" thats visible in the sidebar. The documentation is lacking an index of all functions, e.g. like in the lodash documentation https://lodash.com/docs/4.17.4.\n\nA task (method task) does not initiate any transaction. Only method tx does that.\n\nRight. I misread the docs:\n\nA transaction, for example, is just a special type of task, wrapped in CONNECT->COMMIT/ROLLBACK.\n\n\u2013 https://github.com/vitaly-t/pg-promise#tasks\nThats a dangerous sentence to put just above task usage example.. ",
    "catastrophe-brandon": "Thank you! This helps clarify things a lot!. ",
    "mythical-programmer": "@msjoshi did you get the solution for this in AWS lambda? Do we need to make the connection everytime the lambda is invoked?. @msjoshi Thanks. I will be soon using it in production. Good to know.. ",
    "wickedman1976": "+1 here. ",
    "Crypt-iQ": "\n@mythical-programmer\nWe ended up using a very small connection pool (3-4) with low idle timeout (5 secs). This way if the request comes to the same lambda container, it uses connection from the pool. Ours was a high concurrency requests to Lambda functions so we got good mileage of this strategy.\n\nHey @msjoshi !  Just a quick question that we can move to a private chat if necessary.  How were your lambda functions designed wrt pg-promise such that the containers maintain connection pools to the database?  I am working on a project that uses AWS Lambda + pg-promise and I was wondering what happens when the lambda exits / times out.  In this case, wouldn't the pool be destroyed?  Or is it frozen and possibly thawed out with a later invocation?  Any help appreciated! Thanks!. @kamek-pf \nHey! I am a little confused by what you mean by this \"fire and forget\" possibility.  If you are using pg-promise correctly with promise chaining, then there shouldn't be any write queries in the event loop by the time you use AWS Lambda's callback function.  Am I missing something? Thanks!. @kamek-pf Thanks, that's what I thought! One more question that we could maybe move to some sort of private chat.  What happens when the lambda function times out - is the connection to the database eventually closed or is it frozen in the event loop and possibly thawed out on lambda invocation?  Where can I find out more about AWS Lambda container re-use?  Is the connection to the db even IN the event loop?. ",
    "lfreneda": "@msjoshi do you have any feedback of your implementation in production? I'm facing the same situation.. @kamek-pf I'm facing the same situation, how did you end up implementing?. ",
    "dmnBrest": "Found it so quickly :)\nhttp://vitaly-t.github.io/pg-promise/global.html#event:query\nJust need to add \"query\" event handler.. ",
    "illarionvk": "My queries have up to 10-12 columns and the number of columns with actual data may vary in partial update requests.\nThe data is a sanitized output from Joi validator, where the defaults are set and some fields are stripped, e.g created_at.\nWith cs.names and pgp.helpers, I can rely on the data object shape to build the query.\nI haven't tried to build multi-row operations yet. Thank you for the advice!. ",
    "kiejo": "Thanks for the fast reply. This is very helpful!. Using external SQL files is an interesting approach which I'm already using for my database migrations, but I haven't applied it to my application logic queries yet. Thanks for the demo link :) This gives me a better idea of how this looks in real code.. Thanks for the fast answer! I think it makes sense to not have this ambiguity on the Result object. Another approach that I think could work well, would be to make the duration available via the event that gets passed to receive as the third parameter. It could represent the complete duration of a database call regardless of whether it was called using multiResult or any of the other database methods. What do you think?\nOverall, I don't really like the idea of having to wrap all database calls in a Task just to get access to the duration.. Thanks for the detailed information. Changing the way receive works definitely sounds like a bad idea and I can see now why receive isn't a good solution for duration logging after all. Based on this information I think a new and separate event analogous to the query event would be a much nicer solution for my particular use case:\n While the query notification happens just before the query execution, this new event would be triggered right after the query execution\n The event would be triggered after a streaming query completes\n The event would be triggered for queries that return no data\n The event would make the duration available to the handler\n* Based on the names of the other events it could for example be called result or complete\nOf course I don't expect you to implement this just for me. I'm just looking to provide feedback based on my use case and hope that others might benefit from such a feature as well. I also don't know how much work this would be to implement as I haven't worked with the pg-promise source yet. Thanks for all the work you put into pg-promise and the great support you provide.. That would work for me too as \"duration logging\" is currently my only use case for the receive event!. ",
    "italoandrade": "I used try catch and still doesn't \"work\", I traced the exception and it is unhandled because you have a line on your code that is preventing of throwing.\nStill, I think you should be more polite when talking to people who uses your library. Or maybe don't answer if you don't want to help.\nThank you.. ",
    "pshemk": "Hi,\nI seem to have the same problem, the following sequence causes it:\n```\nconst productInstanceCreate = async (req) => {\nvar instanceId;\nvar customerId;\n\ntry {\n    //check if we have a customer with this name, if not create one\n    customerId = (await db.oneOrNone(\"SELECT customer_id FROM customer WHERE name=$1\", [req.params.customer])).customer_id;\n    log.debug(\"productInstanceCreate: customerId: %s\", JSON.stringify(customerId, null, 1));\n    return customerId;\n} catch (e) {\n    log.error(\"productInstanceCreate: error: %s\", JSON.stringify(e, null, 1));\n    throw new Error(\"productInstanceCreate: error: %s\", e);\n}\n\n```\nThat 'catch' there doesn't see an error thrown, but if I setup a handler for an unhandled rejection I can trap it (but since at that stage it has not context I can't do much with it):\nprocess.on(\"unhandledRejection\", (reason, p) => {\n    log.error(\"Unhadled Rejection at: %s, reason: %s\", reason, p);\n});\nI get this (some output omitted):\n```\n[2017-06-26T01:49:42.255Z] DEBUG: instance-inventory-api/14349 on backend: productInstanceCreate: customerId: undefined\n[2017-06-26T01:49:42.273Z] ERROR: instance-inventory-api/14349 on backend:\n    Unhadled Rejection at: QueryResultError {\n        code: queryResultErrorCode.multiple\n        message: \"Multiple rows were not expected.\"\n        received: 5\n        query: \"SELECT customer_id FROM customer WHERE name='customer name2'\"\n    }, reason: [object Promise]\n```. I'm struggling to reliably replicate this myself. Using a simple snipped like that it works reliably, so it must be my code :-)\nnode.js: 8.1.2\npg-promise:  6.1.0\nos: ubuntu 16.04.2 \nI'll update this issue if I manage to get a reliable way of replicating the problem. . ",
    "stepankuzmin": "I agree, but it makes sense if you make a reusable library which accepts db as a parameter and needs information about connection itself. We can pass the connection object into the library too, though.. Great! I think that it wouldn't be hard to implement. Maybe I can make a PR for this?. That makes sense.. So there is no way to use helpers with this kind of columns then?. I've \"source-layer\" text in my create table. I'm trying to use insert and update helpers on this table.. ~~What do you mean by source object? Query result object? If yes, then it is source-layer too. Actually I'm refering to it as source-layer everywhere.~~\nYes, I'm using  string presentation for property names.. Thanks for the reply! That makes things clear.. ",
    "cedvdb": "Thanks for the quick reply. It wasn't directly obvious to me as I read the part of the doc on array.. ",
    "darknoon": "@vitaly-t I considered using external .sql files but didn't find any information about how to integrate with webpack (I compile the server code as well as the client with webpack before deployment, and currently do not copy any files outside of this bundle).. ",
    "daveisfera": "Ok, here's the updated code with issues 1-3 resolved:\n```\n!/usr/bin/env node\nconst pgp = require('pg-promise')();\nconst os = require('os');\nconst host = process.argv[2] || 'localhost';\nconst database = process.argv[3] || os.userInfo().username;\nconst user = process.argv[4] || os.userInfo().username;\nconst config = { host, database, user };\nconst db = pgp(config);\ndb.tx(t => {\n    const values = ['$1.00', '$2.00'];\nreturn t.batch([\n    t.none('DROP TABLE IF EXISTS test;'),\n    t.none('CREATE TABLE test (a TEXT, b TEXT, c TEXT);'),\n    t.none('INSERT INTO test (a, b, c) VALUES ($1, $2, \"$3.00\");', values)\n]);\n\n})\n.catch(err => {\n    console.log('Error:', err);\n});\n```\nFor 4, I understand that doing the value replacement is the ideal way of handling things, but is \"thou shalt never put values directly in a query\" really the stance that pg-promise is taking?. The server isn't applying any timezone offset. If you do the select from a Postgres client or another language, then you get the correct/expected answer.\nAs you mentioned, there are multiple issues, so how do I go about disabling the application of the local timezone offset as described in the upstream issues?. I understand that, but node-postgres exposes a way to disable this conversion, so how do I access that through pg-promise?. Yes, that's what I was looking for.\nFor the sake of anyone that comes across this issue. Here's how I fixed it:\nconst pgp = require('pg-promise')();\nconst moment = require('moment');\n// 1114 is OID for timestamp in Postgres\npgp.pg.types.setTypeParser(1114, str => moment.utc(str).format());. Actually, further testing revealed that this was still an issue with any() but was just less likely to happen, so what is the proper way to be sure that the transaction is committed when the promise is resolved?. We construct an array of arrays with the queries and data, insertQueries and then run the following:\nreturn db.tx(t =>\n    t.batch(_.map(insertQueries, queryValues => {\n        const [query, values] = queryValues;\n        return t.none(query, formatValues.concat(values));\n    }))\n);\nHere's an example of insertQueries:\n[\n  [\n    'INSERT INTO test AS a ($2~, $3~) VALUES $5^ ON CONFLICT ($2~) DO UPDATE SET value=EXCLUDED.value;',\n    '(0, \\'a\\'), (1, \\'b\\'), (2, \\'c\\')'\n  ] \n]. Sorry, that's a mistake on my part. I stripped down our actual code a bit to make that example.. Sorry, I'm not requesting that pg-promise have embedded support, but just that it expose the ability to call array.parse() directly instead of having to use arrayParser.. That I can do something like the following:\nconst replace = require('lodash/fp/replace');\npgp.pg.types.setTypeParser(1115, str => pgp.pg.types.array.parse(str, replace(' ', 'T')));\nBasically, be able to call array.parse() without needing to import postgres-array.. I can, but it's 1 line of code vs 10+ lines of code. It sounds like you're not interested in addressing something like this, so go ahead and close it.. I have to do the following instead of the single line I showed above:\n```\nconst replace = require('lodash/fp/replace');\nvar parseTimestampArray = function (val) {\n    if (!val) {\n        return null;\n    }\n    var p = arrayParser.create(val, function (entry) {\n        if (entry !== null) {\n            entry = replace(' ', 'T', entry);\n        }\n        return entry;\n    });\nreturn p.parse();\n\n};\npgp.pg.types.setTypeParser(1115, str => parseTimestampArray(str));\n```. ",
    "elmigranto": "Since connect is an event handler, would it be possible for queries to be executed before setupFreshConnection runs? I guess not, since EventEmitter#emit is usually done on the same tick as IO callback? Though I am a bit concerned about suppressing errors part.. > The connection just has been allocated there.\nFrom sources, looks like event is \"emitted\" after promise is resolved but I tried to actually run it and it seems to work in correct order, though. Not sure how that happens, tried to figure out proper order of stuff but got a bit lost, maybe later :)\nAnd for errors, I could always process.reallyExit(1).\nSo yeah, thanks for a prompt reply :). I would think queries run in order on connection without multiplexing. E.g. running this will always return 1, 2, 3 in order.\njs\nconst result = await Promise.all([\n  conn.one('select 1;'), // no waiting\n  conn.one('select 2;'), // queue next query right away\n  conn.one('select 3;'), // it probably is buffered\n]);\nHowever, you can't run initialization that depends on query results. You'd have to wrap the library in some way.\n\nOnce again: I haven't checked sources, so this is just a guess.. @olalonde more details, basically:\n``` js\nconst pgPromise = require('pg-promise')();\nconst pgp = pgPromise('postgres://localhost:5432/database-name');\npgPromise.end(); // close all conections in all pools\npgp.$pool.end(); // close that particular one\n// In tests this should work:\nafterAll(() => pgp.$pool.end());\n```. I get usecases for timeout and server/socket, but not sure about Postgres (and DBs in general).\nYou probably don't want to deal with undetermenistic \"fire and forget\" situations, so the only thing I can imagine is when you have some rows streaming in from read-only select, or listening for PGNOTIFY, but even then, there are better ways to kill that IO. (Or is there, @vitaly-t is it possible to cancel queries?). ",
    "danihodovic": "\nThis approach guarantees execution of queries before anything else for fresh connections :wink:\n\nThat doesn't seem to be the case in the example below:\n```\n$ cat pg-promise-example.js\n'use strict';\nconst config = require('./config/config.js');\nconst options = {\n  connect: (client, dc, isFresh) => {\n    setTimeout(() => console.log('Initializing now'), 10000);\n  }\n};\nconst pgp = require('pg-promise')(options);\nconst db = pgp(config);\ndb.any('select 1').then(() => console.log('selected 1'));\nOutputs:\n$ node pg-promise-example.js\nselected 1\nInitializing now\n```\nThe use case I need this for is initializing custom type parsers before performing any queries in my application. Specifically I need to execute the query below before any point queries happen:\nconst customPoint = require('./type-parsers/custom-point');\n  const oidSQL = `SELECT oid FROM pg_type WHERE typname = 'geometry'`;\n  const { rows } = yield client.query(oidSQL);\n  pgp.pg.types.setTypeParser(rows[0].oid, customPoint);\nI tried putting this code in the connect callback, but as seen in the simple example above, the connect callback isn't blocking, so other queries may execute before it.\nIs there any other workaround for executing arbitrary blocking queries upon initialization? I don't want my users to accidentally get invalid point data if they happen to execute a query before the custom types initialize.. Since the function is asynchronous without a done() callback the setup logic may take longer than initial queries. If you have a long running query in the connect callback other queries might execute before this is done (as simulated by the timeout).. Ah, so there is a FIFO mechanism when storing pending queries. Neat!\nThanks @vitaly-t and @elmigranto !. My concrete use case is to set a custom point type parser before any other queries are executed. \n``\nconst init = Bluebird.coroutine(function * (client) {\n  // Oids varies so we need to query it instead of hardcoding it.\n  const oidSQL =SELECT oid FROM pg_type WHERE typname = 'geometry'`;\n  const { rows } = yield client.query(oidSQL);\n  pgp.pg.types.setTypeParser(rows[0].oid, point);\n});\nconst options = {\n  promiseLib: Bluebird,\n  connect: (client, dc, isFresh) => {\n    if (isFresh) {\n      init(client);\n    }\n  }\n};\n```\nIs the point parser guaranteed to be set-up before any other query executes?. > 1. You can use ES6 generators within connect\nCould you provide an example based on the above code?\n\n\nYou should call setTypeParser in the beginning, not during queries.\n\n\nThis is not possible in my use case since the oidc number varies between our Postgres instances. So it has to be queried for dynamically and set after the query is executed.. It wasn't listed in the official docs (https://vitaly-t.github.io/pg-promise/module-pg-promise.html) so I assumed it wasn't used.. ",
    "LoiKos": "to be sure you understand my need i give you 2 examples :\njavascript\nvar obj = {\n     name: \"$1\",\n     picture: \"$2\"\n}\nsql\nupdate tablename set (\"name\",\"picture\") = ('$1','$2') where id = myid returning *\n============\njavascript\nvar obj = {\n     firstname: \"$1\",\n     lastname: \"$2\"\n     picture: \"$3\"\n     email: \"$4\"\n     phone: \"$5\"\n}\nsql\nupdate tablename set (\"firstname\",\"lastname\",\"picture\",\"email\",\"phone\") = ('$1','$2','$3','$4','$5') where id = myid returning *\nMy need it to adapt the request to an object data. \n(${obj~}) does the job to get keys but passing the obj in another object make it impossible to retrieve values and i can't add id to obj because it make no sense.\n. Sorry I see,\njavascript\nvar obj = {\n     firstname: \"Jean\",\n     lastname: \"Jean\"\n     picture: \"https://www.w3schools.com/css/img_fjords.jpg\"\n     email: \"jean.jean@gmail.com\"\n     phone: 333\n}\nsql\nupdate tablename set (\"firstname\",\"lastname\",\"picture\",\"email\",\"phone\") = ('Jean','Jean','https://www.w3schools.com/css/img_fjords.jpg','jean.jean@gmail.com',333) where id = myid returning *. I wasn't trying to insert variable i choose bad name sorry. Okay i see, you can close this \nThanks ! . That because I don't paste all the code, this is the complete function : \n```swift \nupdate(id, json) {\n        return db.task(function* (t){\n        let product = yield t.oneOrNone(\"select * from products where refproduct = $1\",[id])\n\n        if(!product){\n            return Promise.reject(ApiError.notFound())\n        }\n        for(key in json){\n            if (Object.keys(product).includes(key) && [\"refproduct\",\"creationdate\"].indexOf(key) == -1){\n                    product[key] = json[key]\n            } else {\n                return Promise.reject(ApiError.notFound(\"One key in the json body is not known or can't be modified\"))\n            }\n        }\n        return yield t.one('update products set name = ${name}, picture = ${picture}  where refproduct = ${refproduct} returning *', product)\n    })\n}\n\n```. I'm not familiar with generator I can use it like this ? \njavascript \nif(!product){\n    throw(ApiError.notFound())\n}. Okay thanks you for your answers :) . ",
    "aberenyi": "Sorry if I wasn't clear enough, nevertheless you've answered my question.\nI've gone ahead with a small wrapper class and a cache.\n```js\nconst dbCache = new Map()\nexport class PostgreSQL\n{\n  db: IDatabase&IExtensions\n  constructor({dbName = defaultDbName}: {dbName?: string} = {})\n  {\n    const connObj: IPgConnObj = config.pgConnObj\n    connObj.database = dbName\nconst {host, port, database, user} = connObj\nconst dbKey = JSON.stringify({host, port, database, user})\n\nif (dbCache.has(dbKey))\n{\n  this.db = dbCache.get(dbKey)\n}\nelse\n{\n  const pgpOptions =\n  {\n    capSQL: true,\n    pgFormatting: true,\n    pgNative: true,\n    extend: (obj: any) =>\n    {\n      obj.close = () => obj.$config.pgp.end()\n    }\n  }\n  const pgp: IMain = pgPromise(pgpOptions)\n  this.db = <IDatabase<IExtensions>&IExtensions>pgp(connObj)\n  dbCache.set(dbKey, this.db)\n}\n\n}\n}\n```\nThanks.. ",
    "jdalrymple": "How so? \nbasic syntax:\nUPDATE table SET (cola, colb) = (4, 6) where colc = '3'\nUsing a similar named params seen here: \n```\nvar obj = {\n    one: 1,\n    two: 2\n};\nformat('INSERT INTO table(${this~}) VALUES(${one}, ${two})', obj);\n```\nbut attempting to do the similar object replacement technique in an Update command.\n. Played with it a little longer, figured it out. I think i was sending an incorrect type for the where clause, though the error never pointed to it. My bad! \nThank you for the help :D. I understand! I want to be able to access a transaction from outside of the callback for a specific implementation, but I can't do that currently. That's why I I'm asking if the implementation shown in node-postgres is possible with pg promise :). Perhaps my solution isn't an automatic solutions I guess.\nI'm trying to begin a transaction and maintain a id to that transaction\nPerform some queries in that transaction\nAnd then separately commit/rollback that transaction. \nPractically exactly like my example code above. \nI would like to execute it this way because at the time of the beginning the transaction all the queries that I would like to run within that transaction are not known yet. That's why i can't use the implementation in the docs ( where the execution happens within the callback) . Ah, Hmm, so how would I execute that? Could you provide an example? Thank you!. \ud83d\ude02 No i meant with reference to the db.tx. \nDo you mean like this:\ndb.tx('my transaction', t => {\n    // creating a sequence of transaction queries:\n    const q1 = t.none('UPDATE users SET active = $1 WHERE id = $2', [true, 123]);\n})\n.then(t => {\n    const q2= t.none('UPDATE users SET active = $1 WHERE id = $2', [true, 123]);\n})\nDoes that mean it could also then be executed like:\n```\nawait db.tx('my transaction', t => {\n    // creating a sequence of transaction queries:\n    const q1 = t.none('UPDATE users SET active = $1 WHERE id = $2', [true, 123]);\n})\nawait db.tx('my transaction', t => {\n    const q2= t.none('UPDATE users SET active = $1 WHERE id = $2', [true, 123]);\n})\n```\nWhere the second query (q2) is part of the same transaction?. So clarifying, transaction queries must happen within the same transaction callback. Thanks. ",
    "TimJMartin": "sorry @vitaly-t I should have included my error\nWhen I have tried to run VACUUM ANALYZE from a pgAdmin SQL window I get \nVACUUM cannot be executed from a function or multi-command string\nSo I wanted to be able to run a VACUUM ANALYZE command from my data loading script.\nThis is the error when I try to use \ndb.tx(t => {\n        // this.ctx = transaction config + state context;\n        return t.batch([\n            t.none('VACUUM ANALYZE data1'),\n            t.none('VACUUM ANALYZE data2')\n        ]);\n    })\nERROR: BatchError {\n    stat: { total: 2, succeeded: 0, failed: 2, duration: 15 }\n    errors: [\n        0: { error: VACUUM cannot run inside a transaction block\n            at Connection.parseE (E:\\Projects\\Temp\\promise-test\\node_modules\\pg\\lib\\connection.js:539:11)\n            at Connection.parseMessage (E:\\Projects\\Temp\\promise-test\\node_modules\\pg\\lib\\connection.js:366:17)\n            at Socket.<anonymous> (E:\\Projects\\Temp\\promise-test\\node_modules\\pg\\lib\\connection.js:105:22)\n            at emitOne (events.js:96:13)\n            at Socket.emit (events.js:188:7)\n            at readableAddChunk (_stream_readable.js:176:18)\n            at Socket.Readable.push (_stream_readable.js:134:10)\n            at TCP.onread (net.js:547:20)\n          name: 'error',\n          length: 111,\n          severity: 'ERROR',\n          code: '25001',\n          detail: undefined,\n          hint: undefined,\n          position: undefined,\n          internalPosition: undefined,\n          internalQuery: undefined,\n          where: undefined,\n          schema: undefined,\n          table: undefined,\n          column: undefined,\n          dataType: undefined,\n          constraint: undefined,\n          file: 'xact.c',\n          line: '3156',\n          routine: 'PreventTransactionChain' }\n        1: { error: current transaction is aborted, commands ignored until end of transaction block\n            at Connection.parseE (E:\\Projects\\Temp\\promise-test\\node_modules\\pg\\lib\\connection.js:539:11)\n            at Connection.parseMessage (E:\\Projects\\Temp\\promise-test\\node_modules\\pg\\lib\\connection.js:366:17)\n            at Socket.<anonymous> (E:\\Projects\\Temp\\promise-test\\node_modules\\pg\\lib\\connection.js:105:22)\n            at emitOne (events.js:96:13)\n            at Socket.emit (events.js:188:7)\n            at readableAddChunk (_stream_readable.js:176:18)\n            at Socket.Readable.push (_stream_readable.js:134:10)\n            at TCP.onread (net.js:547:20)\n          name: 'error',\n          length: 143,\n          severity: 'ERROR',\n          code: '25P02',\n          detail: undefined,\n          hint: undefined,\n          position: undefined,\n          internalPosition: undefined,\n          internalQuery: undefined,\n          where: undefined,\n          schema: undefined,\n          table: undefined,\n          column: undefined,\n          dataType: undefined,\n          constraint: undefined,\n          file: 'postgres.c',\n          line: '994',\n          routine: 'exec_simple_query' }\n    ]\n}. Is there a different way to run a query like VACUUM ANALYZE?\nIn pycopg2 you can change the connection to a different isolation level to be able to run that type of query:\nold_level = connection.isolation_level\nconnection.set_isolation_level(0)\nvacuum = \"VACUUM ANALYZE data1\"\ncursor.execute(vacuum)\nconnection.set_isolation_level(old_level)\nSo I just wondered if there was a similar approach for pg-promise I had not seen in the docs\n. I think our messages crossed so I didn't see you advice to switch to task.\nYes just changed the db.tx to db.task and the SQL ran successfully.\nThanks very much for your help.\n. ",
    "tonylukasavage": "Are there plans to actually handle this with native Promises so I don't have to pull in another Promise lib just to get tack traces out of pg-promise?. So the answer then is \"no\" you won't be supporting ES6 Promises with useful stack traces ever and the only way to get them is by pulling in bluebird as pg-promise's Promise lib? Just making sure I'm understanding you here. If this is the case you might want to note it in your docs for promiseLib since this was a major problem for me and I assume many others.\n. Dude, relax, you seem to be just oozing with condescension, insinuating that I have no idea how Promises work. I'm just trying to highlight a pain point that you yourself are aware. I'm suggesting that if you KNOW bluebird with long stack traces is the only way to get useful stack traces out of pg-promise, it MIGHT be worth noting in your section of documentation dedicated to Promise lib support. You don't have to take this suggestion as a persona affront, it's just feedback from a user, not an attack on your project or prowess as a developer. Do with my suggestion what you will.. Not so much noting that bluebird supports long stack traces, but that ES6 stack traces (default) WON'T be helpful and that you should use an alternative lib to get them. You're right, stack traces are the realm of the Promise implementation, but not all libs are affected as negatively as pg-promise appears to be WRT stack traces, hence my initial concern. \nHappy to PR, so long as you can drop the condescending comments. I'd much rather be a contributor than some unfairly perceived insurgent on your repo.. \ud83d\udc4d\nOn Jun 3, 2017 3:35 PM, \"Vitaly Tomilov\" notifications@github.com wrote:\n\n@tonylukasavage https://github.com/tonylukasavage to make you feel\nbetter, following this issue I brought this up within the Massive.js that\njust migrated to pg-promise: New driver considerations\nhttps://github.com/dmfay/massive-js/issues/381.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/342#issuecomment-305996697,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkQwNn9ybWPoVcym6M4PErvBBROqXEyks5sAbWIgaJpZM4NsK9-\n.\n. Understood. Would it be simpler to reference the SO answer you have, or add\nit's content to the promisdLib section?\n\nOn May 31, 2017 5:55 PM, \"Vitaly Tomilov\" notifications@github.com wrote:\n\nWell, they all generate poor stack traces, not just ES6 Promise :)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/pull/343#issuecomment-305329159,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkQwDzXnBtf76hQAhagiJpcv--U6FINks5r_eHLgaJpZM4NsO77\n.\n. I mean, sure, that's better than nothing, but you understand that \"long\"\nstack traces aren't the point, right? It's not that the stack traces aren't\nlong with the default ES6 promises, it's that they are totally not\nactionable with the information given for anything but trivial examples. Do\nwe agree that that's a problem? If not, then we can just close this issue\nand I can be happy I found it early.\n\nI'm not \"blaming\" pg-promise, I'm just noting a problem with the default\nbehavior that surfaces through it. It's up to you whether you care how that\naffects users.\nOn May 31, 2017 6:22 PM, \"Vitaly Tomilov\" notifications@github.com wrote:\n\nI've just updated the section: https://github.com/vitaly-t/\npg-promise#promiselib\nthe change:\nBluebird - best alternative all around, which includes the very important\nLong Stack Traces;\nPerhaps should suffice? ;)\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/pull/343#issuecomment-305335098,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkQwOyoJad2VgbunIfjhaagsjwoYL_Dks5r_eg2gaJpZM4NsO77\n.\n. Kk, thanks for the doc update, good luck on 6.x\n\nOn May 31, 2017 7:08 PM, \"Vitaly Tomilov\" notifications@github.com wrote:\n\nIt's up to you whether you care how that affects users.\nI did reply to all your questions, and I did an update in documentation,\nwhich means that I do care. Beyond that, I'm now too focused on version\n6.x https://github.com/vitaly-t/pg-promise/tree/6.x of the library.\nLegacy documentation isn't a priority for me right now.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/pull/343#issuecomment-305343644,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAkQwCj7pOmk6Tr9QsKRN1vrUo0EmEvOks5r_fLkgaJpZM4NsO77\n.\n. \n",
    "euforic": "I'm going to have to give round one to @tonylukasavage I am looking forward to round 2 \"The Pull Request\" \ud83d\udc4d\ud83d\ude09. Some of us had to work haha. Glad to see it all worked out. \ud83d\ude2c. ",
    "psypersky": "My grain of salt in case someone has the same problem.\nI didn't liked just to extend the stack using an external lib so I just did this in my queries.js files\nso i just replaced the stack, I don't think i need the stack of the library\njs\nasync function getAll() {\n  try {\n    const users = await dbClient.db.any(`\n      SELECT ${memberFields} FROM member\n    `)\n    return users\n  } catch (err) {\n    throw new Error(err.message)\n  }\n}\nI guess you can also do something to call a method and wrap it in a try catch that you end up using something like\njs\nfunction getAll() {\n    return patchedClient.any(`\n      SELECT ${memberFields} FROM member\n    `)\n}\nand this patched client \"any\" function has the try and catch logic inside it, and returns a promise that you can await\n@vitaly-t do you see any potencial problem in doing this?. ",
    "sustained": "And yes, the likes column is most definitely INTEGER.\nI've tried this with $1^, $1~, $1# and I've tried with ::int and ::integer too and all of these produce the exact same error.\nUncaught Promise Error:\nerror: operator does not exist: character varying = integer\n    at Connection.parseE (.../node_modules/pg-promise/node_modules/pg/lib/connection.js:539:11)\n    at Connection.parseMessage (.../node_modules/pg-promise/node_modules/pg/lib/connection.js:366:17)\n    at Socket.<anonymous> (.../node_modules/pg-promise/node_modules/pg/lib/connection.js:105:22)\n    at emitOne (events.js:96:13)\n    at Socket.emit (events.js:191:7)\n    at readableAddChunk (_stream_readable.js:178:18)\n    at TCP.onread (net.js:561:20). I did try to reduce the amount of code that was posted, removing any non-relevant code and marking any code that is long, somewhat relevant but already working as such.\nI'll give that a try.. I've added pg-monitor to my package.json.\nThen I require it, call attach with the same initialization options I use for pg-promise (in lib/Postgres.ts) but I don't get any fancy coloured output or anything.\nI can give you this, which is slightly more information:\n{ error: operator does not exist: character varying = integer\n    at Connection.parseE (.../node_modules/pg-promise/node_modules/pg/lib/connection.js:539:11)\n    at Connection.parseMessage (.../node_modules/pg-promise/node_modules/pg/lib/connection.js:366:17)\n    at Socket.<anonymous> (.../node_modules/pg-promise/node_modules/pg/lib/connection.js:105:22)\n    at emitOne (events.js:96:13)\n    at Socket.emit (events.js:191:7)\n    at readableAddChunk (_stream_readable.js:178:18)\n    at TCP.onread (net.js:561:20)\n  name: 'error',\n  length: 207,\n  severity: 'ERROR',\n  code: '42883',\n  detail: undefined,\n  hint: 'No operator matches the given name and argument type(s). You might need to add explicit type casts.',\n  position: '76',\n  internalPosition: undefined,\n  internalQuery: undefined,\n  where: undefined,\n  schema: undefined,\n  table: undefined,\n  column: undefined,\n  dataType: undefined,\n  constraint: undefined,\n  file: 'parse_oper.c',\n  line: '726',\n  routine: 'op_error' }\n... but I'm getting nothing from pg-monitor, am I doing something wrong with it?. ```ts\nimport * as Bluebird from 'bluebird';\nconst PostgresPromise = require('pg-promise')({promiseLib: Bluebird});\nconst PGMonitor       = require('pg-monitor');\nvar _instance : any,\n    _password : string;\nexport default class Postgres {\n    static setPassword(password : string) {\n        _password = password;\n    }\nstatic db() : any {\n    if (_instance == null) {\n        _instance = PostgresPromise({\n            user:     'username_here',\n            password: _password,\n            database: 'database_here'\n        });\n\n        PGMonitor.attach({\n            promiseLib: Bluebird\n        });\n    }\n\n    return _instance;\n}\n\n}\n```\n:thinking: . The issue is that I'm an idiot.\nThe data proxying:\nts\n        // How in the world do we get Typescript to detect these?\n        for (let [key, value] of Object.entries(data)) {\n            Object.defineProperty(this, key, {\n                get: ()  => this.data[key],\n                set: (v) => this.data[key] = v\n            });\n        }\n... was overwriting this.id (a Discord snowflake) with this.data.id (an integer (our PK)).\nAnd so WHERE discord_id = 123 was causing the error.. @vitaly-t I did, thank you.. ",
    "demurgos": "With all due respect your comment does not say what are my wrong assumptions. The core assumption of this issue is that to access anything from this library, I currently have to initialize it at some point in time. Is it correct?\nTo get a reference to a database, you need two calls. One to initialize the library, one to configure the database. This issue is about removing the initialization step of the library (more exactly to make it optionnal).\njavascript\n// Currently:\nvar db = require(\"pg-promise\")({promiseLib: Bluebird})(\"pg://localhost/db\");\n// Proposed:\nvar db = require(\"pg-promise\").createDatabase(\"pg://localhost/db\");\nI am fully aware of the two integration methods to get a reference for the central Database object. These two methods and the hybrid approach are the best to get a reference for a Database object.\nThese methods make sense for a stateful object such as the Database instance.\nThis issue is about accessing an instance of the main pgPromise object. Since this object currently requires an initialization step, I also have to either pass it as a parameter or initialize my own instance. I claim that it would be easier if I could skip these steps because the pgPromise object is less dependent on its state.\nThe two problems caused by the required library initialization are:\n- Passing a reference or initializing my own pgPromise library instance when I only want access to static properties from the root namespace does not make sense.\n- This is cumbersome when I do not want to provide custom initalization and just want to create a database\nI differentiate three things:\n- The factory function: what I get from require: var factory = require(\"pg-promise\");\n- The library instance: what I get by calling the factory function: var pgp = factory();\n- The database instance: what I get by calling the library instance: var db = pgp(\"pg://...\"). > What I'd recommend is to use an approach similar to Request. The main module exported by request is an instance of the library configured with the default values. If you want a custom version with a special configuration, you can do it by calling a function on the exported object (.options) and it will return you a new instance that does modify the original instance of the library.\nWhy wouldn't a similar approach be possible for this library? With defaults using the system Promise and pgNative being false? It would match the hybrid approach you described in the Wiki page but applied for the whole library: pass parameters or rely on defaults, there's no real difference.\nThere would be some other changes, but the main one would be to replace the main export by:\njavascript\nconst libWithDefaults = $main();\n$npm.utils.addReadProperties($main, libWithDefaults);\n$npm.utils.addReadProperty($main, \"createDatabase\", (...args) => libWithDefaults(...args));\nmodule.exports = $main;\n. Thank you for your time on this.\nI was using the pg package for a long time and wanted to upgrade to your package because it has many interesting features: it's way more than just \"pg with promises\".\nIf I remember well, even when ES6 Promises did not exist, the mongodb driver shipped with q and let you override it if you want. The reason why I opened this issue is that your library no longer has any required initialization option so it could provide a default. Now, I understand that you are busy working on the next version, but would you be interested by a PR bringing this feature? It could be for a next major version eventually, but I guarantee that this would allow to simplify the work with your library (see the example in my first message where you could define free-floating transaction functions).\nStill, I know that it wouldn't be as easy: having a very tight control over the instances of your components brings guarantees that could be challenged otherwise (its mainly about the reliability of the instanceof operator).\nOverall, it's not that I do not want to track my references, but because I feel that this kind of architecture where you have to pass references creates dependencies that tend to be harder to reason about. My example with a custom transaction mode needing a reference to the pgp object is a \"plugin\" problem similar to the one encountered by grunt, gulp, webpack, etc. These have three approaches:\n- Grunt tasks are factory functions that take a reference to the current grunt instance. They are highly dependant on grunt (can peek what are the other dependencies, configs, etc.) and work similarly to the method currently available to expose a custom transaction mode consumable by pg-promise. \n- Gulp tasks do not depend on gulp, they are simple functions that work on vinyl files. It is closer to the use of conventions / structural typing. The identity of the parameters do not matter as long as they provide the right interface. You can reuse them outside of gulp (this file abstraction is pretty handy). This is the kind of architecture I am more fond of.\n- Webpack's plugin management is more based on \"peers\". This is the third way to deal with references (not mentioned previously): have a single db.js file and rely on the cache of Node's resolution algorithm to always get the valid reference. Works fine for the top level package, not so well when trying to compose multiple small packages (I consider peer-dependencies an anti-pattern for node, cache between modules of a single package can be fine).\nThe grunt way is already working well with this library, I'd like a more gulp'y way, the webpack way is fine for top-level consumers.. What is the type of your \"JSON\" column in your table schema? Are you using the native JSON Postgres type? Columns using this type are automatically parsed and serialized by pg-promise.. ",
    "dustincjensen": "I haven't tried it myself, but any call that returns a promise can be converted to async/await assuming you are using a version of Javascript or Typescript that supports it. I don't know the versions off by heart, but here is an example (from the examples)...\ndb.any('SELECT * FROM users WHERE active = $1', [true])\n    .then(function(data) {\n        // success;\n    })\n    .catch(function(error) {\n        // error;\n    });\nWould translate to...\ntry {\n    let data = await db.any('SELECT * FROM users WHERE active = $1', [true]);\n} catch (error) {\n    // error\n}. ",
    "fibo": "Cool!\n@vitaly-t  I am exactly in the situation described in the 4. Intermediate values section of the article you posted, thank you for sharing again.\nThank you for confirm me that this new awesome feature will work out of the box, it would be a pretty hard work to refactor the code base but it is worth to increase the readability and for other reasons.\nIt looks like an happy ending of the JavaScript fatigue :). Ok, I will try that syntax, thank you for the hint.. ",
    "kamek-pf": "Thanks @vitaly-t.\nDefinitely not what we want then, because Lambda re-uses containers as much as possible.\nFor future reference, setting callbackWaitsForEmptyEventLoop to false at the top of the handler is probably what you're looking for.\nThe only down side I can think of if that fire and forget write queries probably won't work, because AWS will freeze the process before they can complete.. What I meant is that the execution will stop as soon as the callback function is executed :\ntypescript\nlongRunningAsyncJob();\ncallback();\nIn this case, longRunningAsyncJob might not run to completion before the Lambda is frozen.\nBut as you said, this is an issue only if you don't chain promises correctly.. ",
    "cultulhul": "thanks i thought all the methods return null if the query returned no rows. ",
    "zhaosjason": "Yes, that's what I meant by null for all fields.  Sorry if that wasn't clear, I was trying to be brief.  What I am trying to say is that when I use the wildcard (*) selector in my SELECT statement with a LEFT JOIN then I only get an array of nulls.  See below for more examples from pgp and psql (what I'm using as my reference).\nUsers:\n```\n    uid    | name | dob | sex \n-----------+------+-----+-----\n 123456789 |      |     |\n(1 row)\n```\nSessions:\n```\n sid | uid | environment | location | created \n-----+-----+-------------+----------+---------\n(0 rows)\n```\npsql:\n```\nmydb=> SELECT * FROM users AS u LEFT JOIN sessions AS s ON u.uid = s.uid;\nuid    | name | dob | sex | sid | uid | environment | location | created\n\n-----------+------+-----+-----+-----+-----+-------------+----------+---------\n 123456789 |      |     |     |     |     |             |          | \n(1 row)\n```\nOk, looks good.  Users has one row and Sessions is empty so this statement should just return that one record from Users.\npgp (example from previous post):\ndb.any(\"SELECT * FROM users as u LEFT JOIN sessions as s ON u.uid = s.uid;\")\n.then(rows => {\n    console.log(rows);          // prints null for all fields\n})\n.catch(error => {\n    console.log(error);\n});\n[ anonymous {\n    uid: null,\n    name: null,\n    dob: null,\n    sex: null,\n    sid: null,\n    environment: null,\n    location: null,\n    created: null } ]\npgp (without wildcard):\ndb.any(\"SELECT u.uid, u.name, u.dob, u.sex, s.sid, s.environment, s.location, s.created FROM users as u LEFT JOIN sessions as s ON u.uid = s.uid;\")\n.then(rows => {\n    console.log(rows);\n})\n.catch(error => {\n    console.log(error);\n});\n[ anonymous {\n    uid: '123456789',\n    name: null,\n    dob: null,\n    sex: null,\n    sid: null,\n    environment: null,\n    location: null,\n    created: null } ]\npgp (without LEFT JOIN):\ndb.any(\"SELECT * FROM users as u;\")\n.then(rows => {\n    console.log(rows);\n})\n.catch(error => {\n    console.log(error);\n});\n[ anonymous {\n    uid: '123456789',\n    name: null,\n    dob: null,\n    sex: null } ]\nAs you can see I'm only getting this error when I use the wildcard and the join.  I'm currently using pg-promise@5.9.0, PostgreSQL 9.6.2 on x86_64-pc-linux-gnu, compiled by gcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-16), 64-bit.  I'm also hosting my database on an Amazon AWS RDS instance, not sure if that'd make a difference.\n// Jason\n. Just noticed that my pg-promise is outdated.  Upgraded to 5.9.4 but still getting the same issue.\n// Jason. Ah ok, sounds good.  Thanks for the info!. ",
    "retorquere": "I get these errors with the connection strings as listed, but I can get you the actual URLs if I can send them to you in private (I'd rather not put live passwords in the issue).. If you say it's outside then it's outside, but I get the same different errors when I pass a config object; this modified script:\n```\n// testpg.js\nconst pgp = require('pg-promise')();\nconst parse = require('pg-connection-string').parse;\nconst url = process.argv[2];\nvar config = parse(url);\nconfig.server = config.host;\nvar db = pgp(config);\ndb.any('SELECT count(*) FROM cvrm.clients')\n  .then(function(data) { console.log('url:', url, 'config:', JSON.stringify(config), 'data:', JSON.stringify(data)); })\n  .catch(function(error) { console.log('url:', url, 'config:', JSON.stringify(config), 'error:', error); })\n```\nwhen run as\nfor u in postgres://emile:emile@localhost/emile postgres://invalid:emile@localhost/emile postgres://emile:emile@pgsvr-cvrmtoolbox.postgres.database.azure.com/emile; do\n  node testpg.js $u\ndone\nshows\nurl: postgres://emile:emile@localhost/emile config: {\"port\":null,\"host\":\"localhost\",\"database\":\"emile\",\"user\":\"emile\",\"password\":\"emile\",\"server\":\"localhost\"} data: [{\"count\":\"10000\"}]\nurl: postgres://invalid:emile@localhost/emile config: {\"port\":null,\"host\":\"localhost\",\"database\":\"emile\",\"user\":\"invalid\",\"password\":\"emile\",\"server\":\"localhost\"} error: { error: password authentication failed for user \"invalid\"\n    at Connection.parseE (/home/emile/github/cvrm/node_modules/pg-promise/node_modules/pg/lib/connection.js:539:11)\n    at Connection.parseMessage (/home/emile/github/cvrm/node_modules/pg-promise/node_modules/pg/lib/connection.js:366:17)\n    at Socket.<anonymous> (/home/emile/github/cvrm/node_modules/pg-promise/node_modules/pg/lib/connection.js:105:22)\n    at emitOne (events.js:96:13)\n    at Socket.emit (events.js:191:7)\n    at readableAddChunk (_stream_readable.js:178:18)\n    at Socket.Readable.push (_stream_readable.js:136:10)\n    at TCP.onread (net.js:561:20)\n  name: 'error',\n  length: 103,\n  severity: 'FATAL',\n  code: '28P01',\n  detail: undefined,\n  hint: undefined,\n  position: undefined,\n  internalPosition: undefined,\n  internalQuery: undefined,\n  where: undefined,\n  schema: undefined,\n  table: undefined,\n  column: undefined,\n  dataType: undefined,\n  constraint: undefined,\n  file: 'auth.c',\n  line: '307',\n  routine: 'auth_failed' }\nurl: postgres://emile:emile@pgsvr-cvrmtoolbox.postgres.database.azure.com/emile config: {\"port\":null,\"host\":\"pgsvr-cvrmtoolbox.postgres.database.azure.com\",\"database\":\"emile\",\"user\":\"emile\",\"password\":\"emile\",\"server\":\"pgsvr-cvrmtoolbox.postgres.database.azure.com\"} error: { error: Invalid Username specified. Please check the Username and retry connection. The Username should be in <user name@hostname> format.\n    at Connection.parseE (/home/emile/github/cvrm/node_modules/pg-promise/node_modules/pg/lib/connection.js:539:11)\n    at Connection.parseMessage (/home/emile/github/cvrm/node_modules/pg-promise/node_modules/pg/lib/connection.js:366:17)\n    at Socket.<anonymous> (/home/emile/github/cvrm/node_modules/pg-promise/node_modules/pg/lib/connection.js:105:22)\n    at emitOne (events.js:96:13)\n    at Socket.emit (events.js:191:7)\n    at readableAddChunk (_stream_readable.js:178:18)\n    at Socket.Readable.push (_stream_readable.js:136:10)\n    at TCP.onread (net.js:561:20)\n  name: 'error',\n  length: 187,\n  severity: 'FATAL',\n  code: '28000',\n  detail: undefined,\n  hint: undefined,\n  position: undefined,\n  internalPosition: undefined,\n  internalQuery: undefined,\n  where: undefined,\n  schema: undefined,\n  table: undefined,\n  column: undefined,\n  dataType: undefined,\n  constraint: undefined,\n  file: 'auth.c',\n  line: '481',\n  routine: 'ClientAuthentication' }\nIs pg-promise passing on an error from a lower library (and if so, which)? I don't understand why one call gets me an expected \"password authentication failed\", and the other an \"Invalid Username specified\". Are those specific errors generated outside pg-promise?. OK, super, I've confirmed the error with the underlying library, I'll post the error there.. Can I get the underlying node-postgres driver connection object from pg-promise?. Oops sorry, I see it now, it's pgp.pg. Still testing the azure problem. It's azure-specific, but all my scripts use pg-promise, so I'd like to keep them as-is where I can but debug by looking lower in the stack when I need to.. Right on the former, thanks on the latter!. ",
    "aahoughton": "Back, but I don't think this is a pg-promise issue.  I worry that I won't get much input from the node-postgres side due to the old version.\nAfter upgrading pg-promise, and with some help from longjohn, this appears to be some variant of https://github.com/brianc/node-postgres/issues/314.  The error shows after 5s (i.e., once around the block) from this code:\n```\nrequire('longjohn');\nconst pg = require('pg');\nconst DSN = 'postgres://my_uid:my_pwd@my_hosted_azure_pg_instance:5432/test?ssl=true';\nfunction status(cb) {\n    var start = Date.now();\n    var client = new pg.Client(DSN);\n    client.connect(function(err) {\n        if (err) {\n            return cb(err);\n        }\n    client.query(\"select 1 as pong\", function(err, results) {\n        client.end();\n\n        if (err) {\n            return cb(err);\n        }\n\n        if (!results.rows || results.rows.length !== 1 || results.rows[0].pong !== 1) {\n            return cb(new Error(\"expected ping of 1\"));\n        }\n\n        return cb(null, Date.now() - start);\n    });\n});\n\n}\nsetInterval(function() {\n    status(function(err, latency) {\n        if (err) {\n            console.error(err);\n        } else {\n            console.log(latency);\n        }\n    });\n}, 5000);\n```. Oh, also:\n```\nError: read ECONNRESET\n    at exports._errnoException (util.js:1022:11)\n    at TLSWrap.onread (net.js:569:26)\n\nat Client.connect (/home/andrew/tmp/node_modules/pg/lib/client.js:167:7)\nat status (/home/andrew/tmp/test-unpooled-pg.js:9:12)\nat Timeout.<anonymous> (/home/andrew/tmp/test-unpooled-pg.js:31:5)\nat ontimeout (timers.js:365:14)\nat tryOnTimeout (timers.js:237:5)\nat Timer.listOnTimeout (timers.js:207:5)\n\n. .. and the same issue with the latest pg driver:\nError: read ECONNRESET\n    at exports._errnoException (util.js:1022:11)\n    at TLSWrap.onread (net.js:569:26)\n\nat Client.connect (/home/andrew/tmp/node_modules/pg/lib/client.js:184:7)\nat status (/home/andrew/tmp/test-unpooled-pg.js:9:12)\n\n```. Still want me to try with latest pg-promise?  It's pretty clear this isn't a pg-promise issue. :). Yes, but this use-case is exactly where I do want to use a single connection -- I need to verify the DB is up for a regular health check, and I don't want to get stuck behind other queries in a pool.  I haven't looked at the latest pg code -- there may be a way to force a connection from a pool even if the pool is considered full (which would be incredibly dangerous, clearly).. You can try to convince me I want something other than what I think I want, I suppose, but right now I think I want this:\n1. I have multiple API servers, in multiple locations.  Any one of them could suffer a net split or something equivalently problematic at any time.\n2. Each API server is checked by the load balancer to verify it's working.  To support that, each API server periodically checks to verify that PG -- among other data stores -- is available.\n3. This isn't just at startup\n4. I only care that I can connect and issue queries -- I really don't want to be stuck behind a whole bunch of long-running queries that are filling up the queue\n.. and finally, there might be work-arounds, but this does seem like a bug (and, obviously, not in your code).\nAnyway -- I'll close this, and I'll file a variant with node-postgres.\nThank you!. ",
    "juanpaco": "EDIT:  Derp.  I found more documentation, and saw that I'm wrong with how to use this.  One doeesn't invoke the require value to connect.  One invokes that with some options and then uses the return value of that to connect.  My bad, and sorry for the noise!\nSorry to comment on an old issue, if I'm missing something or you'd like me to open a different issue, I'm happy to do that.\nI'm trying to use a connection string like in the last comment, but I get an error.  Here is the attempt to make the connection:\n// Verified that env.postgresConnectionString equals ' postgres://user:password@localhost:5432/message_store'\nconst db = pgPromise(env.postgresConnectionString)\nThis gives me the error:\nTypeError: Invalid initialization options: \"postgres://user:password@localhost:5432/message_store\"\n    at $main (node_modules/pg-promise/lib/main.js:194:19)\n    at Object.<anonymous> (src/index.js:18:12)\n    at Module._compile (internal/modules/cjs/loader.js:689:30)\n    at Object.Module._extensions..js (internal/modules/cjs/loader.js:700:10)\n    at Module.load (internal/modules/cjs/loader.js:599:32)\n    at tryModuleLoad (internal/modules/cjs/loader.js:538:12)\n    at Function.Module._load (internal/modules/cjs/loader.js:530:3)\n    at Function.Module.runMain (internal/modules/cjs/loader.js:742:12)\n    at startup (internal/bootstrap/node.js:266:19)\n    at bootstrapNodeJSCore (internal/bootstrap/node.js:596:3)\nWhen I try:\nconst db = pgPromise({ connectionString: env.postgresConnectionString })\nI get the following error:\nWARNING: Invalid property 'connectionString' in initialization options.\nAm I using the connection string correctly, or was this functionality removed?  Looking at the code, I see that at https://github.com/vitaly-t/pg-promise/blob/master/lib/main.js#L193 it only wants an object for the connection information and then at https://github.com/vitaly-t/pg-promise/blob/master/lib/main.js#L198 connectionString is not listed as a valid option.\nThanks for the library!\n. ",
    "phips28": "I am using 6.1.1 (installed it some hours ago)\nI reinstalled everything on my server and it seems to work now. Will text you if it appears again!\nthank you for now.. Now I found a re-produceable way to get the Warning.\nedit: I guess new relic is throwing this warning. afaik it injects/listen all DB queries?\nI have a initialize file for the pg-promise database:\n/model/pg/index.js\n```javascript\nconst Promise = require('bluebird'); // eslint-disable-line\nconst initOptions = {\n  promiseLib: Promise,\n  query: function query(e) {\n    console.log('----------------------------------\\n', e.query);\n  },\n  error: function error(err, e) {\n    if (global.dvel.config.production) {\n      return; // do not log it in production, you should catch the errors\n    }\n    if (e.cn) {\n      // A connection-related error;\n      // Connections are reported back with the password hashed,\n      // for safe errors logging, without exposing passwords.\n      console.error('CN:', e.cn);\n    }\n    if (e.query) {\n      // query string is available\n      console.error('query string: ', e.query.substr(0, 1000));\n      if (e.params) {\n        // query parameters are available\n        console.error('query params: ', e.params);\n      }\n    }\n    if (e.ctx) {\n      // occurred inside a task or transaction\n      console.error('CTX: ', JSON.stringify(e.ctx));\n    }\n    console.error('pg error:', err.message || err);\n  },\n};\nexports.pgp = require('pg-promise')(initOptions); // eslint-disable-line\n// parse numeric, float as floats: https://github.com/vitaly-t/pg-promise/issues/128\nexports.pgp.pg.types.setTypeParser(1700, (value) => {\n  return parseFloat(value);\n});\nexports.db = exports.pgp(global.dvel.config.postgres);\nexports.connect = function connect() {\n  console.log('connect...');\n  return exports.db.one('SELECT version()'); // <-- it fails here if I execute the connect() function from a route\n};\n```\nIn another file I have a /status route (express) to check the database status:\n/routes/ApiStatus.js\n```javascript\nconst model = require('../model');\nRouter.route('/status').get((req, res) => {\n    model.pg.connect()\n      .then(() => {\n        return util.Response.sendOK(req, res); // OK\n      })\n      .catch(() => {\n        return util.Response.sendError(req, res); // FAIL\n      });\n  });\n```\nWhen I curl this route curl localhost/status it throws the warning. If I remove the model.pg.connect() in the route, it doesnt select and therefore no error.\nI tried to execute the connect() function in a different way, I just added the following script in the ApiStatus.js file:\njavascript\nsetTimeout(() => {\n  model.pg.db.one('SELECT version()'); // works\n  model.pg.connect(); // works\n}, 2000);\nboth queries gets executed 2sec after node startup. => no warning!?\nIs there something wrong with express? \nOr maybe new relic? afaik it injects/listen all DB queries? \ud83e\udd14. It is new relic. When I remove the line require('newrelic'); no warnings popping up. . yes, sry for bothering you ;)\nI will get in touch with newrelic.. ",
    "olalonde": "@elmigranto @vitaly-t thanks, that solves the example I mentioned but it would still be useful to have unref(). To clarify what I meant, see the unref methods in the Node.js API docs:\n\ntimeout.unref()\nserver.unref()\nsocket.unref()\n\nThe general idea is that by \"unreferencing\" some object, Node.js will terminate the process even if the \"unrefed\" object is still waiting/listening for I/O.\nThis would probably be achieved in pg-promise by calling .unref() on the postgres socket(s).\nFor another example, see the redis package's unref method documentation and implementation.. I actually implemented something like unref() for the pg client in one of my libraries: https://github.com/binded/advisory-lock/blob/master/src/index.js#L20-L53\nIt made sure that it would only unref if there weren't any pending queries. This implementation probably doesn't support all use cases such as streaming or PGNOTIFY.\nIn hindsight, it may be too tricky to implement this in a fool proof way.. @vitaly-t I realize this feature isn't implemented, it was more of a request. I think it would be possible to implement but perhaps it would be complex.. ",
    "anonrig": "javascript\ndb.one('SELECT * FROM users WHERE id = $1', 1)\nShouldn't this line be like at the bottom?\njavascript\ndb.one('SELECT * FROM users WHERE id = $1', [1]). The second parameter of the function db.one can it be an integer or should it be an array, in order to parse $1 to its corresponding value?. Did you try?\njavascript\ndb.connect().catch(error => console.error(`Connection error ${error.message}`);\nAdditionally, you can add options to pgp using -> require('pg-promise')(options). You can find it from the documentation.. ",
    "kmarchenko": "version: 6.2.1\nexport function getById(id) {\n    return db.one('SELECT * FROM users WHERE id = $1', id)\n}. @vitaly-t \ndb.connect().then(() => {\n  console.log('[INFO] Successful database connection!')\n  db.one('SELECT * FROM users WHERE id = $1', 123).then(console.log).catch(error => console.log(error.code))\n}).catch((error) => {\n  console.log('[DB ERROR]: ', error.message || error)\n})\n\n[INFO] Successful database connection!\nnode_modules/pg-promise/lib/errors/queryResult.js:151\n            gap1 + 'code: queryResultErrorCode.' + errorMessages[this.code].name,\n                                                                           ^\nTypeError: Cannot read property 'name' of undefined\n    at QueryResultError.toString (node_modules/pg-promise/lib/errors/queryResult.js:151:76)\n    at Function.prepareStackTrace (node_modules/source-map-support/source-map-support.js:372:16)\n    at Function.captureStackTrace ()\n    at new QueryResultError (node_modules/pg-promise/lib/errors/queryResult.js:123:15)\n    at Result.ctx.db.client.query (node_modules/pg-promise/lib/query.js:164:45)\n    at Result.Query.handleReadyForQuery (node_modules/pg/lib/query.js:144:10)\n    at Connection. (node_modules/pg/lib/client.js:180:19)\n    at emitOne (events.js:101:20)\n    at Connection.emit (events.js:189:7)\n    at Socket. (node_modules/pg/lib/connection.js:133:12)\nerror Command failed with exit code 1.. @vitaly-t I'm downgraded pg-promise version to 5.9.0, and all is fine. I have no idea too, when I tried to debug vars in QueryResultError, I just get undefined on all vars.. @vitaly-t \nnode version: v7.6.0\nWith v6.0.24 all works fine, with v6.0.25 crashes.\n\nconst error = new pgp.errors.QueryResultError(0, {rows: []}, 'select', 1);\nconsole.log(error);\n\nv6.0.24\nQueryResultError {\n    code: queryResultErrorCode.noData\n    message: \"No data returned from the query.\"\n    received: 0\n    query: \"select\"\n    values: 1\n}\nv6.0.25 error\nnode_modules/pg-promise/lib/errors/queryResult.js:151\n            gap1 + 'code: queryResultErrorCode.' + errorMessages[this.code].name,\nTypeError: Cannot read property 'name' of undefined\n\nWith error-test branch all works fine.. @vitaly-t Yes, I tried it and it work.. @vitaly-t Updated, but unfortunately old error returns back. . @vitaly-t Okay, I'll let you know. Thank!. @vitaly-t  Everything works. Thank you.. ",
    "redben": "Just a note if anyone else still sees similar errors (version: 6.3.1)\nYou get the error bellow if you are using bluebird somewhere in the chain and have not set pg-promise to use bluebrid. At least in my case.\nTypeError: Cannot set property 'name' of undefined\n      at QueryResultError (node_modules/pg-promise/lib/errors/queryResult.js:122:27)\n      at process._tickCallback (internal/process/next_tick.js:109:7). ",
    "jckkong": "great. thank you!. ",
    "brianc": "It only returns an array of results if the query is a multi-statement simple text query such as:\nclient.query('SELECT * FROM foo; SELECT * FROM baz;')\nFor the 99% percent of the time that a single query is sent to client.query the behavior remains the same. The change also doesn't impact prepared statements at all because postgres doesn't allow more than 1 prepared statement to be executed at a time.  If you want to revert to the old behavior you could do something like...\njs\nvar query = client.query\nclient.query = function(text, values) {\n  return query.call(client, text, values).then(result => {\n    if (!Array.isArray(result)) {\n      // return the same thing as it used to return\n      // this is the \"normal\" case\n      return result\n    }\n    const rows = _.flapMap(result, 'rows')\n    const fields = _.flapMap(result, 'fields')\n    const finalResult = result.pop()\n    finalResult.fields = fields\n    finalResult.rows = rows\n    return finalResult\n  })\n}\nAs an aside: I don't recommend pipelining multiple queries in a single query text unless you have a specific edge type of case.  You miss out on parameterized queries and other goodies and its sometimes harder to reason about a query error if 1 of N queries in your bulk of queries fails.  I realize it can be helpful to do in certain situations though & the change in 7.0 was to make it more understandable when you received a result so it wasn't all collapsed into a single big array.  Sorry it introduced some headache!\nHope this helps! :heart: . ",
    "Juanflugel": "Hi Vitaly, Thanks a lot for soon and good answer, casting the data solved the problem.  I also want to say that this library you wrote is just Great. this is my first experience with SQL (Postgres) before I worked with MongoDB but now I need to Change to a Relational DB.  I have been using pg-promise for the last two months and it is so simple to use, so well documented  and you are always there to give support, I just wanted to say thanks for your remarkable work.\nIf one day I can help you in any way, Just ask for.\nGreetings From Germany\nJuan.. ",
    "Atahualkpa": "i resolved. ",
    "MadsRC": "Thank you very much vitaly, that did it! (Had to rename the function to lowercase or encase createUser in double-quotes though, but that's a minor issue).. ",
    "JonathanGonczoroski": "It Worked! Thank you! . ",
    "Brizak": "That does not work.. Ok, i will do that. \nSorry for the inconvenience and thank you for your quick replies !\n. ",
    "keerthivasan-r": "Yeap, seems like. It's not getting into any type checking of the query pass through (QueryFile, ExternalQuery etc) Its typeof query === object returns true. The code that actually loads the external sql file into QueryFile\n```\nimport { QueryFile, TQueryFileOptions } from 'pg-promise';\nconst path = require('path');\nexport class PostgresMapperModel {\npublic static queryFiles : {[key : string] : QueryFile} = {};\npublic get Registry() {\n    return {\n      TransactionRequest: {\n        insert: sql('../../sqls/insert-transaction-request.sql'),\n      },\n      TransactionResponse: {\n        insert: sql('../../sqls/insert-transaction-response.sql'),\n      },\n    };\n  }\n}\nfunction sql(file: string): QueryFile {\n  const fullPath: string = path.join(__dirname, file); \n  const options: TQueryFileOptions = {\n    minify: true,\n  };\n  let query = null;\n  if (PostgresMapperModel.queryFiles.filePath == null) {\n    query = new QueryFile(fullPath, { debug : true, minify: true });\n    PostgresMapperModel.queryFiles.filePath = query;\n  }else {\n    query = PostgresMapperModel.queryFiles.filePath;\n  }\n  return query;\n}\n```. Yeap, I'm doing the same. I will keep the post with updates. Thanks for the quick reply.. This is where the problem is, it didn't pass the query instanceof $npm.QueryFile test\nif (!error && typeof query === 'object') {\n    if (query instanceof $npm.QueryFile) {\n        query.prepare();\n        if (query.error) {\n            error = query.error;\n            query = query.file;\n        } else {\n            query = query.query;\n        }\n    }. This is not a problem or issue with pg-promise.  I also guess that loading the pg-promise in different projects might get this trouble. I have a difference npm package that uses pg-promise. Please shed your thoughts, if that may create issue with QueryFile inheritance.\n\nI think this is where, it doesn't get through.\nif (!error && typeof query === 'object') {\n        if (query instanceof $npm.QueryFile) {\n            query.prepare();\n            if (query.error) {\n                error = query.error;\n                query = query.file;\n            } else {\n                query = query.query;\n            }\n        }\nThe other thing that i see is \nObject.getPrototypeOf(query) returns QueryFile  object and Object.getPrototypeOf($npm.QueryFile) returns function(){}.. I'm using the version -  pg-promise\": \"^5.9.7. Thanks for your time and guidance\n. Nope, It's hard for me to identify the root cause, the same external query file is working in another project. Hi, I'm just reopening this issue since the constructor of __proto__  property of both query variable (my loaded queryfile) and npm.QueryFile of pg-promise are equal. I guess if that should be added on top of checking the instanceof test like so\nif (!error && typeof query === 'object') {\n        if (query instanceof npm.QueryFile ||  query.__proto__.constructor == npm.QueryFile.__proto__.constructor) {\n            query.prepare();\nI guess the instanceof is actually failing in my case due to some loading order or so. Please don't think rude of me that i'm reopening the issue again and again,. Please let me know if i am understanding it wrongly.\n       . ok, i understand. Let me try upgrading the node version. It might be a bug in node version 6.10.0. Upgrading nodejs 7.10.0 also didn't work. I'm facing this issue still. The only difference i see that i am trying to add my package using npm link to the project and then running the application. Do you think that makes something different than the regular npm install. Sure, I will let you know. Its a huge project, may be i should create a demonstrating example and share with you?. I was able to resolve it. I had two versions of pg-promise. Both ^6.5.1 and ^5.6.7. The npm package was using the older version. When i changed both to ^6.5.1, it worked just awesome. I'm not sure why does this create an issues. Do we need to maintain only single version of pg-promise in our application? some of the packages we use use has their own pg-promise version. we can't prevent it. What's your view? Please help me understand.. yeah, you are right! Thanks for the help.. ",
    "Tobbeman": "I thought that captilizing could be the issue, have tried with both small and big I in Id to little avail.\nThe sql is atleast defined as: \"product_bundleId INT REFERENCES product_bundles(id) ON DELETE CASCADE\".\nAnd like I said, using the same query without quotation marks worked fine, does that not imply that the quotation marks can be the source of the issue? . Ahh, I thought closed issues should not be responded to!\nYes you were right, I did try to use small letters when creating the columns but still had big in the values. Using small on all places fixed my issues! Thank you for time. ",
    "ashishjain14": "Thanks, that helps to run the query. However my user authentication fails for the created new user. If I write the same query as below it works with out any issue.\n'CREATE USER '+ username + ' WITH PASSWORD ' + \"'\"+password+\"'\"; \n. Super awesome, it works now thanks a lot.. ",
    "cryptcoin-junkey": "Hmm... I'm using v6.3.8 newer than v5.9.5 (described on #354).\nSurely it is older than the newest. I'm going to upgrade it.. I got same errors even tough I upgraded it to pg-promise@6.5.1.. An additional issue possibly related this.\nAs you know, the default pool size on node-postgres is 10.\nMy PostgreSQL instance (actually on AzureDB) is assigned 50 connects.\nIt shouldn't be exhausted all connections if there is no bugs.\nJust guess but I suspect there have some inconsistency between pg-pool and pg-promise...\n. Thank you for your quick response. I tried 6.5.2. But it seems no effect.\nI'll inspect it deeper today.. I found this : https://github.com/brianc/node-postgres/issues/1344\nIt looks node-postgres has some issues on using with AzureDB.\n(also #354 was on AzureDB.)\nI close this since there is less sense to analysis in situations where there have bugs in the dependent library.\nI'll recheck this issue after brianc/node-postgres#1334 was resolved.\nThank you for your kind support.. My mistake. Not brianc/node-postgres#1334 but brianc/node-postgres#1344.. I see. I agree there are less reason to fix now.. ",
    "ubreddy": "upgrade to 6.5.2 doesnt fix the issue.  any final solution for this issue?. @vitaly-t\nthe driver is still not updated.. There is a pull request pending.\nmeanwhile to avoid this crash is there a workaround?. making the pg point to this version resolves the issue..\n\"pg\": \"pasieronen/node-postgres#ssl-close-econnreset\",\nthis pull request is pending.\n    . @vitaly-t  can you update the dependency to 7.4.1  ? the driver is updated with the pull request.. ",
    "Inkata": "I did got it working by escaping the backslash, but it took me a while.\nMaybe a brief note about naked backslash in the docs?\nThanks again.. ",
    "btd": "\nThere are two ways to provide for an empty array of values:\nCheck for the array length, and not execute the query that will knowingly produce no result, see #391\nUse type casing ::text[] that will always inform the server of the type before parsing the array.\n\nThis both 2 options do not work for my case: first of all empty array perfectly valid value in my schema, and i do not use pg-promise directly, but via massivejs. I got this issue right after update to massive 3.x.\n//ping @dmfay to make you know about another breaking changes.\n\nHow do you figure that? What tests am I missing?\n\nYou know your formatting works because of you read docs and tried to use it manually in code. But you can know that it is working, by doing real insert and updates in your tests. . It is simply reproduced with something like this in massive:\n```js\n//assume connected\ndb.table.update({ array_column: [] }, { id: 1});\n```. Thanks, good to know, i will try. Yep, this is working, a bit inconvinient but as workaround it is ok.. ",
    "pdanpdan": "Thank you. Should I open an issue there?. It works as advertised, thank you very much.. ",
    "ianstormtaylor": "Thanks for the quick reply!\nI'm not sure I understand why it's not possible, since it seems like instead of doing:\njs\nif (v.name in obj) {\n  return formatValue(obj[v.name], v.fm, obj);\n}\nYou could do:\njs\nif (has(obj, v.name)) {\n  return formatValue(get(obj, v.name), v.fm, obj);\n}\nAm I missing something?. What do you mean by requires evaluation? Are you talking about the need to recurse into the object to read the nested paths? Or something for the Postgres pre-evaluated queries?\nAs far as I can tell, obj[v.name] vs get(obj, v.name) is a similar amount of \"evaluation\" if you consider that nested parameters are just constructs, and not the values themselves.\nAs for performance, do you mean the performance of obj[v.name] vs. get(obj, v.name)? That seems like not a huge problem to me because people that are needing to use nested values are already having to resort to hacks around that (like flattening) that probably incur similar performance loss. Having the library handle it natively would greatly reduce the code complexity around some of these queries.\nFor the value transforms, you could still do the same thing user.name:raw or whatever you wanted, and I think the code as architected would already support this.\nI guess I'm missing something deeper...\n. Fair enough, in that case I'll close this.\nFor what it's worth, I use pg-promise for everything I do with Postgres, and I really like it. But the helpers/formatting part of it I have to look up in the docs every time I use them, because they're really unintuitive. Trying to remember ^ or ~ or how this is special-cased, and trying to remember the syntax for insert or ColumnSet or what the import name is. And some of the stuff is in the \"official\" documentation, and some is in the readmes, and some is in the wiki. It's the thing that ends up frustrating me the most when using this lib.. Thanks for your work.\nI've decided to use a different solution that involves a separate formatting library, and to migrate to node-postgres instead since I realized it supports promises now and I'm not gaining that much from pg-promise at this point.. ",
    "jdalton": "@vitaly-t Naw, you gotta give me time to reply before jumping to conclusions \ud83d\ude38 . >  well, I work on GitHub projects full time, and fast.\nI take it a bit more easy. In this case you caught me as I was trying to type a nice detailed reply, which I junked to deal with the false start. Anyway, no need to follow up in the issue. As mentioned, your implementation doesn't align with Lodash's expected behavior.\n. ",
    "devmtis": "Thanks for your answers !\n@demurgos Unfortunately I'm not using the \"JSON\" column type. I'm using the function ST_AsGeoJSON() from PostGIS in my request to convert a geometry binary column to a GeoJSON String.\n@vitaly-t I'll look into that, thanks !\n. A huge thank you for your comment @demurgos ! It made me realise that if I cast the result of \"ST_AsGeoJSON()\" like this : SELECT ST_AsGeoJSON(geom)::json FROM ..., pg-promise understands that it's a JSON String and parse it, exactly like I wanted ! Perfect !. ",
    "mbechchi": "thank you for your reply.\nThe functionality isn't restored (Even after 30 munites, the 20 entries are still there and the api is KO).\nI think that when the pool limit is reached (10 pools per database), the connection is no longer released and new connections fail but is that normal that I have no error even using pg-monitor.\nbelow some catch of my code to understand what I do.\nserver.js\n```ruby\nconst restify       = require('restify'),\n      Promise       = require('bluebird'),\n      options       = { promiseLib: Promise, capSQL: true},\n      pgp           = require('pg-promise')(options),\n      db            = require('./app/controllers/db'),\n      monitor       = require('pg-monitor')\nmonitor.attach(options)\nglobal.physical_shards_connections = {}\nglobal.logical_shards_params = []\nlet types = pgp.pg.types;\ntypes.setTypeParser(1114, function(str) {\n    return moment.utc(str).format(\"YYYY-MM-DD HH:mm:ss\")\n})\nglobal.server = restify.createServer({\n  name    : config.name,\n  version : config.version,\n  log     : bunyanWinston.createAdapter(log),\n})\nserver.listen(config.port, function() {\n     let promesses = [db.set_sharding(pgp), sns.set_sns()];\n     Promise.all(promesses).then(function (data) {\nphysical_shards_connections = data[0].physical_shards_connections\nlogical_shards_params = data[0].logical_shards_params\nsns_api_platform_applications = data[1]\n\nrequire('./app/routes');\n\n}).catch(function (err) {\n    pgp.end()\n    log.error(err, err.stack)\n    setTimeout(function(){ process.exit(1); }, 10000)\n  })   \n})\n```\ndb.js\n```ruby\nconst Promise       = require('bluebird'),\n      errors        = require('restify-errors'),\n      _             = require('lodash'),\n      bigInt        = require(\"big-integer\"),\n      moment        = require('moment'),\n      config        = require('../../config-' + process.env.NODE_ENV)\nfunction set_sharding(pgp) {\n  return Promise.coroutine(function*() {\n    let output = {}\n    output.physical_shards_connections = {}\n    output.logical_shards_params = []\n    let confs = []\n    .forEach(config.sharding.physical_shards, function(physical_shard) {\n      output.physical_shards_connections[physical_shard.physical_shard_id] = pgp(physical_shard.hostname)\n      confs.push(\n        output.physical_shards_connections[physical_shard.physical_shard_id].one(\n          'SELECT \\\n            conf, \\\n            $1::varchar as physical_shard_id, \\\n            $2::float as physical_shard_weight \\\n            FROM public.sharding \\\n          WHERE \\\n            status = 1',\n          [\n            physical_shard.physical_shard_id,\n            physical_shard.weight\n          ]\n        )\n      )\n    })\n    let results = yield Promise.all(confs)\n    .forEach(results, function(result) {\n      let arrtmp = .map(result.conf, function(element) {\n        return .extend({}, element, {\n          \"physical_shard_id\": result.physical_shard_id,\n          \"physical_shard_weight\": result.physical_shard_weight\n        })\n      })\n      output.logical_shards_params = _.union(output.logical_shards_params, arrtmp)\n    })\n    return output\n  })()\n}\nfunction db_health() {\n  return Promise.coroutine(function*() {\n    let confs = []\n    _.forEach(config.sharding.physical_shards, function(physical_shard) {\n      confs.push(\n        physical_shards_connections[physical_shard.physical_shard_id].one(\n          'SELECT \\\n            $1::varchar as physical_shard_id',\n          [\n            physical_shard.physical_shard_id\n          ]\n        )\n      )\n    })\n    let results = yield Promise.all(confs)\n    return results\n  })()\n}\nfunction create_user(physical_shard_id, user, metricdata) {\n  return new Promise(function(resolve, reject) {\n    let sharding =  get_stat_sharding()\n    physical_shards_connections[physical_shard_id].tx(function () {\n      return this.batch([\n        this.func('register_mobile_user', [user]),\n        physical_shards_connections[sharding.physical_shard_id].func('update_metrics', [JSON.stringify(metricdata)])\n      ])\n    })\n    .then(function (data) {\n      resolve(data)\n    }, function (reason) {\n      reject(reason)\n    })\n  })\n}\nmodule.exports = {\n  set_sharding: set_sharding,\n  db_health: db_health,\n  create_user: create_user\n}\n```\nconfig-dev.js\nruby\nmodule.exports = {\n    \"name\": \"API\",\n    \"version\": \"2.0.0\",\n    \"env\": process.env.NODE_ENV,\n    \"port\": 3000,\n    \"sharding\": {\n        \"physical_shards\": [\n          {\n            \"physical_shard_id\": \"DBS1\",\n            \"hostname\": \"postgres://postgres:postgres@192.168.68.8:5432/db_1\",\n            \"weight\": 0.5\n          },\n          {\n            \"physical_shard_id\": \"DBS2\",\n            \"hostname\": \"postgres://postgres:postgres@192.168.68.8:5434/db_2\",\n            \"weight\": 0.5\n          }\n        ],\n        \"factor\" : 10\n    }\n}\nroute.js\n\nruby\n....\n    let create_user_results = yield db.create_user(\n      sharding.physical_shard_id,\n      user,\n      metricdata\n    )\n.... I understood my error after sending the second message.\nSo if I modify the function create_user as below I no longer have the problem.\nruby\nfunction create_user(physical_shard_id, user, metricdata) {\n  return new Promise(function(resolve, reject) {\n    physical_shards_connections[physical_shard_id].tx(function () {\n      return this.batch([\n        this.func('register_mobile_user', [user]),\n        this.func('update_metrics', [JSON.stringify(metricdata)])\n      ])\n    })\n    .then(function (data) {\n      resolve(data)\n    }, function (reason) {\n      reject(reason)\n    })\n  })\n}\nMy problem is that I have at least two databases and a new user is randomly created in one of them (Sharding) and for simplicity I store stats in one database.\nFor stats it's not a problem, I can do it differently. But in some functions, I have to do a transaction between two databases and I use the same bad method as below : query 1 in DB1 and query 2 in DB2.\n```ruby\nfunction modify_relationship(user_id, relationship_data, second_user_id) {\n  return new Promise(function(resolve, reject) {\n    let sharding_user =  get_user_sharding(user_id)\n    let result = {}\nphysical_shards_connections[sharding_user.physical_shard_id].tx(function () {\n  return this.sequence(function (idx) {\n    switch (idx) {\n      case 0:\n        return this.func('modify_relationship', [relationship_data, user_id, user_id, sharding_user.logical_shard_id])\n        .then(res => {\n          if ( res[0] && res[0].result ) {\n            result.relationship = res[0].result\n            return Promise.resolve(result)\n          } else {\n            return Promise.reject(\n              new errors.APIErrorRDS({\n                message: 'Error when adding relationship to the shard of the user 1.',\n                statusCode: 400,\n                restCode: 'User1RelationshipException'\n              })\n            )\n          }\n        })\n      case 1:\n        if ( second_user_id ) {\n          let sharding_second_user =  get_user_sharding(second_user_id)\n          return physical_shards_connections[sharding_second_user.physical_shard_id].func('modify_relationship', [relationship_data, user_id, second_user_id, sharding_second_user.logical_shard_id])\n          .then(res => {\n            if ( res[0] && res[0].result ) {\n              result.second_user_relationship = res[0].result\n              return Promise.resolve(result)\n            } else {\n              return Promise.reject(\n                new errors.APIErrorRDS({\n                  message: 'Error when adding relationship to the shard of the user 2.',\n                  statusCode: 400,\n                  restCode: 'User2RelationshipException'\n                })\n              )\n            }\n          })\n          } else {\n            return Promise.resolve(result)\n          }\n    }\n  })\n})\n.then(function (data) {\n  resolve(result)\n}, function (reason) {\n    reject(reason.error)\n})\n\n})\n}\n```\nIt works but I think that I will have the same problem if I do some load tests on this function.\nDo you have a suggestion to do it properly ?\nYou write this \"Your code is full of promise anti-patterns. You are creating promises + do coroutines in places where none needed, which only convolutes the code, making it much harder to read.\", can you give me an example, I use do coroutines in order to be able to use yield inside a function.\nThank you very much.\n. For your information, my first load tests passes if I modify the function as below : \n```ruby\nfunction create_user(physical_shard_id, user, metricdata) {\n  return new Promise(function(resolve, reject) {\n    //Input physical_shard_id -> the database where the new user have to be created (= DB1 on Server1 OR DB2 on Server2)\n    let sharding =  get_stat_sharding() //sharding.physical_shard_id (= DB1 on Server1)\nphysical_shards_connections[physical_shard_id].tx(function () {\n  let arr = []\n  arr.push(this.func('register_mobile_user', [user]))\n  if ( physical_shard_id === sharding.physical_shard_id ) {\n    arr.push(this.func('update_metrics', [JSON.stringify(metricdata)]))\n  } else {\n    arr.push(physical_shards_connections[sharding.physical_shard_id].func('update_metrics', [JSON.stringify(metricdata)]))\n  }\n  return this.batch(arr)\n})\n.then(function (data) {\n  resolve(data)\n}, function (reason) {\n  reject(reason)\n})\n\n})\n}\n```\nThank you very much and if you have an example explaining how to execute a transaction on two databases it will help me.\n. To simplify, suppose I have two databases (DB1 and DB2) hosted on two servers.\nDB1 contains users with id <= 10 000.\nDB2 contains users with id > 10 000.\nI.e., users are shared accross servers based on their id.\nFor example, when user id=100 (in DB1) initiates a relationship with user id=15000 (in DB2), I need to store this relation in DB1 (query 1) and also in DB2 (query 2). \nI also need : \nIf  query 1 fails don't execute query 2;\nIf  query 2 fails roolback query 1;\nif query 2 is ok commit query 1;\nWhat de you propose to acheive this goal ?\nI can do this using : \n(1) the method I propose and you do not like but it works\n(2) stored procedure and dblink_exec \n. This is how (tx and batch) and also (tx and sequence) work.\nTo understand, below is the output of pg-monitor : \nruby\n0|authenti | 16:20:01 connect(postgres@db2)\n0|authenti | 16:20:01 tx/start\n0|authenti | 16:20:01 tx: BEGIN\n0|authenti | 16:20:01 tx: SELECT * FROM register_mobile_user('{\"block_id\":\"block2\",\"region\":\"eu-west-1\",\"country_id\":\"FR\",\"device_id\":\"xxxx\",\"device_platform\":\"ios\",\"jti\":\"da0f1c57-c1e5-4e69-8b59-cd1bb166d012\",\"app\":\"xxx\",\"locale\":\"fr-FR\",\"encrypted_password\":\"xxx\",\"status\":0}')\n0|authenti | 16:20:01 connect(postgres@db1)\n0|authenti | 16:20:01 SELECT * FROM update_metrics('[{\"metric_name\":\"NumberOfRegistredUsers\",\"metric_value\":1,\"dimension_name\":\"YearMonthCountryId\",\"dimension_value\":\"201710FR\"},{\"metric_name\":\"NumberOfRegistredUsers\",\"metric_value\":1,\"dimension_name\":\"CountryId\",\"dimension_value\":\"ALL\"}]')\n0|authenti | 16:20:01 disconnect(postgres@db1)\n0|authenti | 16:20:01 tx: COMMIT\n0|authenti | 16:20:01 tx/end; duration: .020, success: true\n0|authenti | 16:20:01 disconnect(postgres@db2)\nupdate_metrics is executed on an other database and if error, the promise is rejected and the first query of the array passed to batch is rollbacked ...\n. To acheive the following : \nIf query 1 fails don't execute query 2;\nIf query 2 fails roolback query 1;\nif query 2 is ok commit query 1;\nI use tx and sequence as below : \n```ruby\nfunction create_user(physical_shard_id, user, metricdata) {\n  return new Promise(function(resolve, reject) {\n    //Input physical_shard_id -> the database where the new user have to be created (= DB1 on Server1 OR DB2 on Server2)\n    let sharding =  get_stat_sharding() //sharding.physical_shard_id (= DB1 on Server1)\nphysical_shards_connections[physical_shard_id].tx(function () {//open a transaction TX1 (begin) on the database where user have to be created \n  return this.sequence(function (idx) {\n    switch (idx) {\n      case 0:\n        return this.func('register_mobile_user', [user]) //query 1 executed on TX1\n        //if error on case 0, case 1 is not executed and TX1 is Rollbacked\n        //if no error on case 0, execute case 1 and wait before commit the result of case 1\n      case 1:\n        if ( physical_shard_id === sharding.physical_shard_id ) { \n          //user and stat are on the same server\n          return this.func('update_metrics', [JSON.stringify(metricdata)]) //execute query 2 on TX1 \n        } else {\n          //user and stat are on differents servers, execute query 2 using a new connection to DB2\n          return physical_shards_connections[sharding.physical_shard_id].func('update_metrics', [JSON.stringify(metricdata)])\n        }\n        //depending on the result of case 1, TX1 with query 1 is committed or Rollbacked\n        //i.e. :\n        //if error on case 1, TX1 is Rollbacked\n        //if no error on case 1, TX1 is Commited\n    }\n  })\n})\n.then(function (data) {\n  resolve(result)\n}, function (reason) {\n  reject(reason)\n})\n\n})\n}\n```\nUsing tx and batch is not ideal because whith batch the order when executing queries is not guaranteed.\nI hope I understood how tx and sequence work, i.e., the commit or the rollback of the current trx depend on the resolve or the reject of some promise (this promise can be a query on the same trx, a query on an other database, a function not using database but returning a promise).\n. Thank you for taking the time to help me.. ",
    "jtwebman": "conn.$config.version = 6.10.2 from the log output and here are the exact values I am passing as it is a more complex query then the example above:\nThis is how I call it:\njavascript\nreturn conn.manyOrNone(query, queryValues);\nThis is what is in query:\nsql\nWITH partner_members (partner_id, member_id) AS (\n  SELECT DISTINCT\n     p.id,\n     m.id\n  FROM partner AS p\n  INNER JOIN rider_group AS rg ON p.id = rg.partner_id\n  INNER JOIN rider_group_member AS rgm ON rg.id = rgm.rider_group_id\n  INNER JOIN rider_group_member_status AS rgms ON rgm.status = rgms.id\n  INNER JOIN member AS m ON rgm.member_id = m.id\n  WHERE p.agency = $[agency]\n  AND rgms.name = 'active'\n), partner_access (partner_id) AS (\n  SELECT DISTINCT\n    c.partner_id\n  FROM contact AS c\n  WHERE c.auth_user_id = $[authUserId]\n)\nSELECT\n  p.id,\n  p.type,\n  pt.name AS \"typeName\",\n  ps.name AS status,\n  p.name,\n  p.description,\n  p.url,\n  COUNT(pm.member_id)::integer AS \"riderCount\"\nFROM partner p\nINNER JOIN type AS pt ON pt.id = p.type\nINNER JOIN status AS ps ON ps.id = p.status\nLEFT JOIN partner_members AS pm ON p.id = pm.partner_id\nLEFT JOIN partner_access AS pa ON p.id = pa.partner_id\nWHERE p.agency = $[agency]\nAND (1=$[accessToAll] OR pa.partner_id IS NOT NULL)\nGROUP BY p.id, p.type, pt.name, ps.name, p.name, p.description, p.url\nThis is what I am passing in queryValues:\njavascript\n{\n  \"agency\": \"san-diego\",\n  \"authUserID\": 1,\n  \"accessToAll\": 1\n}. Wow, so sorry!. ",
    "MattBz": "Thank you for your detailed answer!\nRegarding adding addtional JOINS and other parts of the query dynamically . Would I have to use \"Raw Text\" then?. Okay, thank you!. ",
    "lloydh": "Sorry if my question was misleading, or perhaps I've been looking at the wrong solutions.\nIn my case there's only 1 database and the goal is to be able to set the logged-in application user's id in some way so that this can be used for an audit trail.. That would only work if I created a new db object for each request with the user id from that request but my understanding is that's horrible for performance.. I hadn't figured out what the myMethodWithParam example was going to do but I believe the author of that post was trying to accomplish the same thing as me.\nI have the user id at the application layer so in the business logic I could insert into the foo_audit table whenever changing foo. This works but will add noise to the business logic and is more prone to errors should another dev update foo without manually updating foo_audit correctly.\nI think I follow your suggestion of using a template but the audit for inserts will need the id returned, so two statements (or a common table expression) are required.\nAfter insert/update trigger functions seem to solve things nicely however the issue is having access to the user id from the application layer \u2014\u00a0and pg-promise is that bridge.\nIf there's not an elegant way to achieve this then I'll go with one of the other options.. Thanks for the help.\nLloyd. ",
    "pavelkucera": "I understand and agree but I think that using them in tests is a valid use case. For example, I am testing that if a pg-promise function raises a QueryResultError, it is properly processed by the rest of the application.. Thank you for the suggestion but it does not really fit my use case - it requires a working database connection in the test and I am trying to avoid that. Or, to be precise, not having a database connection in the test is the sole reason why I am mocking a pg-promise function and throwing an error from it.. Yes, this specific test is a synthetic test. Of course, there are more tests, some of them even have a live database connection. However, these tests can not be run in parallel (not easily), as they work with a shared resource.\nI disagree that a synthetic test does not make sense in a database service. Take a general service logging any database error and then throwing it again. To me, it makes more sense here to use just a synthetic test, which tests that expected errors are processed correctly, rather than connecting to a database. My actual use case is very similar.\nTo be honest, the tested code is not too well designed and I am writing the tests in order to be able to refactor it and therefore, I don't really need the typings in the long term, but I still think they should be exported for such tests. Also for the sake of consistency & accuracy - the error types are exported, looking as if their constructor is a normal error constructor, but it is not.. I do understand the reluctance but I think it overlooks a valid use case. However, if I have not managed to convince you up until now, I don't think I have any other argument that could sway you.\nIf you still find this unacceptable, please feel free to close this PR. Also, thank you for the work on pg-promise :-).. Hi,\nI would like to get back to this, as I unfortunately don't see a way of mocking these errors cleanly with TypeScript and without the added types. If I want to create a custom error extending a pg promise error, with TypeScript, I have to call the parent's constructor. \nWhen doing that, I get to the same place as before - the exported type differs from the actual one. Either I ignore the actual type, but that leads to a runtime error, or I ignore the defined type, but then TypeScript fails to compile the file.\nWould you please re-consider adding the type definitions? Thank you.\nPavel. ",
    "StevePavlin": "Great, I didn't see the extend method in the docs. Thanks for your help @vitaly-t . ",
    "grinnellian": "\ud83d\udc4d  thanks for the great lib. ",
    "msageryd": "Thanks.\nI thought func was neat, but I'm using the SELECT statement instead so I can see what I put in.. I posted my question at SO instead. Closing this.\nhttps://stackoverflow.com/questions/50621964/automatic-query-retry-if-credentials-has-been-auto-rotated\n. ",
    "lifeiscontent": "@vitaly-t thanks, didn't know that.. ",
    "pyramation": "@vitaly-t Calling db.done() will internally call it correctly when it is the right time.\nso, when is the right time? I'm using db.done() and the client is not disconnecting when I need it (essentially after I call db.done(). Sure! thanks for the response ;) I\u2019m building a module to spin up temporary databases and destroy them for testing purposes.\nI made a wrapper for closeing because calling db.client.end() was the only way to achieve a true disconnection with db client:\nexport const close = (db: any): void => {\n  db.done()\n  db.client.end()\n}\nAll I have to do to create errors and non-passing tests, is comment out the line that calls db.client.end():\nhttps://github.com/pyramation/pg-utils/blob/master/src/lib/connection.ts#L16\nWhen I do this, as it\u2019s cleaning up the database, postgres is not too happy because the client is still connected:\nCommand failed: dropdb -U postgres -h localhost -p 5432 testing-db-95ad9251-680c-4d61-b97d-56cb4f828761\n    could not find a \"dropdb\" to execute\n    dropdb: database removal failed: ERROR:  database \"testing-db-95ad9251-680c-4d61-b97d-56cb4f828761\" is being accessed by other users\n    DETAIL:  There is 1 other session using the database.\n. @vitaly-t yes! thank you. Fixed my issue perfectly :) cheers!. ",
    "bsouthga": "yeah woops, I meant QueryFile. The desired usecase is basically wanting to be able to run the same sql script from the command line under psql -f myfile.sql -v1=12 for debugging while also being able to import the script into pg-promise, without having to switch between variable notation (:v1 vs ${v1}).. makes sense, thanks. ",
    "trylovetom": "@vitaly-t  It's useful. Thanks a lot.\nI change _pgp.end() to _client.end().\nAnd test it expect db.$pool.end() is true.\nreference: https://github.com/parse-community/parse-server/pull/4361. _client in parse-server PostgresStorageAdapter.js. ",
    "AmberDean": "Got it.  I was actually using ${foo}:csv. ",
    "xCmac": "Wow. such a silly mistake. banging my head on this for a while.... should've taken a break.\nThanks for the quick response.\nLove the library!. ",
    "Jonke": "After some more tries I got to the conclusion that the example of streaming from \"Learn by example\"  is if not wrong but maybe a bit misleading.\nI'm sure there is some things that says why A work and C don't work in an obvious way but I wouldn't mind a bit more info in the example.\nThis receiver function works\nA\nfunction receiver2(index: number, data: any, delay: number) {\n  console.log(\"RECEIVED:\", index, data.length, delay);\n  return this.sequence((n: number) => {\n    if (n < data.length) return this.point.insert(data[n].point);\n  });\n}\nThis fails (with lost connection)\nB\n```\nfunction receiver2(index: number, data: any, delay: number) {\n  console.log(\"RECEIVED:\", index, data.length, delay);\nreturn this.batch(data.map(x => this.point.insert(x.point)));\n} \n```\nThis fails( with lost connection)\nC\n```\nfunction receiver(_: any, data: any) {\nfunction source(index: number) {\n    if (index < data.length) {\n      return data[index];\n    }\n  }\nfunction dest(index: number, mdata: any) {\nconst p = this.point.insert(mdata.point);\n\n  p.catch((e: any) => {\n      console.error(\"inner-err\", e);\n    });\n\nreturn p;\n\n}\n  return this.sequence(source, { dest });\n}\n```. ",
    "njlr": "I have to admit that I'm a bit confused by the API. \nFor example, this gives the same error: \njavascript=\nawait db.one(INSERT INTO users (username, password_hash) \n      VALUES ($, $) \n      RETURNING id, {\n      username,\n      passwordHash,\n    });\nHowever, it seems to follow the same conventions as this example: https://github.com/vitaly-t/pg-promise/wiki/Learn-by-Example#named-parameters\n. I did not set that flag. \nInterestingly this works: \njavascript\nawait db.one(\n    'INSERT INTO users (username, password_hash) ' +  \n      'VALUES ($<username>, $<passwordHash>) ' +  \n      'RETURNING id', {\n      username,\n      passwordHash,\n    });. Yep, I got the JS syntax wrong. Thanks \ud83d\udc4d . Yes, that was my work-around. It would have been easier to identify the error with a more specific message though. \nIf there are cases in which :csv is valid without any elements then my suggestion will not work. . ",
    "andyzaharia": "Thanks for the fast response!\nI was looking at the code, the problem is I dont have a domain.js file in my project. Searched in node_modules... nothing in there. Which definitely seems really weird \ud83d\ude43, will try to look into it more maybe I find something. \nLearning by debugging... \ud83d\ude0a. ",
    "timReynolds": "@vitaly-t thanks for the quick response. Writing something similar to aws-xray-sdk-node that works with pg-promise is probably my best option. I'll take a look at that as an option, the implementation looks straight forward. . Sorry I've not had a chance to look at this. Yea, I was thinking of using pg-monitor to some extent. I'll come back when I've looked at this more. . ",
    "tsemerad": "Thanks for the quick response. That answers all of my questions. It's good to know that what I was considering trying is not supported by this library, and probably a bad idea anyway. I'll work on speeding up my inserts and see if that's sufficient for my use case. Thanks.. ",
    "t-0-m-1-3": "I'm sorry for the clutter, I did end up fixing it by changing the uname/passwords again.. ",
    "ejoebstl": "Thank you for the excellent answer!. I think detecting string literals might be out of scope. It would mean that this library actually had to parse queries. . The following workaround works: \nimport * as pgp from 'pg-promise'\nconsole.log(pgp.txMode.isolationLevel)\nThe issue and the workaround apply to the TransactionMode class as well.. Got it!\nI would suggest tweaking the typescript definitions in pg-promise.d.ts to avoid confusion. An easy fix would be to replace class declarations by interfaces, or at least remove them from top level. . ",
    "ethanresnick": "I think maybe this should be re-opened? After all, I'm pretty sure it's wrong for pg-promise to identify $450 as a possible parameter anyway, given that it occurs inside a string literal?. @vitaly-t I'm pretty sure this doesn't have to do with double formatting per se. If you just format this query once: INSERT into some_table (\"name\", \"amount\") VALUES ($1, '$10'), with a values array of [\"My Name\"], I believe you'll get an error. Maybe @ejoebstl is right that actually parsing the query is out of scope, but I wonder if this is an attack vector. If the user controls the query text being fed to format, then maybe it's fine, but I wonder if there's some exploit approach I'm not thinking of.. Of course, fwiw, properly parsing the query would also remove the burden on the user to (remember to properly) specify :name etc as the escaping type. Of course, this proper parsing could slow things down considerably.. ",
    "sgronblo": "Another possibility might be to use Promise.resolve on the result of the callback and have the type be:\ntask<T=any>(cb: (t: ITask<Ext> & Ext) => T | PromiseLike<T>): XPromise<T>\nThis way the change wouldn't be breaking. But people who return promises would not get double-wrapped promises either.. Dug around some more. If I understand Task.execute correctly there is a check at the end to see if the returned object is PromiseLike and if so just return it as is. It seems that maybe just changing the type definition would work for most cases. But to make things more correct, perhaps the PromiseLike interface check should just be dropped and we should just call $p.resolve(result) no matter what to make sure that what we get is actually of type XPromise and not PromiseLike.. 1. I was just wondering if it was possible to use both a connection string AND a config at the same time.\npg.TConnectionParameters doesn't seem to contain any property where you could assign the connection string. Instead you provide the host, username, etc directly. But maybe this would be ok to just use pg-connection-string to parse the values.\n\nI didn't notice min and max when I checked earlier. Sorry about that.. Oh one more follow-up question: Can I disable pooling?. Thanks. So the pool will keep track of whether the connection has been disconnected and re-connect it if necessary?. Thanks.. \n",
    "willm": "I think I found a solution doing the following, since this is using the same substitution function that pg-promise uses, I guess it's fairly safe?\n```javascript\nconst pgp = require('pg-promise');\nconst connection = {};\nconst db = pgp(connection);\nconst inserts = [\n  {name: 'Bob', age: 34},\n  {name: 'Alice', age: 25},\n].map(\n  person => pgp.as.format(\n    'INSERT INTO tmp (name, age) VALUES (${name}, ${age})',\n    person\n  );\nconst query = [\n  CREATE TEMP TABLE tmp(\n    name character varying(10),\n    age integer\n  )\n].concat(inserts).join('\\n');\nawait db.none(query);\n```. ",
    "gigiosilva": "Thank you @vitaly-t \nMy solution below:\n`\nlet myMultiInsert = (obj, callback) =>  {\n  const cs = new pgp.helpers.ColumnSet(Object.keys(obj[0]), { table: 'table' });\n\n  let query = pgp.helpers.insert(obj, cs);\n\n  connection.none(query)\n    .then(data => callback(data))\n    .catch(err => {\n      callback();\n      console.log(err);\n    });\n\n}\n`. ",
    "hillbrm": "\nconst sql = pgp.as.format('SELECT table_name FROM information_schema.tables WHERE table_name LIKE $1', ['%string']);\nconsole.log('SQL:', sql);\n\ni used resulting query in pgadmin and it works.. ",
    "nidin": "@vitaly-t No problem, the problem was the wrong information in documentation. It can happen that some new JS devs read your doc and use + to parse int that can lead to serious problems.  JavaScript engines convert underlaying data type to float 64 when you use leading + and to int 32 when you use trailing |0 <- this is basically a bitwise operation which can only possible with integers. therefore prior to this operation engine will convert data to int 32. However, I have to correct that parseInt() and |0 is not 1:1 equal for example console.log(\"1abc\" |0) and console.log(parseInt(\"1abc\") same thing will happen with parserFloat(\"1.25abc\") and +\"1.25abc\". so in anyway + != parseFloat() and |0 != parseInt()\nThanks for your quick fix.. ",
    "alex-sherwin": "Hi @vitaly-t, you're absolutely right, removing the IEmptyExt typed version would result in the same logical default inferred extension type of {}, effectively the same thing as the IEmptyExt interface\nI've updated the PR with another commit to remove the IEmptyExt variant. ",
    "nazarhussain": "@vitaly-t With the above example...\n```js\ndb.tx('my-out-side-tx', function *(t2) {\nsinonSandbox.spy(t2, 'none'); \nyield t2.DBRepo.action();\nexpect(t2.none).to.be.calledOnce; \n});\n```\nThis test failed, as we spy t2 and none was called on t1 whereas both don't refer to same transaction/task object. \nThis is blocking us to write unit tests for transactions related code. . @vitaly-t If you notice the code int he issue description, you will find t1 the code example just mentioned above should work, but its not. This line always failed expect(t2.none).to.be.calledOnce;. ",
    "DanielLausch": "thats amazing!\nwas so \"easy\" in the end.\nthank your so much for your help!\nand thanks for the reference.\nbest wishes\nDaniel. ",
    "larsthorup": "Thanks for the quick fix! :). ",
    "minotogna": "thank you! . ",
    "chitombo": "\ud83d\ude05 Ahhhh that makes sense now, I was just following the examples you put in the Learn by Example, and copied the Mixing Up code but couldn't get it to work. \nThank you for your fast response. \ud83d\udc4d . ",
    "tjPalSingh": "my query is something like this. \ndb.oneOrNone('INSERT INTO \"Facility\" (name, types) \\\n     values \\\n    (${name}, \n    ${types} returning *', createObj)\nand I have used custom type formatting as well before using this query\nlet finalObj = [];\n    if(createObj[\"types\"].length) {\n      for(let item in createObj[\"types\"]) {\n          let ress = pgp.as.format('$1::FACILITY_TYPE', [ createObj[\"types\"][item] ]);\n          finalObj.push(ress);\n      }\n      createObj[\"types\"] = \"array[\" + finalObj.join(',') + \"]::FACILITY_TYPE[]\";\n   }\nI have tried this too while updating the types : \ncreateObj[\"types\"] = \"array[\" + finalObj.join(',') + \"]\";\nI am getting the same error.. here's the working query with FACILITY_TYPE as enum.\nINSERT INTO \"Facility\" (name, types) \n     values \n    ('fdfdff', '{DC,FC}'::FACILITY_TYPE[]) returning *\n;\n. Thanks alot vitaly . what you suggested worked.. ",
    "shaunakv1": "pg-query-stream@^1.1.1:\n  version \"1.1.1\"\n  resolved \"https://registry.yarnpkg.com/pg-query-stream/-/pg-query-stream-1.1.1.tgz#65e97436ef809d1e160eba84ebf11b9e4742eab4\"\n  dependencies:\n    pg-cursor \"1.3.0\". Awesome, I really appreciate your quick response and looking further into this. I will keep watching. Thanks much! . That is bad news :(  Thanks for looking into it though and narrowing down the problem.  I have added a comment in the issue you referenced. Hopefully the authors will get to it soon. \nWhat would you recommend should be the approach for me that's most sensible if I cannot use streams. Like I said, I have a pg table with 2 million plus rows. I need to iterate on each of it, run it through Redis and insert it back in a new pg table.   I would like to keep the memory usage low but that's not the criteria. Any approach that will let me stably do the ETL will be fine. Just looking for some ideas. \n. @vitaly-t wise words :) streaming probably was not what I needed but just wanted! Going old school like you suggested and paging the database with OFFSET + LIMIT worked beautifully.  Especially when using with spex sequence it worked like a charm \ud83d\udc4d \nI am going to post my solution here for anyone who needs a good example of massive data retrieval and batch processing. @vitaly-t please give this a look and suggest any improvements you can think of to make this more elegant. \nSolution: \nsql\n -- userCount.sql\nSELECT count(*) as count FROM user_dumps;\nsql\n -- usersPaged.sql\nSELECT id, username, address\nFROM users\nLIMIT ${limit} OFFSET ${offset};\n```js \n//db/repos/users.js\n'use strict';\nconst sql = require('../sql').users;\nclass Users {\n    constructor(db, pgp) {\n        this.db = db;\n        this.pgp = pgp;\n    }\n    getUserCount() {\n        return this.db.one(sql.userCount).then(data => {\n            return data\n        }).catch(this.errorHandler);\n    }\n    getUserInBatch(onUsersAvailable) {\n        return this.db.task(t => {\n            return t.one(sql.userCount).then(userCount => {\n                userCount = userCount.count;\n                let pageSize = 10;\n                let numOfPages = parseInt(userCount / pageSize);\n                const source = (index) => {\n                    return new Promise(function(resolve, reject) {\n                        t.any(sql.usersPaged, {\n                            limit: pageSize,\n                            offset: index * pageSize\n                        }).then(data => {\n                            let res = {\n                                users: data,\n                                page: index\n                            }\n                            //send this batch of users back to client for processing\n                            onUsersAvailable(res, function(someDataToInsertBackInDatabase) {\n                                //client calls this callback to indicate\n                                //that it has finished processing this batch\n                                //and send data to insert back in new table as arguemnt\n                                //and to send it next batch\n                                resolve(someDataToInsertBackInDatabase);\n                            });\n                        });\n                    });\n                }\n                const postProcess = (index, data, delay) => {\n                    console.log(Post Process batch: ${index} users:${data.count});\n                }\n                return t.sequence(source, {\n                    dest: postProcess,\n                    limit: numOfPages\n                })\n            })\n        }).catch(this.errorHandler);\n    }\n    errorHandler(error) {\n        console.error(error);\n        return {\n            error: error,\n            message: error.message\n        };\n    }\n}\nmodule.exports = Users;\n```\n```js\n//client.js\nconst db = require('../db');\ndb.users.getUserInBatch(function(data, getPostProcessAndGetNextBatch) {\n    console.log(Batch: ${data.page} : ${data.users.length} users);\n    getPostProcessAndGetNextBatch({\n        count: data.users.length\n    });\n}).then(() => {\n    console.log('All Done!');\n    db.disconnect();\n});\n```\n```bash\nSweet output you want to see :)\n\u2794 yarn run match\nUsers in Database: 2439991\nBatch: 0 : 10 users\nPost Process batch: 0 users:10\nBatch: 1 : 10 users\nPost Process batch: 1 users:10\nBatch: 2 : 10 users\nPost Process batch: 2 users:10\nBatch: 3 : 10 users\nPost Process batch: 3 users:10\nBatch: 4 : 10 users\nPost Process batch: 4 users:10\nAll Done!\n\u2728  Done in 1.23s.\nthis is run over just a subset of data for demo\n```\nFor anyone wondering how these files work together check out awesome demo by @vitaly-t \nNot sure if we still want to keep this open until the streaming issue is fixed, so people can refer to the solution or you would like to close this ticket. I am okay either way.  \nFinally, thanks again @vitaly-t. Being a heavy user of OpenSource code, I have worked with quite a few popular libraries but I have never seen anyone as dedicated and prompt in providing support for their project as you! Cheers, and keep up the awesome work on pg-promise. \n. Thanks @vitaly-t. From knowledge of the library, do you have any recommendations on how I debug why connections are not released? I have been chasing my tail on this by trying stuff. Any tooling or ideas are appreciated.  Is there's anything in pg-monitor that can help me with this? . @vitaly-t  thanks much for looking it up for me! works perfectly just by bumping the library!  I must be searching the webs with wrong keywords (node handles and stuff.. ).\nI say this with every issue I open, but thanks for being an awesome maintainer and quick to reply! . ",
    "ferdizz": "I know, but I've got about 100 occurrences of the same problem in the SQL-dump. And I'm trying to automate the process of dumping the db > drop the db > create new clean db and then execute the queries from the SQL-dump in the new clean db, all without having to manually edit the SQL-file every time. \nThere's no way around the error? When querying with minify: false I get a different type of error. Do you think the same problem is the origin of that error as well?. I can't provide the SQL-file because of the private data it contains. However I've got a workaround solution working by executing the psql-command psql database < data.sql in the Node-script, which will work for now (downside is it requires psql to be installed). Thank you anyways.. ",
    "GiorgosPap": "@vitaly-t It worked. Thank you very much for your help!. ",
    "ericandrewlewis": "\nThe default examples remain using ${prop} syntax because this is the syntax you'd want when building a proper database layer, one where all your SQL is in external SQL files.\n\nI understand separating SQL into external files, but why is ${prop} preferred syntax in external SQL files over something like $<prop>?. ",
    "myDeveloperJourney": "Thank you so much sir, \ud83d\ude4f I really appreciate your taking the time to answer this. I will use the proper protocol for asking questions next time. \ud83d\ude05 . ",
    "benhjames": "This is additionally the case for json and custom enum fields, for example:\nerror: column \"myField\" is of type \"mySchema\".\"myEnum\" but expression is of type text. Never mind - I just read the https://github.com/vitaly-t/pg-promise#custom-type-formatting docs! I'll give this a go.. Turns out I just needed to do:\njs\nconst keys = ['id', new helpers.Column({ name: 'myField', cast: 'jsonb' })];\nwhere cast is whatever the field type is!. Excellent - thank you for sanity checking! \ud83d\ude42 . ",
    "nguyen-tran": "I was able to solve this by performing the exact same query but using prepared statements instead. Still, I wonder what causes this behavior.. ",
    "grebmeg": "But this happens in one instance of the application, the other works fine, but they are on different servers. I suppose it may be have to do with pool, but I am not sure...\nThanks for helping.. Thanks. But I didn't figure out yet in which problem. I will try to locate.\nI think this issue can be closed.. ",
    "JSRound": "I have just realised that you can get the number of rows affected with RETURNING field. ",
    "dfairaizl": "Hi vitaly-t - thanks for the quick reply! Here's some more context.\nWe're making extensive use of QueryFiles and want to share the columns for select statements between the different queries. But most of these queries involve the logic above - retrieving an element in an array or jsonb column and aliasing it for the output.\nThe query files for example look like:\nsql\nselect\n  ${cols:raw}\nfrom ${table}\nwhere type = ${type}\nand ${criteria.field:raw} < ${criteria.val}\norder by ${orderBy.field:raw} ${orderBy.direction:raw} nulls last\nlimit ${limit};\nCan this be achieved via ColumnSets?. I see thanks for clarifying that. So do you have an example of how to build up a list of columns for a select statement where certain columns have different formats?\nThis is what I was able to come up with:\njavascript\nthis.cols = [\n  'id',\n  pgp.as.format('$1:name[1] as $2:name', ['slugs', 'slug'])\n].join();\nAnd formatting cols as raw in the query file.\n. Actually something like this generates the correct sql but its very verbose, is there an alternative?\njavascript\nthis.cols = [\n  pgp.as.name('id'),\n  pgp.as.name('description'),\n  pgp.as.format('$1:name[1] as $2:alias', ['slugs', 'slug']),\n  // + 12 more columns, all formatted as :name\n].join(', ');. To answer your first question - I'm trying to specify a \"static\" list of columns in our model class, and reuse those columns across a handful of query files so we have to do less maintenance.\nAppreciate the help!. Ah yes thats a much cleaner solution. Can you clarify our point about extending it?\n\n...which you can extend to also support something like {name, alias} inside the array of names.\n. Hi @vitaly-t yes I think we're good now too. I'll give this a shot.\n\nThank you for adding an example!\nCheers,\nDan. ",
    "pavan6cs": "i have update the issue with full error. please check. hello i have upgrated the library to latest version 8.4.4 then also im getting same error,below is the formatted code but the query is working fine in pg admin thats why i raised the issue\nconst PS = require('pg-promise').PreparedStatement;\nconst MonthlyData = new PS(MonthlyData,\"select ref.app_name app,ref.task_name,nullif(g1.task_status,0) as task_status,g1.comments,g1.time_end from gap_reports_ref as ref left join gap_reports g1 on ref.app_name=g1.app_name and ref.task_name=g1.task_name and g1.type=ref.type and ref.run_id=g1.run_id and time_end=\n( \n        select MAX(time_end) \n        from gap_reports as g2 \n        where g1.app_name=g2.app_name and g1.task_name=g2.task_name and g1.run_id=g2.run_id and g2.type=g1.type )\n        where ref.run_id=$1  and ref.type=$2 \n        order by app,g1.time_end\", [runId, runType]);. ok thank u i will check the query and debug step by step. ",
    "bbaltatu": "Sorry, it was my mistake\nI called from 2 libraries and in the second one i dont use noWarnings: true. ",
    "jaimesangcap": "Cool! exactly what I'm looking for. I gave up too quickly, sorry \ud83d\ude04 . Thank you for answering. I understand your point. I guess I have to roll my own where builder.. ",
    "ThomWright": "Thanks for the quick response!\nIdeally I'm after an API where I can write my queries without needing to worry about correlation IDs, or any other request context.\nFor example, my code currently looks something like this, where the logger I pass in already has the correlation ID, so the rest of my code doesn't need to manually pass it around.\ntypescript\nfunction createDataAccess({logger, pg}: {logger: Logger; pg: IDatabase<{}>}) {\n  return {\n    getUser(id: string): Promise<any> {\n      logger.info(\"getting a user\")\n      return pg.oneOrNone(\"SELECT id, username FROM users WHERE id = ${id}\", {\n        id,\n      })\n    },\n  }\n}\nI have a much fuller example which I can post later, but it will have to wait until I get home!\nAnd yeah, we're using TypeScript!\nThe Tags feature is useful, is there a reason there couldn't be a context for a single query outside of a task/tx?. ",
    "martincully": "Thanks Vitaly, I figured I would need batch somewhere but my earlier attempts to use it didn't fix my issues. \nSorry if I'm being a bit thick but can you give and example of how to fix \n\nYou fail to chain the task in createSalesOrderLine\n\nI'm new to javascript so I'm on a steep learning curve at the moment :-)\nI'll continue to play around with my code here but appreciate any help you can provide!. Excellent thanks a million. I'm still trying to wrap my head around generators and promises. I was almost there with my current version I was just missing the return in front of the line\njavascript\nt.task('createSalesOrderLine', function* (task)\n. ",
    "allanchua101": "@vitaly-t \nThanks for the help buddy!! :D I figured out that i had to transform the json array to an array literal (eg. {1,2,3,4,5}).\nRegards,\nAllan. ",
    "danh293": "Making the queries into functions was just a naive suggestion that prevents them from executing immediately. The main issue is that the call, t.batch([<an insert that the foreign keys referenced>, ...queries]) looks like it is going to run the first insert and then proceed to run the queries, but, in reality, the queries have already been run when creating the queries variable.. I guess the main point I'm trying to make is that the API makes it seem like t.batch executes all of the queries, but this isn't the case. The queries have already been executed when creating the queries variable and the call to t.batch just does some other stuff.. To answer your edit, yes I have now switched to using helpers :)\nThe main thing I find confusing about using the library in this context is that if I inline the creation of the queries variable into the t.batch call, then it will all work fine.. In your ES6 snippet above, where would you create the queries variable?. Okay, yeah, I see that now. So instead of calling t.batch, you would do something like...\ndb.tx(function * (t) {\n    t.none(<an insert that the foreign keys referenced>)\n    .then(() => {\n        for(let i = 0; i < queries.length; i ++) {\n            yield t.none(<an insert with a foreign key constraint>, queries[i]);\n        }\n    })\n})\nForgive me if I've completely misunderstood how ES6 generators work; haven't used them before. The main thing I was trying to do was to get it work with t.batch as I thought you had to use this when inside t.tx.. Thanks for your help :). ",
    "txssseal": "@vitaly-t very true, would love any help if you have done this. nvm, I just appended the raw sql to the end of the helper.  . ",
    "ea167": "Good to know, thank you Vitaly!. ",
    "hawkeye64": "Sorry, was on vacation and just got back.\nBasically, we have some triggers on various tables in the database. PostgreSQL sends Operation (UPDATE, INSERT, DELETE)[String], table name [String] and keys ( {[table_id: 21]} ) [Array of objects] through the notification to the Node back end.\nWhen a large update is done via a single UPDATE command, the Node back end is getting each individual ID (key) as a single message via pg-promise. However, in testing with Python, it's coming as an array for the \"keys\" part of the notification. I am wondering if I am not doing it right? Or, perhaps this is by design for pg-promise?. Perfect! Thanks. I'll give it  a try. :). ",
    "bradleykirwan": "Sure thing - would be keen to look into this.\nSome brief reading of the node stream docs reveals:\n\nIn general, developers should choose one of the methods of consuming data and should never use multiple methods to consume data from a single stream.\n\nSo in this case I think the returned stream should be wrapped (perhaps in a PassThrough stream) so that the consumer can use data-events/piping freely.. @vitaly-t Not sure I follow - are you talking about pg-query-stream? From what I understand and have read of the source, it uses a refcursor and reads n rows at a time (default 100) - further, PgQueryStream is an object mode stream, and produces a single chunk for each row, so I'm not sure what you mean by processing record-by-record; I might be missing something here?\nAnyway, after thinking some more, I think the main issue with the stream implementation in pg-promise, is that a data event handler is already attached to the returned stream, which can cause a few undesirable effects, the main one being:\n\nStream is switched into flowing mode automatically - this means if your consumer fails to attach to the stream before it starts producing data, then data will invariably be lost.. @vitaly-t Sure, hope to get some time to look at this, this week.. \n",
    "GHNewbiee": "Just consider it as a feature request.\nI wouldn't like to put Feature Request in the title. If you wish, you can consider it so and put a label.. For type casting, such as:\ndb.func(\"check_value_existence\", [account.username::text, \"username\"::text, \"account\"::text, \"customer\"::text])\n.then(found => { ...})\n.catch(error => {...});\nIf you do not agree, just ignore it!. ",
    "oldschoolbg": "Hey thanks for getting back to me; I spent a good couple of hours before posting the question and have identical code, other than one called within the tx and the other (now without tx safety) from a normal promise and the promise works but the tx doesn't - I'll attach some diagnostics as soon as I can and try to find out what is actually going to the server.\nThank you for the suggestions regarding generated; I will have a look at that as well! I'm very new to postgres and am learning a lot every day so that really helps.\nCheers. Hey - just a quick update. I have altered my code to use the generated solution you suggested and this works with the transaction. I have not got to the bottom of why the previous attempt didn't work however this can be closed I reckon.\nThanks again for your assistance.. ",
    "amangeot": "Sorry I'm new to this and thought we had to enforce array types with something like array[]::time without timezone[].\nI'll close this issue, thank you!. ",
    "akhchan99": "For anyone encountering the same problem...\nIts a combination of user privilege and schema...\nmy user account has privilege to schema Test, but the uuid-ossp is installed under public, and I set global pg-promise's schema to Test, therefore it couldn't find the function. As pg-promise send a SET schema command to the server, which makes it not able to access public's uuid-ossp extension.\nSo on the pg-promise init, I set multiple [schema] = ['public', 'Test'] and it works.\n[schema]:http://vitaly-t.github.io/pg-promise/module-pg-promise.html. Got it, thank you. ",
    "xShirase": "ts-node v7.0.1\nnode v8.12.0\ntypescript v3.1.1\n. Ok, tested both, @dimfeld is right, esModuleInterop: true doesn't let the tutorials compile properly. The other flag has no influence. \nWorth a mention in README?. ",
    "dimfeld": "Do you have esModuleInterop or allowSyntheticDefaultImports set in your typescript compiler options?  These options change typescript's import behavior in a way that could cause the behavior you're seeing.. ",
    "AkshayMehta90": "Yes, I was looking for the latter. And your solution worked perfectly. Thank you so much such a quick response.. ",
    "kristjank": "Sure, get well soon. \ud83d\udc4d \nWhat troubles me is the memory consumptions and why is not writing to db during the stream read. . Used a different approach. Would still like to see it working with stream. \nGet better ;). Yes I used that one from the start combining with stream. Wanted a cleaner solution for stream (source, destination)-but looks like the example is faster anyway so will stick with him. tnx. if you have any comments on this import here : https://github.com/ArkEcosystem/core/blob/feat/snapshosts-new/packages/core-snapshots-cli/lib/transport/index.js#L34-L73. feel free to add some. tnx. ok, nice thanks for the answer, will use the second option and let you know. \ud83d\udc4d \n. misread stuff while testing. \nBut still why not change all records, and only the 100th ones?. tnx, closing for now. ",
    "joux3": "That does seem annoying, thanks for your hard work shielding us users from dependency issues. I presume pg-promise could just issue rollbacks on any transaction error unless the driver was broken in this way.\nI opened PR #568 with statement timeout special case handling.. Always returning false from isConnectivityError does fix my issue (the PR just causes false to be returned for this exact case). I agree, this looks like the optimal path.\nLuckily this issue does not cause me to hurry: I could work around this specific issue in my specific case by using a task instead of a transaction, which of course is not always possible.\nIs there something I could do the help test the driver changes? At least modifying the repro case to an actual pg-promise test case should not be too much work.. (Closing based on the discussion in #566). ",
    "kevinjuliano1020": "Sorry, I am just missing \",\" on parameters. ",
    "yhrchan": "I have came across this: https://github.com/spion/promise-streams/issues/7\nand your first comment there is what exactly I am trying to solve but without luck =(. Thanks for your quick reply. Below is what I have currently, been trying different things but couldn't figure it out. Right now it reads a json file (theoretically a huge file) but that will bomb (javascript heap out of memory) since it reads everything in one go and ingest that into the \"t.none(insert)\" command. I am not an advance user of Streams and also recognize that there might be complications due to the nature of Streams and Promises. I am just not sure how to break reading the stream into batches and have many \"t.none(insert)\" so that it will keep the memory footprint manageable. Thanks!\nlet reader = fs.createReadStream('some json file');\nconst jsonStream = JSONStream.parse(['properties']);\nlet lineNumber = 0;\n// commented the below out since not sure how pageIndex relates to a Stream\n//function getNextData(t, pageIndex) {\nfunction getNextData() {\nreturn new Promise((resolve, reject) => {\n    let data = [];\nreader\n  .pipe(es.split())\n  .pipe(jsonStream)\n  .pipe(es.mapSync(line => {\n    ++lineNumber;\n\n    //console.log(`${lineNumber} => ${JSON.stringify(line)} \\n`);\n\n    data.push(line);\n  })\n  .on('end', function() {\n    console.log('Read entirefile. ' + data.length);\n    resolve(data);\n  })\n);\n\n})\n}\ndb.tx('massive-insert', t => {\n    //return t.sequence(index => {\n      //return getNextData(t, index)\n        return getNextData().then(data => {\n          if (data) {\n            const insert = pgp.helpers.insert(data, cs);\n            return t.none(insert);\n          }\n        });\n  })\n  .then(data => {\n    // COMMIT has been executed\n    //console.log('Total batches:', data.total, ', Duration:', data.duration);\n  })\n  .catch(error => {\n    // ROLLBACK has been executed\n    console.log('oh no');\n    console.log(error);\n  });. ",
    "Guzzur": "When Updating data it takes not the current query but the previous one and run it. ",
    "johanneswuerbach": "@vitaly-t sorry for the delay, @tolgaakyuz is a college of mine and we discovered this issue together.\npg-pool is accepting an error as argument here https://github.com/brianc/node-pg-pool/blob/140f9a1242e94c09fc780d8e4bcc82b91a787d39/index.js#L35 and only if the err is passed removes the client from the connection pool. \nThe onError handler on the connection is called between acquiring a connection and actually executing a query on it.\n```js\n// For this the pool size should be configured with max: 1\n// Acquire a connection and do some work\nconst trx = db.tx((async (t) => {\n  await wait(5000);\n  await t.one('SELECT now()');\n});\n// Simulate a connection failure after acquiring, but before query\nawait wait(200);\nawait controlDB.any('SELECT pg_terminate_backend(pid) FROM pg_stat_activity WHERE pid <> pg_backend_pid();');\nawait trx;\n// Try to re-use the connection fails with Client has encountered a connection error and is not queryable\nconst trx = db.tx((async (t) => {\n  await t.one('SELECT now()');\n});\n```\nHappy to sync back with @tolgaakyuz to add this as a test-case here.\n. @vitaly-t updated, the test seems still to be flaky and some runs crash completely with:\n\nAbnormal client.end() call, due to invalid code or failed server connection.\n    at Pool._remove (x/github.com/vitaly-t/pg-promise/node_modules/pg-pool/index.js:139:12)\n    at Client.idleListener (x/github.com/vitaly-t/pg-promise/node_modules/pg-pool/index.js:54:10)\n    at connectedErrorHandler (x/github.com/vitaly-t/pg-promise/node_modules/pg/lib/client.js:148:10)\n    at Connection.con.once (x/github.com/vitaly-t/pg-promise/node_modules/pg/lib/client.js:216:9)\n    at Socket.<anonymous> (x/github.com/vitaly-t/pg-promise/node_modules/pg/lib/connection.js:130:10)\n    at process._tickCallback (internal/process/next_tick.js:63:19). @vitaly-t yes, there seems to be actually more then one issue with error handling, but so far I wasn't able to pin it down.\nIt seems to be that sometimes ctx is disconnected multiple times and sometimes the onError handler is called before going into query causing weird states.. If the delay(100) is removed this still fails, but that seems to be a different bug :-(. ",
    "aldobrynin": "Yay, thanks! I knew it possible but didn't look at format method.\nYea, query is very special and this probably shouldn't be part of common API . ",
    "tmtron": "yes, I am confused...\nSo is there currently a way to execute a multi-query and get back node-result objects for each of them?. thanks - somehow I missed that and concentrated too much on the result function... ",
    "FunnyB0y": "@vitaly-t ah it's okay, rereading the docs I think I can make it work now.\nhttps://github.com/vitaly-t/pg-promise#sql-names. ",
    "lzambarda": "Thanks for the prompt reply.\nExpected behavior\nGraceful error or catchable error when DB connection suddenly drops.\nActual behavior\nThe program using pg-promise crashes with the following trace:\n```bash\nfunction getSafeConnection(cn) {\n                          ^\nRangeError: Maximum call stack size exceeded\n    at Object.getSafeConnection (/Users/.../node_modules/pg-promise/lib/utils/index.js:99:27)\n    at Client.onError (/Users/.../node_modules/pg-promise/lib/connect.js:108:26)\n    at emitOne (events.js:116:13)\n    at Client.emit (events.js:211:7)\n    at Connection.connectedErrorHandler (/Users/.../node_modules/pg/lib/client.js:148:10)\n    at emitOne (events.js:116:13)\n    at Connection.emit (events.js:211:7)\n    at Socket.reportStreamError (/Users/.../node_modules/pg/lib/connection.js:71:10)\n    at emitOne (events.js:116:13)\n    at Socket.emit (events.js:211:7)\n```\nSteps to reproduce\nI have created this small program to showcase the issue:\n```js\nconst pgp = require('pg-promise')();\nconst connString = CONNECTION_STRING_USING_TUNNELING;\nconst db = pgp(connString);\nfunction connect() {\n    return new Promise((resolve, reject) => {\n        db.connect({ direct: true })\n            .then(obj => resolve(obj))\n            .catch(err => reject(err));\n    });\n}\n(async function() {\n    console.log('connecting...');\n    await connect();\n    console.log('connected, now try killing the ssh tunnel');\n})();\n```\nI am currently ssh tunneling to the remote DB. If something happens to the tunnel (e.g. dropped from either side), the program crashes  with the aforementioned trace.\nEnvironment\n\nVersion of pg-promise: 8.5.5\nOS type (Linux/Windows/Mac): Mac, macOS 10.13.6\nVersion of Node.js: v8.10.0\n. I managed to replicate this on Windows too using the above script and a standard SSH tunnel to a remote database (I have used it to forward a local port to the remote pg default 5432).\n\nOnce the tunnel has been established, I ran the above script which printed\nconnecting...\ndone\nAnd then started idling as no other operation has been coded.\nOnce in this state, one can forcibly cause the tunnel to close from their side (e.g. a Ctrl+C really can do this, despite not being ideal to test things) and the overflow will happen. I did not manage to reproduce this when closing the connection from the remote side.\nI hope this can help in any way.. @vitaly-t I managed to replicate this on a Windows machine.\nFirst I created an ssh tunnel to reroute port 2222 to 5432, this was the only way to test the ssh tunnel without having a remote DB to use.\nssh -N -L 2222:127.0.0.1:5432 localhost\nIf you get the following error message: \"ssh: connect to host localhost port 22: Connection refused\" then you are likely to need to enable the developer mode (operation which may require you to install some tools and restart your machine. This should help https://docs.microsoft.com/en-us/windows/uwp/get-started/enable-your-device-for-development ).\nOnce you manage to set up this ssh tunnel you can try running the aforementioned code using a connstring matching the port you are rerouting ('postgres://xxxx:xxxx@localhost:2222/xxxx').\nAt this point terminating the tunnel should result in the error!\nUnfortunately I do not have time to learn how the library works and to try fixing this as my company found an alternative way to programmatically cope with this if it will ever happen.\nI hope this helps you with this issue.\nAnyway this seems to happen only when the tunnel is terminated from the machine running the client, therefore any termination from the remote source will not likely cause any trouble with pg-promise (I have tested this many times, even killing the remote DB).\nBR. ",
    "pennyandsean": "Hi guys, I hope this is related, forgive me if it isn't.  I too have started getting a stack overflow.  My code is pretty much a direct copy-paste from the Robust Listeners example.  I can drop the connection with a admin initiated pg_terminate_backend(pid) as suggested at the bottom of the example.  I can also interrupt the network connection.  Both cases exhibit wanted behaviour, the connection re-establishes gracefully.  \nHowever, at random intervals, the connection drops on its own.   When it does, onLost is not called, a loop is entered which leads directly to a stack overflow and I cannot intervene.   I managed to capture the following output from pg-monitor.\n\nThe db instance is version 10.6 running in a local docker container.  A glance at the code looks like there is special handling for what looks like an .end but is not a code-initiated .end.\n. Just trying to help.\nOn Mon, 11 Mar. 2019, 19:29 Vitaly Tomilov, notifications@github.com\nwrote:\n\n@pennyandsean https://github.com/pennyandsean We still need a way to\nreproduce this -\nHowever, at random intervals, the connection drops on its own.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/585#issuecomment-471446887,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHAs4YrVv60le7xvz9_abRs8zOuZsMEnks5vVhPkgaJpZM4a0S62\n.\n. \n",
    "epm-bt": "Thanks, @vitaly-t, it was useful!. ",
    "MichaelOdumosu57": "Very well, never say never, I will make one, can point me in the right\ndirection concering postgres protocol\nOn Sat, Feb 23, 2019 at 5:58 PM Vitaly Tomilov notifications@github.com\nwrote:\n\nThis library is for server-side only.\nThere is no such thing as PostgreSQL JavaScript driver for the\nclient-side, and never will be.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/589#issuecomment-466708164,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AUVb8cOciJ6CDUZogk10Uv1nQQRKWzhwks5vQceYgaJpZM4bONH9\n.\n\n\n-- \nMichael Odumosu\nClerical Office Associate and Fullstack Freelance Software Developer\nInformation\nTel:929-353-8278\nLinkedin: https://www.linkedin.com/in/michael-odumosu-a58367b1\nFacebook: https://www.facebook.com/mchael.odumosu\nGithub: https://github.com/MichaelOdumosu57\n. The Chrome browser is most likey more integrated with the OS and has access\nto more features than pgadmin4, I assume from inital theory and broad\nevaluation.\nSecurity reasons in what context development or production, this need to to\nbe a complex API, any attacker trying to access the port through my\nsuggested browser API will have to deal with the hosting server's firewall\nwhich in enterprise production, should be strong enough in the first place.\nOn Sun, Feb 24, 2019 at 8:07 AM Vitaly Tomilov notifications@github.com\nwrote:\n\n@MichaelOdumosu57 https://github.com/MichaelOdumosu57 The right\ndirection is not to do it. The reason it can only be a server-side:\n\nIt requires tight integration with the hosting OS, which only\n   Node.js can provide\nMany security reasons\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/vitaly-t/pg-promise/issues/589#issuecomment-466772796,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AUVb8S31uRsentVN27WCpLLfpIqPCFOxks5vQo54gaJpZM4bONH9\n.\n\n\n-- \nMichael Odumosu\nClerical Office Associate and Fullstack Freelance Software Developer\nInformation\nTel:929-353-8278\nLinkedin: https://www.linkedin.com/in/michael-odumosu-a58367b1\nFacebook: https://www.facebook.com/mchael.odumosu\nGithub: https://github.com/MichaelOdumosu57\n. ",
    "idevChandra6": "Thanks much for prompt response Vitaly. \nI think I got something wrong, and I end up in catch with error object as {}. \nDetails:\nHere are the method parameters\nuserid, a string: uYt56RT\nallSubscriptions, an array : [\"Subscription1\", \"Subscription2\", \"Subscription3\"]\nThe queries the I would want to run are:\nINSERT INTO subscriptions(useridcode, strcode) VALUES('uYt56RT', 'Subscription1')\nINSERT INTO subscriptions(useridcode, strcode) VALUES('uYt56RT', 'Subscription2')\nINSERT INTO subscriptions(useridcode, strcode) VALUES('uYt56RT', 'Subscription3')\nWith the below method, I get {} error object: \n```\nlet addTopicsToUser = function (userid, allSubscriptions, callback) {\nconst cs = new pgp.helpers.ColumnSet([\n    {name: 'useridcode', init: ()=> userid},\n    {name: 'strcode', prop: 'item', def: DEFAULT}\n    ], {table: 'subscriptions'});\n\nconst query = ()=> db.helpers.insert(allSubscriptions, cs);\n\ndb.none(query)\n    .then(data => {\n        logThisDev(\"Success inserting, %j\", data);\n        return callback(null, {success: true});\n    })\n    .catch(error => {\n        logThisDev(\"Error inserting, %j\", error);\n        return callback(error);\n    });\n\n}\n```\nI tweaked above method little (given below) by seeing other documentation and stackoverflow and I am getting error Failing row contains (uYt56RT, null) as second values is becoming null. \n```\nlet addTopicsToUser = function (userid, allSubscriptions, callback) {\n    var values=[];\nfor (var item in allSubscriptions) {\n    values.push({useridcode: userid, strcode: allSubscriptions[item]})\n}\n\nconst cs = new pgp.helpers.ColumnSet([\n    {name: 'useridcode', init: ()=> userid},\n    {name: 'strcode', prop: 'item', def: DEFAULT}\n    ], {table: 'subscriptions'});\n\nconst query = pgp.helpers.insert(values, cs);\n\ndb.none(query)\n.then(data => {\n    logThisDev(\"Success inserting, %j\", data);\n    return callback(null, {success: true});\n})\n.catch(error => {\n    logThisDev(\"Error inserting, %j\", error);\n    return callback(error);\n});\n\n``\n. Well, actuallyitemis not null. \nThevalues` is this when I printed: \n```\n[\n    {\"useridcode\":\"uYt56RT\",\"strcode\":\"Subscription1\"}, \n    {\"useridcode\":\"uYt56RT\",\"strcode\":\"Subscription2\"}, \n    {\"useridcode\":\"uYt56RT\",\"strcode\":\"Subscription3\"}\n]\n``\nSo, I believeconst query = pgp.helpers.insert(values, cs)statement is fed with right data asvalueshas right data in it. But the database reports second value which isSubscription1, Subscription2,...etc. arenull`, which is not.\nThought, you could have a look.\nThanks again, for your prompt reply. Much appreciated.. @vitaly-t : Hi, did you get a chance to look at this issue?. @vitaly-t : This below code worked for me. Thanks much for the amazing library.\n```\nlet addTopicsToUser = function (userid, allSubscriptions, callback) {\n    const cs = new pgp.helpers.ColumnSet(['useridcode', 'strcode'], {table: 'subscriptions'});\n    var values=[];\n    for (var item in allSubscriptions) {\n        values.push({useridcode: userid, strcode: allSubscriptions[item]})\n    }\nconst query = pgp.helpers.insert(values, cs);\n\ndb.none(query)\n    .then(data => {\n        logThisDev(\"Success: %j\", data);\n        callback(null, data);\n    })\n    .catch(error => {\n        logThisDev(\"Error: %j\", error);\n        callback({\"error\": error});\n    });\n\n}\n```. ",
    "mkmpvtltd1": "it's not strange-lloking syntaxt :rofl: . "
}
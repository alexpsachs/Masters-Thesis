{
    "dpw": "Hi,\nI'd suggest posting your question to the rabbitmq-discuss mailing list (https://lists.rabbitmq.com/cgi-bin/mailman/listinfo/rabbitmq-discuss).  That's where most rabbit-development discussion goes on, and you no longer need to subscribe to post there.\n. ",
    "djui": "Definitely, excuse me. I felt it was the wrong the place here.\n. ",
    "PiotrSikora": "It seems that it was pulled long time ago as bb793eec22c039a8581645704f10e96c858eaf76, so I'm closing it.\n. Merged in Mercurial.\n. Merged in Mercurial.\n. ",
    "michaelklishin": "Please reopen if this is still relevant.\n. Since this thread is from 2012, keep a couple of things in mind:\n\nThings could have changed since then.\nThe AMQP 0-9-1 spec has known issues, inconsistencies and statements open to interpretation. Team RabbitMQ has an entire errata document. It also has features that 10 year later can confidently be considered poorly thought out and in a couple of places RabbitMQ deviates to do what's more user-friendly.\nIf a particular behaviour is demonstrated by most popular client libraries, including those our team maintains, it is extremely unlikely that it's going to change by now. If something is a truly annoying behaviour, it has been reported plenty of times in the ~ 10 years since the 0-9-1 spec was last revised.\n\nRabbitMQ sends clients a channel.close-ok in response to a channel.close, so why would clients not be required send a response when RabbitMQ initiates a close?\nAnyhow, a quick glance at rabbit_channel/rabbit_reader, Java client and Bunny suggest that\n\nA channel.close-ok is still required.\nJava client and Bunny sent channel.close-ok as expected.\nIn Java client it has been the case since at least Jan 2011 (very likely way earlier, it's just as early as things go for ChannelN in the current codebase)\n\nSo the reporter probably hasn't used one of the clients maintained by our team. node-amqp is known to be a terrible, buggy abandonware and we have been recommending avoiding it for years.\nI cannot name a client that has caused more head scratching and false RabbitMQ bug accusations than node-amqp. Use amqp.node instead.. Any further comments about how the spec should be interpreted belong to the mailing list. . I'm not sure this has to be a part of this repo.\nPlease start a conversation about this proposed change on rabbitmq-users. Thank you!\n. Looks like this PR is incomplete. Please consider submitting it again next week as we are preparing to make GitHub repositories primaries (instead of just mirrors).\n. @hairyhum take a look at AMQP 0-9-1 Errata as well as AMQP 0-8 and 0-9-1 differences.\n. Messages can be fetched from a queue on demand using basic.get and requeued, but this will alter message metadata and may affect message ordering. Please use rabbitmq-discuss for questions.\n. Are you sure that all the commands use the same vhost?\n. Also, are you sure that you actually have any messages to list (e.g. can see N messages ready in the management UI)?\n. Publishers are blocked on per-connection basis. Since aliveness test cannot use your apps connection, it cannot know that they are blocked.\nA new monitoring endpoint may be worth adding but aliveness-test cannot provide information for your connections,\nonly overall node health.\n. \"when I put my queue into a state where it will not allow publishing new items\"\nthis is not a correct statement. Connections can be in that (blocked) state but not queues.\n. There are already HTTP API endpoints for connection info. If you'd like something to be added there, please start a discussion on rabbitmq-discuss.\n. Aliveness test is part of a RabbitMQ plugin which can do/use things AMQP 0-9-1 clients cannot:\n- There is a so-called direct Erlang client that uses distributed Erlang facilities instead of AMQP connections.\n- Plugins can bypass many checks that basic.publish handler normally goes through.\nand so on. Aliveness test is mean to test key broker infrastructure and not every possible failure scenario.\nI need to take a look if management API can provide any information about alarms. Will get back to you.\n. @CVTJNII only specific connections are blocked, so your claim that \"the queue cannot process messages\" is factually untrue even when alarms are in place. Like I said, we are VERY far from having a consensus about how the aliveness test should work. Everybody wants it to work the way their company's monitoring work. Sorry, we cannot accommodate all requests.\n. @CVTJNII publishers are blocked after they publish at least one message, which can (although not guaranteed and typically won't) be accepted fully, e.g. if it has a blank body. How would RabbitMQ know that a connection is publishing otherwise?\nThe aliveness check uses a regular RabbitMQ client and the only thing that could be missing is a timeout \u2014 which again, everybody has their own ideal default for.\nSo instead of assuming that the aliveness test should cover everything, please stop suggesting that we make it do A, B, C, \u2026, X, Y, Z. Use multiple monitoring checks, the HTTP API provides a ton of data, and so does rabbitmqctl, which now reports alarms in status.\n. I'm locking this because this is getting ridiculous. I cannot think of a data service where a single request can discover every possible issue there might be (given that definitions of \"healthy\" varies from company to company, or even project to project). Yet somehow RabbitMQ is expected to provide it.\n. Filed https://github.com/rabbitmq/rabbitmq-management/issues/137 for one specific improvement we can make.\n. @CVTJNII furthermore, you assume that when a publish method returns, the message is published. That's wrong: it simply means that the data was written to the socket. It absolutely does not meant that it has reached RabbitMQ, was read, parsed, and routed. When alarms are in place, connections that see a basic.publish or content metadata or body frame will stop reading from the socket. However, the client knows nothing about that, even if it is actually blocked, unless it registers a connection.blocked handler.\n. This was fixed on the branch bug18626.\n. @kiela thank you for contributing. Pivotal will require you to sign RabbitMQ Contributor Agreement before any submitted changes can be reviewed. Please let me know where I should send one (it's a PDF that you will email back, signed).\nIn addition, RabbitMQ uses github to host repository mirrors, so your patch will have to be applied manually.\n. I'm not aware of one but a lot of monitoring tasks can be accomplished using HTTP API or Firehose.\nPlease use rabbitmq-discuss for questions.\n. Hi @pasku. Thanks for submitting a pull request!\nGitHub repositories are just mirrors, all development happens on hg.rabbitmq.com. We'd need you to sign a Pivotal Contributor Agreement before your patch can be evaluated. What email should I send it to?\n. @pasku may I ask why you find this change useful?\n. There is still no consensus about whether this feature is worth including, given the edge cases (e.g. huge queues with most of messages on disk).\n. I am positive about including this feature. Let's wait for other team members' opinions.\n. I'll take the liberty to close this. Not because it's a feature we don't want but because the implementation doesn't cover some edge cases we are quite worried about.\n. Hi @dirkmueller. Thank you for your contribution.\nRabbitMQ does not use github for development. This repo is just a mirror. We will happily apply your patch\nif you have Pivotal Contributor Agreement signed. If not, let me know where I should email a copy.\n. I ended up fixing this tiny issue myself. Thanks again for bringing this up. I'm happy to assist you with sorting the CLA out for future contributions.\n. This is not connection.unknown but connection.blocked, a RabbitMQ extension to AMQP 0-9-1 which Wireshark dissector is unaware of. Your client should support it (at least it indicates so in capabilities), otherwise RabbitMQ would not send it.\n. class = 10, id = 50 is connection.close. To quote AMQP 0-9-1 JSON file in rabbitmq-codegen:\njavascript\n{\"id\": 60,\n                 \"arguments\": [{\"type\": \"shortstr\", \"name\": \"reason\", \"default-value\": \"\"}],\n                 \"name\": \"blocked\"},\n                {\"id\": 61,\n                 \"arguments\": [],\n                 \"name\": \"unblocked\"}\n. No. Please use rabbitmq-discuss for questions.\n. Also, RabbitMQ treats message payload as opaque bags of bytes. There is a priority property on basic.publish but it is not used by Rabbit.\n. This is not a support forum, neither it is the primary repository of rabbitmq-server. Please post questions to rabbitmq-discuss.\n. @marek-stoj thank you for contributing. This repository is a mirror. Please post your suggestions to rabbitmq-discuss. We will ask you to sign a (PDF) Pivotal Contributor Agreement, too.\n. This repository is a mirror. Please create a patch against default branch in the repo at hg.rabbitmq.com and post it to rabbitmq-discuss. It may be necessary to sign Pivotal's contributor agreement (in which case I'll email you a PDF).\nOne extra question: does this new file need to be listed in some metadata or dpkg/upstart will detect and install it in the right place automatically?\n. This repository is a mirror. Please post your questions and bug reports to rabbitmq-discuss.\n. It was in 3.0: bug 25260. Please provide more info on the mailing list.\n. @jnordberg can you please post your publishing code (in particular the message metadata/BasicProperties) and the output of rabbitmq report? Thanks.\n. @jnordberg is this using node-amqp? node-amqp is known to have issues and is not maintained, so I'd recommend trying with amqp.lib.\nIn general, priority = 0 should be fine but I cannot reproduce the issue so far with Ruby and Java clients. So my guess is that node-amqp simply does not serialize things properly when priority equals 0.\n. Upgrade Erlang (ideally to 17.1). This is a known problem but RabbitMQ cannot do anything about it. \n. > Was it fixed in 17.1?\nI could not find any reports of it for 17.0 or 17.1. 17.0 has other issues as reported by some of the largest users, however, they are happy with 17.1.\nWe have reported this to the Erlang/OTP team as early as 11 months ago:\nhttp://erlang.org/pipermail/erlang-questions/2013-September/075312.html\nThe Rabbit team does all of the user support ourselves and we fairly commonly see obscure issues on R14 and R15 but not on R16 or 17.1.\n\nDoes it relate in any way to the Hipe plugin?\n\nSomeone suggests that it does:\nhttp://rabbitmq.1065348.n5.nabble.com/RabbitMQ-3-2-4-crashes-std-alloc-Cannot-allocate-1125562100111744-bytes-of-memory-of-type-arg-reg-tp34176p34237.html\nbut in any case, it is a runtime issue because 1125316227743680 is about 1 petabyte of memory.\nSounds like an overflow, doesn't it?\n. They were not removed in master to the best of my knowledge. In any case, we've had a bug for this for many months but 18.0 is still quite a while off.\n. Those are type specs, not functions, by the way.\n. Feature requests belong to rabbitmq-users.\n. Thank you. This repository is a mirror. Please post your patch against the Hg repo to rabbitmq-users, and lets discuss it there.\n. Please use the mailing list for questions.\n. Thank you. This repository is a mirror. Please create a patch against the mainline (see hg.rabbitmq.com) and post it to rabbitmq-users. I believe we have a bug filed for this, so your patch will be considered.\n. Please post feature requests to rabbitmq-users.\n. See rabbitmq-tutorials. I also highly recommend using amqp.lib which has its own tutorial port.\n. This repository is a mirror. Please post your question to rabbitmq-users.\n. This repository is a mirror. Please post questions to rabbitmq-users.\n. You are providing too many arguments to join_cluster. It should be given a single node to contact. Also, I assume you mean http://www.rabbitmq.com/clustering.html, as http://www.rabbitmq.com/ha.html doesn't have a clustering transcript.\n. That's how it is currently done: you cluster a node with another node. The node rabbitmqctl contacts (which is only 1 by definition) is joining the cluster, not the other way around.\n. @gonace sorry, that can't be true. Adding nodes cannot remove other nodes. Please reset all nodes and follow the transcript on http://www.rabbitmq.com/clustering.html step by step. There is not a lot of room to make it even more detailed.\n. This repository is a mirror. Please ask questions on rabbitmq-users. I believe we see systemd/RHEL 7 reports a couple of times a month and they almost always end up being SELinux port permission issues.\n. This repository is a mirror. Please post your questions to rabbitmq-users.\nIt should be\nCONFIG_FILE=/etc/rabbitmq/rabbitmq\nas mentioned on http://www.rabbitmq.com/configure.html.\n. @jeckersb thank you.\n@simonmacmullen are you happy with having such conditional code? We have seen some complaints about how our package works with systemd recently.\n. @jeckersb would it be possible to update this PR as @simonmacmullen suggested above?\n. Thank you @jeckersb!\n. check-xref passes for me without errors on OS X and Linux.\n. @binarin @mbroadst I'd recommend using os:cmd/1 as a fallback to what we have. We'd be much more comfortable with including such change in a 3.6.x release than a complete replacement.\nWDYT?\n. Debian and Ubuntu seem to be adopting systemd as well.\n. @lemenkov not to put words into @binarin's mouth but I think the issue is that sd_notify support currently requires a separate RPM. Using os:cmd/1 is a zero-dependency option.\n. Sounds like a plan. We can ask a few sd_notify users to test things with a one-off build from master.\n. The original solution was superseded by #573 and #574 (for 3.7.0).\n. :+1: \n. Thank you.\nI can see the need for this feature but have a couple of concerns.\n- Backing queue state was exposed via HTTP API for debugging. It is not meant to be relied on by the users.\n- Throughput impact.\nDo you have any data on what kind of throughput effect this change has?\n. @dumbbell the entire VQ state is emitted to the stats DB, so yes, it is used.\n. After giving it some thought, I'm inclined to not merge this because\n- I expect it to have a noticeable throughput impact, especially when total queue length in bytes exceeds the amount of RAM.\n- If we merge it, we pretty much cannot remove internal message store stats from the HTTP API.\nI'll wait for some data to see if my guess is correct about 1. Not sure what would make me believe exposing internal message store stats for much longer is a good idea.\n. Excellent, then lets wait for some benchmark data. The consensus seems to be that we should keep BQ statistics in the HTTP API.\n. @alexethomas I'd still do some benchmarks with PerfTest, say with 100 queues, even though stats emission is not on the hot path.\n. @alexethomas -y 100 should use 100 server-named queues.\n. @alexethomas thank you. That's re-assuring.\n. We would certainly prefer tests in Erlang because no other tests currently requires the HTTP API to be enabled.\n. @alexethomas just checking (as it has been a month since the last update): is this still something you'd like to finish?\n. @alexethomas thank you, we'll take a look soon.\n. @alexethomas this PR can no longer merge cleanly with master. Can you please rebase?\n. When testing this with a queue that has messages in it and no consumers, HTTP API always returns head_message_timestamp\nas an empty string:\n\"head_message_timestamp\": \"\"\ncurl -u guest:guest http://127.0.0.1:15672/api/queues/%2F/pr-54 | python -m json.tool            \n\u2026\n{\n    \"arguments\": {},\n    \"auto_delete\": false,\n    \"backing_queue_status\": {\n        \"avg_ack_egress_rate\": 0.0,\n        \"avg_ack_ingress_rate\": 0.0,\n        \"avg_egress_rate\": 1.0165876629440466e-10,\n        \"avg_ingress_rate\": 0.9991605609953158,\n        \"delta\": [\n            \"delta\",\n            \"undefined\",\n            0,\n            \"undefined\"\n        ],\n        \"len\": 94,\n        \"next_seq_id\": 166,\n        \"q1\": 0,\n        \"q2\": 0,\n        \"q3\": 0,\n        \"q4\": 94,\n        \"target_ram_count\": \"infinity\"\n    },\n    \"consumer_details\": [],\n    \"consumer_utilisation\": \"\",\n    \"consumers\": 0,\n    \"deliveries\": [],\n    \"disk_reads\": 0,\n    \"disk_writes\": 0,\n    \"durable\": false,\n    \"exclusive_consumer_tag\": \"\",\n    \"head_message_timestamp\": \"\",\n    \"idle_since\": \"2015-04-10 22:53:18\",\n    \"incoming\": [],\n    \"memory\": 142904,\n    \"message_bytes\": 0,\n    \"message_bytes_persistent\": 0,\n    \"message_bytes_ram\": 0,\n    \"message_bytes_ready\": 0,\n    \"message_bytes_unacknowledged\": 0,\n    \"message_stats\": {\n        \"deliver_get\": 163,\n        \"deliver_get_details\": {\n            \"rate\": 0.0\n        },\n        \"deliver_no_ack\": 163,\n        \"deliver_no_ack_details\": {\n            \"rate\": 0.0\n        },\n        \"publish\": 257,\n        \"publish_details\": {\n            \"rate\": 0.0\n        }\n    },\n    \"messages\": 94,\n    \"messages_details\": {\n        \"rate\": 0.0\n    },\n    \"messages_persistent\": 0,\n    \"messages_ram\": 94,\n    \"messages_ready\": 94,\n    \"messages_ready_details\": {\n        \"rate\": 0.0\n    },\n    \"messages_ready_ram\": 94,\n    \"messages_unacknowledged\": 0,\n    \"messages_unacknowledged_details\": {\n        \"rate\": 0.0\n    },\n    \"messages_unacknowledged_ram\": 0,\n    \"name\": \"pr-54\",\n    \"node\": \"rabbit@ubuntu\",\n    \"policy\": \"\",\n    \"recoverable_slaves\": \"\",\n    \"state\": \"running\",\n    \"vhost\": \"/\"\n}\nThe same happens with rabbitmqctl:\n./scripts/rabbitmqctl list_queues name messages_ready head_message_timestamp\nListing queues ...\npr-54   181\nI'm publishing messages as transient, pr-54 is non-durable.\nam I missing something?\n. @alexethomas thank you, I'll give it another go tomorrow.\n. @ash-lshift yes, I plan on giving it another try soon.\n. I still can reproduce the blank value issue with a very basic Ruby script:\n``` ruby\nconn = Bunny.new; conn.start\nch = conn.create_channel\nq = ch.queue(\"issue-54\", durable: true)\npublishes over default exchange\nloop { sleep 0.5; q.publish(\"\", persistent: true) }\n```\n\u03bb ~/ curl -u guest:guest http://127.0.0.1:15672/api/queues/%2F/issue-54 | python -m json.tool\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  1140  100  1140    0     0   688k      0 --:--:-- --:--:-- --:--:-- 1113k\n{\n    \"arguments\": {},\n    \"auto_delete\": false,\n    \"backing_queue_status\": {\n        \"avg_ack_egress_rate\": 0.0,\n        \"avg_ack_ingress_rate\": 0.0,\n        \"avg_egress_rate\": 0.0,\n        \"avg_ingress_rate\": 1.510160092604313,\n        \"delta\": [\n            \"delta\",\n            \"undefined\",\n            0,\n            \"undefined\"\n        ],\n        \"len\": 40,\n        \"next_seq_id\": 40,\n        \"q1\": 0,\n        \"q2\": 0,\n        \"q3\": 0,\n        \"q4\": 40,\n        \"target_ram_count\": \"infinity\"\n    },\n    \"consumer_details\": [],\n    \"consumer_utilisation\": \"\",\n    \"consumers\": 0,\n    \"deliveries\": [],\n    \"disk_reads\": 0,\n    \"disk_writes\": 40,\n    \"durable\": true,\n    \"exclusive_consumer_tag\": \"\",\n    \"head_message_timestamp\": \"\",\n    \"incoming\": [],\n    \"memory\": 176760,\n    \"message_bytes\": 0,\n    \"message_bytes_persistent\": 0,\n    \"message_bytes_ram\": 0,\n    \"message_bytes_ready\": 0,\n    \"message_bytes_unacknowledged\": 0,\n    \"message_stats\": {\n        \"disk_writes\": 40,\n        \"disk_writes_details\": {\n            \"rate\": 2.0\n        },\n        \"publish\": 40,\n        \"publish_details\": {\n            \"rate\": 2.0\n        }\n    },\n    \"messages\": 40,\n    \"messages_details\": {\n        \"rate\": 2.0\n    },\n    \"messages_persistent\": 40,\n    \"messages_ram\": 40,\n    \"messages_ready\": 40,\n    \"messages_ready_details\": {\n        \"rate\": 2.0\n    },\n    \"messages_ready_ram\": 40,\n    \"messages_unacknowledged\": 0,\n    \"messages_unacknowledged_details\": {\n        \"rate\": 0.0\n    },\n    \"messages_unacknowledged_ram\": 0,\n    \"name\": \"issue-54\",\n    \"node\": \"rabbit@mercurio\",\n    \"policy\": \"\",\n    \"recoverable_slaves\": \"\",\n    \"state\": \"running\",\n    \"vhost\": \"/\"\n}\nSo, simply publishing a few messages doesn't make the new field non-null.\n. OK, there's one obvious this I've missed: my messages didn't have a timestamp set on them. Things seem to work as expected now.\n. @alexethomas @ash-lshift merged. Thank you for contributing!\n. Thank you for providing background on this.\nCertain things related to flow control can be made tunable. These will be advanced options but I see the need for this for some users.\nWe'll discuss what can be made tunable  and whether disabling flow control entirely is a feature we'd feel comfortable shipping, even as an option.\n\nOn 26/2/2015, at 17:09, Peter Dyson notifications@github.com wrote:\nWe have seen flow control kick in when the server itself is under no real stress. It has much more resources available yet rabbitmq has throttled the publish rate.\nThe workload pattern we're servicing is when generating a large (tens of millions of messages) backlog in a large somewhat prolonged burst to then be processed slowly over time, but while having the work queued quickly and reliably.\nWe want the ability to do this at a decent flow rate in order to process the backlog over time as it gets through it. Our servers can easily take a larger backlog at full speed, but rabbit will throttle the incoming publish rate down with flow control.\nIt would be good to have better ways to control this behaviour to either delay/extend the time or queue sizes before this flow control kicks in, or have it flow control slowly/incrementally or in an extreme case, disable flow control all together and rely only on the high watermark to prevent server death and protect stability.\nOur testing is showing flow control is triggering when there is no serious load on any of memory, cpu, disk IO at the time.\nWhat are some thoughts around this?\n\u2014\nReply to this email directly or view it on GitHub.\n. @patmanh #143, #227, #351.\n. I believe #143 largely addresses this, as do related improvements, in particular #227 and friends, and #351. Collecting metrics about flow control and making it more fine-grained are still left to be done but they are generic improvements not related to tuning.\n. @geekpete the combination of lazy queues with queue limits is the only thing we've seen reported. Well, that and #514 which will ship in 3.6.1.\n. @techmng questions belong to rabbitmq-users. \"routed\": false simply means you message wasn't routed to any queue, not that there was an error of any kind.\n. @carlhoerberg thanks so much!\n. The section referenced in this issue has been superseded by a dedicated guide.. Fixed in https://github.com/rabbitmq/rabbitmq-common/commit/bffededc34d5db793988ee13f5bdb2ffeee3a9e4.\n. We decided to expend this in scope and list items (e.g. queues) that were unavailable or seem stuck.\n. Given ongoing community requests for this feature, we will prioritise this. #61 may be resurrected and included into 3.5.x.\n. First improvements landed in #181, will be in 3.5.x.\n. Yes, this is a good question whether we should have per-element timeouts. I don't think this is what our users would expect. \"Global\" timeouts are much more common in similar tools.\n. Also note that as far as ops engineers are concerned, it's the overall operation timeout that they care about most.\n. @Ayanda-D relying in rabbitmqctl output to be sorted is wrong, so if anybody does that, that's too bad. Don't worry about ordering. We've never gotten any feedback on the sorting behaviour.\n. @Ayanda-D sounds good to me.\n. Changing this to effort-medium because this turned out to be much more time consuming in practice.\n. Also relevant: #396.\n. Fixed by #361.\n. Done.\n. Archive bug tracker has 26443 which fixes this for Debian packages, fixed in ad004f9bb669ce139bb435b09b3c4ec87d8fbaf1.\n. This belongs to rabbitmq/rabbitmq-auth-backend-ldap, please file it there.\n. Closing after 90 days of inactivity.\n. Sorry, this is not specific enough. Adding what exactly to what file? (feel free to submit a pull request, by the way)\n. To clarify: RabbitMQ node names are configurable via environment variables. However, there are tools and environments in which node name is either hardcoded or has to be shared. The OCF script is in our own repo, which is why this issue is file here. In some other cases, the limitation has to be eliminated elsewhere.\n\n@dumbbell did #189 address this in any way? If not, can we review and commit the suggestion from the mailing list (which I believe was never submitted as a PR after all)\n. Given that our OCF script was completely overhauled by @bogdando and @dmitrymex, I think this should be closed now. If you have more specific improvements in mind, please file separate issues for the new version.\n. @dumbbell perhaps you mean https://github.com/rabbitmq/rabbitmq-website/commit/735c99268a1b619b14e45cd94b12a25dade05b1c.\n. @dumbbell is this ready for QA? I don't see a pull request for it.\n. I QA'ed this branch yesterday. Both PRs are now merged => closing.\n. It would still be a minor improvement if we ignore file deletion failures and log them as errors. At least there is some chance that the file store may continue working as a whole. If not, the net result for the user is the same as what we have at the moment.\n. Also, Cassandra has 2 disk failure handling strategies to choose from: stop (the node stops itself) and \"best effort\" (keep going). We could introduce something similar.\n. Some clarifications on how we want it to work:\n- rabbit.disk_failure_policy controls how I/O errors are handled, with two modes initially: ignore, die\n- rabbit.disk_alarm_policy controls how disk alarms are handled, with two modes: block_publishers, die\nThe former modes are going to work they way they do today.\n. @Ayanda-D rabbitmqctl wait is used in various packaging scripts, we can't just disable it. We could not reproduce it on 12.04 either.\n. I'm afraid it doesn't depend on just the OS (#91 was affecting a CentOS user who had a custom kernel, for example). Improving error message is never a bad idea but the issue really is about rabbitmqctl wait producing false negatives, and Debian package installation/upgrade failing as a result.\n. Matching on {queue, reason} and having a counter makes sense. Thanks.\n. Ready for another review:\n- x-death events are now identified by {queue, reason}\n- events now have a counter\n- The docs are updated accordingly\n- Channel#queueBind/2 was taken out\n. I've renamed the field, removed the unused import and did some refactoring (introducing rabbit_basic:header/{2,3} and inlining).\n. Why would we include version number in the name if there's already a version column? What's the point of having the version column then?\n. OK, so the argument seems to be \"deployment automation\". Given how poor the automation story is on Windows, I'd not treat it as silly.\n. I QA'ed this yesterday. Works as advertised, the change is what I expected it to be.\n. Thank you. One thing that stands out immediately is that we should be able to keep the existing API compatible, e.g. keep start_link/0 which uses a default name.\nWe'll take a closer look later.\n. stable. Most  small non-breaking changes can go into stable.\n\nOn 14 oct 2015, at 13:20, D Corbacho notifications@github.com wrote:\n@michaelklishin This is solved in https://github.com/rabbitmq/rabbitmq-server/tree/rabbitmq-server-84\nThe original function wasn't exported, so it doesn't break any previous API. Do you want the pull request on stable or master?\n\u2014\nReply to this email directly or view it on GitHub.\n. Fixed in #360.\n. It is not \"enclosed in brackets\". x-death is an array of dictionaries, each with a collection of routing keys.\n. Please see this mailing list thread. We do have a repository on Package Cloud now.\n. Marked as Enhancement because such redirection is not part of the core, so I'm not sure if this can be considered a bug.\n. Specific features are debatable. I've filed this as an indication that we'd like to support CoAP (and more protocols) in the future.\n. Relevant: https://github.com/gotthardp/rabbitmq-coap-pubsub.\n. @michaelplaing please post this to rabbitmq-users or file an issue so that @gotthardp can see your feedback ;)\n. Fortunately this particular API area is very small (1 or 2 functions people care about most).\n. Closing as there seems to be little interest in having one more module. Lets assume rabbit_access_control is our public API for the time being.\n. Fixed in #397.\n. We haven't found the root cause specifically for this version but this doesn't seem to be coming up any more.. Please direct questions to RabbitMQ mailing list, we use issues for questions.\n. @willejs we are pretty busy with other things. It should be very realistic to ship this with 3.6.0.\n. @willejs that said, #128 is something we can finish by early may. Might give you some options.\n. @willejs this is not getting any attention yet for one reason: we are a tiny team. It's still on the roadmap for 3.6.0 and we'll see if we can squeeze it into the list of things for July.\n. @willejs it will not be in 3.6. However, now that we've moved to erlang.mk for the build system, introducing new dependencies is trivial, so this becomes easier.\n. There were logging changes in Erlang/OTP 18.1, which 3.7 will likely require, that make me wonder if Lager is really necessary for this to work.\n. @hairyhum see the comment on rabbit_file:append_file/2.\n. @hairyhum apparently the change dates back to 59d0dcc5d511570a066ec823b2c1fdbed04e808e. Also note that rabbit_file:append_file/2 actually can perform rotation, despite the name.\n. @hairyhum note that we can log something after rotation, e.g. \"opening a new log file after rotation\". error_logger config settings are not commonly used and there is an alternative, so breaking that is fine.\n. @hairyhum can it be overridden?\n. @dumbbell @videlalvaro @essen what are your thoughts on using a Lager fork to get more informative crash reports (with stack traces)?\n\n@hairyhum can you be more specific what the \"some cases\" are?\n. We can live without stack traces in the regular log but SASL log really could benefit from having stack traces. I'm not a fan of using forks but if that's what we have to do, I'm fine with it.\n. This is on its finish line. As part of this, we will pipe the majority of log entries (everything but Erlang process crash reports) to rabbit.log and get access to the debug level, which is quite verbose.\nSee #491 for more details.\n. - Using node-amqp or amqp.lib? (the former must be avoided as it is abandonware)\n- What is in log files?\n- What does your code look like?\n. You are using presence_exchange. It needs updating for 3.5.0.\n. Also, please seriously consider switching away from node-amqp. It is has known bugs and is not maintained.\n. Plugins sometimes need updating as internal RabbitMQ modules they may use change.\nnode-amqp creates connections storms, does not respect max number of channels limit, has internal race conditions, etc. Some hosted RabbitMQ providers outright block it.\n. The erl call that fails is supposed to force start epmd (if needed) because of certain platform-specific issues. So this is completely harmless.\n. More feedback from the same user suggests it may happen even without mirroring.\n. Nothing in the logs suggests there are any node failures or network partitions going on (sadly, I cannot post the log).\n. Attempts to reproduce this with a 2 node cluster so far were unsuccessful. the cluster in question has 8 nodes.\n. More updates: the issue magically goes away after a period of time, sometimes 30 minutes, sometimes a few hours. We have ingress queue message rate charts for 3.4.4 vs. 3.5.0, will post them if allowed to do so.\n. @noahhaon thank you.\nCan you plase post a few more charts for the same intervals, if available? \n- Disk I/O\n- Delivery rates to consumers \n- Egress network traffic (to consumers) \n- CPU utilisation\n. Another data point: the cluster has HiPE enabled on Erlang 17.x.\n. @noahhaon can you please provide some details about an environment we can use to reproduce?\nSpecifically:\n- Is 1 mirror sufficient?\n- What's network bandwidth? \n- If a small producer and consumer(s) can be used to reproduce, can you please post the code?\nI've been unsuccessful in reproducing this so far with 2 development machines.\n. @noahhaon does it happen to you when the queues are mostly empty or only when they back up? Is it common for the queues to have millions of messages in your environment? Have you observed high disk I/O on any of the nodes?\n. One node cannot keep up with the other (e.g. due to paging out a lot of data to disk) and channel that may send it messages gets blocked (no idea why it may last for minutes, though). Previously it would simply result in a higher RAM use on one of the nodes.\nWe'll see if we can make mirroring flow control optional for 3.5.2 as I'm afraid it may take a while to reproduce.\n. No, what has changed in 3.5.0 has little to do with publisher confirms.\n. They are unacknowledged messages delivered to consumers, in the cr record.\n. @noahhaon I understand that but this is the most likely change that may cause processes to become blocked. We've seen stuck processes with exhausted credit in the other report of something very similar.\n. @noahhaon full logs from all nodes, if possible. Thank you.\n. I've shared a tarball with a fix for #114 (https://github.com/rabbitmq/rabbitmq-server/commit/0b91553bafde0e063714b9557a1a76dc5213163b) with @noahhaon. Waiting to see if disabling gm flow control makes things return back to normal.\n. @RonTsai thanks. There will be a way to disable inter-node flow control until we can find out what the root cause is exactly. Yes, disabling it would mean that mirrors can fall back behind the master (the case with 3.2.1 you are mentioning).\n. @RonTsai I don't see any email on your GitHub profile. How do I contact you?\n. #114 was merged into stable and will be in 3.5.2. It will allow those affected to go to the 3.4.x behaviour (no flow control between master and mirrors) if they choose to do so.\n. Uploaded a 3.5.1.114 build: https://drive.google.com/folderview?id=0B12DYRM8lqNodi1nYXoxUDVoSmc&usp=sharing.\nYou can disable inter-node flow control by setting rabbit.mirroring_flow_control to false in the config.\n. I'll close this with an explanation.\nAs of 3.5.2, it is possible to disable inter-node flow control. It does cause flow control to kick in more often but the real issue was that flow control status reporting was very confusing: things were reported as \"in flow\" if they were in flow at any moment in the last 5 seconds. But the flow state is not permanent, it goes on and off many times per second under load.\nSome real improvements (for throughput) here would be:\n- Collecting a % statistics for flow control, much like we have for consumer utilisation\n- Make flow control more fine grained\n- Make mirroring involve fewer nodes by switching to a different algorithm, e.g. a flavour of Paxos. The one we have makes re-delivery in case of node failures a lot easier to reason about but is really inefficient because it involves nodes that may not even host mirrors for the queue.\nThe 2nd one is quite hard. We are working on 1 and 3, hope to have noticeable improvements by 3.6.0.\n. @dallasmarlow mind providing some background/context on this?\n. No pressure :)\n. @dallasmarlow limiting the interval to 4 minutes sounds reasonable. Thank you!\n. @dallasmarlow unless more people ask for GC intervals being configurable, I'm fine with what we have.\nOne improvement I realise we need to is collecting GC interval metrics. While not nearly as important as on the JVM, it's still something we should expose to operations.\nWe may cherry pick this to 3.5.x if @dumbbell and @videlalvaro have no objections.\n. @rade yes, makes sense, I'll file an issue.\n. Answering such questions would be a lot easier if GC related metrics were available.\n. I highly doubt this change can affect event stats collector in any substantial way. Please stop speculating.\n. There are many issues with the event collector that lead to it not keeping up with the load but this has nothing to do with that. https://github.com/rabbitmq/rabbitmq-management/issues/41 provides much more information.\n. @noahhaon except that I did not speculate and this is not the only change in the 3.6.0 cycle. Background GC runs every 30, 40, up to 240 (with this change) seconds while event collector has several GC runs per second under load.\nI don't see any point in continuing this conversation. Interested parties should watch https://github.com/rabbitmq/rabbitmq-management/issues/41 and read comments there.\n. Yes, we can reduce the adjustment ratio or bump the max period to a few more minutes. Either way it's hard to make a sensible decision without any data.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. Please ask questions on the mailing list.\n. Looks like we've found the root case: #118.\n. Overriding LOG_BASE in rabbitmq-env.conf works as expected, including for stream redirection.\n. Effort: medium because it will involve updating client libraries.\n. I'm happy to do the client libraries part of this issue.\n. A quick glance at what changes will be required makes me think it should only go into 3.7.0.\n. That was the plan, more or less.\n\nOn 6 abr 2016, at 16:42, Daniil Fedotov notifications@github.com wrote:\nWe can use client-properties field in connection.start_ok frame to specify connection name. \nWe already report client properties in management and can just change formatting for connections table in management.\nAnd client libraries can introduce API for setting this client property as \"connection name\" or just use existing API to set it.\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly or view it on GitHub\n. Sure but in addition to the TCP tuple-based names we have today.\nOn 6 abr 2016, at 19:06, Daniil Fedotov notifications@github.com wrote:\nWe can also change log messages to support client provided names.\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly or view it on GitHub\n. I should expand on the comment above.\n\nWe cannot completely replace the connection name(s) we have for several reasons:\n- TCP tuple-based ID are used to close connections or get more specific information on them. We absolutely cannot break this for existing systems.\n- User-provided names are not guaranteed to be unique. TCP tuple-based names are (per node).\n- #500 still uses TCP tuple-based names heavily.\nSo the user-provided name, when present, should be listed in a separate column (and on the connection page), not replace the name we have. It will be a useful \u2014 and entirely optional \u2014 piece of information, not a new connection identification scheme.\n. One thing about using client properties that I'm of two minds on is that it's going to work on a per-connection factory basis in the Java and .NET clients. Perhaps this can be seen as a quirk of those two clients, as Ruby, Erlang, Node, Go clients all should effectively have it on a per-connection basis due to how their APIs are structured.\n. Yes, looking at the code, that should be about as hard as it was for custom executors.\n. As an additional piece of information, perhaps.\n\nOn 31 may 2016, at 15:30, Alexey Lebedeff notifications@github.com wrote:\nShould this custom name go in logs?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Please post your ideas and suggestions to rabbitmq-users.. Sorry, I forgot about the base branch.\n. Cherry-picked to stable as I think this can safely go into 3.5.x.\n. standalone-3.5.1.106/ ./sbin/rabbitmqctl eval 'rabbit_misc:otp_release().'\n\"17.5\"\n. @videlalvaro this no longer merges cleanly. Please rebase.\n. Cherry-picked to stable.\n. I think this should be fixed on the case-by-case basis, as there is no socket in the direct client's case. So this is the wrong repo to report this.\n\nWe can either try using a \"fake socket\" record, or try passing the actual client socket, or pass more info in extra client parameters. So belongs to the Erlang client and MQTT/STOMP.\n. Commented what I'd do on the list. Thank you for looking into this!\n. Should be fixed by #110, given that the relevant plugins are updated or already support the new record.\n. Fixed in 341d1a2b3b2abf79bae27853a994073b04cbf42f.\n. Fixed in #355.\n. If we've merged the two improvements discovered on our end, should this be closed?\n. Pivotal folks: also see original Bugzilla bug: 26527, first reported in VESC-335.\n. It seems to be that epmd fails to start because it fails to parse abbreviated IPv6 addresses. Will investigate more but at least for now, this may be an issue we can't fix.\n. Thanks. Not just clustering but also our own code. But it's good to know about that project.\n\nOn 16 oct 2015, at 19:32, Vladimir Eremin notifications@github.com wrote:\nI found that https://github.com/yandex/inet64_tcp fixes clustering over IPv6 if you talking about it.\n\u2014\nReply to this email directly or view it on GitHub.\n. Fixed by #391, thank you, @dcorbacho.\n. Exchanges are just routing tables that are always replicated to all nodes.\n\nSo you'd like to control which node queue masters are declared on via strategies? Currently it's the node the client is connected to.\n. OK, I believe my understanding is correct then. Makes sense, thank you!\n. @Ayanda-D all vhosts.\n. Fixed by #217, https://github.com/rabbitmq/rabbitmq-test/pull/1, and https://github.com/rabbitmq/rabbitmq-website/pull/32.\n. \u2026and https://github.com/rabbitmq/rabbitmq-test/pull/2.\n. 26463 is fixed in 3.5.0. The title says\nMake ha-mode=exactly start up new mirrors when old ones go down\nSo what is left to be done? Ensuring we don't have more than N mirrors at any given time?\n. @Ayanda-D yes, the probability of this is quite low. Given that we will be moving to Raft after 3.6.0, this is indeed a fair question. I'll discuss this with the team.\n. @gsogol we completely agree that it should work correctly 100% of the time. Our question really is, should we try to solve this with an ad-hoc protocol (something we're moving away from) or after we introduce Raft into the core.\n. @gsogol thanks for the feedback! We'll postpone this until after we move this part to Raft. This may be 3.6.0 or 3.7.0.\n. There's more than one partition handling strategy. 2 of them involve dropping client connections. In autoheal's case, the minority nodes will re-sync from the majority after they re-connect.\nIs that not what you're asking for?\n. Ok, this is something we definitely plan on doing.\n\nOn 20/4/2015, at 18:31, gsogol notifications@github.com wrote:\nSo the main ask is to union or merge the 2 clusters when autoheal happens. You can't just discard data. These message could have million $ orders. I'd hate to lose them :)\n\u2014\nReply to this email directly or view it on GitHub.\n. This is being addressed in two ways:\n\n\nQuorum queues (there's a more recent talk, this has been merged in master for 3.8)\nExperimental new schema data store (mentioned in this update from RabbitMQ Summit 2018). This RabbitMQ Summit talk.. There are no mailing list discussions but both Ra and Mnevis are developed in the open.. We already support moving messages within a cluster with the Shovel plugin. If you have Shovel and Shovel management plugins enabled, it's on the queue page (not very visible, that's true).\n. I understand. We already have this for intra-cluster copying which uses a one-off Shovel. We'll see what may be missing.\n\n\nOn 20/4/2015, at 18:34, gsogol notifications@github.com wrote:\nThe request is about a UI plugin. I know about shovels and federation. There are scenarios where you just tell the admin or in devops world a developer and s/he just goes in and manually moves the messages. There are many cases where you want to move messages from a DLQ back to a specific exchange to be processed because you fixed a bug on the subscriber side and want those messages replayed.\n\u2014\nReply to this email directly or view it on GitHub.\n. We think dynamic Shovels and the \"Move messages\" UI feature that works within a cluster are sufficient and no major pieces are missing.. None of the messages in your log snippet are related to connections, only broker startup and termination.\n\nPlease use rabbitmq-users for questions.\n. Looks like 25c8e772c9005c9af60b59929d74c499475dd6db also addresses this one?\n. :+1: \n. After discussing this with @videlalvaro we believe this may be an Erlang release-specific. Other versions we have tried to not throw badarith when we run\nerlang\nmath:exp(-1162.102134881488).\nThe user is running Solaris.\n. @videlalvaro please do that. We'll keep this issue open until we get a response if 17.x works any better.\n. The issue turns out to be 32 bit Erlang on a 64 bit OS.\n. @mattmaniflaf please report it to the Erlang/OTP team. math:exp/1 is a standard library function.\n. We cannot move this flag out anywhere for 3.5.x. This is a way for some users to go back to the old 3.4.x behaviour. Cross-node flow control effects are difficult to predict, as you can see in #114. There wasn't enough loading testing as part of bug 26527: in fact, if you take a look at the test suite, flow is disabled there.\nWe don't advertise this setting anywhere. This affects a couple of high profile users. I'm strongly in favour of having this option for 3.5.x. The current gm won't be around forever either, so we can make this permanent.\n. After discussing it via chat it looks like we can move this flag to channel's record while preserving #delivery. This is a good idea.\n. @videlalvaro ready for another round.\n. Several high profile customers want the 3.4.x behaviour back. This is a change that's compatible with 3.5.x. I strongly feel this should be in 3.5.2, otherwise we'd have to ship snowflake builds.\n. To reproduce:\n- Create a 2 node cluster\n- Fill up a durable queue with persistent messages (say, 10+ GB)\n- Stop and reset the mirror\n- Re-add it back\n- Monitor RAM use on the master node\n. @dumbbell worth delaying 3.5.2 for if you think we can fix this in under a week.\n. It makes little sense to me to have a config value for this. This is what the users probably expect to be \"always on\". What kind of breaking changes are you worried about?\n. Taking the liberty to merge this as we've discussed this and I'd like to avoid chaining branches that revolve around credit_flow.\n. That's why we have an issue for adding a ratio. If alarm-driven flow is not in affect, the UI should not emphasize credit flow because it is transient and not displayed accurately with the current approach.\nYou should still notice resource-driven alarms just as easily with this change.\n\nOn 7/5/2015, at 18:02, Jean-S\u00e9bastien P\u00e9dron notifications@github.com wrote:\nOne question before I merge the branch. IIRC, the management UI is refreshed every 5\": could this change lead to connections/channels to appear as always running, even if they were in flow in the past 5\"? I mean, could we end up in the opposite situation: currently it gives the impression connections/channels are always in flow, but with the change, it gives the impression they are never in flow?\n\u2014\nReply to this email directly or view it on GitHub.\n. @videlalvaro renamed.\n. The ultimate problem is that it can be dangerous: e.g. we can't know what channel the next message is on, so if we let the reader read and frame it, what do we do if it is for the blocked channel? The next message can also push us past the RAM watermark limit. Plus credit_flow already does buffering.\n\nSo this is something that'd need a lot of investigation. Making credit flow values configurable is the first step to make experimentation straightforward.\n. It can be a different process: a channel, including on a different node.\n. We've discussed if sender can be moved out of the record as it is not used in multiple places. We can but this is not a particularly high priority for now.\n. Note that we may be able to do it for 3.5.x if we cache the values in process dictionary. For 3.6.0, we'll use state records and do more refactoring as needed.\n. I'm labeling this as 3.6.0 because I'm not sure if I should cherry-pick to stable. If that's the plan, let me know or do it, then change the milestone here.\n. @videlalvaro we can introduce config validation later. This is not an option we will heavily advertise, only to selected users who've expressed interest.\n. I'm not convinced this falls into the effort-low category, considering the ramp-up effort around credit flow that might be required.\n. You are running into flow control/resource alarms that block publishers.\n. @binarin that sounds good but I'd add \"but no fewer than 64\".\n. @binarin combined with the \"no fewer than 64\" condition above, it sounds good to me. It's unlikely that there will be significantly more schedulers configured than there are logical cores. We might want to cap the maximum number, say, at 512 or 1024, too.\n. I think the right thing to do is to fold existing x-deaths by {queue, reason} pairs before trying to do anything with the counter.\n. Test case: https://github.com/rabbitmq/rabbitmq-java-client/pull/56.\n. @videlalvaro ready for another round. I'll re-format with Emacs once we agree on everything else.\n. Sorting is not sufficient: we may have duplicates in the input that foldl would still iterate over, so they need to be checked. We can avoid an O(n) check by keeping a set of {queue, reason} pairs in the accumulator.\n. Thank you, we will take a look.\n. That's the best default value I can think of for Windows. And we don't hear requests to make rabbitmq-env.conf overridable on nix, so it should be alright.\n. This looks good but there is one difference from rabbitmq-env.conf on nix: you can use NODENAME instead of RABBITMQ_NODENAME in the env file and it will be picked up. I'd like to keep this consistent on Windows.\nI'll re-assign to @dumbbell, who has a deeper knowledge of the scripts and how they relate to packaging than I do.\n. @Jakauppila please go ahead. The right thing to do is to let the contributor make changes based on feedback :)\n. I will take this because there is something closely related that I will test for 3.5.4.\n. Looking into this today as it is a good idea to merge this before #208.\n. Cherry-picked to stable. Thank you.\n. So far I could not reproduce this but a user could with 2 clients (.NET and Python). So there may be something broken.\n. libpcap capture (abridged) provided.\n. Note: this is when the server is running on Windows (as is the publisher).\n. The OP has started another list thread. Not much new information there but should be pointed out.\n. Yes, eventually a blocked publisher will see a socket write timeout exception, this is expected.\n. @rjrizzuto in a word: yes.\n. @dumbbell I'm inclined to think it's something in the reader state machine as I believe it can be reproduced (not as easily but still) with earlier versions. We've had reader changes in 3.5.0, too, to avoid logging connections that sent no data (e.g. load balancer aliveness checks).\n. While other processes and back pressure can affect the reader, the state machine should not enter this incorrect state on valid input. So flow control may be a red herring, the real issue is having multiple frames in a TCP segment.\n. They either block on write(2) or catch EWOULDBLOCK and re-try.\n. @dumbbell we should let @gmr know and fix 0.9.x as it is the version our tutorials use. Plus this is something we've recommended to the OpenStack community.\n. Not just in Pika but in multiple clients then. We should fix it everywhere (by re-trying or even sending all AMQP 0-9-1 frames as a single byte sequence?)\n. @rjrizzuto thank you for your input. The plan after a discussion we had internally is to\n- Send all frames in a single byte array (buffer, string, or similar, depending on the client)\n- Use select(2) to check if a write will succeed (or something similar)\nThe tricky part is telling exceptions that require re-trying vs. those that indicate a dead socket (and thus need to be propagated to start connection recovery).\n. Yes, the .NET client needs logging. I don't know about logging really nitty-gritty details such as this one (ideally we should make things safe regardless, so there's no point in logging it) but suggestions on what we should use for logging are appreciated. Feel free to file an issue for the .NET client!\n. Please move this into a separate issue in rabbitmq/rabbitmq-dotnet-client. We are familiar with logging levels, much less so with what logging library is the most appropriate for a small, 0 dependencies library like our client.\n. Thanks. I'd check amqp.node, Go amqp , and Haskell's Network.AMQP, too.\n\nOn 15/6/2015, at 18:47, Jean-S\u00e9bastien P\u00e9dron notifications@github.com wrote:\nMore tests with more client libraries:\nPika's master branch is fixed.\nBunny (Ruby) is fine too.\n\u2014\nReply to this email directly or view it on GitHub.\n. Yes, we understand what the issue is. Will fix it for .NET after s few other .NET issues are merged.\nOn 15/6/2015, at 19:11, rjrizzuto notifications@github.com wrote:\nHave you had any luck finding the issue in the .Net client? I only tested with Python to see if it was the client lib or the message broker, and apparently that was a red herring anyway.\n\u2014\nReply to this email directly or view it on GitHub.\n. @dumbbell @rjrizzuto I think .NET client is ready for another round (on both stable and master). It now only performs a single flush per basic.publish frame set. Please give it a try and let me know if that's sufficient.\n. If a socket write times out, there will be an exception. But it will be obvious what's going on, unlike what's described above.\n. Blocking or retrying is the plan (this is what other clients do).\n. For those looking to help us test a fix, here's a build from a recent commit on the stable branch.\n\n(if you are paranoid and need a confirmation that indeed it was produced by the RabbitMQ team, contact mklishin in the Pivotal domain and I'll confirm)\n. I'll close this as we've evaluated the issue and what clients it affects, and if the fixes are not sufficient, new issues should be filed for the affected clients.\n. @rjrizzuto I will email you the .dll. Building from source is also straightforward: there are documents on how to do that in the repo.\n. I'm going to file a separate issue for the specific client.\n. https://github.com/rabbitmq/rabbitmq-dotnet-client/issues/108. Unless we can reproduce it quickly it will be for 3.5.5.\n. @greggwon please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Memory monitor is started by rabbit_alarm\nwhich is started later than file_handle_cache, so if the FHC needs to go read\na lot of data from disk very early on, there's a race condition.\n. Fixed in #219.\n. rabbit: start_fhc/0 is used by a different boot step that now depends on the one for rabbit_alarm.\n. rabbit_log depends on error_logger, there is no boot step for it.\n. @videlalvaro that \"shouldn't reference any rabbit stuff\" comment presumably means that FHC could be a standalone library: nothing in it in theory is RabbitMQ-specific. Unfortunately, this means it completely ignored VM memory watermark, which leads to OOM killer killing the VM when a node with a lot of data on disk boots. So this conceptual purity has a very real drawback.\nI think we should remove the comment.\n. The comment on rabbit:start_fhc/0 comes from 07ea1d8f2b9e88f965a41b94f8cee2ead78162c0, which was part of bug24998. I see no specific reasons for FHC to not be referencing \"any rabbit stuff\" mentioned there, the discussion is largely about the separation of concerns, how to not log the same thing twice, and how alarm handlers should interact with the logger.\n. > rabbit_alarm calls into rabbit_log\nOnly when an actual alarm is triggered, which does not normally happen on boot.\n\nwhen the exchanges are ready, it means kernel_ready is there, that might be the reason why\nrabbit_alarm was depending on kernel_ready.\n\nWe can add separate boot steps for memory and disk monitors.\n\nAFAIK, all the files in our repo that don't start with rabbit_* should be independent from the rabbit source code\n\nThis should have been brought up in #134 and has little to do with this PR\n\nI don't see why the rabbit_memory_monitor function required by file_handle_cache cannot be passed as a fun\n\nIt can be. This PR is about something else.\n. See #152, released in 3.5.2.\n. This PR is against master. Is it meant to be against stable?\n. https://github.com/rabbitmq/rabbitmq-server/issues/248 is now being a part of this.\n. Thanks @Ayanda-D. I will do more research and get back to you.\n. @binarin spawning a new process that could be used for calls means you have a natural race condition that will screw up ordering.\nDelegates primarily exist to avoid doing N identical cross-node calls when a single message is routed to N queues on a remote node. The whole thing may cease to exist when we move to Raft. We can perhaps reimplementing parts of it but any substantial rework would seem like (mostly) a waste of time.\n. delegate certainly has similarities with rpc. Both eventually use the same thing under the hood for multi calls, gen_server.\ndelegate uses a pool of processes (16 by default) on every node and hashes the caller (typically a queue or channel process) to find out what process on the other end should perform function application. rpc uses a single process locally registered as rex. This is hardcoded in the rpc module.\nFor calls specifically, as @binarin points out, on the calling end (RPC client, \"first half\" in RabbitMQ Erlang client speak), there is no real difference as the middleman process in rpc that collects late responses is gone by the time rpc:[multi]call returns. However, on the receiving end (RPC server), we are limited by a single named process in rpc and a configurable pool in case of delegate.\nMost other differences are probably a lot less important.For a system such as RabbitMQ, where there are normally many queues and processes, to me it makes sense to use a pool.\n. @Ayanda-D sorry, I re-read your comment about re-implementing delegates using proc_lib and not sure I quite get what specific cases make you worried. Can you cover a few scenarios in more detail, please?\n. @binarin I find it hard to believe that 1 hardcoded RPC handler process will offer better throughput than 16 (or generally, N) processes. Delegates are used by more than one channel in parallel in 99% of realistic workloads. I have a feeling this has been ignored entirely in this conversation.\nIn any case, this issue is not about improving performance, and the title makes that very clear. Not sure what makes you believe it is.\n. @Ayanda-D OK, I see your point. I think timeouts should throw exceptions in the caller, and the delegate handler processes should not terminate. To draw a parallel with rpc, imagine what would happen if the rex processes terminated due a timeout ;)\n. I have no objections to making delegate (more) asynchronous but it should be a separate issue.\n. @Ayanda-D fair enough. Delegates are almost exclusively used to interact with queues, so I'm going to close this for now. Possibly a better solution will emerge in the future. Decisions are still progress.\n@binarin please feel free to submit a PR with the delegate throughput improvements you have in mind. Keep in mind that we are not looking to introduce delegate timeouts any more, at least for now.\n. I will cherry-pick to stable => milestone 3.5.7.\n. @hairyhum we should mention the backup directory, even if things failed (I think, feel free to disagree). The idea is to answer the most common question people have when they see that message: where can I find the backup dir?\n. Moved to https://github.com/rabbitmq/rabbitmq-web-stomp/issues/13. We are aware of the limitations.\nChances are we'll hire someone to work on this soon.\n. Fixed in #209, will also be in 3.5.4.\n. Looks good. I'll rebase to stable to reduce the distance between master and stable.\n. OK, giving it a second thought, this may be a pretty unexpected change for plugins. Will rebase back to master.\n. Sounds good. We also may choose to group commands at some point, namespacing is a good idea for plugins.\n. Fixed in #215.\n. #178 should probably be resolved first.\n. Fixed by #212.\n. If you need to add a header when a message is routed, it should be one of the easiest plugins: a custom exchange type (that otherwise mimics one of the existing ones). If you specifically want to do that when a message is actually delivered, this requires changing RabbitMQ core and is moderately difficult.\n. We already add headers when dead-lettering. We never modify the body or remove user-provided headers other than BCC.\n. @c-datculescu tracing effectively doubles internal message rate. It's not a very good idea for most cases (outside of development environments).\n. Yes, the number of x-death events is now capped as of 3.5.2, see #78 and #152. \n. Note: overriding +A using RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS works better but this issue seems still relevant to me.\n. Merged into stable without a PR.\n. In fact, this is partially the case on Windows already.\n. We decided to keep the process limit intact.\n. The issue seems to be with argument ordering. The last value wins, so SASL logger ends up always logging to a file, for example.\n. Try 3.5.4 RC1?\n\nOn 11/7/2015, at 22:29, Scratch notifications@github.com wrote:\nAny quick workaround for this? Or better wait for 3.5.4?\n\u2014\nReply to this email directly or view it on GitHub.\n. One of the potential fixes is https://github.com/rabbitmq/rabbitmq-server/commit/0802e872c22aabfe8c65aa30beb7f2adec359d52. We'll check one more time before 3.5.4 final.\n. @bassrob you need to make the same change as in https://github.com/rabbitmq/rabbitmq-server/commit/0802e872c22aabfe8c65aa30beb7f2adec359d52 and re-apply config changes before restarting the service.\n. No worries! We've learnt something new about PHP in the process :)\n. Cherry-picked to stable.\n\nThank you, @weisslj.\n. Log level should be warning, not warn. Have you altered log levels in the config? Please post rabbitmqctl report output.\n. Apologies, I meant to say that the expected log level is warning, not warn:\nerlang\nlevel(debug)   -> 4;\nlevel(info)    -> 3;\nlevel(warning) -> 2;\nlevel(error)   -> 1;\nlevel(none)    -> 0.\n. We are not ready to require R16B03 just yet. R13B03 is still the minimum supported version.\n. OK, if it's just a warning then it shouldn't affect CI. Let's give it a shot.\n. If queue master is not available, operations currently will wait for it to come back (unless the queue is not durable, in which case it will be relocated to another node). Use mirroring (then master will be moved) or non-durable queues. There's an issue for introducing timeouts in multiple places, too.\n. @rjrizzuto it is.\n. This function will be used only when you want to reduce RAM use of a node. I think invalidating caches on mirrors is OK. The caches may or may not be around next month, too.\n. This looks OK to me. I don't see your CA in contribute@rabbitmq.com, can you send it to me directly? mklishin in Pivotal domain.\n. Got it.\n. os:timestamp/0 is available even in R13B03, the oldest release we support.\n. @marcelog we are not opposed to this change (in fact, we did exactly that in a few place already), just need to the potential risk here some more. @dumbbell will cast his opinion quite soon.\n. Thank you!\nWill cherry-pick to stable.\n. > Is there a way to set memory limit through RabbitMQ configuration?\nYes, and it is mentioned in our production checklist.\ncgroups is an OS-level feature and RabbitMQ can't be aware of it unless the OS can notify us somehow. If there is a way to get such notification, let us know and we would consider it.\nPlease ask questions on the mailing list in the future.\n. @robsonpeixoto please post your questions to rabbitmq-users.. This was brought up as a somewhat related issue when discussing #1223.\nWe have file a new issue since there were no objections and the scope seems small enough: #1224.\nAlso related: #207.. #1234 introduces a way to configure total available RAM amount via config file.. @videlalvaro @dumbbell thoughts? I think we should make it possible to do this in a plugin rather than adding it to the core.\n. Fixed in #378, thank you @dcorbacho!\n. Fixed in #209.\n. It is: it's still not possible to override 'rabbitmq-env.conf' location IIRC.\n. > On 20 feb 2016, at 5:14, Tomoyuki Saito notifications@github.com wrote:\n\nI'm guessing you mean that we cannot override CONF_ENV_FILE variable by setting a corresponding environment variable like RMQ_CONF_ENV_FILE, right?\n\nYou understanding seems correct.\n. There were recent changes in this area, so chances are, this is already possible or we are close to it. We need to investigate first.\n. Thank you, @hairyhum.\n. Submitted against the wrong branch.\n. @binarin like in https://github.com/rabbitmq/rabbitmq-server/issues/151?\n. @Gsantomaggio good idea.\n. Thank you, @ash-lshift!\n. Can you please provide a script that reproduces the issue?\n\nOn 7/7/2015, at 15:24, Alvaro Videla notifications@github.com wrote:\nThanks, I think I've found the reason, here:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_dead_letter.erl#L148\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_dead_letter.erl#L156\nWe assume that the <<\"count\">> field is long therefore the case clause failure.\n\u2014\nReply to this email directly or view it on GitHub.\n. We understand what the issue is but not what causes it. Can someone post a small sample that reproduces the issue? We will turn it into a test case.\n. @tiredpixel thanks. A more specific question: do your apps set the count header? Or is it only ever touched by RabbitMQ on dead-lettering?\n. @tiredpixel thanks. We do have tests that result in a message being dead-lettered multiple times. I have a couple of ideas. 3.5.4 won't be released without a fix for this.\n. @riyad perfect, thank you!\n. I can reproduce the issue. Looking into it.\n. Cheers.\n. @riyad I have a fix, does this output confirm correct execution? https://gist.github.com/michaelklishin/97355747f6b85dfe3aae \u2014 the exception is gone.\n. I see\n\nrabbitmqctl list_queues\nListing queues ...\ntest.issue_216_crashing_queue   3\nafter 3 runs, so it must be working as expected.\n. \u03bb ~/Development/RabbitMQ/debug/ rabbitmqctl list_queues\nListing queues ...\ntest.issue_216_crashing_queue   0\n\u03bb ~/Development/RabbitMQ/debug/ python server-216.py\nconnecting ...\nsetting up ...\npublishing ...\nfetched message: {'body': 'foo', 'exchange': 'amq.topic', 'promise_number': 7, 'routing_key': 'test.issue_216', 'message_count': 0, 'headers': {'x-puka-delivery-tag': 1}, 'redelivered': False, 'delivery_tag': 1}\ndeclaring timeout queue toq-gen0-test.issue_216_crashing_queue\npublishing to toq-gen0-test.issue_216_crashing_queue: {'body': 'foo', 'exchange': 'amq.topic', 'promise_number': 7, 'routing_key': 'test.issue_216', 'message_count': 0, 'headers': {'x-puka-delivery-tag': 1}, 'redelivered': False, 'delivery_tag': 1}\nfetched message: {'cluster_id': '', 'empty': True}\nfetched message: {'body': 'foo', 'exchange': 'amq.topic', 'promise_number': 11, 'routing_key': 'test.issue_216_crashing_queue', 'message_count': 0, 'headers': {'x-puka-delivery-tag': 2, 'x-death': [{'count': 1L, 'exchange': '', 'time': datetime.datetime(2015, 7, 8, 10, 53, 56), 'queue': 'toq-gen0-test.issue_216_crashing_queue', 'reason': 'expired', 'routing-keys': ['toq-gen0-test.issue_216_crashing_queue']}]}, 'redelivered': False, 'delivery_tag': 2}\ndeclaring timeout queue toq-gen1-test.issue_216_crashing_queue\npublishing to toq-gen1-test.issue_216_crashing_queue: {'body': 'foo', 'exchange': 'amq.topic', 'promise_number': 11, 'routing_key': 'test.issue_216_crashing_queue', 'message_count': 0, 'headers': {'x-puka-delivery-tag': 2, 'x-death': [{'count': 1L, 'exchange': '', 'time': datetime.datetime(2015, 7, 8, 10, 53, 56), 'queue': 'toq-gen0-test.issue_216_crashing_queue', 'reason': 'expired', 'routing-keys': ['toq-gen0-test.issue_216_crashing_queue']}]}, 'redelivered': False, 'delivery_tag': 2}\nfetched message: {'cluster_id': '', 'empty': True}\nfetched message: {'body': 'foo', 'exchange': 'amq.topic', 'promise_number': 15, 'routing_key': 'test.issue_216_crashing_queue', 'message_count': 0, 'headers': {'x-puka-delivery-tag': 3, 'x-death': [{'count': 1L, 'exchange': '', 'time': datetime.datetime(2015, 7, 8, 10, 53, 56), 'queue': 'toq-gen1-test.issue_216_crashing_queue', 'reason': 'expired', 'routing-keys': ['toq-gen1-test.issue_216_crashing_queue']}, {'count': 1, 'exchange': '', 'routing-keys': ['toq-gen0-test.issue_216_crashing_queue'], 'queue': 'toq-gen0-test.issue_216_crashing_queue', 'reason': 'expired', 'time': datetime.datetime(2015, 7, 8, 10, 53, 56)}]}, 'redelivered': False, 'delivery_tag': 3}\ndeclaring timeout queue toq-gen2-test.issue_216_crashing_queue\npublishing to toq-gen2-test.issue_216_crashing_queue: {'body': 'foo', 'exchange': 'amq.topic', 'promise_number': 15, 'routing_key': 'test.issue_216_crashing_queue', 'message_count': 0, 'headers': {'x-puka-delivery-tag': 3, 'x-death': [{'count': 1L, 'exchange': '', 'time': datetime.datetime(2015, 7, 8, 10, 53, 56), 'queue': 'toq-gen1-test.issue_216_crashing_queue', 'reason': 'expired', 'routing-keys': ['toq-gen1-test.issue_216_crashing_queue']}, {'count': 1, 'exchange': '', 'routing-keys': ['toq-gen0-test.issue_216_crashing_queue'], 'queue': 'toq-gen0-test.issue_216_crashing_queue', 'reason': 'expired', 'time': datetime.datetime(2015, 7, 8, 10, 53, 56)}]}, 'redelivered': False, 'delivery_tag': 3}\n\u03bb ~/Development/RabbitMQ/debug/ rabbitmqctl list_queues\nListing queues ...\ntest.issue_216_crashing_queue   1\n. So the culprit seems to be the fact that the script modifies headers (see the filter_headers function). That's why our existing test suite didn't catch this and this is client-specific.\n. Not Puka but I could only trigger this with Puka, it serializes integers differently from the Java client (e.g. JVM distinguishes between integers and longs, and many dynamically-typed languages use automatic promotion on overflow).\n. OK, a decent fix is in #221. We'll release 3.5.4 RC very soon and if new edge cases are found, will add them to Bunny's test suite (one of the client test suites we use).\n. @videlalvaro that would require extracting validators into rabbit_misc or similar. We are under pressure to release 3.5.4 and the clusterer, so we'll do what you suggest for 3.5.5.\n. If the user modifies x-death events then it counts as a user error. I'm not sure what we can do if \"count\" is something other than a numerical value. Ignore it entirely?\n. restarting the node should make it possible to delete the queue before it hits the same exception again. The right thing to do is to upgrade.\n\nOn 15 mar 2016, at 3:41, Ram notifications@github.com wrote:\nhi,\nI am using Rabbit MQ 3.5.4 and experiencing the same issues ?\n=ERROR REPORT==== 14-Mar-2016::20:04:20 ===\n* Generic server <0.1073.0> terminating\n* Last message in was {'$gen_cast',init}\nundefined,undefined,undefined,undefined,0,running}\n* Reason for termination == \n* {{case_clause,{<<\"x-death\">>,longstr,\n<<\"[{reason=expired,\nCan you pls suggest me how i can delete the queue ? I am not able to delete this queue from Admin Console.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub:\nhttps://github.com/rabbitmq/rabbitmq-server/issues/216#issuecomment-196640135\n. @pdoreau please post an code snippet that reproduces it to rabbitmq-users, our public mailing list\n. They were in the moderation queue, I approved them.\n. The x-death value is an array of objects (in JSON terms), so we need to compare what your app is sending. It is certainly possible for apps to confuse RabbitMQ by sending invalid modified headers, we have seen this several times before.\n. Here's the rabbitmq-users thread and so far it looks like the client or one of the libraries on top unintentionally modifies the x-death header which is meant to be modified only by RabbitMQ itself. In any case, this is not the same issue.\n. @Ayanda-D is this ready for another round of reviews?\n. @Ayanda-D thanks. I immediately see that the argument name is not what is in the docs. We should rename it to x-queue-master-location-strategy or x-queue-master-locator (\"locator\" is already used in a few places in the code).\n\n@dumbbell which one of the above do you prefer?\n. @Ayanda-D this looks good. I will continue with various functional tests but there's one last change we'd like to see: lets rename the queue-master-location header to x-queue-master-locator and rabbit.queue_master_location config key to rabbit.queue_master_locator. This is more to the point and also what module names already use.\nThank you.\n. Renaming the header and config key aside, this LGTM.\n. Submitted against the wrong branch.\n. Thanks. Can you post logs of all nodes or send them to me? Replace IPs with generic 192.168.*.* ones, for example.\nHow did you produce a list of stuck processes?\n. What about 3.5.4 RC1 (see GH releases)?\n\nOn 14/7/2015, at 18:51, John Eckersberg notifications@github.com wrote:\nAlso this doesn't seem to be a recent thing, I can reproduce with 3.3.5 as well.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thank you, we need to take a closer look.\n. @jeckersb thanks, we indeed have seen (and fixed just recently) issues that are heisenbug race conditions. It's a good thing this is easier to reproduce with local VMs.\n. Note that we've started using Raft for mirroring, which in turn means queue master election. Once we get that part to be reasonably mature, we'll look into using Raft in more places. So the above plugin does not solve every possible consensus-related problem. Rather it is Raft applied to one specific area.\n. Yes, that repo is private a.t.m. Sorry :)\n. We should not ship compiled .beam files. This has bitten us in the past. If @jeckersb isn't willing to build from source, we should produce a one-off release.\n. @jeckersb thank you for the update. A fix should appear in tomorrow's nightlies and 3.5.5 (there will be at least 1 release candidate for it, too).\n\nThank you for your help. And major props to @dumbbell for digging it out.\n. @aungkoko96 please post your question to rabbitmq-users, RabbitMQ's public mailing list. RabbitMQ log directory location is documented.. @aungkoko96 your question has nothing to do with this issue. Please post it to rabbitmq-users.. Fixed by #229.\n. @gmr adding to our discussion yesterday, can you try reducing rabbit.queue_index_max_journal_entries (2 to 4 times, for example) and see what kind of difference that may make?\n. We have a few updates on this that other team members will post.\nOne outcome of this is that the QI journal size default is quite excessive. We should lower this as it makes throughput much more predictable. It also would lower the time window in which messages embedded into QI may stay in RAM only (but lets not digress).\nAs a result, the plan is for 3.5.5 to reduce QI journal size and make another related value configurable (and lower). For 3.6.0, we are looking into various ways to parallelise folding over the sparse array of entries or avoiding it entirely if we can.\n. While #283 is merged, we have a few other optimisations coming, so we'll leave this open for some more time.\n. Note that this issue is closed but we have a bunch of others (primarily #289) that are related and continue improving things in the same area. They too will be in 3.5.5.\n. Related: https://github.com/rabbitmq/rabbitmq-server/issues/291 (will be in 3.5.5).\n. Fixed in bde0a08bc3d80b4df7a9f5a78c97d69c05338240.\n. Should we log when buffering is enabled/disabled? For debugging/support purposes.\n. Please ask questions on rabbitmq-users.\n. Correct.\n. eheap_alloc: Cannot allocate 229520 bytes of memory (of type \"old_heap\").\nis a runtime error. We can't do anything about it. Disk monitor termination should not affect the rest of the server. I feel these messages are completely unrelated to each other.\nSo, what can we do here?\n. I'm not sure what that means.\nDisk monitor does not bring the VM down. We have plenty of users that are examples of that.\nRabbitMQ cannot bring the OS down. The error in the log comes from the VM unable to allocate memory (known issue resolved in 17.x).\nThe user falsely associates the two errors.\n\nOn 22/7/2015, at 18:50, Alvaro Videla notifications@github.com wrote:\nI think we need to preven the broker from bringing the whole system down, even more after something so trivial like running the disk monitor\n\u2014\nReply to this email directly or view it on GitHub.\n. @gmr in some cases other plugins won't be able to start. Some may be crucial (e.g. rabbitmq-clusterer or rabbitmq-autocluster). I agree that we should not terminate without a sensible error message, of course.\n. You haven't installed delayed exchange for the new version.\nOn 25 sept 2015, at 8:29, Alexandr N. Zamaraev notifications@github.com wrote:\nBOOT FAILED =========== Error description: {error,{\"no such file or directory\", \"rabbitmq_delayed_message_exchange.app\"}}\n. After giving it some thought, I don't think simply copying the plugin archive is going to be safe in all cases: plugins can be no longer compatible with the new version. So this needs to be considered. Perhaps we need to give plugins a way to indicate RabbitMQ version support ranges, which is a can of worms of its own.\n. :+1: \n. @legoscia FYI.\n. Both 403 and 404 are channel-level exceptions.\n\n530 is the best fit for those cases. We'll evaluate if changing this is feasible: many (if not all) client libraries may need updating.\n. I don't think it would, server-sent errors are\na part of the protocol.\nYour suggestion sounds good, please submit a PR.\n\nOn 29 oct 2015, at 5:01, Ayanda-D notifications@github.com wrote:\nWould adding a handle_exception clause to rabbit_reader for such AMQP errors, avoiding the thrown exception, and sending #'connection.close'{reply_code = 530, reply_text = <<\"not allowed\">>, ...} back to the client before closing the socket be a way around this? or will this be violating amqp?\n. Configuration and environment variable files are not created by default.\n. Will cherry-pick into stable manually.\n. Please post questions to rabbitmq-users and provide server logs as well as rabbitmqctl report output and ulimit -a one.\nOn 29/7/2015, at 20:24, Jason Harrelson notifications@github.com wrote:\nAfter upgrading a production environment to v3.5.3 from v3.5.1 that has been running RabbitMQ since January of this year with no issues, we began to have issues with the system. Specifically, immediately upon upgrading and starting rabbitmq-server, the service would not stop when issued the service rabbitmq-server stop command. As a result, the service would have to be killed, which sometimes resulted in the corruption of the mnesia DB.\nAdditionally, if the system were allowed to run for a period of 6 or more hours, some of the subscriber processes would eventually report a Failed to create thread: Resource temporarily unavailable error. We also observed sluggishness in the other services running on the same host as the rabbitmq-server process. However, the memory usage was within the expected normal range.\nWe downgraded to 3.5.1 and restarted all services. The same results outlined in the prior paragraphs were observed overnight. The next morning, we attempted to stop the service and it would not stop. Thus we issued a kill command and the mnesia DB became corrupted yet again. Our next step was to delete the /var/lib/rabbitmq/mnesia directory and start the rabbitmq-server service again. At this point we adjourned to consider a path forward. When we returned to the site the next morning, we observed that rabbitmq-server and all queues were performing as expected. Additionally, issuing a stop command now works as expected.\nIt seems that some kind of bug or issue was introduced between 3.5.1 and 3.5.3 that is making some artifact in the /var/lib/rabbitmq/mnesia directory cause issues for rabbitmq-server.\nWe are running on Redhat Linux 6.5.\n\u2014\nReply to this email directly or view it on GitHub.\n. > On 29/7/2015, at 20:24, Jason Harrelson notifications@github.com wrote:\nsome of the subscriber processes would eventually report a Failed to create thread: Resource temporarily unavailable error\n\nThe fact that your consumer failed to allocate a thread and eventually the system stabilized suggests the consumer app had to use a higher that usual number of threads. RabbitMQ cannot influence this and it uses fewer than 100 threads by default (unless you have 70 or more cores).\nOnly logs can help understand what was going on.\nrabbitmqctl eval \"rabbit_diagnostics:maybe_stuck().\" will list Erlang processes that are potentially stuck.\n. There is no evidence that this is a bug. Most of such reports end up being environment-specific problems.\n\nOn 29/7/2015, at 21:22, Jason Harrelson notifications@github.com wrote:\nI am most certainly willing to take this course of action, but can you tell me why you consider this a question and not a bug report?\n\u2014\nReply to this email directly or view it on GitHub.\n. @Jakauppila if you intend your fixes to be in 3.5.x, make sure the PR is submitted against stable.\n. I'm assuming this is intended for 3.5.5 => closing expecting it to be re-submitted against stable.\n\nIf I'm wrong, just leave a comment.\n. Queue process, not the server.\n. :+1: \nVery good find @priviterag @dumbbell.\n. This explains why I wasn't able to reproduce it some of the time: I was on the stable branch.\n. _compat all the things!\n. That sounds good to me.\n. We've evaluated this and the real issue lies in delegate:call/2 which has the timeout of infinity. All its callers I could find are in the rabbit_amqqueue module, so we can introduce delegate:call/3 and apply timeouts on a case-by-case basis.\nIdeally to propagate the specific timeout reason to the client, we'd need to introduce a new protocol status code that would mean \"a timeout has occurred\".\n. Naive retries can break more things than they'd fix. Using Raft (and thus having an actual replicated log of operations) is the right solution.\n. Publisher confirms are sent only after a message has been replicated. Using casts at least makes this significantly trickier, turning parts of the system to AP earlier than we intend to do so. Calls are fine as long as they have a sensible timeout.\n. @binarin I'm not sure how using a cast would solve what you consider the root cause. This issue is about having timeouts and evaluating what kind of unforeseen side-effects that might bring.\n. The above assumes nothing can go wrong with a cross-node call. I wouldn't be so optimistic. Not having timeouts is an opinionated idea that has been proven to be wrong over and over in RabbitMQ history.\nWe are investigating a couple of things that block processes, e.g. #581, too. Regardless of what we find and fix elsewhere, calls must use a sensible timeout.\n. We've been hearing this exact argument for a good couple of years. What happens in practice is that when a queue/node/entire cluster lock up, technical ops never inspect things to provide us any info. They kill things left and right to make the system functional again. If anything, having a sane finite timeout would serve as a circuit breaker.\nAfter discussing this with @Ayanda-D we decided to use a value of net_ticktime + 10 seconds. This way DOWN messages have a good chance of being handled first but channels, and, in turn, connections and possibly entire nodes (e.g. during shutdown) do not lock up completely.\n. A more thorough call site inspection suggests that delegate:call/2 is not used on the critical path (maybe in request/response heavy systems):\n- queue.delete\n- stats emission\n- queue.purge\n- message re-queueing\n- basic.get (the closest it gets to the critical path)\n- basic.consume\n- basic.cancel\n- Manual mirror sync and sync cancellation\nbasic.deliver, basic.ack, basic.reject, and QoS settings changes use delegate:cast/2. Therefore it should be fine to load the timeout from application config.\n. We don't argue with the fact that there are other issues that are simply revealed by this problem. In fact, in the last week we've discovered at least 3 of them, resolved in #581, #541 (we have a partial fix on a branch for rabbit_channel_sup), and more. Some are the consequences of a known mnesia_locker bug(s).\nBut this needs to be addressed as well.\n. This specific issue is fixed. Closely related: #166.\n. This was answered on rabbitmq-users \u2014 please use the list for questions.\nMessages that never go to the message store (can be delivered to a consumer immediately) are not prioritised. This is documented.\n. Discussed solutions:\n```\n1) establish some UUID for the channel-queue pair, generated by the queue the first time it hears of a channel and returned by the queue when acking. If a queue gets some acks from a UUID it has never seen before, it ignores them.\nDownside: complicated.\n2) Make each acktag into a UUID and have the queue ignore unrecognised ones.\nDownside: expensive (probably).\n3) Have the channel forget acktags when it sees a queue go down.\nDownside: not obvious it completely eliminates the problem if the queue sees the channel down before the channel sees the queue down.\n4) Just ignore unrecognised acktags.\nDownside: still possible to have incorrect behaviour even if no crash.\n```\nOption 2 may be worth it for the next feature release. 3 and 4 may work as temporary workarounds that do not change inter-node communication protocol. The work on switching to Raft will open up additional options. \n. So https://github.com/rabbitmq/rabbitmq-server/pull/496 implements option 4. We may try a variation on 1 in the future, or even make internal ack transfer a part of the Raft log (once that is integrated).\n. previous_upgrade_failed suggests RabbitMQ previously couldn't upgrade its database. Since this is a Homebrew-provisioned node, you can simply remove the database directory and the node will start.\nPlease ask questions on rabbitmq-users in the future.\n. I can no longer reproduce this exception.\n. Why does Ranch filter out TLS depth? It is for sure used by some of our users.\n. @essen excellent, then lets make sure we extend Ranch to accept all ssl_options known and go for it. Keep in mind that the pool is used by plugins such as STOMP and MQTT. They don't do anything sophisticated, just provide more data points for testing.\n. @essen are you sure you mean #122? It seems unrelated to Ranch (or client connections).\n. @essen thank you! @dumbbell should we put this on hold until you finish erlang.mk work?\n. I did some fairly extensive testing and none of the minor regressions I could find were Ranch related. Thank you, @essen!\n. What version do you run?\n. Please give 3.5.4 a try.\nHow can we reproduce this issue?\n. This particular exception doesn't appear in any of our issues (including the legacy tracker). So this may still be relevant. We need a way to reproduce.\n. So looks like an Erlang message that's supposed to be sent to a rabbit_amqqueue_process process ends up being sent to a mirror one, which doesn't expect it. So whatever rabbit_diagnostics:maybe_stuck(). produces is a red herring.\n@jingHW besides a way to reproduce this, can you post\n- full logs (post them to gist.github.com)\n- rabbitmqctl report output (it can be fairly extensive, so using gist.github.com is also a good idea) \u2014 feel free to edit out sensitive information\n. @baoyonglei correct. I'd move this discussion to the mailing list since there is no evidence that this is a bug (we use issues for actionable items).\n. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. No immediate plans but we are not opposed to implementing it some day.\n. @pycuichen I'm not sure how your original issue requires message grouping. Consumers can have priorities these days, you can use channel QoS to alter how consumers are picked.\n. @pycuichen please post general questions to rabbitmq-users. I'm going to remove comments that have nothing to do with the issue.\n. No update. It's not on the roadmap for 3.8.0 at the moment.. This feature is not on the roadmap for 3.7 and likely won't be on the roadmap for 3.8. We don't need any more +1s as they contribute very little to this \"discussion\" at this point. We will unlock this as there are specific details as to how message grouping might work and why.. Sadly for non-technical reasons we probably won't be using Package Cloud as much as we'd like to. We'll release 3.5.4 and likely keep the most recent release available there.\n. Not at the moment. We are considering hosting our own vs. using services such as Package Cloud (but not PC specifically, even though our team likes it a lot).\n. How can we make the installer do what you want?\n\nOn 12/8/2015, at 21:03, ryanzink notifications@github.com wrote:\nI currently have a large number of servers that have v3.2.4 installed. I am trying to automate the upgrade process to v3.5.4 through a scripted install. I tried using the standard NSIS silent parameter by executing the following: \n.\\rabbitmq-server-3.5.4.exe /S\nHowever, the installer has not been configured to be fully silent... I am facing the following issues:\nThe \"RabbitMQ is already installed\" dialog box appears even when the installer is run in silent mode. NSIS installers can be set up to have a default option for silent installs (examples are here: http://nsis.sourceforge.net/Examples/silent.nsi) to override the message box from appearing.\nInstalling in silent mode as an upgrade between versions does not perform the same actions that a non-silent install does. I haven't performed an exhaustive analysis, but right away a major issue is that the RabbitMQ service does not get installed after the previous version is uninstalled.\n\u2014\nReply to this email directly or view it on GitHub.\n. @ryanzink we do not know much about NSIS or Windows installer automation. If you'd contribute a PR that does the necessary changes, and a way for us to test it, we would be happy to include your work in 3.5.5.\n. Good point about ERLANG_HOME. To alleviate this, we need a way to make the installer aware that this is an upgrade and not an uninstall: is there a standard way to do that?\n. Please ask questions on rabbitmq-users in the future.\n\nYou can copy RabbitMQ's directory (ideally when the server isn't running) to back it up, then try upgrading on another machine from the copy, making sure that node name matches. Also keep in mind that the node will try to contact the nodes it has seen online if this is in a cluster.\n. No milestone because we may or may not have enough time to get it right before 3.6.0.\n. Closing in favor of https://github.com/rabbitmq/rabbitmq-server/issues/487 (which is the same thing, just scoped).\n. I think this can be closed, we haven't seen this in a while since #465/#466.\n. @erylee you can submit a pull request with your change against the stable branch. It's not very clear what your proposed change is: having a diff would be better.\n. Main open question with switching to SHA-256 is how to provide a migration path, including for users\nwho may be importing definitions.\nOne way would be to store the algorithm used in the user record and fall back to MD5 if none given, assuming it's a legacy database.\n. @freemansoft @gmr we already have a salt value. What do you mean by \"configurable\" when it comes to salt?\n. thanks @essen.\nSo, trying to narrow down the scope here:\n- We need hashing and password verification functions to be pluggable\n- We need to use sha256 by default\ndo we care about pluggable ways to generate salt values? In master we already use local time and a unique integer: is this good enough?\n. @noahhaon thank you for the input!\n. PR submitted: https://github.com/rabbitmq/rabbitmq-server/pull/310.\n. @baiyusysu please use rabbitmq-users for questions. The only thing that has changed is that hashing functions are now pluggable, with SHA-256 being the new default.\n. @pycuichen please ask questions on the mailing list. We have ask you to do that before.\nThere is no. The right thing to do here would be to make queue deletion dead letter all messages in it, but this is potentially a very expensive operation, so we haven't decided on that yet.\n. You can set node name. If that's not possible with Kubernetes, this sounds like a Kubernetes limitation to me.\n. Running two nodes on the same host is perfectly possible: change the part before the @. For example, some of our test suites use rabbit@hostname and hare@hostname.\n. Kudos to @nishan for submitting #275. The change looks good, so it's a matter of small technicalities to get it in.\n. @nishan I will pull your changes manually in a bit.\n. Note: this was in a packaging script and since recent OS X versions have a modern sed, this only needed testing on Linux.\n. @nishan thank you. The -E option isn't even in the sed man on Linux, although two 4.2.x version I have at hand do accept it.\nCan you please re-submit your PR against the stable branch?\n. @dumbbell FYI.\n. Closing because we want to merge this into the stable branch, which then will be merged into master.\n. Sorry, this is still submitted against master.\n. I'm testing a branch with cherry picked commits. Thank you.\n. I couldn't find what sed version Solaris 10 has.\n. @videlalvaro so since this is a shared packaging script, Solaris is not of concern here. Modern OS X versions have sed which supports -E.\n. For client-server communication, probably not: most protocols we support do not really supported compression (but the apps can compress payload however they need). For other areas, possibly but it's not a priority right now.\nPlease ask questions on rabbitmq-users.\n. After discussing with @videlalvaro, this means tweaking SEGMENT_ENTRY_COUNT, and we may want to look into making it configurable.\n. This is less relevant if we avoid the expensive fold but it is still relevant to getting data to disk more frequently. I see no good reason to have a 65K entry journal.\nWith changes from #227, the only significant throughput drops I observe with the publishers outpacing consumers test is from flow control, which is expected.\n. @videlalvaro this issue is only about QI journal size and values that depend on it. Your suggestions may be perfectly relevant with or without this change but should be discussed in a separate issue.\n. Fixed in #281.\n. @videlalvaro comment restore, I think this is ready.\n. @videlalvaro except for the two naming remarks, this LGTM.\n. Renaming is done. @videlalvaro please take a look.\n. I cannot reproduce with 3.5.4 and the following config:\nerlang\n[\n {rabbit, [\n           {tcp_listeners, [15683]}\n ]}\n].\nso the issue is environment-specific (most likely your config is not picked up). RABBITMQ_NODE_IP_ADDRESS is for configuring what IP address the node binds to, not port.\n. @sukinsky please post questions to rabbitmq-users.\n. @sukinsky what happens if you override RABBITMQ_CONFIG_FILE? There were Windows-specific environment variable-related changes in 3.5.4, however, RABBITMQ_CONFIG_FILE default seems correct.\n. @sukinsky there is nothing Windows-specific after the config is loaded. I wonder if the config file value you see in the management UI ends up being different from what is actually used, although that is quite unlikely. Have you checked RabbitMQ logs?\n. What does rabbitmqctl report output?\n. @sukinsky please use gist.github.com.\n. OK, this seems like a genuine bug. Filed #288.\n. @sukinsky I still cannot reproduce this issue. Can you clarify what exactly you do?\n- How was RabbitMQ installed?\n- What path the config file is at\n- How do you make RabbitMQ use that file\n. Thanks. I cannot reproduce with a config file overridden with RABBITMQ_CONFIG_FILE, will try RABBITMQ_BASE.\n. @sukinsky is RabbitMQ running as a Windows service?\n. @sukinsky does overriding RABBITMQ_NODE_PORT make it use the custom port?\n. @sukinsky what does\nrabbitmqctl.bat eval \"os:getenv().\"\noutput? (feel free to edit sensitive pairs out)\n. managed to reproduce. Lets take this to #288 which is a lot more focused on what the actual issue is.\n. To answer my own question: RABBITMQ_NODE_PORT works just like before (but requires RABBITMQ_NODE_IP_ADDRESS to be set) and can be used as a workaround.\n. Just leaving a note here for those not watching #288: the issue is fixed in stable and will be in 3.5.5. @sukinsky if you get in touch with me at michael in the RabbitMQ domain, I can send you a build from the tip of the stable branch to try out.\n. From #287, effective value ends up being:\n{tcp_listeners,[{\"auto\",5672}]}\ninstead of\n{tcp_listeners,[9999]}\n(where 9999 is the desired port).\n. https://github.com/rabbitmq/rabbitmq-server/commit/1f61e4fae109561572beca3f0ddc9c7624622126, https://github.com/rabbitmq/rabbitmq-server/commit/77052f42d1389caedff1f4febdb384a81535d49b,  and  https://github.com/rabbitmq/rabbitmq-server/commit/374901966f8aa6262bed374b55b844ded2eccf21 by @Jakauppila seem to be relevant (and merged after 3.5.4 was released).\n. This indeed seems to be fixed by the commits above. Will merge stable into master tomorrow or so and leave a nightly build link to try it out.\n. @sukinsky please see above.\n. @sukinsky well, I'd like to get a fix confirmation from someone running into this in an environment different from mine. Up to you, of course :)\n. @sukinsky sorry, I was referring to a comment in #287. If you contact me at michael in rabbitmq domain, I'd be happy to send you a build of the tip of the stable branch (what will be 3.5.5 fairly soon).\n. I don't think this is a blocker for 3.5.5 (given our other recent improvements) but it also doesn't have to wait for 3.6.0, so labelling this for 3.5.6, per discussion with @dumbbell.\n. Assuming this is safe for 3.5.5.\n. @greggwon it's important to remember that both operations have to follow protocol semantics. For example, there is a response for each method (queue.delete-ok and so on) and every client I can think of awaits it by default. Then queues must guarantee that all operations are applied in order. Combined with message reference counting (a message routed to N queues will not be stored N times), it gets a little more involved than doing deleting \"in the background\".\nThere are plans to revisit certain parts of the message store and queue operation logs by 3.8 and 4.0.\nFirst step towards that in 3.7.0 is per-vhost message stores: each virtual host has its own pair of message stores, so wiping a vhost can be done very efficiently.\nThis discussion belongs to rabbitmq-users.. I edited out 90% of the log because it had the same exception for every channel.\nIf stats DB cannot be reached, channels won't be able to emit stats to it. There is only one node that collects statistics and it is entirely in RAM \u2014 all of the data it has is transient by design. Enabling and disabling the plugin is unnecessary \u2014 there's plenty of threads on the mailing list that demonstrate how to make stats DB terminate with rabbitmqctl. It then will be restarted on another node.\nIt looks like the stats DB became unreachable right after it queried a bunch of channels. I'm not sure how much can be improved here.\n. 1. https://groups.google.com/d/msg/rabbitmq-users/P5g8CRI1Owg/T01ob_Bn9oAJ\n2. I don't see any evidence of a \"plugin crash\". The errors you see in the log are from channel processes that couldn't emit stats because they couldn't contact stats DB. Eventually their supervisor was restarted.\n   Even though there is a \"crash\" in the name, these are non-fatal termination and only affect a single unfortunate connection.\n. @evanccnyc there should be a rabbitmqctl command for that. To make it possible we'd have to first look into making rabbitmqctl extensible from plugins. The plan is to do that after 3.6.0.\nI'll add a note to the site, which is open source, by the way :)\n. @Ayanda-D thank you! Is this ready for QA or should we wait?\n. @Ayanda-D thank you!\n. @Ayanda-D hi, any feedback on @videlalvaro's questions?\n. @Ayanda-D when you think this is ready, please let us know with a comment here.\n. @videlalvaro this is ready for another QA round.\n. The point of this change is to produce output continuously, so that if an item fails to response in time, at least there is some output. Using a cursor wouldn't change much if we want the timeout to be \"global\" (for all items), which is what the users would expect. So I find this an improvement.\n. I understand. Again, the goal of this improvement is not to make loading more efficient \u2014 even if using a cursor would make it so, which may or may not be the case \u2014 but to provide output to the user in a more sensible way.\n. Lets not change the scope of this, please. Cursors can be added at a later point. We need to factor out as much duplication as reasonable and given that things work as expected, merge this and move on.\n. I'd keep those functions in the ctl namespace: they are pretty specific to that module's usage patterns, and rabbit_misc is already huge and stuffed with all kinds of functions.\n. I don't find structural duplication in\n``` erlang\nlist_users() ->\n    [internal_user_filter(U) ||\n        U <- mnesia:dirty_match_object(rabbit_user, #internal_user{ = ''})].\nlist_users(Ref, AggregatorPid) ->\n    rabbit_control_main:emitting_map(\n      AggregatorPid, Ref,\n      fun(U) -> internal_user_filter(U) end,\n      mnesia:dirty_match_object(rabbit_user, #internal_user{ = ''})).\n```\nto be excessive. Chances are, a version without this duplication would be more difficult to understand, not less.\n. If this is systemic, then I agree.\n. Running make lite hangs because there's an unhandled exception in a queue process:\n=INFO REPORT==== 10-Sep-2015::00:30:52 ===\naccepting AMQP connection <0.17976.1> (127.0.0.1:51142 -> 127.0.0.1:5672)\n                       {<<\"x-message-ttl\">>,signedint,5000}],\n                               <0.17958.1>,[],[],[],undefined,[],[],live},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,1441834252222868,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,running}\n** Reason for termination ==\n** {function_clause,\n       [{gb_trees,is_defined,[9,0],[{file,\"gb_trees.erl\"},{line,221}]},\n        {rabbit_variable_queue,is_msg_in_pending_acks,2,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1095}]},\n        {rabbit_variable_queue,'-betas_from_index_entries/4-fun-0-',4,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1076}]},\n        {lists,foldr,3,[{file,\"lists.erl\"},{line,1275}]},\n        {rabbit_variable_queue,betas_from_index_entries,4,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1066}]},\n        {rabbit_variable_queue,maybe_deltas_to_betas,2,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,2016}]},\n        {rabbit_variable_queue,init,7,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1179}]},\n        {rabbit_priority_queue,init,3,\n            [{file,\"src/rabbit_priority_queue.erl\"},{line,146}]}]}\n. @carlhoerberg no. It should be updated when you update the password, though. We don't want to be too smart.\n. This is ready to go. We will cover upgrades testing later when we have proper build system and testing infra for that. FWIW, this migration is very straightforward.\n. @videlalvaro I think this is ready for QA.\n. Please ask questions on the mailing list.\n1.5K messages is not a high number. There can be many reasons for what you see, from low consumer utilisation to #227 to consumer getting overwhelmed or being blocked by something.\nFeel free to try 3.5.5.RC2. Given that you use .NET client 3.5.x, it has concurrent delivery dispatch with per-channel ordering guarantees.\n. Closed in #318, will be in 3.5.5. Thank you, @ben-page!\n. @ben-page you submitted this against master, do you intend this to be in 3.6.0 only? If not, please rebase against stable.\n. What if an argument contains a quote? Are we sure we're not trying to be too smart here?\n. Please use rabbitmq-users for questions. Your Erlang lacks eldap and xmerl: see if you may need to install additional packages.\n\nOn 17/9/2015, at 4:13, fansgit notifications@github.com wrote:\nError description: {error,{missing_dependencies,[eldap,xmerl],\n. @ben-page no worries, thank you for looking into this!\n. The title isn't very specific, I'll assume this is addressed by #566.\n. See https://github.com/ruby-amqp/bunny/issues/352 for one example.\n. Should probably go into 3.5.7 or 3.5.8 because 3.5.6 will have to go out early, see #324.\n. User experience is already better thanks to #237 but I really would like to get this into both 3.5.7 and 3.6.0.\n. Changed milestone to 3.6.0 because doing the same change twice is pretty fiddly and 3.6.0 ships this month.\n. We are looking at building a new package revision. Packages other than Debian and RPM are not affected. 3.5.4 is still available from Bintray and Package Cloud.\nOn 24 sept 2015, at 19:25, Emilien Macchi notifications@github.com wrote:\nRight, we need a 3.5.6 as soon as possible.\n\u2014\nReply to this email directly or view it on GitHub.\n. Denis,\n\nIf you check the stable branch you would notice that we already did.\nWe appreciate how proactive you are but really, we are well aware of what needs to be done and why.\n\nOn 24 sept 2015, at 19:47, Denis Egorenko notifications@github.com wrote:\nWe need backport this:\n67c24aa\n\u2014\nReply to this email directly or view it on GitHub.\n. Should probably go into 3.5.7 because 3.5.6 will have to go out early, see #324.\n. Should probably go into 3.5.7 because 3.5.6 will have to go out early, see #324.\n. @dumbbell do you think our new shell test suites and ShellCheck address this or more tests are necessary?. I'd extract a function that returns OTP version as major.minor accepting a string from rabbit_misc:opt_version/0, using 16.3 for releases earlier than 17.0 (since R16B03` is the requirement for 3.6.0 and good enough for this case), then add unit tests for it.\n. Fixed in #334.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Only RabbitMQ 3.5.6+ can run on Erlang 18.x. Otherwise Erlang version requirements are documented.\n. Makes sense, thank you.\n. Different master selection strategies are already implemented in master.\nOn 25 sept 2015, at 18:11, pickettphil notifications@github.com wrote:\nSince masters (in a cluster) are selected at random and don\u2019t change for as long as the nodes are running, it is possible that one or few nodes become masters for all or most queues, thus not leveraging all nodes. It would be helpful if there was a way to tell Rabbit to elect a different master for a queue, or to rebalance across all queues, or even to rebalance automatically. The only way to force rebalancing at this time is seemingly to restart the node acting as the master for many queues. If so, this is too disruptive.\n\u2014\nReply to this email directly or view it on GitHub.\n. See #121.\n. @igreenfield this is not a support forum. Please ask questions to rabbitmq-users and search the archives, in which this trick can be found.. There are plans to introduce a way to move queue masters and perhaps do some kind of online migration, for 4.0.0 (or possibly 3.8.0, depends on how things go).. There are no specific plans about how we should approach this or much planning for 4.0 at the moment, so there is no GitHub issue. We will file one as plans solidify.. No updates besides this rebalancing script.. Is there any standard for these \"REST guidelines\"? We'd like to support OAuth 2 instead of providing something ad-hoc.\n. This doesn't work on version 18.0.3, which is an invalid argument for erlang:list_to_float/1.\n. There was but we no longer support Macports. Use Homebrew.\nOn 29 sept 2015, at 11:17, dbl notifications@github.com wrote:\nOr, is there a Macport package?\n. Fixed and will be in 3.5.6: that you, @rtraschke!\n. \u2026and @Gsantomaggio.\n. Please use rabbitmq-users for questions. Access control over routing keys is not available: use separate exchanges.\nOn 1 oct 2015, at 22:07, Tom Heinan notifications@github.com wrote:\nI'm looking for a way to ensure that a particular user (say, with user ID 123) can only publish to a set of known topics, e.g. users.123.messages. I've got a back-end service subscribed via the STOMP plugin to users..messages, and I'd like to use the destination of incoming messages to positively identify their origin, but I can only do that if I'm sure that only user 123 can write to users.123..\nI've implemented a custom authentication plugin that takes care of making sure connections are tagged with appropriate user IDs, but the problem is that check_resource_access/3 doesn't contain any routing information when publishing to the amq.topic exchange. For example:\ncheck_resource_access(AuthUser, #resource{virtual_host = VHost, kind = Type, name = Name}, Permission) ->\n    rabbit_log:info(\"~p wants to ~p on ~p ~p~n\", [AuthUser#auth_user.username, Permission, Type, Name]),\n    true.\n=INFO REPORT==== 1-Oct-2015::13:33:04 ===\naccepting STOMP connection <0.668.0> (192.168.99.1:52159 -> 192.168.99.100:6080)\n=INFO REPORT==== 1-Oct-2015::13:33:04 ===\nAuthenticated as <<\"user123\">> with tags [user,123]\n=INFO REPORT==== 1-Oct-2015::13:33:42 ===\n<<\"user123\">> wants to write on exchange <<\"amq.topic\">>\nSince I don't know what the routing key is that user123 intends to access, I can't know whether to allow or deny their write to amq.topic.\nIn rabbit_channel.erl:903, the basic.publish clause of handle_method/3 looks like it could be modified to include something like\nExchangeName = rabbit_misc:r(VHostPath, exchange, ExchangeNameBin, RoutingKey),\nif the resource record was updated to something like\nr(VHostPath, Kind, Name, RoutingKey) ->\n    #resource{virtual_host = VHostPath, kind = Kind, name = Name, routing_key = RoutingKey}.\nApologies if this isn't the right forum for this kind of issue, or if there's a better way to achieve what I'm trying to do that I have overlooked. I'm happy to give it a shot in PR form, but wanted to make sure this wasn't in progress elsewhere first.\n\u2014\nReply to this email directly or view it on GitHub.\n. Including routing key may be a fair idea but it is not a part of the exchange, so doesn't beling to the resource record. Let's continue on the list.\n. 4-6 weeks for point releases. We can produce a one-off build and post it to the list: do you need a Windows installer or binary build?\n. @codenaked can you please ping us on the original list thread about this, so that we can upload a build to it? I can't find it, sorry.\n. @codenaked OK, I'll upload a build first thing in the morning. 3.5.6 can ship any day now anyway but hey, we try delivering on what we promise ;)\n. @videlalvaro is this still relevant?\n. basic.nack with multiple set to true is probably the only realistic scenario.\n. So this changes one authz function signature but given that without it things cannot really work as advertised in the docs, I guess this can be considered to be a bug fix.\n\n@gotthardp FYI.\n. @rtraschke yes, please. That would prevent authz plugins that don't really care/rely on tags from breaking.\n. Note that we can fix this for 3.7.0 only and thus consider solutions that require Erlang 17.5+.\n. 3.6.1 sounds good. I was just mentioning it is a possibility.\n. Sounds reasonable to me (or at least as reasonable as we can make dynamic code loading ;)). Note that I'm definitely not an expert on the mechanics of dynamic code loading in BEAM.\n. Note that we can fix this for 3.7.0 only and thus consider solutions that require Erlang 17.5+.\n. @Ayanda-D do you think there's more that we can do for this than https://github.com/rabbitmq/rabbitmq-server/issues/470? or should we close this?\n. :+1: \n. It's not clear what you are trying to do. Please provide the exact steps you take and link to specific areas of the codebase if you have questions. I suggest doing that on rabbitmq-users: if there is an issue, we'll file one with a more specific description. Thank you.\n. I'll leave a note from the mailing list discussion of this. This may be https://github.com/rabbitmq/rabbitmq-server/issues/224, which was fixed in 3.5.5.\n. The above can work but it's not very user-friendly. I suggest that we develop a plugin that provides a verification function that uses a list of certificates e.g. from a directory, driven via config file.\n. @Dzol no. The goal is pretty standard. But there can be multiple ways to implement it. This is up for debate. I think whitelisting of certificates from a directory can be a reasonable way in some cases.\n. @uvzubovs do you have an opinion on how CRLs should work in RabbitMQ, from a user's perspective?\n. @uvzubovs thank you for the input.\n. I updated the description that we'll focus on certificate whitelisting (providing a trust store) first. CRL (blacklisting) may come later if there's enough interest (and we can find a way to make the two work together in a sensible way).\n. @Dzol where /path/to/them assumes a /path/to/directory, correct?\nBy the way, this doesn't have to be a plugin. If we can make it one, and compatible with 3.6.x, all the better but if we can't, I think this should then be a core feature in 3.7.0 instead.\n. @Dzol my point is that we should target (develop & test against) 3.6.x. If small core changes are necessary, they likely can go into a future 3.6.x version.\n. @biiiipy we are not looking to support Windows certificate store at first but that's certainly a sensible idea.\n. Now that https://github.com/rabbitmq/rabbitmq-trust-store is open sourced (although we don't consider it to be finished, leave alone have much community feedback on it), I think this can be closed. All new ideas belong to the trust store plugin repo.\n. This would be a breaking protocol change, unfortunately.\n. This is already possible e.g. in rabbitmq-server.bat. Good catch.\n. @dcorbacho no, we don't expect this to be a commonly used option, so this example is sufficient.\n. Thank you!\n. No, cluster_status sounds appropriate to me.\n. Fixed in #706, closing w/o milestone in favour of #688.\n. This solution is OK. head_message_timestamp/4 already doesn't assume we will always have the message loaded, which it accounts for by returning an empty string. Which is a curious choice of a value to say \"nothing\" but that's a separate issue (and present in other parts of RabbitMQ).\n. Management plugin in master then converts such values to undefined, so we'll get rid of empty strings in the future.\n. @dcorbacho a minor suggestion: when you think that something is ready for a review, assign the PR to someone (me or @videlalvaro, until we move to erlang.mk). That person will be notified and take notice.\n. Thank you!\n. @binarin also a possibility.\n. Please file a separate issue for case two.\n\nOn 22 oct 2015, at 16:15, Alexey Lebedeff notifications@github.com wrote:\nThere are 2 problems there:\non node_down rabbit_alarm notifies its subscribers with single notification with Source=[]. If instead notification will be sent for every type of alarm from down node, it will solve the case from original message.\nrabbit_alarm doesn't send information to its subscribers about which node the alarm corresponds to. As a result I was able to find another bug: set disk alarm for 2 of 3 nodes, try to pubsish message and become blocked, and after clearing disk alarm only on one node it becames unblocked. But I think in this case the publisher should be unlocked until alarms an all nodes were cleared. Fixing this will require more work: rabbit_alarm subscription protocol should be updated with node information, and node information should be saved along with alarm type in rabbit_reader #throttle{}.\n\u2014\nReply to this email directly or view it on GitHub.\n. @binarin would you be interested in looking into a PR for case 1? We'll handle case 2 because it involves protocol method changes. Both are only reasonable for 3.6.0.\n. OK, so maybe I've misread case 2. If it doesn't require a client-server protocol change, even better. However, it does change the internal protocol so can break earlier 3.5.x releases in mixed clusters => still can only go into 3.6.0.\n. A single PR that fixes both is acceptable. Thank you.\n\nNote that I'm away next week for OpenStack Summit Tokyo, so PRs submitted late this and next week may see little activity for a while.\n. @binarin sounds reasonable.\n. I believe this is addressed by #395 (n\u00e9e #381) by @binarin. Thank you!\n. @bogdando so you want this in master only?\n. We don't have CI for OCF and are happy to merge changes from you regularly. However, I suspect you want this to be in 3.5.7? Then this PR is against the wrong branch.\n. Thanks, I'll close this one then.\n. @bogdando all stable changes are merged into master. We have scripts that nag us when that's not the case.\n. @bogdando see https://github.com/rabbitmq/rabbitmq-server/commit/98fc9f7e29eb57d1e0765420301db9d321657684, which is in master.\nThank you for your work on this!\n. :+1: \n. @dumbbell can it be the same as #267?\n. @videlalvaro so we'd still keep the queue.declare arguments key prefixed with an x-? So far I've updated the docs to only mention setting locator via policies, to not have to explain this difference.\n. @videlalvaro https://github.com/rabbitmq/rabbitmq-server/pull/380, https://github.com/rabbitmq/rabbitmq-test/pull/6, https://github.com/rabbitmq/rabbitmq-website/pull/98 are ready. The queue.declare argument key is unchanged.\n. #380 is no longer relevant, see https://github.com/rabbitmq/rabbitmq-server/pull/390. https://github.com/rabbitmq/rabbitmq-test/pull/6 has been updated.\n. @videlalvaro friendly reminder that this is awaiting QA.\n. If only the docs are incorrect, shouldn't this be in rabbitmq-website?\n. This seems like a natural part of #369, so closing.\n. A fix contributed by @binarin will be in 3.5.7.\n. Please try with 3.5.6 on 64 bit Erlang and re-upload the results.\n. There are two process crash reports that are known:\n=CRASH REPORT==== 20-Oct-2015::16:11:20 ===\n  crasher:\n    initial call: gen:init_it/6\n    pid: <0.1023.0>\n    registered_name: []\n    exception exit: {{badmatch,{error,{\"d:/Dropbox/workspaces/customer-repository/routing-compiler-os/dev/rabbits/db/node2-mnesia/queues/7YGRMMI7HBTA5ZFF529N6D48J/journal.jif\",\n                                       eacces}}},\n                     [{rabbit_queue_index,reset_state,1,[]},\n                      {rabbit_variable_queue,reset_qi_state,1,[]},\n                      {rabbit_variable_queue,purge_and_index_reset,1,[]},\n                      {rabbit_variable_queue,delete_and_terminate,2,[]},\n                      {rabbit_priority_queue,delete_and_terminate,2,[]},\n                      {gen_server2,terminate,3,[]},\n                      {proc_lib,init_p_do_apply,3,\n                                [{file,\"proc_lib.erl\"},{line,239}]}]}\n      in function  gen_server2:terminate/3 \n    ancestors: [<0.1022.0>,rabbit_amqqueue_sup_sup,rabbit_sup,<0.756.0>]\n    messages: []\n    links: [<0.1022.0>,<0.1024.0>,#Port<0.22555>]\nis https://github.com/rabbitmq/rabbitmq-server/issues/341 and is Windows-specific, seemingly fixed in 3.5.6 (at least we haven't heard about it from Windows users ever since).\nThe other one is https://github.com/rabbitmq/rabbitmq-server/issues/267. I see no other errors and SASL logs seem to be empty.\nClosing as a duplicate of https://github.com/rabbitmq/rabbitmq-server/issues/267.\n. @smee thank you, your reports are informative and easy to work with, as usual :+1: \n. Thank you! This looks good. We have a fairly non-trivial PR landing in the same area in the near future (see #303) but there's less overlap than I expected.\nCan you please rebase this against stable and re-submit? This can go into 3.5.7.\n. @videlalvaro it's not a breaking change and it is very small, why not ship it earlier?\n. @videlalvaro we've been shipping small features in earlier 3.5.x releases, including some that are arguably larger than this one.\n. @dumbbell do you have an opinion on this?\n. @binarin no need to rebase anything: we can cherry-pick to stable if we decide to do so. Thanks again for your contribution.\n. @dumbbell we never tried to support cross-version rabbitmq-server and rabbitmqctl. But this change is hardly breaking: it only adds a new stats info and output column.\n. @binarin thank you. Works like a charm.\n. Cherry-picked to stable.\n. Thank you!\n. Fixed in #376, thank you @Gsantomaggio.\n. Followed-up with https://github.com/rabbitmq/rabbitmq-server/issues/447.\n. Any alarm should behave the same way. We should not clear the alarm if there is at least 1 node online that is in alarmed state. That has been the intent all along.\n. PRs mergeable into master:\n- https://github.com/rabbitmq/rabbitmq-server/pull/395\n- https://github.com/rabbitmq/rabbitmq-common/pull/1\n. Fixed in master. Thank you, @binarin.\n. Needs to be resubmitted because of erlang.mk.\n. @binarin thank you. Due to traveling for OpenStack Summit this w/e and the actual conference next week I'll have little time to look into it until late next week.\n. @binarin with on going contributions from you (which we greatly appreciate!) we need to ask you to sign RabbitMQ Contributor Agreement and email the signed document to contribute in RabbitMQ domain when you have a moment. Thank you.\n. This needs updating post-erlang.mk, because rabbit_reader is now in rabbit-common. I'll do it when I have a chance.\n. Closing, see https://github.com/rabbitmq/rabbitmq-common/pull/1 and https://github.com/rabbitmq/rabbitmq-server/pull/395.\n. @lemenkov completely agree. Can you please rebase against stable and re-submit? Then it would be in 3.5.7.\nThank you!\n. Closing assuming @lemenkov will re-submit.\n. :+1: \n. We are aware of this. From our research it is not possible to know that a connection is closed by the peer without reading from the socket. So far nobody could come up with a workaround.\n\nOn 23 oct 2015, at 19:58, Alexey Lebedeff notifications@github.com wrote:\nSteps to reproduce:\n1) Set some resource alarm\n2) Connect to server through amqp and try to publish message, become blocked\n3) Do ungraceful shutdown of blocked connection (I've used kill -9 on publisher)\n4) Observe that there is nothing in logs about lost connection\n5) Clear resource alarm\n6) Observe message(s) about lost connections\nSo, mechanism intended to protect cluster from resource starvation, coupled with impolite clients, can actually result in more memory consumption.\nThe only part of the system that is aware of lost connection is heartbeater process SendFun, which tries to send heartbeats through dead socket, and receives closed and notconn, but happily ignores them - it even has catch there.\nGiven that on receiving side {active, once} option is used and it is not being set when throttling is in effect, rabbit_net:send/2 in the SendFun is the only place where lost connection could be detected.\n\u2014\nReply to this email directly or view it on GitHub.\n. To clarify: we would be interested in solving this but so far none of the suggestions we've seen can avoid the fact that peer disconnection can only be detected when/after a socket read is performed. Now that\nwe are going to require R16B03 for 3.6.0 and 18.x for later versions, there may be OTP updates that would pave a way for us to finally solve this. But so far there's been no success in 4 years or so.\n. Reopening as @binarin may have found a sensible solution.\n. The submitted solution looks reasonable but there is one important aspect we need to consider: with many clients, a blocked connection won't be considered to be dead due to heartbeats. It's debatable whether it should be\nbut that's certainly the case for some clients.\n\nIf this is merged, with lower-ish heartbeat intervals (e.g. 5-10 seconds, which is not uncommon), connections would be dropped by the server fairly quickly when it encounters a resource-based alarm.\nIs this nonetheless a better outcome than what we have today for end user?\n. Some experiments suggest that I may be wrong: only connections of disconnected clients end up being cleaned up, which is what we want.\n. Right, which makes sense, because we only stop reading from the socket but writing still succeeds as long as the peer is alive/reachable.\n. @binarin merged. If you feel this has to also be in 3.5.7, you know what to do ;) Thank you for figuring out a straightforward and effective solution. This issue has been open for much of RabbitMQ's existence.\n. Apparently this conflicts with erlang.mk changes. Will re-submit.\n. All merged. Thank you, @dumbbell and @essen, it was a gigantic amount of work. Our build system is now from the 21st century!\n. Part of https://github.com/rabbitmq/rabbitmq-server/issues/369.\n. @hairyhum yes but it also prints a ton of other stuff. So this can still be useful. @dumbbell WDYT?\n. @Dzol the proposed improvements sound good to me.\n. @aboroska please re-submit against stable.\n. @aboroska thank you!\n. OK, this has passed a basic review and I think the changes are reasonable. Now needs some functional QA.\n. The error scenarios/categories make sense\nto me.\n\nOn 2 nov 2015, at 20:23, Davanum Srinivas notifications@github.com wrote:\n@binarin i see the same in http://fossies.org/dox/glibc-2.22/misc_2sysexits_8h.html so +1 from me to use some of the standard ones from that list\n\u2014\nReply to this email directly or view it on GitHub.\n. Here's an additional data point.\n. I believe #412 largely addresses this. Feel free to suggest/contribute smaller improvements. Better command line arguments framework will also greatly help make the exit codes more fine grained and sensible.\n. Merging this to master will require some manual effort because tests are now in rabbitmq-test. I will take care of it.\n. @dcorbacho please submit it as a PR. All ready to go branches across all repos need to be submitted as PRs.\n. @binarin that sound OK but should perhaps be combined with what the aliveness check HTTP API resource does: opens a connection, declares a queue, publishes one messages and consumes it with basic.get.\n\nThis would be a decent health check for a single node.\n. I'm growing convinced that there should be two improvements made:\n- Single node health check improvements plus a rabbitmqctl command\n- Cluster state health check (the specifics are not yet clear), which should be a separate issue\n. I'd rather avoid the dependency. Declaring a topology and publishing messages using the internal RabbitMQ API is easy, I am less sure about consuming messages.\n@binarin's list of features for this check does not involve publishing messages, and many people don't find that particular check important (at least the way it works today). So we can even leave publishing out if it proves to be difficult without a client library.\n. #44 and friends look good. We'll pull them after 3.6.1 is released.\n. #652 and rabbitmq/rabbitmq-common#60 are merged. rabbitmq/rabbitmq-management#139 doesn't report failure details in a human-readable way and needs more work.\n. https://github.com/rabbitmq/rabbitmq-management/pull/139 was merged, closing.\n. If you don't have any information to share, there is nothing for us to do. Please start a mailing list conversation instead, we use issues for specific actionable items.\n. Please ask questions on the mailing list.\n. For example, this thread explains how to deal with entities that use non-UTF8 characters in them.\n. Another point of improvement. When there's a cookie mismatch, RabbitMQ will log\n=ERROR REPORT==== 25-Jan-2016::16:08:25 ===\n** Connection attempt from disallowed node 'rabbitmq-cli-7344@ip-172-31-XY-XYZ' **\nWe should make it clear that this is an authentication failure and the cookie must match. Also, nodes do not reload cookies once they are change.\n. This will only have an effect on Erlang/OTP 18.4 but exactly the kind of improvement I had in mind. Thank you, @dcorbacho.\n. OK, Erlang/OTP 19 then.\n. 3.4.x is no longer supported. Please try 3.5.6.\nHeaders exchange is by far the least efficient exchange type. This could also be #227.\nHowever, if a certain connection count triggers the issue, you may want to look into the logs and see if you are running into alarms or similar. See Networking and Production checklist for more info.\n. @venikkin sorry but #619 is no different: you use an outdated version, provide no logs, no code samples, and there is simply no evidence that it is an issue with headers exchanges. We keep issues to specific, reasonably well defined cases that are still true with the most recent release.\n. Closing per @Ayanda-D's request, will be re-submitted.\n. Per your own words the issue is with OS configuration and has nothing to do with RabbitMQ. Please use rabbitmq-users for questions.\n. Plus node name can be overridden using RABBITMQ_NODENAME.\n. Please post questions to rabbitmq-users. Most likely reason is that you try to connect from a remote host with default credentials, which is not allowed by default.\n. @bogdando I assume you folks communicate behind the scenes but JFYI :)\n. It's unfortunate that before we move CLI tools to a sensible CLI library (read: Elixir), EX_SOFTWARE would be returned much more often than it deserves. But from what I understand, #396 is mostly about separating failures such as timeouts and unavailable nodes.\n. I don't have objections to stopping early. Of course, we should log something very visible, too.\n. @videlalvaro please do.\n. @videlalvaro 1024 is the default value on Linux. I can't think of anything safer than what we already do.\n. @videlalvaro we can log something like \"could not determine effective open file handles limit, assuming 1024 which is a low value\"\n. Depending on a future Ranch version is fine now with erlang.mk as long as we are confident in its stability.\n. Broker cannot start with all defaults for me (on OS X):\n```\nBOOT FAILED\n===========\nError description:\n   {could_not_start,rabbit,\n    {{case_clause,\n      {error,\n       {{shutdown,\n         {failed_to_start_child,\n          {ranch_listener_sup,{acceptor,{0,0,0,0,0,0,0,0},5672}},\n          {shutdown,\n           {failed_to_start_child,ranch_acceptors_sup,\n            {function_clause,\n             [{ranch,filter_user_options,\n               [[inet6,binary,\n                 {backlog,128},\n                 {nodelay,true},\n                 {linger,{true,0}},\n                 {exit_on_close,false},\n                 {active,false},\n                 {packet,raw},\n                 {reuseaddr,true}],\n                [backlog,ip,linger,nodelay,port,raw,send_timeout,\n                 send_timeout_close]],\n               [{file,\"src/ranch.erl\"},{line,127}]},\n              {ranch,filter_user_options,2,\n               [{file,\"src/ranch.erl\"},{line,129}]},\n              {ranch,filter_user_options,2,\n               [{file,\"src/ranch.erl\"},{line,129}]},\n              {ranch,filter_options,3,[{file,\"src/ranch.erl\"},{line,124}]},\n              {ranch_tcp,listen,1,[{file,\"src/ranch_tcp.erl\"},{line,65}]},\n              {ranch_acceptors_sup,init,1,\n               [{file,\"src/ranch_acceptors_sup.erl\"},{line,30}]},\n              {supervisor,init,1,[{file,\"supervisor.erl\"},{line,272}]},\n              {gen_server,init_it,6,\n               [{file,\"gen_server.erl\"},{line,328}]}]}}}}},\n        {child,undefined,'rabbit_tcp_listener_sup_:::5672',\n         {tcp_listener_sup,start_link,\n          [{0,0,0,0,0,0,0,0},\n           5672,ranch_tcp,\n           [inet6,binary,\n            {backlog,128},\n            {nodelay,true},\n            {linger,{true,0}},\n            {exit_on_close,false},\n            {active,false},\n            {packet,raw},\n            {reuseaddr,true}],\n           rabbit_connection_sup,[],\n           {rabbit_networking,tcp_listener_started,[amqp]},\n           {rabbit_networking,tcp_listener_stopped,[amqp]},\n           \"TCP Listener\"]},\n         transient,infinity,supervisor,\n         [tcp_listener_sup]}}}},\n     [{rabbit_networking,start_listener0,4,\n       [{file,\"src/rabbit_networking.erl\"},{line,305}]},\n      {rabbit_networking,'-start_listener/4-lc$^0/1-0-',4,\n       [{file,\"src/rabbit_networking.erl\"},{line,294}]},\n      {rabbit_networking,start_listener,4,\n       [{file,\"src/rabbit_networking.erl\"},{line,294}]},\n      {rabbit_networking,'-boot_tcp/0-lc$^0/1-0-',1,\n       [{file,\"src/rabbit_networking.erl\"},{line,125}]},\n      {rabbit_networking,boot_tcp,0,\n       [{file,\"src/rabbit_networking.erl\"},{line,125}]},\n      {rabbit_networking,boot,0,\n       [{file,\"src/rabbit_networking.erl\"},{line,120}]},\n      {rabbit_boot_steps,'-run_step/2-lc$^1/1-1-',1,\n       [{file,\"src/rabbit_boot_steps.erl\"},{line,49}]},\n      {rabbit_boot_steps,run_step,2,\n       [{file,\"src/rabbit_boot_steps.erl\"},{line,49}]}]}}\n```\nNothing stands on the list of socket options.\n. make clean in the server, common and umbrella did not help but manually removing deps/ranch did. Things seem to work now.\n. Java client, Bunny, March Hare, and Erlang client test suites pass on multiple OS X and Linux hosts.\n. This seems to be ready for merging, we now need to wait for some build system updates to accommodate dependencies more than 1 level deep (rabbit => rabbit_common => ranch).\n. Testing GH notifications.\n. Ditto.\n. @essen I'm observing an exception that only this branch has in the SASL log:\n```\n=CRASH REPORT==== 19-Nov-2015::17:25:04 ===\n  crasher:\n    initial call: rabbit_reader:init/4\n    pid: <0.6803.0>\n    registered_name: []\n    exception exit: {noproc,{gen_server,call,[undefined,delete_all,infinity]}}\n      in function  gen_server:call/3 (gen_server.erl, line 212)\n      in call from rabbit_reader:close_connection/1 (src/rabbit_reader.erl, line 687)\n      in call from rabbit_reader:send_error_on_channel0_and_close/4 (src/rabbit_reader.erl, line 1463)\n      in call from rabbit_reader:handle_input/3 (src/rabbit_reader.erl, line 998)\n      in call from rabbit_reader:recvloop/4 (src/rabbit_reader.erl, line 444)\n      in call from rabbit_reader:run/1 (src/rabbit_reader.erl, line 426)\n      in call from rabbit_reader:start_connection/4 (src/rabbit_reader.erl, line 384)\n    ancestors: [<0.6801.0>,<0.280.0>,<0.279.0>,<0.278.0>,rabbit_sup,\n                  <0.169.0>]\n    messages: [{'EXIT',#Port<0.19890>,normal}]\n    links: [<0.6801.0>]\n    dictionary: [{process_name,\n                      {rabbit_reader,\n                          <<\"[FE80::1]:52718 -> [FE80::1]:5672\">>}}]\n    trap_exit: true\n    status: running\n    heap_size: 2586\n    stack_size: 27\n    reductions: 4046\n  neighbours:\n=SUPERVISOR REPORT==== 19-Nov-2015::17:25:04 ===\n     Supervisor: {<0.6801.0>,rabbit_connection_sup}\n     Context:    child_terminated\n     Reason:     {noproc,{gen_server,call,[undefined,delete_all,infinity]}}\n     Offender:   [{pid,<0.6803.0>},\n                  {name,reader},\n                  {mfargs,\n                      {rabbit_reader,start_link,\n                          [<0.6802.0>,\n                           {acceptor,{0,0,0,0,0,0,0,0},5672},\n                           #Port<0.19890>]}},\n                  {restart_type,intrinsic},\n                  {shutdown,4294967295},\n                  {child_type,worker}]\n```\nin other words, when connection process tries to close a client connection, queue_collector, which tracks and cleans up exclusive queues, is undefined. It can only be undefined before connection.tune* steps complete. I do not observe this with master.\nThe easiest way to reproduce is with Bunny:\n```\nstart rabbitmq\ncd /path/to/deps/rabbit\nmake run-broker PLUGINS='rabbitmq_management rabbitmq_consistent_hash_exchange'\nwith Ruby 2.0 or later\ngit clone https://github.com/ruby-amqp/bunny\ncd bunny\ngem install bundler\nbundle install\nexport RABBITMQCTL=/path/to/deps/rabbit/scripts/rabbitmqctl\n./bin/ci/before_build\nbundle exec rspec -cfd spec/higher_level_api/integration/connection_recovery_spec.rb\n```\nand then tail SASL logs.\n. I did a rebuild while switching to master and back, and no longer have this exception. Asking more people to give this a try.\n. @essen nevermind the above. The issue is unrelated to Ranch but rather my error message refactoring, which now leads all early terminating connections (e.g. due to authentication issues) through a function that tries to clean up exclusive queues. Sometimes there would be nothing to clean up, so it needs to be handled.\n. Please post questions to rabbitmq-users and specify the version used and rabbitmqctl report output.\n. @sazary we don't know anything because there's not enough information provided: exactly why this belongs to the mailing list.\n. To me, reporting errors to the parent/accumulator makes most sense. Thanks for looking into this, @essen.\n. This was introduced in this cycle as part of #62.\n. Erlang's public_key module cannot\nsign a value. Please ask on erlang-users.\nMK\n\nOn 16 nov 2015, at 15:47, biiiipy notifications@github.com wrote:\nWhat could be the reason for rabbitmq crashing on startup when using ECDSA + SHA256 server certificate? I've tried:\nWorking:\nECDSA+SHA1\nRSA+SHA1\nRSA+SHA256\nNot Working:\nECDSA+SHA256\nRunning rabbitmq 3.5.6, erlang r18.1, windows 7\nLog:\n* Reason for termination = \n* {function_clause,\n       [{public_key,sign,\n            [{digest,\n                 <<57,64,231,69,138,82,239,218,169,237,223,217,16,61,160,220,\n                   38,83,122,74,96,74,92,46,235,162,222,157,233,111,207,245,\n                   237,166,225,110,42,139,85,177,43,201,68,255,30,213,34,130,\n                   253,247,193,2,46,47,152,212,60,82,211,243,9,70,247,136>>},\n             sha512,\n             {'PrivateKeyInfo',v1,\n                 {'PrivateKeyInfo_privateKeyAlgorithm',\n                     {1,2,840,10045,2,1},\n                     {asn1_OPENTYPE,<<6,5,43,129,4,0,34>>}},\n                 <<48,129,155,2,1,1,4,48,237,83,11,120,206,206,60,186,204,57,\n                   67,34,207,48,160,53,106,128,99,184,234,248,246,226,38,18,\n                   224,179,13,181,132,114,19,60,30,120,156,126,72,62,95,230,\n                   102,28,152,71,84,22,161,100,3,98,0,4,240,84,45,30,54,167,65,\n                   215,99,151,193,226,179,244,90,187,157,21,201,191,106,28,196,\n                   5,69,246,127,70,247,120,180,66,179,61,88,156,98,238,168,12,\n                   110,114,192,137,183,45,106,177,200,173,116,41,191,190,203,\n                   108,166,91,239,129,27,203,165,160,126,165,9,156,158,145,81,\n                   28,57,88,38,236,146,11,76,38,61,178,65,245,62,52,7,20,26,\n                   123,198,154,162,221,231,2>>,\n                 asn1_NOVALUE}],\n            [{file,\"public_key.erl\"},{line,441}]},\n        {ssl_handshake,enc_server_key_exchange,6,\n            [{file,\"ssl_handshake.erl\"},{line,846}]},\n        {ssl_connection,key_exchange,2,\n            [{file,\"ssl_connection.erl\"},{line,1245}]},\n        {ssl_connection,server_certify_and_key_exchange,2,\n            [{file,\"ssl_connection.erl\"},{line,1142}]},\n        {ssl_connection,new_server_hello,3,\n            [{file,\"ssl_connection.erl\"},{line,1015}]},\n        {tls_connection,next_state,4,[{file,\"tls_connection.erl\"},{line,466}]},\n        {gen_fsm,handle_msg,7,[{file,\"gen_fsm.erl\"},{line,518}]},\n        {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}\n=ERROR REPORT==== 16-Nov-2015::14:07:40 ===\n    application: mochiweb\n    \"Accept failed error\"\n    \"{'EXIT',\\n    {{function_clause,\\n         [{public_key,sign,\\n              [{digest,\\n                   <<57,64,231,69,138,82,239,218,169,237,223,217,16,61,160,\\n                     220,38,83,122,74,96,74,92,46,235,162,222,157,233,111,207,\\n                     245,237,166,225,110,42,139,85,177,43,201,68,255,30,213,\\n                     34,130,253,247,193,2,46,47,152,212,60,82,211,243,9,70,\\n                     247,136>>},\\n               sha512,\\n               {'PrivateKeyInfo',v1,\\n                   {'PrivateKeyInfo_privateKeyAlgorithm',\\n                       {1,2,840,10045,2,1},\\n                       {asn1_OPENTYPE,<<6,5,43,129,4,0,34>>}},\\n                   <<48,129,155,2,1,1,4,48,237,83,11,120,206,206,60,186,204,57,\\n                     67,34,207,48,160,53,106,128,99,184,234,248,246,226,38,18,\\n                     224,179,13,181,132,114,19,60,30,120,156,126,72,62,95,230,\\n                     102,28,152,71,84,22,161,100,3,98,0,4,\n 240,84,45,30,54,167,\\n                     65,215,99,151,193,226,179,244,90,187,157,21,201,191,106,\\n                     28,196,5,69,246,127,70,247,120,180,66,179,61,88,156,98,\\n                     238,168,12,110,114,192,137,183,45,106,177,200,173,116,41,\\n                     191,190,203,108,166,91,239,129,27,203,165,160,126,165,9,\\n                     156,158,145,81,28,57,88,38,236,146,11,76,38,61,178,65,245,\\n                     62,52,7,20,26,123,198,154,162,221,231,2>>,\\n                   asn1_NOVALUE}],\\n              [{file,\\\"public_key.erl\\\"},{line,441}]},\\n          {ssl_handshake,enc_server_key_exchange,6,\\n              [{file,\\\"ssl_handshake.erl\\\"},{line,846}]},\\n          {ssl_connection,key_exchange,2,\\n              [{file,\\\"ssl_connection.erl\\\"},{line,1245}]},\\n          {ssl_connection,server_certify_and_key_exchange,2,\\n              [{file,\\\"ssl_connection.erl\\\"},{line,1142}]},\\n          {ssl_connection,new_server_hello,3,\\n              [{file,\\\"ssl_co\n nnection.erl\\\"},{line,1015}]},\\n          {tls_connection,next_state,4,\\n              [{file,\\\"tls_connection.erl\\\"},{line,466}]},\\n          {gen_fsm,handle_msg,7,[{file,\\\"gen_fsm.erl\\\"},{line,518}]},\\n          {proc_lib,init_p_do_apply,3,[{file,\\\"proc_lib.erl\\\"},{line,240}]}]},\\n     {gen_fsm,sync_send_all_state_event,\\n         [<0.533.0>,{start,infinity},infinity]}}}\"\n\u2014\nReply to this email directly or view it on GitHub.\n. Please ask questions on the mailing list. This is most likely inefficient disk paging that leads to flow control, see #227 and related.\n. Milestone: n/a because this is a refactoring piece that is only relevant to master.\n. Your understanding seems to be correct, thank you.\n. Fixed in #444.\n. The regression was introduced in https://github.com/rabbitmq/rabbitmq-common/commit/51422941644a371f5b586aa50f01e142a9224ab6.\n. Milestone => none because this is a regression from a change that was never part of a release.\n. Fixed in #440.\n. Thank you!\n. Fixed in #378.\n. References #378.\n. @hairyhum we need to introduce a new value, {absolute_mb, N} (the best name I could come up with given that absolute is already taken).\n\nUnifying MB vs. MiB along the way is possible. I believe we are supposed to use 10^6 these days, although in my (no longer very young) mind a megabyte is still 1024 * 1024 bytes :)\n. @dumbbell that sounds good but the number of prefixes we might need to support is a bit alarming: kb, k, KB, KIB, and so on.\nA Google search for \"1 megabytes in bytes\" yields 10^6, so I still vote for MB.\n. OK, I think we can support\n- k, K, kb, KB\n- m, M, mb, MB\n- g, G, gb, GB\nas suffixes, as well as numerical values that mean bytes.\n@hairyhum thoughts?\n. @hairyhum confusing for whom? A megabyte is 10^6 bytes these days, not 1024 * 1024, so if we use megabytes in memory monitor, we need to correct it. It won't be a major breaking change in practice, I think.\n. @hairyhum setting RAM watermark as an absolute value is an unreleased feature new in 3.6.0, so we can pretty much assume there are no users who rely on it.\n. I agree with @dumbbell.\n. Log an error and ignore the value. Don't terminate node startup.\n. Error reporting for rabbitmqctlis fairly obvious.\n. @hairyhum please file a separate issue for that\n. It means falling back to the relative setting, which has a default.\n. It currently has a default: 0.4 of available RAM. Use that instead of infinity.\n. Thanks. It may or may not be intentional, only @videlalvaro can say for sure :)\n. Fixed by #452.\n. This can be cherry-picked to 3.5.x => marking as such.\n. @javaforfun with what Erlang version and architecture?\n. @javaforfun is this on Windows?\n. @javaforfun ok, should be fixed by #481.\n. RabbitMQ has no native code in it and cannot be the root cause of a segmentation faul. Use a different Erlang/OTP version, and make sure it's 64 bit.\n. Fixed in https://github.com/rabbitmq/rabbitmq-website/pull/112.\n. @hairyhum thank you, please submit a PR in that repo as well.\n. It possibly can make it into 3.6.x, we will see how intrusive this ends up being.\n. @jmoney8080 we'll keep you posted ;)\n. We will investigate if there are reasonably safe ways to fix this in 3.6.x.\n. The failure above is, unfortunately, in the code that executes before RabbitMQ is started. Perhaps something can be improved in the shell script.\nWhat version do you use?\n. Related: #396.\n. Thank you, @hairyhum!\n. @Ayanda-D can this go into 3.5.7? If so, can you please rebase #471 against stable?\n. Please ask questions on rabbitmq-users.\nThe Telnet connection was successful as confirmed by the log. Telnet does not log when a connection is established. It then waits for your protocol-specific commands to be entered.\n. @dmitrymex do you intend this to be in 3.5.7? Then please rebase against stable.\n. Closing, re-submitted as #480.\n. @dmitrymex stable is merged into master pretty much after every change.\n. It turns out, Mod:module_info(native) is OTP 18.x-specific but there is code:is_module_native/1 which I wasn't sure about at first but @essen figured that's what Dialyzer uses. It also seems to work as expected.\n. @bogdando @dmitrymex keep in mind that we plan on shipping 3.5.7 next Monday.\n. @MichalMichalak as part of investigating this we found out that we do, in fact, log source IP addresses, just in different logs:\n- Client connections have an IP address logged\n- HTTP API requests (which is what your original thread was concerned with) are logged in a dedicated log file\nI don't know if mentioning IP addresses in every error message would be beneficial or not. We decided to add a test and close this issue.\n. Fixed by https://github.com/rabbitmq/rabbitmq-server/pull/511.\n. See also #487.\n. After a long in person discussion of the scope of this we have a list of things to focus on. This is therefore an umbrella issue now.\n. This issue has become a placeholder for all open ended questions and references around cluster formation improvement in 3.7.0. I believe we now have all the answers and no further changes are expected. This can be considered done.. Got a stack trace by modifying gen_server2:terminate/3 to log more:\nStacktrace: [{rabbit_priority_queue,cse,2,\n                 [{file,\"src/rabbit_priority_queue.erl\"},{line,656}]},\n             {rabbit_priority_queue,'-combine_status/3-lc$^0/1-0-',2,\n                 [{file,\"src/rabbit_priority_queue.erl\"},{line,647}]},\n             {rabbit_priority_queue,combine_status,3,\n                 [{file,\"src/rabbit_priority_queue.erl\"},{line,647}]},\n             {rabbit_priority_queue,fold0,3,\n                 [{file,\"src/rabbit_priority_queue.erl\"},{line,469}]},\n             {rabbit_amqqueue_process,'-infos/2-lc$^0/1-0-',2,\n                 [{file,\"src/rabbit_amqqueue_process.erl\"},{line,844}]},\n             {rabbit_amqqueue_process,'-infos/2-lc$^0/1-0-',2,\n                 [{file,\"src/rabbit_amqqueue_process.erl\"},{line,844}]},\n             {rabbit_amqqueue_process,emit_stats,2,\n                 [{file,\"src/rabbit_amqqueue_process.erl\"},{line,920}]},\n             {rabbit_event,if_enabled,3,\n                 [{file,\"src/rabbit_event.erl\"},{line,142}]}]\n. So both arguments to rabbit_priority_queue:cse/2 end up being default.\n. My understanding of this is:\n- A barely initialised priority queue process emits stats\n- It is asked to combine backing queue statuses but some have none to report yet because of inherent concurrency between queue init process and the timer\n- It trips up on (uncalculated) values being returned as default\nWe can do two things:\n- Delay stats timer initialisation by, say, a second\n- Ignore default values and consider combined values to be infinity\nI think the latter makes more sense.\n. It ended up being a lot more straightforward: it's queue mode that could be emitted as default (or lazy) that tripped up priority queue stats emitter.\n. Erlang unit tests fail with\nOUT=$(RABBITMQ_PID_FILE='/var/folders/6v/nzn6gr5d6k31r39x57fddtpm0000gp/T//rabbitmq-test-instances/rabbit/rabbit.pid' echo \"rabbit_tests:all_tests().\" | erl_call -sname rabbit -e) ; \\\n      echo $OUT ; echo $OUT | grep '^{ok, passed}$' > /dev/null\n{ok, \"Tests failed\nError: {error, {badmatch,[true,{error,enoent}]}}\nStack trace:\n[{rabbit_tests,test_log_management,0,\n [{file,\"test/src/rabbit_tests.erl\"},{line,742}]},\n {rabbit_tests,all_tests0,0,[{file,\"test/src/rabbit_tests.erl\"},{line,63}]},\n {rabbit_tests,all_tests,0,[{file,\"test/src/rabbit_tests.erl\"},{line,34}]},\n {erl_eval,do_apply,6,[{file,\"erl_eval.erl\"},{line,669}]},\n {lib,eval_str,1,[{file,\"lib.erl\"},{line,106}]},\n {rpc,'-handle_call_call/6-fun-0-',5,[{file,\"rpc.erl\"},{line,206}]}]\n\"}\nmake[1]: *** [run-tests] Error 1\nOther test suites pass.\n. make full fails for me (Java test suites fail, looks like node startup issues) while they all pass when executed individually.\n. Fixing TTY logging is the only thing that's left, it seems.\n. @dumbbell good point. Judging from the above, it makes sense to use a single file. Now that we use Lager and have more flexibility, I like this idea.\n. :+1: from me.\n. Using Lager directly will break a lot of plugins. Lets hold off on that and do it for 3.8.\n. As for deprecating rabbit_log, plugin authors may find it convenient to have this known way of logging to the broker log file.\n. @dumbbell no objections to that.\n. So the extra sinks would be used to allow for finer-grained configuration of things we currently allow to configure. That sounds reasonable to me. As does respecting RABBITMQ_*.\nShould lager_forwarder_backend be rabbit_lager_forwarder_backend, though, because it is RabbitMQ-specific?\n. Fair enough.\n. I have one test failure:\n{ok, \"Tests failed\nError: {error, undef}\nStack trace:\n[{rabbit,log_location,[kernel],[]},\n {rabbit_tests,test_log_management,0,\n [{file,\"test/src/rabbit_tests.erl\"},{line,732}]},\n {rabbit_tests,all_tests0,0,[{file,\"test/src/rabbit_tests.erl\"},{line,63}]},\n {rabbit_tests,all_tests,0,[{file,\"test/src/rabbit_tests.erl\"},{line,34}]},\n {erl_eval,do_apply,6,[{file,\"erl_eval.erl\"},{line,669}]},\n {lib,eval_str,1,[{file,\"lib.erl\"},{line,106}]},\n {rpc,'-handle_call_call/6-fun-0-',5,[{file,\"rpc.erl\"},{line,206}]}]\n\"}\nmake[1]: *** [run-tests] Error 1\n(with rabbitmq-test switched to rabbitmq-server-94, of course).\n. @dumbbell I still have the test failure but otherwise no objections to your plan.\n. With a few more tweaks in the rabbitmq-test counterpart to this, Lager now looks good to me. I'd be OK with merging this now and updating plugins (primarily management and plugins that extend it) later as needed.\nWe should also cut a milestone release after the dust from this change settles.\n. Thank you, @hairyhum. We'll leave this for after 3.6.0 (can be 3.6.x, perhaps).\n. Fixed in #582.\n. @dmitrymex @bogdando any thoughts on this?\n. Please post questions to rabbitmq-users. There are only 2 log files. First thing to check is if your apps can be running into disk alarms.\n. The first feature will very likely have a throughput effect and substantial complexity due to the need to coordinate things across everything in a vhost (in particular, exchanges that can publish to multiple queues at once). Limiting the number of queues per vhost is much easier and wouldn't introduce a lot of complexity.\nLimiting connections per vhost makes sense. The question is, how should we do it. We have a channel limit per vhost, which is a configuration option. Should this work the same way, that is, be configured node-wide?\n. We do it for individual queues but not entire vhosts. Now for every message that flows through the system, channels would have to check and update a counter that every other queue in the same vhost updates. I have concerns about how well that is going to scale even for a single node.\nLimiting how many things (queues or connections) there can be in a vhost is straightforward. A combination of per-queue length limit in bytes x limited number of queues per vhost should effectively give you a way to specify the same limit.\n. @vgkiziakis I agree that some vhosts may have mostly empty queues. However, RabbitMQ developers cannot really come up with a one-size-fits-all solution.\nPerhaps both points can be addressed by making the numbers of connections and queues configurable using policies and not static config file?\nThen specific users can decide what limit should be applied to what vhost(s).\n. @videlalvaro yes, using our existing way of collecting rate metrics (rabbit_event) would makes this easier but there is a different to what we do today vs. what we would have to do to enforce a vhost-wide limit. Stats are currently emitted every N seconds, asynchronously, and event collector in the management plugin can drop them when it finds itself overloaded. In contrast, tracking total # of bytes used per vhost would have to happen on a different schedule (e.g. not every 30 seconds) and dropping some stats wouldn't be a good idea.\n. @vgkiziakis thank you for your feedback. Would you mind if I close this issue and file two more focussed ones (for per-vhost limits for queues and connections) based on this discussion?\n. Closing in favour of #500 and #501, specific improvements that came out of this discussion.\n. @pranjaljain please post questions to rabbitmq-users. Busy queues, connections and channels are primary ways of resource consumption. Max number of channels per connection is a configuration setting in RabbitMQ.. There is one important issue with blocking: we can only block the entire connection. RabbitMQ cannot know where the next message will be routed to.\nSo are you suggesting that we use the same mechanism as with resource-driven alarms (stop reading from connection socket) or use a protocol extension to send publishers notifications (similarly to how we notify them about connections being blocked due to resource alarms)?\nHow would this be configured for queues? How should this work for connections on remote nodes (any client can publish to any exchange, routing to any queue, from any node)?\n. We already have protocol extensions: connection.[un]blocked. We need a publisher and/or queue to indicate that it should be blocked. Perhaps this can be a queue setting.\nExisting methods such as basic.publish cannot be extended, unfortunately. We can introduce new methods as needed. Every client also has an extensible table of \"capabilities\" which server can check and act accordingly.\n. @vgkiziakis it definitely does, except that publishers will still use the regular basic.publish method. Smaller details of this need to be worked out but you got the general idea right.\n. mandatory has a different semantics, so does basic.return.\n. @hairyhum what's the value proposition of this new state machine? It can be transition and state names that confuse me but I don't find it to be any simpler than what we have.\n. @hairyhum there are two separate state machines: a negotiation/running/closing/closed one and another for throttling. If we were to combine them into one, we'd need to come up with descriptive names for such states. This is not going to be easy, and can spill into the management UI. Are you sure the whole thing is worth doing? What does it really improve for this particular issue?\n. I think we should stop discussing this refactoring idea in this issue as there seems to be little connection. Please submit a PR for what you have, and include the state diagrams above.\n. I doubt that blocking publishers in this case is a good idea. For starters, this behavior would be counterintuitive to most, in particular beginners. So it has to be opt-in.\nEmpirical evidence suggests that most applications are not designed to handle such scenarios. Asking publishers to account for nacks is a better option because they kind of have to anyway if they use publisher confirms at all.\nJMS client can potentially use publisher confirms under the hood. Even if that's not the case, it is possible to rely on dead-lettering to collect and retry the messages that \"did not fit\". This trick has been used together with timed expiration for years and possible with any client as long as queue arguments can be tweaked (e.g. using a policy).\nSo I'd rather close this and see if there's enough interest in this feature going forward, now that #995 has shipped.. The more I think about it, the more I'm considering adding a separate mechanism to policies for vhosts. It won't be conceptually different and might reuse rabbit_registry or even more but it has to be separate, otherwise we risk confusing the user (as policies that apply to queues and exchanges already have a logical connection with a vhost).\nFor example, this feature can be called \"vhost limits\" and used much like policies are but with a new rabbitmqctl command.\n@dumbbell @hairyhum @camelpunch @vgkiziakis do you have an opinion on this?\n. Some research suggests the above fits our existing runtime parameters framework very well.\n. I'm going to work on a spike for this this week.\n. New ctl command I'm working with:\nrabbitmqctl set_vhost_limits -p a-vhost --max-connections [val] --max-queues [val]\nalternatively it can be\nrabbitmqctl set_vhost_limits -p a-vhost '{\"max-connections\": 1000, \"max-queues\": 10000}'\n(so, as a JSON object) but I'm leaning towards the former.\nHTTP API version will have to use JSON, of course, when we introduce it.\n. After a discussion with the team we ended up with\nrabbitmqctl set_vhost_limits -p a-vhost '{\"max-connections\": 1000, \"max-queues\": 10000}'\n. Setting the limit(s) is in place. We are now looking into what's the best way to keep track of connections scoped per vhost across the entire cluster. The tricky part is nodes going down, being restarted, changing names, and so on.\n. An update on this: naive connection tracking couples rabbit_common with rabbit via rabbit_reader's  need to use Mnesia (via rabbit_connection_tracking). A better way to do this would be to introduce a gen_event handler that only listens to events on the node it is running on.\nThis is similar to how management stats DB keeps track of connections, except that here we don't want all nodes to perform updates, only the local one.\n. Considering the scope of the changes, I'm moving this to 3.7.0 because our internal data store schema changes in a way that will prevent 3.6.0 nodes from accepting it from later 3.6.x releases.\n. Connection limit is in place, I'm going to work on some (synthetic) benchmarks to see how efficient our queries are with a few millions rows. In case it's not efficient enough we may need to take a different approach and use separate tables with counters.\n. I created a synthetic benchmark that loads all connections in one of 10 vhosts with 1m records in the table. It takes over 1 second to count connections that way. Not surprising but a data point. So we need to use ets:select_count/2 (benchmarking it next) or separate tables with counter fields.\n. Expectedly counter columns produce < 100 microseconds responses even with 10m connections.\n. Limit support is done, and two more related features are extracted into #627 and #628. What's left to cover in this issue:\n- Integration testing\n- Node unavailability handling\n. Moving to 3.7.0 as it shares changes with #500.\n. This is not really a requirement, and message properties don't have a field for redelivery count, only a boolean flag. Still worth having, of course.\n. @mikljohansson we will do our best to fit it into 3.7.0 but no promises.\n. An update: this won't be in 3.7.0 because we had quite a few out-of-cycle and new features in 3.7.0 and we need to limit the scope at some point. But we are contemplating it for 3.8.0 (so that it doesn't have to wait for 4.0, that is).\n. It is on the roadmap for 3.8.0.\n3.7.0 introduces operator policies that can be used to force a \"high\nwatermark\" message TTL value.\nOn Thu, Feb 23, 2017 at 12:53 PM, mattheworiordan notifications@github.com\nwrote:\n\nHi @michaelklishin https://github.com/michaelklishin, do you mind if I\nask if any progress has been made on this? Running RabbitMQ on behalf of\ncustomers means RabbitMQ is quite susceptible to overload from one customer\n(unless we rate limit them which is not ideal) simply because of a faulty\ncode loop.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/502#issuecomment-281946707,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQh6FZat4Rqy05AA3vohry3BCJZOTks5rfVckgaJpZM4G2YHW\n.\n\n\n-- \nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. The status is that it has a chance of getting into 3.8.0 but we make no promises.\nUpdate: #1889 has been merged and covers quorum queues.. A comment above asks all discussions and questions to be moved to the mailing list.\nQuorum queues in master keep track of a redelivery count. We make no promises if classic queues will in 3.8. Currently, it's just a number consumers can use to make decisions, nothing else.\nIt's your responsibility to handle exceptions in your consumers. This is not going to change with any redelivery count implementation.. #1889 is in.. Leaving this open until classic queues get a similar feature (or we really decide against that).. Related: #508/#509.\n. We've revisited it a few times in 3.6.x and removed the visualizer in 3.7.0. I consider it done.. @michaelplaing do you have an opinion on how this should work?\n. @videlalvaro the user won't configure any topic permissions for such exchanges. All queries will be local reads, just like with topic trie segments.\n. @uvzubovs right, just like rabbitmqctl set_permissions does.\n. This issue is not about MQTT specifically but there is one MQTT-specific aspect that's hard to ignore: the wildcards. We have a way to translate them into AMQP 0-9-1 ones. The question is, how should this be exposed to end user, given that rabbitmqctl is currently not extendable from plugins.\n. We'll try to get it into 3.6.7 but there's a chance that it will go into 3.6.8 or 3.7.0.. For topic pattern definitions, what if instead of AMQP 0-9-1 patterns we used regular expressions? We already use them for permission patterns anyway. They are not perfect but more protocol-neutral and I expect that in most cases basic topic prefixing would be sufficient.\nOn the upside for us, it would be easy to implement and the matching function is already quite heavily optimized in the standard library.. Unfortunately this will require schema changes that cannot go into 3.6.x.. Moreover, this will likely require updates to existing authz plugins, which also rules out the possibility of being included into a 3.6.x release.. We have our second prototype of this feature undergoing QA this week.. MQTT plugin still needs a bit of work so we will close this issue but keep rabbitmq/rabbitmq-mqtt#95 and our (private) Pivotal Tracker story open.. @Gsantomaggio thanks to @hairyhum this has made it into 3.6.0 :)\n. Feel free to make similar improvements in more places. There aren't that many commands that require RabbitMQ to be stopped, though.\n. Leaving the cluster and stopping is what I had in mind.\nWe considered doing the same as an option for disk alarms after 3.6.0 ships.\nI think having a designated named \"shutdown coordinator\" process that would handle it is the right\nway to go about it.\n\nOn 24 dic 2015, at 13:55, Daniil Fedotov notifications@github.com wrote:\nWill it be enough to just stop node? Because I guess it could be restarted again and cause trouble if in cluster. \nSo there is an option to leave cluster (if clustered) and stop the node. \nIt still require some tests.\n\u2014\nReply to this email directly or view it on GitHub.\n. This feature is mean to be opt-in: you can tell the node to shut down when it encounters a local disk I/O error. Simply terminating message store process doesn't do that, in fact, it worse than what we have today (and will have by default in the future).\n. As described in https://github.com/rabbitmq/rabbitmq-server/issues/75, this PR is a WIP and should not be merged.\n. I am closing this as it misinterprets the idea I had in mind for #75 and now fails to merge anyway.\n. Thanks.\n. Please ask questions on rabbitmq-users. This is not a RabbitMQ issue by any means. If you intend to use Ubuntu 12.04, you need to make sure you have a way to provision the minimum required Erlang version. Or use an older release of RabbitMQ.\n. In case the error message (coming from apt, not RabbitMQ) isn't clear: the minimum required version is R16B03, your Ubuntu release only has R14B04. Erlang Solutions provides most recent version of Erlang for a bunch of OSes. Chances are their Debian packages supports Ubuntu 12.04 (it certainly works fine on older releases).\n. @tasmaniski we've made it pretty clear in release notes for 3.6.0 that it now requires a newer Erlang version.\n. @tasmaniski minor nitpick: this is not installing RabbitMQ from source. You install a binary Debian package.\n\n@rodvela's question was answered in https://groups.google.com/forum/#!topic/rabbitmq-users/GDpuD8NQkFY.\n. There are several decisions we need to make as part of this. First of all, what Authorisation Grant types do we care about? implicit is going to be quite common for e.g. Web apps. We also need to integrate with Cloud Foundry's UAA server. Which leads to another decision: should we support external authorisation servers, provide or own, or both? Certainly the former (to integrate with UAA) but likely\nalso the latter, to make sure a basic server is available without deploying external services.\nAll that has to fit with existing features (e.g. authentication and authorisation backends are pluggable) and future features such as #505. So it's easy to underestimate the scope.\n. Can you be more specific? Exchange federation replicates messages, while queue federation moves them as needed (for better data locality). What exactly isn't covered?\n. So you want two-way exchange federation. You already can set it up today. We'll consider this at a future point.\n. Closing as this does not belong to this repo (and we haven't decided how useful this feature would be).\n. Fair enough. Please submit a PR against stable?\n. Actually, some may argue that we should not do this because server capabilities only demonstrate non-standard features. But I guess direct reply-to can be considered that.\nAdding a capability is a single line change.\n. Yeah, we already list per_consumer_qos which is comparable to direct_reply_to.\n. Will be in 3.6.1.\n. @vikinghawk just to clarify: your system had at least one exchange-to-exchange binding, correct? I think we have a fix merged thanks to @Ayanda-D, can produce a one-off server build for you to verify :)\n. @vikinghawk can you contact me at michael in RabbitMQ domain so I know where to send the file/link?\n. tarball sent.\n. Please post questions to rabbitmq-users. Sounds like a blank page is served.\n. There is no evidence that it is a RabbitMQ issue. We do not use issues for discussions. See what markup is returned and what JS errors there may be. Management is easily the most popular plugin, this issue hasn't been reported to date.\n. @Michael-Ekkert moved to https://github.com/rabbitmq/rabbitmq-management/issues/98.\n. @beermattuk this was moved to https://github.com/rabbitmq/rabbitmq-management/issues/98 which was fixed for 3.6.1.\n. Thank you!\n. Is this intended to be for master?\n. :+1: \n. @binarin is this meant only for master?\n. No, master will become 3.7.0 in months. stable is used for point releases.\n. 10 or 16 sounds good.\n\nOn 6 ene 2016, at 12:10, Lo\u00efc Hoguin notifications@github.com wrote:\nDepending on the machine the ideal number tends to be between 10 and 100. A default of 10 is probably fine, these processes don't tend to take much memory.\nI know it is already configurable in some places (web stomp i think?) so I will check everywhere and make it an option where it is missing.\n\u2014\nReply to this email directly or view it on GitHub.\n. Can be relevant: https://github.com/rabbitmq/rabbitmq-server/issues/248.\n. Thank you, @Ayanda-D and @dcorbacho. We will QA the issue before Monday noon and produce a build for the user to try.\n. We believe this is fixed in https://github.com/rabbitmq/rabbitmq-server/pull/533. Happy to provide a one-off build from stable to let interested folks help verify.\n. @Dixeat wait for 3.6.1.\n. It does so starting with 3.6.0.\n. Thank you.\n. Oops, my comments ended up being on a commit in a fork.\n\nI suggest that we rename exchange-delete to exchange-delete-in-progress. This is a fairly unexpected use of runtime parameters but no different in practice over using a separate local table, so why not.\nAlso, have we considered what happens when a node fails right after storing the parameter, then coming up? Runtime parameters are stored in a durable table. Should we perhaps clean up all such parameters on boot?\n. It is indeed a similar structure and it is already scoped per vhost, just like what we want. So I'm OK with using it.\n. There are test failures in the Java client (exchange.delete, exchange.declare tests). I'm investigating.\n. This can go into stable.\n. Batch sync is available in 3.6. Building indices has little to do with that, however.\n\nOn 9 ene 2016, at 1:44, noahhaon notifications@github.com wrote:\nApologies if this is the wrong venue, but this has been an ongoing operational issue for us with large RMQ clusters\nWhen adding a new node on a cluster with many dozen queues and thousands of bindings, the new nodes get stuck in \"rebuilding indices from scratch\" for quite some time (hours in some cases) with no progress indicator This happens after autoclustering, with no queues assigned to the new node by policy, or otherwise\nAn improvement would be to add support for batch synchronization of configuration information (or whatever it is synching, the logging is vague here) and improve the logging to give some sort of indication of progress\n\u2014\nReply to this email directly or view it on GitHub.\n. No need to guess: is there a lot of CPU and some disk activity going? Or a constant stream of network I/O (not necessarily very high) the entire time?\n\nSee rabbit.mirror_sync_batch_size and lets continue this on rabbitmq-users as there's currently no specifics to work with.\n. Will merge as soon as I have a go-ahead from @dmitrymex.\n. Thank you, gents.\n. Because #392 was for 3.6.x, this should have been submitted against stable. I rebased and merged manually.\n. @dcorbacho @hairyhum we certainly can introduce another constant, DISTRIBUTED_SUPERVISOR_WAIT, and make it 70s by default (kernel.net_ticktime default + 10s). However, ideally we should make these values configurable.\n. @binarin thank you. Note that we are almost done with #94, which basically replaces our logging subsystem.\n. Absolutely, just making sure you're not caught by surprise. Once we merge #94, would you be interested in submitting another PR (with just the test) against master?\n. @binarin FYI, #94 is in.\n. Should be fixed by https://github.com/rabbitmq/rabbitmq-common/pull/47.\n. @lemenkov FWIW we haven't seen this stack trace in a while (3.6.3 is a year old).. Yes, it makes sense to handle normal exits from the socket and terminate early. Thanks for digging in this.\nI feel we should modify some error messages to produce a different message when the process is waiting for something other than channels and hitting a timeout.\nEither way this is just log noise but log noise sometimes translates into severity 1 escalations, so we are all for reducing it.. Clean stops in mainloop/4 make sense, the only thing I'd test is whether all dependents such as channels are cleaned up correctly but they should be or we have a different (and bigger) problem.. unfortunately our team couldn't find a way to reproduce the scenario described in https://github.com/rabbitmq/rabbitmq-server/issues/544#issuecomment-371504926 but we're testing a PR that incorporates @lemenkov's idea in #1550, which will ship in 3.6.16 and 3.7.5.\nI'm not going to change the milestone because the issue as originally reported was addressed in 3.6.1,\nand https://github.com/rabbitmq/rabbitmq-server/issues/544#issuecomment-371504926 describes is a different scenario which produces an identical stack trace in the logs (but is still nothing more than log noise).. Note that eexists is very different from eaccess and most likely can simply be ignored (given the file can be used, of course).\n. Will close until we come across a different way to reproduce. Thank you, @dcorbacho and @Gsantomaggio.\n. I vote for 18.x for a different reason: fewer obscure cipher suite issues for TLS connections, in particular with ECC.\n. @essen thanks, that's good to know. By the time 3.7.0 ships we'd have 18.4 or 18.5 or maybe even 19.0, so it's 18.x vs 17.5.\n. We can bump it to 18.3 for now and 18.4 (or even 19.0) at a later point. We have months to do that before 3.7.0 ships.\n. Effort medium as the number of PRs involved in this spans across our entire sub-project portfolio.\n. I am not 100% sure but suspect that CLI tools use generated node names so that they can run concurrently. How can we  accomplish that while also using a limited number of names?\n. @hairyhum :+1:. Feel free to work with @binarin on a proof of concept when you have a chance.\n. The PR provided by @binarin looks reasonable, I will test it later today. Thank you for your ongoing contributions.\n. I've been running 2 instances of rabbitmqctl and one rabbitmq-plugins in tight loops with this patch for half an hour now, looking good. epm -names reports that CLI node names are what we expect. Looking good so far.\n. Fixed by https://github.com/rabbitmq/rabbitmq-server/pull/552 (and https://github.com/rabbitmq/rabbitmq-common/pull/48). I like the solution we now have, thank you @binarin!\n. See https://github.com/rabbitmq/rabbitmq-server/issues/551 for preliminary discussion of Cuttlefish, their approach and how the sysctl format deals with collections.\n@hairyhum lets continue here.\n. We can't break existing configuration files that use our values. It's not a lot of code and I think our format works OK. I'd keep it.\n. The above comment raises a couple of questions:\n- Given we will be introducing a new config format, how can we support existing configs? That would be highly desirable.\n- For the new format, how many breaking changes can we tolerate, given that this will ship in Q4 2016, so even relatively new features such as byte size formats will be used in a lot more installations.\nThe former question is much more important and if we can do it, it will largely take care of the latter.\nAnother question: if rabbitmq.config is an \"advanced\" pure Erlang config, what should the sysctl version be (should we go with that over YAML)? rabbitmq.conf will be an awful lot similar to rabbitmq-env.conf to some. On the other hand, we used to see people name the config rabbitmq.conf, so maybe the -env part is, in fact, a good differentiator? Just something to think about.\n. If we can things backwards compatible and Cuttlefish has a data type which is effectively what we reinvented, I think we should use Cuttlefish with a backwards-compatible translation function.\n. Per discussion with @hairyhum and @dumbbell, we're going to proceed with sysctl format/Cuttlefish over YAML with forked Cuttlefish.\n. We've found an interesting problem with this branch: https://github.com/rabbitmq/rabbitmq-server/issues/649 :) History repeats itself. rabbitmq.conf was once used to set environment variables, and later became rabbitmq-env.conf. So we need to eliminate all warnings related to that change as part of this branch.\n. Merged. We will be refining the config syntax in the upcoming months, so if you have feedback on it, please post it to rabbitmq-users or here.\nGreat job, @hairyhum.\n. One follow-up issue discovered recently: https://github.com/rabbitmq/rabbitmq-server/issues/987.\n. It would be interesting to know what @essen, @dcorbacho, @Ayanda-D, and @Dzol think of this.\nWhile #550 is going to happen, supporting Erlang term file configs would be very nice for backwards compatibility. We don't use any particularly sophisticated data structures so there should be a way to do that by coercing YAML to the Erlang terms we use today.\n. So they effectively load two configs and merge them? That sounds reasonable for us.\n. OK. Whether or not we want to use the same format and Cuttlefish belongs to #550. Is the generated .config file still loaded by application controller or one of Riak apps?\n. So in other words, if we are to follow this route (which is quite clever IMO), validation would be handled in #550 and this issue should be simply closed because there's no real need to do this.\nAny objections to this approach?\nAlso, @hairyhum please take a look at how they handle file generation (e.g. where the target file is located \u2014 is it next to the source one?) and how we can make it work with our current way of setting config file path (which is via environment variables). This also can complicate upgrades, so we need to consider that.\n. @Ayanda-D yes, your thinking about the kind validation we could provide is close to what I had in mind.\n. Good finding @hairyhum. This alone may be what tips the scale towards YAML: collections are well covered there. Choices, choices.\n. It looks pretty good. Some names may be worth revisiting and some examples are missing (e.g. with multiple listeners) but it does demonstrate that Cuttlefish is quite adaptable.\n@hairyhum please continue expanding the example with more cases (and settings, we likely don't list 100% there). I'm personally increasingly warming up to the idea of using Cuttlefish with its native sysctl format with a custom mapping over YAML.\n. Closing this as this discussion got gradually intertwined with #500. We won't be validating the config ourselves, at least not yet. But we did learn enough about how Cuttlefish approaches this and what it can do (for #500).\n. That's a good idea. Can get pretty hairy \u2014 thanks, shell script! \u2014 as well.\n. Two things to consider:\n- Tokens can and in practice will come from the clients, e.g. Web clients, so implicit grants should be supported.\n- Tokens need renewals, e.g. Cloud Foundry Java client performs token renewals automatically.\n. A prototype is available and undergoing evaluation (e.g. if the feature set offered makes sense).\n. We have not one but two prototype plugins now plus a library that works with the JW* soup. There are still open ended questions around UAA and OAuth 2.0/OpenID Connection integration but this part is done.. Do you expect this to go into 3.7.0 only? If not, perhaps you want to submit this against stable.\n@dmitrymex @bogdando any comments on this?\n. Closing per @bogdando's comment.\n. Can you briefly describe your workload? E.g. median message rates, how many queues are used, etc (approximates are fine).\n. It can be; we need to reproduce to tell for sure ;)\n. Thank you for the data.\n. As part of this we need to update plugins that provider interceptors. For example, rabbitmq-message-timestamp fails to activate with  https://github.com/rabbitmq/rabbitmq-server/pull/578 and https://github.com/rabbitmq/rabbitmq-common/pull/49 checked out.\n. I'm getting an undef when I make run-broker with rabbitmq-server-559 checked out. I will give it another try.\n. I confirm that with manual testing against rabbitmq_message_timestamp, disabling the plugin does have effect on newly opened channels, and so does re-enabling. However, test suites in rabbitmq_test and rabbitmq_federation fail.\n. Please ask questions on rabbitmq-users.\n. You are not specifying any information that can be used to reproduce: neither RabbitMQ version nor any error messages. There's also a page on Windows quirks.\n. Is this mean to be included into 3.7.0 only?\n. @bogdando should we merge?\n. Please ask questions on rabbitmq-users.\n. effort-high because migration of existing data can take a while to cover well with tests. Otherwise this is effort-medium.\n. I'm afraid that's not gonna fly with hosted RabbitMQ providers, and at least some Cloud Foundry users. It would be great to automatically migrate messages on boot (if we can).\n. @hairyhum given that upgrades perform a backup of the node data dir, that sounds reasonable.\n. Correct.\n. @tenor this is exactly the kind of thing we are trying to avoid: current default backing queue module is just way too complicated and tries to be very smart. It falls on its ass in practice. Default queues are much dumber and much more predictable.\nIt's not any more difficult to switch queue mode to \"variable\" than it is to \"lazy\" right now. This change is not going to ship until Q4 2016 anyway, so we may find all kinds of improvements to either mode before that.\n. What is in RabbitMQ log files?\n. No, this means that as far as RabbitMQ is concerned, it has shut down. It's the service script that had an error.\nIt can be a real struggle to reproduce, e.g. as #76 and #92 have been.\n. I now think this can be caused by the incorrect PID being passed to systemd, see #664, #573, #666.\n. @ejogee what RabbitMQ package version do you use?\n. Our team uses rabbitmq-users for questions.. Merging per discussion in #346 and another round of QA on R16B03 and 18.2.1, with and without HiPE.\n. @binarin your commit message says that\nThe only thing is that you need to add `NotifyAccess=all` to your unit\nfile to make everything work well.\nshould we do it in our own RPM package?\n. @binarin someone brought up a few related questions today: should we use --pid to communicate RabbitMQ node PID to systemd? Can it be that os:cmd/1 uses a separate PID? Can we notify systemd when RabbitMQ shuts down?\n. I see that every os:cmd/1 call starts a separate port/OS process. Try running\nos:cmd(\"sleep 10 &\\necho $!\").\nmultiple times in a row.\n. The comment above means that we can't let systemd-notify use the caller OS process' pid.\n. @binarin is this shortcoming worth delaying 3.6.1 for in your opinion?\n. This is in follow-up to https://github.com/rabbitmq/rabbitmq-server/pull/573, references #52 (where most of the discussion is).\n. Merged too early, there is a comma missing on the changed line.\n. rabbit.cluster_nodes is a collection so we should support it in the new config. I can't think of a scheme better than numbers, although we don't care about ordering.\n. @hairyhum that's fine because #546.\n. Some initial thoughts after a quick QA session.\nThe changes are backwards compatible: I managed to use my rabbitmq.config by overrinding RABBITMQ_CONFIG_FILE just like I do today.\nWe need to make sure that RABBITMQ_GENERATED_CONFIG_DIR is local to the RabbitMQ \"home\" directory for generic UNIX tarball and make run-broker when running from source, otherwise node fails to start with a fairly obscure error message.\nSysctl format examples need to be easier to find, I think they should be under docs just like rabbitmq.config.example.\n\"Additional\" config should be \"advanced config\", because it is meant to be used for more complex cases.\nNot sure if scripts/rabbitmq.schema belongs under scripts. The file is currently treated as binary by git, while it's fairly human-readable.\nUsing the new format seems nice so far. Will try to replicate more complicated examples and report back.\nLastly, we need to begin porting documentation examples (as found on the website). We can't remove existing Erlang term examples entirely, so they'd have to co-exist for some time. I'm happy to help with that and explaining the difference in various documentation guides.\n. make generate-config is currently broken: cuttlefish is now under script, plus it exists with code 1 and produces no error messages.\n. We currently do not log correct config path. E.g. if I export\nRABBITMQ_CONFIG_FILE=./etc/rabbitmq/rabbitmq.sysctl\nand start the node, it will log that effective config path is ./etc/rabbitmq/rabbitmq.sysctl.config and that it is not found.\nWe should log the .conf path if it's present, and separately log generated config path (say at debug level).\n. Related to the comment above: config file paths are displayed in the management UI, we need to make sure those values are correct and what the user would expect as well.\n. @hairyhum @dumbbell suggested that eliminating plugin extraction entirely should be possible with modern OTP releases. If using .ez files directly is possible, can we perhaps scan code path for schema files?\n. or we could include bundled plugins schema in the server repo, then 3rd party plugins would use advanced config. Not ideal but would be an improvement over what we have today.\n. @hairyhum it sounds reasonable to me to ask for kernel values to be tweaked via advanced config. What kind of Windows-specific behaviour do you have in mind?\n. We need to add schema and examples for Web STOMP and Web MQTT, otherwise this looks good to merge to me.\n. Is this still relevant with 3.6.6?. @binarin I believe this expectation is fair, so we'll leave it open.. I'll close this, @binarin feel free to re-open if there's anything specific we still can do.. @essen what was the Elixir erlang.mk plugin you recommended again?\n. Plugins supply their own commands. We need to account for clashes, however.\n. This complicates things quite a bit, by the way:\n- CLI tools will need to account for plugin expansion\n- Loading CLI extensions from plugins will slow down tool startup, as demonstrated by Leiningen and likely CF CLI\nWe can consider using a git-like system where commands are convention-based and if such a command is used (e.g. rabbitmqctl federation-plugin status), we translate it into a function call, e.g. rabbit_federation_cli:status(Args). This does not free us from having to worry about the items above, just thinking out loud.\n. effort-medium => effort-high because the sheer number of commands in our CLI tools made the total effort several man-months, not man-weeks.. @bdshroyer has expressed interest in QA'ing this. We would still highly appreciate feedback from @videlalvaro, of course.\n. I'll close this because there were changes in relevant areas and new information suggests there may be other improvements worth considering first.. @noahhaon I'd file a separate issue for that but OK, we can include RabbitMQ app status into the list or filter out nodes that are not fully started (which might be confusing to some, by the way).\n. FTR, this was discovered in an escalation and the investigation is still in progress. We have a patch candidate but it needs testing. We also have a couple of potentially related issues which we haven't gotten to the bottom yet, which may or may not be actually related.\n. To make it clear: this issue is about (one more) missing/unreasonable timeout value. There can be more issues reproduced by the sequence posted by @Gsantomaggio. Our candidate patch introduces a timeout. The rest is still under investigation.\n. @dumbbell this fails to merge cleanly into master. You are more familiar with the recent logger changes, please resolve them.\n. Please ask questions on rabbitmq-users. Stats indicates that the node hosts statistics database which is used by the management plugin.\n. From what I see in the docs, it would require client library modifications. I'm afraid we can't do it for every client library and every protocol we support.\n. @williamsandrew not on our team. I trust @carlhoerberg's judgement that no client modifications will be necessary. We will consider this for 3.7.x or 3.8.0.. We have a working prototype, looks like this can go into 3.7.0 with little risk (and no delays).. We have contributed a couple of fixes upstream and they were accepted, this is approaching the finish line.. PRs for AMQP 0-9-1, MQTT, STOMP are merged. AMQP 1.0 is left to be done.. At this point the Proxy Protocol is supported by enough popular vendors/projects (from HAproxy and Nginx to AWS ELB) that it's on F5 to add support to their products.\n@Jakauppila thank you, your input is very helpful as always. May I ask you to share a small example, either here or on rabbitmq-users? :) I'd happily add a note for F5 users to our docs.. Not all proxies terminate TLS. This protocol sends a bit of data before any other protocol-specific data, at that point TLS upgrade has been completed. The protocol only relays some basic client information such as the real IP address. No certificate information is exchanged and the goal of this issue is to only obtain the real IP address, nothing else.. @gotthardp @gmr would love to hear your thoughts on this.\n. We cannot determine if a plugin is compatible. We should give plugin authors the tools to explicitly say what version range(s) they support, then filter out those that do not match.\n. @kendraper see #273 and #275. I'm afraid the -r flag isn't supported on OS X:\necho \"abc\" | sed -r 's/abc/cde/g'\nsed: illegal option -- r\nusage: sed script [-Ealn] [-i extension] [file ...]\n       sed [-Ealn] [-i extension] [-e script] ... [-f script_file] ... [file ...]\n. @dumbbell FYI.\n. Submitted against the wrong branch.\n. @dumbbell FYI.\n. This solution isn't perfect in that it creates some duplication that can be avoided in theory but cross-platform shell script scoping with includes is not my forte.\n. Sorry but 3.3.x is no longer being developed. Please try if that's still the case with 3.6.0.\n. Still present in 3.6.x.\n. There are no plans to make in place upgrades possible between major versions but blue/green deployments are decently documented now and achieve roughly that.. rabbitmqadmin ships with the management plugin. If you have specific suggestions in mind, please file an issue there https://github.com/rabbitmq/rabbitmq-management/.\n. One fairly obvious idea is to produce a symlink to it somewhere in PATH. That wouldn't work for all package types and the tool wouldn't work if rabbitmq_management isn't enabled. So it's not a no-brainer.\nI suggest starting a rabbitmq-users thread.\n. It's not OSes but packages. For example, for Debian and RPM we have a chance to run post-install scripts to copy/link rabbitmqadmin. In generic UNIX packages post-install scripts simply don't exist.\n. Changing milestone per discussion with @dumbbell.\n. You can do this via HTTP API today (poll and force close connections). However, I expected to see this after #500 and they have a lot of similarities.\nWe won't be providing a channel limit per user but you can limit max number of channels per connection via rabbit.channel_max.\n. @johnfoldager in your opinion, should this limit be global or scoped per vhost? We see a lot of interest in per-vhost connection limit (already done, #500) but there is no consensus on this one.\n. We'd like to understand the use cases better before we spend more than a spike worth amount of time on this => clearing milestone.\n. @wmoran-uatc we have introduced per-vhost connection limit in #500. However, we now fail to come up with a compelling use case for this per-user limit and there's no time to fit it into the 3.7.0, too. So it is on the back burner but the benefits of connection limits are clear to our team :). @wmoran-uatc we will consider this feature for a future release, thank you.. @jesferman not into 3.7.0, then we will see.. There are no updates on this feature. We still see relatively little demand for it, so at the moment it is not in the backlog for 3.8 or any other release. We will update this issue should that change.. Locking since a couple of use cases for this have been explained to our team and we don't need any more +1s until we actually get to investigate it.. I'd like to be clear: this is a low priority as this is a feature requested by a single user.\n\"receiving node\" is ambiguous. I assume what you mean is the node that accepts a message from  client?\n. Sorry but there will be no per-protocol configuration. The plugin will inject a header and if protocol is extensible with optional headers (e.g. STOMP), such headers should be propagated. Some protocols, e.g. MQTT, are not extensible.\n. See https://github.com/rabbitmq/rabbitmq-routing-node-stamp.\n. It was developed during the 3.6.1 cycle, I cannot think of a better milestone (if we set one at all).\n. It should appear on the binary Community Plugins page later this week.\n. @bdshroyer sure, you don't need my permission to make such changes ;)\n. Questions belong to rabbitmq-users. Please provide more information about your environment and how you obtained the source.\n. This needs some work, as explained on Slack. It currently uses not particularly clear wording and doesn't distinguish between exclusive and auto-delete queues, which have different removal reasons/events.\n. @dumbbell @hairyhum @essen @bdshroyer any objections?\n. I think @sega-yarkin's point is that it should be ts by default. Introducing an environment variable for easy overriding is a good idea.\n. I don't think many users even know you can tune scheduler-to-CPU bindings, so there are no particular expectations. @sega-yarkin provides a realistic example where the default is clearly sub-optimal.\nPerhaps @dcorbacho @Dzol and @Ayanda-D have an opinion on this?\n. Scheduler migration between cores is becoming something we see reported more and more often. We should consider picking a value other than default soon.\nThanks for the data point, @binarin.\n. Related: #1528.. Please ask questions on rabbitmq-users.\nYes, it is currently expected. Perhaps it'd be a good idea for rabbitmqctl to pay more attention to RABBITMQ_USE_LONGNAME. rabbitmqctl accepts the -n option which allows you to use any node/host name, however.\n. CLI documentation now covers this case and 3.7.0 introduced --longnames.. 3.3.x isn't getting any more releases, please upgrade to 3.6.0.\n. @darshanmehta10 the docs currently cover RabbitMQ 3.6.0, queue parameter adjustment via STOMP headers isn't available in 3.3.x.\n. Policies only control optional arguments (\"x-arguments\"), not durability, exclusivity or auto-deletion.\n. Looks reasonable, thank you.\n. This can (should?) go into stable, too.\n. @cbandy no need, this was for our team members. We will cherry-pick. Thank you.\n. Tested this on OS X and Ubuntu (with a Debian package), found no issues with it. @dumbbell FYI.\n. Fixed in #617.\n. @juliocr-ciandt we do not use GitHub issues for investigations, discussions and such. Please post your findings to rabbitmq-users, our public mailing list.. @juliocr-ciandt there is no shortage of threads on the list that explain how to get a really detailed memory usage breakdown from a node, e.g. using rabbitmq-top (which ships with 3.6.12).. @juliocr-ciandt there are also existing threads and a doc guide that mention what VM high memory watermark values are recommended and what easy to try alternative queue properties (such as lazy queues or durable queues + messages published as persistent) you can try.. I confirm that with this change, triggering a memory alarm on a mirror node that's undergoing a sync will pause, and then continue when the alarm clears. I used a queue with 2M messages enqueued while the node that hosted a mirror was shut down. \n. Using werl makes sense to me. Will discuss the pros and cons of using erlsrv on Slack.\n. Thanks for the update, @Ayanda-D.\n. @Ayanda-D thanks for the update.\n. I'm not sure what this should be labeled with, we need a label for blocked/waiting issues and possibly OTP bugs.. @GauravSharmaDA our team does not use GitHub issues for questions, investigations, RCA and so on. Please take this to rabbitmq-users.\nAs for an update, we have identified a known Erlang/OTP issue that makes -detached on Windows not 100% reliable. Not sure if it was addressed in recent releases.. RabbitMQ Windows installer sets up a service that runs the node in foreground. That's the recommended way of doing it (just like service managers are on Debian and RPM-based Linux distributions, even though -detached doesn't have any known issues there).. Just like in #403, you are using a version that's not getting any more updates. Please use 3.6.0. I'm also not convinced that what you observe has anything to do with a specific exchange types. Your node is probably hitting a RAM alarm (connections use RAM, a few MBs per connection unless you configure TCP socket options).\nThis belongs to rabbitmq-users. Please use 3.6.0 and provide full server logs.\n. Thanks for digging this one out, @StevenBonePgh. I think the right thing to do here would be to normalize (e.g. force lowercase) and de-duplicate node lists. With that it may or may not matter if we revert to using uppercase hostnames.\n@dumbbell @hairyhum thoughts? \n. I've submitted a PR that upper cases calculated hostname. This restores the case used in 3.5.7 and will only affect those upgrading from 3.6.0 (or simply set RABBITMQ_NODENAME): https://github.com/rabbitmq/rabbitmq-server/pull/637.\n@StevenBonePgh what do you think of this solution?\n. Now that I think of it, perhaps lowercasing was the point of https://github.com/rabbitmq/rabbitmq-server/commit/1fb451090a2557f35a2800f9d04c476ff1ad6a22 and thus the solution for those upgrading from 3.5.x should be to set RABBITMQ_NODENAME explicitly. We need to discuss with the team.\n. Someone points out that using lowercase has one advantage: it is consistent across all platforms. We found that %COMPUTERNAME% value can use either case. Since there is a workaround for this issue \u2014 simply set RABBITMQ_NODENAME \u2014 I think perhaps we should mention this in the release notes but keep the current behaviour.\n. After discussing this with the team, we decided to keep the behaviour 3.6.0 has. It's not necessarily better or worse than what we had with %COMPUTERNAME% but it is more predictable and consistent across OSes.\nA workaround is to set the RABBITMQ_NODENAME environment variable to a value that uses the desired hostname case. We will update the docs and release notes. Again, thank you for your help, @StevenBonePgh!\n. We have a separate repo for next gen CLI tools (not yet public) but the issue can stay here. I think rabbitmq-diagnostics is a good idea. rabbitmqctl report and friends cannot be removed immediately but we can switch all the docs to use rabbitmq-diagnostics.\n. @bdshroyer @dumbbell @camelpunch WDYT?\n. report, status for starters. We can introduce a new operation for rabbit_diagnostics:maybe_stuck/0 there, too.\n. rabbitmqctl has a lot of stuff in it. Sub command delegation can be used to retain backwards compatibility but we can fairly easily do the same thing without it.\n. Plenty of people do this by polling (using HTTP API or passive queue.declare). I'm not sure how you expect alerts to be triggered but this is something that belongs to a separate plugin and might be easier done in a small separate app.\n. @carlhoerberg with the tip of stable at least, I can't reproduce with the following steps:\n- Start a 3.5.7 node, add a user server-623 with password \"server-623\"\n- Tag the user as administrator\n- Stop the node, copy its database directory to /tmp/3.5.7\n- Start a 3.6.x node with RABBITMQ_MNESIA_DIR=/tmp/3.5.7\n- List users with ets:tab2list(rabbit_user).\n- Change password with rabbitmqctl change_password server-623 server-623-new\n- Try authenticating with rabbitmqctl authenticate_user server-623 server-623-new (it succeeds)\n- Try authenticating with rabbitmqctl authenticate_user server-623 server-623 (it fails)\nAre the steps I take different from yours?\n. Some digging with\ngit diff rabbitmq_v3_6_0..stable -- src/rabbit_auth_backend_internal.erl\ngit blame -- src/rabbit_auth_backend_internal.erl\nsuggests it was fixed in f1f28eac1bc955580bbead82721d75c45e69999d by @hairyhum.\n. Assigning to @hairyhum because he fixed the issue as part of https://github.com/rabbitmq/rabbitmq-management/issues/117.\n. @edmorley I'm afraid adding a test that uses two different database schemas (from 3.5.x and 3.6.0) is going to be quite hard.\nThe best I can think of is manually injecting a user with a non-standard password hashing function. I have no objections to having an issue for that.\n. I'm wrong, it is listed in help. I was working on an issue that involved both 3.5.x and 3.6.x nodes and used rabbitmqctl from 3.5.x. Sorry about the noise.\n. We believe this is fixed in #631. We will produce a build for users to help verify.\n. @jcasale thanks, I'm attaching a build which includes #631. Please give it a try.\nrabbitmq-server-3.6.0.625.exe.zip\n. @jcasale thank you for stepping up. We'll consider it for 3.7.0.\n. What directory is used and how are things configured then? And by \"doesn't require install\" you mean \"installed via uncompressing an archive\" or something similar?\nPlease refrain from using vague and meaningless terms such as \"real enterprise deployments.\" Let's talk specific problems and improvements or I'm going to close this.\n. The idea of configuring RABBITMQ_BASE in an installer step makes sense. Upgrades will be tricky, however.\n. @dcorbacho @Dzol @Ayanda-D @dumbbell @Gsantomaggio do you have experience with low ERL_FULLSWEEP_AFTER values?\n@bdshroyer FYI as you expressed interest in how the runtime works.\n. Doh, forgot @essen.\n. Correct. Well, it's not much effort to merge this for 3.6.1\nbut we'll do as you say :)\n\nOn 16 feb 2016, at 15:29, Dmitry Mescheryakov notifications@github.com wrote:\n@bogdando: please take a look\n@michaelklishin: am I right that stable will go into 3.6.1? If yes, then I suggest postpone merging the fix until 3.6.1 is release. The fix is not that important.\n\u2014\nReply to this email directly or view it on GitHub.\n. I will produce a build for the reporter to try. Thank you, @hairyhum @Gsantomaggio.\n. @hairyhum I'd personally seed the user table with data that would mimic a 3.5.x user record migrated, and mess with application env less. But your suggestion is not unreasonable, my point was that having a real 3.5.x data directory is unnecessary in this case.\n. Please ask questions on rabbitmq-users. STOMP supports multiple destinations and in 3.6.0, even allows you to override queue name and properties. All that is headers-based and should be available to Web STOMP (which is what you use).\n. Also note: queue names and subscription IDs are not the same thing to RabbitMQ, in particular you can have multiple subscribers/consumers on a shared queue. But then again, whether a server-generated or client-provided queue name is used, depends on the destination type and client-provided headers.\n\nThat is an issue in 3.6.0 which ignored the durable header in some cases. persistent, and older value, should work like before. I suspect this also can make RabbitMQ generate a queue name in your case.\n. Now that I think of it, perhaps lowercasing was the point of https://github.com/rabbitmq/rabbitmq-server/commit/1fb451090a2557f35a2800f9d04c476ff1ad6a22 and thus the solution for those upgrading from 3.5.x should be to set RABBITMQ_NODENAME explicitly. @dumbbell would know.\n. Closing in favour of the current behaviour, see #620 for details.\n. If this is intended for 3.6.x, please re-submit it against the stable branch.\n. Thank you.\n. @dmitrymex @bogdando can I have your blessing, please? This is the first PR submitted by @lefremova .\n. @bogdando we would consider it. I can't immediately say where this belongs. Probably rabbitmq_test but that eliminates the benefits of having Travis CI set up for pull requests (because it's a different repo).\n. Merged and will be in 3.6.1, thank you @lefremova.\n. This issue belongs to rabbitmq-web-stomp, and probably even rabbitmq-users because there is no evidence that this is a bug.\nThe headers you use are\n{'auto-delete' : false, 'x-queue-name' : 'test_name', ack : 'client'}\nwhich DOES NOT mean you have a durable subscription: you didn't use the persistent (aka durable) headers. Please see STOMP documentation and beware of this issue in STOMP plugin 3.6.0.\n. There is no evidence that this is a bug. Like I said, questions belong to rabbitmq-users.\n\nOn 20 feb 2016, at 21:27, darshanmehta10 notifications@github.com wrote:\n@michaelklishin should I open another issue in rabbitmq-web-stomp then?\nthanks\n\u2014\nReply to this email directly or view it on GitHub.\n. Thank you for your time.\n\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. We don't have a way to reproduce and the issue is over 2 years old => closing. Please post all related findings to rabbitmq-users first and we'd be happy to file a new issue once/if enough evidence and data is gathered.. Lets start with some automated test examples that'd demonstrate how it works and what kind of edge cases there may be.\n. @hairyhum I haven't looked at it properly yet, sorry.\n. \"Problem loading a plugin\u2026\"\n\"Some plugin do not specify\u2026\"\nI also don't think that using a pair of two versions will be very clear to the user.\n. @hairyhum this is on the finish line but there are still two things to do:\n- I had to update the copy. Please check that I haven't made any mistakes.\n- When I configure rabbitmq_management to validate rabbitmq_management_agent version so that I can trigger a dependency version mismatch error, I get effective version reported incorrectly.\nSpecifically, with the following added to rabbitmq_management.app.src:\n{dependency_version_requirements, [{rabbitmq_management_agent, [\"3.7.0\"]}]},\nand attempt to start the broker with `make run-broker PLUGINS='rabbitmq_management',\nthe effective version reports doesn't seem correct:\nError: Failed to enable some plugins:\n    rabbitmq_management:\n        Version '[]' of dependency 'rabbitmq_management_agent' is unsupported. Version ranges supported by the plugin: [\"3.7.0\"]\n. Scratch the above, it's due to the fact that in development plugin version was an empty string. When I set it to a realistic value, I get what I expect:\nError: Failed to enable some plugins:\n    rabbitmq_management:\n        Version '\"3.4.0\"' of dependency 'rabbitmq_management_agent' is unsupported. Version ranges supported by the plugin: [\"3.7.0\"]\nSo yeah, this seems to be mostly about copy review and a bit more testing, e.g. with a release build.\n. @hairyhum OK, that sounds like a good idea. Since this is in master, we should also add debug logging for what plugins were checked against what version ranges.\n. Please ask questions on rabbitmq-users.\nSee http://www.rabbitmq.com/passwords.html and https://github.com/rabbitmq/rabbitmq-management/issues/117.\n. @gmr you used WITH_BROKER_TEST_COMMANDS which assumes you want a node to be running.\n. This is a discussion for rabbitmq-users. The Make file you include is not in rabbitmq-server anyway.\n. Another issue with the snippet provided: erl_call isn't found in PATH.\n. You probably want STANDALONE_TEST_COMMANDS: it does not start a node or need erl_call. @dumbbell might be able to explain what requires compiling parts of the Java client.\n. Is this intentionally submitted for master?\n. Lets wait until 3.6.1 is released before merging this.\n. Lets wait and see if this default works well. I have a feeling very few people would use something other than \"restart\".\n. @Gsantomaggio if I understood your comment correctly, this PR works as expected. Merging, feel free to revert if that's not the case.\n. @hairyhum a long long time ago config file names were rabbitmq.config and rabbitmq.conf (the latter is rabbitmq-env.conf now). So on #550, this error seems confusing. We should simply remove that warning in #550.\n. @dmitrymex do I understand it correctly that this PR is based on https://github.com/rabbitmq/rabbitmq-server/pull/647? @bogdando should I merge it in that case?\n. @dmitrymex note that the merge above closed two PRs, meaning one branch was based on the other.\n. @dmitrymex your stable branch contains both commits and it was submitted for merging: https://github.com/dmitrymex/rabbitmq-server/commits/stable. OK, that's not a big deal.\n. @bogdando @dmitrymex should we merge this?\n. I'm not sure what you request is about but this repository is for the core server.\n. FTR: while the issue as reported is in the management plugin, the right thing to do here is to strip line breaks in the server (or reject queue declaration when queue name contains it), so we've filed #710\nin this repo. Thanks, @davidvanlaatum.\n. @dmitrymex @bogdando if you want this as well as #653, #654 to make it into 3.6.1, we have to get a go-ahead from you and merge them today.\n. Yeah, that might be an unintentional issue with list_connections.\nPlease submit this PR against stable.\n. Closing per request from @Ayanda-D.\n. @binarin is socat available out of the box on popular systemd-based distros? I assume that's the case in practice with Perl?\n. @binarin I agree.\n. Compatibility. The NIF was never a requirement but lets keep it around for another feature release or so.\n\nOn 11 mar 2016, at 18:30, Jean-S\u00e9bastien P\u00e9dron notifications@github.com wrote:\nNice patch, thank you! I didn't test it yet, but is there any benefit from keeping support for the NIF?\n\u2014\nReply to this email directly or view it on GitHub.\n. Duplicate of https://github.com/rabbitmq/rabbitmq-server/issues/368.\n. @binarin this no longer merges cleanly, can you please rebase? Thank you.\n. This belongs to https://github.com/rabbitmq/rabbitmq-management/.\n. Please submit this against stable.\n. I suggest using at least 3.6.1 GA.\n\nA general note: we've seen a number of (at least on the surface) similar Mnesia issues in the last couple of months. Several were reported to the OTP team and are supposed to be fixed in OTP master.\n. Fixed in #676 (Erlang/OTP 18.3.1), we've discovered #714 as part of the investigation, will continue there.\n. Closing this as the root cause is supposed to be fixed in OTP 18.3.1. That said, we have a different issue discovered as part of this: #714, will continue there.\n. Please ask questions on rabbitmq-users. I'm not sure what \"federated clustered\" means as federation links do not appear as cluster nodes (they are not cluster members). So please clarify that part in the mailing list thread.\n. Duplicate of #104.\n. Channels don't have any metadata associated with them. I'm not sure if we can extend them without breaking things. Queues already have optional arguments you can use (RabbitMQ will simply ignore those it does not know). I believe all of them are displayed in the management UI.\n. @binarin thank you!\n. RabbitMQ does not allocate memory, the runtime does.\n. Lazy queues will avoid keeping messages in RAM until they need to be delivered to a consumer.\n. @divick please post your questions to rabbitmq-users.\nRabbitMQ does not allocate memory directly and the core and tier 1 plugins do not have any native extensions that could do that. \"old_heap\" in the message suggests a single Erlang process (very likely a queue) used the vast majority of the heap.\nSee Reasoning About Memory Use to learn how to verify that hypothesis. With \"binary\" a more likely reason is memory fragmentation.\nrabbitmq-users archives with the same question.. I suggest that we put this off until we begin working on topic-based authorisation. While this is not directly related, such things need to be designed as a group.\n. User permissions are per-vhost and have always been. The issue description is very vague but I suspect that it's a about having a \"vhost-specific administrator\" since currently administrators can manage everything across all vhosts. This is not on the roadmap for the next release.. Speeding up list_* operations would be great.\n. @binarin do you have any numbers to share from that test? I'm mostly being curious, no objections to making hot code paths of the list_* commands parallel.\n. @binarin thank you, this is a very nice improvement!\n. Questions belong to rabbitmq-users.\n. @bharris47 michael at RabbitMQ domain is one way.\n. @bharris47 I'm afraid there wasn't much progress on this issue recently.\n. @bharris47 can you please trim the logs a bit? 100 MB in size compressed suggests there are probably months of data, or a lot of repetitive entries. The last few days should be sufficient.\n. Just trim to the last 30% then. Thank you.\n\nOn 25 abr 2016, at 18:27, Ben Harris notifications@github.com wrote:\n@michaelklishin this is only around a day of logs. Is there anything specific we can grep for or filter to get a more concise log file for you?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n. Yes.\n. If Pivotal support says they can reproduce, presumably they will share a way to reproduce with our team at some point. The best way the community can help is to help test 3.6.6 milestone releases.\n. @binarin we wouldn't mind shipping it in 3.6.x.\n. @binarin that sounds reasonable. I wouldn't mind using --available and --unavailable over --up and --down if we were after the most descriptive names.\n\nI personally think that --online and --offline are good enough.\n. @bogdando we have an off-site for the entire team this week. We trust your judgement on what's the best way to test OCF.\n. Travis setup for rabbitmq-server is currently semi-broken: it doesn't check out the correct branch in certain cases, plus our test suite doesn't output anything for up to 1 hour sometimes. We are working on moving the test suite to Common Test and will take care of those things shortly after.\n. @dumbbell assigning it to you since you're working on the test suite at the moment.\n. We need to revisit how we use Travis because it wasn't really created for scenarios such as ours: multiple closely related repositories.\n. It's a zero-dependency Python script. Why not bundle it with the Puppet module?\nProducing a .deb package for rabbitmqadmin sounds like an overkill. I'm inclined to think we should consider making it available on PATH in the server package.\n. Fair enough. rabbitmqadmin and the HTTP API do not change often, and if they simply were in PATH it would take care of versioning anyway.\n. Fixed in rabbitmq/rabbitmq-common#75.\n. Thanks. We need to find out if Mnesia has an inter-node communication protocol versioning.\nGiven that 3.6.x supports R16B03, it's worth testing with that version instead of 17.0, too. Or all 3 of them.\nAlso note that ant test-functional in the Java client repo does not use mirrored queues, there is a separate set suite/setup scenario for that and it's going to be tricky to coerce it into using different VMs.\n. Surely we can spend some time (a couple of days, for example) to\ninvestigate Mnesia internals to see if they have any internal communication\nversioning that we can track. Change logs are great but they are rarely\nwritten well, or have 100% of the changes listed.\nOn Tue, Mar 22, 2016 at 6:19 PM, Daniil Fedotov notifications@github.com\nwrote:\n\nAccording to mnesia changelog\nhttp://erlang.org/doc/apps/mnesia/notes.html, it can have incompatible\nversions, but changelog is only way to know it.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/698#issuecomment-199861899\n\n\nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. I suggest that we use that to negotiate version compatibility (the same \"current version\" is accepted).\n8.1 and 8.2 can be considered compatible if we are confident they are for our needs. This means testing RabbitMQ version upgrades on mixed Erlang versions.\nIn the docs we should perhaps recommend the same thing we do today, for now. So this will be a change primarily for PCF that we consider an experiment.\n\nOn 22 mar 2016, at 22:05, Daniil Fedotov notifications@github.com wrote:\nLooks like mnesia have protocol_version, that can be accessed using mnesia:system_info(protocol_version). \nMnesia supports communication with two versions, so called current and previous. \nOnly current version can be accessed directly, while previous is using during mnesia_monitor:negotiate_protocol(Nodes) when adding new nodes.\nLast changes in protocols were:\nOTP version   previous    current\nR14B04    7,6 8,0\nR15B  8,0 8,1\n18.2.3    8,1 8,2\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\n. After spending some time on this, alternatively we could do the following: eliminate the restriction and log warnings when major Erlang versions and/or Mnesia protocol versions do not match. So, turn the requirement into a recommendation.\n. @hairyhum we really care about the list of incompatible nodes. Are they also returns or do we have to rely on the total cluster member list?\n. Thank you. Did you mean this to go into 3.6.x? (we can cherry-pick, since it's a single commit)\n. @lemenkov OK, I will cherry-pick, no need to re-submit. Thank you.\n. @lemenkov also, what lead you to making the change? Is 15s not high enough for your case? Should we bump the default somewhat?\n. Amended the PR a bit, cherry-picked both commits to stable. Thank you, @lemenkov!\n. Thank you!\n. Fixed in #704.\n. Fixed by @Gsantomaggio in #708. Thank you!\n. I installed our Debian package on the same Ubuntu release just yesterday. I believe it was vanilla Ubuntu, so the shell should be dash.\n\nDo you do anything beyond installing the package? What if you try a generic UNIX binary build?\n. We haven't heard of any similar reports, and Debian packages are possibly the most common way of deploying RabbitMQ. That said, it would be interesting to get to the root cause, so lets keep this open.\n. OK, thank you for the update.\n. The cluster thinks it is already s member.\n\nOn 28 mar 2016, at 4:32, Giri notifications@github.com wrote:\nEnvironment\nRabbitMQ 3.6.1\nErlang 18.2.1\nrabbitmqctljoin_cluster fails with below error if a Node is re-joining cluster after it got reset. \nExample, Node A, B and C are cluster members. Server running Node C is crashed and unable to recover, so create a brand new Server but with same HostName as C, install Rabbit and try to join Cluster running with Node A and B, it throws following error.\nError\n{\"init terminating in do_boot\",{function_clause,[{rabbit_control_misc,print_cmd_result,[join_cluster,already_member],[{file,\"src/rabbit_control_misc.erl\"},{line,96}]},{rabbit_cli,main,3,[{file,\"src/rabbit_cli.erl\"},{line,83}]},{init,start_it,1,[]},{init,start_em,1,[]}]}}\nI understand that running RabbitMQ nodes(A and B) cannot know if node C is down temporarily or permanently, but Nodes (A and B) should ignore that fact that C is already a member of cluster but not running now and join C, when join_cluster request comes from Node C.\nother-wise Node C will never be able to join A and B until forget_cluster_node NodeC is manually executed.\nSo, i am wondering why can't running Nodes(A and B) automatically forget_cluster_node NodeC when join_custer NodeA or join_custer NodeB is executed on NodeC.\nSteps to reproduce on EC2.\nLaunch three ec2-instances with host names NodeA, NodeB and NodeC\nInstall rabbitmq on all three hosts and configure cluster with members NodeA, NodeB and NodeC.\nTerminate NodeC.\nLaunch a new ec2-instance with Host name NodeC , install rabbitmq.\nrun `$ rabbitmqctl join_cluster rabbit@NodeA on NodeC server.\nSteps to reproduce on laptop.\n1. Start three rabbimq nodes. \n$ RABBITMQ_NODE_PORT=8001 RABBITMQ_SERVER_START_ARGS=\"-rabbitmq_management listener [{port,15671}] -rabbit ssl_listeners [8101]\" RABBITMQ_NODENAME=rabbit1 ./rabbitmq-server -detached\n$ RABBITMQ_NODE_PORT=8002 RABBITMQ_SERVER_START_ARGS=\"-rabbitmq_management listener [{port,15672}] -rabbit ssl_listeners [8102]\" RABBITMQ_NODENAME=rabbit2 ./rabbitmq-server -detached\n$ RABBITMQ_NODE_PORT=8003 RABBITMQ_SERVER_START_ARGS=\"-rabbitmq_management listener [{port,15673}] -rabbit ssl_listeners [8103]\" RABBITMQ_NODENAME=rabbit3 ./rabbitmq-server -detached\n2. Join rabbit2 with rabbit1 \n$ rabbitmqctl -n rabbit2 stop_app\n$ rabbitmqctl -n rabbit2 join_cluster rabbit1@'hostname -s'\n3. Join rabbit3 with rabbit1 \n$ rabbitmqctl -n rabbit3 stop_app\n$ rabbitmqctl -n rabbit3 join_cluster rabbit1@'hostname -s'\n4. Force reset rabbit3\n$ rabbitmqctl -n rabbit3 stop_app\n$ rabbitmqctl -n rabbit3 force_reset\n5. Start rabbit3, causes running rabbit3 alone with joining cluster with rabbit1 and 2. \n6. So, Try again join_cluster with with rabbit1 or rabbit2\n$ rabbitmqctl -n rabbit2 stop_app\n$ rabbitmqctl -n rabbit2 join_cluster rabbit1@'hostname -s'\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. Resetting a node wipes out all of its data. As far as the rest of the cluster is concerned, that node is still a member. You need to use 'rabbitmqctl forget_cluster_node' first.\n. We cannot assume that every time a node leaves a cluster it also has to be reset. We will consider it.\nOn 28 mar 2016, at 14:50, Giri notifications@github.com wrote:\nI know rabbitmqctl forget_clister_node solves the probelm. My question was why not automatically run this command on running nodes when a node tries to join cluster before making cluster.\nIt will help us automate the thinings on AWS deployment.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly or view it on GitHub\n. Somewhat related: #868 makes rabbitmqctl join_cluster idempotent, which makes sense. Thank you, @binarin.\n. @Giri-Chintala if you are OK with nodes resetting themselves before [re-]joining a cluster, rabbitmq-autocluster is right on the money.\n. @davidvanlaatum my idea was to strip them across the board. But thank you for the suggestion, responding with an error is also an option. We already perform basic queue name validation and had to deviate from the spec slightly where it improved usability.\n. @hairyhum yes, and it is too late to add significant restrictions but stripping off line feeds and possibly unprintable characters makes sense. We only see people shooting themselves in the foot with both.\n. @naag not covered by this issue. Can be considered for 3.7.0 in a follow-up one. With tabs it's less obvious if they may serve a useful purpose. Please start a rabbitmq-users thread so we can discuss?. This breaks support for paths without a filename extension. The problematic line seems to be\nthe one with \"${RABBITMQ_CONFIG_FILE%.*}\":\n\n```\nexport RABBITMQ_CONFIG_FILE=./etc/rabbitmq/rabbitmq\necho \"${RABBITMQ_CONFIG_FILE%.*}\"\nexport RABBITMQ_CONFIG_FILE=./etc/rabbitmq/rabbitmq.config\necho \"${RABBITMQ_CONFIG_FILE%.}\"\n./etc/rabbitmq/rabbitmq\nexport RABBITMQ_CONFIG_FILE=./etc/rabbitmq/rabbitmq.sysctl\necho \"${RABBITMQ_CONFIG_FILE%.}\"\n./etc/rabbitmq/rabbitmq\nexport RABBITMQ_CONFIG_FILE=./etc/rabbitmq/rabbitmq.sysctl.conf\necho \"${RABBITMQ_CONFIG_FILE%.*}\"\n./etc/rabbitmq/rabbitmq.sysctl\n``\n. I've pushed something that fixes this particular problem by usingdirnameandbasename`. I'm hardly great at shell script, so @hairyhum and @dumbbell, please double check.\nWe also need to work on supporting both advanced and advanced.config.\n. @hairyhum this passes the acceptance criteria for rabbitmq.conf and rabbitmq.config but not advanced.config. I think we should support both options for the advanced.config as well.\n. This looks good on OS X (zsh) and Ubuntu (dash). Haven't had a chance to test on Windows yet.\n. Management database is eventually consistent and due to https://github.com/rabbitmq/rabbitmq-management/issues/41 can voluntarily drop events, which can lead to this. I'm closing this as a duplicate of https://github.com/rabbitmq/rabbitmq-management/issues/41, since that's the biggest contributing factor.\nPlease give 3.6.2 Milestone 2 a try.\n. The durable flag is ignored for exclusive queues. It makes no sense for a queue to be durable and exclusive at the same time: exclusive queues cannot survive longer than their declaring connection and connections cannot survive a broker restart.\n. @hammett you can declare a consumer as exclusive without the queue being exclusive. That said, the former is fairly rare. Exclusive queues are not particularly rare but they are supposed to be used for entirely transient data, and so the durable flag for them is intentionally reset by RabbitMQ to false.\nAlso, exclusive consumers sounds like a good (and often necessary) idea but in practice what you really want is multiple coordinating consumers. Otherwise consumer availability becomes difficult to achieve.\n. @dcorbacho \ud83d\udc4d on the idea of GM processes cleanly terminating to re-join later.\n. No more 3.5.x releases are planned at the moment. This particular change is probably easy to backport but there are several related issues that aren't.\n. @bdshroyer values lower than 1.0 are dangerous because this means RabbitMQ and the OS cannot swap out all or even most of the data. This function_clause is probably an issue in rabbit_resource_monitor_misc:parse_information_unit/1, it (or another function that calls it) simply doesn't expect to get this value.\nSo it's not the docs that are not correct, it's a bug.\n. OK, thank you.\n. Looks like this was introduced in https://github.com/rabbitmq/rabbitmq-server/commit/829890be2a664a00205a3cbf04cdf00e18b022a0, and is a misunderstanding of what the value is.\n. Changing a signing key is a very big deal. Since this is a warning, I don't think it's worth the change for now.\n\nOn 31 mar 2016, at 10:09, Riccardo Padovani notifications@github.com wrote:\nSince the release of APT 1.2.7 keys which use SHA1 are considered weak and give a warning on apt update.\nhttps://juliank.wordpress.com/2016/03/15/clarifications-and-updates-on-apt-sha1/\nW: http://www.rabbitmq.com/debian/dists/testing/InRelease: Signature by key F78372A06FF50C80464FC1B4F7B8CEA6056E8E56 uses weak digest algorithm (SHA1)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. @noahhaon thank you, I believe we use gpg. @dumbbell has more info on the release toolchain.\n. @noahhaon that's something Puppet can do. @dumbbell can you please share your findings?\n\nWe are certainly willing to explore options that let us avoid generating a new key.\n. @noahhaon OK, thanks, we'll see if we can do something about this before 3.6.2 (expected next week).\n. New keys are now in place, I've also restored http://www.rabbitmq.com/rabbitmq-signing-key-public.asc temporarily because apparently enough people depend on the sole existence of the file.\n. Announcement of the new keys.\n. @gsker I'm no Debian expert but this was about a whole bunch of things that had to be changed at once. FWIW this happened over 4 months ago and except for those who haven't imported the new key, we didn't have any apt warning reports since then. There are Package Cloud and Bintray Debian repositories available in case that's not good enough for you.\nPlease post your observations to rabbitmq-users since we don't reopen issues that were closed and part of a release.\n. We primarily would like it in stable.\n\nOn 31 mar 2016, at 19:03, Daniil Fedotov notifications@github.com wrote:\nMaybe it should go to stable. I'm not sure.\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly or view it on GitHub\n. Please re-submit against the stable branch. #717 is scheduled for 3.6.2.\n. Not assigning anyone as this will be reviewed by @dcorbacho, @hairyhum, and me.\n. Please re-submit against the stable branch.\n. \ud83d\udc4d \n. Both aspects are actually part of Erlang, not RabbitMQ.\n\nI believe this is addressed by the new config format that also involves validation and (to a much lesser extent) #691.\n. Thanks, folks.\n. This is partially addressed by #550 but we should look into improving things for the classic config format.\n. In 3.7.0 it will for things we know are file paths.\n\nOn 5 Oct 2016, at 10:32, Hydrochoerus notifications@github.com wrote:\nI had an interesting problem with the TLS/SSL options. For some reason it seems that RabbitMQ/Erlang does not verify that the specified cacertfile actually exists. I actually removed my cacert and restarted. The server was listening on the TLS port 5671, but didn't talk back to clients. No log entries were made with the default log levels. I'm running RabbitMQ 3.6.5 with Erlang 19.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @essen is improvements such as these something you'd consider (or already included into) recent/future Ranch releases?. @Algent 3.7.0 will validate file existence and readability for certain values that are known to be files. You can try it today.\n\nUnreadable files do not cause empty strings to be used, they simply cause exceptions when a TCP connection upgrade to TLS is performed.\nI'm not convinced that nodes should not even start if a file cannot be read but that's how it's going to work with 3.7.0, for better or worse.. I'm growing convinced that this is something that can be contributed upstream to Ranch. @essen, would you accept such a feature?\nAs for 3.7.0, the new config format already performs validation for file path values, so some of the most common cases are already covered. We can potentially do even more checks (e.g. that certificates can be loaded/are in the expected format) but there's risk to trying to be too smart with this stuff.. @essen it's not in RabbitMQ itself. Cuttlefish checks that file paths exist and are readable for values of a certain data type.. @evollu this is not a support venue. Please post your question to rabbitmq-users and provide full config file.. @evollu ok, looks like you've used single quotes instead of double quotes for a file path: {certfile,'/tmp/server/cert.pem'}. The config file is an Erlang term file and single quotes in Erlang do not produce a string. Use double quotes.. This is done to the extent we can in #550. We cannot validate every config option in the Erlang term format. #732 focusses on a few specific options.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. @palaiya the DLX guide does not say that. The TTL guide does not say that. Messages are not published to queues but to exchanges. Message expiration has no connection with re-queueing, only publishing to a DLX (if present).\n. I've clarified the language to not use the term \"dead letter queue\" where we can. I found no claims that \"after ttl message should get republished to the original queue\" anywhere (and that is certainly not true).\n. From the DLX guide:\nIt is possible to form a cycle of dead-letter queues. For instance, this can happen when\na queue dead-letters messages to the default exchange without specifiying\na dead-letter routing key. Messages in such cycles (i.e. messages that reach\nthe same queue twice) will be dropped if the entire cycle is due to message expiry.\nDead lettering cycles due to expiry are effectively infinite loops so RabbitMQ disallows them, by design.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. and while we are at it, providing specific code examples that demonstrate what you're trying to do would tremendously help you to get a specific answer sooner, on any forum. \n. If i explicitly publish the failed messages from WorkQueue to RetryQueue.\nThen after ttl messages get back to WorkQueue. Why is it so?\nRe-publishing from your app is not the same as RabbitMQ publishing during dead-lettering: the latter case contains a trail of dead lettering events, which let RabbitMQ break dead lettering cycles mentioned above. When you consume and then publish a message, for RabbitMQ it is a completely new message.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. EACCESS means the effective RabbitMQ user has no permission to load a file, in this case /etc/letsencrypt/live/xxxx.xxxxxx.co/chain.pem.\n. Indexes are flushed to disk every N entries, keep that in mind.\n. @spatula75 since message rate mode do not affect message store, that sounds like a stats collection issue. We had fixed something similar with socket count in the past.\n. Having messages enqueued is certainly a requirement to have any messages in the message store, in fact, messages larger than 4kB (by default). But it is not the message store that reports incorrect values, most likely.\n. @hairyhum I don't think it's the same issue, though. Originally it was reported as a message store size growth. I believe @dcorbacho and I have observed it a few times while investigating other issues. We could all be mislead by the fixed leak but let's try to reproduce the original problem.\n. For those following along, keep in mind that messages smaller than 4096 bytes by default are not even sent to the message store: they are stored in the queue index directly.\n@hairyhum that part is pluggable and we've been considering using a disk-based store for a while. There was a LevelDB-backed plugin at some point. The downsides are predictable: lower throughput (for messages that go to message store) and the can of worms that is using platform-specific native extensions.\nI think we can close this issue and file one for a disk-based message store index.\n. https://github.com/rabbitmq/rabbitmq-server/pull/855 addresses the root cause. We intend to include it into 3.6.4.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. @HW-Siew against my better judgement, even though you've ignored my request to post questions to the mailing list, I'm responding here one last time.\nFirst CA in the chain is example.com, which strongly suggests it is self signed. Either use the same CA for both RabbitMQ and client certificates or add it and its parent CA to the list of trusted certificates system-wide.\nWhen a chain of certificates is used, it is important to make sure both peers use adequate verification depths, as explained in the Certificate Chains and Verification Depth section.\n. Thank you. I haven't applied this to the Lager version in master, not sure if this is applicable there.\n. @binarin sounds OK.\n. When some ring members are unreachable, ignoring log operations for them is probably about as well as we can do.\nEventually gm will be replaced with a Raft-based mirroring, which has a well understood solution for logs getting out of sync.\nI will take a look at the specifics in a bit.\n. This issue is a bit too \"inside baseball\" => not including into release notes.\n. It is not a by product of configuration: schema files ship with plugins and are static.\n\nOn 14 abr 2016, at 23:26, Joseph L. Casale notifications@github.com wrote:\nDo you think that location is maybe better defined as ${install_prefix}/var/lib/rabbitmq/schema? Seems its less config oriented but actually state oriented and a byproduct of configuration.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. @hairyhum ok, please submit another PR if you'd like to change the path one more time.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. @bes1002t believe it or not but some people think exactly the opposite: if a plugin cannot function then RabbitMQ should fail early. Again, all discussions should go to the mailing list, our team does not use GitHub issues for that, sorry.\n. > Because to disable the plugin, the server must be available\n\nYou can disable a plugin without RabbitMQ running: see rabbitmq-plugins docs, in particular the --offline flag.\n. @bes1002t I'm locking this issue because this is not getting anywhere. There is no consensus in the community about how this should work. Sorry but this is absolutely the kind of conversation that belongs to the mailing list.\n. @bes1002t and please cut back the sarcasm. Open source maintainers already get enough of it from people who get  our software for free, expect bug fixes for free, support for free, including on the weekends.\n. Mailing list discussion that followed.\n. @hairyhum @dumbbell is this still relevant after #745?\n. If you intend this to go into a 3.6.x release, please re-submit against the stable branch.\n@bogdando @dmitrymex can I get your blessing?\n. This branch is based on master but submitted against stable.\n. Thank you for your time. You claim that this version is more efficient. Do you have any benchmarks that compare the two versions?\n. @sylvainhubsch any numbers you can share? It'd help us prioritise this issue. Thank you.\n. Thank you, that looks promising.\n. Cherry-picked to stable so it will be in 3.6.3. Thanks again.\n. Moving to rabbitmq-management where it belongs.\n. Thank you.\nNote: this was pre-approved by @bogdando in https://github.com/rabbitmq/rabbitmq-server/pull/757. Waiting for a confirmation from @dmitrymex.\n. Note: this is a follow-up to #550.\nThe reason why rabbitmq.conf is moved to rabbitmq-env.conf today is a historical accident: at some point a long time ago (years), rabbitmq.conf was the name of what is rabbitmq-env.conf today. So the move command was added for a migration period. It now happens to collide with the changes in #550.\nTherefore this issue probably should be considered a part of #550 and excluded from release notes.\n. Another change worth considering is keeping track of vhost message store processes in an ETS table as opposed to using registered processes: with very high vhost churn we run the risk of exhausting the atom table.\n. The above exceptions seems to be due to fact that the first argument to rabbit_msg_store:client_init/4 is msg_store_persistent, which was previously an acceptable value for the node-global persistent message store. However, how the function assumes it is a pid, which is not the case during upgrades.\n. No objections to that. We should consider a separate log file for the\nmigration,\nthen we can only log \"migrating batch N with M queues\" to the main log.\nOn Thu, Nov 24, 2016 at 9:15 PM, Daniil Fedotov notifications@github.com\nwrote:\n\nProbably it makes sense to migrate queues in batches.\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/766#issuecomment-262827868,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQrYz3hxFd9ZXa2bS_kI0FEuhakvYks5rBdQ3gaJpZM4IM0G_\n.\n\n\n-- \nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. I'm pretty sure this is https://github.com/rabbitmq/rabbitmq-server/issues/216 which is already fixed.\n. By the way, if you think it's a separate issue that looks very similar, please let us know.\n. This happens when a message was dead-lettered or federated, and then consumed and re-published by an app. The best we can do is to filter out string values and start from scratch.\n. So this is very similar to https://github.com/rabbitmq/rabbitmq-federation/issues/12.\n. Client libraries cannot be responsible for filtering out such headers. In practice most of them won't. RabbitMQ should be more defensive and ignore the headers it uses when they are provided by clients but pass validation (however we define \"validation\").\n. @Gsantomaggio I'm afraid this message is more confusing than helpful. This needs some more work.\n. The error says there are no LDAP connection pool processes available. Log files might suggest why. In any case, this repository is for neither the management or LDAP auth backend plugins. \n. rabbitmq_auth_backend_ldap.pool_size controls pool size (which is 10 by default). Process unavailability can be natural and transient, so try bumping that to 64, for example.\n. @johnfoldager we appreciate how proactive you are about posting issues but our team uses issues only for specific actionable items and expects them in specific repositories. This repository is not a kitchen sink for plugin issues, or questions.\nPlease ask on rabbitmq-users first, and we will file issues ourselves to the correct repo and with more actionable details\n. @belt-ascendlearning that's a bit too terse. Please post your findings to rabbitmq-users.\n. The queue can still be limited to a fixed number of messages and TTL can still be applied to them, so this is not nearly as some may think it is.\nCalculating message properties size can have a non-trivial throughput effect because it will affect every single message.\n. It does, we cannot store things to disk without knowing the total size. Specifically in your case the runtime knows how much RAM a queue process uses, which is a different piece of information that cannot not be used to calculate queue length in bytes.\n. Some will be willing to take this trade-off, others won't.\n. We need to first see what kind of overhead it has.\nMessage bodies are framed. Finding a way to pre-calculate message size very early is a good idea\nbut chances are it will break inter-node communication protocol and thus can only go into 3.7.0.\u00a0\nOn 23 May 2016 at 19:04:09, uvzubovs (notifications@github.com) wrote:\n\nCorrect. So, is max-length-bytes-include-headers an option?\n\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub:\nhttps://github.com/rabbitmq/rabbitmq-server/issues/785#issuecomment-221018442  \n\n\nMK  \nStaff Software Engineer, Pivotal/RabbitMQ\n. Then let's see if we can make the overhead from including full message size insignificant for most users.\n. That we will try to make it take headers into account and minimize the throughput effect of that before considering other options.\n. I'm not sure what's wrong with that. We intentionally use distinct UNIX exit codes in recent versions, see https://github.com/rabbitmq/rabbitmq-server/issues/396. Please ask questions on rabbitmq-users.\n. rabbitmqctl report certainly works for me here (when invoked without arguments) and we haven't seen this reported elsewhere.\n. @essen that's a good enough start. We will see what more specific requirements come up once the users have a chance to try the SNI option(s).. Should be addressed with https://github.com/rabbitmq/rabbitmq-common/pull/150.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\nThis rabbitmq-users thread could be relevant.\nYour only option to recover the node is to wipe out its data directory or restore it from a backup.\n. Cluster nodes must be upgraded in lock step. See the docs.\n. \u2026between feature releases, which is what you are trying to do.\n. Lastly, the link to \"the docs\" links to a blog post by someone other than our team. Sorry, that doesn't count for official documentation.\n. See Upgrading Clusters on http://www.rabbitmq.com/clustering.html.\n. @lefremova thank you. If you intend this to go into 3.6.x, please re-submit against the stable branch.\n. @dmitrymex can I get your blessing?\n. We are approaching the 3.6.2 release and docs can be updated independently. So I'll merge this tomorrow.\n. Nevermind my earlier comment, I now see a couple of recent comments in #687.\n. Both issues references are scheduled for 3.6.x. Please re-submit against stable.\n. @Gsantomaggio @dcorbacho can we please add a test that publishes messages with priority greater than max configured? It's worth covering for both non-mirrored and mirrored cases.\n. OK, I think it would be helpful to get this into 3.6.2 so tests can come in a separate PR. I will QA this as is.\n. Thank you for the idea. We decided to not implement this as there's little demand for this feature and we see a number of issues. For example, the server and CLI tools don't necessarily use their runtime arguments the same way and it is not a given that both should use the same set of flags. . Thank you. Just in time for 3.6.2.\n. @lemenkov please submit a PR?\n. This was backported to 3.6.6.\n. @binarin WDYT?\n@lemenkov should this also be applied to stable?\n. @lemenkov no problem. I'm waiting for @binarin's opinion on this.\n. Will cherry-pick to stable.\n. No separate issue for this one => assigning milestone to the PR.\n. @binarin wow, thank you.\n. @binarin this does not install for me on Ubuntu 14.04. How was this tested?\nI'm afraid supporting 14.04 is more important than 16.04 at this point for us, so we might have to revert this.\n. dpkg -e suggests that our package depends on\nDepends: erlang-nox (>= 1:16.b.3) | esl-erlang, adduser, logrotate, socat, init-system-helpers (>= 1.18~)\nThe latter is problematic on 14.04.\n. Yeah, so that must be the issue here: we build on a Debian version with a newer init-system-helpers. Will investigate, thanks.\n. Unfortunately the issue with package dependencies is non-trivial.\nWe produce releases on Debian Wheezy which specifies init-system-hlpers 1.18 as a dependency.\nUnfortunately that version is not available on Ubuntu 14.04, only 1.14 is.\nThis means that supporting Ubuntu 14.04 and 16.04 (which uses systemd) in a single package\nis only possible if we switch release infrastructure to Ubuntu 14.04 (not at all expected, of course).\nAn open question remains whether Debian support will work.\nSo we primarily have 3 options for the short term:\n- Revert systemd support and upset 16.04 users\n- Drop 14.04 support and upset 14.04 users\n- Switch to Ubuntu 14.04 for producing Debian packages and hope it will work on Debian\n. Here's a version that hardcodes the problematic version dependency: rabbitmq-server_3.6.2.905-1_all.deb.zip. It works on Ubuntu 14.04 and 16.04 for me. Please give it a try on a nearby Ubuntu or Debian machine and let us know how it goes.\nWe will likely turn the next release into an RC3.\n. Debian Wheezy has init-system-helpers 1.18 but only in wheezy-backports. Oh well. We will document the dependencies.\n. To wrap this up, systemd support will be in 3.6.3 but we'll have to drop support for Ubuntu versions older than 14.04. This only affects the Debian package, of course, generic binary builds will run as long as a supported Erlang release is used.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. I don't understand how npm is involved but we've released new stronger keys earlier today. The old key is still available and 3.6.2 was signed with it.\nLet's move this discussion to the mailing list.\n. http://www.rabbitmq.com/rabbitmq-signing-key-public.asc is now available again. I have no way to verify at the moment but it almost certainly was removed by accident.\n. @NeMO84 you need to import new release key. That's it.\n. https://www.rabbitmq.com/rabbitmq-signing-key-public.asc is not the new key\n(the date in 2007 kind of suggests that).\nSee the docs on http://www.rabbitmq.com/install-debian.html.\nOn Wed, Jun 8, 2016 at 11:18 PM, Nirmit Patel notifications@github.com\nwrote:\n\nMaybe the file was recently updated again?\nHTTP/1.1 200 OKDate: Wed, 08 Jun 2016 20:17:44 GMTServer: Apache/2.2.22 (Debian)Last-Modified: Tue, 07 Jun 2016 09:35:55 GMTETag: \"18f65d-6a6-534acea6414c0\"Accept-Ranges: bytesContent-Length: 1702Vary: Accept-EncodingContent-Type: text/plain\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/810#issuecomment-224714734,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAAEQgeQT3zlCvroDeKdVkeaWm_8C1IDks5qJyOugaJpZM4ImwP5\n.\n\n\nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. My guess is that the author intended to delete this issue but that's not possible on GitHub => closing.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. They key wasn't imported and/or trusted by apt for a reason I cannot claim to know.\nKey 6B73A36E6026DFCA can be retrieved from a public key server with\ngpg --recv-key 0x6B73A36E6026DFCA\nand the keys apt trusts can be listed with sudo apt-key list.\n. @binarin would you be interested in submitting a PR?\n. @bogdando WDYT?\n. In OTP 18.x which I'm looking at it has\nwhich_applications() ->\n    gen_server:call(?AC, which_applications).    \nwhich_applications(Timeout) ->\n    gen_server:call(?AC, which_applications, Timeout).\nso there is no timeout that I can see.\n. OK. I think a value around 30s is more reasonable for this specific case. It would also be much more obvious what the timeout is.\n. Actually, scratch that. It would still be beneficial to use an explicit timeout in our own code but in rabbitmqctl environment specifically it can be closer to 15s. I was thinking about a different place in the code.\n. @hairyhum oops, I had that already done but forgot to push the changes in rabbitmq_common. Done now: https://github.com/rabbitmq/rabbitmq-common/pull/102.\n. Unfortunately having a crash dump alone is next to no specific information. It is not necessary that RabbitMQ itself produced it, tools such as rabbitmqctl could do it.\nCrash dump are generated upon an unwanted exception in one of the standard modules or runtime so try a different Erlang version and make sure it is 64 bit.\nLastly please post questions to rabbitmq-users in the future.\n. Thanks. Folding over a collection of interceptors should be doable and possibly even easier than telling if a plugin provides any interceptors.\n. ./.erlang.cookie: eacces\nexplains what's going on: rabbitmq-plugin is an Erlang tool that tries to read $HOME/.erlang.cookie and fails with EACCESS.\n. The key has changed several weeks ago. It was annouced:\n- On rabbitmq.com\n- On rabbitmq-users\nas well as on Twitter.\nYou're using the old key. See Debian installation docs for instructions.\n. @hairyhum it's fine to keep the module in this repo.\n. @dcorbacho done.\n. Who maintains that package?\n. If @robertlabrie would be interested in transferring package rights to our team we would consider it.\n\nOn 10 jun 2016, at 21:02, Matias Ribichich notifications@github.com wrote:\nFor what it says in the package: Robert Labrie\nChocolatey package\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @robertlabrie Chocolatey docs suggest you need to add us as a maintainer. Can you please add me (mklishin <$> pivotal.io) and our shared team email team <$> rabbitmq.com? I'm new to Chocolatey myself so maybe I need to sign up first?\n. @robertlabrie OK, I've signed up with both of the emails above. Please add us as maintainers or (if possible) co-owners of the package and we'll figure out the rest. Thank you!\n. @robertlabrie rabbitmq and mklishin-pivotal, thank you.\n. It needed a confirmation from me, both accounts should be confirmed now.\n\nThank you, Robert! We should be able to take it from here.\nOn Sat, Jun 11, 2016 at 3:00 AM, robertlabrie notifications@github.com\nwrote:\n\nHi Michael,\nOk, it's done. It says \"pending approval\", I don't know what that means,\nbut lets keep an eye on it and see if it resolves itself automatically.\nIf it's still pending on Monday then I'll drop a message on the Google\nGroup and try to find out what the scoop is.\nOn Fri, Jun 10, 2016 at 7:48 PM, Michael Klishin <notifications@github.com\n\nwrote:\n@robertlabrie https://github.com/robertlabrie rabbitmq and\nmklishin-pivotal, thank you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225321214\n,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe/AEVU5YrG71ss-3Yogpo6nfx7vQYoFqz7ks5qKfe2gaJpZM4IzLn2\n.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225322541,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AAAEQoL-vXGOZXuD--JcB3lUuIfu3EXyks5qKfqAgaJpZM4IzLn2\n.\n\n\nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. We will do a Chocolatey release of 3.6.3 once it is ready.\n. Package source is now at https://github.com/rabbitmq/chocolatey-package.\n. 3.6.3 is submitted for moderation.\n. 3.6.3 is up. Note that it requires using Erlang 18.x due to undocumented breaking changes in 19.0.\n. It is by design. How do we define if two policies have a conflict?\n\nOn 10 jun 2016, at 21:01, A Dube notifications@github.com wrote:\nUnless this is by design, I think when, for example, the following non-conflicting policies are defined, declaring queues should result in all of these being applied;\nrabbitmqctl set_policy ha-2-policy \".*\" '{\"ha-mode\":\"exactly\",\"ha-params\":2,\"ha-sync-mode\":\"automatic\"}' --apply-to queues --priority 0\nrabbitmqctl set_policy lazy-policy \".*\" '{\"queue-mode\":\"lazy\"}' --apply-to queues --priority 0\nrabbitmqctl set_policy qml-policy \".*\" '{\"queue-master-locator\":\"random\"}' --apply-to queues --priority 0\nand only when policies are of the same type and priority should they be applied non-deterministically. Currently, in such a case, only a single policy is applied non-deterministically, regardless of there not being any possible conflicts amongst defined policies.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. They are applied in a deterministic manner: top priority wins, equal priorities are not supposed to be used.\n. I think we can explore a way to merge policies with equal priorities. This will be a moderately significant breaking change so can only go into 3.7.0.\n\nAnother (related) feature request that goes with that is global (vhost-wide) policies for RabbitMQ-as-a-Service kind of environments.\n. Merging policies will be a significant breaking change. I am currently not convinced than doing anything beyond #930 is worth it. I suggest that we close it and wait for feedback from those who needed #930 the most.\n. We already have an issue for that: https://github.com/rabbitmq/rabbitmq-server/issues/541.\n. Please post questions to rabbitmq-users. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. rabbitmq-components.mk is out of date in one of the projects. Please post the exact steps you use to the list, e.g. how do you obtain the source and why you need to build from source in the first place.\n. @nathanejohnson well that's too bad. The issue is that rabbitmq-common is cloned from github while what's in source tarballs is frozen in time. You can clone the server repo and build from the stable branch, it will have rabbitmq-components.mk in sync with what's in git in rabbit_common. We already have an issue for getting rid of this file entirely if possible.\n. In fact our building instructions do just that, clone the repo.\n. Adding a delay is a possibility. Closing a subset of connections is not: 99% of those asking for this feature want to close all connections in a vhost, not a random subset of them.. This raises another possible request: a --global flag that would cover all vhosts. That's really dangerous, though, so I am not sold.\nAlso related: in master when a vhost is deleted, all connections in it are forcefully closed already.. @johnfoldager I guess a numerical --limit is a reasonable request. We will consider it. Thank you for chiming in. It's always good to see specific ideas around new features.. Moved to https://github.com/rabbitmq/rabbitmq-cli/issues/155, since that's where all CLI tool issues belong to now.. https://github.com/rabbitmq/rabbitmq-cli/issues/155 is closed and will be in 3.7.0.. Please post full log files.\n. Aslo, have you considered that sending up to 0.5 GB of messages to disk takes some time and while that happens queues do perform any other operations? Do queues go out of this state eventually?\n. There are no errors in the logs. We'd have to investigate after the 3.6.3 release.\n. @binarin with #911 merged, should this be closed?\n. OK, let's edit issue description and close it then.\n. I cannot access the patch file (could be a temporary S3 glitch) so here it is from rabbitmq-users, inline:\n``` diff\n src/rabbit_variable_queue.erl | 14 ++++++++++++--\n 1 file changed, 12 insertions(+), 2 deletions(-)\ndiff --git a/src/rabbit_variable_queue.erl b/src/rabbit_variable_queue.erl\nindex 5b86cbd..f97ea8b 100644\n--- a/src/rabbit_variable_queue.erl\n+++ b/src/rabbit_variable_queue.erl\n@@ -556,14 +556,15 @@ delete_crashed(#amqqueue{name = QName}) ->\n     ok = rabbit_queue_index:erase(QName).\npurge(State = #vqstate { len = Len }) ->\n-    case is_pending_ack_empty(State) of\n+    io:format(\"purge~n\"),\n+    case is_pending_ack_empty(State) and is_unconfirmed_empty(State) of\n         true ->\n             {Len, purge_and_index_reset(State)};\n         false ->\n             {Len, purge_when_pending_acks(State)}\n     end.\n-purge_acks(State) -> a(purge_pending_ack(false, State)).\n+purge_acks(State) -> io:format(\"purge_acks~n\"), a(purge_pending_ack(false, State)).\npublish(Msg, MsgProps, IsDelivered, ChPid, Flow, State) ->\n     State1 =\n@@ -1612,10 +1613,12 @@ collect_by_predicate(Pred, QAcc, State) ->\n %% and an ack to the queue index for every message that's being\n %% removed, while the later just resets the queue index state.\n purge_when_pending_acks(State) ->\n+    io:format(\"purge_when_pending_acks~n\"),\n     State1 = purge1(process_delivers_and_acks_fun(deliver_and_ack), State),\n     a(State1).\npurge_and_index_reset(State) ->\n+    io:format(\"purge_and_index_reset~n\"),\n     State1 = purge1(process_delivers_and_acks_fun(none), State),\n     a(reset_qi_state(State1)).\n@@ -1646,8 +1649,15 @@ reset_qi_state(State = #vqstate{index_state = IndexState}) ->\n                          rabbit_queue_index:reset_state(IndexState)}.\nis_pending_ack_empty(State) ->\n+    io:format(\"is_pending_ack_empty~n\"),\n+    io:format(\"~B~n\", [count_pending_acks(State)]),\n     count_pending_acks(State) =:= 0.\n+is_unconfirmed_empty(#vqstate { unconfirmed = UC }) ->\n+    io:format(\"is_unconfirmed_empty~n\"),\n+    io:format(\"~w~n\", [gb_sets:is_empty(UC)]),\n+    gb_sets:is_empty(UC).\n+\n count_pending_acks(#vqstate { ram_pending_ack   = RPA,\n                               disk_pending_ack  = DPA,\n                               qi_pending_ack    = QPA }) ->\n``\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. This is #812. Use3.6.1or [3.6.3 M2](https://groups.google.com/forum/#!topic/rabbitmq-users/1tGYhMoMKR8).\n. I'm getting test failures, shared details on Slack.\n. Thank you very much for digging it out.\n. @kjnilsson in theory this could be caused by an incorrect/unexpected set of arguments passed on the command line. Take a look at the topmost directory name, for example.\n. @rhmoult OK, so there were spaces involved, thanks. On an unrelated note, Erlang 19.0 breaks RabbitMQ management plugin, so please stick to 18.3 for now.\n. I'm afraid we cannot reproduce the original issue without explicitly includingrabbitmq-env.bat(which is not meant to be included into user scripts).\n. @ash-lshift thank you. So spaces in the Erlang installation directory seem to be key. @kjnilsson have you tried this scenario?\n. Therandmodule is only [available in18.0+](https://github.com/erlang/otp/commit/95aff702b5e4b21ec277b1e0125f639ce30f997a). That's painful.\n. Norandom:seedfunctions are used on the hot path so we can introduce a shim module, e.g.rabbit_rand. I don't think we should put in the effort comparable totime_compatandssl_compatin this case.\n. Actually, now that we have an internal module that does hot swapping for_compatmodules, maybe we should do the same thing forrand.\n. @jj1bdx thank you. Is it supposed to be a drop-in replacement forrand?\n. Fixed in stable, needs merging into master.\n. OK, apparently therandmodule is only [available in18.0+`](https://github.com/erlang/otp/commit/95aff702b5e4b21ec277b1e0125f639ce30f997a), so this needs more work.\n. @dumbbell thank you, this now needs merging into master.\n. Closed in favour of https://github.com/rabbitmq/rabbitmq-server/issues/863.\n. RabbitMQ does not pick any cipher suites by default and I doubt it should do that. You can either specify those you want or rely on Erlang/OTP itself to use a reasonable default.\n. @dumbbell if you feel there's something that we can do please file a more specific issue. This is not an actionable item => closed.\n. @firewave RabbitMQ passes the list of ciphers as is to Erlang/OTP. It does not filter them in any way. The thread above provides no evidence that this is a RabbitMQ issue or that RabbitMQ can somehow work around it.\nAgain, I'll let @dumbbell re-open if there are specific suggestions.\n. It may be time to expand our Troubleshooting TLS guide with some of @lukebakken's recent findings.. @Ayanda-D how do we know if those applications are not used by other plugins or even other apps?\n. If we can find a way to retrieve application dependency graph from e.g. application_controller then it makes sense. Rabbit already has a graph of plugin dependencies.\n. After giving it some thought I'm afraid this is not really feasible: unless we want to depend on application_controller ETS tables and such, keeping track of dependencies seems impossible to get right at the moment.\nThank you for the idea, @Ayanda-D!\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. nxdomain means the the domain you are trying to connect to (ip2) cannot be resolved.\n. Please post your suggestions to rabbitmq-users first. I have doubts that it is as straightforward as you think it is: hostnames must actually resolve, not just \"be statically configured\" to \"just work\" when host names change.\n. Please post your suggestions to rabbitmq-users. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not discussions. Thank you.\n. If an actionable item comes out of a mailing list discussion, we file new issues.\n. There was (not sure if it was moved to GitHub) a more generic issue about dead-lettering queue contents when it is deleted. There are currently no plans to do that: that potentially can result in many millions of messages being re-published when the user least expects that.\n. Also note that exclusive queues are only supposed to be used when you can afford to lose all messages since if your consumer loses network connection then the queue will be deleted, too. So, only transient, consumer-specific state.\n. @marcincinik we understand that dead-lettering of messages when a queue is deleted would be very nice and logical in some cases. However, some features carry a non-trivial amount of risk with them. This is one of those features.\n. Please post questions to rabbitmq-users. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. It's not clear from your question if you are familiar with exchange-to-exchange bindings and have a specific routing scenario or asking if they exist at all. Please clarify this on rabbitmq-users.\n. This belongs to rabbitmq/rabbitmq-management. Please provide the output of GET /api/queues and messages from the JS console. E.g.\ncurl -u guest:guest -X GET http://127.0.0.1:15672/api/queues/ | python -m json.tool\n. Also, full stack trace from the logs, please.\n. I cannot reproduce this, including when using pagination, filtering, regex filtering, with a few thousand queues. So we need a more detailed report about how this can be reproduced.\nPlease try clearing your browser's cache, we see issues such as https://github.com/rabbitmq/rabbitmq-management/issues/242 from time to time after upgrades.\n. Moved to https://github.com/rabbitmq/rabbitmq-management/issues/244.\n. This issue was moved to https://github.com/rabbitmq/rabbitmq-management/issues/244.\n. Have you seen 3.6.2 release notes?\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Connections are blocked after they publish a message (a basic.publish, content headers, or content body frame is processed) because we don't want to block consumers.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions.\nPlease also clarify what you mean by \"opening the persistent\" and \"cannot be used\" on the list.\nThank you.\n. Will cherry-pick to stable.\n. Corrected end_per_testsuite a bit and merged. Thank you.\n. @binarin this fails to merge into master cleanly. Would you have a chance to resolve conflicts?\n. @binarin yup, simply merge stable into master and resolve conflicts. Thank you.\n. @binarin just in case: for both server and common :)\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. @wyardley your question is answered in the RPM installation guide.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. The root cause isn't known but the workarounds you list make some sense. There has been dozens of bug fixes, including in Mnesia itself in 18.3.3 from our team, so consider upgrading to 3.6.3 (which means a cluster shutdown because it's a feature version A => feature version B upgrade).\n. We would not consider reopening this bug. 3.6.12 is 3 versions behind even 3.6.x which is technically out of support.\nWe have one know scenario where this was caused by a queue that had non-ASCII characters in the name. We don't know much details but if there is a way to reproduce from scratch, we'd like to hear about it on rabbitmq-users (the mailing list).. I'm sorry but we don't have much to add to this issue at this time. We have been trying to find a way to reproduce for over a year. Maybe one day we will dedicate an engineer to work on this issue for months before we understand what's going on. Today is not that day.. Those who have hypothesis and evidence to back them as to what the root cause is are welcome to share them on the mailing list.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. See Permissions in the docs. Creating a separate management (but non-administrative) user is the right thing to do.\n. There is no easy fix to this. Policy changes are generally applied sequentially but those that can change queue master location can create a distributed race condition in queue state updates. Quorum queues in theory should be a lot less susceptible to this since they by definition pass all queue state changes through a well understood consensus algorithm log.\nThere is no ETA on when/if this is going to be addressed for classic queues.. Yes, simply avoiding rapid policy changes that affect mirroring should be enough to avoid the problem.. The \"unconditionally\" above means \"when a long name looks like a short name\"?\nI'll let @dumbbell decide how risky complicating our shell scripts further can be.\n. Yes, keeping the new CLI up-to-date would be great. @binarin note that we are reviewing 7 or so pull requests for CLI so I'd wait for a few days until the dust settles.\n. I assume this can be closed now.\n. @binarin now that the new CLI is merged and being dogfooded in master, is backporting this still relevant? and something you'd be interested in doing?. @binarin thank you!. Feel free to QA but I have one more test case to add => do not merge yet.\n. Also: this has a rabbitmqctl(1) entry but still needs a doc guide.\n. This approach suffers from the lost updates problem when connections from a failed node are removed concurrently. We have evaluated a couple of approaches and the most prominent so far seems to be:\n- Have a pair of connection tracking tables per node\n- Every node updates its own tables\n- Every node reads from all tables in order to come up with a total count\nSo, one writer, many readers. This should also make it possible to eliminate the connection re-registration mechanism in place today.\n. I've missed one more obvious improvement: when a node is deleted from the cluster, its tables should be deleted, too.\n. @dumbbell there is a boot step that does it for the starting node (rabbit.erl:181).\n. This affected connection tracking in the stats DB, now fixed: https://github.com/rabbitmq/rabbitmq-management/pull/274.\n. @dumbbell this is ready for another round.\n. Was this meant to be submitted against stable then?\n. @lemenkov please re-submit it against stable, thank you.\n. Note: no table entry pre-allocation takes place so bumping the limit doesn't lead to higher memory usage of a newly started node.\n. If RabbitMQ believes it uses 3 GB total then that's what the runtime reports. Move to 18.3.4 and install rabbitmq-top.\n\nOn 28 jul 2016, at 18:04, sepich notifications@github.com wrote:\nHello,\nWe have debian jessie with stock Erlang 17.3 and RabbitMQ installed from this site package v3.6.3.\nThere is 2 node cluster and load is mostly based on high number of anonymous queues. (we have atom_tab limit bump to 8M)\nIssue is that memory is leaking:\nPer rabbitmqctl status only 3Gb consumed:\n{memory,\n     [{total,3659726336},\n      {connection_readers,15589856},\n      {connection_writers,37993440},\n      {connection_channels,198255072},\n      {connection_other,119335072},\n      {queue_procs,1643307920},\n      {queue_slave_procs,0},\n      {plugins,651024},\n      {other_proc,14400760},\n      {mnesia,272017288},\n      {mgmt_db,33336},\n      {msg_index,30659920},\n      {other_ets,29675440},\n      {binary,1235604488},\n      {code,27595472},\n      {atom,1443969},\n      {other_system,33163279}]},\nBut erlang is the only daemon running on box:\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n29819 rabbitmq  20   0 16.188g 0.013t   6584 S  53.9 43.4   3513:14 /usr/lib/erlang/erts-6.2/bin/beam.smp -W w -A 64 -P 1048576 -t 8388608 -K true -- -root /usr/lib/erlang -progname erl -- -home /var/lib/rabbitmq -- -pa /usr/lib/r\nfree -h\ntotal       used       free     shared    buffers     cached\nMem:           31G        17G        14G       337M       362M       2.2G\n-/+ buffers/cache:        14G        16G\nSwap:         3.7G         0B       3.7G\nWe've tried old trick with\nexport ERL_FULLSWEEP_AFTER=10\nbut this changed nothing. Restarting of management_plugin and manual start of GC does not reduce memory used. But we see that leakage only happens on node hosting management_db.\nIs there some info we could provide to help resolving this?\nFor now we have to restart daemon manually once in couple days (see drop at 07-26).\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. @hoalequang this is not a support forum and you haven't provided a whole lot of information to work with.\n\nUpgrade to 3.6.8 and monitor memory usage breakdown as well as number of messages in enqueued and unacknowledged. That will help you narrow down the problem.. @tpwow this is one of the most common questions on rabbitmq-users, see list archives. You have provided absolutely no information and this is not a support venue, so I'm afraid no one can help you.. Use Erlang 18 or move to RabbitMQ 3.6.4.\n. Duplicate of https://github.com/rabbitmq/rabbitmq-management/issues/244.\n. Thank you.\n. OK, so this is in the lager section => no need to cherry-pick to stable.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. OK, looks like I can reproduce something similar on the Overview page but not elsewhere https://github.com/rabbitmq/rabbitmq-management/issues/266.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. You can limit specific protocol operations on a queue.\nThis is not a commonly requested feature, so we will focus on other things for now. It's not clear how this is going to work for existing outstanding deliveries, too, so perhaps start a mailing list thread where this can be hashed out and others who have the need for this can chime in to convince our team. Thank you.\n. Thank you, @Gsantomaggio, for fixing this so quickly.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Currently employed release keys do not use SHA-1. Anyhow, this belongs to the mailing list.\n. RabbitMQ does not perform peer verification except if the trust store plugin is used. Plus this is very little information to work with. How can this be reproduced with a two sets of certificates generated via tls-gen, for example?\nSorry but this belongs to the mailing list or a similar discussion venue.\n. If both intermediate certificates were signed by a trusted root CA then their own issued certificates must be treated as valid per section 6.1 of RFC 5280. The root CA is the trust anchor in the RFC.\n. Sounds good.\n. Thank you!\n. Not clear whether #851 should be closed now or more things are coming => assigning milestone to this PR.\n. If you want this to go into 3.6.x, please re-submit against the stable branch.\n. also, thank you!\nI think it can go into a 3.6.x release, no real reason to wait until Q4 for 3.7.0.\n. Yes, CentOS 6 compatibility is a must for us, so we should be careful with any potentially breaking changes.\n. @Gsantomaggio hmm, that was out of scope originally but we can consider it. @dumbbell @dcorbacho WDYT?\n. I suggest that re-publish @harlowja's branch into this repo, close this PR and split our RPM package into two for 3.6.6. make package-rpm then will produce both versions, for CentOS 6 and 7.\nThoughts?\n. @harlowja it's in the top 3 things for someone on our team, we hope to get to it next week. Our build and release system changes are probably quite difficult for outside contributors to make sense of. Thank you for the offer, though!\n. @Ayanda-D feel free to take a look into a test case and fix :)\n. @Ayanda-D please post an update here when you have one.\n. What's the conclusion on this? Should we merge it?\n. This currently doesn't merge cleanly.\n. Thank you!\n. Thank you. If this is intended for the 3.6.x series, please re-submit (or submit another PR) against the stable branch.\n@binarin @dmitrymex @bogdando can I have your blessing, please? :)\n. Please file this for https://github.com/rabbitmq/rabbitmq-website. That page probably wasn't updated since 2007 or so :) Not having a frame limit is not practical, in particular for hosted RabbitMQ scenarios.\n. Thank you!\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Thank you!\n. We'd like to get this into 3.6.x. Not yet clear if it's going to be possible.\n. Alright, this cannot make it into a 3.6.x release due to the internal database schema changes we have to introduce.\n. Kudos to @harlowja for the original pull request. This will be in 3.6.6.\n. It already does.\n. Using most messaging protocols for presence is not something I'd recommend regardless.\n. This feature doesn't really fit the AMQP 0-9-1 protocol. We would consider it if there's enough user interest.\n. @meverett the only way to amend AMQP 0-9-1 spec is via extensions. This is not a discussion forum, please post your ideas to rabbitmq-users.. @meverett it's not discouraged, I simply pointed out that this team does not use GitHub issues for discussions, questions, and so on. Specifically this means that\n\nit is unlikely that you will get a detailed response here from our team\nif other users turn this into a free form discussion, the issue will be locked\n\nInterest can be expressed in many ways that do not assume a response. Those looking to convince our team or suggest a specific protocol extension idea can post to rabbitmq-users. If/when we are convinced and have a reasonably formed idea, we'd be happy to update this issue description or even file a new one.\nOther \"repositories\" are free to use GitHub issues however they please.. Thank you!\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. We cannot recommend anything without having logs and knowing what exact Erlang and RabbitMQ versions are used.\nA couple of rabbitmq-users threads that are likely describing the same issue (bug 26014) from this year ( for 3.2.4 and 3.5.4):\n- For 3.2.4\n- For 3.3.5\nWe haven't see this issue reported in 3.6.x, so please upgrade to 3.6.5. 3.5.x isn't getting any new releases. But, of course, I'm guessing here.\n. I edited out some of the claims in the original report that we have no evidence of. Those who have a subscription with Pivotal should file support tickets.\n. @darkgray you can contact us at info@rabbitmq.com.\n. @dmitrymex @bogdando waiting for a go-ahead from you :)\n. Is this ready to be merged?\n. @lemenkov feel free to submit a PR for stable.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Nothing in RabbitMQ or Erlang prevents you from having deeply nested subdomains. They just need to resolve on all cluster hosts. Node names can be overridden instead of changing the hosts file. Erlang even supports it's own hosts file known as inetrc.\n\nOn 6 sept 2016, at 20:06, Esity notifications@github.com wrote:\nHello,\nNot sure if this is expected or reported but here is what I have found on Ubuntu 16.04. I have named everything via apps and instance count. In example for logstash you have 1.logstash.test.com 2.logstash.test.com 3.logstash.test.com. However rabbitmq-server wont start with that, the hostname cannot have a subdomain name.\nSetting up rabbitmq-server (3.5.7-1) ...\nJob for rabbitmq-server.service failed because the control process exited with error code. See \"systemctl status rabbitmq-server.service\" and \"journalctl -xe\" for details.\ninvoke-rc.d: initscript rabbitmq-server, action \"start\" failed.\ndpkg: error processing package rabbitmq-server (--configure):\n subprocess installed post-installation script returned error exit status 1\nProcessing triggers for systemd (229-4ubuntu7) ...\nErrors were encountered while processing:\n rabbitmq-server\nWhen changing the hostname to just logstash.test.com, it works just fine for installing via apt-get. Everything is setup properly in /etc/hosts and /etc/hostname.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. rabbitmq/internals.\n. So, is this good to go?\n. Available as of RabbitMQ 3.2.0 as a protocol extension (the spec dictates that the server must close the socket without sending any more data).\n. @edmorley please take a look at the doc page. We have revisited this design choice for clients that opt-in. That is not a plugin. The page describes how it works and what clients need to do to get a notification before the socket is closed.\n. I updated 2 out of 3 authentication related guides to link to each other and added a section on authentication failures per your suggestion. Thank you! The updates are already live.\n. Fixed in https://github.com/rabbitmq/rabbitmq-server/pull/955.\n. Is there an issue we can link to, e.g. on erlang-bugs?\n. To make it clear, there are plans to at least evaluate Raft in a few places after the 3.7.0 release.\n. To make it clear, there are plans to at least evaluate Raft in a few places after the 3.7.0 release.\n. I am reliably getting one failure in the dynamic_ha suite:\n\n```\n System report during dynamic_ha_SUITE:promote_on_shutdown/1 in cluster_size_2 2016-09-14 19:54:47.413 \n=WARNING REPORT==== 14-Sep-2016::19:54:47 ===\nConnection (<0.239.0>) closing: received hard error {'connection.close',320,\n                                                     <<\"CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\">>,\n                                                     0,0} from server\n System report during dynamic_ha_SUITE:promote_on_shutdown/1 in cluster_size_2 2016-09-14 19:54:47.414 \n=ERROR REPORT==== 14-Sep-2016::19:54:47 ===\n Generic server <0.239.0> terminating \n Last message in was socket_closed\n When Server state == {state,amqp_network_connection,\n                            {state,#Port<0.9134>,\n                                <<\"client 127.0.0.1:53005 -> 127.0.0.1:21440\">>,\n                                10,<0.242.0>,131072,<0.238.0>,\n                                {server_initiated_close,320,\n                                    <<\"CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\">>},\n                                false},\n                            <0.241.0>,\n                            {amqp_params_network,<<\"guest\">>,<<\"guest\">>,\n                                <<\"/\">>,\"localhost\",21440,0,0,10,infinity,\n                                none,\n                                [#Fun,\n                                 #Fun],\n                                [],[]},\n                            0,\n                            [{<<\"capabilities\">>,table,\n                              [{<<\"publisher_confirms\">>,bool,true},\n                               {<<\"exchange_exchange_bindings\">>,bool,true},\n                               {<<\"basic.nack\">>,bool,true},\n                               {<<\"consumer_cancel_notify\">>,bool,true},\n                               {<<\"connection.blocked\">>,bool,true},\n                               {<<\"consumer_priorities\">>,bool,true},\n                               {<<\"authentication_failure_close\">>,bool,true},\n                               {<<\"per_consumer_qos\">>,bool,true},\n                               {<<\"direct_reply_to\">>,bool,true}]},\n                             {<<\"cluster_name\">>,longstr,\n                              <\"rmq-ct-promote_on_shutdown-1-21440@mercurio\">},\n                             {<<\"copyright\">>,longstr,\n                              <<\"Copyright (C) 2007-2016 Pivotal Software, Inc.\">>},\n                             {<<\"information\">>,longstr,\n                              <<\"Licensed under the MPL.  See http://www.rabbitmq.com/\">>},\n                             {<<\"platform\">>,longstr,<<\"Erlang/OTP\">>},\n                             {<<\"product\">>,longstr,<<\"RabbitMQ\">>},\n                             {<<\"version\">>,longstr,<<\"0.0.0\">>}],\n                            none,\n                            {closing,server_initiated_close,\n                                {'connection.close',320,\n                                    <<\"CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\">>,\n                                    0,0},\n                                none}}\n Reason for termination == \n** socket_closed_unexpectedly\n```\nThis may or may not be related to the change, at first I'm inclined to think the test needs an update.\n. We do not consider this to be solved even post #978.\n. This issue ended up being a kitchen sink of questions. I'm leaning towards closing it because there is no definition of \"done\" any more for this, in particular since some workloads run into consumers not having enough network bandwidth (which is shared with producers).. We have identified the root cause in a specific app that was struggling with this: it used some 10K channels with 2 queues. With this ratio internal flow control no longer works effectively.\nIt has close to nothing to do with lazy queues.. Thank you for a suggestion that's based on actual observations. You can reduce the paging ratio instead if this is the case you're hitting. Or you could use lazy queues. And even then there are things that consume RAM (connections, channels) that paging cannot help with.\nThere are no plans to change the default. Different values accommodate different workloads and environments, and we have no way to measure what's most common for our users.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Deleting a policy cannot delete queues (the only exception to this is if another policy will come into effect because it has the highest priority, and that policy defines a queue TTL or similar).\nPlease take this to the mailing list and include the output of\nrabbitmqctl eval 'rabbit_policy:list().'\n. Note that this will only change for lazy queues.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. nxdomain means RabbitMQ fails to resolve your machine's domain name.\n. \u2026or a different node's domain.\n. RabbitMQ node by default uses local hostname in data directory path. Besides that, when mirroring or certain plugins are used, hostnames stored in a few internal database tables. You can use rabbitmqctl to rename nodes.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Your code does not use publisher confirms as it should. Therefore you cannot assume that all messages were sent even made it to the server.\n. FYI, we are aware of one potentially related issue that requires a certain partial partition scenario. It leaves very visible errors in the log, however. We are working on that one for 3.6.x.\n. 7 reactions in 1 hour for an issue that's not so trivial to reproduce? Sounds legit.\n. This is the official bug tracker (and the only one we have that's public). The very mailing list thread you've linked to provides an explanation.\n. Messages that were published to an exchange may or may not route anywhere. I don't have any immediately ideas as to why a policy could affect this but you don't use publisher confirms and haven't provided any logs. So this is clearly mailing list material at this point.\n. Oh, right. You run 3.6.2. I can't recommend upgrading away from that version enough because of #812, which affects mirrored queues only and renders them dysfunctional unless they are restarted (or the node is). But again, we have no evidence without log files.\n. To clarify why this would not make the system meaningfully more secure: if an attacker has access to the config file, more often than not she has access to the entire file system or can eventually obtain it, and thus access the private key. This may change if the key was loaded differently but key management in distributed systems is hard and there are no alien technologies to make it straightforward, so that is outside of scope of this issue.\n. Your Erlang version is likely too old (or too new), and it doesn't support the type declarations used in one of the dependencies.\n. R16B03 is supported for running 3.6.x but we produce releases on 18.x or 19.x. Building on both R16B03 and 19.x is practically impossible because of typespec syntax changes.\n. Your node reports it had encountered a forced shutdown\nand cannot parse queue index file.\n\nOn 1 Oct 2016, at 00:31, michaelcmundo notifications@github.com wrote:\nHi. We've servers running rabbitmq 3.5.6 and it ran into disk full at one point. The server has issue so we tried to copy the files from mnesia folder to another server that has rabbitmq as well. We changed what it requires on the new server to match the hostname from the previous one.\nWe tried this with files from couple of crashed servers and we're able to recover the queue messages.\nBut some it seems it's not able to do it and just hangs or frozen when I ran the service rabbitmq-server start command. Base on the rabbit@host.log it seems there're some parse segment issue maybe.\nBut it will have a line mentioning Restarting crashed queue 'clicks' in vhost '/'. and either took too long (don't know if it's stuck) or it will repeat some of the lines again and restarting crashed queue.\nAny help will be much appreciated. Thanks.\nHere's the log we've from the server when it starts up...\n=INFO REPORT==== 30-Sep-2016::16:45:15 ===\nStarting RabbitMQ 3.5.6 on Erlang R14B04\nCopyright (C) 2007-2015 Pivotal Software, Inc.\nLicensed under the MPL. See http://www.rabbitmq.com/\n=INFO REPORT==== 30-Sep-2016::16:45:15 ===\nnode : rabbit@mq-mt10\nhome dir : /var/lib/rabbitmq\nconfig file(s) : /etc/rabbitmq/rabbitmq.config\ncookie hash : OHOm6cAi0pRByqTsrYdG1A==\nlog : /var/log/rabbitmq/rabbit@mq-mt10.log\nsasl log : /var/log/rabbitmq/rabbit@mq-mt10-sasl.log\ndatabase dir : /var/lib/rabbitmq/mnesia/rabbit@mq-mt10\n=INFO REPORT==== 30-Sep-2016::16:45:15 ===\nMemory limit set to 12838MB of 16047MB total.\n=INFO REPORT==== 30-Sep-2016::16:45:15 ===\nDisk free limit set to 50MB\n=INFO REPORT==== 30-Sep-2016::16:45:15 ===\nLimiting to approx 924 file handles (829 sockets)\n=INFO REPORT==== 30-Sep-2016::16:45:15 ===\nFHC read buffering: ON\nFHC write buffering: ON\n=INFO REPORT==== 30-Sep-2016::16:45:16 ===\nPriority queues enabled, real BQ is rabbit_variable_queue\n=INFO REPORT==== 30-Sep-2016::16:45:16 ===\nManagement plugin: using rates mode 'basic'\n=INFO REPORT==== 30-Sep-2016::16:45:16 ===\nmsg_store_transient: using rabbit_msg_store_ets_index to provide index\n=INFO REPORT==== 30-Sep-2016::16:45:16 ===\nmsg_store_persistent: using rabbit_msg_store_ets_index to provide index\n=ERROR REPORT==== 30-Sep-2016::16:45:18 ===\n* Generic server <0.240.0> terminating\n* Last message in was {init,{<0.159.0>,[non_clean_shutdown]}}\n* When Server state == {q,{amqqueue,\n{resource,<<\"/\">>,queue,<<\"clicks\">>},\ntrue,false,none,[],<0.240.0>,[],[],[],\nundefined,[],undefined,live},\nnone,false,undefined,undefined,\n{state,\n{queue,[],[],0},\n{active,1475268316149018,1.0}},\nundefined,undefined,undefined,undefined,\n{state,fine,5000,undefined},\n{0,nil},\nundefined,undefined,undefined,\n{state,\n{dict,0,16,16,8,80,48,\n{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n[],[]},\n{{[],[],[],[],[],[],[],[],[],[],[],[],[],\n[],[],[]}}},\ndelegate},\nundefined,undefined,undefined,undefined,0,running}\n* Reason for termination ==\n** {function_clause,\n[{rabbit_queue_index,parse_segment_entries,\n[<<\"y\">>,false,\n{{array,16384,0,undefined,\n{{{{{{{true,\n<<6,79,246,149,241,55,30,251,55,72,30,36,11,132,\n161,214,0,0,0,0,0,0,0,0,0,0,4,167>>,\n<<131,104,6,100,0,13,98,97,115,105,99,95,109,101,\n115,115,97,103,101,104,4,100,0,8,114,101,115,\n111,117,114,99,101,109,0,0,0,1,47,100,0,8,101,\n120,99,104,97,110,103,101,109,0,0,0,0,108,0,0,0,\n1,109,0,0,0,6,99,108,105,99,107,115,106,104,6,\n100,0,7,99,111,110,116,101,110,116,97,60,100,0,\n4,110,111,110,101,109,0,0,0,3,16,0,2,100,0,25,\n114,97,98,98,105,116,95,102,114,97,109,105,110,\n103,95,97,109,113,112,95,48,95,57,95,49,108,0,0,\n0,1,109,0,0,4,167,123,34,64,118,101,114,115,105,\n111,110,34,58,34,49,34,44,34,64,116,105,109,101,\n115,116,97,109,112,34,58,34,50,48,49,54,45,48,\n57,45,50,57,84,49,48,58,48,57,58,50,56,46,54,49,\n56,90,34,44,34,98,101,97,116,34,58,123,34,104,\n111,115,116,110,97,109,101,34,58,34,105,112,45,\n49,48,45,50,53,50,45,50,45,49,48,48,34,44,34,\n110,97,109,101,34,58,34,105,112,45,49,48,45,50,\n53,50,45,50,45,49,48,48,34,125,44,34,115,111,\n117,114,99,101,34,58,34,47,118,97,114,47,108,\n111,103,47,102,105,108,101,98,101,97,116,47,114,\n97,98,98,105,116,109,113,95,99,108,105,99,107,\n115,47,102,105,108,101,98,101,97,116,46,106,115,\n111,110,34,44,34,116,121,112,101,34,58,34,108,\n111,103,34,44,34,104,111,115,116,34,58,34,105,\n.\n.\n.\n50,56,46,57,54,52,55,125,44,34,108,111,103,116,\n                      121,112,101,34,58,34,67,108,105,99,107,79,114,\n                      69,114,114,111,114,34,125,106,109,0,0,0,16,209,\n                      241,202,203,147,189,214,158,240,46,46,46>>},\n                del,no_ack},\n               undefined,undefined,undefined,undefined,undefined,\n               undefined},\n              10,10},\n             100,100,100,100,100,100,100},\n            1000,1000,1000,1000},\n           10000,10000,10000,10000,10000,10000,10000,10000,10000}},\n      16384}]},\n{rabbit_queue_index,recover_segment,3},\n{rabbit_queue_index,'-init_dirty/3-fun-0-',5},\n{lists,foldl,3},\n{rabbit_queue_index,init_dirty,3},\n{rabbit_variable_queue,init,6},\n{rabbit_priority_queue,init,3},\n{rabbit_amqqueue_process,init_it2,3}]}\n=ERROR REPORT==== 30-Sep-2016::16:49:54 ===\nRestarting crashed queue 'clicks' in vhost '/'.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Discovered while working on #486.\n. This depends on #996 (or rather, makes a lot more sense to merge after it) but otherwise seems to be ready for evaluation.\n. @gmr FYI.\n. Ready for another round.\n. I believe this can be closed as https://github.com/rabbitmq/rabbitmq-server/commit/c1c56a29b098b7c5ad0915a340a2a039e61a22a3 is already in stable.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. I don't see how this is even a question about RabbitMQ server. A socket object in Pika is None. Take your server logs and ask whereever Pika maintainers prefer to take questions.\n. Fixed in 3.6.4. See release notes.\n. Specifically in https://github.com/rabbitmq/rabbitmq-management/issues/245.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Similar in spirit to #499, should be evaluated together with it.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. There is a field for head-of-the-queue timestamp in recent releases. It can be a very fast moving thing and I'm not sure I'd make my system depend on it.\n\nidle_since indicates when a queue process last went idle \u2014 it had no message routed to it or other operations to perform. Again, this is largely an implementation detail that's primarily useful for troubleshooting, not much else.\n. This belongs to rabbitmq-users.\n. @dumbbell WDYT?\n@binarin I wonder how colon-separate lists would work on Windows?\n. Fixed in #1016.. Tracing is not meant to be used an ongoing basis but introducing a config option should be quite trivial.\n. It puts significant extra load on the system.\n\nOn 21 Oct 2016, at 11:19, Johan Haleby notifications@github.com wrote:\n@michaelklishin Thanks, that would be really great for us! Could we expect to see this change in a minor release anytime soon?\nI understand that the intention behind tracing is not meant to be used on an ongoing basis but are there any downsides of always having it on like this (besides what would be expected in terms of increased resource allocation)?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. and please mention what version is used in your mailing list post.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Acknowledgements cover deliveries, not enqueued messages. Tutorial 2 covers how it works.\n. Fixed in https://github.com/rabbitmq/rabbitmq-server/pull/1010.\n. Ready for another round.\n. So what is the expectation here? Setting QoS prefetch to 0 makes no sense. I don't think the spec dictates what kind of error should be returned but treating it as a \"hard error\" (in AMQP 0-9-1 parlance) doesn't sound unreasonable to me. Hard errors result in immediate connection closure. \n\nCan you please post a few earlier log entries and entries around the same time in the SASL log?\n. If this was the case it would be reported all over the place but you are the first person to report it in at least a few years. By default there is no QoS prefetch value set at all.\nIf the SASL log contains no entries then there were no unhandled exceptions in the server. This is very likely something application or environment specific. Please take it to rabbitmq-users and provide full logs and a snippet of code (Scala is fine) to reproduce.\n. If the connection isn't closed immediately \u2014 and the reason is ENOTCONN, which means a socket is no longer active \u2014 then it's even less clear cut that the QoS setting is to blame.\n. I cannot reproduce:\n```\nscala> val c = cf.newConnection\nc: com.rabbitmq.client.Connection = amqp://guest@127.0.0.1:5672/\nscala> val ch = c.createChannel\nch: com.rabbitmq.client.Channel = AMQChannel(amqp://guest@127.0.0.1:5672/,1)\nscala> ch.basicQos(1)\nscala> ch.basicQos(1000)\nscala> ch.basicQos(0)\nscala> ch.basicQos(1000)\n// wait for some 10 seconds\nscala> ch.isOpen\nres4: Boolean = true\nscala> c.isOpen\nres5: Boolean = true\n```\nRabbitMQ log around the time of this experiment:\n=INFO REPORT==== 27-Oct-2016::11:18:28 ===\naccepting AMQP connection <0.1212.0> (127.0.0.1:49339 -> 127.0.0.1:5672)\n. Various attempts to reproduce by fiddling with the global setting, adding a consumer or even using prefetchSize (which RabbitMQ does not support):\n```\nscala> ch.basicQos(1000, true)\nscala> ch.basicQos(1000, false)\nscala> ch.basicQos(0, true)\nscala> ch.basicQos(0, false)\nscala> ch.basicQos(0, 0, true)\nscala> ch.basicQos(0, 0, false)\nscala> import com.rabbitmq.client.DefaultConsumer\nimport com.rabbitmq.client.DefaultConsumer\nscala> val cons = new DefaultConsumer(ch)\ncons: com.rabbitmq.client.DefaultConsumer = com.rabbitmq.client.DefaultConsumer@67cffead\nscala> val q = ch.queueDeclare()\nq: com.rabbitmq.client.AMQP.Queue.DeclareOk = #method(queue=amq.gen-ln7CTGgr_nHPVUaT3Hrm_A, message-count=0, consumer-count=0)\nscala> q.getQueue\nres18: String = amq.gen-ln7CTGgr_nHPVUaT3Hrm_A\nscala> ch.basicConsume(q.getQueue, true, cons)\nres19: String = amq.ctag-Lj-GB7qzdUqRcxmOA_1nOA\nscala> ch.basicQos(0, 0, false)\nscala> ch.basicQos(0, 0, true)\nscala> ch.basicQos(0)\nscala> ch.basicQos(0, true)\nscala> ch.basicQos(100, true)\nscala> ch.basicQos(0, true)\n. `enotconn` means the the socket is no longer open (connected). This is not an indication of an issue in a client or the server. That happens from time to time just like other socket operation errors.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Moved to https://github.com/rabbitmq/rabbitmq-website/issues/293.\n. @binarin thank you. @hairyhum is away from GitHub for a few days, we'll review once he's back..\n[E*] rabbitmq_management                  3.6.6 (pending upgrade to 3.6.7)\n```. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. It needs to be not just installed but detected by RabbitMQ. See Set ERLANG_HOME in the docs.\n. This is not something that's commonly reported, so there is something system-specific going on. Please start a thread on rabbitmq-users, that's what we use for discussions. If something comes out of it, we will file a specific issue about that, just like I did with #1018.\n. @oznetmaster if we don't have any specifics then it's not at all obvious if something is a bug or not. We do not use GitHub issues for discussions or investigation of environment-specific issues.\n. @j0nimost see https://github.com/rabbitmq/rabbitmq-server/issues/1017#issuecomment-257708662. When system variables are used, detection works as expected.. It's an application-level problem generally known as \"poison messages\" (note: this also means a particular messaging pattern that's a separate thing) and the best RabbitMQ can offer is\n- Queue length of message TTL (both already available)\n- The limit on how many times a message can be requeued\nI believe we have an issue for the latter, will dig up or update this one (or file a new one) a bit later.\n. OK, so this is a duplicate of #502 but thank you very much for providing the context!\n. Hooray, several months worth of work are finally integrated! \ud83c\udf86 \n. Thank you for reporting. FYI, the website is open source and can be found at https://github.com/rabbitmq/rabbitmq-website.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. RabbitMQ nodes and CLI tools use the cookie to authenticate with each other. It's an Erlang runtime feature. You cannot disable it and I would not recommend doing it anyway.\n. You can pass the cookie on the command line, it should be mentioned in the docs. I don't know if that's any more convenient (it is certainly less secure, however) but if you add -setcookie {cookie value} to your VM arguments, having the file in place won't be necessary.\n. @cocowalla I can't speak for the OP but we certainly see -setcookie used successfully from time to time. Please take this to rabbitmq-users and provide\n\nVersion of RabbitMQ used and how it was installed\nHow exactly was -setcookie set for the server\nHow exactly was it set for ctl\nServer logs\n\nOur team does not use GitHub issues for questions, investigations or discussions.. I quickly verified -setcookie for rabbitmqctl:\nStarted a node with\nRABBITMQ_ALLOW_INPUT=true RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=\"-setcookie cookie1\" rabbitmq-server\nVerified that the cookie value reported by this VM is what I expect (using RABBITMQ_ALLOW_INPUT to drop into a REPL):\n(rabbit@mercurio)1> erlang:get_cookie().\ncookie1\nListed ~/.erlang.cookie to make sure its value isn't the same as above:\ncat ~/.erlang.cookie\nMALIXVU\u2026\nVerified that rabbitmqctl fails to authenticate:\n```\nrabbitmqctl status\nStatus of node rabbit@mercurio\nError: unable to connect to node rabbit@mercurio: nodedown\nDIAGNOSTICS\nattempted to contact: [rabbit@mercurio]\nrabbit@mercurio:\n  * connected to epmd (port 4369) on mercurio\n  * epmd reports node 'rabbit' running on port 25672\n  * TCP connection succeeded but Erlang distribution failed\n\nAuthentication failed (rejected by the remote node), please check the Erlang cookie\n```\n\nVerified that -setcookie for ctl makes authentication succeed:\n```\nRABBITMQ_CTL_ERL_ARGS=\"-setcookie cookie1\" rabbitmqctl status\nStatus of node rabbit@mercurio\n[{pid,28271},\n {running_applications,\n     [{rabbitmq_web_mqtt,\"RabbitMQ MQTT-over-WebSockets adapter\",\"3.6.10\"},\n      {rabbitmq_mqtt,\"RabbitMQ MQTT Adapter\",\"3.6.10\"},\n      {rabbitmq_shovel_management,\n          \"Management extension for the Shovel plugin\",\"3.6.10\"},\n      {rabbitmq_shovel,\"Data Shovel for RabbitMQ\",\"3.6.10\"},\n      {rabbitmq_management,\"RabbitMQ Management Console\",\"3.6.10\"},\n      {rabbitmq_web_stomp,\"Rabbit WEB-STOMP - WebSockets to Stomp adapter\",\n          \"3.6.10\"},\n      {rabbitmq_stomp,\"RabbitMQ STOMP plugin\",\"3.6.10\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.6.10\"},\n      {rabbitmq_jms_topic_exchange,\n          \"RabbitMQ JMS topic selector exchange plugin\",\"3.6.10\"},\n      {rabbitmq_consistent_hash_exchange,\"Consistent Hash Exchange Type\",\n          \"3.6.10\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.6.10\"},\n      {rabbitmq_auth_mechanism_ssl,\n          \"RabbitMQ SSL authentication (SASL EXTERNAL)\",\"3.6.10\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.6.10\"},\n      {rabbit,\"RabbitMQ\",\"3.6.10\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.14.3\"},\n      {cowboy,\"Small, fast, modular HTTP server.\",\"1.0.4\"},\n      {cowlib,\"Support library for manipulating Web protocols.\",\"1.0.2\"},\n      {rabbit_common,\n          \"Modules shared by rabbitmq-server and rabbitmq-erlang-client\",\n          \"3.6.10\"},\n      {sockjs,\"SockJS\",\"0.3.4\"},\n      {syntax_tools,\"Syntax tools\",\"2.1.1\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.4.2\"},\n      {ranch,\"Socket acceptor pool for TCP protocols.\",\"1.3.0\"},\n      {ssl,\"Erlang/OTP SSL application\",\"8.1.1\"},\n      {public_key,\"Public key infrastructure\",\"1.4\"},\n      {xmerl,\"XML parser\",\"1.3.13\"},\n      {crypto,\"CRYPTO\",\"3.7.3\"},\n      {asn1,\"The Erlang ASN1 compiler version 4.0.4\",\"4.0.4\"},\n      {compiler,\"ERTS  CXC 138 10\",\"7.0.4\"},\n      {inets,\"INETS  CXC 138 49\",\"6.3.6\"},\n      {sasl,\"SASL  CXC 138 11\",\"3.0.3\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"3.3\"},\n      {kernel,\"ERTS  CXC 138 10\",\"5.2\"}]},\n {os,{unix,darwin}}\n\u2026\n```\nVerified that ~/.erlang.cookie value hasn't changed.\nThis wasn't on Windows but rabbitmqctl.bat has supported RABBITMQ_CTL_ERL_ARGS for at least 3 years.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. Reasons can range from \"consumers do not acknowledge messages quickly\" to \"RabbitMQ does a lot of paging\" to \"publisher connections were down\" to many reasons in between. Start with monitoring queue lengths and message rates, plus consumer utilization \u2014 all of that information belongs to the mailing list.\n. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. This will have to wait until after 3.6.6 is tagged :(\n. GitHub pull request to cherry-pick: https://github.com/rabbitmq/rabbitmq-server/pull/1023.. Some background: https://github.com/rabbitmq/rabbitmq-server/commit/6b7fc63a5530e5efcd8001aa50a366ab9b23d780 did a couple of changes that we believe prevents a deadlock in application_controller on node shutdown. However, the specific pre_stop/1 change seems to not contribute anything in practice so it seems safe to move it back. Unfortunately the deadlock is quite difficult to reproduce, so I'm less than certain here.. Updated the description to give credit to Tim Stewart, per Kyle's request.. Of course, this is not at all scientific: the point is to not have background GC runs every few seconds because it's completely unnecessary in nearly every workload.. After giving it some thought I think it doesn't make much sense because the interval is scaled up or down depending on how much time it takes. With #1026/#1039, the effective rate with 20K idle queues is \"once a minute\", which is acceptable, at least as far as this issue is concerned.\n. Most RabbitMQ users don't know the difference between stopping an Erlang application and an Erlang VM. So would it really be a net improvement?. @gerhard sounds good.. Thank you!. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. An issue search reveals no issues or questions that may seem related in late 3.5.x and 3.6.x releases. RabbitMQ has a change log and every release has release notes. I cannot immediately think of a known problem like this and recommend anything with the amount of information provided.\nI would certainly recommend 3.5.1 users to upgrade to 3.5.8 or 3.6.6.. Moved to rabbitmq-users.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. @benchristel except for master, RABBITMQ_CONFIG_FILE values must not include the .config suffix. Change\nRABBITMQ_CONFIG_FILE=/path/to/rabbitmq.config\nto\nRABBITMQ_CONFIG_FILE=/path/to/rabbitmq\nand it should start.. Moved to rabbitmq-users.. Note that this is less of an issue in master due to the new config format (which does not use Erlang terms) available but still worth doing.. Thanks for providing the steps to reproduce. We will triage it.. @kjnilsson I think so, unless you see how this can be harmful. If there is an error we should fail early, I think :). @pascaltozzi a fix (#1053) will be in 3.6.7. Thank you for the detailed report that made it much easier to reproduce!. @kjnilsson good idea.. Another good question is what the default should be. For best backwards compatibility it should be a function that always returns true. Of course, that doesn't help security one bit.\nSo, should we set a reasonable minimum, e.g. 5 or 8 characters (will be configurable)?. @dumbbell we don't have a virtual host or tag list at the time of user creation.. @kjnilsson regular expressions can be one of the out-of-the-box modules but I'm not sure it is essential.. @dumbbell the same way modules are provided in other areas (e.g. peer discovery in master): with a plugin.. So far two policy implementations were suggested:\n\nMinimal length\nRegular expression matching\n\nboth seem reasonable to me.. One edge case I've found is the default user. I think we should intentionally bypass validation in this case: if it fails, we can only fail node boot and that may or may not be welcomed. Or make this an option :/. @kjnilsson a few other folks agree with you, alright.. I don't think supporting multiple validators is worth it. Since it's a module, you can develop your own and combine a few existing functions. I doubt complex or multi-step validation rules are going to be all that common.. This is now done for both 3.6.x and master.. All test suites seem to work fine locally and we will very quickly notice any issues I couldn't reproduce today in our day-to-day work => merging and closing this.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. @RafaAguilar thank you. Once we discuss it and understand what's going on on the list we will file an issue on your behalf. There is no conclusion on that yet => this is not an actionable item for our team.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. This is almost certainly either a case with concurrent publishing on a shared channel (and thus incorrect frame interleaving on the wire) or a client library bug. In any case, this is mailing list material.. @AndrewDryga thank you but I cannot accept this. I'm not sure what spell checker you used but it assumes that British spelling of certain words is incorrect. Not to be a language purist but RabbitMQ has its roots in the UK and it should be respected by the core team we have today.\nSome of the changes are indeed genuine typos. If you are still interested in seeing this merged, please submit a new PR with remaining corrections against the stable branch. . There is still one case where your spell checker didn't recognize a valid word. I will correct it manually.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. This is a Homebrew formula question. Last time I tried a week ago the formula worked just fine on Sierra. Try updating your Homebrew copy. Our team does not maintain the formula.. RabbitMQ itself or Erlang versions we support don't have any Sierra compatibility limitations that we are aware of. Some of our team members use Sierra as the primary development environment.\nSee what dependent package may output that.. This branch halves unit_inbroker execution time. This is very encouraging.. ```\n\nrabbit_ct_broker_helpers:rpc failed on line 897\nReason: {error,{badmatch,true},[{...}|...]}\n\nTesting deps.rabbit.unit_inbroker_SUITE:  FAILED test case 196 \n\nrabbit_ct_broker_helpers:rpc failed on line 897\nReason: {error,{badmatch,true},[{...}|...]}\n\nTesting deps.rabbit.unit_inbroker_SUITE:  FAILED test case 198 \n``. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you..rabbitmqctlitself does not requiresudo. It entirely depends on the installation method. You can download the generic UNIX package, unpack it into your home directory and no  RabbitMQ CLI tool will requiresudo`.\nIf you use Debian or RPM packages, sudo is required. Not requiring sudo for administrative tools is a terrible idea and is a much more essential and common expectation from any operator.\nThere's HTTP API and GET /api/overview which includes server version. It returns a JSON response and can be used with curl.\nClient libraries usually provide access to server properties, including version and platform (e.g. JDK or Ruby version). Opening a client connection certainly does not require sudo.\nSo yes, this is a question and not an issue AS DEFINED BY THIS TEAM. There is nothing actionable for our team to work on: you are just looking for help.. Explaining this over and over get a bit old but this is a better example than some others: what would constitute an issue?\n\na specific suggestion to add a new HTTP API endpoint that only returns server version\na specific suggestion to add a new CLI tool that will be world executable in packages and will only return very basic server information (such as version). rabbitmq-diagnostics in 3.7.0 is close to such tool\n\nEven a suggestion to not require sudo for CLI tools in e.g. Debian package \u2014 not something we would agree to, quite obviously \u2014 would be considered an actionable issue and not just a discussion that belongs to the mailing list.\nSomething one of the engineers can assign and begin working on because there's a clear problem definition and enough information to at least get started.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. RabbitMQ does not create the cookie, the runtime does.. Please take this to the mailing list.. In what Makefile specifically?\nThis belongs to rabbitmq-website.. You seem to be doing this from the server archive while RabbitMQ distribution consists of the server and a dozen of plugins. Full distribution is produced using rabbitmq-public-umbrella, in which most recently there is rabbitmq-server-release.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. This repository is not for management plugin and we only accept HTTP API request examples (what Celery does is opaque). Please take this to the Celery mailing list.. Also, we have no information on RabbitMQ version used.. Message distribution happens at the routing stage, therefore that's where the permission is verified.\nEverything that's routed is then consumed by consumers unconditionally. This is not really different from any past version of RabbitMQ.. MQTT plugin has a chance to perform checks as it needs to but it introduces no new settings, by design:\n\nhttps://github.com/rabbitmq/rabbitmq-mqtt/pull/128/files\nhttps://github.com/rabbitmq/rabbitmq-mqtt/pull/115/files\nhttps://github.com/rabbitmq/rabbitmq-mqtt/pull/116/files. So when MQTT plugin handles a SUBSCRIBE or PUBLISH frame or has to send a message on behalf of a client (think Last Will), it has a way of checking topic authorization permissions.. Topic authorization patterns are regular expressions. There are no variable substitution in this feature or anywhere else in RabbitMQ.. @uvzubovs I can see how it'd be useful to have. We will discuss it with the team.. @uvzubovs there is a separate issue for this feature specifically in the MQTT plugin https://github.com/rabbitmq/rabbitmq-mqtt/issues/95.\n\nWe will consider a spike that replaces username and client ID (or similar, e.g. subscription ID in STOMP) in patterns before 3.7.0.. Before 3.7.0 RC1 ships.. Sorry but we no longer investigate issue with the pre-#236 stats collector.. You can try to reproduce this with 3.6.7 Milestone 3.\n. @abra7134 this is something entirely unrelated. Please take this to rabbitmq-users. I suspect that some Erlang/OTP modules are not available on your system: please mention what version is used and how it was provisioned on the list.. Right, so this was also filed against the wrong repo since management plugin is in https://github.com/rabbitmq/rabbitmq-management.\n@abra7134 please post this and your Erlang version and installation details to the list. Thank you for providing the steps to reproduce, we will run this against a 3.6.7 milestone.. @Gsantomaggio unfortunately some users stop reading as soon as they see a proplist logged.. Alright.. wontfix because this will be fixed upstream instead.. FTR, here's the commit that improves eaddrinuse in Ranch 1.3.0: https://github.com/ninenines/ranch/commit/f33ff7cbacb204adae9d53ad15829f44c4140525.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. See Cluster Formation. Node replacement is something that node provisioning tools are concerned with (AWS has autoscaling groups, BOSH resurrection and so on), not RabbitMQ itself.. Do you have a script that reproduces this?\nAlso, does it make sense to use signed integers for TTL?\n\nOn 30 Jan 2017, at 19:09, lukasgeyer notifications@github.com wrote:\nSetting the x-message-ttl argument to a long-long-int value (which is allowed according to https://www.rabbitmq.com/ttl.html#per-queue-message-ttl, The argument can be of AMQP type short-short-int, short-int, long-int, or long-long-int.) causes a no function clause matching exception to be thrown :\n=CRASH REPORT==== 30-Jan-2017::16:20:52 ===\n  crasher:\n    initial call: rabbit_reader:init/4\n    pid: <0.24601.16>\n    registered_name: []\n    exception error: no function clause matching\n                     rabbit_binary_parser:parse_table(<<13,120,45,109,101,115,\n                                                        115,97,103,101,45,116,\n                                                        116,108,76,0,0,0,0,0,\n                                                        0,11,184>>) (src/rabbit_binary_parser.erl, line 50)\n      in function  rabbit_framing_amqp_0_9_1:decode_method_fields/2 (src/rabbit_framing_amqp_0_9_1.erl, line 786)\n      in call from rabbit_command_assembler:process/2 (src/rabbit_command_assembler.erl, line 81)\n      in call from rabbit_reader:process_frame/3 (src/rabbit_reader.erl, line 978)\n      in call from rabbit_reader:handle_input/3 (src/rabbit_reader.erl, line 1035)\n      in call from rabbit_reader:recvloop/4 (src/rabbit_reader.erl, line 446)\n      in call from rabbit_reader:run/1 (src/rabbit_reader.erl, line 428)\n      in call from rabbit_reader:start_connection/4 (src/rabbit_reader.erl, line 386)\n    ancestors: [<0.24599.16>,<0.284.0>,<0.283.0>,<0.282.0>,rabbit_sup,\n                  <0.164.0>]\n    messages: [{'EXIT',#Port<0.43454>,normal}]\n    links: [<0.24599.16>]\n    dictionary: [{process_name,\n                      {rabbit_reader,<<\"127.0.0.1:42236 -> 127.0.0.1:5672\">>}},\n                  {{ch_pid,<0.24609.16>},{1,#Ref<0.0.1572865.75873>}},\n                  {{channel,1},\n                   {<0.24609.16>,{method,rabbit_framing_amqp_0_9_1}}}]\n    trap_exit: true\n    status: running\n    heap_size: 1598\n    stack_size: 27\n    reductions: 4193\n  neighbours:\nUsing long-long-uint (6c) instead of long-long-int (4c) works. The frame is identical in both cases (except for the value type).\n0000   01 00 01 00 00 00 2c 00 32 00 0a 00 00 09 71 75  ......,.2.....qu\n0010   65 75 65 4e 61 6d 65 08 00 00 00 17 0d 78 2d 6d  eueName......x-m\n0020   65 73 73 61 67 65 2d 74 74 6c 6c 00 00 00 00 00  essage-ttll.....\n0030   00 0b b8 ce                                      ....\n0000   01 00 01 00 00 00 2c 00 32 00 0a 00 00 09 71 75  ......,.2.....qu\n0010   65 75 65 4e 61 6d 65 08 00 00 00 17 0d 78 2d 6d  eueName......x-m\n0020   65 73 73 61 67 65 2d 74 74 6c 4c 00 00 00 00 00  essage-ttlL.....\n0030   00 0b b8 ce                                      ....\nRabbitMQ is 3.6.6, Erlang is 19.2.1. The same problem is also experienced on 3.6.5 and 19.1.5.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @lukasgeyer well, what client did you originally reproduce this with? I haven't seen anyone report this in ~ 8 years around RabbitMQ clients, so I am considering just updating the docs. But perhaps we can support signed values easily and safely, needs a bit of investigation.. So it can be a matter of adding one more clause for L fields (as opposed to l, that's a lowercase L):\n\n?SIMPLE_PARSE_TABLE($L, Value:64/signed, long);\nWill wait for a client to reproduce with without hacking Pika.. While not really relevant for TTL, I've noticed that Java client doesn't support decoding of L values or encoding them, and as you've pointed out, with a bunch of dynamically typed languages unsigned value types typically do not exist.\nSo this might explain why this wasn't reported earlier: most likely this can only be reproduced with a handful of clients, like librabbitmq-c and perhaps Objective-C, Go, Rust, Haskell ones.. Yup, .NET client is the same way.. @lukasgeyer yes, it does mean that. I'm pretty surprised the docs even mention those types, it could easily say \"an integer\".\nI do not object to supporting L fields.. See RabbitMQ Errata to AMQP 0-9-1 spec, which mentions a couple of relevant topics in sections 3 and 4.\nWhile supporting L fields is a matter of adding one function clause in rabbit_binary_parser, it looks like the behaviour we currently have is not necessarily a mistake.\nI did discover another curious thing. In rabbit_binary_parser, l (a lowercase L) is treated as a signed 64 bit value prefix. The spec in section 4.2.1 suggests it should mean \"unsigned\" and L should mean signed.\nWhile we can update a few clients we maintain, I'm not sure this is worth fixing in general, given how many existing apps can be affected in subtle ways without any real upside to the fix.\nAs a middle ground solution that won't affect existing clients, we can add a clause for L fields that also decodes them as 64-bit signed integers.\n@lukasgeyer WDYT?. No, it's treated the same way as if a client sent something else that's unparseable.. @lukasgeyer I have submitted a PR that will decode L values as 64-bit long signed values without changing how l fields are handled. See RabbitMQ Errata document section 3 and the above comment. I hope this is sufficient for your case. Since it's backwards-compatible, we can ship it in 3.6.7.. @lukasgeyer thank you, unless there's something serious that I'm overlooking, this should land in 3.6.7.. This is not necessarily by accident: if a backend is down, RabbitMQ can only do one thing: deny access. Clients do not really care what really happened most of the time (hosted RabbitMQ is the only case I can think of), nor can they do much about it. Server logs should make it reasonably clear what happened.\nLastly, some backends can be down while others can't: internal and cache don't necessarily have the same context.\nSo I'm not convinced this is a good idea, or would be much of an improvement for many cases. Please take this to rabbitmq-users and convince the team there.. We agreed to not do this and propagate all errors to clients. Closing per discussion with @hairyhum @kjnilsson.. Yes but I wouldn't expect any significant throughput \nchanges. Since credit flow settings can be configured already, you can give it a try with your workloads and see for yourself.\n\nOn 5 Feb 2017, at 18:31, ShawnLi notifications@github.com wrote:\ndoes credit_flow affect consume speed? if there are a few consumers and small prefetch_count.\n\u2014\nYou are receiving this because you were assigned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @acogoluegnes topics in MQTT and STOMP are built around the idea of reading and writing. However, binding is a different story. Currently {queue, exchange}.{bind, unbind} operations require a read permission on the source exchange: http://www.rabbitmq.com/access-control.html. I assume this is coherent with your thinking?. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. There is no real reason to use the SockJS endpoint any more. RabbitMQ Web STOMP supports Web Sockets directly. I don't know if it would let you avoid UTF-8 encoding issues like this but we'd be much more interested in finding ways to improve that endpoint over SockJS.. I'm being told that SockJS and WebSockets both assume UTF-8, so that part cannot change.. Thank you!. They are perfectly fine.. Err, a PR alone would have been just fine :). Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. See Unsynchronised Slaves. If you have further questions please post them to rabbitmq-users and clarify what specifically what they are.. This also affects stable and could be related to https://github.com/rabbitmq/rabbitmq-top/issues/20:\n\n```\n=ERROR REPORT==== 16-Feb-2017::01:48:59 ===\nRestarting crashed queue 'bunny.tests.queues.with-arguments.priority 0.6913933871720638' in vhost 'bunny_testbed'.\n=ERROR REPORT==== 16-Feb-2017::01:48:59 ===\n Generic server <0.848.0> terminating\n Last message in was {'$gen_cast',init}\n When Server state == {q,{amqqueue,\n                               {resource,<<\"bunny_testbed\">>,queue,\n                                   <<\"bunny.tests.queues.with-arguments.priority 0.6913933871720638\">>},\n                               false,false,<0.827.0>,\n                               [{<<\"x-max-priority\">>,long,5}],\n                               <0.848.0>,[],[],[],undefined,[],[],live,0},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,-576460718277872,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,0,\n                           running}\n Reason for termination ==\n** {{delta,undefined,0,0,undefined},{delta,undefined,0,0,undefined}}\n```. To reproduce, start a node with management plugin enabled, then in Bunny repo:\ngem install bundler\nbundle install\nexport BUNNY_RABBITMQCTL=rabbitmqctl\n./bin/ci/before_script\nbundle exec rspec -cfd spec/higher_level_api/integration/queue_declare_spec.rb. 3.6.7.RC1 is also affected.. 3.6.6 is not affected.. ruby\nq = ch.queue(\"\", arguments: {\"x-max-priority\" => 5}, exclusive: true)\nreproduce it but\nruby\nq = ch.queue(\"\", exclusive: true)\ndoes not, so it is definitely priories-related.. Please post questions to rabbitmq-users or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.. We would be happy to fix that but need more information than \"it doesn't work\". Consider posting more details (full error messages) to rabbitmq-users. Thanks.. RABBITMQ_BASE should be set by the Windows installer. If you use a different installation method, you may need to set it manually. So it's not necessarily an indication of an issue in the script or elsewhere. Again, discussions and investigations like this belong to rabbitmq-users.. @OdellDotson thank you.. Ignoring is sufficient since core metrics are node-local by definition.. My point is that stats GC processes should ignore non-local processes.. You are right, we should RPC to target process' nodes (or do what we already do if the pid is local).. There isn't much information here for us to work with. We routinely enqueue\ntens of millions of messages in some of our long running tests.\nI'd recommend trying a different Erlang version as there's nothing\nFreeBSD-specific in RabbitMQ.\nSorry but as it is right now this belongs to rabbitmq-users. And please\nattach full log files there.\nOn Sat, 25 Feb 2017 at 04:12, NV notifications@github.com wrote:\n\nI've discovered I can crash an otherwise happy RabbitMQ server running on\nFreeBSD by enqueueing a relatively large number of messages to the broker\n(~1 million, although crashes have been seen as low as 400k). Messages were\nrelatively short (~60 bytes per message)\nBoth RabbitMQ and erlang installed with pkgng (pkg install rabbitmq) on\nFreeBSD 11. Package versions are rabbitmq-3.6.6_1 erlang-19.2.3,3\nWhen the issue occurs, beam.smp crashes with \"pid 25230 (beam.smp), uid\n135: exited on signal 10\" or similar appearing in dmesg output. Nothing is\nlogged to the log files (most recent log entries usually either accepting\nor closing STOMP connection).\nNote the server does not run low on memory when it crashes.\nSteps to reproduce:\n\nInstall rabbitmq on FreeBSD 11 (x64) with \"pkg install rabbitmq\"\nOpen admin interface and create a queue\nStart a process to enqueue 1 million messages (my messages were ~60\n   bytes each, although I don't believe size matters)\nWatch queued messages increase in the admin interface until RabbitMQ\n   crashes. Crash should happen before the messages have finished enqueueing.\n\nI couldn't find any crash dumps. I can reproduce the issue on demand if\nthere is anything I can do to gather more useful debugging information.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1126, or mute the\nthread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQiK3YXBx8dQwUWXqKU5sz6ILMtYhks5rf4AYgaJpZM4ML2G6\n.\n-- \nStaff Software Engineer, Pivotal/RabbitMQ\n. There is no information to work with + no evidence it is something in RabbitMQ => this is mailing list material.. Also please post your code to the list as it is critically important to know whether the queue(s) is durable and if messages are published as persistent, as transient messages are kept around in memory for as long as possible accounting to RAM watermark configuration settings and it's a different code path.. Thanks for the detailed report. I'm not sure what user should be the owner of said file because rabbitmq-plugins actually is NOT required to be executed by root and besides package-based installation methods there is never a user named rabbitmq on the host.\n\nSo it's a good idea but not at all clear what user should be the owner. Perhaps this should only be addressed in package scripts (which could pre-create this file and others and set up the permissions).. Consider the following example: every RabbitMQ team member or contributor runs RabbitMQ nodes primarily as a non-privileged user, often from source and in foreground, every single day. Every Homebrew and standalone MacOS build user runs RabbitMQ as a non-privileged user only.\nSo how would whatever piece of code has to create enabled_plugins know what user should be the owner? It's known in package post-install scripts only.. enabled_plugins does not contain anything sensitive (just a list of enabled plugins), so maybe making it world-readable wouldn't be too bad. It will probably cause some security scanners and operators to scream \"bloody murder!\" for no reason, though.\n@dumbbell readable by what group, rabbitmq or similar? that is, a well-known group as opposed to something dynamically computed?. Unfortunately RabbitMQ Puppet module and such use rabbitmq-plugins instead of pre-creating a file.. So doing a couple of things:\n\nSwitching to rewriting the file instead of creating and deleting it\nPre-creating a file in package post-installation scripts\n\nwould yield some improvement.. Unfortunately \"out of the box\" here means a really large number (and growing) of possible deployment scenarios. This is only solvable as long as we known what user or group RabbitMQ's effective process uses.. There's little appetite for messing with umask in our scripts. As explained above, we won't even try to solve this for the general case, only packages.. This is mostly for @Gsantomaggio :)\nCurrently there are two options that seem to have support on our team:\n\nMaking the enabled_plugins file world-readable in package post-install scripts\nAvoiding re-creating the file (this can end up being much more involved than it sounds). @Gsantomaggio group readable may be sufficient. @selivan's idea in https://github.com/rabbitmq/rabbitmq-server/issues/1129#issuecomment-292957764 sounds like a safe and straightforward option as well, so we should investigate it.. @Gsantomaggio I like the 2nd option better as well. Covering packages (Debian, RPM) and eventually Chef cookbook, Puppet module, etc. should be good enough as we don't have control over how generic UNIX binary builds are provisioned.. @selivan would you be interested in testing a one-off build of https://github.com/rabbitmq/rabbitmq-server-release/pull/30? If so, would you like a Debian or RPM package? . I tend to agree with @Gsantomaggio. The solution we have in place is safe, easy to reason about and covers a lot of cases. The last few % can be solved in a way appropriate for a given system using deployment tools such as Chef, Puppet, BOSH and so on.\n\n@dumbbell WDYT?. See the PR above. Some are not happy about the changes that help this use case.\nI strongly recommend people who want to run RabbitMQ in a particularly strict environment to use their deployment tools to set the permissions and umask to their needs instead of expecting our packages to cater to every possible case: sometimes their needs are on the opposite ends of the usability/security spectrum.. This was incorrectly submitted against master. I will cherry-pick.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. Please take this to rabbitmq-users and post full server logs. It is impossible to suggest much with just this output.. @lalit65 this is not a support forum.\n\nunable to connect to epmd (port 4369) on work: timeout (timed out)\n\nstrongly suggests that something (e.g. SELinux or a firewall) blocks traffic on the inter-node communication port.. There's also a dedicated doc guide to troubleshooting networking efficiently and with as little guessing as possible.. Virtual and username aren't available at the time when a connection is accepted. They are provided by clients at a later step.\nWe will discuss if we should log more as some users want exactly the opposite: less logging, which will be easier to configure in 3.7.0.\n589 addresses the load balancer IP concern.. Not all connections have client-provided names. I will add a new log entry and let the rest of the team decide.. I personally think that if we log vhost, TCP socket details and client-provided connection name (if any) it would be really odd to not log usernames.. Closing as this PR is 18 months old and we have a new/different next gen message store plan.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. See How Nodes (and CLI tools) Authenticate to Each Other. The app is running but rabbitmqctl fails to authentiate because its effective user doesn't have the same Erlang cookie file as the server.. We introduced per-vhost message stores to avoid the entire node being hosed should a message store fail for any reason.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. See server logs and please post them to rabbitmq-users, plus relevant sections of your code. I tried PHP tutorial 1 against 3.6.7 and it works just fine, as do at least 4 other clients we test against (Java, .NET, Ruby, Go).. I can't reproduce with Node.js tutorials either:\n```\nconsumer\nnode ./src/receive.js\n [*] Waiting for messages in hello. To exit press CTRL+C\n [x] Received Hello World!\n```\n```\npublisher\nnode ./src/send.js\n [x] Sent Hello World!\n```. This rabbitmq-users thread is likely what you're hitting (this is a guess).. @BlaM we are investigating what may be missing so for now the solution is to upgrade Erlang to something newer than R16B03 (which is the only version affected as far as we can tell), e.g. 19.x.. R16B03 and 17.x are affected.. They won't be in 3.6.8 which should be out very soon.. 3.6.8 is out and fixes Erlang R16B03 and 17.x compatibility. We recommend all users to move to at least 18.0, and ideally 19.x.. @robvelor I'm sorry but we cannot possibly help you with that much information provided. Please make sure you are familiar with the Heartbeats guide and post server logs and a snippet of your code to rabbitmq-users, RabbitMQ's public mailing list.. See https://groups.google.com/d/msg/rabbitmq-users/XfQgta5v6Z0/1V6y2h8GFAAJ.. Yes, that's the one,  thank you.\nOn Thu, 16 Mar 2017 at 03:20, Daniel Carwin notifications@github.com\nwrote:\n\nInvalid topic ID. Are you referring to this thread?\n\n\nhttps://groups.google.com/d/msg/rabbitmq-users/XfQgta5v6Z0/1V6y2h8GFAAJ\n\n\nOn Wed, Mar 15, 2017 at 5:04 PM, Michael Klishin <notifications@github.com\n\nwrote:\nSee https://groups.google.com/forum/m/#!topic/rabbitmq-users/XfQgta5v6Z.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\n<\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1149#issuecomment-286918244\n,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe-auth/AFDVVJa41w2P8i8StswwvuVDHZ7rPrAFks5rmHyRgaJpZM4MenXO\n.\n\n\n--\nDaniel Carwin | dcarwin@pivotal.io | 415-425-0307 |\nRabbitMQ | tc Server | Web Server | Pivotal App Suite |\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1149#issuecomment-286920675,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQuezR3kevztEquiBbw8ViBGg3L-Fks5rmIBIgaJpZM4MenXO\n.\n-- \nStaff Software Engineer, Pivotal/RabbitMQ\n. We are releasing 3.6.8 today that will be compatible with R16B03. It was a matter of building against R16B03, no code changes were necessary. But using 19.x is definitely recommended over R16 ;). 3.6.8 is out and fixes Erlang R16B03 and 17.x compatibility. We recommend all users to move to at least 18.0, and ideally 19.x.. This is mentioned in release notes and is due to our embedded HTTP server change (which won't be reverted).. Hm, apparently it was mentioned in the mailing list announcement but not release notes. Sorry. I will copy it over, thanks.. Done. Note that we've been mentioning it in something like 6-7 preview release announcements. It's unfortunate that those don't get much attention. But now it's in the release notes and should be visible.. I agree with @essen but honestly, our own test suites checked for specific status codes right before we made the switch to Cowboy ;). @marlier oh, fantastic, thank you very much!. The message says cannot_create_parent_dirs: it does try to create the log directory. Debian and RPM packages prepare the directories they will use. Sorry but I'm not sure what else RabbitMQ can realistically do.. It's not technically correct to link to master but still: here's an exact place where RabbitMQ will try to pre-create its log directory. . @powersj what it tries to do is basically mkdir -p. If /var/log/ permissions don't allow RabbitMQ's effective user to create /var/log/rabbitmq, it will fail.\n\nThis is another thing that package scripts take care of that but if you install via binary distribution you can either let it use a local (./var/log/\u2026) directory or override log directory or pre-create it with the suitable permissions.. I wouldn't necessarily say that you shouldn't do that because I know nothing about your goals and how RabbitMQ was provisioned. I think storing logs on a durable volume makes more sense (if log rotation is set up).\nIt's been a while since I last used tmpfs or ramfs. A few years ago storing the entire node data directory for RabbitMQ, MySQL, PostgreSQL worked OK.. I see a couple of curious things in your strace output (major props for posting it):\n\nI don't see any calls that use /var/log/*\nI do see calls that involve SELinux, which is known to wreck havoc and confusion by restricting more than the operator expects by default ;). RabbitMQ user permissions are pretty boring: packages set up a non-privileged user, rabbitmq, and pre-create some directories for the node to use that rabbitmq has permissions to. That's basically it.. @powersj my pleasure. Feel free to post questions to the rabbitmq-users Google group next time you have them :). Thank you for your time.\n\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. No and while we are interested in making message store or at least parts of it pluggable, external data stores are not as attractive as it may seem.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. The exception comes from the MQTT plugin which seems to run into duplicate message IDs. I am not aware of any known scenarios when that might happen. Please post a way to reproduce to rabbitmq-users and if it turns out to be an actionable issue in the plugin, we will file a new issue on your behalf. Thanks.. Interesting, we should at least add it to the docs. Thank you for reporting back, we will investigate and file new issues as needed (or just update the docs directly).. Duplicate of https://github.com/rabbitmq/rabbitmq-mqtt/issues/132.. I cannot reproduce with bsdtar 2.8.3 and libarchive 2.8.3. The way we compress tarballs hasn't changed since 3.6.0 and this is not a commonly reported issue.. Please note for the future:\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. Please make it RABBITMQ_ERLANG_COOKIE. We don't want to invent a new term.. It should not be RabbitMQ-specific, we probably just pass BEAM CLI arguments in such a way that it's not parsed correctly (or passed on at all). Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. Channel state can (and often does) change multiple times a second. rabbitmqctl list_channels queries channels on the spot (\"in real time\") while management UI uses statistics that are emitted every N (5 by default) seconds and reports channel state as in flow if it's been in flow in the last several seconds (IIRC). A really busy channel may seem like it's almost always in the flow control state \u2014 even that isn't necessarily a reason to be concerned.. @893271511 please use English as automatic translation tools can be really confusing. This is not a support forum. Credit flow is applied to all operations, not just publishes, so publishing rate can be 0 but a channel is still throttled by another process it uses.. I think that's reasonable.. It's the same issue as rabbitmq/rabbitmq-management-agent#34 but in a different place. How can this be reproduced so that I can file a new issue there?. OK, so looks like adding a consumer with extra arguments should be enough.. Moved to https://github.com/rabbitmq/rabbitmq-management-agent/issues/39, we can reproduce.. @micdenny https://github.com/rabbitmq/rabbitmq-management-agent/issues/39 is fixed and will be in 3.6.9, thank you for reporting.. Falling back to MD5 is a terrible idea. If a module doesn't exist perhaps the node should not boot.. undefined means you want the default. A non-existent module means a misconfiguration of some kind.. pause_minority isn't supposed to shut everything down and FWIW it's not a commonly requested behaviour.\nWhat RabbitMQ version is used?\nWhat specifically do you mean by \"healthcheck\" in The healthcheck in the pause_minority mode doesn't verify if the node on the other side is actually working? Node detect for peer availability using a heartbeat-like mechanism but pause_minority recovery assumes that all peer nodes are reachable (using a similar yet different mechanism since net ticks is a runtime feature as far as RabbitMQ is concerned).. > They still listening on the management interface though, and rabbitmq-05 is still connected to the other two on the management interface.\nSorry but you are getting ahead of yourself with these claims. Management UI uses periodically emitted stats, so it is often (if not always) a little behind the actual state of events. Those stats are NEVER used to make decisions, only for displaying them in the UI over HTTP API.\nSo node 5 cannot possibly not shut down because of the management UI.\nThe reason why it does not shut down is simple: pause_minority sees that after 2 and 3 did shut down there was a majority of 1 node left and node 5 was in it. Therefore it does not have to shut down by definition. I also don't think that many users would want it to shut down.\nIf you are happy with manually resolving such scenarios, use the ignore strategy and have your monitoring system stop all nodes in case any partitions are reported. Also take a look at Unsynchronised Mirrors in the docs, it describes a scenario in which you can decide to keep the data and sacrifice availability or do the opposite.. Another incorrect assumption: it's not on node 5 to determine if its peers are available or awaiting anything. They might have gone for good, for example. It's on nodes 2 and 3 to detect that all previously known cluster peers are reachable (again, not using net_tick but a similar an fairly basic connectivity check known as net_adm:ping/1) and then resume.\nI don't see anything in the logs that suggests that pause_minority doesn't work as expected. Perhaps one of my fellow team members has more to say before I ask to move this to the mailing list is an open ended discussion.. @szank so you want all nodes to shut down whenever there's a single partial partition instead of \"introducing a SPoF\" by having a single operational node?\nSure, there are systems where losing the majority of nodes is considered unrecoverable but I assure you not everyone prefers that kind of behaviour and it's possibly the biggest issue we are currently contemplating in a future RabbitMQ version which may mirroring to Raft (which is what modern MongoDB versions use for operation log replication). Let me just say that not everyone is happy with this kind of behaviour and it is particularly problematic with partial partitions which RabbitMQ currently promotes to full partitions.\nThere is pause_if_all_down and autoheal strategies but they aren't any closer to a cluster-wide shutdown than pause_minority.. @NicholasMarty post as much detail as you can to rabbitmq-users.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. The only way is to re-create the vhost. It should be no longer possible to delete the default exchange in recent 3.6.x releases and master.. Also, amq.* are not default exchange. The default exchange is something very specific in the protocol. Those are pre-declared exchanges. I don't think the spec prohibits their deletion but it does reserve declaration of amq.* exchanges and there are system exchanges that use that naming pattern.\nThe answer is still the same: you have to re-create the vhost. Use definitions export/import and Shovels if you need to move data out of it to a new vhost first.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. There is no evidence of RabbitMQ \"crashes\" in the log, only that a memory alarm was triggered.\nAbusive publishers or publishers that continuously outpace consumers is a topic that has been discussed numerous times on rabbitmq-users. Your options are:\n\nUsing durable queues and publishing messages as persistent (your code explicitly publishes them as transient, instructing RabbitMQ to keep them in memory as much as possible)\nLazy queues\nReducing limits for alarms\nMessage and/or queue TTL\nExclusive queues\n\nYou don't need to restart RabbitMQ for alarms to clear you need to acknowledge messages or wait until client connection is considered to be gone.. @chernser I don't have much to add to the above. This is a pathological case of a messages published as transient piling up without a consumer. The consequences and ways to address this are fairly well known and are discussed multiple times each month on rabbitmq-users.\nThe \"not running\" state is due to a resource alarm being in effect.. @chernser please do read the options listed above and my explanation. Are you sure you really understand what's going on and not jumping to the conclusions too quickly? Are you sure that your problem is novel and is absolutely should be considered a RabbitMQ bug or can it be an unfortunate combination of your application's behaviour, resources available to your node and your interpretation of the outcome? You are so confident that you know what the root cause is, is it really the case or are you guessing (the \"seems\" part suggests you are).\nYou script tells RabbitMQ to keep 10K of 100kB messages in memory for as long as possible (delivery_mode = 1) and then you assume that a memory alarm with 1 GB of RAM total on the host is an issue with RabbitMQ. You're ignoring a few important points:\n\nYou could have used a durable queue with messages published as persistent (delivery_mode = 2)\nEven then at some point a large number of unacknowledged messages will likely trigger an alarm\nConsumer disconnects are not detected immediately in distributed systems\nYou could have used a message TTL value\nNot having consumers online for a long enough time means you have to make tough choices (or use a long term data store, which RabbitMQ isn't really meant to be). If you really investigated how persistence works (docs, internals) you would have known that messages are not stored in Mnesia.. In addition, your consumer consumes without a QoS on its channel, which tells RabbitMQ to deliver messages to it as quickly as possible, and that means they are either kept in RAM or loaded from disk eagerly.. The has drawn quite a longer answer on rabbitmq-users.. Duplicate of https://github.com/rabbitmq/rabbitmq-server/issues/1164. Your publishes instructs RabbitMQ to keep 10K of 100kB messages (published as transient) in memory with only 1 GB available and 40% of that available to RabbitMQ.. @chernser filing the same issue while ignoring our responses is not going to get you far. Fix your publisher to use durable queues and persistent messages, or your consumer to use a limited consumer QoS, or your expectations.. @chernser I have explained why but you aren't listening or trying to understand what is really going on.\n\nYou script tells RabbitMQ to keep 10K of 100kB messages in memory for as long as possible (delivery_mode = 1) and then you assume that a memory alarm with 1 GB of RAM total on the host is an issue with RabbitMQ. You're ignoring a few important points:\n\nYou could have used a durable queue with messages published as persistent (delivery_mode = 2)\nEven then at some point a large number of unacknowledged messages will likely trigger an alarm. Some information about delivered but unacknowledged messages is kept in RAM (channel state). For fairly obvious reasons RabbitMQ has to read a message even if it only was on disk before delivering it.\nConsumer disconnects are not detected immediately in distributed systems\nYou could have used a message TTL value\nNot having consumers online for a long enough time means you have to make tough choices (or use a long term data store, which RabbitMQ isn't really meant to be). If you really investigated how persistence works (docs, internals) you would have known that messages are not stored in Mnesia.. In addition, your consumer consumes without a QoS on its channel, which tells RabbitMQ to deliver messages to it as quickly as possible, and that means they are either kept in RAM or loaded from disk eagerly.. The has drawn quite a longer answer on rabbitmq-users.. There is no evidence that RabbitMQ terminates. It's a single metric GC (removal) process. This was earlier reported in https://github.com/rabbitmq/rabbitmq-management-agent/issues/42. Please provide your config there.. Starting with 3.6.7 RabbitMQ has a completely different metric storage and management plugin.. Thank you for your time.\n\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you..  You can't. rabbitmqctl, HTTP API or management UI, and definitions import are the 4 options you have (the latter requires federation-management).\n. @hairyhum is this an explanation or are you requesting a change?. @srenatus I love the branch name :) Should this be backported to 3.6.x?. @srenatus understood, we will backport it. May I ask how/what for do you override epmd_module?. @srenatus the reason why I'm asking is that we've seen one case for doing that so far, would be great to know if there may be others.. @srenatus some companies are paranoid about having services that don't require authentication, even if they are seriously limited (like modern epmd is). The only solution in some of those cases is to avoid using epmd.. Thank you for contributing!\nI will cherry-pick to stable.. @jleroy your findings are not correct. First of all, the module you are looking at is not a part of any GA release yet, it will only be available in 3.7.0. \"nxdomain\" comes from epmd, which is not a part of RabbitMQ but rather an Erlang/OTP daemon, and it does support IPv6.\nThere can be things in RabbitMQ that assume IPv4 but we aren't aware of them except for the (unreleased) module above.. Peer discovery DNS backend (currently in master only) now supports both IPv6/AAAA and IPv4/A records thanks to @Gsantomaggio.. @Gsantomaggio thanks for the detailed report!. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. Error when reading ./.erlang.cookie: eacces is the line you are looking for. rabbitmq-plugins cannot read /.erlang.cookie and thus cannot load the shared secret it will use to authenticate with the node.\nSee How Nodes (and CLI tools) Authenticate to Each Other: the Erlang Cookie.. Let's keep this as DO NOT MERGE until we have some experience with a new backend-specific plugin (such as Consul) on top of this.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. We will investigate.\nIn the meantime you can use a Web version of the man page.. No. Using that many priorities is unnecessary. We never intended to see even 50 used.. Now that I'm near a keyboard, here are some more details:\n\n\nChanging a property type will both deviate from the spec (which we really don't like) and require all existing clients to be updated. It is currently possible to use clients from years ago with modern RabbitMQ versions with very few limitations.\n\n\nFor priority queues behind the scenes a new backing queue is created. Using 255 or even thousands of priorities means you will have resource usage similar to having close to that many queues.\n\n\nLastly, I'm yet to see a justification where 10 or maybe 20 \u2014 leave alone over 255 \u2014 priorities are really necessary.. CLI node names are intentionally made unique and RABBITMQ_NODENAME is not supposed to be used by CLI tools.. According to https://github.com/aweber/rabbitmq-autocluster/issues/28#issuecomment-237293428, https://github.com/rabbitmq/rabbitmq-server/pull/892 makes it possible to use IP addresses in containerized environments. @binarin may recall what were the specific issues. \nHowever, RABBITMQ_NODENAME never was meant to be used for CLI tools and CLI tools intentionally use uniquely generated names, otherwise you cannot run more than one instance (or tool) in parallel (there will be a node name conflict).. What I mean by CLI tools.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. There is no evidence that this is something in RabbitMQ, please post your config file to rabbitmq-users.\n3.6.7 upgraded the TCP acceptor library we use which may include stricter validations of listener parameters. RabbitMQ itself certainly did not introduce any.. \u2026as well as server logs. Thanks.. @essen do you see from the trace what option may trip up Ranch validation?. I tried it and the problematic option is server_renegotiate. It is not supported by Ranch (try searching the ninenines/ranch repository) and not listed in the Erlang ssl app doc guide. Therefore it was previously ignored but Ranch now rejects all unknown options.\nI suspect you want secure_renegotiate, which is recognized and listed in the docs.. Well, this repository is not for the visualiser and there is no need to report this multiple times.. Since I can't find this thread on rabbitmq-users, I suspect that it was filed here with the idea that it's a core server problem. It's not but sorry about confusing you. HTTP API is in rabbitmq/rabbitmq-management but I've noticed what path is in question and reopened https://github.com/rabbitmq/rabbitmq-management-visualiser/issues/8 as there is now evidence of what endpoint is at fault.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. Opening a new channel is a round trip. So is closing a channel. There is absolutely no need to open a new channel for every message you publish. Channels are supposed to be reasonably long lived but can be closed due to a protocol exception (e.g. if you try to publish to an exchange that does not exist or re-declare a queue with mismatching parameters).. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with. Questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. When/if we have enough details and evidence we'd be happy to file a new issue.\nPlease post this to rabbitmq-users. Thank you.. Channels in the server share no state and it should not matter at all whether they have publisher confirms enabled or not.\nThere is no evidence behind your claim and you have provided no code or any other details that can be used to reproduce (even server and .NET client versions used). This is mailing list material.. Also, see (and post) server logs.. One case where creating a channel could fail is when you create enough of them to hit a memory alarm, after which the node will stop reading from the socket and no response will reach the client until the alarm clears.\nChannels do consume some resources, and when that is multiplied by several thousands it can make a difference.. @rmoriz can you please be more specific when you claim that something is \"in the wrong place\"?\nsystemd support has been around for at least 6 months and somehow it works for many.. @rmoriz \"the wrong place\" is not specific, I'm sorry. We have a separate CI service (not on Travis) which tests RPM and Debian packages against 8 distributions or so. It does work there and it does work for at least some real world users, including those who contributed systemd notification support (and some of them deploy RabbitMQ in a non-trivial number of varying environments). \nAnyhow, providing specifics would be a lot more productive than finger pointing.. @rmoriz I should also point our that the cookbook is under our GitHub org but it is maintained by a different group of folks (mostly from Chef, Inc).. @rmoriz I did and I do not understand what does \"the wrong service\" mean exactly. Not everyone is a systemd expert.. Here's a CentOS 7 package verification build from Concourse.\nI don't know if it's publicly accessible so here's a gist of the most interesting part. It does roughly the following:\n\nInstalls an RPM package\nDoes some basic sanity checking for server startup and CLI tools and service commands\nUninstalls the package\n\nThe output strongly suggests that the node does start. Therefore there must be a difference between your environment and the one we use. Concourse also happens to be using containers, although it's not the point of the test.. @rmoriz do you have any thoughts on the above?. OK, so we depend on systemctl status output here. That's probably explains it. Thanks, that was very helpful.. OK, now that we understand where the variability comes from and have a failing test suite, we can look into it. I wonder how much of the escaping would be sufficient here.. No worries, I think this is a legit problem that may affect some. We will discuss it next week.. And thank you for looking into the issues with the cookbook test suite!. The conclusion is that the issue is way more complex and deeper than it seems. It involves an interplay of many things that RabbitMQ packages do not control. Even if were to switch to using UNIX sockets, https://github.com/systemd/systemd/issues/2739 is still not resolved and when it is, it will take years\nuntil services can depend on it (only support distributions that ship a systemd version that new).\nSo there is no solution that our packages can provide. I'm inclined to close this as it is not actionable as things stand right now and https://github.com/rabbitmq/rabbitmq-server/issues/1187#issuecomment-296404502 suggests the OP at least somewhat agrees with that.. @axot this is not a support forum. Please post your questions to rabbitmq-users. systemd notification support was contributed by Fedora/RHEL engineers, so I assume the simple strategy doesn't work very well, e.g. when a node can take a while to start, e.g. during an upgrade or just recovering/reindexing a large enough data set.. @lukebakken please submit a PR.. We merged a short term fix for 3.7.4 in https://github.com/rabbitmq/rabbitmq-server/pull/1494.. Your config has an incorrect value (a nested list) for TLS protocols. This is mailing list material.. Also worth noting: the exception comes from the HTTP API (which uses an HTTP server called Mochiweb). I don't know what exactly you mean by \"workers\" but I'm not sure how this can affect client connections. So this may be a red herring.. @Gsantomaggio the second option is much better, we should make sure that if either IPv4 or IPv6 is not available, that respective function returns an empty list, the rest should just work.. @Gsantomaggio let me know when it's time to QA this.. @Gsantomaggio OK, feel free to add a test that uses a publicly available AAAA record.. @elw00d sure (as will everything else that's merged into master). In fact, we've merged a couple of substantial PRs earlier today so I'd like to release a new 3.7.0 milestone tomorrow or so.\nare you trying the peer discovery features in 3.7.0 milestones out?. @elw00d neither this nor anything else around peer discovery will be in 3.6.x.. master will become 3.7.0. 3.6.x releases are produced from the stable branch.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with.\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Please help others help you by providing as much relevant information as possible on the list:\n\nServer and client versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThanks you.. The diagnostics info under TCP connection succeeded but Erlang distribution failed hinted at a possible hostname mismatch.. * Relevant doc guide (see the How Nodes and CLI Tools Authenticate to Each Other section)\n* TCP connection succeeded but Erlang distribution failed, explained on rabbitmq-users. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with.\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Please help others help you by providing as much relevant information as possible on the list:\n\nServer and client versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThanks you.. @sodre another thing we have to ask: if you expect this change to go into a future 3.6.x release, please rebase this against stable (and edit this or submit a new PR). Or force-push your changes, if it's a single commit we can cherry-pick.. Patrick,\nJust to be sure, are you asking us to try #1200?\nOn Thu, Apr 27, 2017 at 4:20 AM, Patrick Sodr\u00e9 notifications@github.com\nwrote:\n\nWe will be going with the \"wait\" trick. However, I think the answer is\nsimpler than the second proposed patch. Using the first patch, I added a\nsilly If statement to look at the result of wait, surprisingly it works!\nWhen you guys have time please verify it also works for you and that I am\nnot getting things messed up. In particular testing it for FreeBSD would be\ngreat.\n!/bin/sh\nset -e\ndo_things () {\n  sleep 20\n}\ntrap \"echo 'Got SIGINT'\" INT\ndo_things &\nif wait $!; then\n  :fi\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/1192#issuecomment-297592851,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQqxpU678eoK96DsT1p7cuXaGhRSNks5rz_tYgaJpZM4NHAdM\n.\n\n\n-- \nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with.\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Please help others help you by providing as much relevant information as possible on the list:\n\nServer and client versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThanks you.. Consider searching rabbitmq-users archives before posting questions. See rabbitmq-top and upgrade to Erlang 19, where binary heap collection is a lot more proactive (this means upgrading RabbitMQ from 3.5.1, which is not a bad idea anyway).. @dcorbacho this needs a manual merge into master.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes we have a certain amount of information to work with.\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions for potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Please help others help you by providing as much relevant information as possible on the list:\n\nServer and client versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThanks you.. rabbitmqctl already provides 3 options which you posted:\nsuggestion: hostname mismatch?\nsuggestion: is the cookie set correctly?\nsuggestion: is the Erlang distribution using TLS?\n9 times out of 10 it's due to a Erlang cookie (a shared secret) mismatch. See How Nodes (and CLI tools) Authenticate to Each Other: the Erlang Cookie in documentation.\nThis is one of the most common questions that's been discussed numerous times on rabbitmq-users and elsewhere on the Web.. @sandeeppalla please do read the docs and the above comment. This is not a support venue. There is plenty of existing threads and a mailing list solely dedicated to questions and discussions.. @cap10morgan may I ask you move this to rabbitmq-management? I suspect that the issue can be with the visualization part, not stats collection or aggregation, so let's start with that assumption. Also, any script we can use to reproduce?\nThank you for providing the evidence of the discrepancy.. FTR, it was moved to https://github.com/rabbitmq/rabbitmq-management/issues/384.. How can we reproduce this?. Also, please test against 3.6.9 as 3.6.4 isn't getting any more updates.. @lukebakken thanks. Yes, it would be great to see some code that triggers this, even after a period of time.. Moved to https://github.com/rabbitmq/rabbitmq-mqtt/issues/132 since this repo is not concerned with MQTT.. Should this and https://github.com/rabbitmq/rabbitmq-website/pull/375, https://github.com/rabbitmq/rabbitmq-public-umbrella/pull/49 be backported to stable?. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions fofor potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Connections in the CLOSE_WAIT state are not necessarily an indication of an issue. In\n99% of the time they come down to TCP listener settings and kernel TCP settings. See rabbitmq-users archives and general Web articles on dealing with connections in that state as well as TIME_WAIT.. One relevant article I highly recommend is https://vincent.bernat.im/en/blog/2014-tcp-time-wait-state-linu. It is not RabbitMQ specific but neither is the problem.. This is not a support venue. Please take this to the mailing list as asked\nabove.\nOn Fri, 28 Apr 2017 at 14:30, balajik1992 notifications@github.com wrote:\n\nserver log as below\n=INFO REPORT==== 28-Apr-2017::08:01:42 ===\nStarting RabbitMQ 3.6.9 on Erlang 19.3\nCopyright (C) 2007-2016 Pivotal Software, Inc.\nLicensed under the MPL. See http://www.rabbitmq.com/\n=INFO REPORT==== 28-Apr-2017::08:01:42 ===\nnode : rabbit@sd-0aad-6587\nhome dir : /home/aimops\nconfig file(s) : /etc/rabbitmq/rabbitmq.config\ncookie hash : yl7M4HTRmS+l2YF5Tc3RYg==\nlog : /var/log/rabbitmq/rabbit@host.log\nsasl log : /var/log/rabbitmq/rabbit@host.log\ndatabase dir : /var/lib/rabbitmq/mnesia/rabbit@host\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nMemory limit set to 6351MB of 15879MB total.\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nDisk free limit set to 50MB\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nLimiting to approx 924 file handles (829 sockets)\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nFHC read buffering: OFF\nFHC write buffering: ON\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nWaiting for Mnesia tables for 30000 ms, 9 retries left\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nWaiting for Mnesia tables for 30000 ms, 9 retries left\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nPriority queues enabled, real BQ is rabbit_variable_queue\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nStarting rabbit_node_monitor\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nmsg_store_transient: using rabbit_msg_store_ets_index to provide index\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nmsg_store_persistent: using rabbit_msg_store_ets_index to provide index\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nstarted SSL Listener on [::]:5671\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nServer startup complete; 0 plugins started.\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1203#issuecomment-297985528,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQr95Gi7DeHmEUYwXetMAIwUGuSjtks5r0dvPgaJpZM4NLZ5z\n.\n-- \nStaff Software Engineer, Pivotal/RabbitMQ\n. @ebelew thank you. Can you please provide more details?. Makes sense. Thank you!. @kjnilsson we should cherry-pick to stable.. Merging per verbal approval from @dcorbacho.. @binarin well, how high is high? I have used the same approach with e.g. Cassandra more than once in the past and 60-90 second delays worked just fine. I can't say I was running cluster formation in a loop, of course.. The docs will be rewritten to cover peer discovery, so there will be a chance to dedicate a subsection to it.. @binarin I am not questioning that but distributed locking also has a natural race condition and is known to be a much harder problem than it seems, so we'll see how it goes for inclusion into the core.. Err, just noticed a typo in a comment above that made it look like I'm questioning @binarin's feeback. I'm definitely not, the randomization is a good enough solution but not a guarantee. Unfortunately neither is locking :(. When there is no ~/.erlang.cookie it will be created with a generated value, not the previous value from the command line.. The above is how the Erlang VM works; RabbitMQ does not manage the cookie or affect what it value is when it is generated.. The value is respected:\n\n```\nRABBITMQ_ALLOW_INPUT=1 RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS='-setcookie QWERTY' rabbitmq-server\nErlang/OTP 19 [erts-8.3] [source] [64-bit] [smp:8:8] [async-threads:128] [hipe] [kernel-poll:true] [dtrace]\nEshell V8.3  (abort with ^G)\n(rabbit@mercurio)1>\n              RabbitMQ 3.6.9. Copyright (C) 2007-2016 Pivotal Software, Inc.\n  ##  ##      Licensed under the MPL.  See http://www.rabbitmq.com/\n  ##  ##\n  ##########  Logs: /Users/antares/Tools/rabbitmq/generic/var/log/rabbitmq/rabbit@mercurio.log\n  ######  ##        /Users/antares/Tools/rabbitmq/generic/var/log/rabbitmq/rabbit@mercurio-sasl.log\n  ##########\n              Starting broker...\n completed with 16 plugins.\n(hit Enter twice to drop into an Erlang shell)\n(rabbit@mercurio)1> erlang:get_cookie().\n'QWERTY'\n```. That's a good question. I verified that it is indeed the case, I suspect that the VM unconditionally generates the file if it's not there. Perhaps someone on the erlang-questions list can answer this.\nI'd recommend generating/populating the file instead of using -setcookie as the latter is visible in ps output and can be captured by tools without a lot of privileges, intentionally or not.. I don't see any explanation in http://erlang.org/doc/reference_manual/distributed.html, and -setcookie on the command line is the same as calling erlang:set_cookie/2 after the node starts, which according to the docs is the function used to set the cookie from the cookie file.\nSince both methods eventually call the same built-in function, I doubt there is a big idea behind this behavior, more likely that the VM flag was introduced at a later point e.g. for developer convenience.. Interestingly, http://erlang.org/documentation/doc-5.4.3/doc/reference_manual/distributed.html contains this piece:\n\nIf the node is started without the command line flag, then the node will create the cookie from the contents of the file $HOME/.erlang.cookie,\nwhere $HOME is the user's home directory.\nIf the file does not exist, it will be created and contain a random character sequence.\n\nThat doc guide is from an OTP release which is about 13 years old. So it is possible that at some point things did work the way you expect. Looks like the docs have been edited since then. Figuring out why will take some code archeology :/. Here's the same guide from 19.3: http://erlang.org/documentation/doc-8.3/doc/reference_manual/distributed.html#id88372. So yes, the guide is there but the wording is different.. @LeeFowlerCU I understand but I don't have an explanation.\nAlso note that the cookie can be set programmatically, in fact, our next gen CLI tools have a command line switch for setting it because it can be more convenient or obvious to some.\nTherefore the number of possible scenarios (and possible reasons for confusion) is greater than 2 :). @binarin good find. Yes, that process may be very short lived but will still create the cookie.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions fofor potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. A single queue can only use one CPU core on the hot code path. If a workload absolutely cannot be adapted to use multiple queues (it's a rare scenario), rabbitmq-sharding can be used to trade off total ordering for parallelism (provided enough cores).\nIt is also important whether PerfTest runs on the same node as RabbitMQ, what scheduler binding type in the Erlang VM is used, and so on. This is mailing list material.. Please re-submit against the stable branch.. ahem, then yes. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions fofor potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. amqp.gen-* queues are server-named queues, one of the clients declares queues and provides an empty string for the name. Such queues are expected to be exclusive or auto-delete, or applications must clean them up.\nYou can set TTL for queues using a naming pattern, too.. Our team does not use GitHub issues for questions.\nYour observations are missing one critically important piece of information: node A gives up and then stops on step 5. If that's the case, then rabbitmqctl wait then also times out?. @binarin FYI, I hope it doesn't go against your embedding efforts.. @gfoligna dynamic shovels support custom publish-properties which covers delivery mode amongst other things. Please start a rabbitmq-users thread if you think that's not sufficient for any reason.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions fofor potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Only one policy is in effect at a time. If multiple policies have the same priority, assume that the effective one will be picked at random.. Please post actual rabbitmqctl (or HTTP API) calls when you post this to rabbitmq-users.\nha-two queues .* {\"ha-mode\":\"exactly\",\"ha-params\":2,\"ha-sync-mode\":\"automatic\"} 100\nuses positional arguments and that's not how priorities are specified to rabbitmqctl set_policy. This could be a typo or a misunderstanding of the CLI, that's why we must have an actual shell transcript.. Of course, it's quite easy to verify if the policies have the priorities you think they do: rabbitmqctl list_policies.. The kernel.* settings are not part of RabbitMQ. They come from Erlang/OTP.. Windows is particularly problematic here because gathering certain OS level metrics currently requires an external tool to be installed. The plan is to fall back to the less precise implementation if that's not available.\nAlso worth pointing out that we already obtain the total amount of memory in an OS-specific way (such as /proc/meminfo on Linux).. A follow-up change for Windows: https://github.com/rabbitmq/rabbitmq-server/pull/1270/.. An update: due to what we've learned in #1343 and a few other places, as of 3.6.13 there are further adjustments to the strategy: as of https://github.com/rabbitmq/rabbitmq-common/pull/225 it now uses runtime's allocators stats (which supposedly track every single malloc performed). This means no external tools are invoked. In addition, as of https://github.com/rabbitmq/rabbitmq-common/pull/221 we avoid frequent calls to the function in question: it is now invoked once a second.\nBecause existing strategy names no longer make sense with these changes, we renamed them to allocated (n\u00e9e rss) and legacy (n\u00e9e erlang). rss and erlang are still supported for backwards compatibility, allocated/rss is still the default.. I do see trace.lz4.* files created in the server directory after stopping the node but gmake profile fails with an undef:\ngmake profile\n GEN    profile\n{\"init terminating in do_boot\",{undef,[{lz4_nif,lz4f_create_decompression_context,[],[]},{lg_file_reader,fold,3,[{file,\"src/lg_file_reader.erl\"},{line,20}]},\n{lg_callgrind,profile,3,[{file,\"src/lg_callgrind.erl\"},{line,76}]},\n{lg_callgrind,'-profile_many/3-lc$^1/1-1-',2,[{file,\"src/lg_callgrind.erl\"},{line,98}]},{lg_callgrind,profile_many,3,[{file,\"src/lg_callgrind.erl\"},{line,98}]},\n{erl_eval,do_apply,6,[{file,\"erl_eval.erl\"},{line,670}]},{erl_eval,exprs,5,[{file,\"erl_eval.erl\"},{line,122}]},{init,start_it,1,[]}]}}\ninit terminating in do_boot (). @essen we already have a module named rabbit_trace which has to do with message tracing.\nI suggest that we rename rabbit_tracer to rabbit_looking_glass_tracing.. I start a node with Looking Glass like so:\ngmake run-broker RABBITMQ_TRACER=\"rabbit_looking_glass:connections\"\nthen run PerfTest or another way to generate some connection activity, then C-c the node.\nThe entire time I have 8 trace files in the working directory that are all empty:\ndeps/rabbit/ rabbitmq-common-198 ls -lha . | grep trace\n-rw-r--r--    1 antares  staff     0B Jun 27 11:13 traces.lz4.1\n-rw-r--r--    1 antares  staff     0B Jun 27 11:13 traces.lz4.2\n-rw-r--r--    1 antares  staff     0B Jun 27 11:13 traces.lz4.3\n-rw-r--r--    1 antares  staff     0B Jun 27 11:13 traces.lz4.4\n-rw-r--r--    1 antares  staff     0B Jun 27 11:13 traces.lz4.5\n-rw-r--r--    1 antares  staff     0B Jun 27 11:13 traces.lz4.6\n-rw-r--r--    1 antares  staff     0B Jun 27 11:13 traces.lz4.7\n-rw-r--r--    1 antares  staff     0B Jun 27 11:13 traces.lz4.8\n@essen is this expected?. I think now I know why, when PerfTest is running, the log is full of exceptions such as:\n2017-06-27 11:23:32.088 [info] <0.380.0> accepting AMQP connection <0.380.0> (127.0.0.1:62440 -> 127.0.0.1:5672)\n2017-06-27 11:23:32.136 [info] <0.380.0> connection <0.380.0> (127.0.0.1:62440 -> 127.0.0.1:5672): user 'guest' authenticated and granted access to vhost '/'\n2017-06-27 11:23:32.167 [info] <0.405.0> accepting AMQP connection <0.405.0> (127.0.0.1:62441 -> 127.0.0.1:5672)\n2017-06-27 11:23:32.174 [info] <0.405.0> connection <0.405.0> (127.0.0.1:62441 -> 127.0.0.1:5672): user 'guest' authenticated and granted access to vhost '/'\n2017-06-27 11:23:32.900 [warning] <0.416.0> The on_load function for module lz4_nif returned:\n{error,{load_failed,\"Failed to load NIF library: 'dlopen(../lz4/priv/lz4_nif.so, 2): Library not loaded: /usr/local/lib/liblz4.1.dylib\\n  Referenced from: /Users/antares/Development/RabbitMQ/umbrella.git/deps/lz4/priv/lz4_nif.so\\n  Reason: image not found'\"}}\n2017-06-27 11:23:32.984 [error] <0.373.0> CRASH REPORT Process <0.373.0> with 0 neighbours crashed with reason: call to undefined function lz4_nif:lz4f_compress_frame(<<0,67,131,104,4,100,0,2,105,110,103,100,0,12,114,97,98,98,105,116,64,117,114,97,110,111,0,0,1,...>>, #{})\n2017-06-27 11:23:32.985 [error] <0.368.0> Supervisor {<0.368.0>,lg_tracer_pool} had child {tracer,5} started with lg_file_tracer:start_link(5, \"traces.lz4\") at <0.373.0> exit with reason call to undefined function lz4_nif:lz4f_compress_frame(<<0,67,131,104,4,100,0,2,105,110,103,100,0,12,114,97,98,98,105,116,64,117,114,97,110,111,0,0,1,...>>, #{}) in context child_terminated\n2017-06-27 11:23:33.042 [warning] <0.419.0> The on_load function for module lz4_nif returned:\n{error,{load_failed,\"Failed to load NIF library: 'dlopen(../lz4/priv/lz4_nif.so, 2): Library not loaded: /usr/local/lib/liblz4.1.dylib\\n  Referenced from: /Users/antares/Development/RabbitMQ/umbrella.git/deps/lz4/priv/lz4_nif.so\\n  Reason: image not found'\"}}\n2017-06-27 11:23:33.079 [error] <0.370.0> CRASH REPORT Process <0.370.0> with 0 neighbours crashed with reason: call to undefined function lz4_nif:lz4f_compress_frame(<<0,72,131,104,4,100,0,2,105,110,103,100,0,12,114,97,98,98,105,116,64,117,114,97,110,111,0,0,1,...>>, #{})\n2017-06-27 11:23:33.079 [error] <0.368.0> Supervisor {<0.368.0>,lg_tracer_pool} had child {tracer,2} started with lg_file_tracer:start_link(2, \"traces.lz4\") at <0.370.0> exit with reason call to undefined function lz4_nif:lz4f_compress_frame(<<0,72,131,104,4,100,0,2,105,110,103,100,0,12,114,97,98,98,105,116,64,117,114,97,110,111,0,0,1,...>>, #{}) in context child_terminated\n2017-06-27 11:23:33.630 [warning] <0.421.0> The on_load function for module lz4_nif returned:\n{error,{load_failed,\"Failed to load NIF library: 'dlopen(../lz4/priv/lz4_nif.so, 2): Library not loaded: /usr/local/lib/liblz4.1.dylib\\n  Referenced from: /Users/antares/Development/RabbitMQ/umbrella.git/deps/lz4/priv/lz4_nif.so\\n  Reason: image not found'\"}}\n2017-06-27 11:23:33.641 [error] <0.369.0> CRASH REPORT Process <0.369.0> with 0 neighbours crashed with reason: call to undefined function lz4_nif:lz4f_compress_frame(<<0,67,131,104,4,100,0,2,105,110,103,100,0,12,114,97,98,98,105,116,64,117,114,97,110,111,0,0,1,...>>, #{})\n2017-06-27 11:23:33.642 [error] <0.368.0> Supervisor {<0.368.0>,lg_tracer_pool} had child {tracer,1} started with lg_file_tracer:start_link(1, \"traces.lz4\") at <0.369.0> exit with reason call to undefined function lz4_nif:lz4f_compress_frame(<<0,67,131,104,4,100,0,2,105,110,103,100,0,12,114,97,98,98,105,116,64,117,114,97,110,111,0,0,1,...>>, #{}) in context child_terminated\n2017-06-27 11:23:35.681 [warning] <0.380.0> closing AMQP connection <0.380.0> (127.0.0.1:62440 -> 127.0.0.1:5672, vhost: '/', user: 'guest'):\nclient unexpectedly closed TCP connection\n2017-06-27 11:23:37.096 [warning] <0.405.0> closing AMQP connection <0.405.0> (127.0.0.1:62441 -> 127.0.0.1:5672, vhost: '/', user: 'guest'):\nclient unexpectedly closed TCP connection. It is on OS X Sierra, I can try a Debian machine to compare.. The shared library that can't be loaded by the node (../lz4/priv/lz4_nif.so) exists and contains the following:\nnm -gU ../lz4/priv/lz4_nif.so\n0000000000003370 S _atom__nif_thread_ret_\n0000000000003378 S _atom_done\n0000000000003380 S _atom_ok\n00000000000016e0 T _dtor_LZ4F_cctx\n0000000000001710 T _dtor_LZ4F_dctx\n0000000000001560 T _load\n0000000000001960 T _lz4_erlang_lz4f_compress_begin\n0000000000001d40 T _lz4_erlang_lz4f_compress_end\n0000000000001740 T _lz4_erlang_lz4f_compress_frame\n0000000000001ac0 T _lz4_erlang_lz4f_compress_update\n00000000000018a0 T _lz4_erlang_lz4f_create_compression_context\n0000000000001e60 T _lz4_erlang_lz4f_create_decompression_context\n0000000000001f40 T _lz4_erlang_lz4f_decompress\n0000000000001c20 T _lz4_erlang_lz4f_flush\n0000000000001f20 T _lz4_erlang_lz4f_get_frame_info\n0000000000002750 T _nif_create_main_thread\n0000000000002860 T _nif_destroy_main_thread\n00000000000016d0 T _nif_init\n00000000000024b0 T _nif_thread_call\n0000000000002140 T _nif_thread_cast\n0000000000003360 S _res_LZ4F_cctx\n0000000000003368 S _res_LZ4F_dctx\n00000000000016c0 T _unload\n0000000000001690 T _upgrade\nI tried re-cloning and rebuilding LZ4* and Looking Glass from scratch.. Indeed that was the root cause, sorry for the noise.. We will postpone this until after 3.6.10 GA ships.. @binarin I updated a few error messages and tuple tags in https://github.com/rabbitmq/rabbitmq-server/commit/fb17eed76e872b240f1f30a3bdaac7f7a8d7945f (https://github.com/rabbitmq/rabbitmq-server/pull/1227). Can you please update this PR accordingly? Thank you.. @binarin I changed the wording and error tuple tags a bit. Thank you.. RabbitMQ does not synchronise writes of, say, basic.deliver (or any other frames) with heartbeats. In fact, any on the wire activity must considered to be a heartbeat according to the spec, by both RabbitMQ and client libraries. Take a look at rabbit_writer if you'd like to see it for yourself.\nYou haven't provided any details as to what your RabbitMQ TCP listener or kernel settings are but \nunless Nagle's algorithm is enabled \u2014 which RabbitMQ and all clients maintained by our team disable by default \u2014 new data should generally be sent out as fast as possible (ignoring TCP congestion control).\nDecreasing heartbeat interval to a value lower than 60 is perfectly suitable for production, I'm not sure what makes you think it's not. In most environments values between 6 and 15 seconds work best and appease TCP proxy and load balancer idle connection timeouts as a nice side effect.\nWhat is dangerous about timeouts is values that are too low, as they result in false positives.. And sorry to sound like a smug smartypants but empirical observations such as those provided above are not really evidence of write synchronisation with heartbeat frames. A traffic dump, rabbitmqctl environment output, kernel TCP settings  and some actual code are all necessary in my opinion to be able to come to an informed conclusion.. If you'd like to continue digging, please do this on rabbitmq-users, as there is little information for us to work with and our team does not use GitHub issues for questions, root cause analysis and discussions.. Also worth mentioning that the heartbeat timeout is not the same as heartbeat interval. In other words, when heartbeat timeout is set to to be 60 seconds, both peers will send two heartbeat frames (assuming there was no other traffic) in the same period of time because according to the spec it takes two missed heartbeat deliveries for the peer to be considered unavailable.\nIf the behaviour claimed in the title was correct, then it should take ~ 30 seconds till first delivery, not 60.. Note that the syntax in the example above is just that, an example. We can use %username or even %u (I'm not a fan of abbreviations, though).. @hairyhum we won't escape values as no matter how you do it, it is always wrong for someone. Use such usernames (and vhosts) at your own risk.. Topic authorization is not available in any GA release, there will be a clear warning in the docs. There already are ways to mess up the routing by using values with slashes, and they even slightly vary between protocols.\nWe can't just reject usernames with slashes but we could validate newly created ones (we have a pluggable credential validation mechanism already). That's not in scope of this issue, however.. We will consider it.. We can keep two most recent milestone releases for 3.6.x (current stable release series). For 3.7.x it makes a lot less sense because the number of people relying on those milestones is really small.\nWhat we would like to avoid is having a lot of preview releases that make it more difficult to find a GA one on GitHub unless you have a direct link.. I believe we've discovered that one before. OTP-13425 in http://erlang.org/download/otp_src_19.0.readme?. It should be noted that /proc/meminfo actually reports values in mebibytes/kibibytes according to RHEL documentation.\nTherefore the use of each value should be analysed on a case by case basis.. It is not missing from the example config:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/stable/docs/rabbitmq.config.example#L302\nOn Mon, Nov 20, 2017 at 7:22 PM, Thomas Riccardi notifications@github.com\nwrote:\n\nDocumentation is missing for total_memory_available_override_value at\nhttps://www.rabbitmq.com/configure.html (or anywhere else on the website\nit seems).\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/1234#issuecomment-345747481,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQhgLNviBW5gAjrYICPqvQLKkuCUBks5s4adagaJpZM4NnkOB\n.\n\n\n-- \nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. @thomas-riccardi fair enough. See https://github.com/rabbitmq/rabbitmq-website/commit/62b52677ecf39d9d7b9f8ad11b497d4b959c9858.. @thomas-riccardi we are aware of that. Currently the plan is to make every guide link to its source on GitHub. Our small team is unlikely to get to it any time soon, though :(. There is a bunch of other places that imply mebibytes/gibibytes, e.g. https://github.com/rabbitmq/rabbitmq-server/blob/91fc8a18225ae72e104d9fab2578f7fe56bf8e96/src/vm_memory_monitor.erl#L364 and https://github.com/rabbitmq/rabbitmq-server/blob/91fc8a18225ae72e104d9fab2578f7fe56bf8e96/src/vm_memory_monitor.erl#L389.\nShould also we switch other places to the currently defined IEC value of 1 kilobytes/megabyte/gigabyte?. According to Red Hat documentation, /proc/meminfo actually reports values in kibibytes when it says \"kilobyte\". I wouldn't be surprised if more OS'es did the same. So this needs an investigation first.. We decided to not change the calculations and instead change the unit we display (to be MiB, GiB, etc) and log.\nThe reason for that was mentioned by @dumbbell in https://github.com/rabbitmq/rabbitmq-server/pull/1235#issuecomment-305731143: there is plenty of existing tools that use kibibytes, mebibytes, etc at the OS, filesystem, networking layers. Some tools (e.g. /proc/meminfo) claim to use kilobytes but actually use kibibytes.\nSome monitoring tools (e.g. Data Dog) use MiB, GiB values as well. So switching to kB, MB, GB may end up making things more confusing, not less.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions fofor potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. You can disable management plugin with rabbitmq-plugins disable rabbitmq_management --offline (has to be done on all nodes) and then re-enable it once the node starts. That's about as much as I can recommend from a single stack trace.. Actually, so there is no evidence that this prevents node startup as it comes from an HTTP API handler for GET /api/overview. However, disabling and re-enabling the plugin should reset all of its state. Other ways to reset the plugin (e.g. in versions older than 3.6.7) are documented.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions fofor potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. FILO \"queues\" are called stacks. Several protocols RabbitMQ supports require the FIFO behavior from queues.\nThe only deviation from that RabbitMQ supports is priority queues. There are no plans to add support for stacks.\nThere are existing data services (e.g. Redis) that providing this or similar features.. Backported in #1241.. This is intentional. Almost always those who disable kernel polling do so by accident.. Why would anyone want to disable kernel polling (other than cases such as ERL-430)?. There is RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS now but a lot of Stack Overflow threads and such use RABBITMQ_SERVER_ERL_ARGS, which means kernel polling, max process limit, [disabled] Nagle's algorithm will all be overwritten. Warnings in the log won't change much because as we know all too well, users only read logs when things go south.. Please post questions to rabbitmq-users in the future.\nFrom 3.6.10 release notes:\n\nThis release has no other known incompatibilities with versions 3.6.7 through 3.6.9.\nSee 3.6.7 release notes [\u2026] if upgrading from an earlier release.\n\nYou can upgrade to 3.6.7 or 3.6.8 first and 3.6.7 (or 3.6.8, which is the same release with Erlang R16B03/17 compatibility restored with a build system change) is treated as a release that requires a cluster-wide restart (or provisioning a new cluster) due to the new management plugin.\nNow, where was this \"hidden\" change introduced. I inspected the rabbit_listener table changes from 3.6.6 (it has an attribute mismatch) and it revealed the following:\n\nlistener.opts were introduced in https://github.com/rabbitmq/rabbitmq-common/commit/b1037b14451bb2459de1471ff285dbcf19808463, which was NOT supposed to go into 3.6.x releases and originally targeted master only\nHowever, it then was cherry-picked for 3.6.7 in order to make it possible to display additional TLS options in the management UI (IIRC) because 3.6.7 was scheduled to be a \"rolling restart release\" already: https://github.com/rabbitmq/rabbitmq-common/commit/3a827a06efffdcba6c0f5b062f01ba2653548bec.\n\nI will update release notes starting with 3.6.7 to make it clearer how to upgrade.. Release notes updated. Thank you for pointing this subtle incompatibility out.. @djenriquez all nodes must be stopped, upgraded, then started at the same time, as if this was a feature release (e.g. 3.7.0).. @djenriquez my comment above is ambiguous. \"All cluster nodes must be stopped\", then upgraded in lock step, then restarted as explained in the cluster upgrade docs. \nTechnically starting with 3.6.7 (IIRC, could be 3.6.8) arbitrary startup restart order should work. We decided to not advertise this in the docs for 3.6.x before we are sure it works as expected.\nThis reminds me, I forgot to document 3.6.7 in that guide, will do in a bit.. It's not clear if we can make Erlang upgrades from 19 to 20 work for 3.6.x (that is, without breaking changes in RabbitMQ itself), so milestone => 3.7.0.. If we can reproduce the old algorithm in our own function, why can't that go into 3.6.x?. Changed the title to be less alarming as OTP 20 GA ended up including a \"mostly compatible\" term_to_binary/1.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. There is no way to do that with a single queue. Use N queues for N types, and channel prefetch for consumers on each queue to limit the concurrent processing rate.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. I'm getting the following test failures (on 19.3):\n```\ncluster_SUITE > cluster_tests > from_cluster_node1 > refresh_events\n    #1. {error,\n            {{badarg,\n                 [{erlang,byte_size,\n                      [\"cluster_tests/from_cluster_node1/refresh_events-q\"],\n                      []},\n                  {term_to_binary_compat,queue_name_to_binary,1,\n                      [{file,\"src/term_to_binary_compat.erl\"},{line,25}]},\n                  {rabbit_queue_index,queue_name_to_dir_name,1,\n                      [{file,\"src/rabbit_queue_index.erl\"},{line,656}]},\n                  {rabbit_queue_index,blank_state,1,\n                      [{file,\"src/rabbit_queue_index.erl\"},{line,539}]},\n                  {rabbit_queue_index,init,3,\n                      [{file,\"src/rabbit_queue_index.erl\"},{line,303}]},\n                  {rabbit_variable_queue,init,6,\n                      [{file,\"src/rabbit_variable_queue.erl\"},{line,504}]},\n                  {rabbit_priority_queue,init,3,\n                      [{file,\"src/rabbit_priority_queue.erl\"},{line,148}]},\n                  {rabbit_amqqueue_process,init_it2,3,\n                      [{file,\"src/rabbit_amqqueue_process.erl\"},{line,195}]}]},\n             {gen_server2,call,[<16201.924.0>,{init,new},infinity]}}}\ncluster_SUITE > cluster_tests > from_cluster_node2 > refresh_events\n    #1. {error,\n            {{badarg,\n                 [{erlang,byte_size,\n                      [\"cluster_tests/from_cluster_node2/refresh_events-q\"],\n                      []},\n                  {term_to_binary_compat,queue_name_to_binary,1,\n                      [{file,\"src/term_to_binary_compat.erl\"},{line,25}]},\n                  {rabbit_queue_index,queue_name_to_dir_name,1,\n                      [{file,\"src/rabbit_queue_index.erl\"},{line,656}]},\n                  {rabbit_queue_index,blank_state,1,\n                      [{file,\"src/rabbit_queue_index.erl\"},{line,539}]},\n                  {rabbit_queue_index,init,3,\n                      [{file,\"src/rabbit_queue_index.erl\"},{line,303}]},\n                  {rabbit_variable_queue,init,6,\n                      [{file,\"src/rabbit_variable_queue.erl\"},{line,504}]},\n                  {rabbit_priority_queue,init,3,\n                      [{file,\"src/rabbit_priority_queue.erl\"},{line,148}]},\n                  {rabbit_amqqueue_process,init_it2,3,\n                      [{file,\"src/rabbit_amqqueue_process.erl\"},{line,195}]}]},\n             {gen_server2,call,[<16202.1365.0>,{init,new},infinity]}}}\n```. Thank you for your time.\nOTP 20 doesn't \"break RabbitMQ\" for all cases, only upgrades from 19.x and earlier versions are problematic. We are working on supporting OTP 20 in 3.6.x, at least for most cases, hopefully 3.6.11 will tolerate upgrades in most scenarios (e.g. when only AMQP 0-9-1 and 1.0 are used).\nClosing this with that assumption. If we can't support 20 well, we will consider restricting the version for 3.6.x.. Also note that it's possible to restrict a particular package to a version/range without any package modifications:\n\napt pinning\nyum version locking plugin. @FabianPonce thank you. Can you please submit the same change against stable? (note: there won't be rabbitmq.example.conf in that branch).. Thank you for your time.\n\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. This rabbitmq-users thread discusses more or less the same scenario.. As to how the cookie is used, see How Nodes and CLI Tools Authenticate to Each Other.. effort-medium seems reasonable here because this involves 3 different engineers, multiple plugins and quite a bit of testing (on top of what's already been done in https://github.com/rabbitmq/rabbitmq-autocluster/pull/6).. There is no PR for rabbitmq-common where peer_discovery_backend resides.. @dumbbell that's a valid concern. With this change it is more likely that publishers will be blocked earlier, so kernel swapping is less likely with this change. Would you agree?\nIf RabbitMQ node process is being swapped out, I'm afraid our throttling mechanisms won't be very effective either way.. @dumbbell @lukebakken @hairyhum @gerhard @kjnilsson @dcorbacho @acogoluegnes we already have a feature flag for going back to the old calculation method.\nI suggest that we rename rabbit.vm_memory_use_process_rss (a boolean) to rabbit.vm_memory_calculation_strategy (which can be rss, erlang, and who knows what else in the future, defaulting to rss). This will leave the door open for N more options in the future without having to add N more feature flags.\nI'm also curious what @jerrykuch and @carlhoerberg think.. @gerhard default to rss and log a warning.. @gerhard note that with Cuttlefish unsupported values for enum types will terminate boot. So will unreadable certificate and key files and other config validation failures.. I'm reliably getting 7 failures in unit_inbroker_non_parallel_SUITE:\n```\nunit_inbroker_non_parallel_SUITE > non_parallel_tests > app_management\n    #1. {error,\n            {{badmatch,\n                 {badrpc,\n                     {'EXIT',\n                         {noproc,\n                             {gen_server,call,\n                                 [vm_memory_monitor,get_memory_limit,\n                                  infinity]}}}}},\n             [{unit_inbroker_non_parallel_SUITE,app_management1,1,\n                  [{file,\"test/unit_inbroker_non_parallel_SUITE.erl\"},\n                   {line,109}]},\n              {rpc,'-handle_call_call/6-fun-0-',5,\n                  [{file,\"rpc.erl\"},{line,197}]}]}}\nunit_inbroker_non_parallel_SUITE > non_parallel_tests > channel_statistics\n    #1. {error,\n            {badarg,\n                [{ets,lookup,\n                     [rabbit_exchange,\n                      {resource,<<\"/\">>,exchange,<<\"amq.rabbitmq.trace\">>}],\n                     []},\n                 {rabbit_misc,dirty_read,1,\n                     [{file,\"src/rabbit_misc.erl\"},{line,394}]},\n                 {rabbit_trace,init,1,\n                     [{file,\"src/rabbit_trace.erl\"},{line,48}]},\n                 {rabbit_channel,init,1,\n                     [{file,\"src/rabbit_channel.erl\"},{line,386}]},\n                 {gen_server2,init_it,6,\n                     [{file,\"src/gen_server2.erl\"},{line,554}]},\n                 {proc_lib,init_p_do_apply,3,\n                     [{file,\"proc_lib.erl\"},{line,247}]}]}}\nunit_inbroker_non_parallel_SUITE > non_parallel_tests > disk_monitor\n    #1. {error,{noproc,{gen_server,call,\n                                   [rabbit_sup,\n                                    {terminate_child,rabbit_disk_monitor_sup},\n                                    infinity]}}}\nunit_inbroker_non_parallel_SUITE > non_parallel_tests > disk_monitor_enable\n    #1. {error,{noproc,{gen_server,call,\n                                   [rabbit_sup,\n                                    {terminate_child,rabbit_disk_monitor_sup},\n                                    infinity]}}}\nunit_inbroker_non_parallel_SUITE > non_parallel_tests > file_handle_cache\n    #1. {error,{noproc,{gen_server2,call,\n                                    [file_handle_cache,get_limit,infinity]}}}\nunit_inbroker_non_parallel_SUITE > non_parallel_tests > head_message_timestamp_statistics\n    #1. {error,\n            {{badmatch,\n                 {error,\n                     {badarg,\n                         [{ets,lookup,\n                              [rabbit_exchange,\n                               {resource,<<\"/\">>,exchange,\n                                   <<\"amq.rabbitmq.trace\">>}],\n                              []},\n                          {rabbit_misc,dirty_read,1,\n                              [{file,\"src/rabbit_misc.erl\"},{line,394}]},\n                          {rabbit_trace,init,1,\n                              [{file,\"src/rabbit_trace.erl\"},{line,48}]},\n                          {rabbit_channel,init,1,\n                              [{file,\"src/rabbit_channel.erl\"},{line,386}]},\n                          {gen_server2,init_it,6,\n                              [{file,\"src/gen_server2.erl\"},{line,554}]},\n                          {proc_lib,init_p_do_apply,3,\n                              [{file,\"proc_lib.erl\"},{line,247}]}]}}},\n             [{rabbit_ct_broker_helpers,test_channel,0,\n                  [{file,\"src/rabbit_ct_broker_helpers.erl\"},{line,1191}]},\n              {unit_inbroker_non_parallel_SUITE,test_spawn,0,\n                  [{file,\"test/unit_inbroker_non_parallel_SUITE.erl\"},\n                   {line,709}]},\n              {unit_inbroker_non_parallel_SUITE,head_message_timestamp1,1,\n                  [{file,\"test/unit_inbroker_non_parallel_SUITE.erl\"},\n                   {line,651}]},\n              {rpc,'-handle_call_call/6-fun-0-',5,\n                  [{file,\"rpc.erl\"},{line,197}]}]}}\nunit_inbroker_non_parallel_SUITE > non_parallel_tests > log_management\n    #1. {error,\n            {badarg,\n                [{erlang,process_info,[undefined,group_leader],[]},\n                 {unit_inbroker_non_parallel_SUITE,override_group_leader,0,\n                     [{file,\"test/unit_inbroker_non_parallel_SUITE.erl\"},\n                      {line,386}]},\n                 {unit_inbroker_non_parallel_SUITE,log_management1,1,\n                     [{file,\"test/unit_inbroker_non_parallel_SUITE.erl\"},\n                      {line,187}]},\n                 {rpc,'-handle_call_call/6-fun-0-',5,\n                     [{file,\"rpc.erl\"},{line,197}]}]}}\nunit_inbroker_parallel_SUITE > parallel_tests > confirms\n    #1. {error,killed}\n```\nHere's a CT log directory for that suite:\ndeps.rabbit.unit_inbroker_non_parallel_SUITE.logs.1.zip\n. I can reproduce this on two machines reliably even after gmake distclean.. I cannot reproduce the failures in stable.. The tests now pass, we are doing more observatory testing on more OS'es. There's also at least one place in management-agent that needs updating.. RabbitMQ doesn't have any native code. Feel free to try a different Erlang/OTP version.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. @lskbr one hour of logs before the time stamp listed in dmesg plus a few minutes of dmesg entries would be sufficient.\nThere are several 19.2.x and 19.3.x releases. Unfortunately the OTP team has a curious approach to packaging where the original release (e.g. 19.3) is provided as a binary download but subsequent patches are provided as, well, patches. Erlang Solutions provides packages the most recent patch version.\nThere is a Erlang VM change log entry in 19.3.3 that mentions an issue memory allocation.. @lskbr please open a JIRA ticket with Erlang/OTP as RabbitMQ itself does not allocate memory directly.. RabbitMQ has no native code. Segfaults in the Erlang runtime are not common but there is no \"fix for all of them\", so YMMV. Use the newest Erlang release supported and likely you will be fine.. @Pkuutn see the comment above. RabbitMQ does NOT allocate memory directly, the beam.smp process is the Erlang runtime.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. The error says that package erlang-gs-19.3-1.el7.centos.x86_64 requires erlang-kernel(x86-64) = 19.3-1.el7.centos. I'm not sure how it has anything to do with RabbitMQ.\nWhile our team does maintain a stripped down Erlang RPM, the packages in the output seem to come from the Erlang Solutions repository.. @Farit-Biktimirov there is no single piece of advice for resolving package dependency hell. The easiest option is to use the Erlang RPM that we build. It has no dependencies (other than a modern OpenSSL).. @lukebakken this needs a rebase against master.. This was introduced in https://github.com/rabbitmq/rabbitmq-server/commit/8cedd3f53a14e45ccc68868bbc6401bb321ee50f and also present in stable.. @lukebakken can we verify this on Windows 7 or 8?. Will cherry-pick to stable.. @goncalotomas thanks. We've been working on OTP 20 compatibility for about a month now and have never observed this. Can you please clarify how are you building the server?\nRabbitMQ uses a patched version of erlang.mk, not rebar.. Related to #1243.. So the line was introduced in https://github.com/rabbitmq/rabbitmq-server/commit/492e23bf188820756e07076c4539692376cfcbc6 and not present in stable.\nI suspect it was added for more convenient debugging from the shell and can be removed.. Managed to reproduce it with a clean umbrella clone.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. @choyuri this ticket is closed because we don't have enough information to work with.\n\nOur team is multiple orders of magnitude smaller than the RabbitMQ community.\nPlease help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list. Thank you for your time.\n\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. See rabbitmq-top as a source of additional information.. If by \"service\" you mean a client (application), then connections from killed containers won't be detected immediately. Make sure your clients use a sensible heartbeat value, 6 to 20 seconds is the optimal range for systems where client TCP connection churn can be high or clients can be arbitrary killed without a chance to close AMQP 0-9-1 connection first.\nMissed heartbeats, cleanly disconnected client connections, lost/abruptly closed TCP connections are logged by the server.\nYou can configure TCP keepalives as an additional measure and reduce TCP buffer size so that each connection uses less memory (at the cost of lower throughput). See RabbitMQ Networking guide for details.\nLastly, the 1st chart demonstrates total Erlang VM binary heap size. It can and does include things that are not \u2014 at least technically \u2014 part of RabbitMQ. The 2nd chart demonstrates how much\nbinary heap each group of RabbitMQ processes uses.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Please post a way to reproduce to rabbitmq-users and we will investigate. There is a slight chance that this may be a yet another manifestation of https://github.com/rabbitmq/rabbitmq-common/issues/208 but we have very little information to tell.\nRelated issues were resolved in 3.5.2 and 3.6.1.\nAs of 3.6.0 eager sync happens in batches of a configurable size. You can reduce it from 4096 (the default) to, say, 50 and see if that reduces peak RAM consumption. See Batch Synchronization in mirroring docs.\nAs with any memory consumption investigation, use rabbitmqctl status and rabbitmq-top to gather more information.. Digging deeper and fixing a bunch of terminations here and there (instead we log an error and do nothing), I still can reproduce node shutdown because vhost watcher can run into a non-existent vhost. That is by design, see #1158. We will revisit what default strategy should be .. @hairyhum in my case the sequence of events was created-then-deleted \u2014 I could tell from the crashes in the log. But you have a point. Feel free to submit a PR that stops the watcher first, it makes sense.. @hairyhum do you have any particular scenarios in mind?. I conducted a few tests per @hairyhum's suggestion.\nHere's one:\n\nStart a node\nCreate 3 vhosts, say, vhost11, vhost12, vhost13\nRun a seed script similar to this one\nObserve that messages are there using rabbitmqctl list_queues -p vhost11 and so on\nDelete vhost data directory (rabbit_vhost:delete_storage/1 is a convenient way of doing it)\nRe-run the seed script\nObserve that messages are there\nRestart the node\nObserve that messages are there\n\nThe test can be modified to delete data directory manually, it still recovers.. Another test: similar to the above but the script is modified to produce a constant load.\nOutcome: the vhost is intact, queue processes terminate and do not always recover (which is expected and beyond the scope of this PR); newly declared queues are functional.. @hairyhum addressed.. We have a decent understanding of what's going on. Lazy queues only become such after the policy application step and that happens after queues have recovered [most of] their durable data.\nBroadly speaking, there are two different solutions:\n\nTweak variable_queue (defaults or implementation or initial state) to load less data into RAM during initial state recovery\nApply some or all policies before recovery. The conclusion is roughly this. Messages embedded into queue index will be loaded all at once since indices are not \"lazy loaded\". On most workloads this doesn't matter but with a huge backlog of tiny messages embedded, it suddenly does. Disabling embedding avoids the problem.\n\nWe explored 2 or 3 different ways of avoiding this and are not happy with any of them. Embedding messages into queue index has other operational ramifications we now understand well and don't like, so chances are that feature will be going away in a future version.. @gerhard it was never the promise of lazy queues to not keep a reasonable number of messages (e.g. one index segment worth) in RAM. The promise is that messages will be moved to disk aggressively and regardless of whether they were published as persistent or transient.\nI don't have the exact steps to try but I'm quite sure the root of the issue is still queue index embedding since otherwise QI entries are really small. Queue laziness is a red herring and the real issue is that each queue loads 16K messages on start, and they contain small yet non-zero payloads.. Before anyone suggests that SEGMENT_ENTRY_COUNT should be lower or configurable: it you reduce it, it will break existing installations badly.. Looks awesome!. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. A table with users does not exist. RabbitMQ never deletes it, so we typically see scenarios like this when a node runs out of file descriptors during boot (e.g. after a restart).\nSee Networking and Production Checklist.. We should test if this is safe for reads of messages that are already embedded.. @hairyhum there was some before QI embedding was introduced in 3.5.0. We may have some numbers in the legacy bug tracker. The difference is single digit % and this only applies to lazy queues.\nWithout this change a node with a lot of data on disk in lazy queues will load potentially a very large amount of data into RAM on start.. We decided to investigate other options, such as loading and applying policies earlier.. So far we think this is feasible but sudo introduces a new package dependency (Ubuntu, Debian stable, RPM-based distros).. I could find earlier discussions of this plus similar issues reported for other projects. Looks like the issue should be fixed on the distribution end instead of every individual tool. su is not an unreasonable requirement for rabbitmqctl.. Yes, with RSS used to compute total we've seen enough cases where the new [OS] total does not match the [runtime-reported] breakdown. Simply dropping the assertion as something that's no longer relevant makes sense to me.. Good idea.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. I't not clear whether \"the server\" here means a request/reply server or something else. Unroutable messages can be returned to the publisher (set the mandatory property to true when publishing). Unacknowledged messages are requeued to the same queue. TTL can be used to dead letter them eventually.. @smurfix then perhaps you should not use auto-delete or exclusive queues and rely on TTL instead. This is not a discussion venue, sorry.. Doh, I hit Merge in the wrong tab. Sorry.. We agreed to do a couple of small adjustments in stable. The PR looks good in general.. @langyxxl please keep discussions to the mailing list. Thank you.. A basic test on MacOS that compares idle nearly blank node RAM usage with different RABBITMQ_DISTRIBUTION_BUFFER_SIZE values (32000, 64000, 105000 and 128000) suggests that\n\nBy default a node on 64 bit Erlang 19.3.6 uses ~ 39 MB (RSS) and then it goes down (!) quickly because the kernel quite aggressively compresses RAM of \"idle\" processes.\nWith RABBITMQ_DISTRIBUTION_BUFFER_SIZE=64000, the value (RSS) is ~ 82 MB.\nWith RABBITMQ_DISTRIBUTION_BUFFER_SIZE=105000, the value (RSS) is ~ 82 MB.\nWith RABBITMQ_DISTRIBUTION_BUFFER_SIZE=128000, the value (RSS) is ~ 83 MB.\n\nSo after 64 MB the effect flattens out.. On 64 bit Erlang 19.3.6 on Linux (Ubuntu 16.10) (note that the node is completely blank while the above MacOS test had some data from client library test runs):\n\nWith RABBITMQ_DISTRIBUTION_BUFFER_SIZE=32000, RSS is ~ 69 MB.\nWith RABBITMQ_DISTRIBUTION_BUFFER_SIZE=64000, RSS is ~ 68 MB.\nWith RABBITMQ_DISTRIBUTION_BUFFER_SIZE=105000, RSS is ~ 70 MB.\nWith RABBITMQ_DISTRIBUTION_BUFFER_SIZE=128000, RSS is ~ 70 MB.\n\nwhich is quite interesting in comparison to the first test.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. The limit is slightly lower than 2 GB and it is not configurable.. It is computed like so in current 3.6.x releases. Please use 3.6.10, the most recent release to date.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. rabbitmq-users thread.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. rabbitmq-users thread. @sega-yarkin can you provide the contents of the definitions file?. rmq-internal is not a real user. It's a stub used by operations where no user is present. So we need a definitions file to see why it may be granted access to any vhosts.. Looks like it's this import step that fails.. Unfortunately the approach in rabbitmq/rabbitmq-management#470 isn't sufficient. We are investigating alternatives (or whether doing \"more of the same\" would work).. The above snippet from @sega-yarkin as well as the following (provided by a different user and modified to edit out sensitive data by us):\n``` json\n{\n    \"rabbit_version\": \"3.6.9\",\n    \"users\": [{\n        \"name\": \"project_admin\",\n        \"password_hash\": \"A0EX\\/2hiwrIDKFS+nEqwbCGcVxwEkDBFF3mBfkNW53KFFk64\",\n        \"hashing_algorithm\": \"rabbit_password_hashing_sha256\",\n        \"tags\": \"\"\n    }],\n    \"vhosts\": [{\n        \"name\": \"/\"\n    }],\n    \"permissions\": [{\n        \"user\": \"project_admin\",\n        \"vhost\": \"/\",\n        \"configure\": \".\",\n        \"write\": \".\",\n        \"read\": \".*\"\n    }],\n    \"policies\": [{\n        \"vhost\": \"/\",\n        \"name\": \"nd-ns\",\n        \"pattern\": \"^project-nd-ns-\",\n        \"apply-to\": \"queues\",\n        \"definition\": {\n            \"expires\": 120000,\n            \"max-length\": 10000\n        },\n        \"priority\": 1\n    },\n    {\n        \"vhost\": \"/\",\n        \"name\": \"nd-s\",\n        \"pattern\": \"^project-nd-s-\",\n        \"apply-to\": \"queues\",\n        \"definition\": {\n            \"expires\": 1800000,\n            \"max-length\": 50000\n        },\n        \"priority\": 1\n    },\n    {\n        \"vhost\": \"/\",\n        \"name\": \"d-ns\",\n        \"pattern\": \"^project-d-ns-\",\n        \"apply-to\": \"queues\",\n        \"definition\": {\n            \"ha-mode\": \"exactly\",\n            \"ha-params\": 3,\n            \"ha-sync-mode\": \"automatic\",\n            \"expires\": 604800000,\n            \"ha-sync-batch-size\": 100,\n            \"queue-mode\": \"lazy\"\n        },\n        \"priority\": 1\n    },\n    {\n        \"vhost\": \"/\",\n        \"name\": \"d-s\",\n        \"pattern\": \"^project-d-s-\",\n        \"apply-to\": \"queues\",\n        \"definition\": {\n            \"ha-mode\": \"exactly\",\n            \"ha-params\": 3,\n            \"ha-sync-mode\": \"automatic\",\n            \"expires\": 604800000,\n            \"queue-master-locator\": \"min-masters\",\n            \"ha-sync-batch-size\": 100,\n            \"queue-mode\": \"lazy\"\n        },\n        \"priority\": 1\n    }],\n    \"queues\": [],\n    \"exchanges\": [{\n        \"name\": \"project.topic.default\",\n        \"vhost\": \"/\",\n        \"type\": \"topic\",\n        \"durable\": true,\n        \"auto_delete\": false,\n        \"internal\": false,\n        \"arguments\": {\n    }\n}],\n\"bindings\": []\n\n}\n```\nI'll leave this open before we add more tests and review a few more scenarios.. We added tests with 3 different test cases. Seems to be resolved at the moment, if there's a definitions file that master cannot import, please share it with us and we will add it to the test suite.. rabbitmq-management config schema confirms your findings. It should be day.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Your shell expands it. Why it may be expanding quoted values, I'm not sure.. @sylvainhubsch we are already looking into it.. @sylvainhubsch thank you for offering your help, by the way! We'd be happy to receive your feedback on a PR once it is up for QA.. @sylvainhubsch if you're still interested in reviewing our proposed implementation for it, take a look at the PRs above :) Thank you!. We are not trying to improve cycle detection or introduce limiting here. The goal is to make it possible to route on the original dead lettering event attributes (using the headers exchange).\nExpiration count limiting may be an interesting feature to consider in the future but it should be discussed on rabbitmq-users first.. See server and system logs. Very likely it's our good old friend SELinux (https://github.com/rabbitmq/rabbitmq-server-release/issues/39 and no shortage of mailing list threads).. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Syncing data from peers is not directly related to how much disk space is available. Default free disk space alarm threshold is configurable and is 50 MB by default.\nWhen a node detects it may need more file descriptors that the OS limit on start, it logs a warning.\nSee Production checklist for recommended values (the default is an kernel setting, NOT a RabbitMQ one, unfortunately).. Also, attempts to reach a peer to sync metadata tables from are logged as of at least 3.6.7, and so are retries after errors and timeouts, if any.. Tested this locally and deployed into our long-running environment. So far looking promising, no negative stats in over 1 hour (we had a setup that reproduced it every 2 minutes \u2014 so, after every stats GC run \u2014 before).. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. You have lots of messages because you had a lot of STOMP connections when your node hit an alarm and a process that is mean to handle alarms wasn't around. There can be clues as to why it had to terminate (or failed to start, although that should prevent node from starting) earlier in the log.\nWe have no logs and not even RabbitMQ version to look into if this may be something known => mailing list material.. Forced channel shutdown reports (the amqp_channel_sup_sup ones) are not related and are almost certainly harmless.. @lukebakken this was introduced in https://github.com/rabbitmq/rabbitmq-server/issues/1223. Do you know why the sub-process may be sticking around?. @lasfromhell you can set rabbit.vm_memory_calculation_strategy to erlang in your config file and wmic won't be used.. @lasfromhell if you can reproduce this in a minimalistic Windows VM, perhaps you can share it with us?. @atroxes thank you, we will try reproducing it some more. FWIW this is not something that's commonly reported (and Windows is a surprisingly popular deployment target for us), so any help with reproducing it would be greatly appreciated. Any chance you can share an image of your test env with us privately, for example?. The hypothesis (and the proposed experiment/solution) sounds plausible.\nLet's try it.\nOn Thu, Sep 7, 2017 at 9:34 AM, Daniil Fedotov notifications@github.com\nwrote:\n\nObservation:\nWhen running a lot of wmic processes in concurrently, it takes longer to\nreturn and a single WmiPrvSE.exe process consumes most CPU. Same for\ntasklist and get-process (powershell)\nAssumption: All this commands call a blocking API.\nHypothesis: The get_system_process_resident_memory is called via\nget_memory_use from many queues checking if it's the time to page\nmessages to disk.\nProposed experiment: start a rabbitmq windows server, create many queues\nand start publishing transient messages to hit the paging ratio. Monitor\nCPU usage of WmiPrvSE.exe and a number of cmd processes.\nProposed fix: memoize the get_system_process_resident_memory function\nwith ets and fixed update period.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1343#issuecomment-327854330,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQvZ7PDveEGsA7fxpG3SagqBL7oc8ks5sgBsegaJpZM4PC8i7\n.\n\n\n-- \nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. wmic activity is no in way related to message rates. @atroxes can you monitor how often new cmd and wmic processes are launched and for those that do not terminate, what they may be doing? (e.g. run a tracing tool similar to strace on them?). @hairyhum good find. file_handle_cache should then periodically update its known memory used value or cache it another way.. @atroxes the issue were are trying to resolve with the one off build above is is whether we effectively \"leak\" wmic processes by starting them \"too often\" (we figured it uses a shared resource that requires OS-wide synchronisation), not whether CPU usage is 0. It's expected that not starting wmic processes will consume less resources than not starting them.. According to a team member, using wmic adds ~ 4% to CPU utilisation even when it is executed several times a minute, so, not on the hot code path. For cases where the difference is significant it is possible to avoid using wmic entirely by relying on the erlang strategy for memory usage calculation.\nWe also learned that memory usage calculation calls on the hot path can get problematic on UNIX systems as well, so the change in https://github.com/rabbitmq/rabbitmq-common/pull/221 would help there as well. Good job, @hairyhum \ud83d\udc4d . @lasfromhell @atroxes we believe this is addressed. We will have a snapshot build later today that includes a fix: would you be interested in verifying it?. @atroxes I think this snapshot should no longer exhibit this behavior. Please give it a try :). @atroxes asking the OS about how much RAM a node uses every 2-3 seconds seems pretty reasonable to me. If you have suggestions on a more efficient tool that would return the same result and is shipped with Windows 7+, please let us know on the mailing list.. @atroxes it's not Erlang that queries wmic, it's RabbitMQ itself. We need a way to query the kernel about how much RAM the OS process uses because #1223. wmic is the most portable and straightforward (least fragile) way of doing that on the command line that we know. The 2-3 second frequency is about right and seems reasonable to our team.. @jdahl see above. You can revert to the previous strategy without us doing anything. It is already fixed in https://bintray.com/rabbitmq/all-dev/rabbitmq-server/3.6.12-alpha.44#files/rabbitmq-server/3.6.12-alpha.44, so feel free to give it a try.. @lukebakken @gerhard @hairyhum we can bump memory monitor refresh rate on Windows but if wmic.exe turns out to be completely unsuitable for periodic runs (once a second or two is hardly a high rate for what we ask it to do), we can change the default for Windows.. @jdahl ok, thanks. We will default to the runtime strategy on Windows as of 3.6.13 then.. An update: due to what we've learned in this issue and a few other places, as of 3.6.13 there are further adjustments to the strategy: as of https://github.com/rabbitmq/rabbitmq-common/pull/225 it now uses runtime's allocators stats (which supposedly track every single malloc performed). This means no external tools are invoked. In addition, as of https://github.com/rabbitmq/rabbitmq-common/pull/221 we avoid frequent calls to the function in question: it is now invoked once a second.\nBecause existing strategy names no longer make sense with these changes, we renamed them to allocated (n\u00e9e rss) and legacy (n\u00e9e erlang). rss and erlang are still supported for backwards compatibility, allocated/rss is still the default.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Please start a rabbitmq-users thread and provide\n\nA shell transcript of the steps you are taking\nThe Erlang/OTP version used\n\nThere were breaking changes in the type spec syntax between R16 and 18.0, and gm.erl:446 in 3.6.1 is a type spec definition.\n3.6.1 binary packages are also available.. The most recent version supported in the 3.6.1 days was 18.x.. @AceHack 3.6.12 does not include this fix. See this issue's milestone: it's set to 3.6.13.. If anyone would like to test this change, use a recent 3.6.13 snapshot build which are available from Bintray.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. @AceHack if you are asking for something to be supported, consider providing more than one line of information and explain why it is necessary in your opinion. Open source software maintainers are not always great at mind reading (or familiar with every single tool out there). It would really increase the likelihood of your issue being addressed, or even considered as a valid issue (see above).. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. @kareemnaguib from 3.6.11 release notes:\n\nThis release has no other known incompatibilities with versions 3.6.7 through 3.6.10.\nSee 3.6.7 release notes upgrade and compatibility sections if upgrading from an earlier release.\n\n3.6.7 release notes are available from Github just like for any other release in the last 12-18 months.. RabbitMQ Docker image is maintained by Docker, Inc. We only provide examples for rabbitmq-autocluster and related 3.7.0 plugins.. Here's the repository and there's already an issue about Windows support.. Thanks to @Gsantomaggio there are several examples for Docker compose:\n\netcd\netcd + HAproxy\nConsul + HAproxy\n\nPlus a couple for Kubernetes.\nThey all use rabbitmq-autocluster because for \"scaling up and down\" to work, newly added nodes need to be able to discover their peers and that's the recommended way of doing it (and something that will ship in 3.7 as a built-in feature with an extensible interface).\nI'd like to point out that scaling a realistic system usually involves understanding the workload, its limiting factors and how the apps (primarily consumers) scale as well, not just bumping a number. For example, if HAproxy is used, how do you scale it when publishers max out its network link? Are your apps prepared to use more than one connection endpoint? And so on.\nI wish this part could be wrapped in a container.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. @rgl there is no big idea behind not having that line. I recall a similar discussion and a similar question about the Windows service. Feel free to submit a PR that adds Restart=on-failure against the stable branch. I think we should also add a StartLimitIntervalSec value, say, 5 seconds?. @rgl would you like to submit a PR or should our team handle this?. Restarting forever may be OK but not rate limiting (in particular, not limiting concurrency of possible restart attempts) sounds like a very bad idea to me. I \ud83d\udc4e setting StartLimitIntervalSec to 0. A single restart attempt every 10 seconds seems reasonable to me.. Why do we need to restart forever? Is it really such a good idea? I don't know.\nI'm no systemd expert but my reading of the docs is that they have the same \"restart intensity\" settings as in Erlang: a time interval and how many restart attempts (\"bursts\") in that time frame are considered to be reasonable.\nThe only caveat is\n\nthey apply to all kinds of starts (including manual)\n\nand\n\nNote that systemctl reset-failed will cause the restart rate counter for a service to be flushed,\nwhich is useful if the administrator wants to manually start a unit and the start limit interferes with that\n\nSo it sounds like this will effectively restart forever unless things are so broken that it restarts more than N times in T seconds.\nIn which case maybe \"1 restart a second\" or two seconds should be the limit. Because\u2026\n\nNote that this rate-limiting is enforced after any unit condition checks are executed,\nand hence unit activations with failing conditions are not counted by this rate limiting. RabbitMQ does not intentionally stop when the disk is full: enough I/O operations fail and cause certain critically important parts to shut down. Restarting forever is a great idea on the surface, not so much in practice. We highly recommend replacing nodes that ran out of disk space.. With a full disk restarts would have failed as well, possibly forever. Picking a cut-off frequency is pretty challenging for the general case but according to the docs quotes above 1 time per second should be reasonable.. @rgl we updated the example and will make Restart=on-failure the default (with a 10 second restart delay). Thank you for your feedback along the way!. Thank you for your time.\n\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. See server logs for unhandled exceptions.. @sigiesec possibly but in 10 years of RabbitMQ history this is maybe 3rd time I recall someone asking for it. Our small team has much higher priorities (that we are asked about every day), so at this point it's a wontfix.. @sigiesec primarily because our team won't get to this any time soon, possibly at all. That doesn't mean that we wouldn't consider a contribution (or this issue won't be reopened should something change in our opinion or priorities).. making rabbitmqctl stop idempotent is an option for 3.7.0.. So far there is no consensus on whether rabbitmqctl stop should be idempotent, so we need an alternative solution (such as making systemd ignore certain exit codes).. As well as full server logs.\nOn Thu, 14 Sep 2017 at 19:34, Luke Bakken notifications@github.com wrote:\n\n@stawiu https://github.com/stawiu could you please provide a\ndefinitions file to reproduce this issue? Thanks!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1363#issuecomment-329636806,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQpWAZsQHZ9dc1GWZLalWhLGTh5KCks5sibf4gaJpZM4PYTcr\n.\n-- \nStaff Software Engineer, Pivotal/RabbitMQ\n. Was the user used to import the definitions an administrator?. Let's be consistent and move this where it belongs at this stage: the mailing list. Definitions import actually happens in rabbitmq-management, not in the core.. Thank you for your time.\n\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. I don't see anything that would prevent such feature in the spec (e.g. by dictating the default). It's true that misbehaving clients that consume but never ack deliveries can put an unreasonable amount of stress on a node.\nThis will be investigated for 3.7.0 as an exception.. @lukebakken yes, it feels like the right thing to do and 10s is a conservative but reasonable value. Client libraries recover after a 5 second wait, for example. The justification is this: whatever condition caused the failure may be transient (otherwise restarting won\u2019t change a thing) but it likely won\u2019t disappear immediately.. IIRC we did a similar thing for the Windows service before.. https://github.com/rabbitmq/rabbitmq-server-release/commit/2f50f82bf580728443edbdd66e605ee156d177b3 provides the background.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. If you are confident that consumers will be able to keep up, you can choose to publish messages as transient (they can still be moved to disk but it will happen less frequently). I'd think twice before doing so because consumers do slow down or fail from time to time, resulting in a very quick message backlog build-up.\nA much safer and likely more effective option would be to make your publishers compress message  payloads.\n5 MB/s is not a particularly high rate, so provisioning a faster disk should be an option as well.. Also note that the rate alone is not relevant. Disk I/O rate with be roughly message write rate * average message size. One operation per second with a 6 MB payload would produce a disk write rate comparable to what top reports above.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. See Stopping nodes and synchronization. Queue master promotion events are visibly logged. Please upgrade to 3.6.12 and describe the exact steps that can be used to reproduce your test on the mailing list.. And please provide full server logs as well.. Measuring throughput impact of this (in both overflow modes) is what's left to QA.. Initial set of local tests do not demonstrate a meaningful deviation from master. We are going to set up a long running environment to see if this is true for runs that span hours/days in a more realistic deployment environment.. @uvzubovs will be in 3.7.0-rc.2 later this week.. sysctl is a standard OS X (well, BSD) tool that must be available in PATH.\nyou can switch memory calculation strategy to erlang, in which case sysctl won't be used. See the docs or example config file on GitHub.. I don't think so, this is the first time it is reported.RabbitMQ process\ncannot call sysctl.\nOn Mac it's not as important to have a precise memory consumption\ncalculation, so using the old strategy if needed is perfectly fine.\nOn Tue, 26 Sep 2017 at 20:52, Ben Ransford notifications@github.com wrote:\n\nYes, sysctl is /usr/sbin and /usr/sbin is in the default $PATH (via\n/etc/paths).\nIs this a Homebrew packaging issue, then?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1375#issuecomment-332280684,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQn2olywtZCxa3iOBgflLfWdBDXA4ks5smTm7gaJpZM4Pkm4z\n.\n-- \nStaff Software Engineer, Pivotal/RabbitMQ\n. @ransford thank you!. This was later reverted because there is no consensus on whether this is a good idea.. Thank you for your time.\n\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. It's about 2 GB.. @gerhard we can update the post we have. There isn't much to say other than \"according to whole bunch of experiments and metrics, this change does not work as well as it was supposed to\".. @gerhard \ud83d\udc4d . Was this meant to be submitted against stable since CLI tools reside in a separate repo in master?. @lukebakken some team members strongly believe rabbitmqctl stop existing with the code of 0  more harm than good. We introduced rabbitmqctl shutdown in https://github.com/rabbitmq/rabbitmq-cli/issues/181 (and also in 3.6.x) to work around some problems with rabbitmqctl stop (that are specific to certain scenarios).\nThis improves stop for some fairly specific cases and potentially hides node termination problems, say, in PCF RabbitMQ. I think a safe_stop command that systemd will use is the only alternative that seems feasible at this point.\nAlso, rabbit_cli is meant to be gone in master, although I see one usage.. Section 3.1.3.3 of the spec says that the # wildcard matches 0 or more segments. There are only a handful of examples in the spec so it\u2019s not really clear whether it should match the way it does in your case. At this point subtle and not commonly reported edge cases like this one are not worth the risk of behavior changes for existing in stallations.. If you consider your examples in terms of segments (dots are their separators and carry no meaning otherwise), they become:\n\nBinding [routing] key: a, #, b\nMessage routing key: a, b\n\nSince # matches 0 or more segments, this example works as it should if you ask me.. @lamchakchan a.*.b is a, *, b in terms of segments, so it is not expected to match a.b. I don't know if a..b should even be a legal pattern but the spec provides no clarification. It is treated as an a, (empty string), b and therefore matches. \nOur team does not use GitHub issues for discussions, investigations, RCA and so on. Please start a mailing list thread if you have further questions.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. You have a number of exceptions in these logs that are not necessarily related but two of them might have the same root cause:\n\nOne is a badarg from an internal table operation\nAnother is a failed attempt to start a subprocess to re-register with epmd\n\nBoth are common when the node runs out of file descriptors. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Here are some (fairly basic) benchmark results in a constraint environment, both with and without consumer rate limiting.\nWhen consumer throughput is really constrained, we see that publishers are throttled more aggressively but consumers are never completely blocked \u2014 stdev for consumer latency is significantly lower on this branch.\nOn other workloads the difference is fairly small but surprisingly this branch demonstrates a slight higher overall throughput (which I wasn't expecting) and lower consumer latency (which was easier to foresee).\nperf_test_runs_batched_vs_stable.zip\n. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. We highly recommend using the package-provided user (rabbitmq). Using a custom user is an uphill battle with package assumptions. PID file directory has the permissions of 755, for example.\nOr do not rely on the package at all and roll your own provisioning using the generic UNIX package. I don't see any reasons to do so for 99% of users.. Timeouts is a fact of life in a production system under load, as are missed heartbeats.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. This solely means a channel didn't terminate in a certain period of time. Usually that's entirely cosmetic and happens in a system under load.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Windows is a special case. Other operating systems don't use short filenames. RabbitMQ on Windows uses them because of https://github.com/rabbitmq/rabbitmq-server/issues/493, which is a more problematic issue than this one. If someone is aware of a way to avoid both issues, let us know on rabbitmq-users.. As #493 points out, the root of the problem is that paths are passed through batch files. Even with really skinny batch files we have in mind for the future, we currently are not aware of a way to completely avoid the issue and short filenames have been working quite well for close to two years now.\nIt might be a easier to find a way to extend our new logging subsystem, say, to log to an external service exclusively. Using short paths for only a subset of directories might also be an option.\nI don't think overriding RABBITMQ_LOGS_BASE to point to an external location that does not use short filenames would change this behavior but it's quite a quick thing to try.\nAt this stage this belongs to rabbitmq-users.. https://github.com/rabbitmq/rabbitmq-server/commit/79fa87f762c299ec5f023012501843a2821cfb49 doesn't explain the intent but I bet it is: we don't want node shutdown to fail with obscure implementation-specific errors. We want the node to shut down successfully so that tools such as Monit or systemd do not freak out for no good reason. So if you consider that, this is an improvement.. I think our best bet is a (possibly non-standard?) process exit code.\n\nOn 18 Oct 2017, at 15:17, Gerhard Lazu notifications@github.com wrote:\nA rabbitmqctl shutdown can fail due to the following reasons:\nErlang cookie does not match\nRabbitMQ is booting\nRabbitMQ is stopping\nErlang VM is crashing (in some cases, it was observed that this can take many hours)\nThe Erlang VM is running but the distribution is not working correctly - this happens more often than you think in hostile environments, like almost every enterprise, multi-tenant RabbitMQ cluster.\nIn all the above scenarios, the thing that manages RabbitMQ fails because it believes that RabbitMQ is stopped (i.e. shutdown succeeded) while in actual fact the beam process is still running.\nHow should we communicate to the thing that manages RabbitMQ that the Erlang VM failed to stop?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Seems reasonable and straightforward to add.\n\nI think this can go into stable.. @carlhoerberg I will rename the key to rabbit.connection_max (we already have channel_max and frame_max) and update the docs. Thank you.. 3.7.0 will validate file paths for existence (and readability) thanks to new config format.. And yes, 3.6.x does produce log entries but I suspect they go into the -sasl.log file which has all unhandled exceptions.. With this change rabbitmqctl shutdown will produce\nError: Node rabbit@mercurio failed to shut down: {badrpc,nodedown}.\ninstead of a much more specific\n```\nError: unable to connect to node rabbit@mercurio: nodedown\nDIAGNOSTICS\nattempted to contact: [rabbit@mercurio]\nrabbit@mercurio:\n  * connected to epmd (port 4369) on mercurio\n  * epmd reports: node 'rabbit' not running at all\n                  no other nodes on mercurio\n  * suggestion: start the node\ncurrent node details:\n- node name: 'rabbitmq-cli-22@mercurio'\n- home dir: /Users/antares\n- cookie hash: 3Ep2IzZ27XUnuIsAg7vzJg==\n```\nOur goal is to make it easier differential various failure kinds.. @sathishbabu96 this is a question for the maintainers of said plugin.. Updated the issue with some more ideas we have (for post-3.8).. @hairyhum @gerhard gmake ct-clustering_management fails for me on this branch.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. We cannot suggest anything with the amount of information provided. Our team is aware of a certain degradation on Erlang 20 but you have provided no details or way to reproduce or any environment details.\nTimeouts is not necessarily an indication of a \u201cperformance degradation\u201d.. Specifically 3.6.11 and 3.6.12 aggressively use subprocesses, which was addressed already.\nAgain, this is a complete speculation and without environment information, PerfTest flags, server logs and so on we cannot suggest anything. Please take this to rabbitmq-users.. In general 1000 of 1 kB messages/second or 1500 concurrent connections is not a heavy load for any version or client library. Something environment-specific is causing the timeouts.. rabbitmq-users thread.. Note: test/close_connection_command_test.exs in rabbitmq-cli must be updated if/when this is merged into master.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. > Failed to create thread:Resource temporarily unavaliable\n\n{failed_to_start_child,net_kernel,{'EXIT',nodistribution}}\n\nare the lines you are looking for.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. A single message can only be delivered to one consumer. If that consumer immediately requeues it or its channel is closed, it will be delivered to the next one, then the next one, and so on.. The title says \"set to 0\" but the code sets to 10. So what's the target value at the moment?. It is, it's an unfinished experiment (spike).. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Without having the exact config file, node output and/or logs, even the core team cannot help you.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. @mariia-zelenova neither error [message] has anything to with IPv6.\n\nnxdomain (non-existing domain)\n\nmeans that hostname2 fails to resolve, presumably because it doesn't have an AAAA record.\n\nTCP connection succeeded but Erlang distribution failed\n\nis covered both in the docs and in rabbitmq-users archives.. On top of the above, SERVER_ERL_ARGS and SERVER_ADDITIONAL_ERL_ARGS are not the same thing: the former replaces Erlang VM command line arguments while the latter appends to the default set. We definitely recommend using the latter but the point is that you are comparing apples to oranges.. It would help immensely if you provided at least some information about your distribution, systemd version and such.\nSomehow the packages pass a test suite on a dozen of platforms. Anyhow, it is easy enough to remove those comments.. I meant systemd version, a word not known to autocomplete.\nI removed all inline comments and I guess will be producing an emergency release now.. Hm, probably not until I QA https://github.com/rabbitmq/rabbitmq-management/pull/508 and the pipeline tells me things are not obviously broken (hey, they are looking OK without this change already).. The tests start, restart and stop the service plus do some basic sanity checking.. @mctwynne is there any chance you can try the version currently in stable?\nIt will likely take about 2 hours for the pipeline to pass and looks like producing a final (or even one-off package) may have to wait until tomorrow (Nov 8th) because I'm on the road and have limited access to our infrastructure.. @mctwynne I didn't find anything that would override the .service file in the module. I am going to upload a one-off package here, at least you should be able to try it with dpkg -i.. That and https://github.com/rabbitmq/rabbitmq-management/pull/508.. @mctwynne please give this a try\nrabbitmq-server-3.6.13.901-deb.zip\n. I don't have 16.04 handy but our package tests that cover some 7-8 distributions that use systemd still pass and I did some manual testing on 17.04. Works like a charm.\nI also tested 3.6.13 manually and somehow I cannot reproduce this issue on 17.04. We will discuss doing an emergency 3.6.14 release with the team shortly.. FTR, my test VM has systemd 232.. 3.6.14 is out, thanks to @dumbbell our test suite now includes system spec file validation.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Consider posting the entire error to the mailing list. Also make sure that localhost resolves on your machine and the output of hostname and hostname -f.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. RabbitMQ does not implement TLS and it is easy to see that there are no RabbitMQ modules\nin the trace:\n** Reason for termination =\n** {badarg,[{ets,update_counter,[286803,#Ref<0.0.2.323>,-1],[]},\n            {ssl_pkix_db,ref_count,3,[{file,\"ssl_pkix_db.erl\"},{line,207}]},\n            {ssl_connection,handle_trusted_certs_db,1,\n                            [{file,\"ssl_connection.erl\"},{line,1801}]},\n            {ssl_connection,terminate,3,\n                            [{file,\"ssl_connection.erl\"},{line,938}]},\n            {tls_connection,terminate,3,\n                            [{file,\"tls_connection.erl\"},{line,336}]},\n            {gen_fsm,terminate,7,[{file,\"gen_fsm.erl\"},{line,610}]},\n            {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}\nA badarg thrown by an ETS operation often (and counterintuitively) comes down to a node being out of file descriptors.. rabbitmq-users thread. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Networking guide explains what tools can help with investigating networking-related issues.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. The address is taken from the socket. I can think of two things that may be going on:\n\nYour server has multiple interfaces, you inspect one but RabbitMQ/client connections use the other\nConnections go through a load balancer and proxy protocol support is not available in 3.6.x\n\nSee Troubleshooting Networking for some starting points.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. The connection fails to log in:\n** Reason for termination == \n** {{case_clause,{error,not_allowed}},\n    [{rabbit_mqtt_processor,process_login,4,\n                            [{file,\"src/rabbit_mqtt_processor.erl\"},\n                             {line,406}]},\n     {rabbit_mqtt_processor,process_request,3,\n                            [{file,\"src/rabbit_mqtt_processor.erl\"},\n                             {line,81}]},\n     {rabbit_mqtt_reader,process_received_bytes,2,\n                         [{file,\"src/rabbit_mqtt_reader.erl\"},{line,224}]},\n     {gen_server2,handle_msg,2,[{file,\"src/gen_server2.erl\"},{line,1049}]},\n     {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}\nPlease upgrade to 3.6.14 first and then see the authentication section in MQTT plugin docs as well as guides on access control and virtual hosts in the rest of documentation to get a more complete picture.. Thank you. rabbitmq-components.mk is synced from https://github.com/rabbitmq/rabbitmq-common, so https://github.com/rabbitmq/rabbitmq-common/pull/242 is sufficient.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. There is no limit on the number of virtual hosts or users. As of 3.7.0 it is possible to enforce certain limits for specific virtual hosts but that is an opt-in thing.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Heartbeat timeouts is not necessarily (and most likely not) an indication of an issue. Please upgrade to 3.6.14 ASAP regardless.. FTR: the code above is for JMS client. We don't have any evidence of an issue but if there is one, it's in that client, not the server.. And please upgrade from 3.6.1 (to at least 3.6.14).. @tianon thanks for reporting, it should be available in the next alpha (3.8.0-alpha.13 or later) once it goes through the Concourse pipeline.\n3.7.1 will take another week or two but you should be able to make progress on the new style config support in the Docker image using the aforementioned build :). I cannot reproduce:\n```\ntotal_memory_available_override_value.conf\ntotal_memory_available_override_value = 500 MB\n```\n```\nRABBITMQ_CONFIG_FILE=$HOME/Downloads/total_memory_available_override_value.conf rabbitmq-server\n##  ##\n  ##  ##      RabbitMQ 3.7.2. Copyright (C) 2007-2017 Pivotal Software, Inc.\n  ##########  Licensed under the MPL.  See http://www.rabbitmq.com/\n  ######  ##\n  ##########  Logs: /Users/antares/Tools/rabbitmq/generic/var/log/rabbitmq/rabbit@mercurio.log\n                    /Users/antares/Tools/rabbitmq/generic/var/log/rabbitmq/rabbit@mercurio_upgrade.log\n          Starting broker...\n\ncompleted with 14 plugins.\n```\nThis was addressed in 3.7.1.. This is not a support forum. Please take your questions to rabbitmq-users and provide more details.. As a workaround you can define federation upstreams using the federation-management plugin.. Without having logs from both sides and steps to reproduce I'm afraid we cannot suggest much. \nShovel's AMQP 1.0 interop has been tested against 4 implementations, only 1 of which is RabbitMQ itself.\nIn an unrelated note, the Shovel plugin has its own repository so this issue doesn't belong here even if it had all the details.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Check yours apps. Those are client connections, see Connection Event Logging.. @Dean-Christian-Armada we do not know anything about your applications, so we cannot possibly help. Logs, Network Troubleshooting guide and a traffic capture plus adding logging/metrics to your apps is what you should employ.. One thing that's obvious to me from the log: apps connection and immediately close TCP connection (that does not count for disconnecting, as Networking explains). Which almost always indicates a failing app or an unhandled exception that causes it to exit very early on. Application-level monitoring and logs can prove this hypothesis. Don't guess, collect data instead.. This will require fundamental changes to how priority queues are implemented. Not a priority for our team (no pun intended) at the moment, sorry.. This belongs to https://github.com/rabbitmq/rabbitmq-server-release.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Spring AMQP folks take questions on Stack Overflow labelled as spring-amqp. See server logs and consider taking a traffic capture. Collect data, don't guess.\nThere is no limitation in RabbitMQ that prevents clients from publishing and consuming on a single connection (or channel). There are scenarios in which publishers are blocked.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Start with the tutorials, then create a separate user (guest can only connect from localhost) and connect to a node running on the remote host.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. DNS-based discovery requires that the hostname specified in the config resolves to at least one A or AAAA record that can a reverse DNS resolution query would work on. That's it. Docker Swarm doesn't change the way DNS works, so it shouldn't matter.\nHow you manage DNS records in a Docker environment is a question that is in no way specific to RabbitMQ.. 2017-12-18 12:23:02.480 [info] <0.184.0> Peer discovery backend rabbit_peer_discovery_dns does not support registration, skipping randomized startup delay.\n2017-12-18 12:23:02.481 [info] <0.184.0> Addresses discovered via A records of rabbitmq: 10.0.1.22, 10.0.1.7, 10.0.1.12\n2017-12-18 12:23:02.484 [info] <0.184.0> Addresses discovered via AAAA records of rabbitmq:\n2017-12-18 12:23:02.484 [info] <0.184.0> All discovered existing cluster peers: rabbit@bridge-apps-test_rabbitmq.3.txyqdhq5tag4ypvqxmrrv7xux.bridge-apps-test_bridgeoverlay, rabbit@9d08e408217e.bridge-apps-test_bridgeoverlay, rabbit@b133becbbdce\n2017-12-18 12:23:02.484 [info] <0.184.0> Peer nodes we can cluster with: rabbit@bridge-apps-test_rabbitmq.3.txyqdhq5tag4ypvqxmrrv7xux.bridge-apps-test_bridgeoverlay, rabbit@9d08e408217e.bridge-apps-test_bridgeoverlay\n2017-12-18 12:23:02.490 [warning] <0.184.0> Could not auto-cluster with node rabbit@bridge-apps-test_rabbitmq.3.txyqdhq5tag4ypvqxmrrv7xux.bridge-apps-test_bridgeoverlay: {badrpc,nodedown}\n2017-12-18 12:23:02.496 [warning] <0.184.0> Could not auto-cluster with node rabbit@9d08e408217e.bridge-apps-test_bridgeoverlay: {badrpc,nodedown}\n2017-12-18 12:23:02.496 [warning] <0.184.0> Could not successfully contact any node of: rabbit@bridge-apps-test_rabbitmq.3.txyqdhq5tag4ypvqxmrrv7xux.bridge-apps-test_bridgeoverlay,rabbit@9d08e408217e.bridge-apps-test_bridgeoverlay (as in Erlang distribution). Starting as a blank standalone node...\nare the relevant log lines. Peer discovery does success but cluster formation doesn't.. Peer discovery will cluster with the nodes that can be reached from the discovered list. Forming a cluster with a networking split already in place is going to have this effect. There aren't too many alternatives I can think of, one of them \u2014 wait for a set of nodes to be up before forming a cluster \u2014 has been tried in the rabbitmq-clusterer plugin and it turned out to be a true operational disaster. Now if one node is down, your entire cluster won't form.\nSo this works as expected from peer discovery at the moment.. There is no consensus on the best way to address this => mailing list material.. @AceHack I'm sorry for how blunt I'm going to be but you need to stop fixating so much on the scenario you have discovered. This is not a new discovery, all this we have seen before with BOSH and Cloud Foundry, and things work pretty decently in practice as of 3.6.7+ and once we got rid of rabbitmq-clusterer. There is plenty of production evidence of that\nAs rabbitmq-autocluster README states, peer discovery is not a substitute for understanding of how RabbitMQ clustering works. You seem to be a bit confused about that (sorry).\nThe reason why this is NOT a major problem is this: once a cluster is formed, peer discovery isn't used and nodes simply rejoin the peers they know. Do you resize your cluster 10s or 100s times a day? In that case it may or may not be production ready. Existing members will not use peer discovery, only newly brought up will, so the problem won't apply to the majority of nodes.\nYour specific suggestions on what can be changed are welcome on the mailing list.. Another idea brought up on rabbitmq-users that is easy to try is this: when a peer is contacted right after discovery, we can introduce the \"N retries every T seconds\" approach that we already use for re-joining nodes.\nIn theory it should help with the transient network partition scenario as long as it goes away in a certain window of time. As for possible downsides, the only one I can think of is much slower cluster formation in certain scenarios (when retries happen a lot).. This is a consequence of the move to Lager in #94.. Will reopen until this passes the master pipeline and is merged into v3.7.x.. Note that this feature now has to be enabled with log.exchange = true in the config. It is now possible to use a different log severity for it.. @rolatsch as rabbit.log.exchange.enabled (which can be either true or false):\nerlang\n[\n {rabbit, [\n           {log, [\n                  {exchange, [\n                              {enabled, true}]\n                  }\n                 ]}\n          ]}\n].. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. CORS support is available in Web STOMP as of 3.6.0. If it is a change in RabbitMQ itself, it's either in the Web STOMP plugin or due to our migration to Cowboy 2. Either way, it's a mere speculation because we have next to no details about the environment or a way to reproduce plus there's an external HTTP server/reverse proxy involved => mailing list material.. Queue index entries are moved to disk in batches, so when a limit is hit there can be quite a few messages still not flushed out.. Re-openening until #1462 is merged.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. There will be a yet another master version of this PR.. The message will be a single line in the log even if it's a multi-line Erlang string. In other words, they are actually concatenated by erlc.. Some feel very strongly about multi-line log entries, so yes, this is intentional.. This was reported in the management plugin repo several weeks ago and addressed.. Duplicate of https://github.com/rabbitmq/rabbitmq-management/issues/51.. I'm pretty sure we run a Lager version that includes https://github.com/erlang-lager/lager/pull/411. However, https://github.com/erlang-lager/lager/pull/411 will not necessarily make log message dropping disappear.\nBefore I explain what would be an improvement we should take a step back and ask ourselves what the warning means in the first place. Original Erlang/OTP logger (and any other logging library in any language) can be overwhelmed by the callers. When that happens, its memory usage baloons, sometimes to the point it can cause major issues for node stability. There is an entire chapter about this in Erlang in Anger.\nLager and some other logging tools avoid this by dropping some messages when they consider themselves to be overwhelmed.\nRabbitMQ on boot can print quite a few things in a very short period of time, especially when debug logging is used. So some messages are dropped, which is not a problem in and of itself but it can mask log entries that are particularly important for investigations.\nSince boot step messages are unlikely to really overwhelm the logger, we can temporarily bump the limit and then lower it to make sure all boot messages are logged.. There is a section on verifying configuration in the docs. Anyhow, the logging rate watermark will be temporarily 10 times higher during boot as of 3.7.3.. @dimas thank you for providing a reliable way to reproduce.. We don't know but if the runtime doesn't notify RabbitMQ connections of socket events, it breaks the connection state machine.. Closing because https://bugs.erlang.org/browse/ERL-562 is now resolved. We'll add more details about any possible backports here.. @florianajir this is not a support forum.\nThe message means that the client (s_client) and the node cannot agree on a TLS protocol version and it makes sense: you passed -ssl3 and SSLv3 connections are refused by default because of known vulnerabilities.\nEnabling SSLv3 is highly discouraged.. Moved to https://github.com/rabbitmq/rabbitmq-auth-backend-ldap/issues/78.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. After a certain number of enqueued messages queues have to move them to disk and deliver to consumers and those things are not entirely parallel. Paging out a lot of transient messages to disk is certainly a fairly expensive operation. It is possible to see how may messages are in memory/on disk/on disk after a forced page out on the queue page in management UI.\nOn top of that there are several other factors that are dynamic in nature and affect throughput:\n\nTCP window scaling kicks in after a certain point\nHow the VM does scheduling (processes with spikes of activity will get less scheduler time after a certain point)\n\nPlease keep questions to rabbitmq-users.. This is a 4.0-specific change, which means we either have to roll it back or create a branch for 3.8 and reconfigure Concourse and so on to not assume that master targets 3.8.0.. Thank you!. Here's one thing that definitely looks suspicious in the node log:\n2018-01-24 15:38:39.112 [info] <0.198.0> Adding vhost '/'\n2018-01-24 15:38:39.167 [info] <0.198.0> Creating user 'guest'\n2018-01-24 15:38:39.174 [info] <0.198.0> Setting user tags for user 'guest' to [administrator]\n2018-01-24 15:38:39.181 [info] <0.198.0> Setting permissions for 'guest' in '/' to '.*', '.*', '.*'\n2018-01-24 15:38:39.199 [info] <0.462.0> started TCP Listener on [::]:21048\n2018-01-24 15:38:39.204 [info] <0.478.0> started SSL Listener on [::]:21049\n2018-01-24 15:38:39.217 [info] <0.198.0> Setting up a table for connection tracking on this node: 'tracked_connection_on_node_rmq-ct-register_interceptor-1-21048@localhost'\n2018-01-24 15:38:39.228 [info] <0.198.0> Setting up a table for per-vhost connection counting on this node: 'tracked_connection_per_vhost_on_node_rmq-ct-register_interceptor-1-21048@localhost'\n2018-01-24 15:38:39.229 [info] <0.33.0> Application rabbit started on node 'rmq-ct-register_interceptor-1-21048@localhost'\n2018-01-24 15:38:39.229 [info] <0.33.0> Application rabbitmq_ct_helpers started on node 'rmq-ct-register_interceptor-1-21048@localhost'\n2018-01-24 15:38:39.230 [info] <0.33.0> Application rabbitmq_ct_client_helpers started on node 'rmq-ct-register_interceptor-1-21048@localhost'\n2018-01-24 15:38:39.230 [notice] <0.89.0> Changed loghwm of /Users/antares/Development/RabbitMQ/umbrella.git/deps/rabbit/logs/ct_run.ct_rabbit@warp10.2018-01-24_15.37.49/deps.rabbit.channel_interceptor_SUITE.logs/run.2018-01-24_15.37.50/log_private/rmq-ct-register_interceptor-1-21048@localhost/log/rmq-ct-register_interceptor-1-21048@localhost.log to 50\n2018-01-24 15:38:39.293 [info] <0.5.0> Server startup complete; 0 plugins started.\n2018-01-24 15:38:40.651 [info] <0.407.0> node ct_rabbit@warp10 up\n2018-01-24 15:38:41.238 [info] <0.523.0> accepting AMQP connection <0.523.0> (127.0.0.1:51527 -> 127.0.0.1:21048)\n2018-01-24 15:38:41.287 [error] <0.523.0> Error on AMQP connection <0.523.0> (127.0.0.1:51527 -> 127.0.0.1:21048, vhost: 'none', user: 'guest', state: opening), channel 0:\n {handshake_error,opening,\n                 {amqp_error,internal_error,\n                             \"access to vhost '/' refused for user 'guest': vhost '/' is down\",\n                             'connection.open'}}\n2018-01-24 15:38:41.291 [info] <0.523.0> closing AMQP connection <0.523.0> (127.0.0.1:51527 -> 127.0.0.1:21048, vhost: 'none', user: 'guest')\n2018-01-24 15:52:34.032 [info] <0.407.0> node ct_rabbit@warp10 down: connection_closed\n2018-01-24 15:53:05.802 [info] <0.53.0> SIGTERM received - shutting down\n2018-01-24 15:53:05.802 [info] <0.198.0> Peer discovery backend rabbit_peer_discovery_classic_config does not support registration, skipping unregistration.\n2018-01-24 15:53:05.803 [info] <0.478.0> stopped SSL Listener on [::]:21049. @hairyhum and I have a decent hypothesis about what's going on: a newly booting node will try to create the default vhost on all running nodes but with the new all_nodes/0 implementation there will be none (since the node hasn't finished booting yet). So it's a chicken and egg problem. More importantly, all_nodes/0 doesn't have to change for this PR to address its original issue.. A node-local table makes sense to me. I don't have an opinion on whether it would be meaningfully easier to maintain than the ETS version.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. The VM rejects the -stbt flag. Perhaps you want +stbt with a value.. Bintray rate limited our org without any kind of warning. We\u2019ve contacted support.. @liming-gd this is not a support forum. Bintray has blocked us again, we are contacting their support.. The complaint was that it was picked from the ephemeral port range. We can make it configurable e.g. via an environment variable but it might be good enough as is to the reporter.. I'd personally file a separate issue and worry about it for 3.7.4.. Closing, see https://github.com/rabbitmq/rabbitmq-server/pull/1488 for reasoning.. Is this ready to be QA'ed by Concourse or more changes (e.g. for Windows) are coming?. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Consider consulting server logs instead of taking a guess and wiping a node next time.\nThere are several threads that explain how to force delete an unresponsive queue in rabbitmq-users archives.\nUpgrading to 3.6.15 is the only thing that we can recommend without having access to the logs.. This is not an issue in the management plugin. It's either https://github.com/rabbitmq/rabbitmq-federation/issues/67 or a nearly identical issue with a different runtime parameter.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. You are looking at this change in rabbitmq-management. You must use the same rabbitmqadmin version as ships with RabbitMQ, in other words, a 3.7.x version against a 3.7.x node and 3.6.15 against 3.6.15, respectively.. The table in the docs are mostly unchanged from the classic format, they just list rabbit.* keys but your observations are correct. It's trivial to add.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. > RMQ has lost it's connection to the application and the errors persist until\n\nthe resque-pool service is restarted\n\nRabbitMQ does not connect to applications, it's the other way around. Make sure that you have connection recovery enabled in Bunny, understand how it works and there are no proxies or load balancers with low idle connection timeout as your first steps.\nIf you use Resque and your connections to RabbitMQ are short lived, make sure they are not leaked in your code and that RabbitMQ nodes have at least 15-20K of file descriptors available.\nLastly make sure you don't run on Erlang/TP 18.x since there are known bugs that can lead to node not accepting connection (although if restarting Resque helps then it doesn't seem to be an issue).\nThis is 100% mailing list material.. 1 and 2 sound fairly safe. I'm not sure about 3: what if the node hosting master comes back, and quickly?\nIn 4 we have a chicken-and-egg problem since processes are started from a list of queues we have in the database. In fact, I'm quite sure we already delete all non-durable queues early.. So this is expected behavior. We cannot delete such queues (the user never asked, they are not due for deletion due to the auto-delete flag, etc). It should be possible to delete such temporarily stopped queues manually.\nSuggestions 1 and 2 make sense to me.. Neither exclusive nor auto-delete really apply here. If a queue has no mirrors in sync and all mirrors are restarted, in theory we can consider deleting it but that will lead to claims that that is also a bug and we should never delete mirrored queues (e.g. we do not mirror exclusive queues to avoid such confusion).\nWe already have several hacks that work around scenarios if masters going away and quickly back up before queue consensus decisions migrate  to Raft.\nSo items 1 and 2 are fine with me, other options will shift the confusion elsewhere. It's an interplay between 3 features that were not meant to make sense together at all times. Promoting out-of-sync mirrors OR using durable queues for the data you don't want to lose makes a lot more sense than more and more implementation hacks.. There is already an option that makes it possible to promote out-of-sync mirrors. The user just has to opt in.. @gerhard it's a good idea to increase the option's visibility across the board. I don't think that always should be the default or be actively recommended, though. . @gerhard so what's the scope of this issue now? Making queue.delete more aggressive towards non-durable queues without an elected master and updating the docs?. Thank you.. rabbitmq-users thread.. Thank you! We'll backport once this change passes our entire pipeline successfully.. Backported, will be included into 3.7.4.. #1515 and #1516 were reverted due to CI failures that seem related.. Thanks for the details.\nI corrected the persistent option of Bunny::Channel#publish.. The FIFO order assumption is fairly natural given the FIFO-based nature of RabbitMQ queues but I see your point. Using a different data structure should be a possibility. If you'd like to dig deeper we'd appreciate your suggestions.\nBy the way, this is something we'd be able to fit into 3.8.0 but likely not 3.7.x.. A very quick investigation suggests that there's a natural order dependency: clients can acknowledge N messages at once in increasing delivery tag order. Therefore a map or dict isn't necessarily going to be a good fit.. @benmmurphy thanks, that's a useful data point I should have thought of. We do want to improve this, I just wanted to explain that the tendency towards FIFO has a reason behind it (besides also being a common workload).. I'd like to investigate options that would not require new queue arguments. We already have a lot of them and they don't always work well when combined. Since @benmmurphy understands the problem, wrokarounds and has an example that uses different acknowledgement strategies, we can take some time to think this through.. lists:sort/1 is a BIF and queue:out/1 is not. The Big O characteristics matter only when the input is sufficiently large. 8000 items is not a small list but it's nowhere near the \"approaches infinity\" case.. Publishers go through the channel and won't be blocked immediately, though (only via credit flow).\nI'd recommend using Looking Glass, a profiler we've developed for longer running tests. @acogoluegnes was going to do just that, in fact.. With the multi-ack case, does LIFO vs. FIFO really matter? LIFO with multi-ack should be no different from single ack if we start at the end. Should we use a multi-ack FIFO test instead?. Topic exchanges use a trie for storing bindings, so that's not entirely surprising. Feel free to investigate whether it'd be possible to replace N fine grained locks with a coarse grained one.. Let's not tie 3.7.5 release schedule to this issue. It will need a bit more work, as will its cousin #1566. @gerhard any objections from you? We can always do a short 3.7.6 release after.. We believe this is largely addressed by #1589 (a 60% speedup in some common scenarios).. ct-cluster_rename, ct-clustering_management and ct-dynamic_ha seem to be affected by this.. A fix will be included into 3.7.4 and 3.6.16.. It would be interesting to see some profiling data. Ack subtraction search/sorting and conversion to lists seem to be the most costly operations after a very quick look (this is a guess, of course).. We concluded that without switching to a different set of data structures we cannot address this without seriously affecting other code paths. Closing as we won't be looking into this in the medium term.. I could not find any docs that claim that it should be but adding net_ticktime to the schema is easy enough.. @acogoluegnes good point. There\u2019s net_kernel:set_net_ticktime/{1,2}, which we can call in a boot step. I wouldn\u2019t mind moving the example to the advanced config but it is a fairly commonly used setting.. I suggest we name it just net_ticktime to not suggest that other kernel.* keys may be configurable as well (which won't be the case).. This issue ended up being more involved than we expected. See #1524 for discussion. There's a chance we will consider it to not be worth the potential risk.. See the discussion in #1524.. I discovered the following during manual testing: since the transition period by default takes 60 seconds, should a CLI tool contact a running node with net_ticktime in the .conf file with this PR, it will fail with something like this:\nStopping and halting node rabbit@warp10 ...\nError:\nfunction_clause\n Stacktrace [{net_kernel,set_net_ticktime,\n                         [{ongoing_change_to,20},0],\n                         [{file,\"net_kernel.erl\"},{line,195}]},\n             {rabbit_misc,rpc_call,5,\n                          [{file,\"src/rabbit_misc.erl\"},{line,1175}]},\n             {'Elixir.RabbitMQCtl',maybe_run_command,3,\n                                   [{file,\"lib/rabbitmqctl.ex\"},{line,113}]},\n             {'Elixir.RabbitMQCtl','-exec_command/2-fun-0-',5,\n                                   [{file,\"lib/rabbitmqctl.ex\"},{line,80}]},\n             {'Elixir.RabbitMQCtl',main,1,\n                                   [{file,\"lib/rabbitmqctl.ex\"},{line,43}]},\n             {'Elixir.Kernel.CLI','-exec_fun/2-fun-0-',3,\n                                  [{file,\"lib/kernel/cli.ex\"},{line,76}]}]\nbecause CLI tools temporarily tweak node's net_ticktime for the two values to be in sync.\nWe can try a transition period of 0 but I don't know what complications can stem out of that. I don't think the change is worth the risk since it's a minor convenience improvement.. A correction: it's not just CLI tools, it's any code that uses rabbit_misc:rpc_call/5 but it should not be used on any hot code path.\nUpdating that function to support transition periods should be sufficient. It can still potentially be a breaking change for the environment where CLI tools use a significantly higher tick time value vs. the server: the server would then detect inactivity on an idle connection much earlier than the CLI tool. We don't have a lot of reports about that, though, possibly because CLI tool connections do go inactive for long enough periods of time.. According to https://github.com/rabbitmq/rabbitmq-common/pull/60, that function is used by management and possibly other plugins.. What branch was this based on?\nI'm pretty sure you can simply keep the version from master. It has some CLI-related test changes and a few new tests from #1465.. Let's start with #1526 and avoid merging v3.6.x into master. A new branch for 3.6.x can be created later.. \u2026and @dumbbell :). @gerhard @essen should we proceed to QA this or more changes are coming? What are the steps you were/are using to test, in a nutshell?. Not running rabbit_node_monitor:handle_dead_rabbit/2 should probably be a separate issue. With our Raft foundation maturing I'd prefer avoid short term workarounds.. @gerhard is there a version that isn't affected and can be used to test? Is this PR complete otherwise?. I posted some feedback in #1527 that is worth replicating here.\nI tested this with a different setup than described in #1526, namely\n\n2 nodes\n500 connections, 500 channels each\n150K queues (75K each)\n\nsince the changes are heavily focused on the bulk cleanup of queues.\nMy observations confirm significantly quicker node cleanup (from many minutes to about 1 on a 5 year old machine with an SSD) and lower peak RAM usage\nin the process.\nDid another test with 100K queues on each node. The cleanup completes reasonably quickly when peer node is shut down. According to Observer data, there's definitely some room to optimize stats collector/cleanup for cases where a large number of exclusive queues is deleted due to client connection termination. But that's for another PR.. I filed #1566 for all further improvements. This PR only focuses on the scenario where a cluster member stops/loses connection and some or even all of its queues have to be removed all at once.. CI has discovered that ets:select_replace/2 is not available in OTP 19.3 \ud83d\ude22.. Did another test with 100K queues on each node. The cleanup completes reasonably quickly when peer node is shut down. According to Observer data, there's definitely some room to optimize stats collection for cases where a large number of exclusive queues is deleted due to client connection termination.. It can already be done using RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS but I have no objections to these two.. If this is driven by the needs of the BOSH release (which is a guess, perhaps @gerhard can elaborate), I have no objections to having this in 3.6.x as well.. I can confirm that @lukebakken's changes make the issue go away. I was also able to config SCHEDULER_BIND_TYPE and a few other things via rabbitmq-env.conf.\nDoing more testing next.. @gerhard the docs suggest rabbitmq-env.conf should contain var, which takes precedence over DEFAULT_var. Both are overriden by RABBITMQ_var from the environment should it be set. This logic has been around for years. #873 therefore is incomplete.. Another thing I've noticed, which doesn't have to be addressed in this PR, is that in rabbitmq-env.conf environment variables are not supposed to have RABBITMQ_ prefixes. I don't think it works that way for all variables, trying to prove myself wrong a.t.m.. The docs suggest otherwise.. Merging per verbal approval from @dumbbell.. @irfanjs our public mailing list is rabbitmq-users.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. The only piece of advice we have for 3.1.x is upgrade. You are dozens of releases and several years behind.. Sure, makes sense \ud83d\udc4d.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. This is covered in quite a bit of detailed in a dedicated doc section.. It won't change that drastically but the input size can become quite a bit smaller in some cases.. This belongs to a different repo, we will see what can be done.. We're investigating this, so far it doesn't seem to be a typo. The name gets mangled/chopped somewhere in the pipeline.. Update: we've engaged Bintray support a few weeks back, no resolution so far.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. An nxdomain is non-ambiguous. We are not aware of any cases where it would be masking a different issue. You can customise hostname resolution using a special file a great deal with the Erlang VM, without touching DNS or the hosts file.. The limit can be exceeded in a short burst. This intentional: we decided to accept a certain inaccuracy\nwhen the limit is loaded over doing a cluster-wide transaction on a potentially hot and critically important (for the apps) code path.. They all come down to\nsystem_limit, {internal table name}\nSo the internal database failed to create a set of tables. A quick inspection of OTP codebase suggests that system_limit is used for cases that have little to do with kernel limits but nonetheless it's not something that RabbitMQ is responsible for or something that's widely reported. This is mailing list material.. rabbitmq-users thread.\nThere is a couple of interesting scenarios that go beyond peer discovery but come down to the same chicken-and-egg parallel cluster formation issue. If we identify things we can improve in a backward-compatible way, we will file new focused issues. However, a random startup delay for such scenarios [with parallel node startup on initial boot] is a good idea anyway, which we will mention in the Cluster Formation guide.\nThank you for providing the logs, @triggetry.. \nThank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you... We can't suggests anything with the amount of information provided. Please help others help you by posting as many details as possible, including server logs and full exceptions to the mailing list.\nhttp://www.rabbitmq.com/troubleshooting-networking.html documents a methodology we recommend for troubleshooting connectivity issues. Please start there.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. A channel terminates but there are no details that would help identify why. Please provide a way to reproduce with 3.7.4 to the list.. rabbitmq-users thread.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. We cannot identify the cause without full server logs.. There's already #1501 and #1549 for making it possible to delete such queues. There is no single reason that can lead queue masters to become unresponsive and in most cases there are no known ways to reproduce. Unfortunately we cannot fix an issue that we cannot reproduce at least some of the time.. Our small team doesn't have the capacity to work on new peer discovery mechanisms in the near future. The community is welcome to develop whatever they need since most mechanisms are already plugins.\nIn addition we expect issues to have a certain amount of information and this one is really light on details. Feel free to start a rabbitmq-users thread about why it is necessary, what it might look like and who may be interesting in helping you develop this mechanism.. Lager does not support syslog out of the box and we don't have the consensus about shipping Lager plugins with RabbitMQ.. FTR, the exception says \"module lager_syslog_backend is undefined\" (undef).. Yes, we\u2019d definitely appreciate a doc update. Thank you!\nPerhaps putting together an example of Lager + Syslog for 3.7.x would be useful, it definitely can be a small community plugin even if we decide to not include the [Lager] plugin into RabbitMQ core or distribution.\nI\u2019m not a Lager expert, so you\u2019ll have to bribe @lukebakken or @hairyhum :). @majormoses syslog was never supported out of the box. We didn't break anything.. I edited out all Syslog mentions in the logging guide (which was introduced together with 3.7)\nIf someone wants to discuss a RabbitMQ plugin that would allow logging to syslog, you are welcome to do it on the mailing list.. @hairyhum please review my changes from this morning.. Sorry, we now cherry-pick from master into v3.7.x instead of submitting separate PRs. I'm almost done with that.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. See tutorial 2 and the guide on consumer acknowledgements. If that's not enough for your needs, you probably should use a database with a query language, not [just] RabbitMQ.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. This has been discussed numerous times on rabbitmq-users.\n/bin/df: '/usr/local/lib/rabbitmq/var/lib/rabbitmq/mnesia/rabbit@mq84619641': No such file or directory\\n\nmeans that node's free disk space monitor tried to run df against /usr/local/lib/rabbitmq/var/lib/rabbitmq/mnesia/rabbit@mq84619641 and that returned No such file or directory. In modern versions (3.6.15, 3.7.x) that would simply disable disk space monitoring after N retry attempts. It is not a \"node crash\" and alone cannot affect node's ability to serve client connections (but there can be other issues with the same root cause that can).. I have a guess about how df subprocess failures can potentially be related to some clients failing to connect: your node may be running out of file descriptors for short periods of time.\nSee http://www.rabbitmq.com/networking.html and http://www.rabbitmq.com/production-checklist.html.\n\nOn 20 Mar 2018, at 12:19, Brandy notifications@github.com wrote:\nsome errors occur after running for days.\nsome clients get errors such as AMQP 172.x.x.x is unreachable now and then, but its service is ok.\n=CRASH REPORT==== 23-Feb-2018::14:30:24 ===\ncrasher:\ninitial call: rabbit_disk_monitor:init/1\npid: <0.85.0>\nregistered_name: rabbit_disk_monitor\nexception exit: {unparseable,\"/bin/df: '/usr/local/lib/rabbitmq/var/lib/rabbitmq/mnesia/rabbit@mq84619641': No such file or directory\\n\"}\nin function gen_server:terminate/6 (gen_server.erl, line 737)\nancestors: [rabbit_disk_monitor_sup,rabbit_sup,<0.77.0>]\nmessages: []\nlinks: [<0.84.0>]\ndictionary: []\ntrap_exit: false\nstatus: running\nheap_size: 987\nstack_size: 27\nreductions: 18982466\nneighbours:\n=SUPERVISOR REPORT==== 23-Feb-2018::14:30:24 ===\nSupervisor: {local,rabbit_disk_monitor_sup}\nContext: child_terminated\nReason: {unparseable,\"/bin/df: '/usr/local/lib/rabbitmq/var/lib/rabbitmq/mnesia/rabbit@mq84619641': No such file or directory\\n\"}\nOffender: [{pid,<0.85.0>},\n{name,rabbit_disk_monitor},\n{mfargs,{rabbit_disk_monitor,start_link,[100000000]}},\n{restart_type,{transient,1}},\n{shutdown,4294967295},\n{child_type,worker}]\n=SUPERVISOR REPORT==== 23-Feb-2018::14:30:24 ===\nSupervisor: {local,rabbit_disk_monitor_sup}\nContext: start_error\nReason: unsupported_platform\nOffender: [{pid,{restarting,<0.85.0>}},\n{name,rabbit_disk_monitor},\n{mfargs,{rabbit_disk_monitor,start_link,[100000000]}},\n{restart_type,{transient,1}},\n{shutdown,4294967295},\n{child_type,worker}]\n=CRASH REPORT==== 23-Feb-2018::14:35:19 ===\ncrasher:\ninitial call: rabbit_reader:init/2\npid: <0.3517.2>\nregistered_name: []\nexception exit: {aborted,\n{no_exists,[rabbit_runtime_parameters,cluster_name]}}\nin function mnesia:abort/1 (mnesia.erl, line 310)\nin call from rabbit_runtime_parameters:lookup0/2\nin call from rabbit_runtime_parameters:value0/2\nin call from rabbit_reader:server_properties/1 (/usr1/mesos/data/slaves/14630586-f200-43ea-bba9-d42a707bbd6f-S267/frameworks/961fa35f-2c92-4fbd-af3e-b90e969bc459-2023/executors/mesos-jenkins-73f4a31394214adbbde1b3d345fa9618-OceanStorDJ_Pack/runs/235ff257-c6dc-4d88-84fd-02b5eb8e14dd/jenkins/workspace/p5, line 167)\nin call from rabbit_reader:start_connection/3 (/usr1/mesos/data/slaves/14630586-f200-43ea-bba9-d42a707bbd6f-S267/frameworks/961fa35f-2c92-4fbd-af3e-b90e969bc459-2023/executors/mesos-jenkins-73f4a31394214adbbde1b3d345fa9618-OceanStorDJ_Pack/runs/235ff257-c6dc-4d88-84fd-02b5eb8e14dd/jenkins/workspace/p5, line 921)\nin call from rabbit_reader:handle_input/3 (/usr1/mesos/data/slaves/14630586-f200-43ea-bba9-d42a707bbd6f-S267/frameworks/961fa35f-2c92-4fbd-af3e-b90e969bc459-2023/executors/mesos-jenkins-73f4a31394214adbbde1b3d345fa9618-OceanStorDJ_Pack/runs/235ff257-c6dc-4d88-84fd-02b5eb8e14dd/jenkins/workspace/p5, line 871)\nin call from rabbit_reader:recvloop/4 (/usr1/mesos/data/slaves/14630586-f200-43ea-bba9-d42a707bbd6f-S267/frameworks/961fa35f-2c92-4fbd-af3e-b90e969bc459-2023/executors/mesos-jenkins-73f4a31394214adbbde1b3d345fa9618-OceanStorDJ_Pack/runs/235ff257-c6dc-4d88-84fd-02b5eb8e14dd/jenkins/workspace/p5, line 327)\nin call from rabbit_reader:run/1 (/usr1/mesos/data/slaves/14630586-f200-43ea-bba9-d42a707bbd6f-S267/frameworks/961fa35f-2c92-4fbd-af3e-b90e969bc459-2023/executors/mesos-jenkins-73f4a31394214adbbde1b3d345fa9618-OceanStorDJ_Pack/runs/235ff257-c6dc-4d88-84fd-02b5eb8e14dd/jenkins/workspace/p5, line 309)\nancestors: [<0.3515.2>,rabbit_tcp_client_sup,rabbit_sup,<0.77.0>]\nmessages: []\nlinks: [<0.3515.2>]\ndictionary: [{process_name,\n{rabbit_reader,\n<<\"8.46.196.41:45804 -> 8.46.196.41:5671\">>}}]\ntrap_exit: true\nstatus: running\nheap_size: 10958\nstack_size: 27\nreductions: 3685\nneighbours:\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Hash function use when computing directory names has nothing to do with security. We just need to compute a comparable/unique value that won't break when vhost name contains spaces, slashes, backslashes and so on (which is fairly common, e.g. when vhost names are generated or at Windows-centric organisations). We cannot easily switch to a different function when computing directory names, for example: it cannot be done for existing installation.\n\nAs for the cookie hash, we can switch to a different function easily in both RabbitMQ server and CLI tools but it has to happen in lock step.. @Ayanda-D I'm sorry but I strongly disagree with your assessment.\nThe use of MD5 in the cookie hash is only a concern if the attacker has a way to repeatedly produce it against a remote cookie file. The remote cookie hash only appears in server logs on boot. You decide how often that happens.\nAs for the local cookie hash, to display it the attacker must be able to invoke a CLI tool against a remote node and CLI tools require sudo privileges in most environments. Again, you decide about how significant a vector of attack that is and what else the attacker might be able to do if they obtained that kind of permissions.\nAs for message store directories, the idea is to turn a value that might contain whitespace characters ,\nslashes and so on into a value that does not. We might have used a Base N encoding of some kind with the same degree of success but historically MD5 was picked. The only possibility of attack here that I can think of is trying to decipher a vhost or queue name from a snapshot of directory names. To obtain which in practice you need to have sudo privileges or access to the local file system, which likely means you can do a lot more than just list directory names. \nNot every use case of a hashing function has much to do with security or can be exploited through a brute force attack.\nThe only real reason for addressing this issue is various automated scan tools or check lists that do not go into any kind of specifics and are designed around sweeping generalisations such as \"an environment that uses MD5 for anything cannot be considered secure\".. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Please post a code sample that can be used to reproduce to rabbitmq-users as well as server logs. Consumers should be re-registered when a new queue master is promoted since 3.4.0 but we have no details as to what the queue properties and events involved are. This is mailing list material at this point.. rabbitmq-users thread.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Done in #1589 with some non-trivial gains.\nWe will see if this can be backported to 3.6.x.. Unfrotunately we will have to undo the changes that make most of the difference in #1589 as they can lead to inconsistent views of the binding tables: transactions that perform removal are not idempotent and this can lead to situations where a rolled back transaction \"restores\" all bindings deleted by the exchange type module but its retry doesn't find any bindings to delete and so there are bindings left around.\nThe least efficient part of queue deletion is binding deletion because there is currently no way to load bindings of a queue (or exchange) without a full scan on one of the binding tables. This unfortunate design can be mitigated with secondary indices but this would be a breaking schema change and therefore can only go into 3.8.0.\nThis was discovered during QA of https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/pull/39 and is catastrophic for that particular exchange type since it has state that depends on the state of bindings and any consistency will break routing.\nPer discussion with @dcorbacho @hairyhum @kjnilsson @dumbbell, kudos to @acogoluegnes for reproducing the issue in https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/pull/39.. This had side effects on binding deletion consistency and was reverted.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Cluster upgrades between feature versions require an ordered restart, which\n\nCluster upgrade needed but other disc nodes shut down after this one\n\nhints at. That and more (e.g. Blue/Green deployment migrations) are documented in the Upgrade guide.. We test quite a few upgrade permutations, including an upgrade from 3.5.8 as part of our CI pipeline.\nCluster upgrade from 3.5.x to 3.7.x will require a cluster-wide shutdown with an ordered restart, as the docs explain. Or you can do a Blue/Green deployment upgrade.\nSorry but there is no evidence of a bug. This is mailing list material at this point.. I now see you have a section about case sensitivity of node names. This is a never ending source of fun on Windows and keeping track what was done by default in what version from years ago is not realistic. Setting RABBITMQ_NODENAME is a reasonable workaround which we will mention in the docs. Thanks for bringing this up to our attention.\nI suspect that other operating systems which tend to use case-insensitive filesystems are not affected.. @Bhaal22 we have Windows package tests but not upgrades on Windows. I filed a documentation guides issue and this will be covered. I'm not sure we can safely force a particular case assuming there are years worth of releases that do not do that.\nPerhaps we can emit a warning of some kind when COMPUTERNAME and hostname do not match but most of our team have concluded that virtually no developer pays any attention to warnings or logs until things go south (in production, obviously).\nScenarios where hostnames have changed are operator's responsibility. Explicitly setting RABBITMQ_NODENAME or using rabbitmqctl rename_cluster_node in that case will be needed for other reasons.. We recognise that it is not ideal and can be very confusing. Thank you for getting to the bottom of it.\nIf @lukebakken has ideas about what kind of change would be reasonably safe here, we'd be happy to file another issue and consider it.\nWe can modify the list loaded from nodes_running_at_shutdown to be all lowercase and filter out duplicates, for example, but I expect such change to break unexpectedly in other scenarios, possibly even beyond Windows :(\nThis is a yet another argument for Blue/Green deployment upgrades, which our docs don't promote enough.. The PR was indeed closed without merging because we figured it was not a safe things to do.. Duplicate of https://github.com/rabbitmq/ra/issues/15.. This is by design to help compare this to the hash produced by the CLI tools in case CLI-to-node authentication fails.\nThis is a hash, not the actual value.. This is an Elixir requirement, not that of RabbitMQ CLI tools (which are in https://github.com/rabbitmq/rabbitmq-cli, by the way).\nThere are no plans to switch away from using Elixir for CLI tools.. Configuration software that uses CLI tools can override locale environment variables for the calls that where parsing the output is warranted:\nLANG=\"en_US.UTF-8\" LC_ALL=\"en_US.UTF-8\" rabbitmqctl \u2026\nas a workaround. We will consider forcing this in our shell scripts (filed https://github.com/rabbitmq/rabbitmq-server/issues/1573 for that with a more specific problem statement) but IIRC this has been considered before and rejected.. Filed it here and not in https://github.com/rabbitmq/rabbitmq-cli because the wrappers are in this repo at the moment.. We understand that there will be tension about whether this is a good idea and what default should be used. IIRC this is exactly why we didn't do it before. Requiring every user to configure locale settings would also suck.\nFWIW our CLI tools are currently not localised and it's unlikely to change in the medium term, so there's less potential for breakage.\n. @gerhard might have an opinion on this :). Closing in favor of https://github.com/rabbitmq/rabbitmq-website/issues/512. We'll update the Chef cookbook as well. Thank you for bringing this up, @wyardley.. There is a way to install and run RabbitMQ without sudo, it's called the generic binary release. Uncompress the package anywhere you need and run ./sbin/rabbitmq-server, then ./sbin/rabbitmqctl and such to do just about anything that will not require binding to ports below < 1024.. FTR, the last time the same question was asked was in #1076. It was given the same answer: use the generic binary distribution, not a Debian or RPM package which will require sudo (and that is hardly an uncommon requirement for a data service).\nThere are options to run RabbitMQ for development purposes without having local sudo sudo permissions , too: run it in Vagrant (or Docker or similar) which provide have sudo and use local TCP port forwarding to forward local port(s) to those in the Vagrant VM. Celery must have a way to configure the port.\nThis is 100% mailing list material.. Debian, RPM and generic binary build installation guides were updated to mention their respective sudo requirements.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. * There is rabbitmqctl authenticate_user\n * rabbitmqadmin can be used if the user has the management tag (which can be added temporarily)\n * Alternative authN backends have more options: LDAP tooling for the LDAP backend, curl for the HTTP backend and so on.. I'd recommend using RabbitMQ's own change log since Pivotal RabbitMQ releases happen less frequently.. Sometimes things slip through the cracks. It's hardly a major addition. 3.6.5 is 20 months old now and CLI tools have been reworked and migrated to their own sub-project in 3.7.0, so I don't think it's worth mentioning at this point.. Testing with AWS, Consul and etcd found no regressions.. Like nearly everything that's related to packaging and release production, this belongs to https://github.com/rabbitmq/rabbitmq-server-release/.. Thank you. rabbit_net:hostname/0 has to be exported for this to work, I'll take care of that.. FTR, the rabbit-common commit to go with this is https://github.com/rabbitmq/rabbitmq-common/commit/83ba37ef5428f8334f1885558743bd37ff020b72.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. If an exchange does not exists, it's a channel error with code = Not Found. A node will pre-create several exchanges, including a fanout called amq.fanout, in the default vhost on first boot. It is also possible to make it declare just about any topology you please on boot.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. GC here refers to \"file compaction\", not memory GC. Without a way to reproduce at least some of the time we cannot suggest much.. The link does work for me. The release is still there. Perhaps there was a GitHub connectivity issue of sorts.. As installation docs suggest, binary release artefacts can be obtained from multiple sources.. I see @levshanton is Russia-based. Russian Internet censorship agency has been blocking subnets of AWS, Google Cloud, Azure, Digital Ocean and so on en masse over the last week or so (we are talking millions of IP addresses total). I wouldn't be surprised if GitHub was affected at some point.\nGet on an out-of-country VPN to compare.. Our team confirmed that the file can be downloaded successfully from\n\nSpain\nThe UK\nFrance\nNorth America (US and Canada, east and west coasts alike). amq.rabbitmq.trace is a regular exchange that is only used when tracing for the vhost is enabled (of which we have no information). However, there's one thing that's unusual about it: it is not published to in a regular way. It is also declared as an internal exchange, meaning it is used internally by RabbitMQ.\n\nRabbitMQ nodes already collect publishing totals. I see little value to worry about the fact that the tracing mechanism doesn't bother to collect metrics. Our team has hundreds of more important issues to spend our time on.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. There is a way to configure that in Lager, the logging library RabbitMQ uses starting with 3.7:\nerlang\n[\n  {lager, [\n    {error_logger_hwm, 300}\n  ]}\n].\n(the default is 50). It can only be configured via the advanced.config file (or in the classic rabbitmq.config).\nRabbitMQ never logs messages so whatever your test does, it's something else that stresses the logger (possibly a high connection churn or channel exception rate).. I can reproduce the gains mostly with our primary use case with a lot of exclusive queues.. Backported to 3.7.x and will be in 3.7.6.. @antoine-galataud there will be no more 3.6.x releases except for security patches, sorry.. As part of working on https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/issues/37 (https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/pull/38, https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/pull/39) we have identified scenarios in which this grouped binding deletion results in non-idempotent transaction retries, which means stateful exchange types can leave some of their state behind without knowing it.\nWe are not yet sure we can make them idempotent without changing binding table schema (which can only be done for 3.8.0 and beyond), so we will partially or completely revert this optimization \ud83d\ude1e . A lot has been learnt in this PR and https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/issues/37, though, so it will influence our future approach to binding storage and removal.. Unfrotunately we will have to undo the changes that make most of the difference in this PR. They can lead to inconsistent views of the binding tables: transactions that perform removal are not idempotent and this can lead to situations where a rolled back transaction \"restores\" all bindings deleted by the exchange type module but its retry doesn't find any bindings to delete and so there are bindings left around.\nThis was discovered during QA of https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/pull/39 and is catastrophic for that particular exchange type since it has state that depends on the state of bindings and any consistency will break routing.\nThe least efficient part of queue deletion is binding deletion because there is currently no way to load bindings of a queue (or exchange) without a full scan on one of the binding tables. This unfortunate design can be mitigated with secondary indices but this would be a breaking schema change and therefore can only go into 3.8.0.\nPer discussion with @dcorbacho @hairyhum @kjnilsson @dumbbell, kudos to @acogoluegnes for reproducing the issue in https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/pull/39.. This is worth backporting to 3.6.16.. This is related to #1181.. We went with 255 as that's the value we originally advertised in the docs.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. > com.rabbitmq.client.AlreadyClosedException: connection is already closed due to connection error; protocol method: #method(reply-code=320, reply-text=CONNECTION_FORCED - broker forced connection closure with reason 'shutdown'\nis the line you are looking for. The node was asked to shut down. RabbitMQ Java client supports automatic connection recovery, so does Spring AMQP. Those modes have limitations and were discussed numerous times on the mailing list.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. All messaging protocols RabbitMQ supports assume that connections are long lived. This is not always possible with PHP but your code demonstrates the worst possible anti-pattern with every other client that doesn't have PHP's limitations.\nOpening a connection is a multi-step negotiation process before you even get to authentication and virtual host access check (most of what really is \"authorisation\" happens later when a client tries to perform an operation, which your code does not). In other words, it is multiple round trips. Setting up a TCP connection and doing multiple round trips is significantly more expensive than any \"authorisation\" code in RabbitMQ.\nYou can easily see this in a Wireshark traffic capture.\nDon't open and close a connection per operation.. 100K connections can be perfectly reasonable. 100K channels on a connection never is.. Yup, so there seem to be two popular options: \"be really conservative\" (PostgreSQL, WebSphere MQ) or \"have no limit\" (Cassandra, ActiveMQ). I think the file descriptor limit already serves as a natural guard rail for connections.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. How to QA priority definition via policies (a drive-by change in this feature):\n\nSet up a policy\nDeclare a matching queue\nPublish 200 messages with random priorities (I used 1 through 5)\nAdd a consumer that prints message priority and observe that they are consumed in priority order\n\nJava client test PR focuses on the limits and should be self-explanatory.. We found out why policies were not supported earlier: they can change while certain message store aspects of priority queues are not (must be fixed). So I will undo some portions of this PR and docs.. 3.6.10 is 5 releases behind even 3.6.x. There were several unrelated issues with similarly looking stack traces in 3.6.x series. They are no longer common in 3.6.15/3.7.x days and without a way to reproduce we cannot suggest much.. I'm getting the following unit test failures:\n```\nunit_log_config_SUITE > config_syslog_handler\n    #1. {error,\n            {{assertEqual,\n                 [{module,unit_log_config_SUITE},\n                  {line,460},\n                  {expression,\n                      \"sort_handlers ( application : get_env ( lager , handlers , undefined ) )\"},\n                  {expected,\n                      [{lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_default.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info},\n                            {size,0}]},\n                       {lager_syslog_backend,\n                           [{facility,daemon},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {identity,\"rabbitmq\"},\n                            {level,info}]}]},\n                  {value,\n                      [{lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_default.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [info,{},\n                            {lager_default_formatter,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]}]},\n             [{unit_log_config_SUITE,'-config_syslog_handler/1-fun-0-',1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,460}]},\n              {unit_log_config_SUITE,config_syslog_handler,1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,460}]},\n              {test_server,ts_tc,3,[{file,\"test_server.erl\"},{line,1546}]},\n              {test_server,run_test_case_eval1,6,\n                  [{file,\"test_server.erl\"},{line,1062}]},\n              {test_server,run_test_case_eval,9,\n                  [{file,\"test_server.erl\"},{line,994}]}]}}\nunit_log_config_SUITE > config_syslog_handler_options\n    #1. {error,\n            {{assertEqual,\n                 [{module,unit_log_config_SUITE},\n                  {line,478},\n                  {expression,\n                      \"sort_handlers ( application : get_env ( lager , handlers , undefined ) )\"},\n                  {expected,\n                      [{lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_default.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info},\n                            {size,0}]},\n                       {lager_syslog_backend,\n                           [{facility,local1},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {identity,\"foo\"},\n                            {level,warning}]}]},\n                  {value,\n                      [{lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_default.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [warning,{},\n                            {lager_default_formatter,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]}]},\n             [{unit_log_config_SUITE,\n                  '-config_syslog_handler_options/1-fun-0-',1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,478}]},\n              {unit_log_config_SUITE,config_syslog_handler_options,1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,478}]},\n              {test_server,ts_tc,3,[{file,\"test_server.erl\"},{line,1546}]},\n              {test_server,run_test_case_eval1,6,\n                  [{file,\"test_server.erl\"},{line,1062}]},\n              {test_server,run_test_case_eval,9,\n                  [{file,\"test_server.erl\"},{line,994}]}]}}\nunit_log_config_SUITE > config_multiple_handlers\n    #1. {error,\n            {{assertEqual,\n                 [{module,unit_log_config_SUITE},\n                  {line,401},\n                  {expression,\n                      \"sort_handlers ( application : get_env ( lager , handlers , undefined ) )\"},\n                  {expected,\n                      [{lager_console_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info}]},\n                       {lager_exchange_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info}]},\n                       {lager_syslog_backend,\n                           [{facility,daemon},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {identity,\"rabbitmq\"},\n                            {level,error}]}]},\n                  {value,\n                      [{lager_console_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info}]},\n                       {lager_exchange_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info}]},\n                       {syslog_lager_backend,\n                           [error,{},\n                            {lager_default_formatter,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]}]},\n             [{unit_log_config_SUITE,'-config_multiple_handlers/1-fun-0-',1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,401}]},\n              {unit_log_config_SUITE,config_multiple_handlers,1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,401}]},\n              {test_server,ts_tc,3,[{file,\"test_server.erl\"},{line,1546}]},\n              {test_server,run_test_case_eval1,6,\n                  [{file,\"test_server.erl\"},{line,1062}]},\n              {test_server,run_test_case_eval,9,\n                  [{file,\"test_server.erl\"},{line,994}]}]}}\nunit_log_config_SUITE > config_handlers_merged_with_lager_handlers\n    #1. {error,\n            {{assertEqual,\n                 [{module,unit_log_config_SUITE},\n                  {line,246},\n                  {expression,\n                      \"sort_handlers ( application : get_env ( lager , rabbit_handlers , undefined ) )\"},\n                  {expected,\n                      [{lager_console_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,error}]},\n                       {lager_exchange_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,error}]},\n                       {lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_file.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,debug},\n                            {size,0}]},\n                       {lager_syslog_backend,\n                           [{facility,daemon},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {identity,\"rabbitmq\"},\n                            {level,info}]}]},\n                  {value,\n                      [{lager_console_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,error}]},\n                       {lager_exchange_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,error}]},\n                       {lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_file.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,debug},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [info,{},\n                            {lager_default_formatter,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]}]},\n             [{unit_log_config_SUITE,\n                  '-config_handlers_merged_with_lager_handlers/1-fun-0-',1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,246}]},\n              {unit_log_config_SUITE,\n                  config_handlers_merged_with_lager_handlers,1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,246}]},\n              {test_server,ts_tc,3,[{file,\"test_server.erl\"},{line,1546}]},\n              {test_server,run_test_case_eval1,6,\n                  [{file,\"test_server.erl\"},{line,1062}]},\n              {test_server,run_test_case_eval,9,\n                  [{file,\"test_server.erl\"},{line,994}]}]}}\n```. We should the crash even if it's entirely cosmetic. It will be a magnet for questions and support tickets.. Test failures are back in c526e453d3749f8d3b91601aa3376d867eb5c3b4:\n```\nunit_log_config_SUITE > config_syslog_handler\n    #1. {error,\n            {{assertEqual,\n                 [{module,unit_log_config_SUITE},\n                  {line,460},\n                  {expression,\n                      \"sort_handlers ( application : get_env ( lager , handlers , undefined ) )\"},\n                  {expected,\n                      [{lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_default.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [info,{},\n                            {lager_default_formatter,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]},\n                  {value,\n                      [{lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_default.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [info,{},\n                            {lager_default_formatter,\n                                [color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]}]},\n             [{unit_log_config_SUITE,'-config_syslog_handler/1-fun-0-',1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,460}]},\n              {unit_log_config_SUITE,config_syslog_handler,1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,460}]},\n              {test_server,ts_tc,3,[{file,\"test_server.erl\"},{line,1546}]},\n              {test_server,run_test_case_eval1,6,\n                  [{file,\"test_server.erl\"},{line,1062}]},\n              {test_server,run_test_case_eval,9,\n                  [{file,\"test_server.erl\"},{line,994}]}]}}\nunit_log_config_SUITE > config_syslog_handler_options\n    #1. {error,\n            {{assertEqual,\n                 [{module,unit_log_config_SUITE},\n                  {line,476},\n                  {expression,\n                      \"sort_handlers ( application : get_env ( lager , handlers , undefined ) )\"},\n                  {expected,\n                      [{lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_default.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [warning,{},\n                            {lager_default_formatter,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]},\n                  {value,\n                      [{lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_default.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [warning,{},\n                            {lager_default_formatter,\n                                [color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]}]},\n             [{unit_log_config_SUITE,\n                  '-config_syslog_handler_options/1-fun-0-',1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,476}]},\n              {unit_log_config_SUITE,config_syslog_handler_options,1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,476}]},\n              {test_server,ts_tc,3,[{file,\"test_server.erl\"},{line,1546}]},\n              {test_server,run_test_case_eval1,6,\n                  [{file,\"test_server.erl\"},{line,1062}]},\n              {test_server,run_test_case_eval,9,\n                  [{file,\"test_server.erl\"},{line,994}]}]}}\nunit_log_config_SUITE > config_multiple_handlers\n    #1. {error,\n            {{assertEqual,\n                 [{module,unit_log_config_SUITE},\n                  {line,401},\n                  {expression,\n                      \"sort_handlers ( application : get_env ( lager , handlers , undefined ) )\"},\n                  {expected,\n                      [{lager_console_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info}]},\n                       {lager_exchange_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info}]},\n                       {syslog_lager_backend,\n                           [error,{},\n                            {lager_default_formatter,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]},\n                  {value,\n                      [{lager_console_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info}]},\n                       {lager_exchange_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,info}]},\n                       {syslog_lager_backend,\n                           [error,{},\n                            {lager_default_formatter,\n                                [color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]}]},\n             [{unit_log_config_SUITE,'-config_multiple_handlers/1-fun-0-',1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,401}]},\n              {unit_log_config_SUITE,config_multiple_handlers,1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,401}]},\n              {test_server,ts_tc,3,[{file,\"test_server.erl\"},{line,1546}]},\n              {test_server,run_test_case_eval1,6,\n                  [{file,\"test_server.erl\"},{line,1062}]},\n              {test_server,run_test_case_eval,9,\n                  [{file,\"test_server.erl\"},{line,994}]}]}}\nunit_log_config_SUITE > config_handlers_merged_with_lager_handlers\n    #1. {error,\n            {{assertEqual,\n                 [{module,unit_log_config_SUITE},\n                  {line,246},\n                  {expression,\n                      \"sort_handlers ( application : get_env ( lager , rabbit_handlers , undefined ) )\"},\n                  {expected,\n                      [{lager_console_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,error}]},\n                       {lager_exchange_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,error}]},\n                       {lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_file.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,debug},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [info,{},\n                            {lager_default_formatter,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]},\n                  {value,\n                      [{lager_console_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,error}]},\n                       {lager_exchange_backend,\n                           [{formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,error}]},\n                       {lager_file_backend,\n                           [{date,[]},\n                            {file,\"rabbit_file.log\"},\n                            {formatter_config,\n                                [date,\" \",time,\" \",color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]},\n                            {level,debug},\n                            {size,0}]},\n                       {syslog_lager_backend,\n                           [info,{},\n                            {lager_default_formatter,\n                                [color,\"[\",severity,\"] \",\n                                 {pid,[]},\n                                 \" \",message,\"\\n\"]}]}]}]},\n             [{unit_log_config_SUITE,\n                  '-config_handlers_merged_with_lager_handlers/1-fun-0-',1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,246}]},\n              {unit_log_config_SUITE,\n                  config_handlers_merged_with_lager_handlers,1,\n                  [{file,\"test/unit_log_config_SUITE.erl\"},{line,246}]},\n              {test_server,ts_tc,3,[{file,\"test_server.erl\"},{line,1546}]},\n              {test_server,run_test_case_eval1,6,\n                  [{file,\"test_server.erl\"},{line,1062}]},\n              {test_server,run_test_case_eval,9,\n                  [{file,\"test_server.erl\"},{line,994}]}]}}\n```. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. You don't need to cancel consumers before deleting a queue, it should not be necessary even when automatic recovery is enabled. If you delete a queue with active consumers, they will receive a consumer cancellation notification and be \"cancelled\" by the broker.\nWe cannot suggest much without seeing server logs. This is mailing list material.. It's not clear to me what \"works fine\" (as well as \"doesn't work fine\") means in this context.\nThe example uses exclusive queues. They have certain characteristics that can be confusing to beginners. That's my best guess anyway.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. There is no redelivery limit. Your applications have to keep track of what deliveries they've seen (and how many times).. I tested it on 19.3 and 20.3, it works as expected. The effects of these allocators flag on actual workloads needs more research and user feedback.. @essen if we are certain those things are safe for R16B03 and 19.3 users (yes, we technically still allow running on R16B03, even though the docs say it is unsupported), please do. I produced an RC of 3.6.16 earlier today but we can include this into RC2.. We haven't identified any meaningful (statistically significant) regressions with these new defaults and there's some empirical evidence on the mailing list that the new defaults make a big difference for some users. So I think we should adopt these changes.. It was cherry-picked to 3.7.6. We haven't done any 3.6.x testing yet. Also, our 3.7.x LRE runs OTP 20.1.7, so it would be cool to have that upgraded to 20.3.6 before 3.7.6 GA.. If you bring your Millennium Falcon someone might join you :). @gerhard good idea.. #1612 was merged, currently a backport of this PR is on track to be included into 3.6.16.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. The nodes do not do anything sophisticated with plugin archives: they, well, unzip them into an \"plugin expansion\" directory. Which must exist and be writeable, and the plugin file must be readable. There is nothing specific to Kubernetes about that. I suspect that using a stateful set \u2014 which makes sense and is highly recommended for other reasons \u2014 should take care of that.\nAccording to the log, the error is enfile which suggests that the node needs its file descriptor limit to be increased.. We will not be changing the name, it's has been around for years.\nSupport for multiple directories was introduced in https://github.com/rabbitmq/rabbitmq-server/pull/1016 and did not include any Windows support. I wouldn't be surprised if that's still the case because no one asked for it.. @inikulshin just so you know your sarcasm is not appreciated.\nRABBITMQ_PLUGINS_DIR supports a path separator for similar reasons. I don't see how your suggestion would be meaningfully different (or more suitable for Windows).. @inikulshin if you don't know installation paths you may want to revisit your installation procedure. RabbitMQ node data directory and installation location can be customised.\nhttps://github.com/rabbitmq/rabbitmq-server/pull/1016 was contributed to make shipping and upgrading 3rd party plugins easier, in particular for Debian and RPM packages by allowing 3rd party plugins to be in a predefined directory that is known. #1016 did not cover Windows but I see no reason why the same approach wouldn't work there.\nOur team does not use issues for discussions. Please use the mailing list for that.. I updated issue description to explain what's missing. Without a default deploying 3rd party plugins is painful for scripts that are no aware of the target version.. I'm OK with doing that but the root cause here is the lack of a fixed, well known location for Windows.. Paths that include versions can be impossible for installers to compute. C:\\ProgramData\\RabbitMQ\\plugins sounds OK if C:\\ProgramData\\ is an equivalent of /usr/local/lib on modern Windows versions.. There's already a version-specific one. This issue is only about adding a version-agnostic one.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. The log message is pretty clear: you have no synchronised mirrors, so none of them can be promoted using the effective (likely default) settings. There is a separate doc section about specifically that scenario.. Because your effective settings tell it to do so. You can choose to promote unsynchronised mirrors if availability is more important than consistency.\nThere seem to be two separate things going on, the other being unavailable stats (but not necessarily the queue master) for the queues. That's something we will be investigating soon and will file a new specific issue with all the details when we have them.. So the script restarts nodes in a tight loop. This means that a node can be forced to shut down before it had a chance to finish promotion for the queue mirrors it hosts. In addition once 2 nodes go down the remaining one would detect that it is in the minority and pause as instructed. Restarts in a tight loop are not a realistic scenario but unconditional promotion should work much better.\n3.8.0 will feature queue mirroring using a different consensus protocol that will handle cascading restarts as long as the majority of nodes are online (which in this specific test won't be the case).. > Also, LICENSE-MPL-RabbitMQ has been\n\nupdated to store the MPL 2.0 license (instead of the MPL 1.1 license)\n\nSorry, I'm afraid we cannot accept that change. Changing a license is not a matter of just editing a file: we will have to get consensus from the contributors (keep in mind that the project is 11 years old, so there's a fair number of them) and Pivotal's legal department would have to approve this and it is extremely unlikely to happen quickly or at all (license changes were discussed before internally).. A different PR that does not remove the existing license file and does not change the license but simply updates the README in a reasonable way would be considered. Thank you.. I did a couple of experiments and Licensee does not recognize MPL 1.1. Digging in the repo so far confirms that. That has to be addressed first.\nIn the meantime the README has been updated to be clearer about what MPL version is used.. No worries. We've started a conversation internally about moving to MPL2. We previously considered one very popular OSS license and for various reasons it wasn't possible. But MPL1.1 to MPL2 should be a much smaller change that maybe feasible. We will update this issue when a decision is made.\nThank you @eirinikos @dankohn!. This is very light on details. What is \u201cthe actual proxy->server connection information\u201d? How would it be useful? What are the downsides? Are there tools that support the proxy protocol and can be used as examples? Please start a mailing list discussions to hash things out and make a case.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. rabbitmq-users thread.. @josevalim thank you for the update!. @croensch this is not a discussion forum. There can be plenty of reasons for pinning that have nothing to do with OTP 21. In fact the pinning section long predates this issue or OTP 21 RCs.. All discussions, questions, opinions, suggestions, ideas belong to the mailing list. This issue is about making RabbitMQ compatible with OTP 21 and getting OTP 21 test coverage to the same level as other supported series.. There will be a couple of additional OTP 21 compatibility improvements in 3.7.8 but they hopefully don't affect more than a low single digit % of users.. Thank you!. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Start with inspecting server logs and posting to the list you [effective configuration(http://www.rabbitmq.com/configure.html#verify-configuration-effective-configuration). Then use the tools mentioned in the Troubleshooting TLS guide.\nIf inter-node communication is configured to use TLS, CLI tools must be provided certificates and private keys as well or they won't be able to connect. We cannot know if that's the case since the report does not change when TLS is enabled.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. A table used internally did not exist. By far most common reason for this is running out of file descriptors.. We are not guaranteed to have a username when a connection is lost (authentication may or may not have completed by then), so assuming it will be there is not a good idea. You have a connection identifier on connection.created events, consider using that instead.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. > ./rabbitmq-env: 111: /etc/rabbitmq/rabbitmq-env.conf: auth_backends.1: not found\nis not a whole lot of information to work with but almost certainly this is due to the fact that rabbitmq-env.conf, which must be a shell script which contains environment variables, was populated with a new style configuration file, which has a similar name (rabbitmq.conf). Obviously RabbitMQ config file is not a valid shell script so it cannot be loaded.\nrabbitmq-env.conf is not the config file location you want. It is only used to preconfigure environment variables. LDAP or any other configuration belongs to rabbitmq.conf and advanced.config or rabbitmq.config (in the classic format).. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Sorry but that's no evidence of \"a problem in RabbitMQ itself\". A traffic capture will tell exactly what's send by both peers. Server logs will reveal any channel-level exceptions, e.g. due to properties of an existing queue not being equivalent to those your app tries to use \u2014 after which there will be no channel activity and thus no responses even in theory \u2014 which is the most likely scenario. I'm not a JavaScript expect but your code lacks error handling for channel-level exceptions (they are not necessarily communicated as programming language exceptions depending on the client) as far as I can tell.\nThis is 200% mailing list material.. Further investigation suggests this behavior has been in place since at least 3.5.7 and what I remember likely goes back way before that. Looks like the docs are incorrect then.\nNote that a mirrored queue configured to use unconditional promotion will have a similar behavior to a \"migrating transient queue\" I remember from back in the day ;). Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. This is answered in a couple of doc guides:\n\nHeartbeats\nNetworking: Dealing with High Connection Churn. It depends on what the node name is. We can definitely add a note to that section.. EXTERNAL by definition makes the broker assume that authentication has been done via some other method, such as TLS peer [x509 certificate chain] verification.\n\nTwo unrelated notes: RabbitMQ 3.5.x has been out of support for well over a year. In addition you run on an Erlang version with known major bugs that affect RabbitMQ. Consider moving to Erlang 20.3.x and RabbitMQ 3.7.x.. @postables correct, with just EXTERNAL, assuming that the client also specified that mechanism, RabbitMQ will not even try to perform authentication. So using it without TLS or another mechanism RabbitMQ cannot be aware of (e.g. IP address-based or maybe even a hardware authentication device) is a Bad Idea\u2122.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. The plugin assumes that when svc_addr_nic is set (the least common option FWIW), svc_addr_auto must be false. That might be a legitimate mistake carried over from rabbitmq-autocluster but this repo has nothing to do with Consul.. According to rabbitmq-autocluster code comments, auto assumes \"use hostname\"; a configured static address should be used as given; and otherwise use NIC \u2014 but that does not include auto = true for an unknown reason. Moved to https://github.com/rabbitmq/rabbitmq-peer-discovery-consul/issues/12.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. First two hits in Google for \"rabbitmq config file\" for me:\n\nRabbitMQ configuration guide\nRabbitMQ file and directory location guide\n\nWhile not related:\n\nRabbitMQ 3.2.x has been out of support for 3-4 years now\nErlang R16B03 has been out of support for about as long and has major issues that affect RabbitMQ\n\nBoth have improvements and bug fixes related to LDAP. Please upgrade at the earliest opportunity.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Heartbeats are negotiated between RabbitMQ and clients. It's the clients that decide on the final value, so what client is used and what its settings are is critically important here. RabbitMQ doesn't have any limit on the heartbeat value you configure it to suggest. This is mailing list material.. @lukebakken we can address what we understand/can reproduce and then close.. It doesn't do that because in practice those who want to be more defensive against hostname changes override data directory path to not include hostnames to begin with, e.g. the tile has been doing that since 2014 or so, when this command was introduced to make migration to a different host/node naming scheme possible.. That said, it could detect that the current node data directory is one of the old names and move it since we know for sure that the node is stopped when this command runs.. #1650 makes this problem significantly less severe (at least on a few workloads we tested with).. Thank you for your time but our team has a different plan. As of https://github.com/rabbitmq/rabbitmq-server/pull/1615/files#diff-f8aa00d902505f5bd2fd015ff0e32444R19 we silenced the warning and by the time 3.8.0 goes GA we will require Erlang 21 and switch to the new syntax. The proposed change doesn't seem to be necessary, or at least I'm not sure what build environment would need it given that https://github.com/rabbitmq/rabbitmq-server/pull/1615 and similar PRs are in place.. Our team does not maintain Erlang/OTP or its LDAP client or TLS client implementation. Please collect more data using the same approach as described in the TLS troubleshooting guide and post them to tge erlang-questions, the Erlang user mailing list.. According to OTP 21 release notes, there are at least 5 potentially breaking changes in the ssl app:\n\nOTP-14824: Drop SSLv2-enabled clients\nOTP-14768: For security reasons no longer support 3-DES cipher suites by default\nOTP-14769: For security reasons RSA-key exchange cipher suites are no longer supported by default\nOTP-14789: The interoperability option to fallback to insecure renegotiation now has to be explicitly turned on.\nOTP-14882: Remove CHACHA20_POLY1305 ciphers form default cipher suite list\n\nSo those are the first items I'd recommend investigating (perhaps not the first one since RabbitMQ explicitly disables SSLv2 and SSLv3 for server sockets and I'd be very surprised if client sockets used any of those in practice).\nIf such questions become recurring our team will have no choice but to investigate but currently there's not enough information to work with (attempt to reproduce) anyway.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. RabbitMQ and its underlying runtime assume that hostnames (or IP addresses) are unique and can be changed. The runtime even provides a way to customise hostname resolution that doesn't require superuser privileges to manage. This is a pretty fundamental thing and it's not going to change any time soon.\nFWIW this is not a commonly reported issue by containerised environment users. That said, use a different tool if it works for you.. Also, nodes can use exactly the same sets of ports (and in most deployments, they do), which makes this idea\u2026 not universally applicable, at least.. Our team does not use GitHub issues for discussions but here's one last idea. The prefix in node name (the rabbit@ part) can change and that's the part that changes when more than one node runs on the same host (together with ports to avoid conflicts).\nOur team routinely runs multiple nodes on the same host during development (without any containers but it doesn't change much) by overriding node name and ports, e.g.\n```\nnode1, uses all defaults\nRABBITMQ_NODENAME=rabbit@mercurio ./sbin/rabbitmq-server\n```\nand\n```\nnode2 overrides node name and 2 ports\nRABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=hare@mercurio RABBITMQ_SERVER_START_ARGS=\"-rabbitmq_management listener [{port,15673}]\" ./sbin/rabbitmq-server\n```\nAnd that's it.. 3.7.x does verify if certificate and key files exist and the files are readable on start. The key isn't loaded and parsed by the TLS implementation until it is used and we are limited by what kind of errors it reports back and how sensible they are. We report all obvious issues discovered to the Erlang/OTP team, most recently  ERL-664 where blank or ill-formatted key files resulted in no errors.. In fact, the \"does not reply on open ssl port\" thing sounds a lot like ERL-664.. This keeps coming up so to clarify: node's inability to report a TLS certificate or private key file issue early comes down to the TLS implementation. A bug was discovered and reported to the Erlang team. A fix shipped in Erlang 21.1.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. The node fails to start a subprocess (due to an \"enoent\", so, \"no file entry\") that will register it with epmd. rabbit_epmd_monitor is not critical to a node's operation since nodes register with epmd anyway and the only reason why it exists is epmd being killed in certain conditions on Windows while the node is still running.\nThis is mailing list material.. Kernel/OS resource limits and security mechanisms can prevent nodes from starting subprocesses.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. This plugin's README mentions the need to first get familiar with RabbitMQ clustering fundamentals before learning how peer discovery works. There must be errors of some kind somewhere: we cannot possibly suggest anything without them.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. We don't have a whole lot of details to work with but when network interface changes, that's likely the effect of\n\nThe fact that it takes time to detect connection unavailability\nClients such as Java (my best guess this is on Android) will automatically recover from network failures by default.\n\nSee server logs for connection lifecycle events and other clues.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. I deleted the very long comment that contained the crash dump (which is way less useful than logs in over 90% of cases) and resulted in excessive scrolling in this issue.\nAccording to this package listing EPEL only provides Erlang/OTP R16B03. RabbitMQ 3.7.7 requires Erlang/OTP 19.3.6.4 or later. Our RPM installation guide recommends our own zero dependency Erlang RPM.. Curiously in practice the max number of atoms wasn't bumped (tested with 3.7.6 and 3.7.7 nodes) but the distribution buffer was.. @gerhard no worries. I introduced a typo while fixing this \ud83d\ude05 but luckily Chris spotted that, too \ud83e\udd47 . Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. RabbitMQ documentation is fairly extensive and cannot be put into a single man page. There's a dedicated Networking guide.\nPort 25672 is used specifically for inter-node and CLI tool communication and it makes no sense to limit it to localhost (same goes for the epmd port). If you do, you will be able to connect and use CLI tools against that node only from localhost which is not what RabbitMQ was designed for.\nOn an unrelated note, RabbitMQ 3.5.x has been out of any kind of support for a while now. Consider upgrading since you are tens of release behind.. Binding epmd to localhost is possible (it makes about as much sense as limiting a DNS server to localhost, however) but our docs do not really cover it since it's a part of the runtime.\nThere are pretty decent summaries available and epmd has doc guide of its own which has an access restriction section at the bottom.\nPlease direct all further questions to rabbitmq-users and erlang-questions.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. I highly respect the folks who maintain the RabbitMQ Docker image but their approach to configuration is terrible. Environment variables are used for everything, party like it's 1984. Do not follow their example. Nearly everything in RabbitMQ is configured using a config file. For Windows installer-specific things, see rabbitmq-users archives.\nErlang installer is maintained by the Erlang/OTP team.. Addressed in the Erlang client. As expected this had nothing to do with the server or even Shovel. URIs with port but not host should have been rejected as invalid (as in, make no sense in the context of Shovel) earlier and with a more reasonable error but weren't.. You can see that there's quite a number of functions that may be major contributors. I suggest this is moved to the mailing list for now.. Note that we explicitly mention in the docs that a single queue will become a bottleneck regardless of what function ends up being the most significant contributor.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. Thank you for reporting back.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. rabbitmq-users thread. http://www.rabbitmq.com/debian/ has been deprecated for a couple of years and is not mentioned in the Debian installation guide besides the deprecation note.. @fake-name that's unfortunate but things don't last forever in the land of software. You should perhaps check out the change log and Erlang compatibility docs every once in a few years.\nRabbitMQ 3.6.x (now EOL) supports 5 Erlang/OTP series (which have a lot of breaking changes between them). For 3.7.x we identified and announced breaking changes in OTP 21 before it went GA. There are few projects that are as proactive about future Erlang release compatibility.\nThere is a dedicated section on Erlang/OTP package pinning in the docs.\nWe would consider introducing an upper limit for versions but it will take a bit more convincing on the mailing list.\nFTR, this repo has nothing to do with packaging. rabbitmq-server-release.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. RabbitMQ does not keep crashing. \"Crash\" here refers to an unhandled exception. It happens in the node memory monitor because it tries to start a subprocess and that fails with an emfile (\"too many open files\"). This is a kernel limit that RabbitMQ does not control. See Open File Handle Limit in the docs. Production Checklist recommends 30K minimum vs. the default 1024 (on Linux).\nIn 3.7.7 it is not really necessary to use subprocesses in the memory monitor. The allocated strategy doesn't use any and is very close in precision to the rss one which your node seems to be configured to use.. Can we add some bats/shell script tests for this?. Please merge this if there's enough consensus on the general direction and\ncontinue on the same branch ;)\nOn Fri, Aug 17, 2018 at 3:35 PM, Luke Bakken notifications@github.com\nwrote:\n\n@lukebakken commented on this pull request.\nIn scripts/rabbitmq-env\nhttps://github.com/rabbitmq/rabbitmq-server/pull/1674#discussion_r210894684\n:\n\n\n\nit and warn the user.\n\n_rmq_env_saved_RABBITMQ_PID_FILE=\"$RABBITMQ_PID_FILE\"\n+\nif [ -n \"$_rmq_env_saved_RABBITMQ_PID_FILE\" ] && \\\n[ \"$_rmq_env_saved_RABBITMQ_PID_FILE\" != \"$RABBITMQ_PID_FILE\" ]; then\necho \"WARNING: RABBITMQ_PID_FILE was already set by the init script to:\" 1>&2\necho \"           $_rmq_env_saved_RABBITMQ_PID_FILE\" 1>&2\necho \"         The value set in rabbitmq-env.conf is ignored because it\" 1>&2\necho \"         would break the init script.\" 1>&2\n+\nRABBITMQ_PID_FILE=\"$_rmq_env_saved_RABBITMQ_PID_FILE\"\nfi\nunset _rmq_env_saved_RABBITMQ_PID_FILE\n+}\n+\n+_rmq_env_TODO()\n\n\nWe can merge this now, and then we'll be working on the same branch (which\nis A-OK), or I can get all the TODOs done and then merge. Just let me know.\n\u2014\nYou are receiving this because your review was requested.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/1674#discussion_r210894684,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAAEQq6PmAsvoK2SEMIQvHJGBpSagEqpks5uRriFgaJpZM4V-usd\n.\n\n\n-- \nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n. EXTERNAL is not an implemented mechanism (by definition RabbitMQ isn't involved), so the code that loads provided mechanisms will not include it. It's a good question whether it should be unconditionally included. Probably but it doesn't trip up any of the clients we maintain.. The EXTERNAL method turns out to be registered by the rabbitmq-auth-mechanism-ssl plugin.. This looks promising, thank you. We would definitely consider such compatible (within reason) improvements to the protocol and the headers exchange certainly could use some. We'll get to review this some time next week. Thanks a lot!. If bindings are stored in a separate table that's by definition a major breaking change that cannot go into 3.7.x.. In fact, if durable bindings of headers exchanges are not migrated with an upgrade function this PR cannot be accepted even for 3.8.. We are not going to close it. Simply add existing durable binding migration via an upgrade function that follows the one you've added and we'll continue with QA. It's perfectly fine for a PR to evolve a bit and go through several rounds of review. I will add a prefix our team usually uses for something that's known to be a WIP. Thanks.. @sylvainhubsch it should be a separate upgrade function that runs after the headers_bindings one. They way we test upgrade functions is usually\n\nStart a GA release node with data directory at a certain path\nStop it\nStart a node from source with gmake run-broker RABBITMQ_ALLOW_INPUT=1 RABBITMQ_MNESIA_DIR=\u2026 pointing at the same directory\nDrop into a REPL if you have to or observe the migration any other way\n\nWe have separate upgrade tests but they operate at the package level. We can extend them to declare some headers exchanges with bindings. I don't think they are open source since our pipeline code isn't.. @sylvainhubsch is this ready for another round of QA or are more changes coming?. @sylvainhubsch ping. Should we expect more changes or this is ready for QA? Thank you.. @sylvainhubsch no worries and thank you so much for your ongoing contributions!. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. This repository has nothing to do with Web MQTT. It has a separate one.. The assertion seems to be introduced in 7c76ca3a965d54197d51af339bbda5877c7b4974. The intent seems to be to only retry of there no newly elected master yet, at least that's my reading of it. In which case it shouldn't be an assertion violation if there is?. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Unlikely. Topic authorization and Exchange-to-exchange bindings should provide enough to simulate a headers exchange using a topic naming convention and with one more authorisation layer.. @lukebakken's hypothesis sounds plausible to me.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Items 1 and 2 contradict each other. If load is \"shared\" (distributed between) N consumers then by definition they are consuming and ordering between deliveries to those consumers is in part their responsibility.\nFor better or worse all 4 protocols RabbitMQ supports today use nearly identical models for delivery acknowledgements. Specific examples of what you have in mind are welcome on the mailing list.\nCurrently our team focuses on things other than extending protocols.. Merging per discussion with @dumbbell.. @hairyhum WDYT about backporting this to 3.7.x?. Leaving this test by @acogoluegnes here so that we don't have to dig in Pivotal Tracker for it. The test must output the sum number that matches the number of published messages and have a reasonable StdDev (e.g. on my machinee it's in the 180 to 360 range vs. over 4000 with RabbitMQ 3.7.7).\nAfter the test stops there should be no hash ring state entries left on the RabbitMQ node.\n``` groovy\n@Grab(group = 'com.rabbitmq', module = 'amqp-client', version = \"5.3.0\")\n@Grab(group = 'org.slf4j', module = 'slf4j-simple', version = '1.7.25')\n@Grab(group = 'org.apache.commons', module = 'commons-math3', version = '3.6.1')\nimport com.rabbitmq.client.Channel\nimport com.rabbitmq.client.Connection\nimport com.rabbitmq.client.ConnectionFactory\nimport org.apache.commons.math3.stat.descriptive.SummaryStatistics\nimport java.util.stream.IntStream\nprintln \"No binding in the middle\"\niterate(5, { consistentHashExchange(20, 1) });\nprintln \"Bindings in the middle\"\niterate(5, { consistentHashExchange(10, 2) });\nstatic void consistentHashExchange(int nbQueuesInEachIteration, int nbOfIterations) {\n    ConnectionFactory cf = new ConnectionFactory();\n    Connection c = cf.newConnection()\n    Channel ch = c.createChannel();\nString exchange = \"hash-exchange-test\";\nch.exchangeDeclare(exchange, \"x-consistent-hash\");\n\nList<String> queues = new ArrayList<>();\n\niterate(nbOfIterations, {\n    for (int i = 0; i < nbQueuesInEachIteration; i++) {\n        String queue = ch.queueDeclare().getQueue();\n        queues.add(queue);\n        ch.queueBind(queue, exchange, \"1\");\n    }\n\n    for (String queue : queues) {\n        ch.queuePurge(queue);\n    }\n\n    // focus on hash ring management here, so avoid concurrent bindings and publishes\n    // for now\n    Thread.sleep(1000);\n\n    List<String> rks = new ArrayList<>();\n    IntStream.range(0, 10_000).forEach({ i -> rks.add(UUID.randomUUID().toString()) });\n\n    int nbMessages = 100_000;\n    int count = 0;\n    ch.confirmSelect();\n    Set<String> rksUsed = new HashSet<>();\n    while (count < nbMessages) {\n        int remaining = count % rks.size();\n        String rk = rks.get(remaining);\n        rksUsed.add(rk);\n        ch.basicPublish(exchange, rk, null, \"\".getBytes());\n        count++;\n    }\n\n    ch.waitForConfirmsOrDie();\n})\nList<String> counts = new ArrayList<>();\nSummaryStatistics summaryStatistics = new SummaryStatistics();\nfor (String queue : queues) {\n    int messageCount = ch.queueDeclarePassive(queue).getMessageCount();\n    counts.add(messageCount + \"\");\n    summaryStatistics.addValue(Integer.valueOf(messageCount).doubleValue());\n}\n\nprintln String.join(\",\", counts)\nprintln summaryStatistics\n\nch.exchangeDelete(exchange);\nc.close()\n\n}\nstatic void iterate(int nbOfIterations, Closure action) throws Exception {\n    int count = 0;\n    while (count < nbOfIterations) {\n        action.call();\n        count++;\n    }\n}\n```. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. There has been a solution for connections to potentially changing IP addresses for a few decades now: hostnames and DNS A/AAAA records.. @alues it depends on the TTL for those records used on your network. It can be under a minute. We are not interested in reinventing parts of DNS in RabbitMQ, sorry.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. mnesia:system_info/1 is undefined. Your Erlang/OTP installation must be incomplete, e.g. on Debian and Ubuntu erlang-mnesia must be missing. We highly recommend using the recommended repositories for installing a compatible version of Erlang.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. Without a way to reproduce even 1% of the time this is mailing list material.. \"Mnesia overload\" is merely a warning and has no relation to the failed assertion during compaction.\nThis is not a discussion forum. If there's more details that can be provided (e.g. node data directory state at the time of the assertion failure), please post them to rabbitmq-users.. I'm not sure how common it is to enable plugins before starting the server and without --offline. rabbitmq_peer_discovery_common is not mean to be enabled directly either since it's just a dependency of specific peer discovery mechanisms.. Moved to the packaging repo, https://github.com/rabbitmq/rabbitmq-server-release/issues/85.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. We don't have much information to work with but our guess is that it may be one of a few obscure scenarios hopefully addressed by https://github.com/rabbitmq/rabbitmq-server/pull/1689.. See #1873.. https://github.com/rabbitmq/rabbitmq-server/pull/1691 addresses this and was discovered as part of https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/issues/37.. As part of https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/issues/37 we have added quite a few unit tests and there were at least 2 sets of client tests (here's the latest one) that we used to discover that bindings and ring state were not reset with queue deletion.\nSo give 3.7.8-rc.4 a shot.. epmd (which is not a part of RabbitMQ, by the way) documentation says:\n\u00abThe TCP/IP epmd daemon only keeps track of the Name (first) part of an Erlang node name. The Host part (whatever is after the @) is implicit in the node name where the epmd daemon was contacted, as is the IP address where the Erlang node can be reached\u2026\u00bb\n\u00abLets this instance of epmd listen only on the comma-separated list of IP addresses and on the loopback address (which is implicitly added to the list if it has not been specified). This can also be set using environment variable ERL_EPMD_ADDRESS\u2026\u00bb\nIn other words, epmd can bind to specific interfaces but hostname is taken from the OS in some cases and node names it is asked to resolved in others.\nThis can be mentioned briefly in this doc section, although we strongly prefer to not document parts of Erlang in detail.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. There are two ways to delete a message: consume and acknowledge it or purge the queue. You can also use a data store or a coordination service (ZooKeeper, etcd) to mark a message as \"deleted\" and consumers then can use this information to acknowledge them without processing.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. 504 Gateway Timeout is not a response RabbitMQ HTTP API can return. It's pretty clearly a message from an intermediary but that can be easily verified by enabling HTTP API request logging and inspecting the log.. On an unrelated note, there are no master nodes in RabbitMQ.. rabbitmq-users thread. This is partially blocked (depends on) by the Ranch 1.6.x backport to v3.7.x scheduled for next week.. @mohag good catch, thank you.. It\u2019s on pause and likely will remain so until the end of the month.\n\nOn 2 Nov 2018, at 00:19, Michael Dent notifications@github.com wrote:\nAre there any updates on this investigation or is it on hold for other priorities?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. The update is that our team is busy with other things.\n\nOver 80% (in one case, 95%) of time reported by microstate accounting is in the Sleep column. That's certainly something to investigate. I'd use rabbitmq-top to find top processes by the number of reductions. If there are no obvious outliers this is likely something non-deterministic in the runtime.\nOTP 21.2 change log mentions an issue where \"dirty schedulers could stay awake forever\".\n@essen have you seen something like this? Any advice as to how this can be narrowed down further, besides looking for reductions outliers?. Thanks for the metrics. We already have a way to reproduce. We will not be providing any advice here, this is what the mailing list is for.. Actually maybe we will \ud83d\ude06. The biggest contributor on both screenshots above is the queue metrics collector and the node has 2 CPU cores available to it. With 5K queue replicas on every node this is not particularly surprising to see.\nThe original environment used by @gerhard has several times more cores and fewer queues, and a lot less obvious metrics from msacc and other tools.. After discussing this with someone there are two major differences between the original report by @gerhard and the one by @JochenABC which I will cover below. In order to avoid this being a honeypot for all kinds of \"my node uses more CPU resources than I expected\" kind of question I've locked the issue.\nIn the original report\n\nThere are more CPU cores and fewer queues\nRuntime thread stats report a massive % spent in idle state\nBoth master and mirrors exhibit pretty different CPU usage rates\nNo data from rabbitmq-top but I somewhat doubt that metrics collectors are key contributors with 4-8 cores\n\nIn the 2nd report\n\nThere are only 2 cores for 5K queues with a replica on every node (so, 3x5K stateful things that emit stats)\nMetrics collector for queues is the biggest contributor \u2014 as expected, on an idle node it will be the only thing that does any meaningful amount of work\nCPU usage (number of reductions) goes up and then mostly plateaus after 2 mirrors are added \u2014 as expected because starting 5K mirrors and syncing them is a non-trivial spike in activity\n\nWhile there can be some overlap (e.g. mirrors use more resources than they should), there is nothing particularly surprising in the 2nd report but there is a whole lot of things that we don't fully understand (having tried a few known runtime scheduler settings).\nAny further feedback is welcome on the mailing list.. Here's a rabbitmq-users thread for the 2nd question. TL;DR: switching to +sbwt none (that's an Erlang VM flag) made a major difference.. @hairyhum we can append them in HTTP API handlers and rabbitmqctl list_bindings in CLI.. \u2026or functions that list bindings, that'd cover more cases with a single change?. This branch passes the test from #1691. LGTM.. #1721, a spin-off from this issue, was merged for 3.8.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. @bjoernHeneka you need to remove the 2007 signing key and import the new one. This is not a commonly reported issue and therefore I highly doubt the problem is with packages.\nPackage Cloud signs packages published to it with their own key.. See #1717. Bintray doesn't allow us to sign the repo (possibly because our account is OSS). The Debian repo on Package Cloud is signed and signatures of individual artifacts can be verified.\nWe've been working with JFrog/Bintray on resolving some of the pain points that RabbitMQ users have when using their service but the progress has been very slow. We have no ETA and can make no promises on their behalf.\nWe also have no intention of hosting our own apt repo again, so Package Cloud or dpkg -i installation (which means you are responsible for provisioning all package dependencies) are the most pragmatic options available right now.\nThis is 100% mailing list material => I'm locking this issue.. The repo is signed again now.. This repository has nothing to do with apt or signing keys or Bintray. Please move this to the mailing list.\nBintray blocked our account yesterday so all downloads from it failed. It can be a consequence of apt being unable to download a file or similar.. Actually, this is a known issue with Bintray apt repos for OSS accounts. @dumbbell can provide more details.\nOur Package Cloud repository is signed but PC uses its own key instead of our own.. Our team is working on making package revision publishing possible; so we might \"re-release\" to Debian and possibly RPM repos reasonably soon that would trigger reindexing.. The repo is signed again now.. In RabbitMQ's case we can also assume that there are very likely no other syslog user on the node, though, and override some of the defaults as we see fit.. We concluded that an \"external\" (issue-specific) test would be nice but since it's highly time-sensitive and requires a lot of iterations [to prove an absence of the problem] it should not be included into the integration suite. Perhaps property-based testing of key functions would be optimal. Let's merge this.. @hairyhum I don't see any reason to force -q, so it is also gone now.. So far I see two types of failures:\n\nGET /bindings/{vhost}/e/amq.default/q/{queue}/{key} returns 404\nSome tests expect names or resource names but seem to be getting full queue records/objects. This was backported to v3.7.x in a backwards compatible way plus converted to a feature flag for further mixed version cluster compatibility in master.. Mostly addressed in the PRs above. We have more optimizations in mind but they are moderately drastic and the team decided to forego them without additional evidence that they are really necessary.. @vrsbrazil there never was a leak. This was (and depending on the environment, is) event exchange's single consumer inability to keep up with the stream of events.. This team does not use GitHub issues for discussions, investigations and so on. They belong to the mailing list.\n\nWe also do not speculate if we can. There are tools that provide evidence of what consumes memory on a node. You have provided no evidence that it's rabbit_event or the event exchange plugin that are responsible for this.\nThere always will be an upper limit to what the event exchange consumer can do. Switching to multiple consumers would require a significant redesign that cannot go into a patch release. We don't see enough demand for that work compared to other major changes.. From our experience it's almost always high connection or channel churn that produces a lot of internal events. Reduce the churn or don't use the event exchange plugin if it cannot keep up in your environment. Again, collecting data e.g. with rabbitmq-top is highly recommended over guessing.. Per discussion with @dumbbell, our only option is to silence the warning/ignore it. As long as the build works, this obviously has a very low priority but we'd try to account for it next time we have to do a round of makefile updates.. @lukebakken I merged master into this branch so cherry-picking the merge commit to v3.7.x might introduce a 3.8.0-specific change \ud83d\ude13. Sorry. Cherry picking individual commits should be safe. . AMQP 1.0 plugin relies on the acceptors/listeners started in the core. STOMP, MQTT and their WebSocket counterparts needed updating.. A very similar question filed as an issue for a different repo: https://github.com/rabbitmq/rabbitmq-management/issues/617. When in doubt, please post to the mailing list.. Thank you. I definitely think it should be added to rabbitmq-plugins.bat. rabbitmq-diagnostics.bat is less commonly used and almost always interactively but it's probably worth having there, too, as it can be involved in automation as well.. @ar7z1 just amend this PR. Thank you!. By \"amend\" I mean \"push more changes to the same branch\", not git commit --amend ;). No worries. The change will ship in 3.7.9 \ud83d\udc4d.. Thank you!. Can we take a step back and explain why the current behavior has to change?. @noxdafox under what circumstances did the client not receive a nack? The commit message doesn't explain that. Why is this a good idea to nack such messages? What kind of applications can be affected?. So the context is in https://github.com/noxdafox/rabbitmq-message-deduplication/issues/21. I'm not convinced this is what RabbitMQ users who don't use that plugin want.. The backing queue behavior docs do not suggest if duplicates should be rejected but it does mention that the caller might consider treating the message as \"dropped\".\nrabbit_variable_queue:is_duplicate/2 always returns false and rabbit_priority_queue:is_duplicate/2 passes through to the next BQ in the chain (e.g. the variable one).\nIt gets a lot more interesting in the classic mirrored queue implementation. Mirrors can see a message more than once due to how the gm module (the \"transport\" and current \"consensus\" implementation) works and handlers failures.\nA duplicate in that case is not a reason to unconditionally communicate a nack to the client. In fact,\nthe is_duplicate/2 implementation there lists several cases in which the \"caller\" may or may not have sufficient information to decide on when to send confirms.\nSo this would be a breaking and surprising change to those who use classic mirrored queues: they would get false negatives.. We'd welcome a different PR for 3.8 (master) that would make it possible to address https://github.com/noxdafox/rabbitmq-message-deduplication/issues/21 without breaking changes.\nI can't immediately think of a way to do this for mirrored queues but perhaps with quorum queues, when they are merged, there can be a way.. @noxdafox I can see why the current behavior in amqqueue_process is problematic for your plugin.\nAs @kjnilsson and I concluded in a 1-on-1 conversation, the callback was originally added to introduce some of the idiosyncrasies and flaws in the gm module and thus classic mirrored queues, not to actually let plugins deduplicate :(. @noxdafox we can make any internal interface changes for 3.8.0. As long as on disk data and client protocol are not affected, we'd consider it ;). We already have an exclusive consumer feature (in AMQP 0-9-1). The naming has to change to indicate the difference explained in #1743.\nx-single-active-consumer is the best name I can think of.. @acogoluegnes I agree with @hairyhum. Plus we now have conflicts due to the QQ merge.. Superseded by #1802.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. There is no support for either of those in the core in 3.7.x. 3.8 is going to include a redelivery counter for quorum queues. There's also a delaying exchange type (not a direct equivalent but somewhat close).. This and related PRs have been backported to v3.7.x.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. rabbitmq-collect-env is in a separate repo.\nMonitoring and inspecting system logs should be your next steps. There is not much else in the except for\n\nA lot of connections being opened and very few closed (but some were closed unexpectedly, so chances are applications terminated as well)\nAn erl_child_setup closed message which means a subprocess unexpectedly failed\n\nThere's a good chance this is nothing other than the out of memory killer or similar (there are also no alarms in the logs, however).. Your Erlang build has HiPE enabled but not kernel polling. That's an unusual combination, at least for Linux. Consider using OTP 21 (where polling cannot be disabled since the I/O subsystem is significantly different and assumes kernel polling API availability) and without HiPE. HiPE has routinely caused obscure runtime segfaults in the pre-18.x days, for example.. That's a good point. If HiPE is enabled for RabbitMQ it will also incur a several minute long startup penalty which is taken by compiling most RabbitMQ modules with HiPE.\nLet's continue on rabbitmq-users.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. The only repeated exception in the logs looks like https://github.com/rabbitmq/rabbitmq-server/issues/1538. That cannot affect all but maybe 2 rarely used HTTP API operations.\nThere are known Erlang bugs that can prevent activity on all TCP sockets but 20.3.x should not be affected. The node can also be out of file handles, which would prevent all new connections from being accepted.\nPlease upgrade to 3.7.8 and OTP 20.3.8 or 21.1 first, make sure you have monitoring in place and start a mailing list thread if you find any more clues with 3.7.8.. Does the shared escript have enough context to determine its RABBITMQ_NAME_TYPE? If so it can be moved there.. So most of this can be moved to Elixir. Let's do that then.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. This has been discussed on the mailing list multiple times. Keywords: hot standby, topology export and import, exchange federation, message TTL.. rabbitmq-users thread. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. According to the log, JSON parser threw an exception but there are no details to work with. Please upgrade to 3.7.8 and if that still can be reproduced, take a traffic capture and post it to rabbitmq-users. Thanks.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. There is no evidence of any issues with your data or that RabbitMQ crashed. A crash in runtime terms is an unhandled exception. A channel failed to shut down before timeout.\nets:lookup/2 failures that report an badarg in practice mean that a node is out of file handles. In case you monitor file handle usage \u2014 as one should in production \u2014 see if the usage is anywhere near the limit.. Could reproduce with 3.7.8 and master without the patch, plus https://github.com/rabbitmq/rabbitmq-management/pull/626 fails without it. All server suites pass \ud83d\udc4d.. We certainly wouldn't mind community contributions. Thank you for considering one \ud83d\udc4d My first question around new package types is always \"who is going to maintain this going forward?\" since our team is small.\nThis is something that's best discussed on the mailing list. You might find potential contributors there and this is where new projects are best announced.\nIf there's enough interest over time it is something that can be added to our release pipeline.. We will restage the app. Please post your observations about the self-service app to the mailing list and not GitHub issues.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. A message is published when its properties (Content Header in the traffic dump) frame and then 0 or more message payload frames are, which happens 41ms after the basic.publish frame according to Wireshark. So this is a matter of client implementation (specifically framing) and client-side TCP stack settings. It should be fairly easy to compare by just switching to e.g. the Java client/PerfTest.\nYou can find similar questions on rabbitmq-users in the past where client latency wasn't the primary contributing factor. In those cases runtime scheduler flags usually made most difference.. More findings previously posted to https://github.com/gmr/rabbitpy/issues/116:\nYour code opens N channels before publishing concurrently and this client has a per-connection lock around channel.open operations. So all threads in your code contend on that lock and possibly more (I couldn't quickly find where published message framing happens: clients often use a per-connection lock for serialised outgoing frames as well).\nPre-allocating channels will avoid contention on the channel management lock.. We have already provided some feedback on the observations and the workload being tested. The rest of this discussion belongs to the mailing list.. @kjnilsson the function was renamed. @dcorbacho this now needs conflict resolution with master.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. The node in question has a lot more processes, which when combined with the memory usage breakdown suggests it hosts masters for many more queues than other nodes. It can be easily verified via management UI or HTTP API (queue listing includes queue master's hosting node information). rabbitmq-users thread. Thank you, this is an interesting find.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. There's a window of time in which a publisher confirm can be lost (never make it to the client) due to TCP connection failure even though the message was successfully routed. Publishers must treat all unacknowledged messages as requiring republishing. That means there's a window of time and certain failure scenarios which can produce duplicates. If it's important in a given system consumers should be developed with that in mind.\nA \"confirm for confirms\" mechanism (3 phase commit and such) or a full blown consensus algorithm between clients and nodes would help address that but it would also introduce significant overhead and FWIW the messaging protocols we implement today use a basic \"publish and confirm\" algorithm.. This looks good after a quick review. Have you done any long-running benchmark to assess the impact of this, in particular on priority queues? Your changes seem roughly equivalent but this is a pretty sensitive code path.\nThank you for your ongoing contributions!. I'm not sure what message that is but it can be discussed in a separate management plugin PR.. @noxdafox thank you, that sounds like a good start. I will QA this and then test a few workflows we use on our team.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Installing the package starts the server. Node's data directory must exist and have the correct permissions by then but your code sets up the directory at a later point.. @lukebakken . Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. The plugins guide explains how to check plugin status.\nAccording to the message (it's not clear where it is coming from but looks like it's the client), the header value cannot be of type uint16. The plugin supports several numeric types defined in the protocol. I'd try a different numerical Go type first.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. I don't see any evidence of causation between those events, or evidence of a bug in Erlang or elsewhere. \nTLS alerts happen on individual connections and cannot affect RabbitMQ. As far as I know besides a certificate cache, there is next to no shared state in the TLS implementation in OTP.\nMore importantly, the logs themselves demonstrate that\n\nOne side detects irregularities on a connection that make it impossible for TLS (well, ASN.1) parser to continue\nThe same side reports a partial partition\nThe other end repots the same inability of the TLS state machine/parser to continue\n\nSo from the little information there is, this is a genuine short lived, possibly asymmetric network partition that also happened to trip up TLS. If TLS is used for inter-node communication this makes perfect sense that those events will appear together in the logs. You haven't mentioned if that was the case.\nIf you run two nodes with partition handling mode pause_minority, add another node. There is no minority to select with 2 nodes when they lose connectivity. Future RabbitMQ releases will require at least 3 nodes if certain consensus-dependent features are used.. Looking at https://bugs.erlang.org/browse/ERL-770, I suspect that inter-node communication is encrypted using TLS. In which case any TLS alert likely closes the connection (I'm not sure if there are other ways to continue), which nodes detect as peer disconnection and therefore act on it as they would if it was abrupt.\nWhat causes the alert, I don't know but RabbitMQ does not implement TLS or the TLS-encrypted inter-node communication part.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. RabbitMQ 3.1.3 is over 5 years old. I'm sorry but we wouldn't investigate this even if we had enough details. Please upgrade first. You will have to upgrade Erlang as well.\nPublishing messages as persistent is one thing. They have to be routed to durable queues. They might have had TTL or the queues did. There are too many unknowns to provide even the most basic guidance.. I don't see any evidence of messages being published as persistent by this code but even more importantly, your snippet has a glaring omission: it closes the connection without using publisher confirms to ensure successful handling of the publishes. Fire-and-forget publishing doesn't carry a lot of guarantees.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Backported to v3.7.x.. @sylvainhubsch we really appreciate the amount of time you are putting into optimizing the headers exchange (or finding an optimal alternative). We historically have chosen to develop plugins for new exchanges instead of bundling them with RabbitMQ. I think it's the right thing to do here, too.\nI don't know if x-open is a very descriptive name but I also don't have any suggestions right now. Please wrap this into a plugin and our team would be happy to take a look. Note that it's currently a bit hard to do without at least some high level test cases.. Note that as a plugin this exchange can\n\nTarget RabbitMQ 3.7.x\nEvolve independently and at the pace you choose both in the near and long term\nBe evaluated by the community and battle tested by your own systems with very little risk (compared to trying to get it into RabbitMQ core)\n\nSo I think it's in everyone's interest right now to proceed with this being a separate plugin.. Backported to v3.7.x.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. This questions can be found in the mailing list archives, e.g. this thread. You have two options (that can be combined):\n\nUse different runtime allocator flags (RabbitMQ does not allocate memory directly)\nTry re-enabling background GC (won't make a different for all workloads). In this particular case the node doesn't seem to have any connections open, so you can simply restart it.\n\nAlso, rabbitmq-users has archive threads that discuss memory fragmentation and how allocators affect it.. Addressed in #1807.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. Generic binary builds should be used when running from arbitrary installation locations. Package-provisioned versions can override default paths, including runtime code path (where the modules are loaded from).\nFWIW I use binary builds at non-standard locations many times a day and do not run into this.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. I'm afraid I don't know what \"timing delivery\" means to you. Consider starting a mailing list thread if there's something you'd like to see. There's already a delayed publishing plugin, for example.. This is not something on the immediate roadmap but can be achieved with a plugin.\nThere\u2019s interest in making the delayed exchange plugin distributed, this may be just\nanother feature eventually offered by it. There is no ETA and this is not a delivery promise.\n\nOn 29 Dec 2018, at 09:56, athinboy notifications@github.com wrote:\n@michaelklishin\nnot delay delivery . delivery on specific time , \u5b9a\u65f6\u6d88\u606f.\nsorry ,I can not access google .\nhttps://help.aliyun.com/document_detail/43349.html\nhttp://rocketmq.apache.org/docs/schedule-example/\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Even then, why not make it a constant in the channel state and eliminate several new extra calls that would effectively always return the same value?. I understand but it is unlikely to be a common event. We also can claim that the updated value (e.g. via rabbitmqctl eval will only take effect for new channels. That's how things work for a few configurable settings already.. Didn't mean to close this, it's been a casualty of the Spring PR bot fallout.. Fair enough, quorum queues make little sense for transient data, so this is acceptable.. At the very least the code that force deletes should handle down nodes?. @kjnilsson I strongly oppose \"more manual intervention required\" kind of solutions for such a basic operation. How hard is deleting a queue?. We concluded that another test is needed here on my end since the goal of this PR is to avoid manual intervention.. rabbitmq-message-timestamp does just that.\n\nThere is a metric on how old the head of the queue is as of #54, it doesn't work all that well in practice\nand may or may not survive a major message store redesign.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. According to the stack trace the command returned an {:error, :badarg} which is not an error that's formatted. There really isn't much to work with. See server logs of all nodes involved for clues. Don't forget to reset and stop_app on the joining node. This is mailing list material at this point.. Management plugin suite fails because https://github.com/rabbitmq/rabbitmq-server/commit/76acfa47cc7da7c1b9072747a0a2bb14b51d3094 wasn't backported for backwards compatibility. We have to do a bit more duplicate filtering in rabbit_binding:list_for_destination/1.. Merged per discussion with @kjnilsson. There will be another leg of the length limit work, so any new findings in this area can still be addressed.. This question comes up nearly every month so we are considering handling more connection exceptions explicitly to avoid logging connection state. That state can be very useful when troubleshooting, though, in fact, in some cases it's the only piece of information we have to work with.. Always copying the schema sounds reasonable. I don't see any major risks with that.. @mohag schema files also come from plugins, so your understanding is basically correct.. @hairyhum has most context here. I think copying the file and overriding the one that's already there is what most people would expect: we want to have the latest schema file. The overhead of this is negligible.. This is expected. Exchanges are just names for routing tables and E2E routing simply \"expands the table\" before routing actually happens.. Backported to v3.7.x.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. RabbitMQ does not implement TLS, Erlang/OTP does. RabbitMQ TLS guide reflects what versions are known to be fully supported.\nTLS 1.3 development started in January 2018 and has been active in the last few months. I don't know what version of Erlang is going to ship TLSv1.3 support, chances are it will be 22.0 this summer.\nThe earliest it can be is 21.3 but I doubt it.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. A node that's shutting down will close all of its connections. This is in no way coordinated with queue replica promotion and should not be. Queue replica promotion should in no way depend on client connections. In fact, because of properties such as exclusive and auto-delete, closing connections first makes more sense because some queues may be imminent for deletion (why would anyone use auto-delete mirrored queues is a separate discussion).. I responded in https://groups.google.com/forum/#!topic/rabbitmq-users/0OVgPKKY1jU.. Backported to v3.7.x.. Cherry-picked to v3.7.x.. The \"no guarantee\" part means that you are invoking an API that's not intended to be used by either users or operators. Any client-facing code paths are updated to use the API signature available in a specific version. If you use those using eval (highly discouraged), it's up to you to determine what arguments must be passed and why. I don't see a single reason to mess with that over using rabbitmqadmin's public API (commands).. And for the truly curious, the function above requires an extra argument as of 3.7.0: the acting user.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. I'm not sure I understand the intent. If it is \"how to remove a cluster node\", see this doc section.\nThere are error messages at several steps in the above example. Some of them suggest to me that\nthe operator may or may not understand what they say or what certain commands really do.\nA node that expects a peer to be there fails to sync from it, force_boot is involved (likely to address that), then forget_cluster_node is used but with the --offline flag. This needs clarification and is mailing list material.. Most likely what you want is to keep only one node which wasn't the last to shutdown. Then\n\nStop it\nRemove its peer with remove_cluster_node --offline\nUse force_boot if needed (it failed in your example because the app was already running and trying to start), in which case remove_cluster_node can be used in the standard (online) mode\n\nRestarting Cluster Nodes is very relevant here and explains how nodes behave when they rejoin a cluster, try to contact peers and how that fits into mass restarts (e.g. redeployments).. rabbitmq-users thread. Backported to v3.7.x.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Since Erlang doesn't have an application to add in the UI, an exception must be added using /usr/libexec/ApplicationFirewall/socketfilterfw --add /path/to/bin/erl.. man socketfilterfw is scarce on details but looks like to whitelist an executable it has to be added then unblocked:\n`/usr/libexec/ApplicationFirewall/socketfilterfw --add /path/to/bin/erl`\n`/usr/libexec/ApplicationFirewall/socketfilterfw --unblock /path/to/bin/erl`\nSome threads suggest that if a service binds to localhost only, its traffic is always allowed. See RabbitMQ Networking guide to find out how to do that.. If rabbit_log isn't suitable we can extend it. I doubt we really need more logging modules.. I see. I'd say let's make it a new OTP logger shim not specific to Ra. How about rabbit_otp_logger in rabbitmq-common?. We can consider a separate category. My point is that all logging modules should probably be colocated.. According to the port used and message, this has to do with STOMP over WebSockets, not just STOMP, and so almost certainly there's a Web app involved. Our docs only really cover CORS configuration for the HTTP API but it should be possible to configure the same headers for any HTTP-enabled listener. Please post a small example Web app that demonstrates what's going on to the lsit.. Cherry-picked to v3.7.x.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. According to the stacktrace, the node failed to open a subprocess repeatedly in a connection process, with a rate high enough to cause certain parts responsible for MQTT connection acceptance to shut down. The most common reason for that is when the node is runs out of file descriptors.\nWe'd be interested in learning of a way to reproduce and increase the expected rate on the mailing list.. I don't see where recent MQTT plugin code can be responsible for opening new [Erlang] ports (subprocesses) and it definitely cannot affect management UI in any way. So both likely have the same root cause and running out of file descriptors will affect management UI and any other listener/connecting client.. #1863 (also redirected to the list) confirms that the node indeed runs out of file descriptors.. FTR, the port references above refer to sockets and come from rabbit_net. We are trying to reproduce connection supervisor termination when the node is exhausted of file descriptors.. Further investigation concluded that\n\nWith 100 and 500 exceptions per second but enough file descriptor new clients can successfully connect\nThe supervisor in question, rabbit_mqtt_connection_sup, is per connection (meaning that there's one for every MQTT client connection), so its exit is expected and not going to affect the plugin's ability to accept more connections.\n. File descriptors of a node can be inspected with lsof -p [pid] on Linux and handle on Windows.\n\nIf the latter is available, RabbitMQ will report file and socket handles separately in the management UI.\nThere is no evidence that RabbitMQ itself leaks file descriptors. It is extremely likely that one of the applications leaks connections or messages pile up, meaning the message store has to use more and more segment files.\nCollecting more data about the node(s) that exhibit this behavior will lead you to an answer.\nConsider at least providing RabbitMQ version information when you ask for help, too.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Reasoning About Memory Usage will help you collect relevant data (please don't do it with a version that's 18 releases behind, as Carl suggested).\nOn disk data has to be transferred to a remote node and with some exceptions where sendfile(2) can help, this requires loading at least some data into memory. Indexes that can have embedded message content for small messages are always stored in memory as well as on disk.\nSync batch size can be reduced and lastly, automatic sync can be disabled entirely, so can message embedding.\nThis is mailing list material.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. Please post this and future questions to the mailing list, as GitHub issue template suggests. 3.6.9 is 18 releases behind and has been out of support since May 2018. Very importantly here, starting with 3.6.11 nodes use a different memory reporting algorithm that includes memory allocated but currently unused (possibly due to memory fragmentation) by the runtime. Memory fragmentation is a pretty involved and workloads-specific topic, see e.g. one strategy to reduce it.\nWe have no code or workload details but according to the memory breakdown, 78% of memory is used by queue master processes, which suggests they kept a non-trivial number of messages in RAM previously and in current state there are no triggers for runtime GC. Modern versions force a minor collection every 1000 messages by default.\nYou can either publish messages as persistent and use durable queues only to make them go to disk or use lazy queues that sent all messages to disk as aggressively as possible. In environments with mostly idle queues re-enabling background GC\nPlease conduct all future tests with a supported, recently released version such as 3.7.11, which will require upgrading Erlang from 19.0 as well.. I deleted the now outdated comment, thank you, @essen.. This passes all core and plugin test suites I tried (about 10 of them) \ud83d\udc4d. #1913 moves listener startup later. I still does not coordinate with every other component; that would be a major refactoring task (and a fairly high risk one since there's a fair number of components and plugins that initialise concurrently).\nI considered adding a \"sleep\" boot step but expect it to be ill received by other team members :P. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes that we have a certain amount of information to work with.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving and sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. OS/systemd logs, a traffic capture, deployment tool logs, etc\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have a complete enough understanding of what's going on, a recommendation will be provided or a new issues with more context will be filed.\nThank you.. I'm filing a new umbrella issue for similar threads even though we never managed to reproduce it, even with steps that supposedly \"do it 100% of the time\".. Also note that default exchange routing is a special case and will be even more explicitly so as of #1833 and friends. There will no longer be any explicit bindings to the default exchange as they are not necessary and only consume resources.. Duplicate of #1873.. The removal of explicit default exchange bindings is meant to be backward compatible. They are already gone in 3.8.0-beta.2, we can add a test with default exchange being set to AE.\nWe already explained many times on the list that shutting down all nodes concurrently (at the same time) is not a scenario we consider realistic and plan dedicating time to. Non-replicated parts of the system (e.g. Shovels) cannot deterministically work out which node they should migrate to when all of them are going down concurrently. Stopping nodes one by one is a different story.\nRaft-based features in 3.8 will require a quorum of nodes to be online and in some cases, extra steps\nfor the operator to take to indicate that a system now has one extra node that certain quorum queues should use as a member or that a certain node is about to be removed for good. https://github.com/rabbitmq/rabbitmq-cli/issues/286, https://github.com/rabbitmq/rabbitmq-cli/issues/287 are two examples. It is easy to see how a simultaneous node shutdown is a scenario where even a robust and predictable consensus protocol cannot guarantee much.. @Avivsalem you can export definitions in the environment that you use to reproduce and send it to me privately (mklishin at Pivotal domain).. @Avivsalem thanks, we have a PR under review that may help with this specific case. If you let us know what package type you use we can produce a \"pre-merge\" build for you to try.. We wouldn't ask you to test it in production. Is there really no other way?. Please take this to the mailing list.. #1872 mentions that one way of reproducing is to stop RabbitMQ on all cluster nodes at the same time. While this is not a scenario we find realistic and \"solvable\", it can be tried to gain improved understanding.. Not scheduling for any milestone because we are not confident that #1878/#1879 address every reported case.. We introduced 3 different changes that should at least narrow this issue down significantly (in one case I was able to reproduce the report by wiping one of the internal tables from a node's REPL): #1878 (#1879 is a backport to v3.7.x, #1884 (will be backported to v3.7.x soon) and previously #1721 (#1833 is a backport to `v3.7.x) which has other benefits.\nCurrently they are all scheduled to ship with 3.7.13. This issue will remain open for some time, we'd like to see some more real world anecdotal evidence of their effectiveness.. Also, #1884 review notes contain a synthetic way of reproducing.. Doh, my comments have been sitting unsubmitted for like 40 minutes \ud83e\udd26\u200d\u2642\ufe0f. Sorry.. Because rabbitmq-components.mk must be in sync in every repo. Travis does not handle multi-repo changes well (was never designed for such projects).. @jsoref I will QA things when they are ready. The rabbitmq-common PR is looking good. As long as we leave out any licensing file changes I can test and correct the rest on your behalf. Thank you.. @jsoref hit Comment too soon. Let me know when you are happy with the PRs so that I continue.. These two and the Erlang client and the only \"special\" ones (other plugins depend on them, they have some interdependencies as well). Once they are done the rest can be done in more or less any order. Perhaps MQTT and STOMP should be updated before Web MQTT and Web STOMP.. @Ayanda-D we don't yet know and would not claim it is sufficient because there was no reliable way to reproduce any of those reports :(. Setting milestone on the PR here because we are not yet ready to consider #1873 resolved (and thus can't know what milestone it should use).. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. @Gsantomaggio please submit all future PRs against master.. @hairyhum what does \"which subscribe to binding actions\" mean? If I manually remove a rabbit_route row and then try queue.{bind,unbind} on the exchange the row belonged to, would this be close enough?. Managed to reproduce the NOT_FOUND error with the following synthetic test on a 3.7.12 node and in master at the time of writing:\n\nDeclare a few durable queues\nBind them to amq.fanout since default exchange bindings are now implicit and leave route rows\nIn the running node REPL, inspect routes with ets:tab2list(rabbit_durable_route). and ets:tab2list(rabbit_route).\nWipe all objects in rabbit_route with ets:delete_all_objects(rabbit_route).\nInspect both tables again\nTry to bind the above queues to amq.fanout again\nObserve a channel exception that says NOT_FOUND - no binding  between exchange 'amq.fanout' in vhost '/' and queue 'q.durable.0' in vhost '/'\n\nWith the same test on this branch I observe no channel exceptions and all binding rows for the queues in question in rabbit_route are reinstated \ud83d\udc4d. I observed no binding table inconsistencies or issues of any kind with subsequent queue operations.. Conducted the above test with non-durable queues (and thus semi-durable bindings) and the outcome is the same \ud83d\udc4d.. There is a concern that this can affect stateful exchange types, namely rabbitmq_consistent_hash_exchange. That exchange only allows one binding between a queue and exchange starting with https://github.com/rabbitmq/rabbitmq-consistent-hash-exchange/issues/37 and that's the right direction for it anyway.\nI could not identify any regressions and the above tests look promising in addressing #1873 at least in part.. Backported to v3.7.x.. Setting milestone on the PR here because we are not yet ready to consider #1873 resolved (and thus can't know what milestone it should use).. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. So with this change each channel is aware of what connection type (\"source\") spawned it and if it's AMQP 0-9-1/AMQP 1.0, it avoids a call to the connection to get information that would result in a silent exception and a channel process restart. Which was on the hot code path.\nThanks, I can imagine it was a fun thing to investigate. We also identified a few side effects from this in #1539.\nI wonder if channels should be provided connection info on start since the info items relevant to  topic authorization (the variables) are immutable, e.g. vhost and client_id do not change.. @Ayanda-D can the same thing be refactored to use channel state instead of process dictionary with a follow-up notification?. @Ayanda-D this is node-local state so we are significantly less constrained with breaking changes. \nrabbit_channel:handle_method/5 seems to be exposed for HTTP API calls only: is it really in scope for this change? Can make it work since we know that there is no long-lived connection in that case anyway? By the way, it's also a function that I think is never invoked by a remote node but management plugin may need updating.. @Ayanda-D thank you. Let us know when you think this is ready for another round :). I reproduced the exception, and it only happens when a topic exchange is used.. Backported to v3.7.x. @Ayanda-D please double check that branch, there was a fair number of conflicts.. @essen you and Dialyzer indeed discovered a bug in #1082. https://github.com/rabbitmq/rabbitmq-server/commit/7b1f550c17d9cd7da4dc8394df7941bd7dd9fd1b#diff-be8625a78839f910ad12827007d277caR114 introduced a new element to the consumer tuple but https://github.com/rabbitmq/rabbitmq-server/commit/7e3aae8c1af08775dd3094c17aadfd380a41e383 didn't update the match pattern.\nSuch silent list comprehension failures in Erlang have bitten us in the past \ud83d\udc4d . Note that the silent failure to invoke emit_consumer_deleted/4 caused some internal events to not be emitted but otherwise was harmless as far as I can tell.. We submit releases regularly but they can take months to go through approval (for example, last week 3.7.7 was approved. It was released in July 2018).. @rgl yes and no. Submissions to NuGet don't require committing or pushing to GitHub. We will submit 3.7.12 next week but as I said above, you may get it months from now. The bottleneck in the NuGet release process is not submission automation from our side.. 3.7.12 was approved by a reviewer. It doesn't support Erlang 21 as a dependency but 3.7.13 will.. java.net.SocketException: Connection timed out (Write failed) is pretty descriptive. See Troubleshooting Networking.. Thank you for the heads-up. I suggest that you post this to rabbitmq-users, our public mailing list.. Let's wait for a bit and perhaps backport after 3.7.13 GA ships next week.. Good job, @hairyhum \ud83d\udc4d\ud83d\udc4d . References #1513, #1690, #1589, #1715.. 3.7.x feels most appropriate before we backport.. Backported to v3.7.x.. > A lazy queue stores messages only on-disk but requires an Erlang process which does consume memory and by default has a limit.\nA queue is a stateful entity thanks to how protocols we support were designed. That state has to be stored somewhere, and even if that somewhere is \"on disk\", it can only be done by a process. While a different design could use fewer processes for a given number of queues, I don't see how it could be 0,\nespecially if you also want to have metrics about queue activity. Queue process pooling was discussed at some point but it's not in the first 10 of priorities at the moment.. With Node.js the common way of achieving concurrency is fork(2), which requires that a new connection is opened from each child process because of how file descriptors are [not] inherited.\nIn any case, there is nothing wrong with using more than one connection per process, e.g. one to publish and one to consume is a common strategy to avoid consumer connections being blocked.\nThe rule of thumb is \"when in doubt, open a new channel, not a new connection\". Channels are also not free and apps should not open a lot of them but what \"a lot\" means is system-specific. Modern RabbitMQ versions have a hard limit of 2047 (plus one special channel used for protocol negotiation and error reporting) channels per connection by default, and even that is very high in our opinion. 1-10 channel should be enough for most applications.\nRelevant doc guides: Connections, Channels.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team).\nWe get at least a dozen of questions through various venues every single day, often light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because GitHub is a tool our team uses heavily nearly every day, the signal/noise ratio of issues is something we care about a lot.\nPlease post this to rabbitmq-users.\nThank you.. I'm pretty certain that the queue never had any consumers. An auto-delete queue that never had a consumer will not be deleted.\nIf you have an executable way to reproduce, please start a mailing list thread.. rabbitmq-event-exchange allows applications to monitor connection, channel, queue and consumer events. Since it require using a regular RabbitMQ queue and consumers, connection recovery should be tested by closing [other]  connections with rabbitmqctl or HTTP API. Stopping and starting the node won't work very well.. I start a node with gmake run-broker, that's it. No custom config file involved.. It works as expected with an explicitly provided config. Could be something specific to my environment.. With this PR I suggest that https://github.com/rabbitmq/rabbitmq-common/commit/c04944235b8ea477735b78d3ed2d7138a93d9343 is reverted.. I backed out a couple of unrelated boot step ordering changes after seeing how they can potentially be unsafe. I no longer can reproduce one curious startup race condition after that. Let's get this through the pipeline as all the tests I tried locally pass.. @thedrow short strings are supported in attribute tables, just were not expected by the authentication mechanism. https://github.com/thedrow/rabbitmq-server/commit/5e97011b9869a0ff579290df6e3bf8ab315b5c6e looks reasonable. Please submit a PR?. No worries. You thinking is on point, we are fine with tweaking the small details on your behalf.\nCan you please post a script that we can use to reproduce?. I tried the following:\n\nClone py-amqp\nSwitch to the branch hypothesis\ntox -e 3.7-integration-rabbitmq -- -k test_connect\n\nIt outputs\n```\ncollecting ...\n t/integration/test_integration.py::test_connection.test_connect \u2713                                                                                                                                                                                                10% \u2588\n t/integration/test_integration.py::test_connection.test_connect_no_capabilities \u2713                                                                                                                                                                                20% \u2588\u2588\n t/integration/test_integration.py::test_connection.test_connect_missing_capabilities \u2713                                                                                                                                                                           30% \u2588\u2588\u2588\n t/integration/test_integration.py::test_connection.test_connection_close \u2713                                                                                                                                                                                       40% \u2588\u2588\u2588\u2588\n t/integration/test_integration.py::test_connection.test_connection_closed_by_broker \u2713                                                                                                                                                                            50% \u2588\u2588\u2588\u2588\u2588\n t/integration/test_integration.py::test_channel.test_connection_methods[method0-_on_blocked] \u2713                                                                                                                                                                   60% \u2588\u2588\u2588\u2588\u2588\u2588\n t/integration/test_integration.py::test_channel.test_connection_methods[method1-_on_unblocked] \u2713                                                                                                                                                                 70% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\n t/integration/test_integration.py::test_channel.test_connection_methods[method2-_on_secure] \u2713                                                                                                                                                                    80% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n t/integration/test_integration.py::test_channel.test_connection_methods[method3-_on_close_ok] \u2713                                                                                                                                                                  90% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\n t/integration/test_rmq.py::test_connect \u2713                                                                                                                                                                                                                       100% \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\nResults (0.34s):\n      10 passed\n      31 deselected\n3.7-integration-rabbitmq docker: remove 'ed1914501a' (forced)\n3.7-integration-rabbitmq run-test-post: commands[0] | ./rabbitmq_logs.sh\nERROR: InvocationError for command could not find executable './rabbitmq_logs.sh'\n___________________ summary ______________________\nERROR:   3.7-integration-rabbitmq: commands failed\n```\nWhat I am not sure about is why do the tests pass? Do they already use the patched version?\nAlso, is there a way to prevent the container from being force removed so that I can inspect the logs manually?\nI'd be happy to run the tests without Docker against a local RabbitMQ node, in fact, that would be the easiest way to compare e.g. 3.7.13 with the patched version. Is there a document that describes how to do that?. Not sure what happened to my comment :( Running tests locally on 3.7 and 2.7 with Tox suggests that they also pass with a 3.7.13 node running locally.\nSo I suspect there is no failing test case on that branch just yet? Any chance I can run a basic script that assumes that the library is on PYTHONPATH?. ``` python\n!/usr/bin/env python3\nfrom amqp.connection import Connection\nc = Connection()\nc.connect()\nc.close()\n```\nalso succeeds. Getting closer.. I tried https://github.com/celery/py-amqp/commit/6c627986c312cdb7575ab0616a63f56cf40c0be7 and discovered that it assumes that s type designator is implemented exact as in the spec. Sadly that's not the case.\nFrom https://www.rabbitmq.com/amqp-0-9-1-errata.html:\nA1, A2: Notice how the types CONFLICT here. In Qpid and Rabbit,\n         's' means a signed 16-bit integer; in 0-9-1, it means a\n         short string.. There are. When in doubt, please consult Java client, .NET client, Ruby's amq-protocol and Pika. With all respect to the Rust client mentioned in one of the issues, it is not a reference or even a mature implementation.\n\nJava client's ValueReader\nJava client's ValueWriter\n.NET client's ReadShortstr\n.NET client's WriteShortstr. FWIW this is the first time I recall someone bring this up since 2010 or so :). @thedrow I don't know if Qpid has implemented it since then but I don't see a reason to have short strings and long strings separately. They are not encoded meaningfully different on the wire. I kind of doubt the team would bother updating a half a dozen client libraries we maintain just to implement this feature. Unfortunately the spec has ambiguities and unfortunate decisions. We've corrected some problems that keep coming up. This one is not it.. If we encode strings as S and short strings as, say, s (or W or whatever), how does that save any bytes on the wire?. @thedrow this is not a discussion venue. Please post your questions to the mailing list.\n\nTo answer this last one: not all types make sense for all clients. Dynamically typed languages generally don't have a way to represent unsigned types. Some statically typed languages don't have the same fine grained types as, say, C. So the types Java client does implement is what Java can represent and hopefully some other types that it is most likely to deal with.\nSee https://github.com/streadway/amqp/issues/391 where a user of a client that's capable of representing more types than most decided to basically not bother. The benefit is not worth the revision of every reasonably popular client. In case someone cares enough they can review every client we have a tutorial for and document the gaps with some recommendations. In some cases it won't be an easy decisions, e.g. does a JavaScript user ever care about 32-bit floats?. Backported to v3.7.x.. Closing because it updates a topic branch that wasn't deleted.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. Closing because it updates a topic branch.. We take queue name and reason from the current item, so at least one item will match.\n. Currently it takes a proplist and returns a proplist. We could make it accept a pair and return a pair.\n. Unfortunately, even with pre-sorted input we may have duplicate entries (on the two fields we group by).\n. No. If we have an event, the number of times it happened is >= 1.\n. erlang:float_to_list/2 is not available in older Erlang/OTP releases, will get rid of it and push an update.\n. Yes, if we want to handle empty values, it needs to be a separate issue (not sure how many run into that).\n. Yes, the Erlang runtime does not support kernel polling on Windows.\n\nOn 7/7/2015, at 18:09, Jean-S\u00e9bastien P\u00e9dron notifications@github.com wrote:\nIn scripts/rabbitmq-server.bat:\n\n!RABBITMQ_LISTEN_ARG! ^\n !RABBITMQ_SERVER_ERL_ARGS! ^\n+-kernel inet_default_connect_options \"[{nodelay, true}]\" ^\nI don't see any +K true flag for the Windows startup script. Is it on purpose?\n\n\u2014\nReply to this email directly or view it on GitHub.\n. FTR, Pivotal is technically Pivotal Software Inc. these days.\n. @Ayanda-D I clarified this, please use the same headers as in other source files.\n. @bogdando I'd focus on the platforms/shells you plan using this script on. We are not pedantic. That said, @dumbbell can provide you a lot of feedback on *BSD compatibility.\n. Yes, please. In some environments, namely EC2, using long names are more convenient. RabbitMQ itself has an option for using long names these days.\n. I believe this comment from @videlalvaro wasn't addressed. @rtraschke was this ever investigated?\n. This is not the attribute name our preliminary docs use. I find queue-master-location to be incorrect: this is not location that we're defining but location strategy. I'm OK with renaming it to x-queue-master-locator instead of x-queue-master-location-strategy and updating the docs.\n. @Ayanda-D can you please tell me the date it took place on?\n. OK, so ha-params effects on this change have been discussed out-of-band. The conclusion is: ha-params with specific nodes listed currently overrides the locator. This is fine: using them at the same time makes little sense. We'll mention that in the docs.\n. The above comment also applies to the app config key, queue_master_location is not precise (just in case: we don't want the x- prefix there, of course:)).\n. Please prefix these with msg_store_, e.g. we already have msg_store_file_size_limit, queue_index_embed_msgs_below, and queue_index_max_journal_entries.\n. This is a very generic name but we actually validate only 2 settings. I think this needs a more specific name.\n. I find this name confusing. What about queue_index_embedding_threshold or even queue_index_embed_msgs_below?\n. I'd rename Pid to something that reflects that the pid/process is a result aggregator. AggregatorPid, maybe?\n. Should this be wait_for_info_messages? We don't mind longer function names.\n. We need to account for R13\u2026 to R16\u2026 before splitting, too.\n. If we reverse the list after foldl, how about using foldr?\n. Is this meant to say \"when the slave process handles the publish command\"?\n. Lets keep foldl.\n. Are we talking about pre-3.6.0 versions here? Mixed 3.6.0/3.5.x clusters are not allowed, so we can use a different default.\n. It is used via rabbit_mirror_queue_misc:sync_batch_size/0. I just don't think it serves any compatibility purpose and therefore has to be 1. I'd suggest making it 16K or so and moving to the app config.\n\nThoughts?\n. Changing the default to 16K leads to sync test failures.\n. OK, that makes more sense. I had changes that moved the default to the app file, bumped the default and simplified sync_batch_size/0, the error is\n```\nRunning 5 of 72 tests; FILTER=eager_sync; COVER=false\neager_sync\neager_sync:                       [setup] [running]rabbit_test_runner: make_test_multi...failed\nin function sync_detection:wait_for_sync_status/5 (test/src/sync_detection.erl, line 159)\nin call from eager_sync:sync/2 (test/src/eager_sync.erl, line 167)\nin call from eager_sync:eager_sync/1 (test/src/eager_sync.erl, line 63)\nin call from rabbit_test_runner:'-make_test_multi/7-fun-2-'/3 (src/rabbit_test_runner.erl, line 129)\n**error:{sync_status_max_tries_failed,[{queue,<<\"ha.two.test\">>},\n                               {node,c@urano},\n                               {expected_status,true},\n                               {max_tried,100.0}]}\n  output:<<\"\">>\n```\n. I see no reason to not batch all the time, only making batch size configurable (with 16K or so by default).\n. Messages that are hundreds of MB in size are probably very rare. Most messages on common workloads are < 4K in size.\nWe can go with 4096 as default value and those with large messages can adjust it. 4K * 4 KiB per message = 16 MiB of payload, not particularly excessive.\n. @carlhoerberg can you please help us pick the default batch size for eager (full) mirror sync? Maybe you have some stats on median/95th percentile message size distribution at cloudamqp, or any other data that can help us here?\n. This is worth discussing. If a node in a cluster runs out of disk space, we currently alarm all nodes. The reasoning behind this is fairly easy to guess: any client can connect to any node and publish in a way that would route messages to the alarmed node.\nHowever, I can see how we may want to be more permissive in this particular case. Unlike running out of RAM, where\nany incoming message can put us over the limit, with disk it's less likely to be the case.\n. It's not just paging that could use the disk. However, RabbitMQ probably tries to be too smart and too conservative at the moment: disk limit is never set to 0 in practice, so writing to disk is not necessarily going to make a node run out of space; we also cannot completely avoid running out of space because it can be another process that's been filling up the disk. The new behaviour is a reasonable compromise between not blocking every other node and not doing anything to avert the problem.\n. @videlalvaro we can't prevent a node from using disk at all, and even if we could it can't help if another process consumes most space. How exactly messages to flow to the alarmed node doesn't change this. Blocking all publishers across all nodes seems excessive to me given that we can't guarantee much as explained above.\n. @videlalvaro currently we block all publishers. I can understand this for RAM alarms because anything that enters a node consumes RAM. For disk alarms this sounds like an overkill and we still cannot guarantee the node won't run out of disk.\n. @binarin can you please revert the change that makes only one node alarmed? I failed to convince other team members, even though I completely agree it is a more reasonable behaviour. For 3.7 we would consider making this configurable, with a possible new mode: voluntary node shutdown.\n. @binarin ok, thanks. I overlooked that alarm_remote callers remain the same.\n. @essen is this still relevant? amqp_sup is used in the Erlang client.\n. I think it would make more sense to have an example in gigabytes.\n. We should log the value that we failed to parse.\n. There are similar functions in rabbit_disk_monitor \u2014 have you considered moving these there?\n. It would be great to have a few unit tests for this function (in rabbitmq-test).\n. @hairyhum rabbit_monitor_misc, similarly to rabbit_misc and rabbit_control_misc?\n. Hm, maybe rabbit_resource_monitor_misc would be clearer, because it's resource monitoring that we are talking about, not, for instance, Erlang process monitoring.\n. if we add this in rabbitmq-server, we need to do the same in rabbitmq-server.bat.\n. Are you sure this will shut down the node instead of just the message store? The goal is to make nodes voluntarily stop/leave the cluster if there's a possibility of disk corruption.\n. rabbitmqctl rotation is used by Linux packages. I don't think we should deprecate it.\n. One can run make package-generic-unix UNOFFICIAL_RELEASE=true from the umbrella to produce a tarball. It should take a couple of minutes.\n. Thinking out loud: os:cmd/1 always returns a string, even when execution failed. I couldn't make it throw an exception, so this is hopefully safe.\n. Yes, registered is not a very specific name. @videlalvaro what functions do you suggest the behaviour would include?\n. Behaviours tend to use short names but function name conflicts in multi-behaviour modules are also a known problem. If we were to use a longer name, I'd go with added_to_rabbit_registry and such. At least this way we are as specific as we can :)\n. This bundles Cuttlefish binary. Is there a way to manage this via erlang.mk? @essen, is there a standard way?\n. There won't be but that's not my point. We use erlang.mk to manage dependencies. Bundling a binary goes against that. Using erlang.mk to manage this dependency and having cuttlefish the binary available at runtime are not mutually exclusive.\n. It's OK to keep it for now but by the time we are ready to merge this PR, I'd like the dependency to be managed by erlang.mk if we can help it. \n. The line above says \"uncomment \u2026\" but loopback_user.guest is not commented out.\n. What would listening on multiple interfaces look like?\n. We could use minute, hours, and 24hours here.\n. Note: it's no longer 30 minutes by default.\n. We should use amqp10.\n. As discussed for authentication backend, myserver here probably can be just a number. Not because ordering matters but because picking up names for each LDAP server would be a bit silly.\n. You probably mean \"queue\", not \"query\". Also, this conflates exclusive and auto-delete queues. That's confusing, please use separate functions with clearer wording (as discussed on Slack).\n. I suggest we rename it to \"advanced config\" because it is meant to be used for more complex cases, e.g. LDAP query configuration.\n. @binarin what UNIX return code should we use when a check fails?\n. @binarin good idea\n. So this check is for us, or the user? If it's the latter, we should change this message to mention rabbitmq-env.conf and environment variables.\n. OK, I assume this is for us.\n. Please add a comment linking to #664. It's not obvious why we have to use socat and other things that are going on here.\n. Can we please use true and false here?\n. I'm curious what will the Mnesia directory value be in this case (or well, by default)?\n. This value is updated at build time. @dumbbell do you agree with this default value bump?\n. I'd recommend renaming this to is_version_supported or check_version_compatibility.\n. I find them roughly equally descriptive but true and false a much more standard ;)\n. Excellent, that sounds pretty safe.\n. Should we address this?\n. We should rename this to dependency_version_mismatch to be more specific.\n. I don't understand this comment. Can you please elaborate?\n. I don't think this message is very descriptive. It lacks context entirely. What it should say (for example):\nReceived a 'DOWN' message from X but still can communicate with it\n. Should this be {error, Msg}?\n. This breaks compatibility with older Ubuntu/Debian versions, e.g. Ubuntu 14.04. Can this be made optional?\n. @udf2457 I understand that. I also think we should keep in mind that not all RabbitMQ Debian package users are on Ubuntu 16.04 or will be any time soon.\n. As of right now, the package fails to install on Ubuntu 14.04 with\n```\nDependencies: \n1.14 - perl (0 (null)) debhelper (0 (null)) \nProvides: \n1.14 - dh-systemd:i386 \nReverse Provides: \nantares@ubuntu:~/Downloads$ sudo dpkg -i rabbitmq-server_3.6.2.904-1_all.deb \n(Reading database ... 164317 files and directories currently installed.)\nPreparing to unpack rabbitmq-server_3.6.2.904-1_all.deb ...\nUnpacking rabbitmq-server (3.6.2.904-1) over (3.6.2.904-1) ...\ndpkg: dependency problems prevent configuration of rabbitmq-server:\n rabbitmq-server depends on init-system-helpers (>= 1.18~); however:\n  Version of init-system-helpers on system is 1.14.\ndpkg: error processing package rabbitmq-server (--install):\n dependency problems - leaving unconfigured\n``\n. I'd rename this toRABBITMQ_SCHEDULER_BIND_TYPE. That's how the docs describe this option (whether that's the clearest name from the English grammar point of view). I don't think we use the_FLAG` prefix for other values.\n. Any reason why this didn't go into rabbitmq-env.bat?\n. Thank you.\n. Good idea, thank you.\n. OK, that makes sense.\n. No, they can be deleted.\n. This seems to be a copy-and-paste artifact?\n. Perhaps here we can use it, unlike in upgrade_functions.\n. Sorry but you are worrying about the wrong thing here. What really needs to happen is doing a parallel map but for small clusters it will likely be less efficient so I decided that it can be done at a later point.\nIt's not common to see clusters with more than maybe 7-9 nodes and it's unlikely to be a major contributor to how many connections/second can be accepted. Plus if you don't have any limits configured, none of this code will be executed.\n. We are no longer in the age of 80 character wide IBM punch cards and terminals.\n. Sorry but I don't see how using a macro instead of an actual function would make anything clearer.\n. In the new CLI all Elixir strings are binaries, so I don't think we can get a list here.\n. I share the above sentiment.\n. That sounds safer to me.\n. It would be a bit clearer if we used {node_database_dir}/vhosts/{vhost}/queues/{queue}.\n. See below. It would be less risky if we used {node_database_dir}/vhosts/{vhost}/queues/{queue} for path naming.\n. After a brief internal discussion we decided it's not safe to use Base64 or Base32 for our needs because they both use lower and upper case characters. Queue directory naming uses MD5 hashing, which is safe but annoying for operators as it's not easy to infer what directory corresponds to what queue.\nI suspect we can create a text file in each vhost (and possibly queue) directory that contains its actual name.\n. This is not really correct. The actual interval will adjust depending on how long it takes to run the operation. This is our \"target\" (\"ideal\") interval which will be used if background GC never takes longer than 1% of it.. This is hardly a serious concern for this function given that it will be used only by CLI tools but I'd cache the return value of active/0 instead of calling it N times.. This should be if [ \"x\" = \"x$ERL_MAX_ETS_TABLES\" ] (note the spaces), otherwise shell will attempt to execute a command. Fixed in stable.. This is not a typo.. This is not a typo either.. Same as above.. Same as above.. this is meant to say \"throughout\", not \"throughput.\". I agree with @Ayanda-D (thanks for the review, by the way).. Let's use \"preview\" instead of \"alpha.\". It's a good opportunity to fix it there. Not all preview versions are alphas.. I'm somewhat on the fence about this. Can we at least rename it to client_reported_error for clarity?. Is there a STOMP plugin PR that extends the schema? I cannot find one.. Is there an MQTT plugin PR that extends the schema? I cannot find one.. I'd rename this to clean_up_entries_without_file.. I'd rename this to index_clean_up_entries_without_file.. I think that's an implementation detail that can change over time. \"without_file\" is entirely generic.. Right, then either clean_up_entries_with_undefined_file or clean_up_temporary_reference_count_entries_without_file is good with me.. I'd suggest renaming this to either is_strictly_plugin/1 or is_actual_plugin/1.. Same as above, strictly_plugins/2 or actual_plugins/2.. I believe https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_misc.erl#L864 does roughly the same thing, can we use that fn instead?. Some users can't tell the difference between tuples and lists.. Oh, I thought I did that as part of https://github.com/rabbitmq/rabbitmq-server/issues/1143 but looks like that change was lost. Adding it now.. Done.. Then it'd have to return a proplist for default value, which seems pretty weird to me.. This makes sense, I will change this.. Thanks.. Hm, perhaps in stable but not in master? https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_misc.erl#L1139. I will test again without the explicit seeding.. We support MiB, GiB and such, so can change rabbitmq.config.example to use that.. Should we use #resource here to make things a bit more forward-compatible?. You floated a version of this with comments next to each byte on Slack. I found them very useful.. Makes sense.. rabbit.background_gc_enabled has an application-level default, so this should not break anything.. Should the backend indicate whether locking is supported, like we do for registration? Should lock/1 and unlock/1 be a part of https://github.com/rabbitmq/rabbitmq-common/blob/rabbitmq-server-1257/src/rabbit_peer_discovery_backend.erl?. There was a missing PR: https://github.com/rabbitmq/rabbitmq-common/pull/204/files.\nThe lock/1 return value of unsupported is not very consistent with supports_registration/0 but\nit's consistent with rabbitmq-autocluster :/. Arguably this is not a show stopper for #1258 and 3.7.0 release plans.. AFAIK this code is not executed on the hot path. Compatibility with more OSes seems like a bigger gain to me.. According to this document == is meant to be used for string comparison anyway.. I managed to find this but no details about how that can be adapted for one-off commands.. Apparently chcp does not change local-specific formatting of numbers/dates/etc. So it's not an option here.. We should test that all protocols we currently support either don't have this entry or have a connection info module.. OK, that takes care of what I had in mind (protocol plugins that we do not maintain).. After as in \"when switching back to the default mode\"?. I'd make it a warning and explain why it's being deleted.. stopped sounds reasonable.. Some may read this as \u201cthis is not recommended\u201d. How about \u201csystemd service restarts are not a replacement for service monitoring\u201d?. These paragraphs belong to a guide on rabbitmq.com. Comments to configuration keys can be brief descriptions with a link to more detailed docs.. Here is a good candidate. Might need some rearranging and editing but it tries to cover the same set of settings.. @gerhard Cassandra's config does not explain what Hinted Handoff is or why it exists in several paragraphs, it says \"See http://wiki.apache.org/cassandra/HintedHandoff\". A few sentences per key makes sense but config key descriptions should not end up being small guides. There are 2 config examples in master, would you duplicate all those \"micro guides\"? That's my point.. It is not controversial and disabling it is dangerous (as is not having any flow control) with well known outcomes: masters outpacing mirrors at some point, making them consume more and more RAM.. How large is \"very large\"? I'd say \"more than 16 cores and plenty of bandwidth\".. We should mention that increasing these values may help with throughput but is dangerous (very high values are no different from having any flow control).. nodelay is forced by Ranch even if you disable it. I see no reason to do that, by the way.. Any reason why there is no link to http://www.rabbitmq.com/networking.html here? It's a pretty extensive guide.. Please change it to \"Message store operations are stored in a sequence of files called segments. This controls max size of a segment file. Increasing this value may speed up (sequential) disk writes but will slow down segment GC process. DO NOT CHANGE THIS for existing installations.\". This is an interval of time disk monitor waits before retrying, not a query timeout.. This makes it sound like anyone wants to enable an attack.\n\"Makes RabbitMQ accept SSLv3 client connections by default. DO NOT DO THIS IF YOU CAN HELP IT.\". application:get_env/3 isn't available in Erlang/OTP R16B03. Please use rabbit_misc:get_env/3.. It depends on what exactly rabbit:stop_and_halt/0 does. Our modification returned an error as a value. Both should be covered.. We don't want to wrap all possible errors into error_during_shutdown, only those that we are absolutely sure related to rabbit:stop_and_halt/0 execution.. We don't want to wrap all possible errors into error_during_shutdown, only those that we are absolutely sure related to rabbit:stop_and_halt/0 execution.. There was supposed to be a line that clears it \ud83e\udd26\u200d\u2642\ufe0f .. Corrected.. I'll add the missing continuation character. Noticed when gmake run-broker started hanging without it.. Not sure why the value here had to change but it's just an example.. This has to traverse the entire collection. If we can avoid this, it should make a difference.. We should mention what key that is for our own debugging sanity.. Do we need maps:update_with/3 here? Looks like we can safe a function call by using maps:update/3 since we're only incrementing a counter.. == seems to be bash-specific and is not available in zsh.. == seems to be bash-specific and is not available in zsh.. ShellCheck catches it, too:\n```\nIn scripts/rabbitmq-env line 91:\n[ \"x\" = \"x$RABBITMQ_MAX_NUMBER_OF_PROCESSES\" ] && RABBITMQ_MAX_NUMBER_OF_PROCESSES==\"${DEFAULT_MAX_NUMBER_OF_PROCESSES}\"\n                                                                                  ^-- SC1097: Unexpected ==. For assignment, use =. For comparison, use [/[[.\nIn scripts/rabbitmq-env line 94:\n[ \"x\" = \"x$RABBITMQ_MAX_NUMBER_OF_ATOMS\" ] && RABBITMQ_MAX_NUMBER_OF_ATOMS==\"${DEFAULT_MAX_NUMBER_OF_ATOMS}\"\n                                                                          ^-- SC1097: Unexpected ==. For assignment, use =. For comparison, use [/[[.\n``. This must include our own node, however.. Technically this won't be available on pre-3.7.4 nodes. But given that vhosts are not created during rolling upgrades in practice, this might be OK.. This error message is too much inside baseball.. \"gm_deaths\" is an implementation detail that makes no sense to the user.. Note that this relies on resource name to be unique, which with resource names for e.g. queues (which include vhost name) should be good enough. The table name here is only used to find the list of replicas, which will be the same for all tables.. I suggest that we rename this tolog.syslog.host, much like the syslog library does (\"dest_host\").. Yup, thenipis a better idea.. Can we leavelog.syslog.ipin place for compatibility? (it also makes some sense as an alias anyway). Oh, OK.. This code is many years old so I can only guess the intent here. I think it tries to assert that the ref/pid combination is known. I'd leave it as is for now, this is not a hot code path event.. The clauses simply demonstrate what 3 scenarios are possible. We can combine the latter two.. I don't see this function in Ra master. Does it depend on a local branch?. If we log this for every message we can swamp the log very quickly.. I confirmed that-fis named consistently and seems to advertise the same behavior for Linux, MacOS and FreeBSD.. We discussed this: this is a fallback timeout and CLI tools default to 5 minutes, decided to leave it as is.. Corrected.. It's 100 ms.. For that reason, please back it out. We won't be changing any license files.. That's fine.. There are only so many places where channels are started. We can make the parameter required with a shim function head that passes?MODULEfor it if it's not explicitly provided. These are technicallyrabbit_channel` API changes but that's fine since this is node local state.. Thanks for clarifying. We don't expect any new connection implementations to appear any time soon :)\n@lukebakken should I continue with QA or would you like to see more changes?. Possibly to handle rabbitmq-autocluster values or because we've seen someone try to use a list instead of a tuple in the classic/advanced config file. I think by now we can support tuples only, that's what Cuttlefish produces and the docs only provide new style config file examples.. It might be easier to use --silent on the command line.. Should this say rabbitmq_queues_cmd?. This feels like it could be moved to rabbitmq_ct_helpers and thus easy to reuse.. But it's the handler's choice/responsibility to handle them. I don't know if we want to mask those errors for every caller and what the return value should be in that case.. Should we document inline why it is the case?. ",
    "simonmacmullen": "Thanks, I have applied a slightly different version of this patch to our hg repo; it should show up on git shortly.\n. Ah, this is the management agent. It's running every 5s and ignoring the interval set in the disk monitor itself. Will fix.\n. I don't think anyone is debating whether dlx-on-queue-delete is a useful feature in the abstract. It totally is!\nMy concerns with the design of this pull request are:\n1) It doesn't use flow control; therefore it's possible that if the deleted queue is large then it can spam the DLX-ing queues with messages fast enough to use up all available memory and crash the server.\nYes theoretically this issue exists already with messages getting DLXed by other means. But it's quite hard to hit; this patch makes it very easy. There's a good chance deleting any large queue with a DLX set would do it. So this really needs addressing.\n2) It doesn't guarantee messages will not be lost when queues with DLX set are deleted (e.g. if you shut down at the wrong time). That's more fixable by documentation though.\nI also have concerns with the implementation:\n3) The bit about removing bindings first is dubious; other clients could add bindings back after this point. And calling into exchange callbacks is especially dubious. It happens to work for the CHX but I am not at all convinced that it would do the right thing for (e.g.) federation. See @rade's previous discussion here:\nhttp://lists.rabbitmq.com/pipermail/rabbitmq-discuss/2014-January/033354.html\nNow, @rade was not too concerned with offering solutions there beyond \"it's complicated\". But I would suggest that doing this in the termination phase (after rabbit_amqqueue:internal_delete/1 has already happened) is definitely a preferable way forward.\nYou still have the issue with \"dark queues\" he mentioned, but there is now infrastructure that could let you fix that too; we can show crashed queues which are there-but-dead, storing the fact that they're crashed in Mnesia: https://github.com/rabbitmq/rabbitmq-server/blob/9388c6613c1789516614077868b1b7417979ddff/src/rabbit_amqqueue_process.erl#L237\nThis could be extended to support deleting queues too.\n. I don't like it as-is since it will presumably cause our xref tests to fail due to a call to a \"nonexistent\" module. It needs to fool xref like this: https://github.com/rabbitmq/rabbitmq-auth-backend-ldap/blob/83044408f7e0fa6da2969e7532376e37256d66df/src/rabbit_auth_backend_ldap.erl#L304\nOther than that it's OK.\n. @michaelklishin I don't know what you mean by \"the entire VQ state is emitted to the stats DB\" but that field is not used and can safely be removed. Also, while measuring throughput is definitely valuable, but this patch does not look too bad in terms of throughput. Note that it will not page messages in from disk if necessary, just ignore them.\n. Now, my comments on the pull request itself...\nFirstly @alexethomas, thank you for your work here! This looks like a useful feature, and in principle I have no objection to it. In practice there are a few issues with the implementation.\nAs mentioned, backing_queue_status is not really correct, that's a dumping ground of debug information. You can make this into a \"real\" queue info item by adding head_msg_timestamp to rabbit_backing_queue:info_keys/0 then adding a new head to rabbit_variable_queue:info/2. This will then make it available through rabbitmqctl list_queues name head_msg_timestamp or similar (so you should then update the manpage, see docs/rabbitmqctl.1.xml).\nThe implementation of head_msg_timestamp has a couple of bugs. Firstly, it assumes that (the head of) RPA is newer than DPA, which is newer than Q4, which is newer than Q3. I think of those relations only Q4 to Q3 is guaranteed. You need quite a pathological case to get DPA newer than RPA, but it's trivial for RPA/DPA to be older than Q3/Q4, unacked messages do not inherently live at the head of the queue.\nAlso, the master branch has a qi_pending_ack field as distinct from RPA and DPA (for unacked messages subject to the queue index embedding optimisation), so you need to think about that.\nSo I think for every source of messages here you need to get the head timestamp and then compare timestamps, trying to find the \"newest\" head is not going to work.\nThere's also the issue that some of these sources of messages have the message paged out (i.e. #msg_status.msg is undefined) (DPA definitely, Q3 if the message is not small enough to be embedded in the queue index). Your patch doesn't crash with these, but it does return undefined, which suggests that checking DPA is completely redundant, and Q3 rather dubious.\nYou could page messages in if needed, but you'd certainly have to work harder to argue that the patch has an acceptable performance impact.\nPersonally I'd forget RPA / DPA / QPA and just define it as working for ready messages only. I might be tempted to also write a peeking version of queue_out/1 which pages in from Q3 if necessary (since the queue is presumably going to page in from Q3 soon if Q4 is empty) so that empty Q4 does not lead to the info item becoming blank.\nI was going to comment on the tests being in Python and needing management, but I see that's already a TODO :-)\n. So this is moving in a decent direction, cool!\nIf you are not going to page messages in for this (which is probably the right thing to do, or rather not do) then:\n- There's no point checking disk_pending_ack at all, messages in that are guaranteed to be paged out.\n- It might be worth only checking q3 if q4 is empty, since q3 can't contain newer messages than q4.\n- The manpage should mention the fact that paged out messages are ignored for this, somehow.\nAlso:\n- The final return from the info item should be '' (i.e. the atom with a zero-length name) not undefined if there is no value. Yes, that's weird, but it's how our info items work.\n- Indentation is currently pretty weird. \"Whatever emacs does\" would be a good start :-)\n- Since this isn't paging messages in, I'm not really worried about performance.\n. I would rephrase \"The message must be paged-in from disk to appear\" since a determined user could misinterpret that as meaning \"inspecting this info item could cause extra messages to be paged in\". Maybe \"Messages which are paged out will not contribute to this field\" or similar?\nI think this is nearly ready. If you are going to keep the tests they need to be moved to Java or Erlang. Having said that, we haven't had automated tests for info items in the past. Maybe we should though.\n. That phrasing is fine.\nActually either Java or Erlang tests could work - Erlang tests have the advantage that they can bypass invoking rabbitmqctl and just RPC to rabbit_amqqueue:info_all() or whatever, but we do have some Java tests which invoke rabbitmqctl list_* operations too - see https://github.com/rabbitmq/rabbitmq-java-client/blob/master/test/src/com/rabbitmq/tools/Host.java#L148\n. Flow control is not the problem here. Not saying there isn't a problem, but thinking about it in terms of flow control is unlikely to be helpful.\nFlow control is just saying \"the broker is unable to accept messages as fast as publishers would like to publish them\". It's more important to focus on why that is happening. Every complaint about flow control in the past - and there have been a few - has always turned out to be about something else performing badly.\n(Analogy: if there is too much traffic in your city, the solution is not to remove all the traffic lights.)\nSo what actually is the bottleneck? This blog post: http://www.rabbitmq.com/blog/2014/04/14/finding-bottlenecks-with-rabbitmq-3-3/ talks about how to interpret flow control state to find bottlenecks, but if I had to take a guess from the symptoms described, I would think that queues have gone over their paging ratio and have started to page messages out to disk. Everything I write from here on assumes that's the case.\n(Aside: RabbitMQ 3.5.0 will expose information about how much disk activity queues are doing, so this should be a lot easier to spot. See \"per queue paging stats\" at http://imgur.com/a/tLwQs#vU8pQSj)\nSo first of all, I would suggest @geekpete that you look at http://www.rabbitmq.com/memory.html#memsup-paging and possibly tweak the values of vm_memory_high_watermark_paging_ratio or vm_memory_high_watermark.\nBut there is a real issue here: while queues manage their workload to balance the work done accepting messages vs delivering messages, they don't attempt to balance paging in the same way; it's at the mercy of the scheduler. It's possible that once a queue decides it has to page it will devote 90% of its time to doing so (until enough messages are paged out, anyway!)\nThis is a possible area of future improvement, but not going to make it into 3.5.0 I'm afraid.\n. rabbitmqctl report already includes rabbitmqctl environment and thus dumps all (configured) environment settings, so if it's not there it's the default.\nThere may be value to adding it to rabbitmqctl status. Should be careful of not adding everything to everything though...\n. I guess it would be polite to backport this to the 3.4.x plugin when fixed...\n. > Hard code a smaller buffer size\nSeems rather sad.\n\nDynamically shrink the buffer size if we determine it is not working\n\nThis is the approach I've gone for, primarily because it should be able to stop other pathological behaviour.\n\nRead the buffer backwards from our seek point if we detect we are seeking backwards\n\nThis might be a nice option to get syncing going still faster, but it's also fiddly and only solves this exact problem. I'll settle for having it no worse than it was in 3.4.4.\n. Just for clarity, this bug:\n- Only affects 3.5.0\n- Requires messages to be larger than the queue index embedding threshold (by default 4kB)\n- Requires messages to be paged out before synchronisation starts\nYou can see in the I/O stats on the master that if (say) 250 messages are read from disk per second, we also read 250MB/s even if the messages are much smaller than that.\n. Note that you don't need the -R 100, you can use -y0 -u test -p to get PerfTest to publish to a queue with no consumers which might be easier to work with.\nThe 4MB sizes probably refer to other files (queue index files?)\nNot sure whether the flicking between 10468 and 20936 is worth fixing, what do you think?\n. Oh, also you can set a very low vm_memory_high_watermark_paging_ratio rather than vm_memory_high_watermark, that way you can publish indefinitely but get paged out rapidly.\n. If message store GC fails with eacces there's a good chance lots of other things will too - there's not much point in proceeding with this issue unless you are going to tackle all of them, otherwise it will just crash a millisecond later on something else.\nAnd in general recovering from permission failures is hard - for deletes you can just ignore the fact that the delete failed and leak a file, but what do you do if something else fails?\nI don't think this is a useful thing to do, FWIW. RabbitMQ needs to be able to have full access to its own data store, patching up after the fact is intractable.\nReporting such errors better would certainly be interesting.\n. The new code doesn't count the number of times we have been at a certain queue, it just records the fact that that we have been there. That's removing a bit too much information IMO, a counter would be useful, non-leaky, and easy to add.\nI am not completely convinced this has not broken cycle detection for some more arcane types of cycles. I shall try to have a think about that.\nThe Java tests introduce new Channel APIs! That's an unexpected side effect of this bug :smile: I don't think Channel.queueBind(String queue, String exchange) is actually a great idea; it de-emphasises routing keys and thus might rather confuse beginners.\n. Yes, but cycle detection makes decisions based on the {reason, queue} pair, but the branch only keeps one header per queue. That smells dubious to me.\nSo for example if you have a message which (for some odd reason) keeps getting dead lettered to the same queue, but with alternating reasons of expired and rejected then this branch changes the behaviour - we used to not drop it (since we only drop messages on dead-letter cycles that are fully automatic, requiring no client intervention to keep them going) but on this branch we would drop it.\nI'm not sure if I can get a change in behaviour the other way round (which would be much more dangerous of course). And that is a bit of a contrived example. But it's still a real behaviour change that I think we can avoid.\nWhat I would like to see when adding a new x-death entry:\n- Look for a matching one, matching on both queue and reason\n- If there, increment its count and bring to the front of the list\n- If not, create one with count=1 and add to the front of the list\n...which I think is what @michaelklishin has done but with the addition of a counter and matching on reason too.\n. The logic of this looks OK to me now. Admittedly I haven't actually run it.\nI have a bunch of style complaints, take them as seriously as you wish:\nAutorecoveringChannel has grown an import for no obvious reason.\nI would inline x_deaths_from_header/1 and x_death_not_for_queue/2, I don't think they make it any clearer. I also wonder if x_death_header/1 could become header(HeaderName, Headers, Default) and be moved to rabbit_basic, since I suspect it would be more generally useful.\nPersonally I don't find abstracting away the names of headers to be any clearer either (since you always have to mentally look it up) so I'd kill ?X_DEATH_HEADER and friends too. I know we do that elsewhere, but I don't like it elsewhere either :smile:\nI would rather the new field be called count rather than counter: it reads more naturally to me.\n. Since bug 25938 so 3.3.0.\n. Thanks to Dustin K for his work on this.\n. rabbit_access_control is meant to be that API.\nIt might be that bits have escaped and need to be pulled back into it.\n. Also note that (at the moment) there are not considered to be any public (read: stable) APIs inside the broker; anything can change at a feature release and plugin authors are expected to keep up. You could change that promise of course, but APIs would benefit from some review before becoming frozen.\n. ",
    "bitonic": "Not having experience with Chef or Opscode, it is not clear to us what issues are you having. Could you give more details?\nBesides, the suggestion is to use the \"--background\" flag of start-stop-daemon, but the man page warns:\n\nWARNING: start-stop-daemon cannot check the exit status if  the process fails to execute for any reason. This is a last resort, and is only meant for programs that either make no sense forking on their own, or where it's not feasible to add the code for them to do this themselves.\n\nThe erlang way to daemonise is run_erl, but that creates pipes for the pty which we don't want.\n. In the end we \"solved\" it by writing stdout/err to files in /var/log/rabbitmq/. The change will be included in the next release.\n. ",
    "cgriego": "@bitonic The problem isn't exclusive to Chef, but it is the easiest way I've found to recreate the issue. The problem is one of who the parent of the rabbitmq-server process is. In the Chef bootstrap case without the fix applied, the parent of the process ends up being part of the Chef chain of processes, causing rabbitmq-server to exit when Chef completes running. Here is that hierarchy of processes:\n* SSH login shell\n** `bash -c` subshell running a script provided as a String\n*** chef-client Ruby process\n**** Ruby execs a subshell\n***** `/usr/sbin/invoke-rc.d rabbitmq-server`\n****** Debian magic between invoke-rc.d and init.d script\n******* init.d script calls `start-stop-daemon`\n******** rabbitmq-server\nThe & in the init.d script only backgrounds the start-stop-daemon process, but since start-stop-daemon (without the --background) flag, expects the process it is starting to daemonize itself. The --backround flag replaces the & backgrounding and also adds a process fork to ensure the parent of the rabbitmq-server process ends up being init and not one of the many processes listed above. The previous setsid call was performing this fork, but start-stop-daemon needs the --background flag to know that it needs to fork on behalf of the child process.\nThe --background flag is not optimal, it's definitely preferred for the process to self-daemonize, but it is an improvement over the use of & and the use of rabbitmqctl wait effectively makes up for not being able to check the exit status of rabbitmq-server directly.\n. @emile Good catch and thanks for the update.\n. Excellent. Thank you @bitonic and @emile.\n. ",
    "emile": "@cgriego Thanks for your suggestion. We are currently looking for a way to incorporate it without losing stdout and stderr, which contains potentially crucial information. See the mercurial branch named 'bug24930' for commits relating to this.\n. Thanks for your feedback. This is a known issue and an internal ticket has already been filed for it.\n. Can you please confirm that this problem still exists with the latest version of the broker? There have been improvements in that area which may well have fixed this problem.\n. This sounds like a question for your package manager rather than the rabbitmq-server package. Please redirect to your OS vendor or the author of your package manager.\n. I'm unable to reproduce this result using the latest version of RabbitMQ on R16B (erts-5.10.1) on Darwin 10.4. I see \"df\" checks spaced exactly 10s apart when using an instrumented version of \"df\" instead of \"execsnoop\".\n. The output you provide could be explained by 3 timers, but I'm unable to reproduce that. Are there any errors in any of the logfiles when the broker starts? What does this return:\nrabbitmqctl eval 'rabbit_disk_monitor:get_check_interval().'\nDoes this make any difference?\nrabbitmqctl eval 'rabbit_disk_monitor:set_check_interval(10000).'\n. The broker can not recover from arbitrary filesystem corruption or missing files. Please post follow-up questions to the rabbitmq-discuss mailing list.\n. ",
    "rade": "Thanks for reporting this. I have filed a bug in RabbitMQ's internal bug tracker. We should hopefully fix this soon. The required change is more complex than you propose - a fixed interval really doesn't cut it; instead the timer should be set to go off at the time the next message at the head of the queue is meant to expire.\n. This got fixed in RabbitMQ 3.1.0. From http://www.rabbitmq.com/release-notes/README-3.1.0.txt:\n25490 limit frequency with which the server invokes \"df\" or \"dir\" to measure disc use\n. We've seen this crop up on a number of occasions in the past, yet have never been able to reproduce it. The condition to trigger this appears to be both environment specific and temporary.\n. @dallasmarlow any idea why in your setup the gc would have backed off so much?\n. Also, I'd like to question the utility of this whole logic now. With the upper bound set at 3 minutes, and the default at 1 minute, there is preciously little adjusting going on. You might as well ditch all the adjusting and run at a fixed interval.\n. Heap size isn't the only factor. I doubt you'll get much out of simulations. The best you can hope for with metrics collection is that when some users see weird behaviour you might have a chance of figuring out what's going on.\n. And, btw, I wasn't suggesting that there should be no interval adjustment, or other ways to control how gc gets done and when, just that the changes in #99 have made the current adjustment mechanism pretty pointless.\n. ",
    "jameskeane": "I have done some more investigation, it seems this problem, happens for all errors.  And has existed since at least 2.6.\nRabbitMQ sends a channel.close message, but it doesn't actually close the damn channel.\nI have created an example of this here: https://gist.github.com/9fe5507c2e7169bb93b2\n. This is a bug in all of the client libraries I have used.\nWhen a server sends a channel.close, the client is expected to acknowledge with a channel.closeOk.  If the client does this the channel is properly released. \n. ",
    "thejuan": "My reading of AMQP Spec is that no CloseOk is required when a channel is closed with an exception.\nI would suggest this a bug in the server code?\nhttps://www.rabbitmq.com/resources/specs/amqp0-9-1.pdf\n2.2.3 No Confirmations\n4.5 Channel Closure\nThe server will consider a channel closed when any of these happen:\n1. Either peer closes the channel or the parent connection using a Close/Close-Ok handshake.\n2. Either peer raises an exception on the channel or the parent connection\n. ",
    "max0x7ba": "I do not see that error in the logs any more since I switched to rabbitmq-server-2.8.7 on 2013-01-02.\n. ",
    "atombender": "I replaced /bin/df with a bash script that logs to a file. Example output after running for almost a minute:\nThu Mar 14 11:44:06 CET 2013\nThu Mar 14 11:44:11 CET 2013\nThu Mar 14 11:44:11 CET 2013\nThu Mar 14 11:44:16 CET 2013\nThu Mar 14 11:44:21 CET 2013\nThu Mar 14 11:44:21 CET 2013\nThu Mar 14 11:44:26 CET 2013\nThu Mar 14 11:44:31 CET 2013\nThu Mar 14 11:44:31 CET 2013\nThu Mar 14 11:44:36 CET 2013\nThu Mar 14 11:44:41 CET 2013\nThu Mar 14 11:44:41 CET 2013\nThu Mar 14 11:44:46 CET 2013\nThu Mar 14 11:44:51 CET 2013\nThu Mar 14 11:44:51 CET 2013\nInstalled via Homebrew, want me to try compiling and running from an original source tarball?\n. Your first command returns 10000, the second command does not affect anything.\nTried with clean source and run with the following in enabled_plugins:\n[rabbitmq_management].\nI don't get the df\u00a0behaviour if I remove the management plugin.\n. Oh, and nothing interesting the log files.\n. ",
    "hyperthunk": "Hi @phillpafford - this kind of functionality isn't really appropriate for internalising in a message broker. It's definitely the kind of thing that should like in a client library/application of plugin. I suggest popping over to the rabbitmq-discuss mailing list and asking how other PHP users are doing aggregation. The PHP client library authors are quite likely to be on the list too.\n. Hmn, this is an interesting question. I suspect this won't happen if you set TTL on the queue rather than the individual messages, but even for per-message-ttl you raise an interesting question about how the semantics should work. We will have a discussion about this and let you know which way we're going shortly.\nThanks for reporting it!\n. In conclusion, we've decided to remove the 'expiration' property from messages that are dead-lettered, since we consider this less surprising behaviour from a user perspective. The fix is visible in http://hg.rabbitmq.com/rabbitmq-server/rev/ecd2b82b69cc and will be included in the next release.\nThanks again for the report!\n. Fix is due to go out in 3.1.2, which should come out in the next couple of days.\n. This was included in today's release - see the release notes for details. Thanks again for raising it.\n. ",
    "phillpafford": "https://groups.google.com/forum/?fromgroups=#!search/phillpafford/rabbitmq-discuss/hElmMpIFQmU/_Ve5HuaUhR8J\nThanks for the reply, I will keep searching\n. ",
    "denisenkom": "Just for the record, removing corrupted rabbit_serial file helps to recover rabbitmq server in this case\n. ",
    "rjrudman": "Thank you very much for the response and quick turn around! The fix looks great. ",
    "q42jaap": "Without any guarantees, but this line works on my MINGW setup\n. I just had to fix the logging directories by setting RABBITMQ_LOG_BASE to /utils/MinGW/var/log/rabbitmq (MinGW is installaed in C:\\utils\\MinGW)\n. ",
    "hairyhum": "There are bunch of types from 0-9-1 missing in rabbit_binary_parser. I'm not sure though what RType values should be used for them.\nMissing types:\n 'B' short-short-uint\n 'U' short-int\n 'u' short-uint\n 'i' long-uint\n 'L' long-long-int\nAnd there are also 's' type tag, which means 'short-string' which is OCTET *string-char and in current implementation it is being parsed like signed short \nhttps://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_binary_parser.erl#L66\n. So we can at least add 'B', 'u', and 'i' types as 'unsignedbyte', 'unsignedshort' and 'unsignedint' respectively.\n. Lager receives messages from error_logger, so it's possible to just start lager application on rabbitmq node and set logging app_env and logs will go to lager. (without lagers fancy formatting, which is parse_transform).\nSo in first approach we can just add log location to rabbitmq.config to lager application.\n. Do anyone know why is there append_file and then delete instead of just rename in log rotation? https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_error_logger_file_h.erl#L170\n. We have following config variables:\nRABBITMQ_LOG_BASE \nRABBITMQ_LOGS - general logs (default %RABBITMQ_LOG_BASE%\\%RABBITMQ_NODENAME%.log)\nRABBITMQ_SASL_LOGS - sasl logs (default %RABBITMQ_LOG_BASE%\\%RABBITMQ_NODENAME%-sasl.log)\nRABBITMQ_LOGS and RABBITMQ_SASL_LOGS can be set to absolute filename or - which means log to console.\nThis configs transform to application env variables error_logger and sasl_error_logger, which can be\neither tty or {file, FileName}.\nLager log_root and handlers for logs can be configured from .config file for different loglevels and console/file/something else backends.\nSo I think it is better way to configure non-default logging, rather than env variables.\nCurrently log_root is set to RABBITMQ_LOG_BASE if not set in .config\nhandlers also can be formaed from env variables if no handlers config present.\nSo RABBITMQ_LOGS=\"-\" becomes {lager_console_backend, debug}\nRABBITMQ_LOGS=\"filaname\" becomes {lager_file_backend, [{file, \"filename\"}, {level, debug}]}\nThere are change in log rotation. Customers can rely on logrotate with 'suffix', which \nis copying file to a new location and creating new empty one.\nWhile using lager it is possible just to move file or delete it (if suffix = '')\nDifference will be in that there could be no logfile after rotating until lager log something in it.\nLager is checking if file exist on every log and create new ones without loosing messages \nLager itself supports log rotation by date and file size and can keep given count of\nrotated files, which could be useful.\nBut it is possible to leave existing rotate_logs command.\nSo main changes so far will be:\nerror_logge app config variable is not relevant anymore.\nAfter rotate_logs, there could be no log file until next log. \nApplication config for lager will override ENV variables.\n. And I didn't found out yet how to separate sasl and general logs so far.\n. Found an option to separate sasl and general logs. It is lager sinks. It even allows us to log different categories rabbitmq_log:log(Category, Level, Msg) in different files. Maybe it can be useful.\n. Currently lager has it's own way to format error_logger messages. This format skips stack trace in some cases, which could be inconvenient. \n. @michaelklishin unfortunately, not in current implementation. And they are not too quick at accepting pull requests right now. But we can use forked version, if it's an option.\n. As I can tell from code, stacktraces are omitted if errors are from gen_server, state_machine, en_event and supervisor. So it's should be OTP messages.\nIt also reformats ranch and cowboy messages, but there are stacktrace in this cases.\n. I mentioned fork because lager team are too busy right now to deal with PR, like they said. So I hope it could be merged eventually.\n. We can use client-properties field in connection.start_ok frame to specify connection name. \nWe already report client properties in management and can just change formatting for connections table in management.\nAnd client libraries can introduce API for setting this client property as \"connection name\" or just use existing API to set it.\n. Then we can add this to management plugin and clients in 3.6.x.\nThere will be no breaking changes.\n. We can also change log messages to support client provided names.\n. I'm not sure if it makes sense to implement per factory setting. We can add optional parameter to newConnection/CreateConnection method to start a connection with specific name.\n. What I get from code is there are log of backup dir if backup is taken, but previous_upgrade_failed error message is thrown if it cannot take a backup. Or should it report directory where it cannot create backup?\n. CONF_ENV_FILE is just being sourced by rabbitmq-env script before redefining RABBITMQ_ variables, so it's safe to override it with something like RABBITMQ_CONF_ENV_FILE.\nIn windows scripts it can already be overridden, in shell scripts it doesn't seem intentional according to commit history.\n. @dumbbell should it be closed?\n. There are rabbitmqctl report which prints status for every node in cluster. Looks like the same thing. \n. In disk_free_limit there are absolute values by default, should there be absolute_ prefix in that case?\n. And it looks like rabbit_disk_monitor operate in megabytes as 10^6, while vm_memory_monitor operate in 1048576 MB. Is it OK?\n. Power of two is used a lot in vm_memory_monitor, it can be confusing to get rid of it there. \nSo there could be no \"special tuple\", but just a string in place of integer in limit config? That will do, I guess. Not so explicit though. \n. @michaelklishin I don't know, maybe someone rely on it. There are some parsing of system memory data and limits defined as 256_1024_1024_1024_1024 for example. It's looks like OS opinion about what megabyte is.\n. It looks a more consistent, but should be documented. Let's continue with this decision. \nThere will be {vm_memory_high_watermark, {absolute, limit()}} and {disk_free_limit, limit()}\nWhere limit() :: integer() | string()\nAnd string can be integer or integer with unit:\nk, K, KiB - kibibytes\nm, M, MiB - mebibytes\ng, G, GiB - gibibyte\nkb, KB - kilobytes\nmb, MB - megabytes\ngb, GB - gigabytes\n. So it will be\nk, kiB - kibibytes\nM, MiB - mebibytes\nG, GiB - gibibyte\nkB - kilobytes\nMB - megabytes\nGB - gigabytes\n. And if we use strings with strict parse rules there will be invalid strings. I guess we should use some error reporting for this case. Is there some error reporting about bad config?\n. Then it will be parsed on startup or changing and used as {absolute, integer()} in internal state.\n. And we have no action to set disk limit in rabbit_control_main, it can be set only from config. Do we need it?\n. Since vm_memory_high_watermark is required when starting vm_memory_monitor \"ignoring\" value means use infinity?\n. How can you fall back to relative setting, when it was overriden in application env, since it was specified in rabbitmq.config? Or maybe I missing something?\n. Also added tests for parse function \nhttps://github.com/rabbitmq/rabbitmq-test/commit/cd8c9a88c0c16b5e56d6e4fbf771b06451fdb31c\n. wait command can wait for specific background job only and will return exit code for it.\n. Fixes #94 .\n. Spend most time trying to make tests with old config working too support same behaviour. \nIt should work like nothing is changed. Same log files, same config. But there is a way to customise it with lager .config \nNext steps could be better integration with parse_transform or adding more sinks for logging different things separately. But this could be a separate tasks.\n. Created pull request for test branch\n. @michaelklishin I don't have any errors with make full. I have tested at MacOS El Capitan and OTP 18 \n. @dumbbell priority of rabbitmq.config parameters was selected on purpose, because it is much more flexible way to configure file locations, log levels and rotating.\n. Messages like CRASH REPORT are formatted by lager here: https://github.com/rabbitmq/lager/blob/master/src/error_logger_lager_h.erl#L256\nWe could add some custom formatting, but I guess then we will have to make it pluggable and make PR to lager upstream.\nIgnoring should be easier by just removing backend, but it will make lager ignore all other error_logger messages.\n. Or as an option we can configure rabbit_log_lager_event sink and enable lager_transform so lager will  send rabbit_log:error|debug|info etc to this sink. At least that's what lager docs say.\n. @dumbbell it's mentioned in sinks doc section https://github.com/rabbitmq/lager#sinks\nIt won't support logging with categories, so could be unsafe if categories are used in some plugin code.\n. Can it be implemented using mandatory flag and proper response code in return message?\n. Message from publisher can go to multiple queues and some of them can be full and support blocking. In that case publisher will still receive connection.blocked and be blocked.\nThen publisher can retry sending the message, and it will be routed to \"free\" queues again while blocked by same or other \"busy\" queue.\nTo be able to avoid it we should know that all queues are able to receive message, but it will require some kind of multi-queue transactions and can be too hard.\nSo using this feature will require understanding that messages can be enqueued multiple times in some routing scenarios.\n. When queue is blocking publisher it should unblock it at some point. Doing it right after the first message was consumed can cause multiple channels to start republishing again and be blocked. Some kind of additional parameters can be provided to the blocking argument. Like free space in the queue on which it will unblock publishers. It can be relative to queue max_length or absolute number of messages (of bytes).\nAnother concern is requeueing messages. Default overflow strategy is to drop messages and they are dropped on requeue as well. If we introduce non-dropping strategy requeues will overflow queue since they cannot be blocked.\nPossible solution is to not unblock publishers until there are place for queued messages + unacked messages count. \n. Idea to refactor blocking states in rabbit_reader\nCurrently there are running blocking and blocked states and throttle field, which contain state information. So transitions indirectly depend on this state information.\nSo proposal is to change this state machine: https://github.com/hairyhum/pics/blob/master/old.png\nInto this: https://github.com/hairyhum/pics/blob/master/new.png\nSo there will be more sane transitions which will not depend so much on throttle values during transition.\n. @michaelklishin currently state are divided between connection_status and throttle and desigions are made based on throttle. \nAlso connection could already receive publishes, but stil will be set to blocking on next round (i don't know, maybe it's by design)\nDesision to send connection.blocked is made by checking alerts in old and new throttle, as well as desision to send connection.unblocked. So state when connection was blocked by flow, or alerts with sending method to client os not explicit.\nBasically this new machine behave same way as old one, but I tried to take conditional logic and indirect state information from inide transition and throttle to state machine.\n. @michaelklishin it was related to this issue, because I've tried to send connection.[un]blocking from channel because of message overflow. And I could not, because blocking state machine is too implicit.\nRight now it's gone too far, I agree. Will move to another PR. \n. First investigation results:\nRequired changes in backing_queue API to support integer redelivery count instead of flag in fetch/2 return value and functions working with fetch.\nAlso queue index format should be changed to support multiple deliver records, or delivery count. But because current implementation assumes exactly 1 deliver for each acknowledge there should be significant changes in index structure.\nAny thoughts?\n. It's in the roadmap for 3.7.x\n. Will it be enough to just stop node? Because I guess it could be restarted again and cause trouble if in cluster. \nSo there is an option to leave cluster (if clustered) and stop the node. \nIt still require some tests.\n. I like idea of separate coordinator process, but could it be dangerous to allow node to continue working during disk errors while this process detect or collect errors? \n. Authorisation server can be plugin as well, and we can implement some specific endpoints to communicate with rabbitmq authorization backend plugin. I think there should be two plugins: one is auth backend and another is authorisation server. If users want to use their own authorisation server - they can use backend only.\n. Current progress is here https://github.com/rabbitmq/rabbitmq_auth_backend_oauth\n. Supervisors shutdown set to infinity, all workers shutdown set to 30000 by default.\n@dcorbacho can you please clarify what \"supervisors across nodes\" are in this context?\n. There are several nasty regressions in 18.3 like https://github.com/rabbitmq/rabbitmq-auth-backend-ldap/pull/42\nMaybe it makes sense to set requirement to 18.3.x or 18.4\n. We can catch net_kernel:start and change name if it's taken. And we should also look at rabbit_nodes:ensure_epmd, because it is also generates big random node names.\n. Here is cuttlefish schema and example config for this schema.\nhttps://gist.github.com/hairyhum/a73c30672acc55125643\nThere are rabbitmq.example.config parts in comments with examples and docs in both files above each setting.\nThere should be more validations to check config values and maybe some other mappings.\nYou can run cuttlefish escript with this files to generate rabbitmq.config.\n. cuttlefish has support for bytesize datatype https://github.com/basho/cuttlefish/wiki/Datatypes#bytesize which correlates with #448 \n@dumbbell, @michaelklishin, do you think we should use our (more flexible) format or cuttlefish bytesize and get rid of parsing code?\n. cuttlefish supports translators, which can transform config values to any erlang term, so it will be no breaking changes at all, since we generate exactly same config.\nWe will include cuttlefish in our shell scripts anyway so there are two options:\n1. If there is any .config file - use it as is, otherwise - generate it from .conf and .schema\n2. Generate always. Use .config file as additional config to merge with generation result\nI will post more comments about collections and possible use of YAML, when finish with current config .\n. As I remember, Riak and Ejabberd use additional .config file for variables with too sophisticated structure.\n. Riak is not using YAML, they use this https://github.com/basho/cuttlefish\nThis library create .config every time riak is started from .conf file (in sysctl format) and advanced.config (erlang config format)\n. It is loaded by application controller just like regular config file. All validations are being executed during config generation with cuttlefish.\n. Riak use cuttlefish_rebar_plugin and shell scripts to generate app.config in .../generated.configs/app.<date>.config and run erl with -config pointing to this location.\nIf we will use cuttlefish env variable could point to .conf (e.g. RABBITMQ_CONF_FILE=rabbitmq.conf) file and do same thing - create app.config in some location and point erlang to it.\nI also looked at ejabberd config implementation. There are separate module that loads config from different sources and store it in mnesia. And there are ~1100 loc.\n. I'm researching cutterfish config generation for rabbitmq config right now and there are some questionable behaviour.\nCutterfish does not support list type in .conf file, so if user want to add list parameter, there should be either string parsing or multiple parameters, that will be merged in .config file.\nIt looks like this:\nUsing string parsing:\n.conf file:\nauth_backends = rabbit_auth_backend_internal, rabbit_auth_backend_http\n.schema file:\n```\n{mapping, \"auth_backends\", \"rabbit.auth_backends\", [\n    {datatype, string}\n]}.\n{translation, \"rabbit.auth_backends\", \nfun(Conf) ->\n    Settings = cuttlefish:conf_get(\"auth_backends\", Conf),\n    [ list_to_atom(Backend) || Backend <- string:tokens(Settings, \",\") ]\nend}.\n```\nUsing multiple parameters:\n.conf file:\nauth_backend.internal = rabbit_auth_backend_internal\nauth_backend.http = rabbit_auth_backend_http\n.schema file:\n```\n{mapping, \"auth_backend.$name\", \"rabbit.auth_backends\", [\n    {datatype, atom}\n]}.\n{translation, \"rabbit.auth_backends\", \nfun(Conf) ->\n    Settings = cuttlefish_variable:filter_by_prefix(\"auth_backend\", Conf),\n    [ V || {_, V} <- Settings ]\nend}.\n```\nBoth ways will generate same result, but I'm not sure which will be better for users to understand.\n. There is also an option to encode values to setting key. So previous examples will look like\nauth_backend.internal = true\nauth_backend.http = true\nWhich can be translated by translation function to same result. But in current example it is not so flexible, but maybe could be used in following example:\ndefault_tags.administrator = true\ndefault_tags.other = true\nSchema:\n```\n{mapping, \"default_tags.$tag\", \"rabbit.default_user_tags\", [{datatype, {enum, [true, false]}}]}.\n{translation, \"rabbit.default_user_tags\", \nfun(Conf) ->\n    Settings = cuttlefish_variable:filter_by_prefix(\"default_tags\", Conf),\n    [ list_to_atom(Tag) || {[\"default_tags\", Tag], V} <- Settings, V == true ]\nend}.\n``\n. So here iscuttlefish` schema and example config for this schema.\nhttps://gist.github.com/hairyhum/a73c30672acc55125643\nThere are rabbitmq.example.config in comments with examples and docs to both files above each setting.\nThere should be more validations to check config values and maybe some other mappings.\nYou can run cuttlefish escript with this files to generate rabbitmq.config.\n@michaelklishin @dumbbell @essen @Ayanda-D can you look at .conf file and say what you think about this way to define config?\n. Speaking about custom mapping, there are several ways to do hierarchical configs (like listeners), and some of them will require more patching.\nHere are listeners example:\nsysctl version:\nshell\nlistener.tcp.local = 127.0.0.1:5678\nlistener.tcp.host  = 10.0.0.10:5678\nYaml simple mapping (same as sysctl, just a different format\nyaml\nlistener.tcp.local : 127.0.0.1:5678\nlistener.tcp.host  : 10.0.0.10:5678\nYaml extended mapping (may require some cuttlefish patching)\nyaml\nlisteners:\n    tcp:\n        - 127.0.0.1:5678\n        - 10.0.0.10:5678\n. This plugin won't worry about grants or renewals. UAA will deal with all of it. Plugin only connect user authorization to UAA with token from client.\n. @michaelklishin what do you mean by 'fails to activate'? I have been testing the feature using this plugin.\n. I have set up clean umbrella clone, checked up rabbit, rabbit_common, rabbitmq_test, and rabbitmq_federation. Tests passed (except clustering management, which is unrelated).\n. Please post questions to rabbitmq-users google group.\nLooks like you should check node name of rabbitmq server node (you can use WMIC to show processes command line args) \n. What if we enable per vhost message stores for new vhosts only, and recommend to shovel messages for migration?\n. Initial research:\nThanks to rabbit_msg_store architecture enabling per-vhost grouping is pretty easy by starting message stores in subdirectories and controlling them with supervisor.\nMigration of old data can still be a problem, since there is no vhost information in message records.\n. Migration mechanism proposal:\n```\nmove_messages_to_vhost_store() ->\n    Queues = get_durable_queues(),\n    OldStore = run_old_persistent_store(),\n    Migrations = spawn_for_each(fun(Queue) -> \n                                    migrate_queue(Queue, OldStore) \n                                end, Queues),\n    wait_for_completion(Migrations),\n    delete_old_store(OldStore).\nmigrate_queue(Queue, OldStore) ->\n    OldStoreClient = get_client(OldStore),\n    NewStore       = ensure_new_store_exist(get_vhost(Queue)),\n    NewStoreClient = get_client(NewStore),\n    walk_queue_index(\n        fun(MessageIdInStore) ->\n            Msg = get_msg_from_store(OldStoreClient),\n            put_message_to_store(Msg, NewStoreClient)\n        end,\n        Queue).\n``\n. \"variable mode\" - is current default mode?\n. @michaelklishin @dumbbell @essen Can you please take a look atrabbitmq.confandschema` files? Maybe there are more validations or config options required. QA comments require discussion.\n. > cluster_nodes is (semantically) a list: do we support multiple values for it at the moment?\nThere is no support for lists. Or you mean using numbers as entry names, like in auth_backends?\n. Added generation to rabbitmq-server scripts. It checks file extension of RABBITMQ_CONFIG_FILE and if it's .conf' generates config with cuttlefish\n. There is a problem with windows service.start-service.bakscript installs service witherlsrvutility and when the service is started or stopped it doesn't call any shell scripts.\nSo using shell script approach to generate config doesn't work in case of windows services. \nLooks like we need to generate it in erlang code, maybe in boot steps.\n. Config generation is performed bycuttlefishescript which is called fromrabbit_configmodule before application load.\nAfter config generation app configuration is updated withapplication_controller:change_application_data/2`\nTo generate config init arguments -conf, -conf_dir, and -conf_gen_script should be passed to erl on start.\n-conf - .conf file (without extension)\n-conf_dir - generated config directory\n-conf_gen_script - cuttlefish script location. rabbitmq.schema should be in same direcroty\nIf -config argument was passed to erl and the .config file exists - nothing will be generated.\nIf -conf argument was passed and .conf file exists - try generate .config from arguments.\nIf generation successful - update applications config.\nUnix scripts pass -conf... arguments only if there is .conf file\nWindows service script always have -conf... arguments if there is no .config file, because it's impossible to change start args without service reinstallation.\n. Config update will require erlang 17.1\n. Added support to additional.config, which will be merged with generated config.\n. To have .config support plugins should provide .schema file in priv directory. But it's not clear how broker should locate this files: copy on plugin install or search in plugin dir when generating config. \nAlso when enabling new plugin perhaps broker should generate and load config for it.\n. Looks like plugin support can be harder than it seems.\nPlugins are extracted after application been loaded and .config file should be already generated, but if there are some plugin specific config parameters in .conf file it's impossible to generate config without plugin .schema\n. It's possible to extract .schema files only. \nIt's also possible to generate separate \"pre_boot.conf\" with log and hipe config only to prepare plugins and then generate from normal config with plugins. \n. Since kernel application cannot be reloaded, its options should be only set using .config. I'm starting to think that it would be easier to have windows specific behaviour.\n. We are always starting broker using rabbitmq-server on UNIX, but windows service cannot run bash scripts, so windows should have ability to generate config from working node. What I have in mind right now is either to restart init after config generation, or start with kernel in advanced.config and with something like -config advanced -conf_advanced advanced -conf rabbitmq ... and check if conf_advanced and config are the same to generate new config.\n. Erlang process is started with advanced.config if rabbitmq.config file is not exist. \nDuring installation on windows rabbitmq.config is being renamed to advanced.config\nPlugins schemas are extracted on config generation step. \n. Looks like it's close to production ready. Issues with windows is fixed. \nHow it works:\nEnv variable RABBITMQ_CONFIG_FILE is used to direct to either .config or .conf file.\nIf it's set to .config file - node is loading this file and works like previous versions.\nIf it's set to .conf - config file is generated in RABBITMQ_GENERATED_CONFIG_DIR using RABBITMQ_CONFIG_ADVANCED as advanced .config file and schemas from priv directory of rabbit and plugins. Plugin schemas are being extracted to priv directory of rabbit.\nOn UNIX if RABBITMQ_CONFIG_ADVANCED exists, it's passed to erl as config argument and loaded duting boot, so values for kernel or sasl can be used there. \nOn WINDOWS to run service RABBITMQ_CONFIG_ADVANCED should exist during service installation to be used. So installer create it or copy old version rabbitmq.config if it can be found.\nI'm not quite sure if schemas should live in priv directory.\nWhat's left:\nConfig regeneration on plugin enable/disable.\nDocs and examples\n. Added module to test config files generation. \nCan be used to test examples from website.\n. Added schemas for web-stomp and web-mqtt and examples for web-stomp.\n. New CLI tools repository:\nhttps://github.com/rabbitmq/rabbitmq-cli. Since plugins are erlang applications, the most obvious idea is to use application versions and format them in some way. But it can be not enough to determine obsolete plugins.\n. Then we can rely on application version. If it is formatted to some expression, that broker can parse. Maybe not the best idea, because it can violate some standards of application versioning.\n. It's possible to define custom application key in .app.src with something like {rabbitmq_versions, [\"3.5.6\", \"3.6.0\"]} wth minimal minor version for each major (major is \"3.6\"). And then check using rabbit_misc:version_minor_equivalent and rabbit_misc:version_compare.\nThis approach is based on assumption that we will not break any plugins by changing minor version.\n. I think we can make db bind type default and introduce environment variable to change it.\n. According to docs db and ts will have different behaviour only on NUMA nodes. But db is called default, so it's more expected.\n. @dcorbacho +stbt should be more safe. +sbt will fail to start VM if runtime cannot support scheduler binding or cpu topology, while +stbt will just ignore it.\n. Maybe it's related to fixes in https://github.com/rabbitmq/rabbitmq-server/pull/575 \nLook at.\nhttps://github.com/rabbitmq/rabbitmq-server/commit/bc3d5247c38e14e938bf8461889c52b15d188aaa\nand\nhttps://github.com/rabbitmq/rabbitmq-server/commit/0f5111cab23e900ccc6794b53f220de594a11c80\nAnd other non log or config related fixes in rabbitmq-service.bat\n. @Gsantomaggio you can also check installation when epmd is not running.\n. Looks like the same error I had when trying to install 3.7.x with epmd not running (e.g. after reboot). \n@Gsantomaggio you can try fixes from commits I mentioned above.\n. It's not recommended to use %ProgramFiles% for the application data, according to this guidelines. %ProgramData% is a better choice for service RABBITMQ_BASE. \nIn order to use users AppData for rabbitmq-server.bat data, we need to create a directory structure on each run, instead of installation.\n. Will it be enough to have a test for changing password_hashing_module application env, while having user with different hash_algorithm and checking that after password change this user can connect?\nWe don't need to check upgrade in this case and inject non-standard users.\nBecause it was not upgrade issue, but password edit issue. \n. In plugin .app.src rabbitmq versions will be defined like {rabbitmq_versions, [\"3.5.4\", \"3.6.0\"]}, which means that this plugin valid for versions \"3.5.4\" and higher \"3.5.x\" and all \"3.6.x\" versions.\n. Should we continue with this?\n. Maybe it makes sense to include version checks for plugins depending on plugins.\n. Here are possible error messages:\nRabbitmq version mismatch: \n[warning] Problem reading some plugins: plugin1: version mismatch, required: [\"3.5.0\",\"3.6.0\"], got \"3.7.0\"\nRabbitmq version is not specified:\n[warning] Some plugins doesn't specify required RabbitMQ version: plugin2, plugin3\nPlugin dependency version do not match: (specified required version for dep1 and dep2, dep1 doesn't exist, dep2 version do not match requirement)\n[warning] Some plugin versions do not match:\n    plugin1: \n        missing dependency: dep1\n        dependency version mismatch: dep2 required [\"3.5.6\", \"3.6.0\"], got \"3.7.0\"\nPlugin will not be loaded in case of version mismatch. But will be loaded if required version is not specified. There should be some way to inform about it.\n@michaelklishin @dumbbell @bdshroyer any ideas on improving messages?\n. Error message format example:\nFailed to enable some plugins: \n    rabbitmq_federation_management:\n        Dependency is missing or invalid: rabbitmq_management\n    rabbitmq_management:\n        Broker version is invalid. Current version: \"3.7.0\" Required: [\"3.6.0\"]\n    rabbitmq_shovel_management:\n        rabbitmq_shovel plugin version is invalid. Current version: \"3.7.1\" Required: [\"3.7.2\"]\nWhen running rabbitmq-plugins enable or rabbitmq-plugins set invalid plugins will not be added to enabled_plugins and error is printed.\nWhen plugins (or broker) are updated and server restarts with existing list of plugins - it will print message to log and start without this plugins.\n. I guess we can skip validations if plugin version is empty, assuming it's development version. Or just warn users that there are empty versions, which won't be checked.\n. @Gsantomaggio yes, it ignores service recovery settings. They seem like not working when erl crashes anyway.\nOther values for restart flag are described in erlsrv doc\nOnFail: This can be either of reboot, restart, restart_always or ignore (the default). In case of reboot, the NT system is rebooted whenever the emulator stops (a more simple form of watchdog), this could be useful for less critical systems, otherwise use the heart functionality to accomplish this. The restart value makes the Erlang emulator be restarted (with whatever parameters are registered for the service at the occasion) when it stops. If the emulator stops again within 10 seconds, it is not restarted to avoid an infinite loop which could completely hang the NT system. restart_always is similar to restart, but does not try to detect cyclic restarts, it is expected that some other mechanism is present to avoid the problem. The default (ignore) just reports the service as stopped to the service manager whenever it fails, it has to be manually restarted.\n. @Gsantomaggio installer has no information about RABBITMQ_BASE. Maybe file creation should be moved to rabbitmq-service.bat?\n. @Gsantomaggio  In theory it's possible to consider RABBITMQ_BASE during installation, but it's not about this issue, I guess.\nIt still can be discussed here: https://github.com/rabbitmq/rabbitmq-server/issues/626\n. I thought that file directory should exist for default file location. We can try to create directory for this file, but I don't know how to get directory for non-existent file in CMD.\n. The initial idea was to make default installation work with default config file location, so I think we can ignore this case.\n. Removed debug echo from shell script and added unquoting to bat scripts.\n. @binarin can wait_for_info_messages be a fold function to collect values, instead of just calling DisplayFun?\nThis will make testing easier and provide a way to use different output formats in future.\n. Fold function should allow to use current logic as well, if accumulator returned from DisplayFun will always be ok for example.\n. Unit tests are failing with\n{badrpc, {'EXIT', {{nocatch, {bad_argument, dummy}}, [{rabbit_amqqueue, info, 2, [{file, \"src/rabbit_amqqueue.erl\"}, {line, 599}]}, {rabbit_misc, with_exit_handler, 2, [{file, \"src/rabbit_misc.erl\"}, {line, 501}]}, {rabbit_control_misc, step_with_exit_handler, 4, [{file, \"src/rabbit_control_misc.erl\"}, {line, 70}]}, {rabbit_control_misc, '-emitting_map0/5-lc$^0/1-0-', 5, [{file, \"src/rabbit_control_misc.erl\"}, {line, 62}]}, {rabbit_control_misc, emitting_map_with_exit_handler, 5, [{file, \"src/rabbit_control_misc.erl\"}, {line, 58}]}, {rabbit_control_misc, emitting_map_with_exit_handler, 4, [{file, \"src/rabbit_control_misc.erl\"}, {line, 53}]}]}}}\n. Running command with invalid info keys from command line\n$ scripts/rabbitmqctl list_queues dummy\nListing queues ...\nError: {{nocatch,{bad_argument,dummy}},\n        [{rabbit_amqqueue,info,2,\n                          [{file,\"src/rabbit_amqqueue.erl\"},{line,599}]},\n         {rabbit_misc,with_exit_handler,2,\n                      [{file,\"src/rabbit_misc.erl\"},{line,501}]},\n         {rabbit_control_misc,step_with_exit_handler,4,\n                              [{file,\"src/rabbit_control_misc.erl\"},\n                               {line,70}]},\n         {rabbit_control_misc,'-emitting_map0/5-lc$^0/1-0-',5,\n                              [{file,\"src/rabbit_control_misc.erl\"},\n                               {line,62}]},\n         {rabbit_control_misc,emitting_map_with_exit_handler,5,\n                              [{file,\"src/rabbit_control_misc.erl\"},\n                               {line,58}]},\n         {rabbit_control_misc,emitting_map_with_exit_handler,4,\n                              [{file,\"src/rabbit_control_misc.erl\"},\n                               {line,53}]}]}\n. @binarin I think error formatting is out of scope of this issue.\n. Looks like this implementation doesn't support --online and --offline arguments.\n$ scripts/rabbitmqctl list_queues --offline name\nListing queues ...\nError: undef\n. Ok. It works with fresh master\n. We have started cluster with erlang versions 17.0 and 18.3.\nWe did tests on mnesia and global apps.\nThen we have started RabbitMQ cluster of two nodes (17.0 and 18.3) and run java functional and server tests.\nAlso tested a bit using management and mirrored queues.\nEverything worked for this setup.\n. Same tests work with R16B03.\n. According to mnesia changelog, it can have incompatible versions, but changelog is only way to know it.\n. Looks like mnesia have protocol_version, that can be accessed using mnesia:system_info(protocol_version). \nMnesia supports communication with two versions, so called current and previous. \nOnly current version can be accessed directly, while previous is using during mnesia_monitor:negotiate_protocol(Nodes) when adding new nodes.\nLast changes in protocols were:\n| OTP version | previous | current |\n| --- | :-: | --: |\n| R14B04 | 7,6 | 8,0 |\n| R15B | 8,0 | 8,1 |\n| 18.2.3 | 8,1 | 8,2 |\n. We can check known nodes for protocol compatibility by calling negotiate_protocol, which will return list of compatible nodes, so we don't need to explicitly check protocol version.\n. We can do this:\nget_incompatible_nodes(KnownNodes) ->\n    CompatibleNodes = mnesia_monitor:megotiate_protocol(KnownNodes),\n    KnownNodes -- CompatibleNodes.\n. By the way, current implementation does not require exchange and queue names to be ASCII. \nhttps://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_channel.erl#L834\n. Or we can just change conditions, because extension is being checked there anyway. So we can use any extension trimming mechanism.\n. Added support for advanced.config. Haven't checked windows scripts yet.\n. Maybe it should go to stable. I'm not sure.\n. Given release process we have right now, we can do this:\n- for plugins that ship with rabbitmq and automatically versioned, we can automatically set required broker version to current version during build.\n- for plugins that ship independently, we can set 3.7.0 and 3.6.0 as basic supported versions and update them manually if there are some incompatibilities or new rabbitmq minor version.\nAs the result, there will be broker requirement for exact same version in plugins, shipped with rabbitmq and custom requirements in independently shipped plugins.\n. @spatula75 can you try 3.6.3 milestone1 and tell if you can see the same increase?\nThere was a memory leak in stats collector, which let to binary memory usage increase.\n. @spatula75 regarding message store index, it's being filled with records about messages in transient message store file. This file is being GCed once it reach msg_store_file_size_limit and message store index table is being cleared. \nI'm not sure why simload.rb sends this messages to disk but it should not be a leak, because index is being cleared eventually. \nBut it could take up to 65MB of RAM for message store index table for 16MB file (default file size limit), which is still not good.\n. Fixed in #743 \n. Can you add some tests, that will check that exchanges and queues policy settings correspond to policy database after errors in this transaction?\n. @dumbbell but does it check that queues and exchanges are being updated after policy update? If policy had been updated, but update_policies, which is updating queues and exchanges parameters to correspond with policies, will fail?\n. As far as I can see from code, this transaction is not updating policy, but only exchanges and queues. Policy is being updated in another function https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_runtime_parameters.erl#L142. So if policy is updated and this function fails there will be inconsistency.\n. I'm going to file additional issue then.\n. I've tested with additional logs and it looks like not restarting transaction.\n. Looks like it detects cyclic errors and retries. But if we fail before this transaction there still be inconsistency mentioned in #756\n. @dumbbell will you add tests for #755 ?\n. If VHost is being deleted when set_policy is running there are {throw,{error,{no_such_vhost,<<\"...\">>}}} error. This error is being caught and propagated like {'EXIT', ...} instead of being matched by {error, {no_such_vhost, _}} \n. It should be this throw\n. I agree, it makes sense to put schema files to /var/lib/rabbitmq\n. Please use rabbitmq-users or stack overflow to ask questions. \nAMQP model doc explaining connections lifetime can be found here: https://www.rabbitmq.com/tutorials/amqp-concepts.html\n. Queue indexes was also moved to vhost storage directory, so it's easier to distinguish queues from different vhosts. \nNew queue index location: mnesia_dir/queues/:queue_id -> mnesia_dir/:vhost/queues/:queue_id\nSo directory mnesia_dir/:vhost effectively contains all messages data.\nIf vhost is deleted - all it's messages data is deleted, including directory.\nIt's possible to delete all vhost data by running rabbit_vhost:purge_messages/1, that will delete vhost message store and all it's queues indexes. It's not safe operation.\nTesting\nMigration to per-vhost message store mechanism can be tested using make in https://github.com/rabbitmq/rabbitmq-server-release/tree/stable/upgrade.\n. Rolled back to the sequential queues migration.\nMove the queues indexes to the new directory before the messages.\nNow it seems like actually moving the messages.. Probably it makes sense to migrate queues in batches.. Queues are migrated in batches of 100 in parallel by default. The batch size can be configured with queue_migration_batch_size environment variable.. Per queue migration messages are logged to rabbit_upgrade.log file.. @dhaval-gusani \nThere are \nheaders.put(\"x-death\", \"[{queue=some-queue, time=Thu Mar 17 22:44:51 UTC 2016, count=1, reason=rejected, routing-keys=[some-queue], exchange=}]\"); line that sets x-death header to string of some format. x-death should be array of tables and it doesn't look like java client can transform this string to array.\nCan you tell where did you get this string?\n. @binarin can you please resubmit this against stable?\n. Accidental PR to master\n. Key will not be added. Only empty list will be replaced. I will submit PRs for plugins with empty requirements\n. @johnfoldager please create rabbitmq-users google group thread to discuss your errors with LDAP and 3.6.2.RC1 and post all logs there.\n. @johnfoldager It looks like all this errors are caused by plugin application not being properly loaded. You can try to re-enable LDAP plugin and restart broker.\n. Can you check if this errors are still logging and tell how did you installed this version?\n. This looks like ldap_pool worker pool has failed to start. This can be caused by error in plugin enable process. If you restart broker are there still errors?\n. Restarting nodes one by one looks like solution for you.\nIssue itself looks like caused by some errors in installation and plugin enabling process.\nHave you installed it from 3.6.2.RC1 deb package, or updated from older version using this package?\n. If it works correctly after broker restart - it will work correctly after maintenance restart as well.\nWhat we will try to investigate is why installation failed to enable LDAP plugin.\nCan you tell how did you configured plugins after installation?\n. Ldap plugin will restart connections that is failing. I cannot tell if your errors are caused by your LDAP connection process, but it's unlikely.\nIf you post logs from startup after installation and initial plugin enabling - we can investigate.\n. Please submit logs from installation and plugin enable process done by your scripts to rabbitmq-users.\n. When server is freshly installed and plugins are being enabled there should not be too much connections yet.\n. Did they connected before plugins are being enabled?\n. @johnfoldager please post next time to rabbitmq-users, so we can discuss there and then extract meaningful issues to GitHub. \n. There is nothing wrong with message ID printed as <<\"\u00ab:l\\r=}j!(U\u00b2\u00df\u00b8tv\u00b9\">>. It is generated binary so it can take almost any form.\nPlease post questions to rabbitmq-users or Stack Overflow. Thank you.\n. application:which_applications uses 5000 timeout by default, should we specify other timeout somehow?\n. gen_server:call has default timeout 5000.\n. There is another use of which_applications\nhttps://github.com/rabbitmq/rabbitmq-federation/blob/master/src/rabbit_federation_queue_link.erl#L62\n. Should we add explicit timeout to rabbit_misc:which_applications as well?\n. Please ask questions in rabbitmq-users google group or Stack Overflow. RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions. Thank you.\n. rabbit_version requires rabbit_upgrade and directory structure in it's functions. Maybe only some functions can be moved to rabbit_common instead? \n. Supervisors were failing to stop children because of too big timeouts. #844 \n. Addressed by https://github.com/rabbitmq/rabbitmq-msg-store-index-eleveldb. And this is the reason why message store index is bigger than message store file.\n. Initial commit of this write https://github.com/rabbitmq/rabbitmq-server/commit/1111d2a20da7502d325b95f1d4357c35fc6171cc\n. This messages for dying clients are required to ignore other messages from the same client, which can arrive after client_dying. But only index records for client termination was used in this check, so it's possible to implement additional index for client terminations only and to not fill primary message index.\n. This issue is related to #930\nIn #930 is policies can be merged with \"operator policies\", but not with each other.\nSo there will be zero or one policy and zero or one operator policy, which will be merged for each queue.\nSince we have the extensible merging mechanism and the UI for displaying effective policy definitions we can consider merging policies as well.\n. We do get the list of plugins dependencies during enable/disable, so we know which applications are used by other plugins.\nBut there is no way to know if application has been started as a dependency or using application:start \n. New config format does not support tuples.\nLinger setting can be split to linger_on(boolean) and linger_timeout(integer) settings.\n. Linger now can be defined by options:\ntcp_listen_options.linger.on = true\ntcp_listen_options.linger.timeout = 10\n. I've found it in the OTP-19 changelog\n```\nOTP-13425    Application(s): erts\n           A bug has been fixed where if erlang was started +B on\n           a unix platform it would be killed by a SIGUSR2 signal\n           when creating a crash dump.\n\n. This seems like it https://bugs.erlang.org/browse/ERL-94\n. This will also require changes in `rabbitmq_cli` plugins discovery. \n. @binarin any update on the comments above?. When I add a new plugin version to one of the plugins directories, this version is displayed by `rabbitmq-plugins list` for the enabled plugin, while the actual application version `application:get_key(<plugin>, vsn).` can be lower. This can be confusing.\nFor example:\nThere is the `rabbitmq_management` plugin with version `3.6.6`enabled.\nI add the  `rabbitmq_management-3.6.7.ez` plugin archive to the common plugins directory.\n`rabbitmq-plugins list` displays\n[E*] rabbitmq_management               3.6.7\nWhile the application version is `3.6.6`. Will there be multiple validators enabled at the same time? If so, can we make validator a pair of functions instead of a module?\nSo we can add new validators like this:\nadd_validator(fun default_validators:validate_length/2, fun default_validators:validate_length_message/2)\n```\nAnd plugin will be able to enable/disable validators on application start/stop.. But the issue happens when we have remote PID in it, so this specific case is not node-local.\nMore context:\n\nstart rabbitmq,\nstart erlang client in separate erlang node,\nopen a connection using amqp_params_direct\nopen a channel\nwait \nsearch logs for:\n2017-02-24 18:11:43.449 [info] <0.11098.4> Stopping RabbitMQ\n2017-02-24 18:12:49.491 [error] <0.11090.4> ** Generic server rabbit_core_metrics_gc terminating \n** Last message in was start_gc\n** When Server state == {state,#Ref<0.0.7864323.113940>,120000}\n** Reason for termination == \n** {badarg,[{erlang,is_process_alive,[<27525.720.0>],[]},{rabbit_core_metrics_gc,gc_process,3,[{file,\"src/rabbit_core_metrics_gc.erl\"},{line,104}]},{lists,foldl,3,[{file,\"lists.erl\"},{line,1263}]},{ets,do_foldl,4,[{file,\"ets.erl\"},{line,585}]},{ets,foldl,3,[{file,\"ets.erl\"},{line,574}]},{rabbit_core_metrics_gc,gc_connections,0,[{file,\"src/rabbit_core_metrics_gc.erl\"},{line,62}]},{rabbit_core_metrics_gc,handle_info,2,[{file,\"src/rabbit_core_metrics_gc.erl\"},{line,42}]},{gen_server,try_dispatch,4,[{file,\"gen_server.erl\"},{line,601}]}]}\n2017-02-24 18:12:49.491 [error] <0.11090.4> CRASH REPORT Process rabbit_core_metrics_gc with 0 neighbours exited with reason: bad argument in call to erlang:is_process_alive(<27525.720.0>) in rabbit_core_metrics_gc:gc_process/3 line 104. But if this non-local process will be killed without an event it can still be lost, right?. rabbit_file:write_file creates a \"backup\" file and renames it. https://github.com/rabbitmq/rabbitmq-server/blob/stable/src/rabbit_file.erl#L169\n\nThis was introduced to avoid an issue when erlang process (presumably) dies after file truncation but before write. bugzilla#25617\n. Missing parts:\nMigration between message stores.\nTests for pluggability.. Migrations between message stores are disabled by checking the old store name when starting the new one. If it's different - message store will fail to start.\nNeed rebase after https://github.com/rabbitmq/rabbitmq-server/pull/1158. So we need to proceed with per-vhost supervisors then.. Cannot reproduce. Need more data on prerequisites from @gerhard \nDownloaded 3.6.8 generic unix.\nConfigured /ets/hosts to see rmq0 and rmq1 hosts.\nSet environment variables:\nexport RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS='-setcookie foo'\nexport RABBITMQ_NODENAME=rabbit@rmq0 #rabbit@rmq1 on other node\nRun rabbitmq server in generic unix dir:\nexport RABBITMQ_HOME=`pwd`\nsbin/rabbitmq-server\nChecked cluster status:\n```\nRABBITMQ_CTL_ERL_ARGS='-setcookie foo' sbin/rabbitmqctl -n rabbit@rmq0 cluster_status\nCluster status of node rabbit@rmq0 ...\n[{nodes,[{disc,[rabbit@rmq0,rabbit@rmq1]}]},\n{running_nodes,[rabbit@rmq1,rabbit@rmq0]},\n{cluster_name,<\"rabbit@MacBookdfedotov\">},\n{partitions,[]},\n{alarms,[{rabbit@rmq1,[]},{rabbit@rmq0,[]}]}]\n```\nChecked cookie:\n```\nRABBITMQ_CTL_ERL_ARGS='-setcookie foo' sbin/rabbitmqctl -n rabbit@rmq0 eval 'erlang:get_cookie().' \nfoo\nRABBITMQ_CTL_ERL_ARGS='-setcookie foo' sbin/rabbitmqctl -n rabbit@rmq1 eval 'erlang:get_cookie().' \nfoo\n``\n. Upgrade from3.4requires old versions of some functions. We should require upgrade to3.6and then to3.7. I think...is supposed to mean \"command is not finished yet\". And commands supposed to have a return value which will be printed after command  finishes. But it's not true anymore.. Just an explanation. Dots after each log do not make much sense.. Looks good. Though I'm a bit concerned about usingplugins_dist_dirin theactive/0function, because it's unzipping an.appfile every time. On other hand, the function is not widely used (only in plugin operation), so it should be fine.. I wonder how would escaping work. For example when username is something likeuser/name.. @gerhard making such usernames not allowed will create tons of problems, because they are allowed now.. Then we should just mention the slashes issue in the topic authorization docs.\nOtherwise it totally makes sense and should be straightforward. . Apparently, it's related to+Biflag. If it's set, theSIGUSR1signal will kill the node, but won't generate a crash.dump. I don't think it's related to OTP-13425 as we don't sendSIGUSR2.\nWe still should verify all signals behaviour with and without+Bi` in different erlang versions, because it seems like Erlang team was working on it in recent versions.. In that case there is not much difference between 3.6 and 3.7, but limiting fix to 3.7 still makes sense.\nThe problem here is in this line https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_queue_index.erl#L694 \nWe get term_to_binary from a record with atoms inside and then calculate MD5 hash of it.\nOTP-20 can decode binaries encoded in OTP-19, but it doesn't encode them the same way. And because we do MD5 we cannot decode it.\nI can see 3 options for us:\n  - Migrate dir names by generating the old binary format ourselves (from OTP-20)\n  - Require OTP-19 to migrate them\n  - Convince Erlang core devs to return backwards-compatibility option.\nThird option can be hard, due to RC process, but could make sense not only for us, but other Erlang users as well.. Discussion in erlang bugtracker https://bugs.erlang.org/browse/ERL-431. OTP team does not want to ad options to term_to_binary, recommending us to hand-craft the \"old format\" values.. The critical parts are queues and vhosts names. We also have several places across plugins which use term_to_binary, but they could be safe (needs verification):\n- rabbit_pbe\n- rabbitmq_clusterer\n- federation status\n- bindings management\n- stomp subscription_queue_name. Stomp and management issues are not node-local, which makes it impossible to apply the changes in 3.6, so it's definitely 3.7. As a workaround for 3.6.x, we can reproduce old term_to_binary. There is no breaking changes in MD5. It's just the fact that we use MD5 to obfuscate binaries.. Just for queue names, the function should be relatively simple. Something like\nterm_to_binary_legacy({resource, Vhost, queue, Name}) ->\n    VLength = byte_size(Vhost),\n    NLength = byte_size(Name),\n    <<131,104,4,                              %% 4-element tuple\n      100,0,8,114,101,115,111,117,114,99,101, %% 'resource' atom\n      109,VLength:32,Vhost/binary,            %% VHost binary\n      100,0,5,113,117,101,117,101,            %% 'queue' atom\n      109,NLength:32,Name/binary>>.           %% Name binary. Since https://github.com/erlang/otp/commit/48e67f5dd1d20b9a1f78c5a97cc7ed4afb489ba5 the old term_to_binary format was brought back, so presumably we won't have problems during upgrades. Still need to investigate.\n3.6.11 now should use term_to_binary/2 to generate a fixed old version of binaries.. Fixed tests, which were using strings instead of binaries for queue names.. Addressed in https://github.com/rabbitmq/rabbitmq-common/commit/63c0dae9f3ede0321b34c1118624c116198479cc. @dumbbell \nLog in to long-running-environment\n``\nps -ppgrep beam` -o rss\nRSS\n450784\nrabbitmqctl eval 'erlang:memory(total).'\n170767520\n``. Replaced by #1268. Should we consider adding VHosts synchronously?. It seems like it's actually added concurrently, but when it's deleted the watcher detects deletion before it's stopped, so it takes the supervisor down abnormally instead of waiting for it to stop normally.\nDisabling watcher before deleting a vhost from the mnesia db should fix this.. Disabling the watcher makes not much sense, making it stop the supervisor usingrabbit_vhost_sup_supAPI instead of restarts should do the trick.. Few questions. Hope this review will not be lost.\nAnother thing to be tested is how resilient thetransientstrategy to data corruption.. @michaelklishin when vhost data directory disappears, unaccessible or when disk is full. . Are there any performance tests for this change? Message store write operations are going through the single process and it can be a bottleneck with large number of small messages. . @michaelklishin can the test be replayed with the current version? Let's say with 100-1000 lazy queues. . When pressing TAB to autocomplete a command, the same error occurs. Probably because autocompletion runs withoutsudo. Maybe we should make surerabbitmqctldoes not fail with this error unless access to theenabled_pluginsfile is required.. Test failures should be fixed. Not sure about naming though. Could be juststate.. Observation:\nWhen running a lot ofwmicprocesses in concurrently, it takes longer to return and a singleWmiPrvSE.exeprocess consumes most CPU. Same fortasklistandget-process(powershell)\nAssumption: All this commands call a blocking API.\nHypothesis: Theget_system_process_resident_memoryis called viaget_memory_usefrom many queues checking if it's the time to page messages to disk.\nProposed experiment: start a rabbitmq windows server, create many queues and start publishing transient messages to hit the paging ratio. Monitor CPU usage ofWmiPrvSE.exeand a number ofcmdprocesses.\nProposed fix: memoize theget_system_process_resident_memoryfunction with ets and fixed update period.. The experiment failed. Simple queues workload does not create much pressure to memory collection. The queues are using cached values from rabbit_node_monitor. \n@atroxes @lasfromhell can you please post more information here about the workload you have?\n- number of queues, channels, connections\n- publish/consume rates\n- message types (persistent/transient) and if queues are lazy\n- message avg sizes\n- frequency of newcmdandwmicprocesses creation/completion.file_handle_cacheis callingvm_memory_monitor:get_memory_useon (some) reads, and since reads are performed by the message store the queue index, it can related to workload.\n. Or we should not use system memory reporting in hot code paths likefile_handle_cacheat all, because it's not parallelizable.. I believe that the PR should fix the issue, but I'm unable to reproduce it. \n@atroxes @lasfromhell I've produced a package with this fix, any chance you can test it in your environment? \nhttps://s3-eu-west-1.amazonaws.com/rabbitmq-share/builds/rabbitmq-server-3.6.12.rc3%2B2.g9086607.exe\n. Logging in Windows should work the same way, but apparently we had a typo in the.batfile, which made it think that-should be treated as a file name. Fixed in the linked PR.. Please note that for windows service console logs are still disabled, because service console is not available.. Should we instead considerrabbitmqctl stopsuccessful if the node is not running?. Sorry, the exit code fromrabbitmqctl stopcommand is 0 instablehttps://github.com/rabbitmq/rabbitmq-server/commit/9cfd5bd64edd527db418ea2cbbb35d53054bfad5 . \nAnd it was changed to 69 inmaster. \nI'm not sure why does the service fail then.. This will change therabbit_backing_queuebehaviour and cannot go to a patch release. That's why I used the process dictionary.. The callback is actually necessary, because not everyresumemeansreceived bump, so there can be another callback, which will set the flag and doresume. \nThe flag here makes sure that there is only one bump message in the message box, so the message box is not polluted. Every time queue process handles the bump message it shouldresume`, but not every time it resumes it should clear the flag.\nIf there is another way to make sure we have a single bump message in the message box at a time, it can be used instead.. Hi,\nI believe this is a hex.pm issue. Try deleting your mix and hex cache (often located in ~/.mix and ~/.hex directories) and run make again\nI'm closing the issue, because we try to keep github issues for feature requests and bug reports. Please post following questions to the mailing list \nThank you.. Hi,\nWe try to keep github issues for feature requests and bug reports only.\nPlease post your question to rabbitmq-users mailing list. \nAlso I would suggest to add more logs and explain the context of this error: what happened with the nodes, who tries to declare a queue, also more logs from all the nodes would help.\nRabbitMQ version 3.6.1 is not officially supported and has a number of bugs in mirroring, we strongly recommend to upgrade to the latest 3.6.x at least.. Rebased to 3.7. With all_running/0 changes reverted a broker can now report that a vhost is failed on some nodes in the management UI when a node is restarting. This is still better than returning 500 and polluting logs.. I think I've made a mistake when created a separate ETS tables for vhost Pid registry. It should have went to the vhost table in mnesia. It's not that mnesia is easier to manage, but at least it's consistent with itself.\n@michaelklishin what do you think of moving vhost Pids to mnesia? Of course we cannot do that in 3.7 anymore, but can be something for 3.8 and above. So that will be a separate PR.. Why is the port constant? We allow to configure rabbitmq distribution port, why not rabbitmqctl?. I did some measurements. Having 100K queues it takes around 54 seconds (without any workload) to remove them on 8 cores. If we skip stats cleanup it takes 53 seconds. Without removing data from mnesia - around 13 seconds. And without cleaning up bindings - around 13 seconds. So the main mnesia contention happens on bindings cleanup and cannot be optimised by batching the operations. It can be optimised in the bindings cleanup code though and will benefit any queue deletion.. Benchmark data for the change. Collected in 10 manual runs.\n| Topology                              | Action           | with table locks & non-ditry ops | with record locks and dirty ops|\n|---------------------------------------|------------------|----------------------------------|--------------------------------|\n| 100000 exclusive queues               | Connection close | ~53 500 millisec                 | ~22 200 millisec                      |\n| 10000 exchanges, 1 queue              | Queue delete     | ~830 millisec                    | ~260 millisec                  |\n| 10000 queues, 1 exchange              | Exchange delete  | ~620 millisec                    | ~350 millisec                  |\n| 1 queue, 1 exchange, 10000 bindings   | Queue delete     | ~610 millisec                    | ~250 millisec                  |\n| 1 queue, 1000 exchanges, 100 bindings | Queue delete     | ~6 450 millisec                  | ~2 670 millisec                |\n| 1000 queues, 1 exchange, 100 bindings | Exchange delete  | ~6 670 millisec                  | ~2 450 millisec                |. @michaelklishin Considering locks in mixed-version clusters. In the current code there is no update operation on bindings. Deletes should be idempotent which leaves add operation as the only one which can conflict with dirty deletes. In the current code add operation is performed in transaction with a resource check - read from mnesia on both resources, and cleanup is in transaction with the delete operation on one of the resources which is being deleted. This means that the \"add\" transaction is supposed to be locked on the deleted resource. In theory the global lock is not needed here, but it is there to avoid leaking abstractions if we change the code in future.. I'm not sure it will be useful in that specific case, because there is the name field already enough to identify a connection. And it is in fact used in the management for that. Still might be useful for something else.. Removed date from the format configuration. Pid is still there because syslog backend does not detect pid from the lager metadata (because it's a string).\nI've made syslog a dependency of rabbit, this might do the trick with the shutdown error.. Should it also be in rabbitmq-service.bat?. As I recall, this was added on purpose, because the service.bat script is executed on install, while the server.bat script is executed on server start. So we can't assume that rabbitmq.conf is there, because installation process does not create it.\nConsidering previous questions from users I start to lean towards no file check solution by default (like with service.bat).\nThis will allow users to create an advanced.config file only if they want and make it a bit more consistent. This also allows us to stop using rabbitmq.config filename, which is too similar to rabbitmq.conf.\nOf course we will have to handle and log the fact that the file is not there.. Didn't run all tests yet, but the behaviour should not be different.. @lukebakken I keep discovering single closing curly braces like $RABBITMQ_MNESIA_BASE}/$RABBITMQ_NODENAME. It feels like there can be more typos like this.\nWhy are you removing the braces?. The integration tests are in rabbitmq-ci branch config_files_check https://github.com/rabbitmq/rabbitmq-ci/pull/22\n. It makes sense. Some other variables like log configuration, alloc args and listener args can also go there.\nAlso .bat files will mostly benefit from that.. I understand that the original assertion was there to fail if an operation fails on a live queue Pid.\nThis behaviour will retry the operation if it's failing and then fail with the timeout error.\nIs there a way to make this behaviour better? Running it for 2000 times and then reporting timeout doesn't look less confusing than the failed assertion.. Maybe. Mirroring does some waiting to avoid false-positives. Unfortunately F is an arbitrary function so we don't know why it fails and it's not logged.. The fact that retry_wait is not called for every failure makes me wonder what is the difference between mirrored master and non-mirrored queue process becoming available again? I thought that the handler is called on any error. . The main issue was not the linear scan, because match_object optimise on key scans, but the fact that entire table had to be locked on every deletion, which made concurrent queue deletion transactions block each other. \nAlso queues without any bindings were bound to default exchange, those bindings were never read during routing (default exchange has it's own routing logic) and were slowing down the cleanup.\nThe #1715 PR contains some optimisations addressing that. Exclusive queue cleanup benchmark:\nTime to cleanup queues on connection:\n| 100K queues | No explicit bindings | 1 binding | 2 bindings |\n|--------------|---------------------|----------|------------|\n| Before            | 57.7 sec                     | 77.9 sec  | 107.1 sec   |\n| After               | 20.9 sec                    | 25.7 sec  | 30.5 sec   |\nSince default bindings are a separate PR now, this numbers should be shifted right for the \"after\" row. Still there is an optimisation.. Time to delete queues/exchanges:\n|  | 10K exchanges 1 queue | 10K queues 1 exchange | 1 exchange 1 queue 10000 bindings | 1 queue 1000 exchanges 10 bindings|\n|-|-|-|-|-|\n| Before | 3.7 sec | 3.8 sec | 4.1 sec | 3.6 sec |\n| After | 0.979 sec | 0.891 sec | 0.765 sec | 0.79 sec|. This can be backported to 3.7 if accepted.. The interesting thing about default bindings and min-master locator is that it's counting queue masters looking at bindings, but all queues were bound. Originally it was counting bindings and selecting a node which min bindings to the queues on the node. Then it was changed in https://github.com/rabbitmq/rabbitmq-server/commit/1d413eac80c2adb6844b82c8ce57e21a04a55b04 to count multiple bindings for the same queue as one. The assumption was that we should not account for unbound queues. But  any queue had at least 1 binding (default).\nI'm going to make it count queues, because it will be the same behaviour.. The test now passes.. So default bindings are shown in the management API. I've missed that somehow. I still don't think we should store them in the database. Maybe we could fake them for the management API\nI'm going to create a separate PR with default bindings removed. . Moved default bindings removal to a separate PR since most of the failures are about that change. It might be even for 3.8.. @michaelklishin maybe. That's why it's a separate PR.. Merged to 3.7. According to the API docs gen_udp:open is required to send messages via UDP protocol. Although by default it's configured to be active, which is not so nice if something starts sending messages to the port. That could be something to address in the syslog app.\nFrom the RabbitMQ perspective, the syslog app may be configured to not listen to any ports unless the syslog logger backend is enabled.. There was a typo-like error. Instead of finding the smallest value in the list with additional value, it was comparing the list with that value. Oops.. TODO left in the code is not new. I still would like to create a test to reproduce the error though.. Added test in 3ae27d2e5a0128c65d1e1312a23ef72103298e9a. Required for https://github.com/rabbitmq/rabbitmq-cli/pull/263. I wonder why -q parameter is specified by default? I know it was there before this change, but why? Also on windows it's not default.. It seems like rabbit_binding:list is used in bindings load in both list_bindings and HTTP API. Also rabbit_binding:list_for_source, rabbit_binding:list_for_destination and rabbit_binding:list_for_source_and_destination. We can add fake bindings there for amq.default and queues.. Fake bindings should have fixed that. I'll take a look.. Fixed rabbit_binding:exists.. Apparently the same thing happens when you run rabbitmq-server.bat and it fails to start. So it also should be fixed.. Good find. Do you think it should be added to other bat scripts?\nIt can definitely be added to rabbitmq-plugins.bat and rabbitmq-diagnostics.bat, maybe in rabbitmq-server.bat as well.. Should exclusive consumer be internally renamed to single active consumer? It can be confusing to read the code otherwise.. Should it be handled in the Elixir code instead? https://github.com/rabbitmq/rabbitmq-cli/blob/master/lib/rabbitmq/cli/core/distribution.ex. It gets the node name type from the arguments https://github.com/rabbitmq/rabbitmq-cli/blob/master/lib/rabbitmq/cli/core/distribution.ex#L35. The script with $$ is actually a good idea. If we use and operating system PID instead of randomly generated number for the CLI nodename it could work faster and easier.\nAnother thing is RABBITMQ_NODENAME in config.ex is used as an equivalent to the --node argument. It's true only for the local node if the --node argument is not specified. To generate a CLI nodename the env variable may be used, but it should not merge with the --node argument as it is for other uses.. Should we consider adding some human-friendly configuration format (for example 128MB), as it is for memory limits? . @michaelklishin no, it's using ets tables https://github.com/erlang/otp/blob/master/lib/kernel/src/application_controller.erl#L332 \nAnd it does not need to get a group leader, because appliction name is explicit in this case.. Fair enough. The value can technically change during a channel lifetime, but that's not a public API, so we can move it to the channel state.. Make it load durable queues and remove durable bindings. When moving to feature flags we will need to load all the queues and remove all the bindings.. It probably makes sense to test to which extent RabbitMQ stays usable in general after partitions.. @Avivsalem Can you please check if you have lines like this in the logs:\nQueue <...> failed to initialise\nAnd maybe even something like that:\n{function_clause,[{rabbit_variable_queue,d,[{delta\nThat would confirm the findings from other people with the similar issue.\n. @kjnilsson does this sound right?. Separate PR for 3.7. @Ayanda-D this PR will fix binding not found errors only if there were failed queues. You can check that in logs searching for Queue <...> failed to initialise in the logs.\nThe option to allow recovery of bindings does also make sense. I was thinking of letting queue.bind to create bindings even if there is an inconsistency, because that's what clients use to recover the topologies.. @michaelklishin some exchanges have callbacks, which are triggered inside/outside of transaction when a binding added or deleted. I'm not sure if there will not be any side-effects like there were with https://github.com/rabbitmq/rabbitmq-server/pull/1589\nThere should not be, but still worth checking.\nYes, for testing it should be enough to delete rabbit_route records (and/or rabbit_reverse_route) while there are rabbit_durable_route records. UPDATE:\nChanged the behaviour to track partial confirms per queue to handle mirrored queues failures only.\nWith this change a message may be confirmed if it was confirmed by at least one process from each queue it was routed to.\nAlso tolerate queue deletion separately by confirming on normal exit reason.. Test: binding a queue to exchange 10 times in each process with different routing keys.\nTested on the node, calling rabbit_binding:add for the same source and\ndestination with different keys.\nTime in microseconds, collected by timer:tc\n| | | Without the change | With the change |\n|-|-|--------------------|-----------------|\n| 100 processes | 1000 bindings | ~420 ms | ~350 ms |\n| 500 processes | 5000 bindings | ~1450 ms | ~650 ms |\n| 1000 processes | 10000 bindings | ~2780 ms | ~1080 ms |\n. More testing:\n| | | Without the change | With the change |\n|-|-|--------------------|-----------------|\n| | | | |\n| 1 node | | | |\n| Durable | | | |\n| 10 processes | 100 bindings | ~260 ms | ~210 ms |\n| 100 processes | 1000 bindings | ~400 ms | ~380 ms |\n| 1000 processes | 10000 bindings | ~2100 ms | ~1380 ms |\n| Transient | | | |\n| 10 processes | 100 bindings | ~30 ms | ~10 ms |\n| 100 processes | 1000 bindings | ~250 ms | ~75 ms |\n| 1000 processes | 10000 bindings | ~2000 ms | ~700 ms |\n| | | | |\n| 2 nodes | | | |\n| Durable | | | |\n| 10 processes | 100 bindings | ~320 ms | ~230 ms |\n| 100 processes | 1000 bindings | ~1230 ms | ~460 ms |\n| 1000 processes | 10000 bindings | ~11700 ms | ~3310 ms |\n| Transient | | | |\n| 10 processes | 100 bindings | ~100 ms | ~23 ms |\n| 100 processes | 1000 bindings | ~860 ms | ~200 ms |\n| 1000 processes | 10000 bindings | ~8300 ms | ~2100 ms |\nAs expected, with more nodes the difference is more visible.\nI'm getting convinced that the change is good enough performance-wise.. There are interpret_limit in both rabbit_disk_monitor and vm_memory_monitor, maybe it could be merged some way. \n. Then it would be reasonable to move parse_limit somewhere.\n. Sleep wait for lager to switch to new files. Nasty solution, but log rotation are called only from rabbitmqctl (should be deprecated maybe?) \nLage internal log rotation is much nicer because support file handle management and multiple rotated files, size, date and other settings. \n. -- Comment from outdated diff --\nSleep wait for lager to switch to new files. Nasty solution, but log rotation are called only from rabbitmqctl (should be deprecated maybe?) \nFunction itself required for backwards compatibility because used in logrotate scripts in Linux packages. \nLager internal log rotation is much nicer because support file handle management and multiple rotated files, size, date and other settings. Maybe we should recommend to use it instead?\n. rabbit_misc:random is undefined.\nrandom:uniform can be used in this case, because it just retries in same process.\n. Agree that a behaviour would be helpful. Not sure what function names should be there. Maybe straightforward added_to_registry, removed_from_registry?\n. This make task should be moved to rabbitmq-server script to enable config update. Will there be  erlang.mk at this point?\n. Package building should generate escript to be used after installation to update config.\n. There is no support for multiple interfaces in management plugin.\nhttps://github.com/rabbitmq/rabbitmq-management/blob/master/src/rabbit_mgmt_app.erl#L28\n. It's not only for us. I think mentioning the env variable possible source is a good idea. \n. Sorry, commited accidentally.\n. There will be \nstopping -> ..., false;\n    starting -> ..., true;\n    yes      -> true;\n    no       -> false\nDoes it look better?\n. {CWD}/Mnesia.{NODENAME}\nBut it won't be created until schema with disc_copies is created. By default schema will have only ram_copies.\n. In previous versions (long time ago), rabbit_mnesia:node_info was returning 4-element tuple. 3rd element was delegate_beam_hash - beam md5 hash of delegate module. \nThis commit adds 4-element tuple back, where 3rd element would be protocol consistency. Type match is here to avoid confusing one with another.\n. Will check OTP consistency if mnesia consistency is unsupported.\n. should it be mnesia:abort/1 instead of exit/1?\n. If there are multiple rabbitmq versions installed, plugins from both will be added to this variable. \nShould there be just predefined \"common plugins directory\" e.g /usr/lib/rabbitmq/plugins instead?\n. Ok, I've missed that. But I still don't think the script should look in directories outside of possible install prefix and predefined paths.\n. If there are several plugins with different versions in the plugins path, smallest version will be picked (because of file sorting). It's inconsistent with standard erlang code loading behaviour and is not what user would expect when installing newer version of plugin.\n. It's called alpha_info in rabbit_semver. {error, Message} can return an arbitrary error reason and will only log it. {error_message, Message} will also add it to the protocol error sent to a client.\n. How about clean_up_entries_with_undefined_file, to clarify that the file field should be undefined?. The function itself is an implementation detail. And I would like to point it out. The better name would be clean_up_temporary_reference_count_entries_without_file. This line will never match. application:get_env/3 returns value itself or a default value.\nDid you wanted to use application:get_env(rabbit, lager_log_root) instead?. Is that a duplicate call? prep_stop is supposed to be called by stop_apps.. What is the purpose of supporting lists here?. It cannot and should not be forward-compatible.\nThe function is a workaround and in 3.7 we should get rid of it. The code assumes that the record will be a tuple of this specific format. The match asserts that we haven't changed the record and the function still makes sense.. I don't think we need ignore anymore, since it's not released yet.. Is this comment still relevant?. Message store recovery failures on freshly started node or when vhost is just added will not cause application to stop. How can we mention that?. Should we raise the exception here? It's not safe to continue recovery process if the vhost supervisor is not found.. Should there be error? rabbit_amqqueque:recover/1 matches _, but it's still not right.. Is it OK to call a function, that an exit safely_...?. We can get here only if a vhost is added and deleted concurrently.. Do we know that vhost no longer exists at this point?. It would be questionable in rabbit_vhost_sup, but not here.. The function cannot return ok anymore. Should it be\n%% we can get here if a vhost is added and removed concurrently\n%% e.g. some integration tests do it\n?. Same as in rabbit_amqqueue_sup_sup. That was a debug message. Do you think it should stay?. Right. We should not.. Should it be stopped maybe? . It does not have to persist messages, just process them. Credit flow ack is sent before message store handles a message, it will take some time to actually persist messages.. There are number of guides already and most of config variables are documented in those guides, so we can keep doing that and add links here.. Good find. Will change.. Right. Missed that.. This code calls two callbacks, one of them just updates the state. Should it be a single callback instead?. We have no nack rates. So it's a future change. I'll create a new issue.. Right. We can check for processes directly instead.. Can you clarify?. It does use dest_host, but only supports IPs (in gen_udp:send/4 )\nIt may be confusing for people trying to configure it.. sys:get_status/1 will send a system call message to the process. This doesn't work with self(). We do have rabbitmq-env.bat. Do you think it can be moved there?. Missing {?\nThere can be more missing. Doesn't shellcheck detect that?. Does it make alloc args configurable?\nShould it be documented?. Why delete ${brackets}?\nIs that for consistency?\nIt could be just me, but it feels safer to have them.. Do you want to merge with this function?\nShould it be renamed then?\nIt looks that some variables are set in other functions.. This exit should stop the rabbitmq-server sctipt, but it doesn't.\nIs there a set -e missing anywhere?\nI wonder if this check should go to rabbitmq-server to keep this file clean from logic.. Looks fine after 55a695307f8480f5eaf6e01bd0c6edd0112dbdc4. Is that a quorum queue specific function?. Can we move this setting to erlang code? We don't start applications on node boot, but from rabbit:boot/start functions. It would be nice to avoid additional bash scripting.. Should quorum queues always be considered mirrored?. Should there be a monitor then? Queue stats and credit flow will be affected.. This is confusing. Can it be refactored somehow to highlight what is the difference between integer and non-integer consumer tag?. Can this function have two callbacks, or highlight the queue type more explicitly?. Was this TODO addressed? Should it be addressed?. The DOWN message handler does something with them after calling this function. \nLine 705.. The git-blame commit 45a7bac81ba53552a3f78eea05b3d0b316b71b02 implies that the integer consumer tag replaced the none atom consumer tag for basic.get. It can be because basic.get is not a part of the raft machine API and is emulated with one-off consumer.\n. The question is: what does is_mirrored mean?\nDo we know all the places where this function can possibly be called and expectations during those calls?\nWhat if the code which calls this function will assume that there are components of the mirrored queue like GM, master and slaves?\nWhat properties should the person, who writes the code expect from the queue, which returns true here? Do we have an API (behaviour) for that?\nWhen we had one type of mirroring it was two behaviours possible (is_mirrored: true or false)\nNow we have three. Should this function return three possible results?. I have a bad feeling about this. What could be a performance impact of having a single process handling all dead-letters from multiple queues?. Should this TODO be addressed? If vhost does not have this process, does it mean that the vhost supervisor should restart?. Is there a mechanism to notify that the queue is gone other than monitors? For example if the queue is deleted?. There is no handle_publishing_queue_down in eol handler code. Is that ok?. Does this branch have #1719 (in latest master) merged? There were some changes around confirms. . I understand that single active consumer feature is reusing the field for exclusive consumer, but should we reuse the info item? If there is a single ective consumer it will be reported as exclusive, which is close, but can be confusing.. This thing implies that all ra processes are quorum queues. Although it's currently true, we should take a note and change that later.\nMaybe we can filter the qeueues somehow? I'm not sure if it's possible.. I remember us talking during the summit about configuring the quorum data dir separately. Can we still keep this option, for example if ra data_dir was configured in the config file?. ",
    "vsychov": "Michael, I sure, about vhost, when I try get other queue, by this command, messages returned. And I have only one vhost.\n. Oh... sorry, I see management UI, and this messages marked  as \"Unacked\"\n. ",
    "LarsFronius": "Thanks for the clarification, got the point it is of course not my apps connection and it can't know the app is blocked for publishing.\nBut this entry in the log:\nPublishers will be blocked until this alarm clear.\nTells me as a user, that no publisher should be able to publish anything onto this RabbitMQ host, doesn't it? So the aliveness-test should not be able to do that as well?\nAnyway - as a user tagged for monitoring, is there any way I can get the information, that RabbitMQ is in the alarm state where publishers will possibly be blocked?\n. ",
    "CVTJNII": "As a user I agree with @LarsFronius that the aliveness check should fail when publishing is blocked by the watermarks being exceeded.  That the aliveness check passes when the queue cannot process messages is both surprising and disappointing.\n. @michaelklishin I respectfully disagree.  Per Rabbit's output when the watermark is breached:\n```\n\n Publishers will be blocked until this alarm clears \n**********\n```\nPer the API documentation at http://hg.rabbitmq.com/rabbitmq-management/raw-file/rabbitmq_v3_3_4/priv/www/api/index.html:\napi/aliveness-test/vhost    Declares a test queue, then publishes and consumes a message. Intended for use by monitoring tools.\nSo per the check's documentation it should publish and consume a message.  However, per Rabbit's log in this scenario publishers are blocked.  So I disagree that this is ambiguity in how the check should work, in my opinion the check is not operating as documented.  That is the spirit of my objection, based on the documentation I've found I would not expect a 200 response when publishers are blocked.\nFurthermore, when I hit this issue I had the watermark set to zero, and per the memory documentation at https://www.rabbitmq.com/memory.html I would expect all publishing to be stopped as per the log message above:\nA value of 0 makes the memory alarm go off immediately and thus disables all publishing (this may be useful if you wish to disable publishing globally); use rabbitmqctl set_vm_memory_high_watermark 0.\nIf there is better documentation for the API please let me know, that was the best I found and aligned with other searches for how the endpoint should function.\nSo again, I disagree with the assertion that this is an issue with consensus on how the check should operate, and instead believe the check is not operating as documented.  As the check is supposed to publish and then consume a message it should not pass when publishing is disabled.\n. I cheated and just opened a new PR, rather than resolve the rebase conflicts.\nhttps://github.com/rabbitmq/rabbitmq-server/pull/327\n. I'm hitting the same problem.  Please reopen.\n=INFO REPORT==== 18-Feb-2016::21:59:16 ===\nnode           : rabbit@trrabbitmq01.vagrant.tjnii.local\nhome dir       : /var/lib/rabbitmq\nconfig file(s) : /etc/rabbitmq/rabbitmq.config (not found)\ncookie hash    : kxwbVsIOZbVw8c9RU5ItXQ==\nlog            : tty\nsasl log       : tty\ndatabase dir   : /var/lib/rabbitmq/mnesia/rabbit@trrabbitmq01.vagrant.tjnii.local\n```\nrabbitmqctl -n rabbit@trrabbitmq01.vagrant.tjnii.local list_users\nError: unable to connect to node 'rabbit@trrabbitmq01.vagrant.tjnii.local': nodedown\nDIAGNOSTICS\nattempted to contact: ['rabbit@trrabbitmq01.vagrant.tjnii.local']\nrabbit@trrabbitmq01.vagrant.tjnii.local:\n  * connected to epmd (port 4369) on trrabbitmq01.vagrant.tjnii.local\n  * epmd reports node 'rabbit' running on port 25672\n  * TCP connection succeeded but Erlang distribution failed\n  * suggestion: hostname mismatch?\n  * suggestion: is the cookie set correctly?\n  * suggestion: is the Erlang distribution using TLS?\ncurrent node details:\n- node name: 'rabbitmq-cli-1562@trrabbitmq01'\n- home dir: /var/lib/rabbitmq\n- cookie hash: kxwbVsIOZbVw8c9RU5ItXQ==\n```\nNote that the cookie hashes match and the nodename passed matches the output from the rabbit log.  As @jurajseffer pointed out rabbitmqctl is using the short name.  Per http://erlang.org/faq/problems.html \"you cannot have a system where some nodes use fully qualified names and others use short names\".\n. Setting RABBITMQ_USE_LONGNAME=true in the environment resolves this for me.\n. ",
    "pasku": "@michaelklishin I think it's useful to see in an easy way the last time the broker started.\nYou can send the CLA to the email in my profile.\nThanks.\n. ",
    "arobson": "Found an edge case in this code: when using a DLX that points to the expiring queue, the dead-letter feature sees this as a cycle and does not re-publish the messages. I tried calling rabbit_bindings:remove_from_destination to remove the bindings before dead-lettering the messages but it appears to still detect the cycle. All this to say, this PR needs more work. \n. This update cleans up the way the original code was written in addition to resolving the undesired behavior when used with a consistent-hash exchange as the DLX.\nInterested to hear back on whether this seems like a feasible approach.\n@simonmacmullen expressed concerns about this feature in general when used with large queues that have messages on disk. I'd imagine the performance would be slow, but I know that I could cause similar behavior today if I set a DLX and Message TTL on a queue, threw tons of messages at it until they hit disk and then started expiring.\n. @michaelklishin - just following up: I realize the edge case could cause performance issues but the same edge case already exists for any queue that is backlogged with a dead-letter exchange and long enough message TTL. In other words: there are already other ways one can create performance problems if one doesn't take the proper precautions when designing their topology.\nWould the Rabbit team consider including cautionary language around this feature as part of the documentation? I've built several systems around Rabbit as the messaging backbone - there has not been a single case where message loss was preferable to temporary performance degradation. The easy way to prevent the performance issue with this feature is to ensure your queues aren't being overrun. With a consistent-hash-exchange (or exchange-queue pairs for each service instance), it's rather simple to solve for this by simply spinning up an additional service instance.\nWithout this feature, we're left with developing an entire subsystem to monitor queues and consumers that waits for a queue's consumer to drop, unbinds the queue from everything, and directs an existing consumer to pick up the slack then monitors the queue so it can be removed when empty. This is actually quite problematic and has the follow undesirable consequences:\n- load imbalance across consumers\n- uneven latency spikes in processing messages from the orphaned queue (since it can't be evenly redistributed)\n- constant load on rabbit's HTTP API, especially if you need to catch the issue quickly to minimize time lost\nDistributed mutual exclusion is a very difficult problem to solve. I believe that with the inclusion of this feature (or a selective-consumer feature), Rabbit would be uniquely positioned to remove or at least greatly reduce the role of a distributed locking mechanism. Without this capability, we're left with complex work-arounds and a new set of problems to address.\n. @simonmacmullen - I can try to address your concerns but I won't pretend that I understand Rabbit well enough to handle it. It's been over a year since I've even looked at this code, I was actually pleasantly surprised to see the PR hadn't been closed.\nIf someone on the core team wanted to take on adding the feature properly, addressing your concerns, we can certainly close this out. Otherwise, do you have recommendations on how I could start working towards addressing your concerns and get some team feedback? Would you prefer I just continue making commits to this branch and request review?\nThanks for being open to discussion and for the guidance provided so far.\n. ",
    "drapp": "Any word on this pull request? I could also use this functionality. It's very hard to have expiring queues without this. I'm trying to have a system that routes to queues that are transient, and I want to have a combination of an alternate exchange and dead lettering to ensure that messages that don't get consumed end up in a fallback queue. \nIf I unbind each transient queue before its consumer goes away, I can make sure the messages expire and get dead lettered before the queue goes away, and once the queue is unbound the messages flow through the alternate exchange. There's big hole though if the consumer dies suddenly though: if a queue expires while messages are flowing into it, those messages are lost.\nIf there's a workaround I'd be open to it, but this pr seems like a fix for a dangerous bug.\n. ",
    "dumbbell": "I agree this seems a useful feature. It may even be what users expect when using TTL on a queue.\n. Alex, thank you, your pull request is well detailed.\nYou add head_msg_timestamp to #vqstate but I don't see where you use it. Is it a leftover from your previous patch?\nYou could squash the two commits into one, so rabbit_variable_queue.erl.rej never appears.\n. We discussed this internaly some time ago, here are more details.\nTo go beyond what was committed in #181, we want to handle timeouts in a finer grain way. Let's take list_queues as an example: if there are 10 queues, but one of them seems stuck, the current implementation will timeout globally and may display no information about the responsive queues.\nAn improved implementation would display information about responsive queues, and indicate a timeout for each stuck element. Note that even if several elements are unresponsive, the timeout specified by the user should be honored: if the timeout is set to one second, then it's not one second per element, it's one second globally.\n. I'm not sure a per-element timeout is interesting. Furthermore it would add another option and complicate the UI.\n. It also depends if the output must be sorted or not. I admit I didn't verify if it is the case already for eg. list_queues. If we don't need to sort the output, I agree we should display everything as soon as we get the information.\n. Since 3.5.0.\n. Since RabbitMQ 1.5.2.\n. Since 3.5.0.\n. Here is what I did to test the correction:\n1. I start two nodes, A and B, with a very low vm_memory_high_watermark to make them page messages out early, clustered them and added the following HA policy on node B:\nrabbitmqctl -n B set_policy ha-all \".\" '{\"ha-mode\":\"all\"}'\n2. I stopped node B using:\nrabbitmq -n B stop_app\n3. I used PerfTest to publish 10 kB messages with a rate-limited consumer so messages stay in RabbitMQ:\nPerfTest -s 10240 -R 100\n4. The producer could publish around 40,000 messages before being throttled.\n5. I started node B again and force synchronisation from the management UI.\nWith the stable branch, the management UI reports I/O read rates of:\n- 150 messages/s\n- 150 MB/s\nWith the rabbitmq-server-69 branch (this fix), it reports:\n- 1000 messages/s\n- 15 MB/s\nI logged the size of the read buffer in file_handle_cache.erl at the same time. With stable, the buffer remains at an expected 1MB size. With the fix, the size continuously switches between 10468 and 20936, with an occasional jump to 4 MB.\n. One correction to my previous comment:\n\n150 messages/s\n1000 messages/s\n\nThose should read:\n- 150 reads/s\n- 1000 reads/s\n\nThe 4MB sizes probably refer to other files (queue index files?)\n\nYou're right, the file handle differs for those reads.\n\nNot sure whether the flicking between 10468 and 20936 is worth fixing, what do you think?\n\nAfter a test:\n- With the flickering buffer:\n  - 1000 reads/s\n  - 15 MB/s from disc\n  - 10-12 MB/s sent to node B\n- With a constant buffer:\n  - 750 reads/s\n  - 7 MB/s from disc\n  - 7 MB/s sent to node B\nIn the first case, we read 20 kB to only use 10 kB, then we read 10 kB, then we double and so on. We don't do this in the second case (we always read 10 kB). When comparing the number of reads to the throughput, we see the 33% decrease of throughput in the second case, corresponding to not wasting 10 kB. However, I can't explain why it is slower...\n. Here are new, more meaningful numbers comparing stable and 8faf4eecd92e0fab572f43e1ae45ed12ee26a472.\nThe protocol is:\n1. Start nodes A and B, cluster them, add a HA policy.\n2. Create a queue from the management UI.\n3. Stop node N.\n4. Use PerfTest to queue 300,000 messages, which is enough to page them out (the filesystem is tmpfs). No clients are connected after that.\n5. Start B and force synchronization. While this happens, look at the time the full sync takes, as well as I/O and network statistics.\nResults with stable:\n- Synchronization finished in 1'55\".\n- Reads: 1600/s (1.6 GiB/s)\n- Network (from A to B): 18 MiB/s while messages are paged in, then 58 MiB/s\nResults with 8faf4eecd92e0fab572f43e1ae45ed12ee26a472:\n- Synchronization finished in 1'10\".\n- Reads: 4500/s (57 MiB/s)\n- Network (from A to B): 58 MiB/s\n. @jmcmeek, I'm looking at this issue but I don't know Pacemaker.\nCurrently, /var/run/rabbitmq is handled by the init script, not the package. So after a fresh install or after stopping RabbitMQ, this directory is missing.\nCould you please explain to me how Pacemaker works and why it needs this directory to be there, even if the pidfile is missing?\n. The init script is supposed to create /var/run/rabbitmq on startup and delete it on shutdown, like the OCF Resource Agent script. See:\n- packaging/RPMS/Fedora/rabbitmq-server.init, line 42\n- packaging/RPMS/Fedora/rabbitmq-server.init, line 62\nI tested this again on Fedora 21 with RabbitMQ 3.5.0 RPM downloaded from www.rabbitmq.com and it worked. What version of RabbitMQ are you using?\n. I think it's reasonable and would even be consistent with the init scripts from the RPM and Debian packages which already do this.\n. The problem was reported on the mailing-list:\nhttps://groups.google.com/d/topic/rabbitmq-users/F-Fbc7sxTrY/discussion\n. An update to the documentation on the website is ready:\nrabbitmq-website@735c99268a1b619b14e45cd94b12a25dade05b1c\n. Yes, thanks!\n. @michaelklishin, yes, it's ready.\n. Please QA the following PRs:\n- rabbitmq/rabbitmq-server#81\n- rabbitmq/rabbitmq-website#2\n. You can do the chmod in a single run:\nsh\nchmod -R g-w,o-rwx .../mnesia\n. I agree, the number of times would be nice. The user who reported the problem mentionned it as well.\nAfter reading the cycle detection code, I think it still works: it only looks for one entry for the current queue.\n. I gave the branch a try and did:\n1. From the management UI, I created:\n   - a my-dlx fanout exchange\n   - a my-dlq queue bound to my-dlx\n   - a test queue with a queue TTL of 1\" and my-dlx as its dead-letter exchange\n2. I used PerfTest to publish one message to test\n3. The message ended in my-dlq after the 1\" TTL. The x-death looked like:\ncounter:        1\n   reason:         expired\n   queue:          test\n   time:           1427292711\n   exchange:       direct\n   routing-keys:   e3e47d2e-8834-481f-8969-ec4125c1847c\n4. Still using the management UI, I moved the message from my-dlq to test. Again, the message expired and ended up in my-dlq. The x-death counter has been incremented:\ncounter:        2\n   reason:         expired\n   queue:          test\n   time:           1427292711\n   exchange:       direct\n   routing-keys:   e3e47d2e-8834-481f-8969-ec4125c1847c\nI tried again but this time with my-dlq configured like test to create a cycle. It was properly detected and the message was dropped.\n. The associated PR (#79) was merged to stable.\n. This PR fixes #78.\n. Hi!\nThe Pivotal network is under maintenance, causing www.rabbitmq.com to badly respond. In the meantime, you can get release archives and packages from GitHub:\nhttps://github.com/rabbitmq/rabbitmq-server/releases\nUnfortunately, we don't have any Debian repository mirror at this time. We are sorry for the caused troubles.\n. The same parsing error happens to the reporter of #76.\nI couldn't reproduce either.\n. Changing how/when the garbage collector is called can cause unexpected behaviour. I'm wary about merging this commit to a stable branch.\n. To expand a bit on the garbage collection topic, while working on the memory monitor and how queues report their respective memory consumption, I noticed the memory monitor doesn't take into account the time the GC takes to free memory, leading to pessimistic decisions (all this is sumed up in the internal bug report 26532 I need to move to GitHub).\nAmong the various solutions I'm playing with, queues are calling the GC just for themselves, keeping the time it takes for that particular queue.\nAnyway, just to say that depending on the result of this work, we may partially move away from a global garbage collection handling.\n. Auto-clustering only happens during the first start of the node, or after using rabbitmq reset. See paragraph #3 in the documentation. There is no way to \"force\" auto-configuration again beside using rabbitmq reset.\nIf you are using a Debian-like Linux distribution for instance, installing the package starts the service as per the Debian policy. Therefore, you need to stop and reset the node before auto-clustering has a chance to kick in.\nAs a side not, please post your questions to the mailing-list in the future. This is our primary place for this purpose.\n. There is an ongoing discussion with someone who has an issue similar to yours on the mailing-list. See:\nhttps://groups.google.com/d/topic/rabbitmq-users/aI4jHe99fQg/discussion\nThis problem is unresolved as of now. Stay tuned :-)\n. Likewise, check #120 on the same topic.\n. The PR's destination is misleading: the branch was merged to stable first, then master, so it will be available in the next bugfix release.\n. I was waiting for some last comments from Docker people. They are happy now :-)\n. The test is expanded to all files and directories configurable in rabbitmq-env.\n. Since RabbitMQ 2.8.2.\n. No, it needs a check in rabbitmq-env.\nHowever, I have an issue on RHEL 6.5 I'm debugging.\n. To expand a bit beyond \"No\" (sorry for the short answer...), the commit you mention only fixes the argument to start-stop-daemon on Debian-like distributions. Red Hat-like distributions are not affected by this change. start-stop-daemon uses this PID file to know if the service is already running for instance. In our case, the bug was not a problem because rabbitmqctl status is executed before reaching this point to achieve the same check.\nThe problem here is that the init script sets RABBITMQ_PID_FILE in the environment when starting (directly or indirectly) rabbitmq-server. But rabbitmq-env.conf may override this value. When this is the case, rabbitmq-server creates the PID file at the location specified in rabbitmq-env.conf, not at the one expected by the init script. Therefore, rabbitmqctl wait (started by the init script) waits forever a PID file which will never be created.\nThe user can only ^C the init script. Depending on the init system, this may just interrupt the init script and leave RabbitMQ alone, or this may kill RabbitMQ.\n. @tianon, any opinion on this branch (regarding your comment on @ec851de0)?\n. Since RabbitMQ 3.5.0.\n. It appears the memory is consumed by the file handle cache: the master queue's process dictionary is full of {Ref, fhc_handle} => {handle, ...} key/value pairs which include the read buffer.\nDuring my test, the queue contains 1000 messages of 10 MiB each. The memory high watermark being set at around 2 GiB, nearly all (if not all) messages are paged to the disk. However, the queue process is happily consuming 5 GiB of memory. This matches what erlang:process_info(Queue_Process, binary) returns (in my case, 500 referenced binaries of 10 MiB each).\nNeither the file handle cache nor the message store ask the memory monitor how much memory they are allowed to consume. They only pay attention to the number of open file handles.\n. I agree, let's delay 3.5.2. If I don't have a good solution in a week, we can do the release.\n. This is a preliminary commit. There is a debug log message left on purpose for instance; I will remove it after further testing.\n. Here are the results of today's testing and benchmark:\nI measured the time taken to sync one mirrored queue containing 1000 messages of 10 MiB each (10 GiB worth of data). The two nodes are running on the same host, the disk is an SSD formatted using ZFS (which does heavy caching). Both had a high watermark set to 2.2 GiB.\nI did four runs for each of the following versions:\n- stable which exhibits the bug described here;\n- rabbitmq-server-134 with the explicit garbage collection;\n- rabbitmq-server-134 without the explicit garbage collection.\nResults:\n$ ministat -s -w 88 stable \\#134-gc \\#134-nogc \nx stable\n+ #134-gc\n* #134-nogc\n+----------------------------------------------------------------------------------------+\n|       *                                                                                |\n|x  +   *** x            +  x                    x         +                            +|\n||____________________A_____M______________|                                             |\n|      |____________________________________A______________M_____________________|       |\n|       |A|                                                                              |\n+----------------------------------------------------------------------------------------+\n    N           Min           Max        Median           Avg        Stddev\nx   4        29.628         32.52        31.263       30.9195     1.2613909\n+   4        29.783        34.909        33.127      32.22575      2.257041\nNo difference proven at 95.0% confidence\n*   4        30.034        30.165         30.11      30.08825   0.061277375\nNo difference proven at 95.0% confidence\nThe explicit garbage collection gives very unstable performance. However, the proposed fix without this explicit garbage collection gives stable timings, even more stable than the stable branch, resulting in a slightly faster sync process.\nDuring the tests, memory consumption with both versions of the fix could be up-to 3 GiB (800 MiB above the high watermark) for a few seconds. I believe it is the garbage collector who can temporarily allocate a lot of memory.\nThe commit I force-pushed removes the call to the garbage collector and the debug log message.\n. Because this change could modify the behaviour in a stable branch, what do you think about hiding it behind a configuration knob?\n. As a side note, this file handle cache, which reimplements what the underlying OS already does, can lead to unexpectedly lower performance. During the test, the high watermark was set to 2.2 GiB. I then tried with the default high watermark (which gave 6.2 GiB on this machine): the average sync time went from 30\" to 40\" (33% decrease of performance)!\nThe problem was that with that much memory, the file handle cache was competing with the ZFS ARC (ZFS' cache). As a result, part of RabbitMQ was paged out to disk... Admittedly, ZFS may not be the best filesystem for RabbitMQ, but we may want to keep that in mind and, one day, revisit how we do file caching.\n. > What kind of breaking changes are you worried about?\nThe current fix arbitrarily drops \"2 * memory above limit\" amount of data (taken among the least recently used read buffers) when the memory use goes above the limit. It seems to work for the tested scenario. But this cache is used by queue paging as well for instance. I have no idea if it could be counter-productive in other production workloads.\n. Since RabbitMQ 2.0.0.\n. One question before I merge the branch. IIRC, the management UI is refreshed every 5\": could this change lead to connections/channels to appear as always running, even if they were in flow in the past 5\"? I mean, could we end up in the opposite situation: currently it gives the impression connections/channels are always in flow, but with the change, it gives the impression they are never in flow?\n. Yes, I agree the current UI does not convey the information appropriately.\n. Since RabbitMQ 3.5.1.\n. Hi @Jakauppila! Sorry for the delay...\nI see the Windows scripts define all the variables themselves (thus, there is duplicated code).\nOn Unix, we have the following dependency graph:\nrabbitmq-server, rabbitmqctl, ...\n|\n`-- rabbitmq-env\n    |\n    |-- rabbitmq-defaults\n    |\n    `-- /etc/rabbitmq/rabbitmq-env.conf\nHere is how it works:\n1. rabbitmqctl & friends load rabbitmq-env which is always installed in the same directory.\n2.  rabbitmq-env loads rabbitmq-defaults who defines a few variables, in particular:\nsh\n   CONF_ENV_FILE=${SYS_PREFIX}/etc/rabbitmq/rabbitmq-env.conf\n3. rabbitmq-env loads $CONF_ENV_FILE if it's there.\n4. rabbitmq-env ensures all variables are initialized.\nSo, all variables are overridable in rabbitmq-env.conf, except its location. This is something we may change in the future (for instance, the package for FreeBSD patches rabbitmq-defaults to put rabbitmq-env.conf in /usr/local/etc/rabbitmq).\nOn Windows, it would be nice to have the same logic:\n- All scripts load rabbitmq-env.bat which loads rabbitmq-defaults.bat.\n- rabbitmq-defaults.bat defines RABBITMQ_BASE and RABBITMQ_ENV_CONF_FILE (the location of rabbitmq-env-conf.bat, defaults to !APPDATA!\\RabbitMQ\\rabbitmq-env-conf.bat); possibly allowing these variables to be user-defined.\n- rabbitmq-env.bat loads RABBITMQ_ENV_CONF_FILE and initializes all variables.\nWhat do you think? Does it match common/best practices on Windows or is it too Unix-centric?\n. Hi!\nI reproduced the issue with @rjrizzuto's Python test script, twice in a row. The Python catch an exception because the socket is closed by RabbitMQ. I captured the network packets and need to review them. But as the bug happens with both the .NET client and Pika, it must lie somewhere in the broker.\nIn RabbitMQ's logs:\n```\n=ERROR REPORT==== 1-Jun-2015::12:59:25 ===\nError on AMQP connection <0.1161.0> ([::1]:48981 -> [::1]:5672, vhost: '/', user: 'guest', state: running), channel 1:\n{amqp_error,unexpected_frame,\n            \"expected method frame, got non method frame instead\",none}\n=INFO REPORT==== 1-Jun-2015::12:59:25 ===\nclosing AMQP connection <0.1161.0> ([::1]:48981 -> [::1]:5672)\n```\nI limited RabbitMQ to ~500 MiB of memory by setting vm_memory_high_watermark. The bug happens when the queue reaches vm_memory_high_watermark_paging_ratio (~250 MiB) and it needs to be paged out. \nOne thing we changed in 3.5.3 is how we clear the read cache when we need to reclaim memory. I don't think this is the bug. However, I suspect the problem is related to our read buffers handling and this change increases the chance to hit it. That would explain why you have it with RabbitMQ 3.4.x but less often.\n. After more investigation, it is not that \"simple\".\nIn all the network captures I did (using Pika as the client on the same host as RabbitMQ), I see the following scenario:\n1. Pika sends perfect sequences of Basic.Publish -> Content-Header -> Content-Body, one AMQP frame per TCP packet.\n2. At some point, Pika sends 3 or 4 TCP packets filled to reach the MTU (in my case, 16384). Those packets obviously contain multiple perfect sequences.\n3. Then Pika goes back to one AMQP frame per TCP packet. However, this packet contains an unexpected frame. For instance, the last \"big\" TCP packet finished with a Content-Body, then the small TCP packet contains a Content-Header or a second Content-Body. In all cases, the Basic.Publish frame is missing.\nI think it happens at a time where RabbitMQ does not read the network (the reader is blocked because the memory is under pressure), but I can't confirm if this is really linked. So far, 66% of the captures shows that a Connection.Unblocked was sent by RabbitMQ to the client just before the big TCP packets happen.\nI will look at Pika source code to see how it works.\n. I compared the captures made by Wireshark to the data processed in rabbit_reader.erl and they both match. So the frames received by RabbitMQ are incorrect and my test case doesn't demonstrate a problem in RabbitMQ.\nMaybe the issue exposed by the .NET client library is different.\n. As a reference, here is the Python script:\n``` python\n!/usr/bin/env python\nimport pika\nimport sys\nimport time\nconnection = pika.BlockingConnection(pika.ConnectionParameters(host='localhost'))\nchannel = connection.channel()\nchannel.queue_declare('test', durable=True)\nmessage =  b'\\x00' * 2048\nwhile 1:\n    channel.basic_publish(exchange='', routing_key='test', body=message)\n```\n. So, I'm using Pika 0.9.14 (the latest release) and after looking at what it is doing, I confirm the problem comes from Pika in my case.\n1. Frames are written to the wire individually by this function:\n   https://github.com/pika/pika/blob/0.9.14/pika/adapters/base_connection.py#L360\nself.socket.sendall(frame) either sends the frame successfully or an exception is raised.\n2. An exception is raised when RabbitMQ do not accept any more TCP packets. The exception is handled by:\n   https://github.com/pika/pika/blob/0.9.14/pika/adapters/blocking_connection.py#L351\nThe if self.is_closing condition is never met so the exception is ignored (the connection is not closed) and the frame is dropped by Pika.\nAnd because there is no retry and no atomicity in the send of the 3 frames composing Basic.Publish, sometimes, the method frame is not sent to RabbitMQ but following frames are, leading the the reported error in RabbitMQ. The code in Pika's master branch is completely different so the problem may have been fixed, I did not try it.\nSo we are back to square 1 with the original issue. I will try to reproduce it with the .NET client.\n. I looked at master and I believe pika/pika@cd8c9b08e8a121acfb8b53344292d7b2ff4eecc6 fixes the problem.\n. I could reproduce the problem with your Publisher.cs test case.\nAgain, Wireshark and RabbitMQ agree on the received data: the received frames are incorrect. In the last run, RabbitMQ received three Basic.Publish frames in a row, without Content-Header and Content-Body in between.\nAnd again, this happens at a time RabbitMQ's host receive buffer is full (because RabbitMQ blocks the reads). Thus it looks like the problem in Pika.\n. I couldn't reproduce the issue with the Java client library and PerfTest, so this one may be good already. I'm going to look at what is done there.\n. More tests with more client libraries:\n- Pika's master branch is fixed.\n- Bunny (Ruby) is fine too.\n. If I understand correctly, with Node.js' amqp.node, handling connection write failure is deferred to the application.\n. streadway/amqp (Go) is fine.\n. Since RabbitMQ 3.5.2.\n. > (...) rabbit_error_logger depends on the exchange infrastructure from being ready. When the exchanges are ready, it means kernel_ready is there, that might be the reason why rabbit_alarm was depending on kernel_ready.\nBy \"exchange infrastructure\", don't you mean \"external_infrastructure\" instead?\nBefore the boot steps are executed, we have:\nerlang\nok = ensure_working_log_handlers(),\nSo after this point, I understand rabbit_log is working (rabbit_log's handler is registered in error_logger). @michaelklishin's patch seems correct to me.\nNow, regarding source files without therabbit_ prefix, I didn't know about the fact they must have no knowledge of RabbitMQ. I will prepare a separate fix once this bug is solved.\n. Thanks @videlalvaro, I see the problem now.\nHere is a new proposal, separated in two commits:\n1. In rabbit_memory_monitor:memory_use/1, support the fact that vm_memory_monitor may not be started yet by catching the noproc exception. This avoids any changes to the boot steps ordering.\n2. Remove the knowledge of RabbitMQ from file_handle_cache by passing an anonymous function to call rabbit_memory_monitor:memory_use/1.\nIf this seems reasonable to you, I will squash everything into a single commit, but I wanted to keep @michaelklishin proposal for the discussion.\n. Only the first few reads before vm_memory_monitor is started won't have the check. Once it's started, the memory limit is applied, possibly clearing the cache.\n. Hi!\nThank you for your contribution! I'm going to post here the comments I previously wrote by email.\n. I agree, let's merge this first and address style changes later.\n. I confirm the patch fixes the performance issue:\n- @b8cb778430df6279716f9ce4d8b849a518c74474: 11k/s\n- @63678f206f5648c64459574ab4b748c5852cf094: 14k/s\n- HEAD of stable: 10k/s\n- This patch: 13k/s\n. @videlalvaro: I moved the clauses.\n. I modified the code to empty the cache from the slave processes as well.\nNote that file_handle_cache.erl now includes rabbit.hrl so it grows a second dependency to RabbitMQ...\n. I would like to keep the read/write buffers logic contained in file_handle_cache.erl as much as possible because this is something we want to get rid of. I would prefer to keep the queue implementation clean from this feature.\n. > Let me guess\u2026 a quick glance at the code tells the file_handle_cache module is the one doing the put/2 and what not, but it's being called from the context of the queue process\u2026\nYes, the cache and many other informations are stored in the dictionary of the process opening/reading the file.\n. > Do we need to clear the cache on the slave Pids? Wouldn't that clear caches on a separate node?\nYes, this will clear the cache on other nodes. In fact, the command as it is clears all read buffers from all queues on all vhosts, so all nodes are affected.\nMaybe I should limit it to the queues hosted on the node running the command, what do you think?\n. Hard to say, it depends on the load obviously. I guess that walking through the dictionary's keys and rewriting some of them is cheap, but there could be many many queues with many buffers.\nThen, then garbage collector needs to free all those binaries.\nAnyway, after more thinking, I would expect this command to limit its action to the node I'm running it on. I will push a new commit.\n. > Can you apply Emacs Erlang's formatting to all these changes?\nDamn, I only see this comment now... I don't know all the rules of the Emacs style. As it doesn't match the style I'm used to, what should I change in this code?\nOnce we move to a common build system, I should take some time to add an Elvis configuration to our projects.\n. If Mnesia log level is set to trace on node A, here are two interesting evens:\n1. The slave node (B) is recorded as synchronised and recoverable slave:\nerlang\n   Mnesia(A): write performed by {tid,170,<23701.278.0>} on record:\n       {rabbit_queue,{resource,<<\"/\">>,queue,<<\"test\">>},\n                     true,false,none,[],<0.555.0>,\n                     [<23701.267.0>],\n                     [<23701.267.0>], %% B is synchronised\n                     [b@magellan],    %% B is recoverable\n                     [{vhost,<<\"/\">>},\n                      {name,<<\"HA\">>},\n                      {pattern,<<\".*\">>},\n                      {'apply-to',<<\"queues\">>},\n                      {definition,[{<<\"ha-mode\">>,<<\"all\">>},\n                                   {<<\"ha-sync-mode\">>,<<\"automatic\">>}]},\n                      {priority,0}],\n                     [{<23701.268.0>,<23701.267.0>},{<0.558.0>,<0.555.0>}],\n                     [],live}\nThe transaction is executed from the slave node.\n1. The slave node is \"unmarked\" as recoverable, though still synchronised, by a transaction running on the master (A):\nerlang\n   Mnesia(A): write performed by {tid,171,<0.1116.0>} on record:\n       {rabbit_queue,{resource,<<\"/\">>,queue,<<\"test\">>},\n                     true,false,none,[],<0.555.0>,\n                     [<23701.267.0>],\n                     [<23701.267.0>], %% B is synchronised\n                     [],              %% B is NOT recoverable anymore\n                     [{vhost,<<\"/\">>},\n                      {name,<<\"HA\">>},\n                      {pattern,<<\".*\">>},\n                      {'apply-to',<<\"queues\">>},\n                      {definition,[{<<\"ha-mode\">>,<<\"all\">>},\n                                   {<<\"ha-sync-mode\">>,<<\"automatic\">>}]},\n                      {priority,0}],\n                     [{<23701.268.0>,<23701.267.0>},{<0.558.0>,<0.555.0>}],\n                     [],live}\n. The culprit is rabbit_amqqueue:on_node_up/1:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/stable/src/rabbit_amqqueue.erl#L780\nIt removes the starting node from all queues' recoverable slaves.\nThis is a timing/concurrency problem: if this code happens to be executed after rabbit_mirror_queue_slave:record_synchronised/1, then the information is lost.\n. Thank you!\n. Hi @marcelog!\nI share @videlalvaro's opinion: os:timestamp(), despite returning a time in the same format as erlang:now(), is not a drop-in replacement. As far as I understand, time returned by os:timestamp() is the OS wall clock, therefore it can go backward, whereas erlang:now() increases monotonically.\nTo fix our (ab)use of erlang:now(), all uses need to be audited to determine what we use this function for: a monotonically increasing value, a calculation of elapsed time, a date/time, and so on. Then, I like the advice given in Erlang's documentation of using a wrapper module to be compatible with the old and new APIs. There is such a module provided in the Erlang source distribution\n. Hi @ash-lshift!\nI have a few more comments on the style, otherwise it looks good to me too.\nFeel free to amend the squashed commit and force-push the new one.\n. Hi @jeckersb!\nWe do not have permission to access:\n- http://people.redhat.com/jeckersb/rabbitmq-issue-224/3.5.4.rc1/rabbit@lb-backend-mac525400797585.log\n- http://people.redhat.com/jeckersb/rabbitmq-issue-224/3.5.4.rc1/rabbit@lb-backend-mac5254005e6a60.log\n. Among the stuck processes on mac525400797585, Erlang tries to contact a remote epmd process, I suppose on the missing node:\nerlang\n[{pid,<5443.5390.0>},\n {registered_name,[]},\n {current_stacktrace,[{prim_inet,connect0,4,[]},\n                      {inet_tcp,do_connect,4,\n                                [{file,\"inet_tcp.erl\"},{line,100}]},\n                      {gen_tcp,try_connect,6,\n                               [{file,\"gen_tcp.erl\"},{line,163}]},\n                      {gen_tcp,connect,4,[{file,\"gen_tcp.erl\"},{line,141}]},\n                      {erl_epmd,get_port,3,[{file,\"erl_epmd.erl\"},{line,293}]},\n                      {inet_tcp_dist,do_setup,6,\n                                     [{file,\"inet_tcp_dist.erl\"},\n                                      {line,258}]}]},\nThis call waits forever for the remote node to respond on the Erlang side. However, I don't know if the kernel's timeout is honored or if Erlang retries again and again. I need to study what Erlang does and why, but I guess it could explain why RabbitMQ calls to the down node get stuck.\nDo you make any tuning to the TCP stack?\n. I don't see anything wrong with the network parameters.\nI'm interested in the *-sasl.log files from both nodes as well. Could you please post them to people.redhat.com?\nAlso, can you double-check that Erlang nodes don't try to communicate through the VIP?\n. Hi!\nSo far, I couldn't reproduce the issue with 3 VMs (ESXi) running only RabbitMQ (no HAProxy). So here is a script to enable the Erlang debugger:\n``` sh\n!/bin/sh\nset -e\nrabbitmqctl eval '\ndbg:tracer(port, dbg:trace_port(file, \"/tmp/rabbitmq-server-224-trace.dbg\")),\n{ok, } = dbg:p(all, [m, c]),\n{ok, } = dbg:tpl(gm, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_node_monitor, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_amqqueue, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_amqqueue_process, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_coordinator, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_master, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_misc, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_mode, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_mode_all, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_mode_exactly, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_mode_nodes, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_slave, dbg:fun2ms(fun() -> return_trace(), exception_trace() end)),\n{ok, } = dbg:tpl(rabbit_mirror_queue_sync, dbg:fun2ms(fun(_) -> return_trace(), exception_trace() end)),\nok.\n'\n```\nAnd this one to disable the debugger:\n``` sh\n!/bin/sh\nset -e\nrabbitmqctl eval '\nok = dbg:stop().\n'\n```\nA file called /tmp/rabbitmq-server-224-trace.dbg will be created after the first script is executed.\nSo, please:\n1. Setup the cluster up to the point you are ready to power off one slave.\n2. Run the first script.\n3. Power off the slave.\n4. Confirm the issue is reproduced.\n5. Run the second script to stop the debugger.\nThe created file, /tmp/rabbitmq-server-224-trace.dbg, will be huge (in the range of several gigabytes): it contains all Erlang messages exchanged (it can contain sensitive informations so please ensure you use the guest:guest account for instance) and all uses of the listed Erlang modules. I hope it can be compressed efficiently.\nAlso, some questions on your setup:\n- When you power off a slave, is the VIP assigned to it?\n- Which flags are set when stuck queues are created (durable, autodelete, exclusive, etc.)?\n. One comment, not related to the discussed issue: I see your Erlang nodes communicate on the TCP port 35672:\nESTAB      0      0            192.168.200.31:33445       192.168.200.32:35672  users:((\"beam\",30972,11))\nIt appears to belong to the ephemeral ports range by looking at other TCP connections. So RabbitMQ has a small chance of startup failure if the port is already taken.\n. > Which sets the TCP_USER_TIMEOUT socket option to 5 seconds, the idea being to quickly detect when an established connection fails. I immediately was suspicious of this, so I removed it and tested without it and it still reproduced. However that was on 3.3.5, so I need to re-run my test on the latest RC and with this bit removed. I'll let you know how that goes.\nDid you have a chance to test again with 3.5.4-rcX and without TCP_USER_TIMEOUT?\n. Thank you @jeckersb for the debugger output, it helps!\nAs I understand, the setup is:\n- lb-backend-mac52540097b4be has IP address 192.168.200.31.\n- lb-backend-mac5254002e2e25 has IP address 192.168.200.32\n- lb-backend-mac5254001aaae6 has IP address 192.168.200.33\nYou powered off the third host, lb-backend-mac5254001aaae6.\nHere is my hypothesis; I will take the example of the queue named reply_b678b4bfbb0e47e5ae083fcf64aebf76:\n- It is flagged as auto-delete\n- lb-backend-mac5254002e2e25 runs the master process for this queue.\n- The two other nodes acts as a slave.\n- Before the problem, the queue has one consumer, identified by the connection 192.168.200.33:51131 -> 192.168.200.32:5672.\nThe consumer happens to be on lb-backend-mac5254001aaae6, the host about to be taken down.\nSo when lb-backend-mac5254001aaae is powered off:\n1. The last consumer is gone so the queue is stopped and deleted.\n2. During the queue shutdown, the master process on lb-backend-mac5254002e2e25 waits for the slave processes to also shutdown.\n3. However, on lb-backend-mac52540097b4be, the slave process detects that the master queue (or more exactly its associated GM) is terminating and promotes itself as the master for the queue.\n4. Therefore, the \"old\" master process waits forever because the slave expected to exit is now the \"new\" master.\n. Oh, I see we posted our analysis at the same time, I will read yours now :)\n. Here are more detailed timeline for the queue I took as example.\nFrom queues.txt:\nreply_b678b4bfbb0e47e5ae083fcf64aebf76  <rabbit@lb-backend-mac5254002e2e25.3.1940.0>    [<rabbit@lb-backend-mac52540097b4be.2.1777.0>, <rabbit@lb-backend-mac5254001aaae6.2.1895.0>]    false   true\n... we know that:\n- the master process is <0.1940.0> on lb-backend-mac5254002e2e25\n- the slave process is <0.1777.0> on lb-backend-mac52540097b4be (let's ignore the other slave)\n- the queue is flagged as auto-delete.\nFrom lb-backend-mac5254002e2e25 log, the client connection is lost:\n=ERROR REPORT==== 17-Jul-2015::13:22:02 ===\nclosing AMQP connection <0.1930.0> (192.168.200.33:51131 -> 192.168.200.32:5672):\n{inet_error,etimedout}\nThe associated channel notifies the connected queue that the consumer is gone (taken from the debugger output):\n(<6678.1936.0>) call rabbit_amqqueue:notify_down_all([<6678.1940.0>],<6678.1936.0>)\nNote: <6678.1940.0> is <0.1940.0> on lb-backend-mac5254002e2e25.\nThe queue can be deleted:\n(<6678.1940.0>) returned from rabbit_amqqueue_process:is_unused/1 -> true\n(<6678.1940.0>) returned from rabbit_amqqueue_process:should_auto_delete/1 -> true\nWe learn from the queue state:\n[{<6986.1896.0>,<6986.1895.0>},\n        {<6996.1778.0>,<6996.1777.0>},\n        {<6678.1943.0>,<6678.1940.0>}],\nthat:\n- <6678.1943.0> is the GM instance for the queue master process.\n- <6996.1778.0> is the GM instance for the queue slave process.\nThe queue master now stops its associated GM and waits for it and the slave processes to exit.\nThe master GM exits:\n(<6678.1940.0>) call gm:broadcast(<6678.1943.0>,{delete_and_terminate,normal})\n...\n(<6678.1943.0>) returned from gm:terminate/2 -> ok\n(<6678.1940.0>) << {'DOWN',#Ref<6678.0.0.111619>,process,<6678.1943.0>,\n                           {shutdown,ring_shutdown}}\nThe down slave is effectively gone:\n(<6678.1940.0>) << {'DOWN',#Ref<6678.0.0.111621>,process,<6986.1895.0>\n                           noconnection}\nBut no other DOWN message is received. Because on the slave, the local GM noticed the termination of the master GM:\n(<6678.1778.0>) << {'DOWN',#Ref<6678.0.0.8159>,process,<7016.1943.0>,\n                           {shutdown,ring_shutdown}}\nTherefore, all other GM instances are gone, the only one remaining being the local one:\n(<6678.1778.0>) call rabbit_mirror_queue_slave:members_changed([<6678.1777.0>],[],[<7016.1943.0>,<7015.1896.0>])\nThen, the slave is promoted as the new  master:\n(<6678.27161.0>) call rabbit_mirror_queue_misc:promote_slave([<6678.1777.0>])\n. Hi @ChrisCGH!\nI re-read the mailing-list thread and you're right, your maybe_stuck output matches this one.\nI need to finish a short task then I'll work on a fix for this race. I don't know yet what the correct fix is, but I will keep you posted.\n. Hi!\nThe GM is meant to handle disappearing members so in a normal situation, it should eventually notice the lost member and send the message to the next hop on the ring. So even in this case, the delete_and_terminate message would eventually reach the other slave.\nTo me, the real cause is that the delivery of delete_and_terminate is done asynchronously:\n1. The queue master wants to stop all slaves. It does this by sending delete_and_terminate to all members (members are the queue coordinator and the queue slaves; the coordinator is local to the master):\nerlang\n   stop_all_slaves(Reason, #state{name = QName, gm = GM}) ->\n         % ...\n         ok = gm:broadcast(GM, {delete_and_terminate, Reason}),\n2. A GM broadcast is asynchronous: it handles the message locally (see next point) and queues the message; a timer is responsible for regularily flushing the queue:\nerlang\n   handle_cast({broadcast, Msg, SizeHint}, State) ->\n       % The message is handed to the callback module (the coordinator in this case).\n       {Result, State1} = internal_broadcast(Msg, SizeHint, State),\n       % The message is queued, waiting for the timer to expire (the timer sends\n       % the 'flush' message at expiration).\n       handle_callback_result({Result, maybe_flush_broadcast_buffer(State1)});\n3. So the first to receive the message is the coordinator:\nerlang\n   handle_msg([CPid], _From, {delete_and_terminate, _Reason} = Msg) ->\n       % Ask the coordinator to exit.\n       ok = gen_server2:cast(CPid, Msg),\n       % Tell the GM it can exit.\n       {stop, {shutdown, ring_shutdown}};\nThis code sends the delete_and_terminate message to the queue coordinator process (who will exit as a consequence) and ask the GM to stop.\n4. Let's look again at the GM code already listed above to see that the GM is stopped immediately:\nerlang\n   handle_cast({broadcast, Msg, SizeHint}, State) ->\n       % The message is handed to the callback module (the coordinator in this case).\n       {Result, State1} = internal_broadcast(Msg, SizeHint, State),\n       % The message is queued, waiting for the timer to expire (the timer sends\n       % the 'flush' message at expiration).\n       %\n       % handle_callback_result() gets the 'stop' returned by the coordinator and\n       % stops the GM.\n       handle_callback_result({Result, maybe_flush_broadcast_buffer(State1)});\n5. The queued GM message didn't had a chance to be forwarded to the other members.\n6. However, after the GM exited, the DOWN message reaches the other slaves' GM, leading to the promotion of one of them.\n. This makes me think we may have the problem with mirrored auto-delete queues, no matter if a slave disappears or not: if my analysis is correct, there is a race between delete_and_terminate and the master's GM exit.\nI couldn't reproduce the problem so far. Now, I'm going to try with a cluster of two nodes to simplify the setup, and artificially increase the flushing timer period to increase the chance of DOWN reaching the slave first.\n. I was wrong: the GM flushes messages before it exits. I wil continue to read the code.\n. Ok, here is a new attempt at describing the problem. I still believe the problem is the asynchronous nature of the delivery of delete_and_terminate.\n1. My previous description is correct up to point 4: the GM queues the delete_and_terminate message internally then wants to stop.\n2. In its terminate callback, the GM flushes the pending messages:\nerlang\n   terminate(Reason, State = #state { module        = Module,\n                                      callback_args = Args }) ->\n       flush_broadcast_buffer(State),\n       % ...\n3. Flushing consists of two actions:\n   1. Actually send messages to the next member on the ring:\n  ``` erlang\n  ok = maybe_send_activity(activity_finalise(Activity), State),\n  ```\n\n  ```\n  (<6678.1943.0>) call gm:maybe_send_activity([{{0,<6678.1943.0>},[{9,{delete_and_terminate,normal}}],[]}], ...\n  (<6678.1943.0>) call gm:send_right({1,<6986.1896.0>}, ...\n  (<6678.1943.0>) call gm:cast(<6986.1896.0>,{'$gm',2,\n         {activity,{0,<6678.1943.0>},\n                   [{{0,<6678.1943.0>},\n                     [{9,{delete_and_terminate,normal}}],\n                     []}]}})\n  ```\n\n\n\nRecord the fact the GM now waits for an acknowledgment that the message reached all members.\nerlang\n  MembersState1 = with_member(\n                    fun (Member = #member { pending_ack = PA }) ->\n                            PA1 = queue:join(PA, queue:from_list(Pubs)),\n                            Member #member { pending_ack = PA1,\n                                             last_pub = PubCount }\n                    end, Self, MembersState),\n  State #state { members_state       = MembersState1,\n                 broadcast_buffer    = [],\n                 broadcast_buffer_sz = 0}.\n(<6678.1943.0>) returned from gm:with_member/3 -> [{{0,<6996.1778.0>},\n                                                      {member,{[],[]},0,0}},\n                                                     {{0,<6678.1943.0>},\n                                                      {member,\n                                                       {[{9,\n                                                          {delete_and_terminate,\n                                                           normal}}],\n                                                        []},\n                                                       9,8}},\n                                                     {{1,<6986.1896.0>},\n                                                      {member,{[],[]},0,0}}]\nNote the pending ack (PA): <6678.1943.0> (the master's GM, ie. the current process) waits for delete_and_terminate to go around the ring.\n4. However, GM's terminate does not store the updated state (with the pending ack), nor it does wait for this ack:\n\n\nerlang\n   terminate(Reason, State = #state { module        = Module,\n                                      callback_args = Args }) ->\n       flush_broadcast_buffer(State),\n       Module:handle_terminate(Args, Reason). % Does nothing in this case.\n5. Therefore, the GM exits without waiting for delete_and_terminate to be delivered properly. We know the consequence.\n. After reading gm.erl and talking with the team, the plan is to add a \"shutting down\" state to the GM.\nWhen asked to stop, instead of exiting almost immediately, the GM would enter this \"shutting down\" state: it will refuse any new broacast but sit there, waiting for acks for the pending messages. When all pending messages are ack'd, it would finally stop.\nI can't reproduce the problem myself: the client disconnection (leading to the deletion of the queue) is always detected some time after the loss of the slave. Would you be ok to test the patch once it's ready?\n. FYI, we are working on a replacement of this homebrew GM by a well-known proven algorithm called Raft. The project is here:\nhttps://github.com/rabbitmq/rabbitmq-ha-raft\nIt is a work in progress. Once finished, we hope this kind of races and unfulfilled promises will never happen again.\n. I committed the proposed patch in the rabbitmq-server-224 branch. It contains several debug messages which will be removed if the patch is accepted and merged to the main tree.\n@jeckersb, do you want me to provide a compiled gm.beam or an RPM package? Or are you willing to compile RabbitMQ yourself?\n. Thank you very much for the test! I wait for your confirmation after additional runs then.\n. As shown in the commit message, here is how to disable read buffering in the FHC:\nerlang\n%% In /etc/rabbitmq/rabbitmq.config\n[   \n  {rabbit, [\n    {fhc_read_buffering, false}\n  ]}  \n].\nBuffering is still enabled by default.\n. @videlalvaro: I used the test you described in #227. I set a low memory watermark for trigger paging earlier and the storage is a tmpfs.\nDisabling read and/or write buffering does not fix or change the problem.\n. After a suggestion from @michaelklishin, a log message tells if FHC buffering is turned on or not:\n=INFO REPORT==== 24-Jul-2015::20:00:06 ===\nFHC read buffering:  ON\nFHC write buffering: ON\nThe message is logged during startup.\n. Hi @tonal!\nThe rabbitmq_delayed_message_exchange plugin is not provided with RabbitMQ at this time. I suppose you installed it yourself when you were running 3.5.3, do you confirm this?\nSo our package does not handle third-party plugins correctly, we need to look at this.\n. Hi @tonal!\nThis is a known issue, see #232.\n. (Sorry about my previous comment, I thought it was a new issue, but you just added a comment :)\n@michaelklishin: I think @tonal idea is fine. In the current situation, an upgrade leaves the service in a broken state. We can also make it a debconf choice, defaulting to \"copy plugins\", and add some filtering (for instance, when a 3rd-party plugin is integrated in the core).\n. Once the entire Umbrella is converted, we can remove rabbit_misc:now_to_ms().\n. The pull requests are ready for review.\nBecause some APIs and exchanged messages are modified, there are inter-dependencies between pull requests:\n1. The following PRs must be reviewed and merged at the same time, otherwise plugins will break:\n   - rabbitmq/rabbitmq-server#254\n   - rabbitmq/rabbitmq-erlang-client#14\n   - rabbitmq/rabbitmq-management#52\n   - rabbitmq/rabbitmq-tracing#2\n2. Then, and only when the PRs above were merged, the following PRs can be reviewed and merged in any order:\n   - rabbitmq/rabbitmq-federation#11\n   - rabbitmq/rabbitmq-shovel#4\n   - rabbitmq/rabbitmq-stomp#25\n   - rabbitmq/rabbitmq-clusterer#2\n. Hi @tianon!\nI pushed the proposed patch to the rabbitmq-server-234 branch. Could you please give a try?\nThe diff is scary but the final RabbitMQ command line is simply put in a shell function. Then it checks the context: if RabbitMQ should be running in the foreground and the Erlang shell is disabled, then it setupts signal handlers. Otherwise, the behaviour is unchanged.\n. Right, dash manpage doesn't mention if the SIG prefix is required, optional or forbidden, but the examples don't have this prefix. I will remove it.\nAbout the strategy, there is no way in Erlang to setup signal handlers. It must be done either in a startup script or from an \"Erlang port\" (a forked program or a dlopen'd module). The latter is heavyweight and, IMHO, far less elegant than a startup script.\n. The SIG prefix was removed.\nOh, and thanks for the ?w=1 trick, I didn't know it :-)\n. Hello @tianon!\nDid you have a chance to test the latest patch?\n. Thank you for testing it!\nYeah, it's not ideal but there is nothing much we can do with Erlang. IMHO, the shell script is the cleanest/safest way to do it.\n. Since RabbitMQ 3.5.4.\n. The problem comes from rabbit_priority_queue:info/2: it has a special case for backing_queue_status and expects all other infos are integers which can be summed.\nThis is not the case for head_message_timestamp, it needs a special case too. I'm adding one.\n. The pull requests are against master and the milestone is set to 3.6.0 because the head_message_timestamp key was added there. It is not supported in 3.5.x.\n. Thanks!\n. Do you mean this should go in an ssl_compat module for instance?\n. I will do that.\n. Both pull requests depend on each other, so they must be tested at the same time.\n. This must be merged with rabbitmq/rabbitmq-erlang-client#13.\n. @Ayanda-D: The channel_operation_timeout:notify_down_all() test case fails with the following output:\n```\nchannel_operation_timeout\n\nnotify_down_all: [setup] [running]rabbit_test_runner: make_test_multi...failed\nin function gen_server:call/3 (gen_server.erl, line 212)\nin call from channel_operation_timeout:notify_down_all/1 (test/src/channel_operation_timeout.erl, line 59)\nin call from rabbit_test_runner:'-make_test_multi/7-fun-2-'/3 (src/rabbit_test_runner.erl, line 143)\n**exit:{{shutdown,\n     {connection_closing,{server_initiated_close,541,<<\"INTERNAL_ERROR\">>}}},\n {gen_server,call,[<0.93.0>,{close,200,<<\"Goodbye\">>},infinity]}}\n  output:<<\"\">>\n```\nLine 59 of test/src/channel_operation_timeout.erl is:\nerlang\namqp_channel:close(HareCh),\nIt fails to close the channel because the channel process crashes here (in rabbit_channel.erl, line 898):\nerlang\nhandle_method(#'channel.close'{}, _, State = #ch{reader_pid = ReaderPid}) ->\n    {ok, State1} = notify_queues(State),\nnotify_queues() returns the following error, which causes a badmatch exception:\nerlang\n{{error,{channel_operation_timeout,5000}}, ...}\nThe exception causes the channel process to be terminated and, as a consequence, the call from amqp_channel to be aborted with an exception (not caught by the test case).\nI don't know what is the best solution here: either handling the error returned by notify_queues() or handled the exception in the test case. I think I prefer the second one (ie. let the channel process terminates with a server error). What do you think?\n. Thank you! I confirm the test passes now.\n. I believe it's fixed by 7f3ab57c3846b98f7a8de055a384465013d02f82.\n@michaelklishin: Can you confirm it fixes the issue for you too?\n. Hi!\nI built your rabbitmq-server-264 branch to produce a test installer. However, I had to comment out line 116:\nhttps://github.com/ryanzink/rabbitmq-server/blob/rabbitmq-server-264/packaging/windows-exe/rabbitmq_nsi.in#L116\nOtherwise, I got the following error:\nmakensis -V2 rabbitmq-3.5.6.264.nsi\nError: can't load same language file twice.\nError in macro MUI_LANGUAGE on macroline 9\nError in script \"rabbitmq-3.5.6.264.nsi\" on line 116 -- aborting creation process\nmake[1]: *** [dist] Error 1\nI have no idea if this is correct. Could you please take a look and push a fix to your branch?\n. Hi @ryanzink!\nI made some fixes to the Windows installer today. While I'm at it, what is the state of your branch? If you think it's ready for testing, please submit a pull request.\n. I'm working on your pull request.\nJust to confirm the problem:\n- If I do a fresh install using .\\rabbitmq-server-3.6.0.exe /S, I get a dialog asking if I want to allow this program from an unknown vendor, then two cmd.exe windows. The installed files seem ok, the service works.\n- If I upgrade using silent mode, I get the same dialog warning about the unknown vendor, then three cmd.exe windows. The install is totally busted: the install directory only contains:\nRabbitMQ Server\n  `-- rabbitmq_server-3.6.0\n      `-- sbin\nI don't remember who suggested that but we have a problem during upgrade: the installer does not wait for the uninstall to complete. When using the interactive mode, it is usually not a problem because the user slow enough that the uninstall finishes before the install. When running in unattended mode, the install finishes before the uninstall (which needs to stop the service for instance). I believe that's why all files are removed after the installer finished in this case.\n. I first changed the installer to wait for uninstall before proceeding in 4dd361c. This fixes the \"install directory is empty after installing\".\nThe solution is documented here:\nhttp://nsis.sourceforge.net/When_I_use_ExecWait_uninstaller.exe_it_doesn't_wait_for_the_uninstaller\n. Thank you @ryanzink for your patch, it fixed the \"unattended\" install needing interaction :)\nNote that instead of merging your branch, I took the liberty to squash your commits into one and I also modified the style (removed trailing spaces and used 2-space indentation instead of tabs).\n. By the way, I never had the blank page so far, before or after your patch.\n. This crash didn't occur since #465 / #466 was fixed. The code does not check for {error, not_found} so it could happen again in theory. I don't know how to reproduce it.\n. Since RabbitMQ 3.5.2.\n. I agree, it would be better to enforce it.\nOptions {packet, raw} and {reuseaddr, true} should be enforced as well, IMHO. For example, someone on the mailing-list had problems overriding tcp_listen_options because he omitted reuseaddr.\n. Hi!\nThis works for me too. I also tried with a backslash in the password.\n. We are recreating the RPM and Debian packages with the script fixed. I will let you know when it's done.\n. The rerolled packages were delayed by #325. The new build is now in progress.\nWe are sorry for these two bugs. We really need to improve the testing of the Debian and RPM packages (the broken script is only used in those packages)...\n. New packages (3.5.5-3) were uploaded. Thank you reporting the problem!\n. In fact, our package are architecture-independent already. I was misled by the fact we build on a i386 VM, not an amd64.\n. Are you talking about test-rabbitmq-server-scripts? It doesn't run checkbashisms(1) currently.\nOur package testsuites might not covered everything either.\nTo me this issue is not addressed yet.. Thank you!\n. Hi!\nI came to the same conclusion. Could you please rebase your change on the stable branch?\n. Fixes #330.\n. The same solution will be applied to ssl_compat (#347).\n. I propose the following solution, tested in the following repository:\nhttps://github.com/dumbbell/erlang-time-compat\nThe idea is to build three modules from the same source file (time_compat.erl):\n- time_compat_old.beam, using the old Time API (ie. based on erlang:now/0);\n- time_compat_new.beam, using the new Time API;\n- time_compat.beam, a module which replaces itself with one of the other modules.\nSee swap_module() and do_swap_module() in time_compat.erl.\nThis example is incomplete because it only exports unique_integer/0.\nThe solution can be used for ssl_compat (#347), though the following test needs to be adapted:\nhttps://github.com/dumbbell/erlang-time-compat/blob/master/time_compat.erl#L57\nI believe the other tricky part is how to plug this solution to the build system (ie. building three modules out of one source file).\n. @michaelklishin: Why not for 3.6.1? The change should be transparent for the consumers of time_compat and ssl_compat. It should be an implementation detail.\n. Ok. I think we should target 3.6.x because the basic try/catch can be a performance penalty, though I didn't measure anything.\n. Hi @dcorbacho!\nEven if your solution is more complex than the proposed solution, it has the benefits of allowing a per-function selection. I like this flexibility. However, you don't use this possibility by hard-coding the switch between before and after Erlang 18.0.\nCould you please update this patch so the condition is inside the version_support attribute? For instance, we'll need a rand_compat module because random is deprecated in Erlang 19.0.\nAs for the syntax, what do you think of:\nerlang\n-erlang_version_support([\n    {unique_integers, 0, % I would keep the arity near the function name.\n     18, % Condition: ErlangVersion >= 18.\n     unique_integers_post_18, % Condition evaluates to true\n     unique_integers_pre_18}, % Condition evaluates to false\n    {new_api, 0,\n     19, % Condition: ErlangVersion >= 19.\n     new_api_post_19,\n     new_api_pre_19},\n    % More functions depending on any Erlang version...\n  ]).\nOr maybe:\nerlang\n-erlang_version_support([\n    {18, [ % Condition: ErlangVersion >= 18.\n     {unique_integers, 0,\n      unique_integers_post_18, % Condition evaluates to true\n      unique_integers_pre_18}, % Condition evaluates to false\n     % More functions depending on Erlang 18.0...\n     ]},\n    {19, [ % Condition: ErlangVersion >= 19.\n     {new_api, 0,\n      new_api_post_19,\n      new_api_pre_19},\n     % More functions depending on Erlang 19.0...\n     ]}\n  ]).\nBy the way, as noted in this example, I would rename the attribute to erlang_version_support because version_support is too vague/generic. Also, I would keep the arity near the function real name.\nI prefer the second example because the condition is not repeated.\nWhat do you think?\nAlso, we must remember to update rabbit_hipe.erl in rabbitmq-server. The use of code_version reverts the module to a non-native version of the module if it was hipe-compiled on the fly. rabbit_hipe uses the value of Mod:module_info(native) (which is back to false) to determine if the module should be hipe-compiled. So it needs to look at a second condition (maybe the presence of the erlang_version_support attribute?) to avoid recompiling such a module.\n. Here are some numbers to evaluate the impact of the try/catch and the proposed solution; probably something we should have done before working on this to know the impact :-)\nTo measure the impact, I use PerfTest to transmit 200,000 messages, five times in a row:\n$ for i in $(seq 1 5); do ./PerfTest.sh -C 200000 -D 200000; done\nI played with Erlang 17.5.6 and Erlang 18.2.2; only the latter has the new Time and Time Correction API. For each Erlang version, time_compat is based on:\n1. direct calls to the underlying API (so either only the direct call to the new API, or only the replacement for Erlang 17);\n2. code reload, as posted in rabbitmq/rabbitmq-common#42;\n3. try/catch, ie. plain stable branch.\nWith Erlang 18.2.2:\n$ ministat -w 74 {direct-calls,code-reload,try-catch}-18\nx direct-calls-18\n+ code-reload-18\n* try-catch-18\n+--------------------------------------------------------------------------+\n|+        x * *     x    x                  x  ++ *      +  *           ** |\n|           |_|____|_____M______A_________A_A___M_M_|_____________|_______||\n+--------------------------------------------------------------------------+\n    N           Min           Max        Median           Avg        Stddev\nx   5         14369         14494         14408       14424.2     49.956981\n+   5         14348         14492         14463         14450     58.749468\nNo difference proven at 95.0% confidence\n*   5         14375         14524         14469       14454.2     73.489455\nNo difference proven at 95.0% confidence\nSo in this scenario, we loose nothing with the try/catch. This is expected because the new Time and Time Correction API is used (almost) directly in the three cases.\nWith Erlang 17.5.6:\n$ ministat -w 74 {direct-calls,code-reload,try-catch}-17\nx direct-calls-17\n+ code-reload-17\n* try-catch-17\n+--------------------------------------------------------------------------+\n|*     *               ** * +     +      +  +     x x+     x          x   x|\n|    |__________A______M____|  |________AM_______||________M_A__________|  |\n+--------------------------------------------------------------------------+\n    N           Min           Max        Median           Avg        Stddev\nx   5         13892         14062         13959       13969.6     75.830733\n+   5         13740         13911         13828       13821.4     65.683331\nDifference at 95.0% confidence\n    -148.2 +/- 103.46\n    -1.06088% +/- 0.740608%\n    (Student's t, pooled s = 70.9387)\n*   5         13546         13724         13703         13653     81.067873\nDifference at 95.0% confidence\n    -316.6 +/- 114.478\n    -2.26635% +/- 0.819476%\n    (Student's t, pooled s = 78.493)\nHere, going through an ever-failing try to always fallback on the catch causes a 2% performance penalty in this scenario. With code rewritten on the fly, the penalty is down to 1%. Why 1%, I don't know; could Erlang apply some optimization we loose by doing on-the-fly reload?\n. Hi!\n@dcorbacho: I finally tested your branch again. I want to run a few more tests but I feel it's ready to be merged.\nOne last point, in the following code:\nerlang\n-erlang_version_support(\n   [{18,\n     [{monotonic_time, monotonic_time_pre_18, monotonic_time_post_18, 0},\n      {monotonic_time, monotonic_time_pre_18, monotonic_time_post_18, 1},\nWhat do you think of moving the arity right after the real function name? Something like:\nerlang\n-erlang_version_support(\n   [{18,\n     [{monotonic_time, 0, monotonic_time_pre_18, monotonic_time_post_18},\n      {monotonic_time, 1, monotonic_time_pre_18, monotonic_time_post_18},\nI find it more readable. What do you think?\n. Thank you!\n. Thank you! I finished testing. I will merge it just after 3.6.1.\n. Sorry, I totally missed the notification...\nI didn't read the code but your analysis is sensible. I agree that rabbitmqctl should check if the node it wants to impersonate is actually running, no matter if RabbitMQ is started or not. For instance, in the case of some pause modes, even if RabbitMQ is stopped, there is a process checking the cluster to determine when to restart RabbitMQ. I believe rabbitmqctl shouldn't mess with the database with this background process running: the user should explicitely stop the node first.\n. @videlalvaro: Do you remember how you triggered this crash?\n. Right, I should open my eyes while reading :)\n. I can't reproduce the crash but your patch looks sensible. I'm going to prepare a pull request for 3.6.0.\n. @michaelklishin, @videlalvaro: I'm fine with small low-impact new features in a stable branch. We already did that in the past. We can discuss what we mean behind our version numbers outside of this issue though, and in particular if we want to follow \"semver\" rules or not.\nIn this specific case, does an old rabbitmqctl still work with a new RabbitMQ? And the opposite?\n. Ok, here is the full explanation after finishing the investigations.\nOn Windows, we definitely had a problem: on my Windows 8.1 laptop, rabbit@%COMPUTERNAME% is rabbit@ROSETTA but erl -sname rabbit becomes rabbit@rosetta. Only the case changes and both names resolve to the correct IP address. But something in Erlang makes both names incompatible: when rabbit@rosetta is running, pinging rabbit@rosetta succeeds, pinging rabbit@ROSETTA fails. So we will keep the change in rabbitmq-env.bat. This means %COMPUTERNAME% is not used anymore which is a breaking change in RabbitMQ 3.6.0.\nThe other host where we had a problem was Travis CI's docker image. Here, rabbitmqctl without -n <nodename> or $RABBITMQ_NODENAME let to rabbitmqctl trying to contact rabbit@localhost. After multiple tests on Travis (see rabbitmq/playground@f83bb055 and https://travis-ci.org/rabbitmq/playground/builds/89185004), this has nothing to do with inconsistencies between hostname(1) and the name picked by Erlang (when using erl -sname rabbit): the Travis environment comes with a /etc/rabbitmq/rabbitmq-env.conf containing NODENAME=rabbit@localhost. Therefore, I will revert the change in rabbitmq-env.\nI will file a new issue so that, in the testsuite, we ignore /etc/rabbitmq/rabbitmq-env.conf.\n. What about implementing your first solution (comparing the number of queues and allowed opened files) to fill the gap, then go for the other solution on the longer term?\n. You are right, let's abort early.\n. References #264.\n. As noted in #264, this patch was merged after squashing commits into one and fixing the style.\nThank you!\n. Fixed in dfcde198635dfcc49db6eca5dfbd1f5a8480b767.\n. Beside disk vendors, I have the impression everybody use MiB naturally :-)\nAs for the syntax, I like when applications accept a suffix; for instance:\nerlang\n{limit, \"64\"}, % bytes\n{limit, \"256k\"},\n{limit, \"200M\"},\n{limit, \"1G\"},\nThis would require us to parse the string, but in the end, it's more user-friendly and it avoids any confusion about the implicit unit. As for error handling, we already do none if the term is not an integer, so it would not change. What do you think?\nWe would still need to choose between MiB and MB :-)\n. I wasn't suggesting we use the complete suffix, just k, M, G and T. For the unit, I more used to power of two, but that's a matter of taste; I'm ok with power of ten if we are consistent everywhere.\nAbout this Google search, it's logical it returns 10^6 because you specified \"megabytes\". It's consistent with SI definition.\n. Yeah, in Unix, \"MB\" is always 2^20, as opposed to SI's 10^6.\nWe can distinguish input and output. On the input side, as I previously said, I prefer the Unix tradition (ie. power of two) if the unit is implicit (eg. \"200M\"). If we support the full unit as a suffix, I would follow SI (\"MB\" = 10^6, \"MiB\" = 2^20) and both suffixes should be accepted.\nOn the output side (the log files or the management UI), we should always explicitely specify the unit. In this case, I prefer \"MiB\".\n. To be explicit on the input side, that means:\nerlang\n\"1\" == 1 % One byte.\n\"1k\" == \"1kiB\" == 1024 % A suffix w/o unit is Unix tradition.\n\"1kB\" == 1000\n. If we want to be consistent with SI, this must be case sensitive: \"m\" and \"M\" mean totally different things. We should stick to \"k\" (lowercase) and \"M\", G\", and \"T\" (uppercase).\n. Done in c8aa445577c8aa3b912649c84bf431325b13e197.\n. When this is fixed, my plan is to add a Jenkins job where the broker testsuite is executed with HiPE turned on. It will prevent issues like #455.\n. Not yet. We have more critical fixes to do first and we are busy with the 3.6.0 release at the moment.\n. Hi @jmoney8080!\nNo, unfortunately, it won't make it to 3.6.0. It requires non-trivial changes and we are too far in the release cycle.\n. Just a random thought while working on something unrelated: we could start an ephemere Erlang node before starting RabbitMQ itself and use net_kernel:monitor_nodes/{1,2} to detect if the RabbitMQ node started but failed to boot.\n. This issue is fixed by #468.\n. Note that I can reproduce the issue locally with master but not with stable, so I could only verify the patch against master. Jenkins can reproduce it with both branches.\n. Hi!\nLooks good to me. Could you please resubmit your patch against the stable branch?\n. Fixes #464.\n. FYI, I submitted #495 as an improvement to the Debian and RPM packages.\n. Fixed in c939cf5c4f45fce678f111f0079a237f4669e4ad.\n. I tested the branch on Windows and it works fine with the default settings, including log rotation.\nHowever, logging to stdout is broken. You can test with:\nmake run-broker RABBITMQ_LOGS=-\nIn rabbitmq-server, this becomes:\n-rabbit error_logger tty\nLager complains with the following message:\nWARNING: Lager's console backend is incompatible with the 'old' shell, not enabling it\nNote that, on Windows, this depends on a patch I didn't commit yet (rabbitmq-server.bat doesn't support RABBITMQ_LOGS=- as opposed to rabbitmq-server).\nThis could be specific to Windows though. I will test on FreeBSD once the profiling of lazy queues is finished (erlgrind is already running for almost three hours...)\n. I'm starting to look at this.\n. The problem with TTY is only present on Windows. It works as expected on Unix.\n. Apparently, this is a known limitation:\nhttps://github.com/basho/lager/blob/master/src/lager_console_backend.erl#L55\nhttps://github.com/basho/lager/blob/master/src/lager_console_backend.erl#L137\nIt's not a regression as it was impossible to enable console logging on Windows anyway.\nI'm testing a few more things but this branchs looks ready to be merged. My concern about the timer:sleep/1 can be addressed later.\n. We can't override the log file locations anymore using the environment variables, if lager is configured in rabbitmq.config. Those variables are only considered as the default values. And indeed, with the current rabbitmq-env and rabbitmq-server, we can't distinguish the default values from user-specified ones. I'm looking for a solution.\n. I agree it's more flexible and powerful. However, I find it handy if I can just set RABBITMQ_LOGS and have this one time log file. Another usecase : I have my complex logging preferences in rabbitmq.config but from time to time, I want RabbitMQ sends everything to stdout.\n. I still have a few things I would like to fix before merging this branch:\n- Lager's documentation recommends to use a parse_transform, {parse_transform, lager_transform}, during the build.\n- When a crash is logged, the message is even more unreadable than with standard error_logger format, except on stdout:\n# In rabbit-sasl.log\n  2016-01-06 12:01:41.341 [error] <0.227.0> CRASH REPORT Process <0.227.0> with 0 neighbours exited with reason: no function clause matching rabbit_memory_monitor:init([]) line 113 in gen_server2:init_it/6 line 601\n  2016-01-06 12:01:41.342 [error] <0.226.0> Supervisor rabbit_memory_monitor_sup had child rabbit_memory_monitor started with rabbit_memory_monitor:start_link() at undefined exit with reason no function clause matching rabbit_memory_monitor:init([]) line 113 in context start_error\n  2016-01-06 12:01:41.343 [error] <0.161.0> CRASH REPORT Process <0.161.0> with 0 neighbours exited with reason: {error,{{shutdown,{failed_to_start_child,rabbit_memory_monitor,{function_clause,[{rabbit_memory_monitor,init,[[]],[{file,\"src/rabbit_memory_monitor.erl\"},{line,113}]},{gen_server2,init_it,6,[{file,\"src/gen_server2.erl\"},{line,559}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}}},{child,undefined,rabbit_memory_monitor_sup,{rabbit_restartable_sup,start_link,[rabbit_memory_monitor_sup,{rabbit_memory_monitor,start_link,[]},false]},transient,infinity,supervisor,[rabbit_restartable_sup]}}} in application_master:init/4 line 134\n  2016-01-06 12:01:41.343 [info] <0.7.0> Application rabbit exited with reason: {error,{{shutdown,{failed_to_start_child,rabbit_memory_monitor,{function_clause,[{rabbit_memory_monitor,init,[[]],[{file,\"src/rabbit_memory_monitor.erl\"},{line,113}]},{gen_server2,init_it,6,[{file,\"src/gen_server2.erl\"},{line,559}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}}},{child,undefined,rabbit_memory_monitor_sup,{rabbit_restartable_sup,start_link,[rabbit_memory_monitor_sup,{rabbit_memory_monitor,start_link,[]},false]},transient,infinity,supervisor,[rabbit_restartable_sup]}}}\n```\n  # In rabbit.log\n  2016-01-06 12:01:41.351 [info] <0.2.0> Error description:\n  {could_not_start,rabbit,{error,{{shutdown,{failed_to_start_child,rabbit_memory_monitor,{function_clause,[{rabbit_memory_monitor,init,[[]],[{file,\"src/rabbit_memory_monitor.erl\"},{line,113}]},{gen_server2,init_it,6,[{file,\"src/gen_server2.erl\"},{line,559}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}}},{child,undefined,rabbit_memory_monitor_sup,{rabbit_restartable_sup,start_link,[rabbit_memory_monitor_sup,{rabbit_memory_monitor,start_link,[]},false]},transient,infinity,supervisor,[rabbit_restartable_sup]}}}}\nLog files (may contain more information):\n     rabbit.log\n     /tmp/rabbitmq-test-instances/rabbit/log/rabbit-sasl.log\n  ```\n```\n  # On stdout\n  BOOT FAILED\n  ===========\nError description:\n     {could_not_start,rabbit,\n         {error,\n             {{shutdown,\n                  {failed_to_start_child,rabbit_memory_monitor,\n                      {function_clause,\n                          [{rabbit_memory_monitor,init,\n                               [[]],\n                               [{file,\"src/rabbit_memory_monitor.erl\"},\n                                {line,113}]},\n                           {gen_server2,init_it,6,\n                               [{file,\"src/gen_server2.erl\"},{line,559}]},\n                           {proc_lib,init_p_do_apply,3,\n                               [{file,\"proc_lib.erl\"},{line,240}]}]}}},\n              {child,undefined,rabbit_memory_monitor_sup,\n                  {rabbit_restartable_sup,start_link,\n                      [rabbit_memory_monitor_sup,\n                       {rabbit_memory_monitor,start_link,[]},\n                       false]},\n                  transient,infinity,supervisor,\n                  [rabbit_restartable_sup]}}}}\nLog files (may contain more information):\n     rabbit.log\n     /tmp/rabbitmq-test-instances/rabbit/log/rabbit-sasl.log\n12:01:41.351 [info] Error description:\n     {could_not_start,rabbit,{error,{{shutdown,{failed_to_start_child,rabbit_memory_monitor,{function_clause,[{rabbit_memory_monitor,init,[[]],[{file,\"src/rabbit_memory_monitor.erl\"},{line,113}]},{gen_server2,init_it,6,[{file,\"src/gen_server2.erl\"},{line,559}]},{proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}}},{child,undefined,rabbit_memory_monitor_sup,{rabbit_restartable_sup,start_link,[rabbit_memory_monitor_sup,{rabbit_memory_monitor,start_link,[]},false]},transient,infinity,supervisor,[rabbit_restartable_sup]}}}}\nLog files (may contain more information):\n     rabbit.log\n     /tmp/rabbitmq-test-instances/rabbit/log/rabbit-sasl.log\n  ``\n- As shown above, the path torabbit.logis missing.\n. Also, I'm wondering if having two separate log files is still relevant. With the default settings, interesting messages are spread accross bothrabbit.logandrabbit-sasl.log`:\n```\nIn rabbit.log\n2016-01-06 11:54:45.332 [info] <0.173.0> Disk free limit set to 50MB\n2016-01-06 11:54:45.338 [info] <0.180.0> FHC read buffering:  OFF\nFHC write buffering: ON\n2016-01-06 11:54:45.347 [info] <0.162.0> Priority queues enabled, real BQ is rabbit_variable_queue\n2016-01-06 11:54:45.361 [info] <0.235.0> msg_store_transient: using rabbit_msg_store_ets_index to provide index\n2016-01-06 11:54:45.364 [info] <0.238.0> msg_store_persistent: using rabbit_msg_store_ets_index to provide index\n2016-01-06 11:54:45.379 [info] <0.2.0> Server startup complete; 0 plugins started.\n```\n```\nIn rabbit-sasl.log\n2016-01-06 11:54:45.317 [info] <0.167.0> Memory limit set to 953MB of 15897MB total.\n2016-01-06 11:54:45.337 [info] <0.179.0> Limiting to approx 470438 file handles (423392 sockets)\n2016-01-06 11:54:45.372 [info] <0.263.0> started TCP Listener on [::]:5672\n2016-01-06 11:54:45.372 [info] <0.269.0> started TCP Listener on 0.0.0.0:5672\n```\nShould be use a single file? What do you think?\n. @essen: Yeah, that's what I understand too. But it's harmless, so I believe we should add that now instead of fighting later to understand why a specific feature doesn't work.\nIn the future, we should probably move to a tighter integration of Lager and use its features to have even better logs.\n. About one vs. two files, Lager creates an additional file, $log_root/log/crash.log by default, where it stores the \"=CRASH REPORT====\" messages.\nSo in the current state, messages logged through rabbit_log goes to rabbit.log (using an extra sink) and those logged through error_logger goes to rabbit-sasl.log (using the standard Lager sink).\nI believe it should simplify log consumption and Lager configuration if we just used the standard sink and let Lager store larger sasl logs to crash.log. That was the purpose of rabbit-sasl.log, but we now don't need to duplicate Lager's job.\n@hairyhum: What do you think about using a single rabbit.log through the standard sink?\n. The more I think about this, the more I believe we should go the extra mile and drop rabbit_log.\nrabbit_log was nice to:\n- drop messages below a minimum level\n- filter by category (though we don't use it a lot).\nLager does both and in a better way.\nIMHO, we should use lager:{info,warning,error}() directly right now. The benefits are:\n- $file:$line is included in the message\n- We can take this occasion to add a category to all messages\nNow, whatever we choose, I'm not entirely satisfied with Lager's default behavior w.r.t. crash logging for messages emitted by Erlang (eg. by supervisor or gen_server). For example, if a queue crashes (I added an incorrect assertion in a()), 31 lines of totally unreadable (for an Erlang developer) mix of text, stacktraces and Erlang terms are written to the log file or stdout. For an end-user, replace \"unreadable\" by \"horrifying\" :-)\nI don't know yet what we can do about this. The same information is available in crash.log in a saner format. Do we have a way to either ignore or reformat those messages in the standard sink? Many users don't know how to parse those Erlang terms/stacktraces, so they are mostly useful to us.\n@hairyhum: Do you have an advice?\nEdit: Not 31 lines, but 58 lines. I forgot those above \"closing AMQP connection\".\nExample: https://gist.github.com/dumbbell/108d708422c00174c7a5\n. I see, so not what we want. Anyway, it's fine for now. In my example, the queue fails to restart, that's why there are in fact multiple crashes logged in a row. This is an extreme (and hopefully rare) situation.\n. @hairyhum, @michaelklishin: What do you think about deprecating rabbit_log and using the standard sink only through Lager API directly?\n. @hairyhum: That's interesting! I will look at that.\n@michaelklishin: By deprecating, I mean stop using it internally (and in our plugins). The module can stay (and should probably be moved to rabbit_common). It would become a wrapper above Lager API / standard sink.\n. Here is what I propose to fully use Lager and its features, while still maintaining compatibility for those using rabbit_log.\n- Let's use the default sink to log everything by default. For the end-user, it's easy to configure, because they only have to mess with the main list of handlers:\nerlang\n  % In rabbitmq.config\n  {lager, [\n      {handlers, [\n          {lager_console_backend, [info]}\n        ]}\n    ]}\n- By default (ie. if nothing is configured in rabbitmq.config), we honor $RABBITMQ_LOGS: a handler based on $RABBITMQ_LOGS is added to the default sink. If there is something configured, but $RABBITMQ_LOGS is forced on the command line (or rabbitmq-env.conf), the configured handlers are replaced with a handler based on $RABBITMQ_LOGS. This is mainly useful to developers when they want to temporarily redirect messages to stdout by starting make run-broker RABBITMQ_LOGS=- for instance.\n- We configure an extra sinks (except if it's already configured) called rabbit_log_lager_event. In combination with lager_transform and the +'{lager_extra_sinks,[rabbit_log]} compile option, this makes all calls to rabbit_log:<level>() to be transformed into direct calls to Lager. The advantage is that the logged message can contain metadata such as file name and line number.\n- To maintain compatibility, the rabbit_log module still exports those <level>() functions: they call Lager, specifying rabbit_log_lager_event as the sink. The result is the same, minus the automatically added metadata.\n- I added a lager_forwarder_backend module: it acts as a Lager backend which forwards all messages to a specified sink. The goal is to use it for rabbit_log_lager_event: if this sink is not configured, a lager_forwarder_backend handler is added to this sink and this handler is configured to forward everything to the default Lager sink. The end result is that messages logged with rabbit_log:<level>() are written to the same log file than messages logged with error_logger. This was necessary because Lager doesn't allow to have multiple lager_file_backend backends to point to the same file.\nLet's try to sum up this with a simple drawing of the default configuration:\nrabbit_log:info() --> rabbit_log_lager_event sink --+\n                                                    |\n                                          +---------+\n                                          |\n                                          V\nerror_logger:info_msg() --> lager_event default sink --> handler based on $RABBITMQ_LOGS\nNow, about categories. rabbit_log:log/{3,4} offers the possibility to specify a category. An admin could then configure per-category minimum log levels. I propose we leverage on extra sinks even further for this purpose:\n- +'{lager_extra_sinks,[rabbit_log]} becomes +'{lager_extra_sinks,[rabbit_log,rabbit_connection,rabbit_channel]} if we take two categories we currentl support, \"connection\" and \"channel\".\n- This means we can use rabbit_channel:info() to log a message to the rabbit_channel_lager_event sink.\n- Category-specific sinks are configured exactly like rabbit_log_lager_event. To be exact, I take the list of extra sinks from the module's compilation options stored in the beam:\nerlang\n  (rabbit@magellan)1> rabbit_lager:module_info(compile).\n  [{options,[\n         ...\n         {lager_extra_sinks,[rabbit_log,rabbit_connection,\n                             rabbit_channel]},\nThey are all treated the same.\n- This still allows the user to specify per-category minimum log level:\nerlang\n      {extra_sinks, [\n          {rabbit_channel_lager_event, [\n              {handlers, [\n                  % Do not send messages to $RABBITMQ_LOGS.\n                  {lager_forwarder_backend, [lager_event, none]},\n                  % Send messages to stdout.\n                  {lager_console_backend, info}\n                ]}\n            ]}\n        ]},\nLet's expand the previous drawing:\nrabbit_connection:info() --> rabbit_connection_lager_event sink --+\nrabbit_channel:info() --> rabbit_channel_lager_event sink --+     |\nrabbit_log:info() --> rabbit_log_lager_event sink --+       |     |\n                                                    |       |     |\n                                          +---------+-------+-----+\n                                          |\n                                          V\nerror_logger:info_msg() --> lager_event default sink --> handler based on $RABBITMQ_LOGS\nAdmittedly, this is more complicated to configure than what we have currently, but it's also way more powerful. Lager allows finer-grain control of the log levels: instead of a bare minimum, it supports masks and ranges.\n. > Should lager_forwarder_backend be rabbit_lager_forwarder_backend, though, because it is RabbitMQ-specific?\nThere is nothing RabbitMQ-specific in it and currently, it lives in rabbitmq-common (where rabbit_log and rabbit_lager should probably go). We could even ship it separately.\n. I just pushed the proposed patch so you can read the code and test if you want.\n. I didn't update the testsuite yet. I want to make sure the behavior is what we want first.\n. That said, I should commit lager_forwarder_backend if you actually want to test :-)\n. Thanks, I will update the testsuite, then.\n. The testsuite is up-to-date so this issue is ready for a final test :-)\n. The tested approach is to use short filenames. For example, C:\\Users\\Jean-S\u00e9bastien becomes C:\\Users\\JEAN-S~1. Short filenames are inherited from the era when Windows needed to support 8.3 filenames from DOS to remain backward compatible with earlier versions of Windows and DOS. Short filenames contain only US-ASCII characters, no spaces and are limited to the 8.3 naming.\nUsing short filenames works nicely when the file already exists. If it doesn't exist, we can't use short filenames, otherwise, the file would be created with its short name on disk.\nSo far, the only downside is that those short filenames are ugly. But it makes RabbitMQ work out-of-the-box, including the management UI.\nI played a bit with PowerShell scripts, but I couldn't find anything promising. They seem nice for sysadmins who want to automate their own stuff, but they look not designed to write startup scripts.\n. In the near future, I plan to use ucf in the Debian package to manage user-controled files, so the default set_rabbitmq-policy.sh will be installed as is on a fresh install. There is probably a similar tool for RPM, I didn't look at it yet.\n. @bogdando: The default script is empty so I don't see why it's needed to have mirrored queues. Do I miss something?\n. Thank you!\n. @Gsantomaggio: If you are still able to reproduce #307, could you please try this patch?\n. I vote for Erlang 18.x because the maps implementation is much better apparently. We could get rid of the time_compat and ssl_compat modules at the same time.\n. @essen: On FreeBSD, the package maintainer does a pretty good job. Erlang 18.2.2 is already available and is what you get if you ask for \"erlang\" (without specifying a version).\n. FYI, our test framework was fixed to work with Erlang 18.2; see rabbitmq/rabbitmq-test#29.\n. There are at least two regressions with TLS in 18.3.x:\n- RabbitMQ doesn't boot at all with 18.3.2 (see ninenines/ranch#145); fixed in 18.3.3\n- The ObjectiveC client fails to connect using TLS to RabbitMQ when it runs on 18.3.3.\n. Hi!\nCould you please rebase your PR to resolve the conflict?\n. Thank you!\n. Cuttlefish's syntax looks simple and straightforward, I like it.\n. This patch was tested on a French version of Windows 8.1. My username contains \u00e9 on this computer.\n. This pull request against master was recreated against stable in #582.\n. Hi @luckydogchina! What do you mean?. All tests in rabbit_tests pass, including the one testing the conversion from default to lazy queue. push_betas_to_deltas() is called in reduce_memory_use() for lazy queues during the latter.\nIn the end, the total runtime for make unit is around 20 minutes, where make lazy-vq-test takes 10 minutes (it was more like 40 minutes before the change). Note that make lazy-vq-test runs only test_lazy_variable_queue(), unlike make unit which runs all_tests().\n. I believe the change to be correct because default and lazy queues now have the same decision process when dealing with beta to delta conversion. It works for default queue and, if I understand correctly, lazy queue doesn't change that part.\n. This pull request replaces #554 and targets stable.\nFixes #493.\n. To test this branch, I tried the following scenario on a French Windows 8.1:\n1. Using make run-broker TEST_TMPDIR=/c/Path/With/Non-ASCII-Characters (note the use of forward slashes here).\n2. Using cmd.exe and rabbitmq-server.bat directly:\n``` batch\n   set RABBITMQ_BASE=%TEMP%\\\u00e9\n   set RABBITMQ_MNESIA_BASE=%TEMP%\\\u00e0\n   set RABBITMQ_LOGS=%TEMP%\\\u00f9\n   set RABBITMQ_ENABLED_PLUGINS_FILE=%TEMP%\\\u20ac\n.\\scripts\\rabbitmq-plugins.bat list\n   .\\scripts\\rabbitmq-plugins.bat enable --offline rabbitmq_management\n   .\\scripts\\rabbitmq-server.bat\n   ```\nIn both cases, I went to the management UI.\n. This is a documented compromise in rabbit_queue_index, when a message store is converted from an old to a newer format:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_queue_index.erl#L1285-L1289\n. In fact, this compromise has a larger impact than what the comment says: this size is also used to determine if the message should be stored in a message store or the queue index directly, depending on the value of queue_index_embed_msgs_below.\nBy default, this value is set to 4096 bytes. This means that messages coming from a converted message store on a master node would all be kept in the queue index on a slave node, because the latter would receive messages with a size of 0 in their properties.\nIf the master node is restarted, it will in turn store messages received from the promoted node in the queue index. At this stage, when the cluster is restarted entirely, the first node to start will load the first segment of queue indexes (which contains message bodies now) into memory. And because rabbitmq_variable_queue.erl doesn't pay attention to memory footprint during queue initialization and because the queue's in/out rates are not computed during init (thus, the memory monitor won't kick in either), the node can blow up.\nA workaround for this is to set queue_index_embed_msgs_below to 0 in RabbitMQ's configuration before the mirroring takes place and put messages in the queue index.\n. Sorry, I didn't receive any notification from GitHub and missed this issue.\nI was using sed -E (sed -r was only for a user of really old GNU sed) because extended POSIX regular expressions are portable, unlike sed without -E. At least in my experience.\nI didn't test this yet, but I would not be surprised if it's broken on *BSD and some OS X releases.\n. Thank you @Gsantomaggio!\n. Hi!\n@gmr: You can use Erlang.mk's eunit facility instead, it should be simpler for you.\nHowever, currently, our test framework has a limitation (among many others): even if you don't set any WITH_BROKER_TEST_COMMANDS or WITH_BROKER_TEST_SCRIPTS, it will still start a node, just to stop it right after.\n. Hi!\nSo I'm testing your pull request on Debian Jessie (8.3, systemd), freshly upgraded from Wheezy (sysvinit). As this is the first time I play with systemd, I'm going to post comments which will probably seem obvious to you, @binarin. I just want to do it because it helps me to understand the problem and correctly test the PR. Feel free to correct me what I say doesn't make any sense :-)\n1. First, how our current package (without your patch) behaves with systemd. I installed 3.6.1 using a package downloaded from www.rabbitmq.com. Once the install is complete, the service is automatically started:\n```\n   root@rabbit-jpedron-debian1# service rabbitmq-server status\n   \u25cf rabbitmq-server.service - LSB: Manages RabbitMQ server\n      Loaded: loaded (/etc/init.d/rabbitmq-server)\n      Active: active (running) since Thu 2016-03-10 11:04:43 GMT; 58s ago\n      CGroup: /system.slice/rabbitmq-server.service\n              \u251c\u25002693 /usr/lib/erlang/erts-7.2/bin/epmd -daemon\n              \u251c\u25002723 /bin/sh -e /usr/lib/rabbitmq/bin/rabbitmq-server\n              \u251c\u25002997 /usr/lib/erlang/erts-7.2/bin/beam.smp ...\n              \u251c\u25003115 inet_gethost 4\n              \u2514\u25003116 inet_gethost 4\nroot@rabbit-jpedron-debian1# rabbitmqctl status\n   Status of node 'rabbit@rabbit-jpedron-debian1' ...\n   [{pid,2997},\n    {running_applications,[{rabbit,\"RabbitMQ\",\"3.6.1\"},\n   ...\n   ```\nA unit file was automatically generated by systemd-sysv-generator and written to /run/systemd/generator.late/rabbitmq-server.service:\n```\n   # Automatically generated by systemd-sysv-generator\n[Unit]\n   SourcePath=/etc/init.d/rabbitmq-server\n   Description=LSB: Manages RabbitMQ server\n   Before=runlevel2.target runlevel3.target runlevel4.target runlevel5.target shutdown.target\n   After=remote-fs.target network-online.target\n   Wants=network-online.target\n   Conflicts=shutdown.target\n[Service]\n   Type=forking\n   Restart=no\n   TimeoutSec=5min\n   IgnoreSIGPIPE=no\n   KillMode=process\n   GuessMainPID=no\n   RemainAfterExit=yes\n   SysVStartPriority=16\n   ExecStart=/etc/init.d/rabbitmq-server start\n   ExecStop=/etc/init.d/rabbitmq-server stop\n   ExecReload=/etc/init.d/rabbitmq-server reload\n   ```\nThe service type defaults to forking, which means systemd expects the ExecStart command to fork a process and exit. The main PID could be derived from the PIDFile but this option is not set. If I understand correctly, systemd doesn't care about the notification we send with systemd-notify because the service type is not notify. See https://www.freedesktop.org/software/systemd/man/systemd.service.html#Options.\nI guess that's the reason systemd is unable to track the service processes correctly and therefore, issueing rabbitmctl stop (ie. bypassing systemd) doesn't tell systemd the service is stopped:\n```\n   root@rabbit-jpedron-debian1# rabbitmqctl stop\n   Stopping and halting node 'rabbit@rabbit-jpedron-debian1' ...\nroot@rabbit-jpedron-debian1# service rabbitmq-server status\n   \u25cf rabbitmq-server.service - LSB: Manages RabbitMQ server\n      Loaded: loaded (/etc/init.d/rabbitmq-server)\n      Active: active (running) since Thu 2016-03-10 11:04:43 GMT; 16min ago\n      CGroup: /system.slice/rabbitmq-server.service\n              \u2514\u25002693 /usr/lib/erlang/erts-7.2/bin/epmd -daemon\n   ```\nIn the example above, we see that systemd has no idea which process is the main one. After rabbitmqctl stop, epmd is still running and the service status is active (running). If I kill epmd, the service status becomes active (exited). Any service rabbitmq-server start does nothing. Only service rabbitmq-server stop sets the status correctly, allowing one to restart the service. But as discussed internally, this is acceptable in this situation: the user is not supposed to mess with the service with a mix of official (service(8)) and unofficial (rabbitmqctl) tools.\n2. Installing a package created from @binarin's pull request gives the exact same result out-of-the-box (ie. without installing the example unit file). That's logical because the patch relies on the provided unit file.\nNow, I'm going to install the unit file and test further. In the end, the probable conclusion is that our RPM and Debian packages should install this unit file out-of-the-box, but I need to see how to handle both init systems with a single package if possible.\n. Just a note while I'm on it: Debian official package of erlang-base for Jessie includes a unit file for epmd. However, Erlang Solutions' one doesn't. We may have a service dependency issue here.\n. 1. I stopped the service with service rabbitmq-server stop.\n2. I copied rabbitmq-server.service.example to /lib/systemd/system/rabbitmq-server.service. Now, the service is listed when I query all unit files (including disabled ones):\nroot@rabbit-jpedron-debian1# systemctl list-unit-files | grep rabbitmq-server\n   rabbitmq-server.service                disabled\nI double-checked that this entry doesn't come from the one generated by systemd-sysv-generator: before I copied the unit file, there was no rabbitmq-server entry in the list.\n3. Using service rabbitmq-server start directly doesn't improve the situation because the generated unit file is still being used (ie. the Loaded: line in the status still points to the SysV init script).\n4. Doing systemctl daemon-reload is enough to make the real unit file override the generated one (which is removed by the way):\n```\n   root@rabbit-jpedron-debian1# systemctl daemon-reload\nroot@rabbit-jpedron-debian1# service rabbitmq-server status\n   \u25cf rabbitmq-server.service - RabbitMQ broker\n      Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; disabled)\n      Active: inactive (dead)\n   ```\nThe service points to the copied unit file.\n5. Let's start it:\n```\n   root@rabbit-jpedron-debian1# service rabbitmq-server start\nroot@rabbit-jpedron-debian1# service rabbitmq-server status\n   \u25cf rabbitmq-server.service - RabbitMQ broker\n      Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; disabled)\n      Active: active (running) since Thu 2016-03-10 12:07:32 GMT; 2s ago\n    Main PID: 13532 (beam.smp)\n      CGroup: /system.slice/rabbitmq-server.service\n              \u251c\u250013532 /usr/lib/erlang/erts-7.2/bin/beam.smp ...\n              \u251c\u250013611 /usr/lib/erlang/erts-7.2/bin/epmd -daemon\n              \u251c\u250013719 inet_gethost 4\n              \u2514\u250013720 inet_gethost 4\n   ```\nIt works and systemd has the correct main PID.\n6. Stopping the service works correctly too. However, starting it again doesn't: service rabbitmq-server start hangs and the service status is activating, even though RabbitMQ finished to start:\nroot@rabbit-jpedron-debian1# service rabbitmq-server status\n   \u25cf rabbitmq-server.service - RabbitMQ broker\n      Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; disabled)\n      Active: activating (start) since Thu 2016-03-10 12:10:34 GMT; 1min 6s ago\n    Main PID: 13943 (beam.smp)\n      CGroup: /system.slice/rabbitmq-server.service\n              \u251c\u250013943 /usr/lib/erlang/erts-7.2/bin/beam.smp ...\n              \u251c\u250014025 /usr/lib/erlang/erts-7.2/bin/epmd -daemon\n              \u251c\u250014133 inet_gethost 4\n              \u2514\u250014134 inet_gethost 4\n=INFO REPORT==== 10-Mar-2016::12:10:36 ===\n   Server startup complete; 0 plugins started.\nMar 10 12:10:35 rabbit-jpedron-debian1 rabbitmq-server[13943]: ######  ##        /var/log/rabbitmq/rabbit@rabbit-jpedron-debian1-sasl.log\n   Mar 10 12:10:35 rabbit-jpedron-debian1 rabbitmq-server[13943]: ##########\n   Mar 10 12:10:36 rabbit-jpedron-debian1 systemd[1]: Cannot find unit for notify message of PID 14136.\n   Mar 10 12:10:36 rabbit-jpedron-debian1 rabbitmq-server[13943]: Starting broker... completed with 0 plugins.\n   Mar 10 12:12:04 rabbit-jpedron-debian1 systemd[1]: rabbitmq-server.service start operation timed out. Terminating.\n   Mar 10 12:12:04 rabbit-jpedron-debian1 systemd[1]: Failed to start RabbitMQ broker.\nIf I understand systemd logs correctly, the READY notification came from PID 14136, which is incorrect. This number looks like a process worked by Erlang though. systemd eventually killed the service.\nI will continue to debug this and learn more about systemd.\n. > epmd dependency isn't a problem - it's implicit because jessie uses socket activation for epmd.\nOk, perfect :-) Thanks!\n. Good job finding the reported issue. I tried to debug this with strace(1), but the problem never occurred with. This led me in the direction of a race with the exit of the shell, but I couldn't anything else.\nI'm not a fan of the sd_notify NIF because we can't ship it easily in our binary packages, and if we don't ship it, it will be painful to end-users.\nHere are two comments:\n1. Instead of introducing $RABBITMQ_RUNNING_UNDER_SYSTEMD, I believe we can rely on the existing $NOTIFY_SOCKET variable which is required by sd_pid_notify(3).\n2. I played with a simple Perl script which does what sd_pid_notify(3) would do: write to the Unix socket pointed to by $NOTIFY_SOCKET. However, in addition, it waits for the service state to not be activating anymore. Here is the script:\n``` perl\n   #!/usr/bin/env perl\n   # vim:ft=perl\nuse strict;\n   use warnings;\nuse IO::Socket::UNIX;\nmy $SERVICE = 'rabbitmq-server';\n   my $MAINPID = $ARGV[0];\n   my $SOCK_PATH = $ENV{'NOTIFY_SOCKET'};\nmy $sock = IO::Socket::UNIX->new(\n       Type => SOCK_DGRAM(),\n       Peer => $SOCK_PATH,\n   );\n   die \"Could not create socket: $!\\n\" unless $sock;\nprint $sock \"READY=1\\nMAINPID=$MAINPID\\n\";\nclose($sock);\nsub is_activating($) {\n       my ($service) = @_;\n   my $cmd = 'systemctl show --property=ActiveState '.$service;\n   my $state = `$cmd`;\n   chomp($state);\n\n   $state eq 'ActiveState=activating';\n\n}\nexit 0 unless is_activating($SERVICE);\ndo {\n       sleep(1);\n   } while (is_activating($SERVICE));\nexit 0;\n   ```\n. I tried socat(1) too yesterday and it worked fine in my tests, but I guess we could still hit the problem. Here is the paragraph from the systemd issue (systemd/systemd#2737) I'm referring:\n\nThis is because the the manager decides which units it applies to based on the cgroup string. And it decides the cgroup string by looking at /proc/${sending_pid}/cgroup, which won't exist anymore if the sending process gets cleaned up before systemd gets to handling the message.\n\nAbout the availability, socat is obviously available as a package on all distributions, but it's not installed by default. Our RPM/Debian package can depend on it of course.\n. About the footprint on disk: Perl is already installed on the Debian Jessie VM I used to test.\n. By the way, we may have another issue waiting for us after this systemd bug: systemd kills the service if it didn't report readyness in time. However, RabbitMQ may take several dozen minutes to load queue data or sync mirrored queues, all this before reporting \"I'm ready!\" to systemd.\nI don't know if we can disable this timeout from systemd for rabbitmq-server (probably not). I saw that the notify protocol allows regular status report in the form of STATUS=<arbitrary text> on the same Unix socket. I have no idea yet if this resets systemd timers, but it's worth investigating.\n. Nice! Your solution is better. I didn't think of making the state check in the pipe itself to maintain socat up.\nA few comments:\n- Could you please get rid of all Bash extensions, such as [[ ... ]]?\n- In the same \"spirit\", please use printf(1) instead of echo where -e and -n are not portable; I know this script is Linux-specific, but it's just to keep all scripts consistent.\n- Instead of putting set flags on the shebang line, I prefer to have an explicit set -eux line in the script itself: if I run the script using /bin/sh <script> instead of ./script, the flags are still set.\nAbout the startup timeout, I saw the TimeoutStartSec and TimeoutStopSec service options. On this Debian, they both defaults to 90 seconds. It's possible to set them to infinity to effectively disable the timeout. It may be the sanest choice for RabbitMQ.\n. Nice patch, thank you! I didn't test it yet, but is there any benefit from keeping support for the NIF?\n. I merged the pull request. Thank you!\n. Thank you @binarin!\n. Here is the behavior of systemd with the proposed patch applied:\n1. systemd sees the main process is gone:\n```\n   # rabbitmqctl stop\n   Stopping and halting node 'rabbit@rabbit-jpedron-debian1' ...\n# service rabbitmq-server status\n   \u25cf rabbitmq-server.service - RabbitMQ broker\n      Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; disabled)\n      Active: deactivating (stop) since Fri 2016-03-18 14:35:09 GMT; 147ms ago\n    Main PID: 23256 (code=exited, status=0/SUCCESS);         : 23874 (beam.smp)\n      Status: \"Initialized\"\n      CGroup: /system.slice/rabbitmq-server.service\n              \u251c\u250023329 /usr/lib/erlang/erts-7.3/bin/epmd -daemon\n              \u2514\u2500control\n                \u2514\u250023874 /usr/lib/erlang/erts-7.3/bin/beam.smp...\n   ```\nThe beam.smp process under control is systemd running rabbitmqctl stop.\n2. This second rabbitmqctl stop now exits with 0 because the node is already stopped. Thus, systemd considers the service is in good shape:\n# service rabbitmq-server status\n   \u25cf rabbitmq-server.service - RabbitMQ broker\n      Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; disabled)\n      Active: inactive (dead)\n. @noahhaon: To answer your last question, reprepro uses a \"pool\" of packages to share them among several distributions, instead of duplicating files. You can find it at http://www.rabbitmq.com/debian/pool/.\nNow, about the signing digest algorithm, I confirm we use the default SHA1 and that Ubuntu 16.04's apt displays a warning. That said, it let me install RabbitMQ.\nAs you suggest, we can set digest-algo in our gpg.conf to change that algo. However, this is discouraged by the documentation and best practices. The appropriate parameter is personal-digest-preferences.\nUnfortunately, our key is an old DSA-1024 key and personal-digest-preferences doesn't apply to that specific kind of key if I understand correctly (according to the FAQ: https://gnupg.org/faq/gnupg-faq.html#hash_widths_in_dsa). With DSA-1024, the preference is \"SHA1 RIPEMD160\".\nSo either we use the non-recommended digest-algo argument or we create a new key.\nBecause our current key is really old and weak, I would prefer to go with a new key, following today's best practices. We'll discuss that internally.\n. After some tests on Ubuntu Server 16.04, it looks like apt-get update displays a warning, but I can still install RabbitMQ without trouble.\nWe decided to go with a new stronger key and start the transition. Because we can sign the repository again whenever we want, the 3.6.2 release cycle will continue independently.\nTo create the new key, I used the following popular checklist:\nhttps://help.riseup.net/en/security/message-security/openpgp/best-practices\nThe new one has a RSA-4096 primary key to certify other keys, and two RSA-4096 subkeys; one to encrypt, one to sign. I tested it with a plain text file and our Debian repository and GnuPG picks SHA-512 as the digest algo without forcing it with digest-algo.\nWith the freshly installed Ubuntu mentionned above and this newly signed repository, apt-get udpate is obviously unhappy because I didn't import the new public key:\n...\nHit:4 http://security.ubuntu.com/ubuntu xenial-security InRelease                                                     \nHit:5 http://fr.archive.ubuntu.com/ubuntu xenial-backports InRelease                                                  \nErr:3 http://.../debian testing InRelease\n  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY EB64493E850B026F\n...\nW: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: http://.../debian testing InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY EB64493E850B026F\nW: Failed to fetch http://.../debian/dists/testing/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY EB64493E850B026F\nW: Some index files failed to download. They have been ignored, or old ones used instead.\nWith the new key imported, apt-get update is happy.\nAbout the transition, we will follow the process described in the link mentionned above:\n- [x] Create a new key for releases\n- [x] Create a new key for nightlies\n- [x] Sign the new keys with the old keys\n- [x] Publish a \"transition statement\" for the Release key, signed with both relevant keys\n- [x] Publish a \"transition statement\" for the Nightlies key, signed with both relevant keys\n- [x] Announce the new keys with the transition statements on our website, the mailing-list and Twitter\n- [x] Start to use the new keys for our Debian repository\n- [x] Start to use the new key to sign the next release artifacts\n- [x] Start to use the new key to sign the next nightly artifacts\nUpdate: The roadmap above was updated to take into account the transition of the Nightlies key which suffers the same weakness.\nUpdate: We decided to use a single key with two signing subkeys for the release and nightly artifacts. That way, end user will only have to import one key, no matter whichi branch they want to use.\nUpdate: Thanks to RPM poor support for PGP keys, we are back to two simple RSA-4096 keys; we can't use subkeys.\n. Thanks! That confirms my findings. And even if we could continue to use the old DSA-1024 key, it's a perfect opportunity to transition to something more current :-) I did that for my own key two years ago and it went more easily than I thought.\n. We need to transition the key for the nightly builds too! I'm updating the roadmap above.\n. So RPM support for PGP key and signature is ten years old and quite poor. In particular, we can't use subkeys. Thus, we are back to two simple keys...\n. The Debian repository is now signed with the new release key; it was made available a few minutes ago.\napt(1) from Ubuntu 16.04 is now happy!\n. > This was never about the Signing key. It was about that Warning.\n@gsker: This was both a problem with the signing key and the signature. The previous key was a DSA/1024 key which didn't permit to use a SHA-512 digest. Thus we had to replace it first with something more current.\nThe warning re-appeared recently because of a regression in our build process. The digest preference was not honored and GnuPG used SHA-1. We restored the configuration to select SHA-512 explicitely and the repository is signed again. The regression should be solved now.. Thanks a lot! This feature was badly needed!\n. This catch was introduced in 44e1aca07b15215dd90ec44f78007e10706e97cf. The goal is to catch throw({error, {no_such_vhost, VHost}}. So just removing this catch isn't enough.\n. I'm going to add tests but only in the common_test branch because our current framework doesn't do parallel tests.\n. Ok, I added tests which check if a policy was properly updated by looking at its properties. This is in the rabbitmq-server-725 branch only, not in stable or master.\n. The tests don't check that, but the Mnesia transaction is completed successfully; otherwise an exception is thrown.\n. In the case of the testsuite and with this patch, the transaction function fails but Mnesia restarts the transaction which succeeds eventually.\nIn rabbit_cpolicy.erl, the transaction is expected to succeed:\nerlang\n{Xs, Qs} = rabbit_misc:execute_mnesia_transaction(\nIf the transaction is aborted, rabbit_misc throws an exception.\nSo the difference between without and with the patch is that the transaction is restarted with the patch. Without the patch the transaction function crashes (so not a transaction abort).\nHowever, with or without the patch, rabbit_policy.erl may not ensure consistency between policy updates and queue/exchange updates. This is something I didn't check. If this is true, this is another bug which this patch doesn't try to address.\nWhile here, I see another problem:\nerlang\n    {Xs, Qs} = rabbit_misc:execute_mnesia_transaction(\n                 fun() ->\n                         [mnesia:lock({table, T}, write) || T <- Tabs], %% [1]\n                         case catch list(VHost) of\n                             % ...\n                             {error, {no_such_vhost, _}} ->\n                                 ok; %% [2]\n                               % ^^\nThe code expects the transaction to return a tuple, not ok.\n. In my tests, without the patch, the transaction function crashes 30% of the time while running the testsuite on policies. With the patch, it always succeeds and thus returns the expected tuple. I'm not 100% sure the transaction is restarted, however, that's the only possibility I see.\n. I filed #755 to address the second problem in the transaction function and I will add a patch to it in this pull request.\n. @hairyhum: Yes, I will add one in the common_test branch.\n. Yes, I see the same thing.\n. What I get is really weird. Here is what I do from a test case (not manually):\n1. Create a vhost\n2. Create a policy on this vhost\n3. Delete the vhost.\nCurrently, I'm doing this synchronously, so I'm not testing the removal of the vhost during the policy update. However, deleting the vhost fails with the same error you get, even though a call to list_vhosts clearly shows it. If I skip the policy addition, everything's fine.\nSo far, I have no idea why this happens. I must misinterpret what I'm seeing.\n. The same test from the CLI confirms there is something broken with vhost removal:\n```\n$ ./scripts/rabbitmqctl add_vhost /bla\nCreating vhost \"/bla\" ...\n$ ./scripts/rabbitmqctl delete_vhost /bla\nDeleting vhost \"/bla\" ...\n```\n```\n$ ./scripts/rabbitmqctl add_vhost /bla\nCreating vhost \"/bla\" ...\n$ ./scripts/rabbitmqctl set_policy -p /bla pouet \".\" \"{\\\"ha-mode\\\":\\\"all\\\"}\"\nSetting policy \"pouet\" for pattern \".\" to \"{\\\"ha-mode\\\":\\\"all\\\"}\" with priority \"0\" ...\n$ ./scripts/rabbitmqctl list_vhosts\nListing vhosts ...\n/bla\n/\n$ ./scripts/rabbitmqctl list_policies -p /bla\nListing policies ...\n/bla    pouet   all .*  {\"ha-mode\":\"all\"}   0\n$ ./scripts/rabbitmqctl delete_vhost /bla\nDeleting vhost \"/bla\" ...\nError: {throw,{error,{no_such_vhost,<<\"/bla\">>}}}\n$ ./scripts/rabbitmqctl list_vhosts\nListing vhosts ...\n/\n```\ndbg shows that the vhost is removed, then its existence is checked during update_policies:\n(<0.947.0>) call rabbit_vhost:with(<<\"/vhost_removed_while_updating_policy-vhost\">>,#Fun<rabbit_runtime_parameters.2.50904568>)\n(<0.947.0>) returned from rabbit_vhost:with/2 -> #Fun<rabbit_vhost.4.112283716>\n(<0.947.0>) call rabbit_vhost:'-with/2-fun-0-'(#Fun<rabbit_runtime_parameters.2.50904568>,<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.947.0>) call rabbit_runtime_parameters:'-mnesia_clear/3-fun-0-'(<<\"vhost_removed_while_updating_policy-policy\">>,<<\"policy\">>,<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.947.0>) returned from rabbit_runtime_parameters:'-mnesia_clear/3-fun-0-'/3 -> ok\n(<0.947.0>) returned from rabbit_vhost:'-with/2-fun-0-'/2 -> ok\n(<0.947.0>) returned from rabbit_runtime_parameters:mnesia_clear/3 -> ok\n(<0.947.0>) returned from rabbit_runtime_parameters:clear_any/3 -> #Fun<rabbit_runtime_parameters.1.50904568>\n(<0.947.0>) returned from rabbit_policy:delete/2 -> #Fun<rabbit_runtime_parameters.1.50904568>\n(<0.947.0>) call rabbit_vhost:'-internal_delete/1-lc$^2/1-2-'([],<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.947.0>) returned from rabbit_vhost:'-internal_delete/1-lc$^2/1-2-'/2 -> []\n(<0.947.0>) returned from rabbit_vhost:'-internal_delete/1-lc$^2/1-2-'/2 -> [#Fun<rabbit_runtime_parameters.1.50904568>]\n(<0.947.0>) returned from rabbit_vhost:internal_delete/1 -> [#Fun<rabbit_runtime_parameters.1.50904568>]\n(<0.947.0>) returned from rabbit_vhost:'-delete/1-fun-1-'/1 -> [#Fun<rabbit_runtime_parameters.1.50904568>]\n(<0.947.0>) returned from rabbit_vhost:'-with/2-fun-0-'/2 -> [#Fun<rabbit_runtime_parameters.1.50904568>]\n(<0.908.0>) call rabbit_vhost:'-delete/1-lc$^2/1-2-'([#Fun<rabbit_runtime_parameters.1.50904568>])\n...\n(<0.908.0>) call rabbit_policy:update_policies(<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.948.0>) call rabbit_policy:'-update_policies/1-fun-0-'(<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.948.0>) call rabbit_policy:'-update_policies/1-lc$^0/1-0-'([rabbit_queue,rabbit_durable_queue,rabbit_exchange,rabbit_durable_exchange])\n(<0.948.0>) call rabbit_policy:'-update_policies/1-lc$^0/1-0-'([rabbit_durable_queue,rabbit_exchange,rabbit_durable_exchange])\n(<0.948.0>) call rabbit_policy:'-update_policies/1-lc$^0/1-0-'([rabbit_exchange,rabbit_durable_exchange])\n(<0.948.0>) call rabbit_policy:'-update_policies/1-lc$^0/1-0-'([rabbit_durable_exchange])\n(<0.948.0>) call rabbit_policy:'-update_policies/1-lc$^0/1-0-'([])\n(<0.948.0>) returned from rabbit_policy:'-update_policies/1-lc$^0/1-0-'/1 -> []\n(<0.948.0>) returned from rabbit_policy:'-update_policies/1-lc$^0/1-0-'/1 -> []\n(<0.948.0>) returned from rabbit_policy:'-update_policies/1-lc$^0/1-0-'/1 -> []\n(<0.948.0>) returned from rabbit_policy:'-update_policies/1-lc$^0/1-0-'/1 -> []\n(<0.948.0>) returned from rabbit_policy:'-update_policies/1-lc$^0/1-0-'/1 -> []\n(<0.948.0>) call rabbit_policy:list(<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.948.0>) call rabbit_policy:list0(<<\"/vhost_removed_while_updating_policy-vhost\">>,#Fun<rabbit_policy.1.39015468>)\n(<0.948.0>) call rabbit_runtime_parameters:list(<<\"/vhost_removed_while_updating_policy-vhost\">>,<<\"policy\">>)\n(<0.948.0>) call rabbit_runtime_parameters:'-list/2-fun-0-'(<<\"policy\">>,<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.948.0>) call rabbit_vhost:assert(<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.948.0>) call rabbit_vhost:exists(<<\"/vhost_removed_while_updating_policy-vhost\">>)\n(<0.948.0>) returned from rabbit_vhost:exists/1 -> false\n(<0.948.0>) exception_from {rabbit_vhost,assert,1} {throw,{error,{no_such_vhost,<<\"/vhost_removed_while_updating_policy-vhost\">>}}}\n(<0.948.0>) exception_from {rabbit_runtime_parameters,'-list/2-fun-0-',2} {throw,{error,{no_such_vhost,<<\"/vhost_removed_while_updating_policy-vhost\">>}}}\n(<0.948.0>) exception_from {rabbit_runtime_parameters,list,2} {exit,{throw,{error,{no_such_vhost,<<\"/vhost_removed_while_updating_policy-vhost\">>}}}}\n(<0.948.0>) exception_from {rabbit_policy,list0,2} {exit,{throw,{error,{no_such_vhost,<<\"/vhost_removed_while_updating_policy-vhost\">>}}}}\n(<0.948.0>) exception_from {rabbit_policy,list,1} {exit,{throw,{error,{no_such_vhost,<<\"/vhost_removed_while_updating_policy-vhost\">>}}}}\n(<0.948.0>) exception_from {rabbit_policy,'-update_policies/1-fun-0-',1} {exit,{throw,{error,{no_such_vhost,<<\"/vhost_removed_while_updating_policy-vhost\">>}}}}\n(<0.908.0>) exception_from {rabbit_policy,update_policies,1} {throw,\n    {error,\n        {throw,\n            {error,\n                {no_such_vhost,\n                    <<\"/vhost_removed_while_updating_policy-vhost\">>}}}}}\nSo there is even no need for concurrent accesses to trigger the problem.\n. Yes\n. After changing the case clause to {'EXIT', {throw, {error, {no_such_vhost, _}}}}, I get the bad match with ok. I'm going to file a third issue.\n. Here it is: #759.\nI will add a test in the common_test branch which will test both #755 and #759 in fact.\n. This will be fixed in pull request #745.\n. @binarin: I cherry-picked your commit onto stable and created a new pull request: #771.\n. @binarin: I cherry-picked your commit onto stable and created a new pull request: #770.\n. Currently, there is no plugin having the broker_requirement key in their .app file. Also, the commit message suggests the key would be added if it's missing, but the Makefile only replace an empty existing value.\nIs it normal?\n. I'm going to merge the two for loops because they both modify the same files with sed, and I will merge the branch.\n. I removed the un.StrContains macro which now unused.\n. This must be merged with rabbitmq/rabbitmq-public-umbrella#40.\n. In CI (both Concourse and Jenkins), the random_policy testcase in dynamic_ha_SUITE fails quite frequently with the following set of policies:\n[\n  {nodes, [NodeA, NodeB]},\n  {nodes, [NodeA]}\n]\nI filed #889 and am going to investigate the issue.\n. I still see transient failures on the Jenkins slave, but those existed before.\n. All repositories pulled by the Umbrella which were using the old -spec format and/or random are fixed.\n. This PR is merged to master.\n. So it's not entirely a timing issue. I just had a case where after 12 seconds, the last policy was still not active.\n. The following failing set of policies was found on my laptop this time:\nerlang\n[\n  {exactly, 3},\n  undefined,\n  all,\n  {nodes, [NodeB]}\n]\n. And this one:\nerlang\n[\n  undefined,\n  undefined,\n  all,\n  {nodes, [NodeB]}\n]\nSo basically, all sets of policies where the last policy only targets one node.\n. A different kind of failing set of policies on my laptop:\nerlang\n[\n  all,\n  undefined,\n  {exactly, 2},\n  all,\n  {exactly, 3},\n  {exactly, 3},\n  undefined,\n  {exactly, 3},\n  all\n]\nAlso a set ending with the policy {exactly, 1} failed in Concourse, but I don't have the complete set.\n. Mirrors are dropped asynchronously as well, so the same kind of scenario applies to mirrors removal:\n- Set policy 1 with many slaves.\n- Set policy 2 with few slaves: some slaves are asked to exit.\n- Set policy 3 which includes the exiting slaves: those slaves are still recorded in the database even though they are about to exit, thus they are left alone.\n- Slaves finish to exit and unregister themselves from the database.\nIn database: #amqqueue's policy says \"replicate to $nodes\", but those nodes don't have a slave anymore.\nTherefore, both mirrors addition and removal need to be treated synchronously.\n. After more time on the problem with Andrew, we believe it is larger than just rapid set_policy commands.\nTo sum up our findings and tests:\n- suggested_queue_nodes() bases its computation on slaves_pids which does not account for slaves pending startup (missing from that list) or shutdown (about to be removed from that list).\n- This problem can probably occur during cluster lifetime when nodes are lost and new slaves are started.\n- We can't make add_mirrors() and drop_mirrors() synchronous because they are executed in the context of the queue master and block it.\n- We could add lists of slaves pending startup/shutdown to the #amqqueue record. Unfortunately, we still have an issue with pending slaves: we can't have two processes for the same queue on a single node because they use the same msg_store file(s) on disk (see #802). Thus we would need to wait for the terminating slave to actually exit before we spawn the new one.\n- For the same reason, just unregistering a terminating slave just before sending it an exit signal is not an easy fix. And it doesn't cover the slaves pending startup: it's unclear if registering the new slave early requires changes elsewhere (eg. how about it joining the GM ring after being registered?).\n- Another solution we mentioned is a separate per-queue process being responsible for doing the suggested_queue_nodes()/add_mirrors()/drop_mirrors() dance serially, without blocking the master.\n. If you are still interested, yes please submit a patch to rabbitmq-cli. Thank you!\n. The connection tracking tables are only created when a cluster is formed. Don't we need those tables for single-node instances?\n. Thank you @binarin! Sorry for the delay...\n. Pull requests rabbitmq/rabbitmq-common#136 and rabbitmq/rabbitmq-federation#38 must be merged first!\n. We found this new issue while working on #889.\n. I agree. We should give as much context as we can. The virtual host could be useful too.. How would an operator define a password policy? Would he implement and provide an Erlang module?. With the second commit to that branch, the Makefile recipe was moved to rabbitmq-cli. Thus it can be reused by package building scripts.\nSee rabbitmq/rabbitmq-cli#175 and rabbitmq/rabbitmq-server-release#19.. This new requirement should be communicated clearly in the release notes as well as on the plugins development documentation. Probably even a mail on rabbitmq-users@.\nI believe many third-party plugins do not depend on rabbit in their Makefile or $plugin.app because it was not possible a year ago, probably less (ie. before the release build was fixed to get rid of the broker depending on its plugins). So if third-party plugin developers do not modify their plugin, they won't be listed anymore.. Instead of trying to determine the user:group who runs RabbitMQ, I believe it would be simpler and easier to reason about if rabbitmq-plugins(1) made sure the file is readable by everyone, or at least readable by the group. In the latter case, the admin would be responsible to make sure the RabbitMQ user belongs to that group.. @michaelklishin: I would not try to change the group from rabbitmq-plugins(1) either, just the permissions. Ownership could/should be handled outside of the script. That said, the fact the file is deleted/created instead of updated makes changing ownership difficult.. Hi!\nI merged @Gsantomaggio's pull request as it improves the situation. I would like to keep this issue open so we remember to completely fix it.. Yes, I think it's fine as well.. Hi @oneiros-de!\nI see the bad rendering issue on various platforms (Debian, FreeBSD, OS X) too. Here is a description of my findings so far.\nCurrently, manpages are generated from a DocBook source using xmlto(1) and stylesheets from docbook-xsl.\nOn my laptop, the man(7) output for the stop command is:\n.HP \\w'\\fBstop\\fR\\ 'u \\fBstop\\fR [\\fIpid_file\\fR]\n.RS 4\nStops the Erlang node on which RabbitMQ is running\\&. To restart the node follow the instructions for\nRunning the Server\nin the\n\\m[blue]\\fBinstallation guide\\fR\\m[]\\&\\s-2\\u[1]\\d\\s+2\\&.\nAs you found out, the command name (stop [pid_file]) doesn't appear in the output:\n\nHere is the description of the .HP macro in man(7), the markup language used here:\n\nHP\nBegin a paragraph whose initial output line is left-justified, but subsequent output lines are indented, with the following syntax:\n.HP [width]\n\n\nSo the macro takes a single argument, but here, we give two:\n the width, \\w'\\fBstop\\fR\\ 'u\n the first line, \\fBstop\\fR [\\fIpid_file\\fR]\nTherefore, the first \"line\" is swallowed by the macro. If I manually split that into two lines, like this:\n.HP \\w'\\fBstop\\fR\\ 'u\n\\fBstop\\fR [\\fIpid_file\\fR]\n.RS 4\nStops the Erlang node on which RabbitMQ is running\\&. To restart the node follow the instructions for\nRunning the Server\nin the\n\\m[blue]\\fBinstallation guide\\fR\\m[]\\&\\s-2\\u[1]\\d\\s+2\\&.\nThen the manpage is rendered correctly:\n\nI have docbook-xsl 1.76.1 installed, which is not the latest version. I tried with the latest, 1.79.1, but the problem is the same. mandoc(1) is used to render and display the manpage on FreeBSD, groff(1) on OS X and I have a hard time to follow which *roff is used on Debian Wheezy. Anyway, they all seem to be strict enough to not render that badly generated .HP macro.\nI'm going to experiment a bit more with xmlto(1). But there could be other unnoticed rendering issues and my confidence in this toolchain is getting low. We may have to switch away from DocBook.. Thank you for the updated patch!\nThe following script is enough to demonstrate the issue:\n```sh\n!/bin/sh\nset -e\ndo_things () {\n  sleep 20\n}\ntrap \"echo 'Got SIGINT'\" INT\ndo_things &\nwait $!\n```\nIf we hit Ctrl+C, the script is supposed to display ^CGot SIGINT. With Bash or FreeBSD's sh, this works as expected. However with Dash, it shows ^C only, meaning the signal handler was not executed.\nI just noticed that even with Dash, the wait command still exits with 130, which is 128 + signal number, in this case SIGINT which is signal number 2 (kill -l lists signal names and numbers). So a possible workaround would be to test the exit status of wait and call the signal handler manually.. Here is the same small script modified with a homebrew signal handler for Dash:\n```sh\n!/bin/sh\nset -e\ndo_things () {\n  sleep 20\n}\nsignal_handler() {\n  echo \"Running signal handler\"\n}\ntrap \"signal_handler; exit 0\" HUP TERM TSTP\ntrap \"signal_handler; exit 130\" INT\ndo_things &\nif wait $!; then\n  :\nelse\n  status=$?\n  case \"$(kill -l $status)\" in\n    HUP|TERM|TSTP) signal_handler ;;\n    INT)           signal_handler; exit $status ;;\n  esac\nfi\n```\nIt behaves the same with Dash, Bash and FreeBSD sh.\nWhat do you think?. @sodre: Note that I changed the trap line for SIGINT to exit with 130. Otherwise, on working shells, both the regular handler and the fake handler are executed.\nAlso, before we convert the exit status to a signal name with kill -l, we must verify that the exit status is greater than or equal to 128. Because kill -l will happily convert eg. 2 to INT and we don't want to convert regular exit codes from wait to signal names.. Thank you @sodre for the patch!\nWe added more comments and cherry-picked it to our stable branch. It will soon be merged to the master branch as well.. The patch from #1192 was cherry-picked to stable so we can close this pull request.\nSee #1192 for the entire discussion around the solution. Thank you again @sodre!. > According to Red Hat documentation, /proc/meminfo actually reports values in kibibytes when it says \"kilobyte\". I wouldn't be surprised if more OS'es did the same. So this needs an investigation first.\nI believe this is true for most cases. Historically, \"kilobytes\", \"megabytes\" and so on were used to qualify values base on power-of-two units. Only persistent storage device makers used SI units on their products. For instance a hard disk of 80 GB is exactly what it says: 80,000,000,000 bytes, unlike what people expected (80 x 1024 x 1024 x 1024 = 85899345920 bytes).\nThe power-of-two units were introduced in 1998 [1][2] and it took time for people to start to use them. Now, applications usually report the same power-of-two-based values but might have fixed the unit to display \"kiB\", \"MiB\", \"GiB\", ...\nIMHO we should do the same: keep the power-of-two-based values and fix the unit we display or document, because nobody uses the power-of-ten values, even if the unit is spelled \"MB\".\n[1] https://en.wikipedia.org/wiki/Mebibyte\n[2] http://physics.nist.gov/cuu/Units/binary.html. @gerhard and @hairyhum, do you have examples where erlang:memory(total) underestimates the memory footprint?\nA freshly started RabbitMQ without messages (or an Erlang shell) is a bad example: it doesn't reflect a production situation and is the worst case scenario because the memory allocated by the kernel to start the process (so outside of the Erlang VM scope) is significant compared to the memory allocated by the Erlang VM.. I have one concern with this approach: RSS reports the physical memory footprint, unlike erlang:memory(total) which reports the used memory.\nWe see that RSS is often larger compared to erlang:memory(total), probably because of pre-allocations done by the Erlang VM in order to improve performance. However, if memory pages used by the Erlang VM are paged out (by the kernel; I'm not talking about our internal paging of queue messages) for whatever reason, the RSS value will remain low while erlang:memory(total) could grow beyond the actual physical memory size.\nIt's possible that today, RabbitMQ doesn't behave nicely w.r.t. queue messages paging and blocking producers when the host starts to swap. But if we use RSS, the behavior will change for sure: I'm wondering if it will be better or worse.. Yes, I agree because in this patch, we are comparing comparable numbers (physical memory footprint vs. physical memory size), unlike before (memory consumption from Erlang applications PoV vs. physical memory size). Thus the new calculation is not just more accurate: it starts to make sense :-)\nAfter thinking a bit more about this, the only situations I see where this could be worse would be:\n Another process is eating all available memory.\n Memory ballooning is being used and the \"physical\" memory size was reduced.\nWe do not support the second situation anyway because we only read the physical memory size during startup. So if it changes later, we don't know and calculations will be off anyway.\nPerhaps in the future, RabbitMQ should decide about queue messages paging and blocking producers by looking at the free memory (+ cache), i.e. something close to what we do with disk space. I don't know, just a random thought.. Good idea about the parameter name!. Is this pull request still relevant?. Here is what is reported on our Ubuntu 16.04 VM used for testing packages:\n```\nsystemctl status rabbitmq-server\n\u25cf rabbitmq-server.service - RabbitMQ broker\n   Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; enabled; vendor preset: enabled)\n   Active: active (running) since Tue 2017-11-07 08:44:20 UTC; 2min 18s ago\n Main PID: 22579 (beam.smp)\n   Status: \"Initialized\"\n   CGroup: /system.slice/rabbitmq-server.service\n           \u251c\u250022579 /usr/lib/erlang/erts-9.1/bin/beam.smp -W w -A 64 -P 1048576 -t 5000000 -stbt db -zdbbl 128000 -K true -- -root /usr/lib/erlang -progname erl -- -home /var/lib/rabbitmq -- -pa /usr/lib/rabbitmq/lib/rabbitmq_server-3.6.13/ebin -noshell -noinput -s rabbit b\n           \u251c\u250022659 /usr/lib/erlang/erts-9.1/bin/epmd -daemon\n           \u251c\u250022793 erl_child_setup 1024\n           \u251c\u250022815 inet_gethost 4\n           \u2514\u250022816 inet_gethost 4\nNov 07 08:44:18 ip-172-31-30-31 rabbitmq-server[22579]:   ##########\nNov 07 08:44:18 ip-172-31-30-31 rabbitmq-server[22579]:               Starting broker...\nNov 07 08:44:20 ip-172-31-30-31 rabbitmq-server[22579]: systemd unit for activation check: \"rabbitmq-server.service\"\nNov 07 08:44:20 ip-172-31-30-31 systemd[1]: Started RabbitMQ broker.\nNov 07 08:44:20 ip-172-31-30-31 rabbitmq-server[22579]:  completed with 0 plugins.\nNov 07 08:44:20 ip-172-31-30-31 systemd[1]: [/lib/systemd/system/rabbitmq-server.service:17] Failed to parse service restart specifier, ignoring: on-failure # https://www.freedesktop.org/software/systemd/man/systemd.service.html#Restart=\nNov 07 08:44:20 ip-172-31-30-31 systemd[1]: [/lib/systemd/system/rabbitmq-server.service:18] Failed to parse sec value, ignoring: 10      # https://www.freedesktop.org/software/systemd/man/systemd.service.html#RestartSec=\nNov 07 08:44:20 ip-172-31-30-31 systemd[1]: [/lib/systemd/system/rabbitmq-server.service:23] Failed to parse value, ignoring: # rabbitmq/rabbitmq-server-release#51\nNov 07 08:44:20 ip-172-31-30-31 systemd[1]: [/lib/systemd/system/rabbitmq-server.service:23] Failed to parse value, ignoring: rabbitmq/rabbitmq-server-release#51\nNov 07 08:45:55 ip-172-31-30-31 systemd[1]: Started RabbitMQ broker.\n```\nSo indeed the errors appear in the log, however they don't prevent the service from starting. That's why the errors remain unnoticed. Perhaps systemd can run in a stricter mode; I'll do some research to understand why it fails to start for you and not us.. I found systemd-analyze(1) which offers a verify command to lint Unit files. I'll add it to our testsuite.. Hi @dimas!\nFirst, thank you for providing the Java test client, it greatly helped to reproduce the problem.\nWhen the SSL error happens, the Erlang ssl library returns the error handshake_timeout to RabbitMQ. Normally, this causes RabbitMQ to terminate the connection at its level, except after the connection.open AMQP frame was received, where the error is ignored:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_reader.erl#L574-L576\nI don't know the background of this special case, therefore I'm digging into the history of this file, as well as our legacy internal Bugzilla. I'll keep you posted.. Hmm, in fact no, ignore my comment above: the handshake_timeout message is an internal one, not something from ssl. It just happened that this internal message is sent after 5 seconds as well.. Different versions of Erlang don't behave the same:\n\n\nWith Erlang 17.5.6.9, the ssl library logs an error and returns {error,{tls_alert,\"bad record mac\"}} to RabbitMQ:\n```\n=ERROR REPORT==== 6-Feb-2018::11:19:09 ===\nSSL: connection: ssl_cipher.erl:216:Fatal error: bad record mac\n=ERROR REPORT==== 6-Feb-2018::11:19:09 ===\nmainloop -> Recv = {error,{tls_alert,\"bad record mac\"}}\n=ERROR REPORT==== 6-Feb-2018::11:19:09 ===\nclosing AMQP connection <0.318.0> (127.0.0.1:44760 -> 127.0.0.1:5671):\n{inet_error,{tls_alert,\"bad record mac\"}}\n```\n\n\nWith Erlang 18.3.4.7, the behavior is the same:\n```\n=ERROR REPORT==== 6-Feb-2018::11:20:40 ===\nSSL: connection: ssl_cipher.erl:301:Fatal error: bad record mac\n=ERROR REPORT==== 6-Feb-2018::11:20:40 ===\nmainloop -> Recv = {error,{tls_alert,\"bad record mac\"}}\n=ERROR REPORT==== 6-Feb-2018::11:20:40 ===\nclosing AMQP connection <0.327.0> (127.0.0.1:44796 -> 127.0.0.1:5671):\n{inet_error,{tls_alert,\"bad record mac\"}}\n```\n\n\nWith Erlang 19.3.6.5, ssl logs an error but doesn't return anything to RabbitMQ which sits indefinitely, waiting for something from the ssl library:\n```\n=ERROR REPORT==== 6-Feb-2018::11:21:12 ===\nmainloop -> rabbit_net:recv...\n=ERROR REPORT==== 6-Feb-2018::11:21:12 ===\nSSL: {connection,{alert,2,20,{\"ssl_cipher.erl\",304},decryption_failed}}: ssl_connection.erl:861:Fatal error: unexpected message\n```\n\n\nWith Erlang 20.2.2, ssl doesn't log anything and never returns to RabbitMQ:\n=ERROR REPORT==== 6-Feb-2018::11:22:12 ===\nmainloop -> rabbit_net:recv...\n\n\nThis looks like a regression in Erlang's ssl library. I will continue to investigate.. I reported the issue upstream: https://bugs.erlang.org/browse/ERL-562\nI also prepared a patch: erlang/otp#1709. The patch was merged \\o/ The ticket says \"Resolved in 20.3\". I will ask if they plan to backport it to previous releases.. Sorry I missed the ping...\nI believe all failures in the v3.8.x pipeline are fixed by now. However, the v3.7.x pipeline is stuck right in the middle for no apparent reason: new resources don't reach the upgrades testing stage.. I believe I see light at the end of the tunnel :-) There was a chain of small issues with Concourse, our pipeline, the Erlang Solutions Debian repository and so on. The last thing on the radat is a regression with Concourse 3.11.0: the Git resource doesn't fetch tags anymore, we have to do it ourselves. A fix for this is being tested.. And that was without anticipating the next problem: no space left on workers!\nAnyway, a 3.7.x snapshot was published yesterday:\nhttps://dl.bintray.com/rabbitmq/all-dev/rabbitmq-server/3.7.5-alpha.23/\nHopefully, it will allow you to make progress.. Forcing the locale to something.UTF-8 will silence the warning, but it won't solve the actual problem: I suppose Elixir expects UTF-8 encoded input and produces UTF-8-encoded output. If the locale is forced just for the rabbitmqctl escript, how the terminal/shell/other tools started in a pipe know about it? If the session is configured with a charset incompatible with UTF-8 or US-ASCII, forcing the locale like that will break data exchanged between the CLI and the outside world.\nIMHO, the caller (whether it is an interactive shell session or a configuration management tool) is responsible for setting an appropriate locale because only it knows about the environment/session.\nAnother minor issue would be that the forced locale is unavailable. But Erlang/Elixir may not call setlocale(3) or may not display a warning if the function fails, so perhaps it's a non-issue. But here are examples with locale(1) and Perl which do log a warning:\n$ LC_ALL=en_GB.ISO-8859-15 locale\nlocale: Cannot set LC_CTYPE to default locale: No such file or directory\nlocale: Cannot set LC_MESSAGES to default locale: No such file or directory\nlocale: Cannot set LC_ALL to default locale: No such file or directory\nLANG=en_GB.UTF-8\nLANGUAGE=\nLC_CTYPE=\"en_GB.ISO-8859-15\"\n...\n```\n$ LC_ALL=fr_FR.UTF-8 perl --help\nperl: warning: Setting locale failed.\nperl: warning: Please check that your locale settings:\n    LANGUAGE = (unset),\n    LC_ALL = \"fr_FR.UTF-8\",\n    LANG = \"en_GB.UTF-8\"\n    are supported and installed on your system.\nperl: warning: Falling back to a fallback locale (\"en_GB.UTF-8\").\nUsage: perl [switches] [--] [programfile] [arguments]\n  -0[octal]         specify record separator (\\0, if no argument)\n...\n```\nIf Elixir does that at some point, we are back to square 1.. That's awesome! Good job :-). The patch is working for me locally (after building a generic-unix package):\nMay 21 15:07:48 localhost rabbit[49457] lager_event - 2018-05-21 15:07:48.075 [warning] <0.421.0> Message store \"628WB79CIFDYO9LJI6DKMI09L/msg_store_persistent\": rebuilding indices from scratch\nMay 21 15:07:48 localhost rabbit[49457] lager_event - 2018-05-21 15:07:48.225 [notice] <0.91.0> Changed loghwm of /tmp/rabbitmq_server-3.7.0+rc.2.27.g9e7e96b/var/log/rabbitmq/rabbit@cassini.log to 50\nMay 21 15:11:09 localhost rabbit[49457] lager_event - 2018-05-21 15:11:09.974 [warning] <0.515.0> closing AMQP connection <0.515.0> (127.0.0.1:61157 -> 127.0.0.1:5672 - perf-test-consumer-0, vhost: '/', user: 'guest'):\nMay 21 15:11:09 localhost rabbit[49457] lager_event - client unexpectedly closed TCP connection\nMay 21 15:11:09 localhost rabbit[49457] lager_event - 2018-05-21 15:11:09.974 [warning] <0.539.0> closing AMQP connection <0.539.0> (127.0.0.1:61158 -> 127.0.0.1:5672 - perf-test-producer-i, vhost: '/', user: 'guest'):\nMay 21 15:11:09 localhost rabbit[49457] lager_event - client unexpectedly closed TCP connection\nHowever, do you know if it's possible to tweak the message format? Because it logs the string lager_event - followed (sometimes) by a copy of the timestamp which is useless because syslog already logs that.. While playing with the generic-unix package, I got this error while RabbitMQ was exiting:\n^CStopping and halting node rabbit@cassini ...\n2018-05-21 15:17:32.703 [error] <0.91.0> ** gen_event handler syslog_lager_backend crashed.\n** Was installed in lager_event\n** Last event was: {log,{lager_msg,[],[{pid,<0.31.0>}],info,{[\"2018\",45,\"05\",45,\"21\"],[\"15\",58,\"17\",58,\"32\",46,\"702\"]},{1526,908652,702699},[65,112,112,108,105,99,97,116,105,111,110,32,\"os_mon\",32,101,120,105,116,101,100,32,119,105,116,104,32,114,101,97,115,111,110,58,32,\"stopped\"]}}\n** When handler state == {state,{mask,127},lager_default_formatter,[date,\" \",time,\" \",color,\"[\",severity,\"] \",{pid,[]},\" \",message,\"\\n\"],undefined,[]}\n** Reason == {killed,{gen_server,call,[syslog_logger,{log,<<\"<30>May 21 15:17:32 cassini rabbit[49457] application_controller - 2018-05-21 15:17:32.702 [info] <0.31.0> Application os_mon exited with reason: stopped\">>},1000]}}\nGracefully halting Erlang VM. Looks good to me. I couldn't reproduce the crash, but I couldn't before the last fix either, so it's not a good indicator :)\nI noticed that the syslog library doesn't seem to support IPv6. But this is a minor thing. Having syslog support over UDP/IPv4 is still a good step forward.. The patch looks good and yes, we have Windows VMs in CI. However, I need to verify which version of Erlang we use there.. We use Erlang 20.0 on all Windows VMs. I need to figure out a way to test with at least two versions of Erlang, like we do for Linux.. My concern is mostly how I integrate that into Concourse configuration :-) Otherwise, we use a new Windows VM each time we do a Windows installer test and throw it away, so installing Erlang is not an issue.. @lukebakken: Yes, perhaps rabbitmqctl(8) should do those steps. I don't know why it doesn't do that in the first place; there must be a reason for that. About the link to the manpage, I have no opinion on that :-)\nNo matter what we decide about an improvement to rabbitmqctl(8), I believe the doc improvement should be committed when ready.. Thank you @blgm for the patch!. Yeah, we don't control the indexation of packages: we just upload them. Bintray is responsible for creating all the files under dists. I'm also surprised they are missing, perhaps related to the blocking they triggered this week-end. Unfortunately, I'm not sure what to do to restore them.... I cherry-picked this commit on the rebase backward-compatible-amqqueue branch after it was also rebased on top of the latest quorum-queue.. Hmm when I start the node with the log level set to debug, the lager application env. still has a high watermark of 50 for me. I used the new config format.\n@michaelklishin: how did you configure the initial log level?. Thank you! I will take care of reverting the change in rabbitmq-common.. The problem with the initial patch is that setting the log level from the configuration does not change the high watermark on startup if that level is set to debug. I prepared another patch to help with this, so we can revert the change in rabbitmq-common.. One minor comment: instead of calling find_prioritisers() twice, we could:\n1. Store the result of the case catch Mod:init ... end in a variable\n2. Call find_prioritisers() + loop() after the case.\n. Please put rabbitmq-server-ha.ocf just below rabbitmq-server.ocf, and edit ${S:5} and ${S:6} below accordingly.\n. Why do you need Bash?\nAfter looking at the code, the main Bash feature used is [[ $var == \"literal\" ]]. This is easily replaced with [ \"$var\" = \"literal\" ].\n. You could re-use the same default value as the current OCF RA.\n. This can only work with Erlang nodes using short names, not long names.\n. You already have rabbit_node_name below to format a node name, you could use it here as well.\n. As stated earlier, this can only work with Erlang nodes using short names, not long names.\n. If $OCF_RESKEY_mnesia_base contains data for several nodes, the above command may remove other nodes data, if they are named rabbit@node1, rabbit@node10, rabbit@node11, etc. if we are resetting rabbit@node1. Adding a slash will fix the problem.\n. This is another bashism. You can replace it with $((tries--)).\n. Style issue: this whole if block would be more readable as a case.\n. Minor comment, not just for this line: awk(1) can perform the regex matching, no need for an additional grep(1) in front of it. On other lines, the same comment applies to the combination of awk(1) and sed(1).\n. SIGTERM is probably enough and less brutal.\n. Would it be worth checking for a running beam or beam.smp without the associated PID file? beam command line contains the node name.\n. Style issue: ! -z $pid could be simplified as just \"$pid\".\nAlso, it would be more logical to test it before the directory.\n. Again, SIGTERM should be enough.\n. rm -f is enough and safer in case of error/typo.\n. Can't you use rabbitmqctl wait for this purpose, instead of this loop?\n. Likewise, can't you use rabbitmq-plugins for this?\n. Can't you use rabbitmq-plugins for this?\nShould we add more options and output formats to the command?\n. Style issue: 5-column indentation, this is inconsistent with the rest of the file.\n. Typo: promition -> promotion.\n. Typo: ohter -> other.\n. You should use $OCF_RESKEY_ctl instead of hard-coding rabbitmqctl.\n. The syntax of this test is invalid: you can't use == with [. It should be:\nsh\nif [ \"$rc\" = \"$OCF_RUNNING_MASTER\" ]\n. You must use a tab to indent the line.\n. The syntax of this test is invalid: [ doesn't support == as an operator, but see next comment.\n. Please quote arguments to [. If the variable expands to the empty string, [ will report a syntax error here.\nThis comment applies to the whole file.\n. Perhaps Args = [\"\"] could be caught here as well?\nI'm thinking of scripts calling rabbitmqctl list_user_permissions \"$user\" but $user is not set.\n. Ok, the current error is fine. Disregard my comment.\n. I don't see any +K true flag for the Windows startup script. Is it on purpose?\n. Please wrap lines longer than 80 columns.\n. Here too, this line should be wrapped.\n. These lines are too long.\n. s/bloc/block/\n. s/whe/when/? I don't understand this sentence.\n. s/will will/will/\n. Why is this timer:sleep/1 needed? And why 2 seconds?\n. Why is this timer:sleep/1 needed? And why 2 seconds?\n. Isn't there a real way to wait for the actual completion of the rotation instead of using a random timeout?\nAbout the relevance of rabbitmqctl rotate_logs, can we change it so it drives lager instead of doing its own rotation? I agree that what RabbitMQ does is limited. Lager must provide a much nicer implementation.\n. I see you added a TimeoutStartSec of one hour. I fear RabbitMQ can take longer in some rare situation. What do you think about disabling the timeout (TimeoutStartSec=infinity)? Can it prevent the host from booting?\n. You never call port_close/1, how is socat(1) termianted?\n. I discussed this with @michaelklishin and he agrees with you, so we'll keep this timeout of one hour.\n. This file should be listed in debian/rabbitmq-server.docs instead of being manually installed by debian/rules.\n. It should be exit() because:\n- If we caught an exit signal triggered by mnesia:abort(), the tuple is already of the form expected by Mnesia ({aborted, Some_Internal_Record}). So here, we just forward it.\n- If we caught a \"normal\" exit signa, we also forward it as is and let Mnesia handle the difference between an abortion and a plain exit signal.\n. I believe you can get rid of the creation of rabbit-env.conf.\n. There is a comma missing at the end of this line. I fixed it and merged your branch.\nThank you very much!\n. You probably want to use ?PER_VHOST_COUNTER_TABLE here.\n. It would be more efficient to unregister all connections in a single transaction.\n. The spec should be:\nerlang\n-spec is_over_connection_limit(rabbit_types:vhost()) -> {true, non_neg_integer()} | false.\n. What about using -1 as the \"no limit\" value or even a more meaninful value such as undefined or false? 0 could be used to block any connections to a vhost.\nSorry if I missed a previous discussion about this.\n. The \"no limit\" value could be a macro so we understand its meaning everywhere.\n. Comments are duplicated.\n. Msg is being used once, you can probably get rid of it and use the string literal directly.\n. Is this a debugging message?\n. Please do not use the '_test' suffix to not confuse the testcases with EUnit tests.\n. There is no partition_handling group.\n. You could call init_per_multinode_group() instead of duplicating the code.\n. Connection counts should be verified for both virtual hosts in the entire testcase.\n. Are those two Mnesia tables still used by the patch? I can't find where.\n. Any reason for not using record_info(fields, ...) for those two table creations?\n. You can't modify erlang.mk because it's a generated file which will get overwritten with the next update of Erlang.mk (make erlang-mk).\nThose changes should go the Makefile.. The copyright year should be updated to 2017.. You can get rid of the unit_inbroker prefix for most of them. That name only made \"sense\" because unrelated testcases were in the same testsuite.. The implementation of ct-slow and ct-fast can also go to one of our Erlang.mk plugins so we could define $(CT_SLOW) and $(CT_FAST) in other projects too.. Is there a reason why you splitted that into two groups with the same name? Is it to get a dedicated broker/cluster for each top-level group?. I mean, did having the variable_queue_* groups inside ?BACKING_QUEUE_TESTCASES prevent them from running successfully in parallel for instance?. You can't hardcode the path to Bash because it could be installed somewhere else. For instance, on various *BSD, Bash is installed as /usr/local/bin/bash. Please use /usr/bin/env bash instead.\nIn the process, you'll have to remove the -e and add a set -e (which is also cleaner) after the license.. The current implementation of get_ps_memory() will work on FreeBSD and probably other *BSD as well.\nThe command line you used is portable to POSIX systems by the way. The only non-portable part is the field name (rss).. I confirm the difference on Windows too: for a simple Erlang shell, Windows reports 19 MiB but erlang:memory(total). reports 14 MiB.. I agree with @lukebakken that this is a better solution for Linux. However, note that this is specific to Linux.. This was a bad example, please disregard this comment.. I looked at various systems' manpages: all common *BSD support the rss field (DragonFlyBSD, FreeBSD, NetBSD, OpenBSD). Solaris should support it too.\nPlan9 and HP-UX do not provide a POSIX ps(1), and I couldn't find manpages for AIX.. /me wants to add Plan9 VMs in CI!. No because this module is moved to rabbitmq-common in the resolve-deps-circles branch to solve interdependencies between the broker, the Erlang client and the rabbitmq-common. But the configuration key will remain in the rabbit application.. The output of tasklist is localized, thus this function can only work on an English Windows.\nHere is a sample output from a French Windows:\n```\nC:\\Users\\Jean-S\u00e9bastien>tasklist /fi \"pid eq 7116\" /fo LIST\nNom de l\u2019image:      explorer.exe\nPID:                 7116\nNom de la session:   Console\nNum\u00e9ro de session:   1\nUtilisation m\u00e9moire: 72\u00a0008 Ko\nC:\\Users\\Jean-S\u00e9bastien>\n```\nThus:\n the label is translated (\"Utilisation m\u00e9moire\")\n the number is localized (a space to separate thousands).\nPerhaps there is a way to set the locale for a particular command, but I don't know how.. You're right, we'll change that. Thanks!. Maps are not supported by Erlang R16B03 so this won't compile at all.. The maps module isn't available in Erlang R16B03 either:\n$ gmake xref\n(...)\nsrc/rabbit_looking_glass.erl:24 `rabbit_looking_glass:boot/0` calls undefined function `maps:from_list/1`\nmaps:from_list/1` is not defined as a function\ngmake: *** [erlang.mk:6755: xref] Error 1\nYou could add another -ignore_xref() directive as we won't run this code on R16B03 anyway.. The file was created in 2018, this shouldn't be a range in the copyright.. No need to return a special atom. You can end the function after the previous statement.. Perhaps this queue can be named classic-q and the next one default-q? Just to make the name consistent with the code.. Could you please pin the version, so the library is the same for everyone?\nThe latest tag is 3.4.2.. I would say\n\nIf you have /etc/rabbitmq/rabbitmq-env.conf and configured the node name there, update this configuration.\n\nBecause not many people are using rabbitmq-env.conf, thus they shouldn't be afraid if the file is missing.. Please use a numbered list here (.Bl -enum -compact).. I suggest you replace the three .Dl by this block:\n.Bd -literal -offset indent -compact\nmv \\\\\n  /var/lib/rabbitmq/mnesia/rabbit@misshelpful \\\\\n  /var/lib/rabbitmq/mnesia/rabbit@cordelia\nmv \\\\\n  /var/lib/rabbitmq/mnesia/rabbit@misshelpful-rename \\\\\n  /var/lib/rabbitmq/mnesia/rabbit@cordelia-rename\nmv \\\\\n  /var/lib/rabbitmq/mnesia/rabbit@misshelpful-plugins-expand \\\\\n  /var/lib/rabbitmq/mnesia/rabbit@cordelia-plugins-expand\n.Ed\nThis way, the command lines are split to remain in the 80-column limit and are more readable.. ",
    "poducavanje": "Hi Michael,\nThanks for pointing this out. This will narrow down our effort to trace the problem on the client side only.\nAppreciate the info you provided\nCheers,\nNemanja\n. And thanks for quick response too!\n. ",
    "lemenkov": "Looks like dupe of #368.. @binarin sorry didn't understand initially what's your problem. I believe this should be fixed in Debian / Ubunty.\n. Hello All!\nWe also got this with relatively recent RabbitMQ versions - at least 3.6.5 and 3.6.15.\nSometimes deleting queue doesn't work and we've seen the same thing. Some erlang messages were delivered to wrong process, the same rabbit_mirror_queue_slave:handle_call/3. In our case it's not a {delete,false,false}, but a {info,[name]} tuple which normally should be handled in rabbit_amqqueue_process:handle_call(sync_mirrors, _From, State).\nThis (as we suspect/believe) led to a even more strange situation. Queue might be deleted anyway (inactivity timeout), but associated bindings will still be around. Let me show how it looks:\n[root@controller-1 /]# rabbitmqctl list_bindings source_name destination_name routing_key | grep ^scheduler_fanout\nscheduler_fanout        scheduler_fanout_8d22dc1db1134e23800b29d2adf36ec9       scheduler\nscheduler_fanout        scheduler_fanout_90f17b0f3bdc466f876a398ff2567b2a       scheduler\nscheduler_fanout        scheduler_fanout_b8ee661524e94a9a97a7280162d15905       scheduler\nscheduler_fanout        scheduler_fanout_cb3525d4ecae46ae8242df1f8a7e2408       scheduler\nscheduler_fanout        scheduler_fanout_d3c5f126eed5475ca6f2aad8dbdcc2a5       scheduler\nscheduler_fanout        scheduler_fanout_ea23bbca01cd4da2a6ea2a672f7cf5de       scheduler\nAnd here is a list of actually existing queues:\n[root@controller-1 /]# rabbitmqctl list_queues name pid slave_pids | grep ^scheduler_fanout_\nscheduler_fanout_b8ee661524e94a9a97a7280162d15905       <rabbit@controller-1.1.17881.0> [<rabbit@controller-2.3.30886.0>, <rabbit@controller-0.1.2538.0>]                                                                                                                                  \nscheduler_fanout_cb3525d4ecae46ae8242df1f8a7e2408       <rabbit@controller-2.3.14973.0> [<rabbit@controller-1.1.2851.0>, <rabbit@controller-0.1.3053.0>]                                                                                                                                   \nscheduler_fanout_90f17b0f3bdc466f876a398ff2567b2a       <rabbit@controller-2.3.2932.0>  [<rabbit@controller-1.1.3059.0>, <rabbit@controller-0.1.3283.0>]\nWhat happens next is a bizarre thing. A publisher posts message but never gets an confirmation because brocker cannot get anything back from nonexisting queues (that's our guess).. Just for the record, after all these patches applied I still see it regularly. This one is made by RabbitMQ 3.6.3\n=CRASH REPORT==== 20-Jun-2017::23:41:57 ===\n  crasher:\n    initial call: rabbit_reader:init/4\n    pid: <0.18631.0>\n    registered_name: []\n    exception exit: channel_termination_timeout\n      in function  rabbit_reader:wait_for_channel_termination/3 (src/rabbit_reader.erl, line 767)\n      in call from rabbit_reader:send_error_on_channel0_and_close/4 (src/rabbit_reader.erl, line 1504)\n      in call from rabbit_reader:terminate/2 (src/rabbit_reader.erl, line 612)\n      in call from rabbit_reader:handle_other/2 (src/rabbit_reader.erl, line 537)\n      in call from rabbit_reader:mainloop/4 (src/rabbit_reader.erl, line 499)\n      in call from rabbit_reader:run/1 (src/rabbit_reader.erl, line 424)\n      in call from rabbit_reader:start_connection/4 (src/rabbit_reader.erl, line 382)\n    ancestors: [<0.18628.0>,<0.336.0>,<0.335.0>,<0.334.0>,rabbit_sup,\n                  <0.77.0>]\n    messages: [{'EXIT',#Port<0.13578>,normal}]\n    links: []\n    dictionary: [{{channel,1},\n                   {<0.18644.0>,{method,rabbit_framing_amqp_0_9_1}}},\n                  {{ch_pid,<0.18644.0>},{1,#Ref<0.0.10.3477>}},\n                  {process_name,\n                      {rabbit_reader,\n                          <<\"11.120.0.10:38434 -> 11.120.0.10:5672\">>}}]\n    trap_exit: true\n    status: running\n    heap_size: 1598\n    stack_size: 27\n    reductions: 1593123\n  neighbours:. @michaelklishin lucky you! I'm investigating this currently - I'll ping you if I find anything.. @michaelklishin back to this issue. Actually we've got these messages with 3.6.5 as well. So what I've got here is that.\nI've got 3 nodes A,B,C.\nNode C is experiencing some connectivity issues (lost heartbeats from clients).\n  =ERROR REPORT==== 22-Jan-2018::11:00:21 ===\n  closing AMQP connection <0.2203.2> (10.210.8.9:60368 -> 10.210.8.7:5672 - nova-compute:43126:5a13b6b0-31c6-453b-bacc-51f5066c9888):\n  missed heartbeats from client, timeout: 60s\n\nNode C is going down.\n  =INFO REPORT==== 22-Jan-2018::11:02:38 ===\n  Stopping RabbitMQ\n\n  =INFO REPORT==== 22-Jan-2018::11:02:38 ===\n  stopped TCP Listener on 10.210.8.7:5672\n\n  =INFO REPORT==== 22-Jan-2018::11:02:38 ===\n  Stopped RabbitMQ application\n\n  =INFO REPORT==== 22-Jan-2018::11:02:38 ===\n  Halting Erlang VM\n\nNode A noticed C going offline:\n  =INFO REPORT==== 22-Jan-2018::11:02:38 ===\n  Mirrored queue 'reply_aa2bb108ffb344288ea16efce66e4164' in vhost '/': Slave <rabbit@jncloud-controller-0.2.16099.0> saw deaths of mirrors <rabbit@jncloud-controller-2.1.6441.0>\n\n  =INFO REPORT==== 22-Jan-2018::11:02:39 ===\n  node 'rabbit@jncloud-controller-2' down: connection_closed\n\n  =INFO REPORT==== 22-Jan-2018::11:02:44 ===\n  Stopping RabbitMQ\n\n  =INFO REPORT==== 22-Jan-2018::11:02:44 ===\n  stopped TCP Listener on 10.210.8.1:5672\n\nAnd here I've got a strange crash report:\n...............\n=CRASH REPORT==== 22-Jan-2018::11:02:47 ===\n        crasher:\n          initial call: rabbit_reader:init/4\n          pid: <0.17029.1>\n          registered_name: []\n          exception exit: channel_termination_timeout\n            in function  rabbit_reader:wait_for_channel_termination/3 (src/rabbit_reader.erl, line 771)\n            in call from rabbit_reader:send_error_on_channel0_and_close/4 (src/rabbit_reader.erl, line 1508)\n            in call from rabbit_reader:terminate/2 (src/rabbit_reader.erl, line 616)\n            in call from rabbit_reader:handle_other/2 (src/rabbit_reader.erl, line 541)\n            in call from rabbit_reader:mainloop/4 (src/rabbit_reader.erl, line 503)\n            in call from rabbit_reader:run/1 (src/rabbit_reader.erl, line 428)\n            in call from rabbit_reader:start_connection/4 (src/rabbit_reader.erl, line 386)\n          ancestors: [<0.17027.1>,<0.17093.0>,<0.17091.0>,<0.17089.0>,rabbit_sup,\n                        <0.747.0>]\n          messages: [{'EXIT',#Port<0.16148>,normal}]\n          links: []\n          dictionary: [{{channel,1},\n                         {<0.17037.1>,{method,rabbit_framing_amqp_0_9_1}}},\n                        {{ch_pid,<0.17037.1>},{1,#Ref<0.0.1.157017>}},\n                        {credit_flow_default_credit,{200,50}},\n                        {process_name,\n                            {rabbit_reader,\n                                <<\"10.210.8.1:58502 -> 10.210.8.1:5672 - nova-conductor:916198:cfb9f0d8-33bd-4cc4-97a4-75e6d1652eb0\">>}},\n                        {{credit_from,<0.17037.1>},199}]\n          trap_exit: true\n          status: running\n          heap_size: 1598\n          stack_size: 27\n          reductions: 5548\n        neighbours:\n...............\nLater Node A stopped.\n  =INFO REPORT==== 22-Jan-2018::11:02:49 ===\n  Stopped RabbitMQ application\n\n  =INFO REPORT==== 22-Jan-2018::11:02:49 ===\n  Halting Erlang VM\n\nWhat happened is the following sequence of the events in src/rabbit_reader.erl:\n\nI receive {'EXIT',#Port<0.16148>,normal} from the closed socket\n```\nmainloop(Deb, Buf, BufLen, State = #v1{sock = Sock,\n                                       connection_state = CS,\n                                       connection = #connection{\n                                         name = ConnName}}) ->\n    Recv = rabbit_net:recv(Sock),\n    ...\n    case Recv of\n        ...\n        % Other == {'EXIT',#Port<0.16148>,normal} here\n\n\n{other, Other}  ->\n        case handle_other(Other, State) of\n            stop     -> ok;\n            NewState -> recvloop(Deb, Buf, BufLen, NewState)\n        end\nend.\n\n```   \n\n\n\n\nInstead of acting fast and cleaning up immediately, rabbit_reader follows the standard handle_other(...) procedure where it sends some messages back to the closed socket.\nEventually it waits in wait_for_channel_termination(...) for {'EXIT', Sock, _Reason} after which it must cleanup socket as the following:\n [channel_cleanup(ChPid, State) || ChPid <- all_channels()],\n            exit(normal);\nAlready dead socket never send anything back, so it has timeout instead and exits with exit(channel_termination_timeout).\n\nThe question is that - should we just add {other, {'EXIT', Sock, _Reason}} match expression before {other, Other} and initiate channel_cleanup right there, w/o passing it down through handle_other? From a first glance I don't see a lot of trouble here.. I guess this is related to PR #1302.. @michaelklishin yes, I believe this should be applied to 3.6.x.\n. Nope, nothing in particular. We just feel uncomfortable w/o ability to change that setting fast enough. Frankly speaking, \"15 seconds\" sounds too arbitrary, so we're certain there will be situations when we have to change it.\n. This looks like #687.. @michaelklishin I think you should remove syslog.target (like we did already). Neither rabbitmq nor any dependent services in chain actually relies on syslog, so why keep it?\n. @michaelklishin done. See PR #804.\n. @michaelklishin applying t to stable isn't absolutely necessary. Removing additional service startup dependencies will only decrease startup time, won't fix any issues. So this is just a minor improvement. Feel free to apply it there though but I'm not insisting :)\n. Ok, I'm taking my words back :)\n@michaelklishin please push it to stable as well (upcoming 3.6.2 I suppose, right?) - this would save us one additional patch (or even the entire service-file) from shipping with src.rpm.\n. This patch introduces another one service-file copy. I believe it's better to use one already shipped (rabbitmq-server.service.example).\n. @binarin it should be the same - that's the selling point :). All small custom changes should be done with snippets/overrides in /etc/systemd/system.\n. Nice catch :)\n. @michaelklishin sure :). I always forgot what branch I should use, so I just go to \"Pull request\" and see what others use as a base for their PRs :). This time I saw that master is a preferred base branch :)\n. Cool! Thanks, @binarin for that, and thanks @dcorbacho for merging it finally!\n. Please before merging address the issue mentioned in my comment:\nrabbitmq/rabbitmq-server@c49e2949451178175987702cfb59b515113940f0#commitcomment-18758501\n. Just for the record - this looks like #714.. Hello All!\nThis patch seems to fix #544 (and #530) fully. At least we can no longer see it.. ",
    "jnordberg": "```\nReporting server status on {{2014,8,13},{14,33,50}}\n...\nStatus of node rabbit@safetyos ...\n[{pid,1401},\n {running_applications,\n     [{rabbitmq_management,\"RabbitMQ Management Console\",\"3.3.5\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.3.5\"},\n      {webmachine,\"webmachine\",\"1.10.3-rmq3.3.5-gite9359c7\"},\n      {mochiweb,\"MochiMedia Web Server\",\"2.7.0-rmq3.3.5-git680dba8\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.3.5\"},\n      {rabbit,\"RabbitMQ\",\"3.3.5\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.2.15\"},\n      {inets,\"INETS  CXC 138 49\",\"5.10.2\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.12.1\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.3.5\"},\n      {xmerl,\"XML parser\",\"1.3.7\"},\n      {sasl,\"SASL  CXC 138 11\",\"2.4\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"2.1\"},\n      {kernel,\"ERTS  CXC 138 10\",\"3.0.1\"}]},\n {os,{unix,linux}},\n {erlang_version,\n     \"Erlang/OTP 17 [erts-6.1] [source-d2a4c20] [smp:2:2] [async-threads:30] [kernel-poll:true]\\n\"},\n {memory,\n     [{total,23947552},\n      {connection_procs,2776},\n      {queue_procs,2776},\n      {plugins,240824},\n      {other_proc,8844672},\n      {mnesia,31272},\n      {mgmt_db,59388},\n      {msg_index,12544},\n      {other_ets,553828},\n      {binary,198488},\n      {code,10566398},\n      {atom,531937},\n      {other_system,2902649}]},\n {alarms,[]},\n {listeners,[{clustering,25672,\"::\"},{amqp,5672,\"::\"}]},\n {vm_memory_high_watermark,0.4},\n {vm_memory_limit,794127564},\n {disk_free_limit,50000000},\n {disk_free,13210066944},\n {file_descriptors,\n     [{total_limit,924},{total_used,3},{sockets_limit,829},{sockets_used,1}]},\n {processes,[{limit,1048576},{used,179}]},\n {run_queue,0},\n {uptime,91}]\nCluster status of node rabbit@safetyos ...\n[{nodes,[{disc,[rabbit@safetyos]}]},\n {running_nodes,[rabbit@safetyos]},\n {cluster_name,<\"rabbit@safetyos\">},\n {partitions,[]}]\nApplication environment of node rabbit@safetyos ...\n[{auth_backends,[rabbit_auth_backend_internal]},\n {auth_mechanisms,['PLAIN','AMQPLAIN']},\n {backing_queue_module,rabbit_variable_queue},\n {channel_max,0},\n {cluster_nodes,{[],disc}},\n {cluster_partition_handling,ignore},\n {collect_statistics,fine},\n {collect_statistics_interval,5000},\n {default_permissions,[<<\".\">>,<<\".\">>,<<\".*\">>]},\n {default_user,<<\"guest\">>},\n {default_user_tags,[administrator]},\n {default_vhost,<<\"/\">>},\n {delegate_count,16},\n {disk_free_limit,50000000},\n {enabled_plugins_file,\"/etc/rabbitmq/enabled_plugins\"},\n {error_logger,{file,\"/var/log/rabbitmq/rabbit@safetyos.log\"}},\n {frame_max,131072},\n {halt_on_upgrade_failure,true},\n {heartbeat,580},\n {hipe_compile,false},\n {hipe_modules,[rabbit_reader,rabbit_channel,gen_server2,rabbit_exchange,\n                rabbit_command_assembler,rabbit_framing_amqp_0_9_1,\n                rabbit_basic,rabbit_event,lists,queue,priority_queue,\n                rabbit_router,rabbit_trace,rabbit_misc,rabbit_binary_parser,\n                rabbit_exchange_type_direct,rabbit_guid,rabbit_net,\n                rabbit_amqqueue_process,rabbit_variable_queue,\n                rabbit_binary_generator,rabbit_writer,delegate,gb_sets,lqueue,\n                sets,orddict,rabbit_amqqueue,rabbit_limiter,gb_trees,\n                rabbit_queue_index,rabbit_exchange_decorator,gen,dict,ordsets,\n                file_handle_cache,rabbit_msg_store,array,\n                rabbit_msg_store_ets_index,rabbit_msg_file,\n                rabbit_exchange_type_fanout,rabbit_exchange_type_topic,mnesia,\n                mnesia_lib,rpc,mnesia_tm,qlc,sofs,proplists,credit_flow,pmon,\n                ssl_connection,tls_connection,ssl_record,tls_record,gen_fsm,\n                ssl]},\n {included_applications,[]},\n {log_levels,[{connection,info}]},\n {loopback_users,[]},\n {msg_store_file_size_limit,16777216},\n {msg_store_index_module,rabbit_msg_store_ets_index},\n {plugins_dir,\"/usr/lib/rabbitmq/lib/rabbitmq_server-3.3.5/sbin/../plugins\"},\n {plugins_expand_dir,\"/var/lib/rabbitmq/mnesia/rabbit@safetyos-plugins-expand\"},\n {queue_index_max_journal_entries,65536},\n {reverse_dns_lookups,false},\n {sasl_error_logger,{file,\"/var/log/rabbitmq/rabbit@safetyos-sasl.log\"}},\n {server_properties,[]},\n {ssl_apps,[asn1,crypto,public_key,ssl]},\n {ssl_cert_login_from,distinguished_name},\n {ssl_listeners,[]},\n {ssl_options,[]},\n {tcp_listen_options,[binary,\n                      {packet,raw},\n                      {reuseaddr,true},\n                      {backlog,128},\n                      {nodelay,true},\n                      {linger,{true,0}},\n                      {exit_on_close,false}]},\n {tcp_listeners,[5672]},\n {trace_vhosts,[]},\n {vm_memory_high_watermark,0.4},\n {vm_memory_high_watermark_paging_ratio,0.5}]\nConnections:\nChannels:\nQueues on /:\nExchanges on /:\nname    type    durable auto_delete internal    arguments   policy\n    direct  true    false   false   []\namq.direct  direct  true    false   false   []\namq.fanout  fanout  true    false   false   []\namq.headers headers true    false   false   []\namq.match   headers true    false   false   []\namq.rabbitmq.log    topic   true    false   true    []\namq.rabbitmq.trace  topic   true    false   true    []\namq.topic   topic   true    false   false   []\ntest-exchange   topic   true    false   false   []\nBindings on /:\nConsumers on /:\nPermissions on /:\nuser    configure   write   read\nguest   .  .  .*\nPolicies on /:\nParameters on /:\n...done.\n```\n. I just figured it out when trying to write a small code example that could reproduce it, in some cases I was sending priority: 0 which causes this error.\n``` javascript\nvar amqp = require('amqp')\nvar connection = amqp.createConnection({host: 'localhost'})\nconnection.on('error', function(error) {\n  console.log('connection error', error)\n})\nconnection.once('ready', function() {\n  console.log('connection ready')\nvar exchangeOpts = {\n    durable: true,\n    autoDelete: false,\n    confirm: true\n  }\nconnection.exchange('test-exchange', exchangeOpts, function(exchange) {\n    console.log('exchange open')\n    var msg = {hello: 'world'}\n    var opts = {priority: 0}\n    console.log('publish message', msg);\n    exchange.publish('test', msg, opts, function() {\n      console.log('message published', msg, arguments)\n    })\n  })\nconnection.queue('my-queue', function(queue) {\n    console.log('queue ready')\n    queue.bind('test-exchange', 'test')\n    queue.subscribe(function(message) {\n      console.log('got message', message);\n    })\n  })\n})\n```\n$ node queue-crash.js\nconnection ready\nexchange open\npublish message { hello: 'world' }\nqueue ready\nUnhandled connection error: INTERNAL_ERROR\nconnection error { [Error: INTERNAL_ERROR] code: 541 }\n. @michaelklishin yep, node-amqp was the culprit. Sorry for the hassle and thanks for the amqp.lib tip, setting priority to 0 there causes no problems.\n. ",
    "geekpete": "Do you have any more specifics around this known problem? \nWas it fixed in 17.1? \nDo you have any links to info about it?\nDoes it relate in any way to the Hipe plugin?\nCheers.\n. Thanks for your response.\nThe paging behaviour sounds like it's triggering the flow control.\nA test scenario we've used to replicate the behaviour is having 5 queues with 2-5 million msgs in them each then try to publish more messages to the server. Flow control kicks in dropping the publish rate from 5k/sec to 300 msgs/sec. Purging one queue at a time sees the speed incrementally speed up, first jumping back up to 1700/sec then back to 5k/sec once enough messages have been purged.\nSo from my server's point of view, it can totally handle that paging as it has enough IO being a physical server with SSD and using the deadline scheduler (another option would also be to use NOOP scheduler but usually similar raw speed to deadline). We also know that the server can hande very large volumes of messages in a backlog.\nI do realise that wanting a backlog on purpose might be an edge case.\nI think the analogy for my case is closer to a very large car park next to a loading dock rather than a city with traffic. I want to unload all the cars from all the ships as fast I can so the ships can then depart for the next port and the cars can be collected whenever. Making the ships unload slower once half the car park is full isn't helping.\nIt feels a bit like having to bump up max files open or similar settings on the OS, the limit is a general average case setting which keeps most people out of trouble but for high performance scenarios it's a limit that becomes artificial and has to be increased when using much larger/faster hardware.\nSo while I can tweak the vm_memory_high_watermark_paging_ratio setting in combination with the vm_memory_high_watermark, I'm guessing that once that ratio is hit the flow control will kick in. This is great if I can get a server with enormous amounts of ram, but otherwise adjusting this on my current rig will only delay the flow control. It would be great to have it page out to disk to avoid using ram, hit the IO hard and I'll give it enough fast disk to deal with it.\nI'm also using both durable and persistent for my queues, so would this be writing the messages out to disk twice in a way? Once for the persistence and again for paging them out of ram?\nI'd love to be able to disable the flow control memory paging trigger, keep hitting the rabbit with messages, have it pour them out to disk and as long as I don't go near the high water mark and it's coping then have it continue to go full publish speed.\nI'll do some more testing today with different high water mark settings and see how I go.\nI'll also investigate using servers with a great deal more ram.\n. Hey Team,\nThanks for the follow up with these improvement references.\nWe'll test out these new features in the new version.\n. Hey Team,\nJust like to say thanks for this lazy queue feature.\nWe now don't hit any flow control from initial testing. 100+ million in the queue and still flowing and no back pressure. For all intents and purposes with some caveats (disks and filesystem cache are fast enough to deal with read/write workload) this solves our flow control problems at the queue level.\nFrom what it looks like, the disk will have to to 100% before we see any problems.\nAnd this performs quite well even on SATA RAID 10 so SSD should cover it fully.\nCan anyone think of or has seen any other issues/caveats with using lazy policy for all queues?\n. Excellent, thanks.\n. We'll look to bandaid this with our config management for now.\nBut I do see your point in that different OSes would behave differently for symlink targets and if the plugin wasn't enabled, then you'd not want that too to be available until it is.\nIt gets a bit hairy.\n. ",
    "Mimikoo": "michaelklishin, well, I'm not a user of erlang so don't know what are those.\nAnyway, master branch of erlang is v.18.0-rc0 and are removed those specs. The maint branch of erlang is v.17.1-rc0, which have those and erlang give warnings.\n. ",
    "adrai": "ok: https://groups.google.com/forum/#!topic/rabbitmq-users/Nvh0MvbsLoA\n. ",
    "marcosdiez": "I just ported @dezmodue 's idea. I hope it gets merged this time.\n. ",
    "imran-uk": "Hi Michael,\nSorry, I posted to the wrong repo. It was \"https://github.com/rabbitmq/rabbitmq-tutorials\" that I was referring to, specifically: rabbitmq-tutorials/javascript-nodejs\nI'm following those alongside http://www.rabbitmq.com/getstarted.html\n. ",
    "gonace": "Okei, but when I add one then the other the first is removed from the cluster.\n. I'm sure that I'm the one missing something, I'll take a look at the documentation you provided. Thanks!\n. ",
    "offlinehacker": "Ok, will do, we don't have selinux on nixos enabled by default, so this is not the case ;)\n. ",
    "mbroadst": ":+1: \n. @binarin I'm pretty sure that wasn't available back then, it was still early days for systemd :smile: I'd say go ahead and submit the change, see what the maintainers say!\n. @michaelklishin sounds like the right way to me, unless @binarin has actually seen an issue with the current code\n. ",
    "jeckersb": "Updated, passes check-xref for me\n. http://people.redhat.com/jeckersb/rabbitmq-issue-224/\nThat's the logs from the two nodes that remained up.  I've left the other one off for now, there shouldn't be much of interest on it anyway since it was violently slayed.  If you want the logs from it I'll power it back up and grab them.\nYou can see at the end of the mac5254005e6a60 where I killed the queue pid, and all the clients in turn reconnect.\nThe process list is from rabbitmqctl eval 'rabbit_diagnostics:maybe_stuck().'.  I edited it down to only include the chain of stuck pids related to this queue.  I put the full list in the directory above with the logs.  You can see the others that stuck in the same manner.\n. Also this doesn't seem to be a recent thing, I can reproduce with 3.3.5 as well.\n. I haven't tried the RC yet, I'm building it now for fedora rawhide.  Stay tuned.\n. Reorganized my dump directory by version, added result for 3.5.4.rc1.  Still reproduces.  The only thing of note is that all of the hung pids ended up on one node, but I think that's more dumb luck than anything.  I'll try a few more times to make sure that it happens on both nodes sometimes.\n. Doh, sorry about that.  I fixed the permissions.\n. A bit more info...\nMy reproducer setup consists of a cluster of VMs running on a single workstation.  I can't reproduce this with the same configuration when used on bare metal servers.  This leads me to believe it might be a race condition that is more easily triggered either because (a) VMs are slow, or (b) the network is faster than a real network.\n. @dumbbell \nYes, we've got the following:\n- In rabbitmq.config:\n{tcp_listen_options, [binary,{packet, raw},\n                                {reuseaddr, true},\n                                {backlog, 128},\n                                {nodelay, true},\n                                {exit_on_close, false},\n                                {keepalive, true}]},\nAnd the associated sysctls:\nnet.ipv4.tcp_keepalive_intvl = 1\n  net.ipv4.tcp_keepalive_probes = 5\n  net.ipv4.tcp_keepalive_time = 5\nTo turn on keepalives, because the current client version on the other end does not support heartbeat. \n- And then also setting:\nRABBITMQ_SERVER_ERL_ARGS=\"+K true +A30 +P 1048576 -kernel inet_default_connect_options [{nodelay,true},{raw,6,18,<<5000:64/native>>}] -kernel inet_default_listen_options [{raw,6,18,<<5000:64/native>>}]\"\nWhich sets the TCP_USER_TIMEOUT socket option to 5 seconds, the idea being to quickly detect when an established connection fails.  I immediately was suspicious of this, so I removed it and tested without it and it still reproduced.  However that was on 3.3.5, so I need to re-run my test on the latest RC and with this bit removed.  I'll let you know how that goes.\nFor some more rationale behind the TCP_USER_TIMEOUT thing, check http://john.eckersberg.com/improving-ha-failures-with-tcp-timeouts.html.  I think some of that is misleading or flat out wrong since I have learned more since writing it, but you'll get the idea.\n. I provisioned a new cluster, since I managed to destroy the last one beyond repair.\nHere's the latest set of data from the new cluster - http://people.redhat.com/jeckersb/rabbitmq-issue-224/3.5.4.rc1-2015-07-16-14:25/\nI've included the -sasl.log (it's always empty).\nThe nodes are configured in rabbitmq-env.conf explicitly to bind to a statically-allocated IP address:\n[root@mac52540097b4be ~]# cat /etc/rabbitmq/rabbitmq-env.conf \nRABBITMQ_NODENAME=rabbit@lb-backend-mac52540097b4be\nRABBITMQ_NODE_IP_ADDRESS=192.168.200.31\nRABBITMQ_NODE_PORT=5672\nRABBITMQ_SERVER_ERL_ARGS=\"+K true +A30 +P 1048576 -kernel inet_default_connect_options [{nodelay,true},{raw,6,18,<<5000:64/native>>}] -kernel inet_default_listen_options [{raw,6,18,<<5000:64/native>>}]\"\n[root@mac52540097b4be ~]# ss -ntp | grep beam\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:44629  users:((\"beam\",30972,37))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:47190  users:((\"beam\",30972,35))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:60884  users:((\"beam\",30972,26))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:46579  users:((\"beam\",30972,30))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:40713  users:((\"beam\",30972,42))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:57951  users:((\"beam\",30972,24))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:46374  users:((\"beam\",30972,28))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:60837  users:((\"beam\",30972,20))\nESTAB      0      0            192.168.200.31:33445       192.168.200.32:35672  users:((\"beam\",30972,11))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:57921  users:((\"beam\",30972,10))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:52197  users:((\"beam\",30972,40))\nESTAB      0      0            192.168.200.31:5672        192.168.200.34:47773  users:((\"beam\",30972,31))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:44036  users:((\"beam\",30972,33))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:41653  users:((\"beam\",30972,22))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:57924  users:((\"beam\",30972,21))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:60904  users:((\"beam\",30972,23))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:43413  users:((\"beam\",30972,27))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:57957  users:((\"beam\",30972,29))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:41464  users:((\"beam\",30972,19))\nSYN-SENT   0      1            192.168.200.31:56730       192.168.200.33:4369   users:((\"beam\",30972,17))\nESTAB      0      0            192.168.200.31:5672        192.168.200.32:44482  users:((\"beam\",30972,36))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:37812  users:((\"beam\",30972,41))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:32903  users:((\"beam\",30972,38))\nESTAB      0      0                 127.0.0.1:49082            127.0.0.1:4369   users:((\"beam\",30972,7))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:33089  users:((\"beam\",30972,39))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:46971  users:((\"beam\",30972,34))\nESTAB      0      0            192.168.200.31:5672        192.168.200.34:47770  users:((\"beam\",30972,25))\nESTAB      0      0            192.168.200.31:5672        192.168.200.31:60960  users:((\"beam\",30972,32))\nThe clients hit the bound address on each node directly and do not traverse any load balancer.\n. Jean-S\u00e9bastien P\u00e9dron notifications@github.com writes:\n\n\nWhich sets the TCP_USER_TIMEOUT socket option to 5 seconds, the idea being to quickly detect when an established connection fails. I immediately was suspicious of this, so I removed it and tested without it and it still reproduced. However that was on 3.3.5, so I need to re-run my test on the latest RC and with this bit removed. I'll let you know how that goes.\n\nDid you have a chance to test again with 3.5.4-rcX and without TCP_USER_TIMEOUT?\n\nI did, it still reproduced in that configuration.\n. New dump: http://people.redhat.com/jeckersb/rabbitmq-issue-224/3.5.4.rc1-2015-07-17-13:25/\nKudos to xz for compressing the debug data down!\nTo answer your questions...\nThere are VIPs and haproxy is running, but none of the AMQP traffic goes through any of the VIPs or through haproxy.  I purposefully removed that after I wrote that blog post, until the user timeout stuff fixes landed in the kernel and in haproxy.  Those changes have landed only recently and I haven't re-enabled load-balanced AMQP traffic yet.\nI updated the queues.txt file this time to include columns for durable, auto_delete, and exclusive_consumer_pid.  It looks like none of the queues are durable or exclusive, and some of them are auto_delete.\nI left everything on the openstack account instead of moving to guest:guest.  The password is randomly generated cd5d6f99f0e2e87a808ec35bcc9689df and is inaccessible so all the world can know :smile: \n. My naive assessment of the debug dumps seems to indicate this is some\nkind of race with mirrored, auto_delete queues?\nThe master goes through rabbit_amqqueue_process:handle_ch_down and\ndetermines rabbit_amqqueue_process:should_auto_delete/1 -> true, so it\nstart doing the needful to tear down the queue.\nMeanwhile, on the surviving slave, I see a bunch of stuff that seems\nto stem from\n2> (<7121.2197.0>) call gm:prioritise_info({'DOWN',#Ref<7121.0.0.10304>,process,<7358.2273.0>,{shutdown,ring_shutdown}},0,{state,{0,<7121.2197.0>},\nwith a lot of << timeout and\nrabbit_misc.execute_mnesia_transaction. sprinkled in.\nSo my best (and probably wildly incorrect) guess is...\n- The master has halfway deleted the queue, has some mnesia\n  transaction going that is blocking the slave, and is waiting for the\n  slave to reply.\n- Meanwhile, the slave is trying to handle the 'DOWN' message, but\n  can't complete its transaction to mnesia (since the master has it\n  locked?), so it tries to poke the master, gets no reply, and then\n  decides the master is down, too.  So it promotes itself as the new\n  master.\n. I think we have roughly the same hypothesis!\n. An observation as I dig around in the code.  From https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_mirror_queue_coordinator.erl#L186-L189:\nThe design of gm is that the notification of the death of the master\nwill only appear once all messages in-flight from the master have been\nfully delivered to all members of the gm group.\nIf I understand correctly, that does not hold in this case.  In\nstop_all_slaves, the master sends gm:broadcast(GM, {delete_and_terminate, Reason})\nbut the first message I see arrive for the gm on the slave is:\n2> (<7121.1778.0>) << {'DOWN',#Ref<7121.0.0.8159>,process,<7358.1943.0>,\n                           {shutdown,ring_shutdown}}\nEdit: nevermind, I realized this message comes from the monitor and doesn't come through the GM.  See next comment.\n. I only see two messages get sent by the master GM.  The first is to the coordinator process, which just stops as a result:\n2> (<7121.1943.0>) <7121.1942.0> ! {'$gen_cast',{delete_and_terminate,normal}}\n...\n2> (<7121.1940.0>) << {'EXIT',<7121.1942.0>,{shutdown,ring_shutdown}}\nand the second is to the dead slave GM\n2> (<7121.1943.0>) <7330.1896.0> ! {'$gen_cast',\n                                 {'$gm',2,\n                                  {activity,\n                                   {0,<7121.1943.0>},\n                                   [{{0,<7121.1943.0>},\n                                     [{9,{delete_and_terminate,normal}}],\n                                     []}]}}}\nwhich of course never gets there.  And since the gm messages flow in a ring, the living slave never gets the message either since it would have needed to come through the dead node.\nSo, the master GM exits.  The slave GM notices via its monitor:\n2> (<7121.1778.0>) << {'DOWN',#Ref<7121.0.0.8159>,process,<7358.1943.0>,\n                           {shutdown,ring_shutdown}}\nAnd then the whole thing continues to fall apart as previously described.\n. That certainly seems to be a good assessment of the problem.  :+1: \n. Once you have a patch I will gladly test it!\nRaft sounds promising for some use cases.  I get a 404 for the rabbit-ha-raft repo above (probably private) but I at least read the Wikipedia page :smile: \n. I'll patch and build from source.  Thanks @dumbbell for the patch!\n. http://people.redhat.com/jeckersb/rabbitmq-issue-224/3.5.4.github-224-2015-07-28-11:57/\nSeems to have done the trick!  Zero stuck PIDs on both of the two remaining nodes so I just omitted the maybe_stuck output.\nI'll run through a few more times just to make sure it doesn't reproduce in a later run.  Fingers crossed.\n. I ran through three more times, no stuck processes.  So the patch gets a hearty :+1: from me!\n. Many thanks to you guys as well, especially @dumbbell for untangling the GM web.  I owe you a beverage of your choosing should we ever cross paths!\n. ",
    "binarin": "@jeckersb Hi. Is there a need to do this hard way, with the architecture-specific sd_notify? Or this change could be replaced with simple os:cmd(\"systemd-notify --ready\") ?\n. Not an issue, but more of annoyance when backporting/building RedHat packages - and looks like RedHat package is primary user of this feature.\nBut removing code and dependencies is always good )\n@lemenkov, what do you think about removing sd_notify dependency from RedHat package altogether, and just executing systemd-notify instead?\n. Yes, but currently systemd unit for rabbitmq in debian uses Type=simple so it's not affected by presence/absence of sd_notify.\n. @lemenkov Exactly, it's about removing sd_notify dependency altogether. I think I'll check that everything is indeed working correctly with os:cmd/1, and then propose following changes:\n- Add os:cmd/1 as a fallback for stable branch\n- Remove sd_notify and only leave os:cmd/1 there - for master branch\n. Should this custom name go in logs?\n. So is default value like 16 * erlang:system_info(logical_processors_available) good enough?\n. Looks like detecting number of CPUs is not as simple as it seems on some OSes. Lev Walkin posted the following snippet in erlang-russian ML:\nncpus() ->\n    case erlang:system_info(logical_processors_available) of\n        unknown -> % Happens on Mac OS X.\n            erlang:system_info(schedulers);\n        N -> N\n    end.\n. I don't think gen_server is a problem - using noreply and spawning additional helper processes will allow any timeout policy to be implemented. \nBut after looking at delegate I have following questions:\n1. First stated purpose of delegate is optimisation of numerous inter-node calls. But actually this can be solved by using just rpc:call on delegate:safe_invoke - as rpc already performs calls in separate process. Or if rpc is going to be performance bottleneck, then something like gen_rpc could be used instead.\n2. Second stated purpose is preserving ordering of messages. But as it happens based solely based on pid of process invoking delegate is it actually needed? At least for calls it happens naturally, because invoking process is naturally blocked on delegate:call at this point. So only casts need some additional mechanism for enforcing ordering.\n3. Looks like the third purpose is reducing number of inter-node monitors. But does it actualy improve anything? Because delegate:monitor/2 still involves gen_server call, and death of monitored process still result in sending of inter-node message.\nGiven all of the above are the separate delegate processes actually needed? Isn't it better to remove as much custom code as possible and switch to some readily available thing like rpc or gen_rpc?\n. One more thing that I can't wrap my head around is the comment at https://github.com/rabbitmq/rabbitmq-server/blob/master/src/delegate.erl#L113 and corresponding code. What's wrong with using timeout here? What wrong order of calls/cast infinite timeout prevents at that point?\n. I think I'm still missing something regarding ordering and timeout.\nHelper process starts at the beggining of gen_server:multi_call and it life ends before this gen_server:multi_call returns. Helper sends calls in exactly the same order as it happens with infinity timeout. So for the outside world there should be no difference at all: request were sent in the same order and there is no extra process hanging around after gen_server:multi_call returns.\nSo where is the race condition?\n. > For calls specifically, as @binarin points out, on the calling end (RPC client, \"first half\" in RabbitMQ Erlang client speak), there is no real difference as the middleman process in rpc that collects late responses is gone by the time rpc:[multi]call returns. However, on the receiving end (RPC server), we are limited by a single named process in rpc and a configurable pool in case of delegate.\n\nMost other differences are probably a lot less important.For a system such as RabbitMQ, where there are normally many queues and processes, to me it makes sense to use a pool.\n\nYes, but currently delegates in that pool are blocked when they perform invoke - they are completely synchronous. So probably some measurements should be made to compare those variants - there is even a chance that using rpc will improve performance.\nActually I see following easy improvement for delegate:call/2:\n1. It should invoke delegate on all nodes simutaneously. Currently calls on local node are done after remote nodes. The easiest way to do this is to use single gen_server2:multi_call - both for remote and local nodes.\n2. delegate:call/2 should be supported by separate handle_call clause in delegate server - to make it as asynchronous as possible - by spawning separate process to collect replies and handle timeouts. Something like this (heavily borrowed from gen_server:multi_call) - b\n``` erlang\nhandle_call({call, Msg, Grouped, Timeout}, From, State = #state{node = Node}) ->\n    Pids = orddict:fetch(Node, Grouped),\n    Tag = make_ref(),\n    Collector = spawn_link(delegate, call_collector, [Pids, From, Timeout, Tag]),\n    %% Here we sending all call requests from delegate process, just to be 100% sure\n    %% that message ordering will be preserved as in original delegate implementation.\n    %% But reply collection will happen in separate process, which will also handle timeouts.\n    [ catch Pid ! {'$gen_call', {Collector, Tag}, Msg} || Pid <- Pids ],\n    {noreply, State, hibernate);\ncall_collector(Pids, From, Timeout) ->\n    Monitors = [ erlang:monitor(process, Pid) || Pid <- Pids ],\n    TimerId = erlang:start_timer(Timeout, self(), ok),\n    Result = rec_pids(Tag, Monitors, TimerId, []),\n    gen_server2:reply(From, Result).\n%% Collecting successful replies and/or waiting for timeout.\nrec_pids(Tag, [], TimerId, Replies) ->\n    %% Every pid responded, everything is ok.\n    case catch erlang:cancel_timer(TimerId) of\n        false ->\n            receive\n                {timeout, TimerId, } -> ok\n            after 0 -> ok\n            end;\n        _ -> ok\n    end,\n    Replies;\nrec_pids(Tag, [Monitor | Monitors], TimerId, Replies) ->\n    receive\n        {{Tag, N}, Reply} -> %% Tag is bound!!!\n            %% One more successful reply\n            rec_pids(Tag, Monitors, TimerId, [Reply|Replies]);\n        {timeout, TimerId, } ->\n            %% We are out of time\n            rec_pids_rest(Tag, [Monitor|Monitors], Replies)\n    end.\nrec_pids_rest(Tag, [Monitor|Monitors], Replies) ->\n    receive\n        {'DOWN', Monitor, , , _} ->\n              rec_pids_rest(Tag, Monitors, [bad_rpc | Replies]);\n        {{Tag, N}, Reply} -> %% Tag is bound!!!\n             rec_pids_rest(Tag, Monitors, [Reply|Replies])\n     after 0 ->\n         rec_pids_rest(Tag, Monitors, [no_reply | Replies])\n     end;\nrec_pids_rest(_Tag, [], Replies) ->\n     Replies.\n```\n. > @binarin I find it hard to believe that 1 hardcoded RPC handler process will offer better throughput than 16 (or generally, N) processes.\nIt depends on type of a workload you have. Because RPC server spawns process for every request and its throughput is in theory is limited only by number of erlang:spawn_monitor/1 it could do a second. On the other hand delegate server is blocked by it's call workload. So performance probably degrades at the point where that call workload becomes more computationally intensive that 16 spawns.\n\nDelegates are used by more than one channel in parallel in 99% of realistic workloads.\nI have a feeling this has been ignored entirely in this conversation.\n\nActually not :smile: I started to look into some ways to implement timeouts, and it looks like that by correctly implementing timeouts increased throughput could also be easily gained. I just forgot to mention that changes I've mentioned in previous message are about both delegate timeouts and improved throughput.\n. So imagine we have the following set of processes:\n- Pid_Local_10, Pid_Local_20, Pid_Local_Infinity - some gen_server's on local node which take 10, 20 and infinity(faulty process) seconds to process some gen_server:call.\n- Pid_Remote_10, Pid_Remote_20, Pid_Remote_Infinity - remote gen_servers which take corresponding amount of time to do their job.\nFirst scenario\nerlang\ndelegate:call([Pid_Remote_10, Pid_Local_Infinity], do_work)\nIs such problem also out of scope of this issue? Because delegate:call will hang, but it will not involve any cross-node calls.\nSecond scenario\nSuppose now there is new delegate:call/3 which accepts timeout as a last argument.\nerlang\ndelegate:call([Pid_Remote_10, Pid_Remote_20, Pid_Local_10], do_work, 25000)\nHere we have call with timeout of 25 seconds, and every referenced gen_server can do it's job during that time. But:\n- In current implementation local calls are performed only after remote ones are done. And call to Pid_Local_10 will timeout, because it'll have only 5 seconds left to do its job.\n- As calls are performed independently and  sequentially, Pid_Remote_20 will also time-out.\nAnd the question is what semantics should that timeout argument have:\n1. delegate:call/3 will be doing everything sequentially and will allow every gen_server to spend no more than timeout doing it's job. In that case we'll have only  some loose upper bound on amount of time delegate:call/3 will take - Timeout * ( MaxNumberGenServerPerRemoteNode + LocalGenServerCount). It's a completely insane API design.\n2. delegate:call/3 should return no later than after timeout. We are doing everything sequentially, and every next gen_server will have less and less time to handle the request. Reasonable for consumers of delegate API, but still very suboptimal.\n3. delegate:call/3 will be doing everything in parallel (going to nodes, spreading work to gen_servers on that nodes). And only in that case the timeout argument makes perfect sense: delegate:call/3 should return after that amount of time has passed, every gen_server in the list will have the same amount of time to do its job. Making delegate more asynchronous will be just a side-effect of making delegate:call/3 API sane.\n. What do you think about applying this (unscientific) guess automatically, if it isn't explicitly configured?\n. Maybe in this particular case of rabbit_amqqueue:notify_down_all/2 it will be easier to switch to delegate:cast/2? Or is there a reason for doing this notification synchronously?\n. Btw, adding timeout will help, but underlying issue will still be there.\nBecause the problem is not the absence of timeout, but the fact that a queue process became stuck in some infinite loop or in some receive.\nIt's not about down/unreachable nodes or unexpectedly terminated queue processes, as this cases are actually handled by delegate:call/2 (which has gen_server2:multi_call underneath) - erlang:monitor/2 is extensively used there.\n. I mentioned root cause just as a side note.\nWhen looking into a case of a channel hanged on delegate:call/2 I've noticed that it has monitors to some queue processes and those queue processes weren't emptying their inboxes. So if this behaviour of queues could be fixed there will be no need to add any timeouts to notify_down_all.\n. This optimism is based on erlang:monitor/2, which should detect all problems with remote node/process. And gen_server, which should correctly use those monitors. Not trusting this mechanisms while writing distributed Erlang code will lead one to insanity )\n. @michaelklishin, I still think you're putting wrong emphasis on cross-node calls. Timeouts are indeed needed. But they are needed to alleviate results of bad processes behaviour (BOTH remote and local).\nSame as for #166  - if timeouts will be added only to cross-node calls, it will not help.\nAdding timeouts has nothing to do with remote node failures. And so it doesn't make sense to calculate timeout from value of net_ticktime. Suppose we have scenario where we pass to delegate:call/2 200 pids: 100 for local processes and 100 for processes on single remote node. So in worst case this delegate:call/2 will return in approx 200*net_ticktime seconds - just some value that doesn't have any meaningful connection to real world.\nAnd then there is a problem that remote delegate process could become stuck itself (e.g. because it was handed bad batch of work). So multi_call at https://github.com/rabbitmq/rabbitmq-server/blob/master/src/delegate.erl#L119 should also has some sort of sensible timeout - but definitely not proportional to net_ticktime :smile: \nAnother possible fix that'll improve system responsiveness and will prevent delegate processes from becoming stuck is to spawn a new process for every request at https://github.com/rabbitmq/rabbitmq-server/blob/master/src/delegate.erl#L217 \n. That stacktrace is very interesting in itself.  I've seen similar one,  and I can't grasp what's happening. And there are 2 things that I can't explain:\n- Somebody is sending unasked for messages to channel process\n- Stacktraces of dead processes contain strange function calls to module_info in different, completely unrelated modules.\nEverything starts with mnesia event about inconsistent database.\nThen channel process receive strange message:\n{{#Ref<0.0.23.89627>,'rabbit@messaging-node-202'},\n                        [{ok,<8705.5665.45>,ok}]}\nwhich it doesn't know how to handle (as there is no even remotely similar clause of handle_info/2). \nAnd it results in an even more strange stacktrace:\n** {function_clause,[{rabbit_channel,handle_info,2,[]},\n                     {gen_server2,handle_msg,2,[]},\n                     {ssl_config,module_info,1,[]}]}\nHow did ssl_config:module_info/1 end there? Normal stacktrace contains proc_lib:wake_up/3 there.\nAnd on another occasion I've seen queue slave process terminated with \n** {function_clause,[{orddict,fetch,2,[]},\n                     {dtls_connection,module_info,1,[]},\n                     {gen_server2,handle_msg,2,[]},\n                     {dtls_connection,module_info,1,[]}]}\nWhat have queue slave process and DTLS (which is not even used anywhere) in common? What is responsible for all those module_info calls?\n. Situation itself is triggered by some other bug (but I'm not sure this is #224, there were no similar log records and no processes with similar stacktraces).\nLoop happends inside rabbit_channel:handle_method(#'queue.declare'{}, ...).\nIt calls maybe_stat/2, which in turn calls rabbit_amqueue:stat/1. This call\nresults in nodedown exit, and it finally triggers infinite loop in rabbit_amqqueue:with/3.\nWhile something is wrong in different part of the system, I don't think it's a good idea to loop forever at this point - as a result we have server which doesn't respond to rabbitmqctl list_channels and also a client that is forever blocked on queue.declare.\nSo at least some safeguard should be added that limits number of rabbit_amqqueue:with/3 iterations - with some reasonable number of attemts or with some overall timeout.\n. @dims I'm going to provide a separate PR, for further discussion.\n. I think that alarm list is actually being updated in this case. The problem is that in case of alarmed node brutal death, blocked publishers don't receive 'connection.unblocked' frame. Publishers connected after the moment of death don't have any problems.\nSo 'connection.unblocked' should be sent to every blocked publisher even in case of unclean death of alarmed node.\n. There are 2 problems there:\n1. on node_down rabbit_alarm notifies its subscribers with single notification with Source=[]. If instead notification will be sent for every type of alarm from down node, it will solve the case from original message.\n2. rabbit_alarm doesn't send information to its subscribers about which node the alarm corresponds to. As a result I was able to find another bug: set disk alarm for 2 of 3 nodes, try to pubsish message and become blocked, and after clearing disk alarm only on one node it becames unblocked. But I think in this case the publisher should be unlocked until alarms an all nodes were cleared. Fixing this will require more work: rabbit_alarm subscription protocol should be updated with node information, and node information should be saved along with alarm type in rabbit_reader #throttle{}.\n. The more I look into rabbit_alarm, the messier it appears )\nActually, I think I want to solve both problems with one patch, because simple fix for the first issue will not be final.\nI'll try to make a draft patch tommorow, so there will be something to discuss.\n. Currently I'm thinking about splitting rabbit_alarm into 2 parts: that handles local-only alarms (file_descriptor_limit) and another one that handles cluster-wide alarms (resource_limit). The main reason is that currently the alarmed_nodes and alarms are sometimes not consitent inside #alarms{}, and the effort to make them consistent is not worth it (in my opinion). Is this OK?\n. As heartbeater process is doing constant rabbit_net:send/2 to client socket, it is possible to detect dead socket after 2 heartbeat intervals. And it will be possible to notify parent process about lost connection at that point. So at least some sort of fix is possible now.\nThere is no portable way to detect lost connection without doing send or recv, and looks like it is not an Erlang problem, but just the way sockets work.\nI haven't looked into other OSes, but on linux you can use linux-specific getsockopt TCP_INFO and if state!=ESTABLISHED, you can infer that socket is dead. So if this getsockopt call is wrapped into NIF, than on some OSes it will be possible to detect dead sockets faster than 2 heartbeat intervals. Will be the patch with such NIF accepted?\n. When connection is blocked due to resource alarm, heartbeat receiver is paused and no heartbeat checks are being performed. But heartbeat sender is not stopped and periodically sends data over network. And only when it is not able to send another packet to the network, it'll notify parent process.\nSo this change should only affect the behaviour of dropped connections.\nThe only \"incompatibility\" I can see is that rabbit_reader:stop/2 could be called with different Reason (compared to pre-patch code). But it'll require very precise timing of events and the outcome will be still the same - a properly stopped connection.\n. The only attempt to standartization of exit codes that I found is http://www.sbras.ru/cgi-bin/www/unix_help/unix-man?sysexits+3\nIn the following table I summarized those that I think are relevant. Is this list reasonable and complete?\n| Numeric Value | sysexits (if any) | Description |\n| --- | --- | --- |\n| 0 | EX_OK | Self-explanatory |\n| 64 | EX_USAGE | Bad command-line args |\n| 65 | EX_DATAERR | Some parameters do not make sense |\n| 69 | EX_UNAVAILABLE | Failed to connect to node |\n| 75 | EX_TEMPFAIL | Temporary failure (e.g. something timed out) |\n| 70 | EX_SOFTWARE | Any other error discovered when running command against live node |\n. Yes, I no longer think that just publishing something is a useful test. Especially without taking into account a lot of site-specific thing, like whether clustering or HA is used.\nAnd problems are usually more subtle and involve existing queues/channels. \n. @michaelklishin  Is 'master' going to become 3.6.1? Then it will suffice.\n. Yep, I'm aware of this - but it will only go to 3.7.0. And I think that new related test case will be useful even for lager logging.\n. Yes, I'll do it.\n. @dumbbell Done.\n. Actually, it's not a problem with rabbit_nodes:ensure_epmd - starter node is not connecting anywhere, so there will be no pollution.\n. I think I'll provide some preliminary patch today.\n. Fixed it differently, should go to 3.6.X.\n. @bogdando Yes, I did minimal testing by simulating 'list_channels timeout.\n. Currently there is no systemd unit files in source tree and so there is no place where this change can be applied.\nRedHat maintains their unit file externally, and they will need to make that change there when sd_notify will be completely removed.\nAnd I'll keep an eye on Debian packages and will suggest to switch from rabbitmqctl wait in their unit file. Right after they'll start to package 3.6.1 or some later version.\n. I though that systemd-notify will infer parrent pid correctly because \nos:cmd(\"echo $PPID\").\nworks correctly and systemd-notify by default uses this parent pid. But after some strace-ing I observe \nthat os:cmd/1 introduces one extra intermediary shell process. And even if it was working as intended, it wouldn't have lasted long: erlang 19 is going to change this scheme.\nSo yes, --pid should indeed be used. \n. No, because old users aren't affected - as 3.6.1 still contains old code which uses  sd_notify NIF.\n. Sorry about that. As always when you think that nothing could go wrong it'll definitely go wrong )\nAlso I've filed bugs about 'NotifyAccess=all' to RedHat and Debian, so it'll not catch them as a surprise.\n. Yes, there is still the same error. But not sure if it worth fixing.\nMy expectation was that release source tarball is a full equivalent of rabbitmq-server-release checkout with all dependencies downloaded. But currently unpacked tarball knows about all make targets, just some of them are non-functional. . https://youtu.be/tW49z8HqsNw?t=11m2s - WhatsApp is using tnnps (same as db).\n. So the channel in question:\n1) Has no reference to the bad queue in queue_names and queue_monitors.\n2) Retains references to the bad queue in queue_consumers and consumer_mapping.\nIt shows us that channel received 'DOWN' message and at least partially handled it in its handle_info/2 callback. But handle_consuming_queue_down/2 failed to do its job during that time.\n. Some testing reveals that there is a problem with guessing MAINPID.\nCurrently the first clause executes under systemd. But when it's done that way systemd is actually unhappy with:\nerlang\nos:cmd(\"systemd-notify --ready MAINPID=\" ++ os:getpid()),\nbecause it's the top-level shell process PID that should be passed to systemd.\n. Maybe additional argument for scripts/rabbitmq-server should be introduced - -systemd, in addition to already existing -detached?\n. epmd dependency isn't a problem - it's implicit because jessie uses socket activation for epmd. \n. > Mar 10 12:10:36 rabbit-jpedron-debian1 systemd[1]: Cannot find unit for notify message of PID 14136.\nSo looks --pid should be used to prevent this, and it'll set 'MAINPID=%s' automatically. Probably my problem with --pid was that I was testing from a user unit. \n. It's some sort of permission problems with systemd-notify - looks like sd-notify daemon ignores pid passed. But if I invoke the same command from root shell, startup ends up successfully. I hope there is some permission-related option that'll allow to fix it.\n. systemd-notify is broken for non-root users - https://github.com/systemd/systemd/issues/2739\nSo I think NIF dependency should be restored in the master branch for a time being.\n. I was thinking about Perl myself, but wasn't sure it would be acceptable solution :smile: \nShould I amend this PR with your suggestions from above or are you going to do it yourself?\n. Another alternative to perl is using socat and erlang:open_port.\nI'm using following implementation of systemd-notify and running start/stop in a loop works completely fine:\necho -en \"MAINPID=$PPID\\nREADY=1\\n\" | socat unix-sendto:$NOTIFY_SOCKET STDIO\n. socat is readily available on centos7 and on every supported version of debian and ubuntu. At least on centos7 perl is not a mandatory package, and the difference in package size between perl and socat is 30-fold. I think we should stick with socat.\n. 1. Here is my glorified systemd-notify :smile: using socat - it avoids problem of early process exit\n```\n   #!/bin/bash -eux\n   TEMP=$(getopt -o \"\" --long ready,pid: -n \"$0\" -- \"$@\")\n   eval set -- \"$TEMP\"\nwhile true; do\n   case \"$1\" in\n       --ready) shift;;\n       --pid) PID=$2; shift 2 ;;\n       --) shift; break;;\n       *) echo \"Internal error on arg $1\"; exit 1 ;\n   esac\n   done\n(\n   echo -en \"MAINPID=$PID\\nREADY=1\\n\"\n   while [[ $(systemctl show --property=ActiveState rabbitmq-server) == 'ActiveState=activating' ]]; do\n       sleep 1\n   done\n   ) | socat unix-sendto:$NOTIFY_SOCKET STDIO\n   ```\nI'm going to test whether the same approach will work with erlang:open_port.\n2. CentOS 7 doesn't have perl installed by default, and here we have difference between 300k of socat and more than 10M of perl.\n3. Default start timeout is 90 seconds, some reasonable value should be choosen instead and specified in unit file, e.g.TimeoutStartSec=3600\n. Next funny finding - if epmd was not running initially, it'll send acknowledgement to systemd itself. It will result in marking rabbitmq as running prematurely.\n\n. I'm still debugging one more issue with this patch, so please don't merge even if it passes your tests )\nInitially idea was to add systemd-notify as a fallback for NIF in stable branch, and to get rid of NIF completely only in master. So as not to cause any pains for users of the current stable version.\n. Now the patch is ready. Unless I've missed something this time )\n. Added port_close calld and more explanations about usage of socat.\n. Testing uncovered some more issues, fixed them also.\n. I've decided to introduce call_emitter due to following reasons:\n- Less confusion, it took me some time to discover that emitting versions of rabbit_channel:info_all etc. are indeded used. And because rabbit_control_main:call/2 is completely different function, which should be called something like call_utf8_args - but I'm leaving it for some later PR.\n- Less confusion, again. Functions with more than 3 or 4 arguments are usually very confusing.\nSo my current plan is replace all calls to rabbit_control_main:call/4, call/5 and call/6 with a call to the new call_emitter/4.\n. Done.\nI have my own test-suite for this issue at https://github.com/binarin/rabbit_destructive_tests - but it requires linux and password-less sudo for tc command. Still, it allows anybody to see that promised performance improvements were indeed achived. And I have some distant plans on re-doing this tests without sudo/tc dependency, using only pure erlang - then those tests could be incorporated into rabbitmq-test.\n. I have the following test setup:\n- single physical machine: i5 at 3.3 GHz, 16 GB RAM\n- 3-node cluster connected through loopback interface\n- 15000 queues evenly distributed across those 3 nodes\n- 10000 channels on 50 connections evenly distributed across those 3 nodes.\n- delayed in the table below means introducing 1ms delay on loopback interface.\n- all results are in seconds\n| command | original, w/o delay | original, delayed | patched, w/o delay | patched, delayed |\n| --- | --- | --- | --- | --- |\n| list_channels | 1.2 | 15.5 | 0.78 | 0.82 |\n| list_queues | 3.6 | 24.4 | 0.94 | 1.07 |\n. WIP again, I've found some issues with current version of this code.\n. Now I believe it's really done )\nSolution is somewhat complex, but performance gain is worth it - there will be significantly less false positives when list_XXX commands are used as a health checks.\n. Different output formats already can be supported by providing different display fun.\nThis patch is primarily targeted at a cases where there are thousands and thousands of items to be listed. And there are several reason to display items immediately, without collection:\n- Overall run-time - with intermediate collection step, rabbitmqctl will be sitting mostly idle initially, and will start formatting (i.e. actively using CPU) only after receiving everything. With immediate display this load will be evenly distributed, and rabbitmqctl will exit almost immediately after receiving last item.\n- Memory consumption - collection of tens of thousands info items can be relatively costly in terms of RAM\n- Interactively running lines will make ops people happy - there will be no way to suspect that something is stuck when you see some running lines on your terminal :smile: \n. Aha, got it. Will amend it in several hours.\n. I've updated the patches.\n. Somewhat unexpected side effect of making wait_for_info_messages/5 fold-like. I intentionally converted returning error to exit at https://github.com/binarin/rabbitmq-common/commit/5aae3c274ac8c41c143520a0d9450481c9c34df1#diff-b3c3007fa0d605476c3d9646fb7956b9R166 - to be completely like fold, which either return an accumulator or barfs.\nSo I need to:\n- Add code that will unwrap this nocatch thing in rabbit_cli:main/3, so it'll be reported in simple form {bad_argument, dummy} (as it happens in previous version).\n- Change unit-test so it'll expect the control action to throw error instead of returning it.\nDoes this sound reasonable?\n. BTW, what is expected error output in this case? I've checked both in 3.6.1 and in master, they both print\nListing queues ...\nError: {bad_argument,dummy}\nShould it be handled in this clause ? I've tried to add similar clause that will catch the same error in 3.6.1, but it looks ugly due to the big usage message.\n. Changed PR so that error messaged during interactive use is the same and the unit tests are passing.\n. Can't reproduce - in my checkout --online/--offline work flawlessly.\n. I'm going to implement it for master in https://github.com/rabbitmq/rabbitmq-server/pull/683\nIs the stable version needed at all or it could just wait for the PR above?\n. rabbit_cli.hrl contains --offline and --online options, may I use them instead of --down and --up?\n. https://github.com/ClusterLabs/resource-agents/blob/4030a3bc7a32a04ef49b76614da2af281f2b654e/heartbeat/ocf-shellfuncs.in#L434\n. And it's safe to remove the directory completely - it'll be automatically recreated.\n. Some more things that should be validated:\n- Server certificate has proper \"extended key usage\" field\n- Certificates/keys should be in PEM-format\n- Cipher list should contain at least one cipher suitable for key type (EC, DSA, ...)\nBecause at runtime this errors will be reported as insufficient_security/handshake_failure/bad_certificate without any useful details.\n. I don't think it's applicable to lager.\nAlso I've found that my patch is buggy: warning/2 will actually enter infinite loop on such a condition. But I'm not sure how to make sure that operator will get this error message and will be able to act on it:\n- Calling error_logger:warning_msg doesn't result in anything appearing in the logs\n- Calling io:format(standard_error, ..., ....) will at least get something to startup_err (or systemd journal)\n- Calling exit({io_broken, erlang_vm_restart_needed}) will get message directly to an operator invoking rabbitmcqtl. \nI'm in favor of combining the last 2 items. And now I think that throwing an error is a good thing in the end, as I've discovered that stop/stop_app/start_app can also stop to function after user process is dead.\n. LGTM. Especially given that there is no portable way to use syslog from erlang\n. The only potential drawback that I see is that on systemd-enabled systemd it'll require erlang where epmd is also managed by systemd. It can be a problem for some rare combinations, but e.g. erlang 17.3 from debian 8.4 is already ok.\n. @lemenkov I created this copy intentionally. While current example file is compatible with both .deb and .rpm based distros, I'm not sure it will always be the case. \n. I've built a package on 14.04, and then installed it both on 14.04 and 16.04. It worked everywhere, and on 16.04 it was started using systemd.\nJust tested it again, package built on 14.04 has\nDepends: erlang-nox (>= 1:16.b.3) | esl-erlang, adduser, logrotate, socat, init-system-helpers (>= 1.13~)\n. Yes, it's the way it should be.\n. Yes, will do it in a nearest future.\n. I don't know. Regexp filtering is not implemented yet and it can be useful for some use-cases. But we are no longer interested in this feature.\n. Should I just merge current stable into master? It also pushes some commits that are not authored by me.\n. Here it is - https://github.com/rabbitmq/rabbitmq-server/pull/884\n. Done.\n. Here is one of the lines that I want to change - https://github.com/rabbitmq/rabbitmq-server/blob/db33a5669e3b48f216dd7003a4d076bc3bd992c8/scripts/rabbitmq-server#L80\n. I thought about it overnight and did some experiments. I think that @localhost in prelaunch is completely safe. And with rabbit_cli.erl I decided to preserve as much of old behaviour as possible, even if it wasn't strictly necessary.\n. If it's OK, should I also make the same change to rabbitmq-cli?\n. Yes, I'll try do it in a nearest future.\n. Yes, this is still on my TODO list.. @michaelklishin Please don't merge it yet, I'm still testing.\n. Rebased and tested again. Now I'm happy with this patch.\n. LGTM\n. > I'm not sure which problem you solve by replacing eval \"mnesia:system_info(running).\" to \"lists:member('%s', rabbit_mnesia:cluster_nodes(running)).\". And how to test this.\nThis are actually equivalent. Functions return the same values and can be used interchangeably, and wrapping result in lists:member/2 allows us to get rid of grep call in script itself, so we can just compare with true or false using if statement.\n. Windows PATH variable uses ; as delimiter. So path can be splitted depending on os:type/0  return value.\n. I've already implemented it in https://github.com/binarin/rabbitmq-server/commit/9dd2c6cd2dfea0baca6f4cd264bdc4db8ae5f574\nJust haven't tested it yet.\n. Yes, I'm working on this. . Done. What is the preferred output in such case? 2 different lines for single plugin? Or something like:\n[E*] rabbitmq_management                  3.6.6 (3.6.7 after restart). Fixed. BTW, this can be re-implemented in a more clean and robust way using unix domain socket support in erlang 19. And even this will not be needed if https://github.com/systemd/systemd/issues/2739 gets fixed by the time of dropping 18 support (but I don't put much hope on it :trollface: ). It won't help, real problem is that we can't reliably determine systemd unit name, which gives us `-.slice`. The only real solution is to re-implement notifications using unix-domain socket support which is now available in Erlang for some time. That will make all problems completely go away, as in that case it will be rabbitmq process itself who reports to systemd.\n\nAnother option is to allow forcing unit name from environment or config, but I'm not sure that it'll be helpful - strange things happen with systemd inside containers.. @lukebakken All this mess with detecting systemd unit name and then querying its status is needed to detect whether systemd has successfully received our notification, so we can then safely close 'socat' process. But if notifications are sent from within the main process itself, it will be just fire and forget - systemd will be always able to detect who sent a message to it. And then NotifyAccess=all can be also removed from unit files.. This should be accompanied by a BIG WARNING somewhere in the documentation that external split-brain monitoring is 100% necessary. We were doing a lot of manual and automated testing, and the rate of failures was unacceptable even with high delays.. Even a single failure is too much for a production system ) And I've seen this several times with timeouts ranging from 30 to 60 seconds while developing. But maybe I'm just extremely unlucky.. Can it be the prelaunch process at https://github.com/rabbitmq/rabbitmq-server/blob/4ca58835cf227703a9fc8953d8d089571fbf6461/scripts/rabbitmq-server#L83 ? Looks like it don't get _ERL_ARGS but still tries to start a named node.. strictly_plugins is only in master, should I split this into 2 separate PRs?. I've updated the commit message.. I've cherry-picked fb17eed here.. Wow, unit= format is really a great idea!. I've understood it that way: when node senses that it has not enough RAM, it notifies all local queues to start pushing data to disk. So suppose we have 2 nodes in cluster that are actively paging data to disk (there is no alarms yet), and first node runs out of free space. All nodes are alarmed at that point and publishers are blocked. But it is safe to continue paging on the node without disk alarm, so we can still relieve RAM pressure there. It needs to be done anyway, and there is no reason to wait for clearing alarms on other nodes.\n. > as discussed on slack with @dumbbell what would happen when node A that doesn't have an alarm, receives a publish that has to be routed or replicated to node B (who has a disk alarm for instance). What to do here?\nAre we all discussing the same thing? ) \nThere is no significant change in alarm behaviour, here is the picture with explanation what the guard expression on following line changes:\n\n. perl is already used within the script, so I think it don't matter what to use\n. Yeah, you'll need https://github.com/rabbitmq/rabbitmq-common/pull/48\nAnd also you can't simply run ./scripts/rabbitmqctl from rabbitmq-server checkout - rabbit_common will be missing from code path (but on installed server it will be available through rabbit_common.ez archive).\n. Then every sequence of \"random\" numbers will be the same - and it's exactly the thing that we want to avoid, to reduce contention.\n. To remove warnings from script output.\nAs descriptions says:\n- \"Without double quotes it complains about misplaced '-eq' operator\" - rc_check is not always initialized\n- \"With quotes but without default value it sometimes complains about bad integer\" - empty string is still not a valid integer\n. What default value should be used? $OCF_SUCCESS? Isn't it conterintuitive, given that default value for rc is $OCF_ERR_GENERIC?\n. Currently there is only one possible reason - node_is_ko. EX_SOFTWARE (70) suits it the best.\nAnd is it possible to add separate reason with exit code EX_NOHOST (68) for cases when a node is unavailable?\n. It's possible to avoid duplicating variable name at a call site with a function body like this:\n``` shell\nlocal name=\"${1:?}\"\nlocal value\neval value=\\$$name\nif [ -z \"$value\" ]; then\nerror\n....\n``\n. Recently I'm very suspicious ofinfinity` timeouts. I don't remember who said something like: \n\nWhen I see a timeouf of inifinity, I suggest to replace it with some insane value like 30 years. And if the reply to this is that 30 years doesn't make sense, I reply that infinity also makes no sense.\n\nWith infinity timeout some problem with startup could go unnoticed for a long period of time. The question is what is the sane timeout value - in my opininon it's no more than several hours.\n. I saw that it terminated every time and never asked that question. This happens unintentionally due to controlling process death. I'm going to make it explicit.\n. Yes, you're right. While printed result will be the same, it's better to be explicit.\n. Fixed.\nOn Thu, Jun 2, 2016 at 7:38 PM, Dmitry Mescheryakov \nnotifications@github.com wrote:\n\nIn scripts/rabbitmq-server-ha.ocf\nhttps://github.com/rabbitmq/rabbitmq-server/pull/819#discussion_r65574639\n:\n\n@@ -1630,6 +1634,36 @@ get_monitor() {\n     return $rc\n }\n+ocf-update-attr() {\n-    local attr_name=\"${1:?}\"\n-    local attr_value=\"{2:?}\"\n\nSeems like you have forgotten '$' sign here\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/819/files/6eb69925f1a01368cc2e9d00a26c4d50b207921e#r65574639,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AALUY9sBAA7w-wZGj0RszoeXXszj79Uxks5qHwbzgaJpZM4IsvOD\n.\n. Fixed.\n. Actually 14.04 was working just fine, as dh-systemd in trusty is 1.14\nBut I'm not sure about debian oldstable.\n. Yes, here should be --local instead. Fixed.\n. Moved this block of documentation inside the <para>, so the dreaded usage.xsl will skip them when generating this list.\n. Fixed.\n. Because it's possible to implement better diagnostic messages inside rabbitmqctl. It's not implemented yet, but is definitely possible.\n. No, rabbitmq-env.conf is interpreted by rabbitmq scripts themselves, and variables here should be given without RABBITMQ_ prefix\n. Shouldn't (null) be transformed to $attr_default_value?\n. Shouldn't the epmd dependency be specified here using Requires (or at least Wants)?\n. Yes, that's correct. I'm checking here that given node is missing from the list of running nodes. \n. By approach do you mean using lists:member/2 or checking both running and partitioned states? \n\nI've added lists:member/2 because I needed to add another check and did it just for unification with new piece of code. It's slightly cleaner IMO, but it's not worth changing it everywhere. And for partition check it's just way more harder to implement it using grep.\n. As for cheek_need_join_to() it got me thinking. Is the check for running state is a correct check here? Or should we check list of known nodes instead?\n. No, plugins from other versions will be ignored, because we are moving towards fs root and find is called with -maxdepth 1. - there is now way we can reach sibling directories that way. Using predefined path is simplier, but it'll fail for other standard fs layouts, e.g.:\n/usr/local/lib/rabbitmq/lib/rabbitmq-3.6.6/plugins/\n/usr/local/lib/rabbitmq/plugins/\n. What about check like this - use /usr/lib/rabbitmq/plugins  additionally but only if we are installed below /usr/lib/rabbitmq/lib?\n. Yes, my mistake.. @michaelklishin I've started to think about a possible reason for my extreme unluckiness, and I believe I've found it. Random number should be taken from a  range specified in milliseconds right from the start, and not by multiplying by 1000 afterwards.. rabbit_misc:random/1 is already responsible for PRNG seeding (or non-seeding for erlang > 18.3). It was a problem for old random module, thus there is some code to work around this in stable. But modern rand  (i.e. rand:uniform/1) will initialize seed automatically and thus all this support code was dropped from rabbit_misc.. ",
    "alexethomas": "Very impressed with the prompt feedback, thank you all!\n@dumbbell Yes, the head_msg_timestamp in #vqstate is a relic of the previous version. Thank you! Removed and previous 2 commits squashed.\n@michaelklishin Very happy to benchmark this, would the following be acceptable:\n- Single persistent queue\n- Single sender (confirms)\n- Single consumer\n- Java\n- Time-to-send-N 1kB messages and time-to-receive N (separate phases)\n@simonmacmullen The tricky stuff! \nPer the best home for the head_msg_timestamp stat, I'm certainly happy to promote it to being an info key as you suggest. (I was originally wary of changing anything in rabbit_backing_queue but this was probably a misinterpretation of earlier advice). \nThanks particularly for the explanation of the likely status of the internal queue heads. Although it's tempting to only include ready messages (as a JMS QueueBrowser impl. would do) I think we can score a win here by including those pending acks. This catches the case where a consumer is in a reject loop and the message spends most of its time pending ack, or pending nack rather, and so remains mostly invisible to monitoring. (On another MQ this situation sometimes resulted in the head_msg_timestamp oscillating between the true head msg value and the next in the queue and so \"flapping\" the associated alert).\nSo if you agree I'd like to try peeking all of Q3, Q4, RPA, DPA and QPA heads and finding the oldest timestamp among them for the stat.\n. Implemented as per above except better tests still ToDo.\nNo attempt is made to page messages in as I'm assuming that this will happen \"soon enough\" for our purposes.\nOn this basis, no code being added to the send/consume paths, I suggest benchmarking focuses on the monitoring loop only.\n. @michaelklishin Is there a way to use multiple queues in PerfTest? I can see settings for multiple producers and consumers of a single queue.\n. @michaelklishin Thanks again, just done a couple of runs with that setting and it seems OK on the main path.\n./runjava.sh com.rabbitmq.examples.PerfTest -z 10 -y 100\nI also tried adding a timestamp to the Producer to see if that would make a difference here, and it does - a slowdown of about 6% - but the slowdown affects master and patched versions equally.\nmaster (commit bcb88a2e53ac7e2c63e5a5cbf7346c193992cf1b)\nrun 1\nsending rate avg: 230 msg/s\nrecving rate avg: 23041 msg/s\nrun 2\nsending rate avg: 225 msg/s\nrecving rate avg: 22544 msg/s\nsla_tracking_v2 (commit 6317a384b0ac5fc4490f8668bad74866f7d8c92a)\nrun 1\nsending rate avg: 233 msg/s\nrecving rate avg: 23396 msg/s\nrun 2\nsending rate avg: 243 msg/s\nrecving rate avg: 24376 msg/s\n. Changes made as per above. \nI've simply said in the doc that timestamps for messages paged out won't appear, and I'm assuming that they'll \"soon\" be paged in - is that valid?\n(Re q4 and q3 messages, q3 can't contain earlier messages than q4, I think, hence skipping q3 unless q4 is empty).\nAlso now re-indented as per Emacs.\n. Ah right, yes it is ambiguous, thanks. How about \"Timestamps of messages only appear when they are in the paged-in state.\"?\nThe idea of Java tests is tempting if they would be acceptable - any objections? I suppose direct Erlang tests could avoid going through the mgmt plugin but will be harder to wrangle.\n. OK I will have a look at doing Erlang tests first. \nIn principle it doesn't look too hard to extend the existing test_statistics fn but I'm wary of simply throwing more checks into the existing logic so think I should clone it. I can follow the same portmanteau style of multi-step test, so doing the existing 5 little tests in 1.\nI'll be travelling for the next few days so may not get around to this for a while but won't forget!\n. Hi Michael, yes despite the interval I'm actually still on it, just got diverted to rabbitmq/rabbitmq-auth-backend-ldap#4. I'll be checking in the new tests today or at latest tomorrow.\n. @michaelklishin have now replaced the Python tests with Erlang ones so ready for review. I also renamed the stat to head_message_timestamp in line with other stats that have 'message' in full. Happy to merge in current master if that helps.\n. Yes of course, will try that now.\n. Resolved.\n(The conflict was in rabbit_basic.erl where both my extract_timestamp fn and the new header fns had been added to the exported fns.)\n. Thanks Michael, I've reproduced this but only once! Clearly there's some subtlety about first surfacing the message properties - perhaps on a queue that's never had a consumer - that I need to investigate.\n. Strange! I've still only reproduced this once and can't make it do it again.\nI've re-added the old python test script in case this helps isolate it (currently that works too of course, at least for me). Any further pointers welcome.\nI noticed that your message bodies are empty so thought it might be something to do with that but it appears not.\nOf course, I can't rule out that I made a typo in the property name the first time (e.g. left a space after it) and that's caused it not to appear.\n. @michaelklishin no luck here - tried various permutations in the python script but couldn't reproduce. I'll ask @ash-lshift to have a go.\n. Terrific, thank you!\n. ",
    "ash-owl": "just as another data point, i've run the python script here and all the tests pass.\n. FWIW I started off by tinkering with rabbit_channel_interceptor: https://github.com/rabbitmq/rabbitmq-server/compare/master...lshift:interceptors?expand=1 with a view to doing this as a plugin. However, it looks like it would require significant changes to the interface. (On the other hand, I am only aware of one plugin which uses it.)\n. I haven't done any benchmarking yet but one reason to do it with a core change is to reduce the performance impact. Given that check_user_id_header is already on the hot path I am not expecting much impact.\nrabbit_channel_interceptor also already does a tiny bit of work on the hot path, so my feeling is that it is would be possible to rework it to do what I want with no impact. I have a little empirical data to support this feeling.\nAnyway, I'm not wedded to either of these approaches so I look forward to hearing any better ideas you have. \n. Well, the only reason I have at the moment to modify messages is to inject authentication information. The reason for injecting that is so that custom exchanges or consumers can make finer-grained authorization decisions.\n. I've updated the other branch and implemented this user_id thing as a plugin. Somehow my rabbit_channel_interceptor_new is slightly faster than the original when no interceptors are loaded. With a plugin on the hot path it is a bit slower, as expected.\nWhat do you think about the other branch? If you like it better than this one I will close this PR and open a PR for the other branch.\n. > I think your interceptor is faster because it caches in its state the modules that apply to a certain method as far as I understand the code,\nYes.\n\nwhile the current interceptor asks for this information every time, so that's a nice improvement.\nOn the other hand you seem to skip a couple of validations along the way:\nLike the one here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_channel_interceptor.erl#L82 where we check the register module actually exist.\nAnd here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_channel_interceptor.erl#L66 where we check that the method returned by the interceptor is the same as the one that was intercepted.\n\nTrue; I'll add those back if we go ahead with this approach.\n\nIf I understand this correctly your interceptor seems to be compatible with the existing one?\n\nThat was the intention. It would be simpler just to change the API though. Which would you prefer?\n\nIn any case if your interceptor needs to change the API, then we will need to upgrade the sharing plugin along the way, and also I think your changes would have to be pushed to 3.6.x version since its changed a current public API.\n\nThat would be fine by me.\n\nDo you mind elaborating what's the advantage of this new interceptor over the existing one (besides the speed due to caching)?\n\nThe goal of the new interceptor is to be able to support more tweaks via plugins rather than core changes. For example it might be possible to push the direct reply-to extension to a plugin.\nAgain, though, I am not wedded to this approach. Another way I have considered is to move message delivery into the exchange. (That is, rather than rabbit_exchange:route/2 returning a list of destinations and the channel delivering the message into the destinations, route returns nothing and does the delivery itself.) I think this might also be a cleaner way of doing 'side-effecty' routing like in https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/blob/master/src/rabbit_exchange_type_delayed_message.erl.\n. (Another example of 'side-effecty routing': https://github.com/rabbitmq/rabbitmq-management-exchange/blob/master/src/rabbit_exchange_type_management.erl)\n. > Exchanges have always been stateless routers in RabbitMQ so I think this is a big change. Also is the channel responsibility to know what happens with messages in order to issue replies to mandatory messages, or send back confirms, so I'm pretty sure this proposal won't advance.\nFair enough.\n\nBTW, what would be nice tho is to have a \"fixed\" current interceptor that does the caching you do in your new one. If the API doesn't change I think we could even include it in a future PATCH release\n\nYou mean similar to the backing_queue_module config entry?\n. I'll close this and continue with #214 .\n. It is proving quite tricky to continue to support the old rabbit_channel_interceptor API. What do you think about just changing it? (That is, removing the current rabbit_channel_interceptor and renaming rabbit_channel_interceptor_new to rabbit_channel_interceptor)\n. @videlalvaro It's probably ready for another look when you get a moment, thanks.\n. Sure.\n. Reimplementation of #206 as a plugin: https://github.com/lshift/rabbitmq-inject-user-id/blob/master/src/rabbitmq_inject_user_id.erl\nChanges for sharding plugin: https://github.com/rabbitmq/rabbitmq-sharding/pull/10\n. Warning fixed, spec changed, commits squashed.\n. @dumbbell: I have wrapped the lines.\n. Thanks all for your input.\n. Just tried again with a fresh install of erlang 18.3 & rabbit 3.6.3, same issue. I installed erlang to D:\\dir with spaces\\erl and rabbit to D:\\dir with spaces\\rabbit. (D: being an otherwise empty disk.)\nHere's what happens when I run rabbitmqctl status after removing the @echo off lines from the .bat files: https://gist.github.com/anonymous/10c22e58f46d7c9997f3c0528a458e98\nAs you can see, ERL_LIBS ends up being set to ;d:\\dir.\nI can probably arrange to give you access to the box if that would help.\n. we should probably have a header which defines the default worker pool name rather than repeating it here. see also this line in worker_pool.erl:\nerlang\n-define(DEFAULT_SERVER, ?MODULE).\n. It would currently fail; I will deal with that.\n. Yes, I am planning to check that no two interceptors apply to the same method.\n. The aim is to avoid applying 'cold' interceptors to 'hot' methods. If there are only 'cold' interceptors then we only ever apply fun (M, C) -> {M, C} end to 'hot' methods/content. This is possibly a premature optimization. I'll do another benchmark without it.\n. The comment in refresh_config is gone.\nrabbit_channel_interceptor can now handle the case where an interceptor plugin is unloaded while a channel is still open.\n. It now supports multiple interceptors so long as they don't overlap. (This should be the same as the original behaviour.)\n. I'll see about removing this now.\n. Added a comment for that.\n. Seems to make no difference. Removed.\n. Yes. That turned out to be necessary to support my original use case.\n. Yes. It was that or move the definition of ch to a header.\n. Sure.\n. Good point!\n. (They used to be because the 'hot' methods were handled separately.)\n. Good idea.\n. Agreed.\n. D'oh, you're right. Want me to fix it?\n. ",
    "patmanh": "@geekpete where did you land with this? I'm in a similar boat e.g. seeing flow control being enabled when I'd rather the messages go straight to RAM or disk - plenty of both left. Although, my server (r3.large on ec2) is maxed out on CPU. \nAny learnings you discovered would be appreciated!\n. as a follow up here - we tried playing with the flow control credit settings but couldn't find a configuration that did more good than harm to overall cluster performance when we were undergoing heavy publish loads without enough time to add more consumers (~millions of messages in <3 mins).\nin the end, we set all of our queues to \"lazy\" queues. despite all messages going to disk being slower (vs transient queues), our overall throughput is high enough (easily XX,XXX's of messages per second for each of publish/deliver/ack on decent ec2 hardware - especially after bumping our number of IOPS on EBs a little bit). in the end, this allows everything to perform reliably regardless if there are 0 messages in our queues or millions.\nwhen there is more guidance published re: flow control credits we'd be up to give things another try but we're very happy with lazy queues for now!\n. ",
    "videlalvaro": "@patmanh this is pretty cool, thanks fro the feedback\n. Perhaps the \"one second timeout\" could be specified either per element or globally, the later being the default one\n. I think it's fine as long as we output things as they happen. ie: we print as much stuff as we can during the specified timeout; if the timeout happens, then fine. Stop the whole thing\n. There's this already that can help a bit: https://github.com/hyperthunk/rabbitmq-lager I'm using it on a plugin that I'm building\n. I think whatever we fork we should actively try to incorporate upstream,\notherwise not sure the team should invest resources on maintaining a fork.\nOn Mon, Dec 14, 2015 at 6:44 PM Michael Klishin notifications@github.com\nwrote:\n\nWe can live without stack traces in the regular log but SASL log really\ncould benefit from having stack traces. I'm not a fan of using forks but if\nthat's what we have to do, I'm fine with it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/94#issuecomment-164506827\n.\n. There's no need to start/stop RabbitMQ to reproduce this issue. Just running rabbitmqctl will generate the crash dump\n. As explained in our internal bug system here https://bugzilla.rabbitmq.com/show_bug.cgi?id=26441#c3 we need to pass the boot file explicitly when using the standalone release in every place where we call erl.\n\nThe code here https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_nodes.erl#L207 doesn't pass this option, therefore Erlang crashes since it can't find the boot script.\nAdding: \"-boot\", \"/path/to/start_clean\" to the list of arguments passed to open_port solves the problem. I'll try to fix this once I'm back.\n. With this PR ./sbin/rabbitmqctl eval 'rabbit_misc:otp_release().' should output something like \"17.1\"\nTo build only the standalone release run this on a Mac:\nmake -f release.mk rabbitmq-server-mac-standalone-packaging VERSION=3.5.1 UNOFFICIAL_RELEASE=1 SKIP_EMULATOR_VERSION_CHECK=1\n. Note that we were having an erl_crash.dump because rabbit_nodes:ensure_epmd/0 wasn't finding the start.boot script. \nAccording to the docs -boot defaults to $ROOT/bin/start.boot therefore the proposed fix.\n. I think agreeing on the number of copies is per-se a group-membership/consensus problem best to be solved with something like Raft\n. I would suggest just upgrading Erlang and see if the issue persists. The user is running a fairly old version of Erlang as well.\n. Code-wise I'm fine with the implementation. Also whenever we modify basic.publish or related methods, we should be very wary about performance impacts (in this case I see we are resorting to the process dictionary as a cache, but since this is a global setting, we could even moving outside basic.publish and have this as another flag inside the channel state, passed during channel creation).\nSolution-wise I'm not sure this is the right thing to do. This seems more like avoiding flow control entirely, than solving the issue behind the original bug. Perhaps 26527 did the wrong thing to solve issue? I don't know, but overriding it altogether doesn't seem right.\nAlso once we put a new feature out, people will use it and depend on it, so if this happens to be the wrong thing to do, it will be hard to take it back.\nAbout putting this on 3.5.2, seems rushed, for the previous reasons and because .2 is a Patch release, so no new features should go in there. \n. Codewise is fine. About merging it into stable for the next patch release, I would wait for @dumbbell's opinion. I particularly don't agree.\n. Perhaps not moving it out of the #delivery record but at least make it explicit when rabbit_basic:delivery/4 by adding an extra param. Note: this will break plugins depending on that function (we can add a rabbit_basic:delivery/5 tho).\n. Something we didn't think about is what would happen if a user sets negative values here\u2026\n. Setting a negative value for InitialCredit will just disable flow control https://github.com/rabbitmq/rabbitmq-server/blob/master/src/credit_flow.erl#L90\n. Message size is limited to 2GB https://github.com/rabbitmq/rabbitmq-server/blob/master/include/rabbit.hrl#L137\nAFAIK there's no way to tune that value.\nIt's better to ask these kind of questions in our mailing list https://groups.google.com/forum/#!forum/rabbitmq-users\n. About the sort, I think lists:partition should take care of partitioning the list by duplicates, provided the list is sorted\n. Also see the comment related to duplicated fun code in queue_and_reason_matcher\n. Added some comments, after those, I think just the indentation ones remain\n. How does this interact with this code? https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit.erl#L885\n. AFAIK the file handle cache shouldn't refer to any rabbit_* code as it does here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/file_handle_cache.erl#L967\nThe comment here suggest that: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit.erl#L883\nping @dumbbell \n. ### About rabbit_alarm and error_logger\nrabbit_log has no boot step, yes, but rabbit_alarm calls into rabbit_log: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_alarm.erl#L212 which in turns call into error_logger https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_log.erl#L58\nThe problem is rabbit_error_logger sets itself as a report handler for error_logger: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_error_logger.erl#L45 but rabbit_error_logger depends on the exchange infrastructure from being ready. When the exchanges are ready, it means kernel_ready is there, that might be the reason why rabbit_alarm was depending on kernel_ready.\nAbout \"code purity\"\nAFAIK, all the files in our repo that don't start with rabbit_* should be independent from the rabbit source code, like libraries. Besides the \"purity\" details, I don't see why the rabbit_memory_monitor function required by file_handle_cache cannot be passed as a fun as these other things are already done in the start_fhc function.\n. I meant exchange infrastructure, since this: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_error_logger.erl#L108 needs a topic exchange to work.\nAccording to this: https://raw.githubusercontent.com/videlalvaro/rabbit-internals/master/images/boot_steps.png the exchanges are ready once kernel_ready has run.\nAbout the file with the rabbit_ prefix, I think I've read a comment from @simonmacmullen in our Slack channel about that, besides from the comment pointed above about FHC initialisation in rabbit.erl. \n. @dumbbell the purpose of having the FHC into the memory monitor is to not OOM rabbit at startup right?\nI think one thing is detecting the problem, another one is to tell the user there's a problem.\nAs a user I don't want my rabbit crashing (FHC part of the memory monitor feedback system). If it crashes, I want to know why (FHC/memory monitor logging that things went wrong)\n. About 2. I think that could be a separate PR, unless to fix this issue we have to change that part of the code, then it could be part of this bug\n. damn, did I fail again? I will resubmit :(\n. I've made the PR against master since it adds a new API, which should only be used from Erlang, but still not sure this should go on a PATCH release, feel free to rebase and make it part of a PATCH release.\nApart from that, there's no BC breaking changes with the code we had before.\n. I was thinking about rabbitmqctl and its commands. Why don't we add some sort of plug in system for rabbit_control_main that allows to plug in commands easily? Plugins could add their own commands and it could be easier to add features like these\n. Besides effort, AFAIK, per-spec the broker shouldn't modify messages. It should just pass messages as-is, without modifying them.\n. That DLX issue might have been solved in the latest RabbitMQ release\nOn Thu, Jun 4, 2015 at 12:32 PM cristian datculescu \nnotifications@github.com wrote:\n\nI would not enable tracing full time. my idea was to enable it just enough\nto troubleshoot the issue and disable it afterwards.\nRegarding dead-lettering and headers, we hit a funny issue in production.\nif a message is dead lettered more than 498 times, as i remember, we start\ngetting some errors. this is an edge case that we dealt with by limiting\nthe number of times we dead-letter, but it was a little tricky to discover\nthe reason.\nThanks for the answers.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/177#issuecomment-108833156\n.\n. It seems this issue trips Postgres as well http://php.net/manual/en/function.serialize.php#96504\n. We tried to reproduce the issue with php-amqplib, Bunny on ruby and Clojure for the JVM but we couldn't. Perhaps there's a problem on how your library encodes the data?\n\nI even added a print statement inside rabbitmq to see what message body I was receiving. For this string in ruby: \"123\\0|45\" rabbitmq prints as body: 123^@|45 which is correct. Our ruby process is able to consume the same message back.\n. :+1: \n. Output of:\nshell\n$ ./scripts/rabbitmqctl list_user_permissions\nerlang\nError: {'EXIT',\n           {function_clause,\n               [{rabbit_control_main,action,\n                    [list_user_permissions,rabbit@avidela,[],\n                     [{\"-q\",false},{\"-n\",rabbit@avidela}],\n                     #Fun<rabbit_control_main.9.21280602>],\n                    [{file,\"src/rabbit_control_main.erl\"},{line,177}]},\n                {rabbit_cli,main,3,[{file,\"src/rabbit_cli.erl\"},{line,66}]},\n                {init,start_it,1,[]},\n                {init,start_em,1,[]}]}}\n. What if you move the clear_queue_read_cache closer to the queue logic? Is it worth?\n. > Maybe I should limit it to the queues hosted on the node running the command, what do you think?\nWhat impact could this command have on a node that's heavily loaded?\n. I will push a new file with the changes. In Emacs with the Erlang mode just select the region and apply formatting, on other editors, I have no idea\n. The problem I see is that according to these docs: http://www.erlang.org/doc/apps/erts/time_correction.html#Dos_and_Donts yes, using erlang:now/0 is deprecated, but is not just a job of search/replace for os:timestamp/0, as far as I can understand.\nThen we have the problem of RabbitMQ supporting quite old Erlang versions, so we might need to do something like what's suggested on section 2.8 of that document.\n. I will prefer this to be a plugin as well.\nWhat's the performance impact of this change? Is it doing this check/set for every message passing the broker?\nThe channel interceptor purposely doesn't let users modify basic.publish along with other methods, because they are on the hot path and will impact performance negatively.\n. The methods singled out here https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_channel_interceptor.erl#L54 AFAIK were actually benchmarked by @rade back when we introduced this feature.\nYou could try offering your own channel_interceptor which could be plugged in the same way different backing queues are plugged in today, not sure that's a good thing to do, but could be a way to go.\n. BTW, what's the point/use case for modifying the message? I think we avoid that as much as possible\n. I think your interceptor is faster because it caches in its state the modules that apply to a certain method as far as I understand the code, while the current interceptor asks for this information every time, so that's a nice improvement.\nOn the other hand you seem to skip a couple of validations along the way:\nLike the one here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_channel_interceptor.erl#L82 where we check the register module actually exist. \nAnd here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_channel_interceptor.erl#L66 where we check that the method returned by the interceptor is the same as the one that was intercepted.\nIf I understand this correctly your interceptor seems to be compatible with the existing one? \nIn any case if your interceptor needs to change the API, then we will need to upgrade the sharing plugin along the way, and also I think your changes would have to be pushed to 3.6.x version since its changed a current public API.\nDo you mind elaborating what's the advantage of this new interceptor over the existing one (besides the speed due to caching)?\n. > Another way I have considered is to move message delivery into the exchange.\nExchanges have always been stateless routers in RabbitMQ so I think this is a big change. Also is the channel responsibility to know what happens with messages in order to issue replies to mandatory messages, or send back confirms, so I'm pretty sure this proposal won't advance.\nwrt the interceptor: I'm all for having a better API if you can provide examples of use cases of said API. Of course if we introduce a new API, then these changes go to 3.6.x\n. BTW, what would be nice tho is to have a \"fixed\" current interceptor that does the caching you do in your new one. If the API doesn't change I think we could even include it in a future PATCH release\n. So the interceptor has two APIs, the one used in the callbacks by other interceptors, and the one used by the channel.\nAFAIK, having an interceptor that caches applies_to and so on like yours only needs to modify the API used by the rabbit_channel, while at the same time improves performance. So I would welcome a change improving the current interceptor, since we can consider the current non-cached behaviour to be a bug.\n. I think it's fine to replace it, provided there's a migration path for the sharding plugin, and of course then this will go into 3.6.x\n. @ash-lshift just ping me when you think I should go for a new review of the code\n. Besides the small comments, can you provide a sample interceptor implementation or send a PR to the sharding plugin showing how this works?\n. pretty cool, I'll take a look again at the PR\n. Except for the small warning while compiling rabbit_channel.erl I think we are ready to merge.\nAlso take a look at interceptor init spec change\n. once you fix the last couple of things, do you think you could squash all the commits into one?\n. Thanks @ash-lshift \n. Thanks, I think I've found the reason, here: \nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_dead_letter.erl#L148\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_dead_letter.erl#L156\nWe assume that the <<\"count\">> field is long therefore the case clause failure.\n. Saw the fix. I think you can still check if the client sent one of the\nAMQP's \"integer types\". See here for an example\nhttps://github.com/rabbitmq/rabbitmq-delayed-message-exchange/blob/master/src/rabbit_delayed_message_utils.erl#L39\nwe do the same when we handle queue arguments inside rabbit_amqqueue I\nthink.\nOn Wed, Jul 8, 2015 at 1:53 PM Michael Klishin notifications@github.com\nwrote:\n\nClosed #216 https://github.com/rabbitmq/rabbitmq-server/issues/216 via\n508d76d\nhttps://github.com/rabbitmq/rabbitmq-server/commit/508d76d31984caf91977d2b0fdb095ed829981b3\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/216#event-350694132.\n. I'll just want to prevent data corruption/future crashes if the user sends\nsome stuff that's not even an int anymore. In that case if we do some math\nwith the count header, then we will have another crash.\nOn Wed, Jul 8, 2015 at 10:21 PM Michael Klishin notifications@github.com\nwrote:\n@videlalvaro https://github.com/videlalvaro that would require\nextracting validators into rabbit_misc or similar. We are under pressure\nto release 3.5.4 and the clusterer, so we'll do what you suggest for 3.5.5\n.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/216#issuecomment-119719379\n.\n. Reset to default?\nOn Wed, Jul 8, 2015 at 10:48 PM Michael Klishin notifications@github.com\nwrote:\nIf the user modifies x-death events then it counts as a user error. I'm\nnot sure what we can do if \"count\" is something other than a numerical\nvalue. Ignore it entirely?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/216#issuecomment-119726906\n.\n. A few comments:\n\nThe idea of the rabbit_queue_master_locator behaviour is good, but there are a few details, some of them just coding style related, so I hope are easy to fix.\n1) The behaviour expects a validate_policy/1, but then in all the implementations said callback just returns ok. I don't think that's needed, since there's the rabbit_policy_validator behaviour that covers validate_policy/1 already.\n2) If I understand correctly, the rabbit_queue_location_min_masters strategy counts bindings around nodes and vhosts and from there tries to infer which node is the best one. I don't think this is going to work since a RabbitMQ cluster replicates queue and exchange definitions, together with bindings to all nodes in the cluster, so whichever node you ask for said info (bindings, declared queues, etc), will and should give you the same answer. Also there's no need to rpc_call for said info, since each node has it locally.\n3) I see that rabbit_queue_master_location_misc exports some functions that are not used. If a function is exported, then that function becomes part of the module API, and it's hard to make said function private later since we don't know why it was exported in the first place. \n4) delay_ms/1 seems to do the same as Erlang's timer:sleep/1. Then policy/1 could have been used here perhaps? https://github.com/rabbitmq/rabbitmq-server/pull/217/files#diff-86750e2c3649c6ea97321ef96f5207d5R88 Please correct other cases that might be similar to these ones.\n5) Coding style: as a general rule we use the code indentation Emacs' Erlang mode provides by default. We use 80 chars as max line length. Apart from that we try to keep case clauses aligned, see: https://github.com/rabbitmq/rabbitmq-server/pull/217/files#diff-86750e2c3649c6ea97321ef96f5207d5R110 if you can't keep them aligned due to line length, then move them to the next line. We try to use the same alignment for function guards as well. See https://github.com/rabbitmq/rabbitmq-server/pull/217/files#diff-86750e2c3649c6ea97321ef96f5207d5R55 vs https://github.com/rabbitmq/rabbitmq-server/pull/217/files#diff-86750e2c3649c6ea97321ef96f5207d5R46 The later is what Emacs suggests. Then small things like binaries, which we usually define like this: <<\"queue-master-location\">> not like << \"queue-master-location\" >>. with spaces in between. We also don't use spaces before list elements on -export declarations.\n6) the rabbit_queue_master_location_misc module seems to have helper functions and do policy validation as well. Misc modules should just implement helpers, but not do any behaviour things. Please implement the policy validator on its own module.\n7) As discussed with @michaelklishin already, please use the same file headers we have in other files of the project. Author, creation date, and so on, are recorded on git already, plus Github will display you under the contributor's tab.\n. @Ayanda-D ping me once you think we should do a new review round\n. @Ayanda-D @michaelklishin as I asked when I was reviewing this PR, what's the point of counting bindings here:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_queue_location_min_masters.erl\nHow is that expected/supposed to work?\n. @Ayanda-D so this will filter out queues that have no bindings, the group them by node where they live, then count them?\n. @Ayanda-D go it. I'd assume \"filter-out\" means \"keeps out of the resulting list\" but anyway. Thanks for clarifying\n. Fixed in https://github.com/rabbitmq/rabbitmq-server/commit/4352e0c23fdc7ae87c8eb5e63204f29746bf64c1\n. Can you compare how disabling the FHC Buffer affects this: https://github.com/rabbitmq/rabbitmq-server/issues/227\nThe case is publishers with 20k + msgs/sec in the 1kb size, without consumers.\n. What needs to be optimized is the rabbit_queue_index:publish/6 case where rabbit_queue_index:maybe_flush_journal/2 is called at the end.\nThe problem here is: we have a use case where the consumers have been stopped for whatever the reason, and messages start accumulating in the queue, with lots of Journal entries in the index. At some point the Journal will be flushed to disk, and there performance will drop terribly. We are talking of going from 20k 1kb msgs/sec down to a few hundreds per second.\nYes, we can say: don't publish faster than you can consume, and that's fine, but on the few tests I ran on my mac with SSDs and 16gb of RAM, after two minutes or so, the broker performance drops significantly, because the Journals must be flushed. I think this has to be optimized. Yes, we can't escape flushing stuff that's in memory to disk, but we have to find a strategy that doesn't do this so eagerly and expects the world to stop while doing so.\nTo test this case I used the following:\n./runjava.sh com.rabbitmq.examples.PerfTest -x 2 -y 0 -u bench -s 1000 -z 360\nAs you can see the message size is 1kb (depending who you ask) so messages are embedded in the queue index. See  queue_index_embed_msgs_below.\nIt might be worth investigating the relation between the ingres rate and the queue_index_max_journal_entries config setting.\nAlso be aware that many of these Journal entries/Segments end up stored in the process dictionary when they are passed down to the FHC.\n. Thanks for taking the time to investigate this as well. For me this is a bit embarrassing to be honest :(\n. Some updates:\nAfter running a couple of fprof instances around flush_journal/1 the main culprit seems to be this line here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_queue_index.erl#L709 Which is totally understandable since with default config values, the function array:sparse_foldr/3 is iterating over 65k elements. Also there doesn't seem to be anything suspicious in entry_to_segment/3 so it's hard to say what to optimize there.\nWhat could be fixed/improved:\n- Parallelize the call to segment_fold, which each segment having it's on worker flushing contents to disk. See worker_pool.erl and gatherer.erl which are already used in the rabbit_queue_index module to load the index during startup/recovery. This approach could benefit from allowing users to set SEGMENT_ENTRY_COUNT as explained here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_queue_index.erl#L124\n- Completely avoid the array:sparse_foldr/3 bit by converting the journal entries to their disk format as they enter the index. This approach uses more memory of course, but avoids blocking the process while folding the whole journal. This approach reduces flush_journal times from 1.3 seconds to 0.3 seconds according to fprof. This change can be tested in branch https://github.com/rabbitmq/rabbitmq-server/tree/rabbitmq-server-227\n@gmr and everyone else, please whenever you have time, run some tests with the branch I've mention above. Note that my tests were done with the Java client and default rabbitmq configuration.\n. rabbitmq-test make full pass without problems.\n. The segment entries cache from the branch I've pushed might need to take this into account: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_queue_index.erl#L1035\nI have a possible solution already, using arrays for the cache instead of a list.\n. I think we need to preven the broker from bringing the whole system down, even more after something so trivial like running the disk monitor\n. \"Whole system down\" of course refers to the Erlang VM, not the OS.\nOn Wed, Jul 22, 2015 at 6:02 PM Michael Klishin notifications@github.com\nwrote:\n\nI'm not sure what that means.\nDisk monitor does not bring the VM down. We have plenty of users that are\nexamples of that.\nRabbitMQ cannot bring the OS down. The error in the log comes from the VM\nunable to allocate memory (known issue resolved in 17.x).\nThe user falsely associates the two errors.\n\nOn 22/7/2015, at 18:50, Alvaro Videla notifications@github.com wrote:\nI think we need to preven the broker from bringing the whole system\ndown, even more after something so trivial like running the disk monitor\n\u2014\nReply to this email directly or view it on GitHub.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/231#issuecomment-123771314\n.\n. Have you considered using the the Consistent Hash Exchange and then hashing your messages IDs, so they are distributed to queues. In that way one consumer will manage its own queues, and process those messages in order.\n. @michaelklishin pushed these packages already\n. @erylee the original bug report mentioned version 3.5.4. Can you confirm it only happens since that version of RabbitMQ?\n. Thanks!\n. Is this a Linux only breakage or we might see this in other platforms as well, say Solaris and so on?\n. After running some tests over the weekend I think it'd be wise to play with the values for IO_BATCH_SIZE from here https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_variable_queue.erl#L326 2048 is way too low in my tests.\n\nThe next thing to change then is CREDIT_DISC_BOUND https://github.com/rabbitmq/rabbitmq-server/blob/master/include/rabbit.hrl#L127 You can have a big IO_BATCH_SIZE all you want but then every 2000 msgs or so you get control_flow'ed. \nI think having my changes from #227 make queue_index_max_journal_entries less relevant than before, and make these two constants more relevant.\nIO_BATCH_SIZE is quite relevant since it affects when push_betas_to_deltas is called and with how many items.  push_betas_to_deltas then calls rabbit_queue_index:publish which does quite some work per message and might flush the journal (I have a 5GB fprof dump where I checked some things). \n. I think we are talking about slightly different points:\n- Reducing the journal size, might make sense, and since we don't fold anymore, flushing an 8k items journal should be pretty fast. So that's fine.\n- About IO_BATCH_SIZE this controls how many messages get persisted on a single run, I think an IO_BATCH_SIZE with a default of 2048, is VERY conservative. Just compare that with the amounts of elements in the journal.\n\nthe only significant throughput drops I observe with the publishers outpacing consumers test is from flow control, which is expected\n\nYes, this is expected, but again, having a default CREDIT_DISC_BOUND of 2000 messages is very conservative as well.\nSo I think we should consider all these constants together, since the original issue reported on the ML was that if there are no consumers, performance drops terribly. I know we are not going to fix the unbounded queues problem, but at least we can let the users tune RabbitMQ for their workload. I've managed to get decent results by tweaking these values and setting a max publisher rate per second.\n. Please thoroughly review this PR since bugs in it might result in message loss/message duplication/missing acks and so on\n. I see is not possible to have two people assigned to the ticket, but @dumbbell if you can please give it a review since changes here are quite critical\n. @javaforfun thanks for the feedback. I've created a separate issue: https://github.com/rabbitmq/rabbitmq-server/issues/290\n. See discussion https://groups.google.com/d/msg/rabbitmq-users/vj_9YGUfDgg/-_fx2BkqAwAJ and follow up messages\n. @michaelklishin the only thing we lose, kinda, is the ability to change this value at runtime. Not sure anyone is doing that. We can still change it with some rabbit eval magic\n. It's worth pointing out that the state assertion function a/1 and also the function fetch_from_q3/1 assume that q3 cannot be empty if messages have been paged to disk. ie: if Q3 is empty, then it assumes there are no more messages to load from disk. So as it is, we can't page every message to disk.\n. See comment above, closing as this is expected behaviour\n. Yup. I was just handling other fires :)\nOn Thu, Oct 1, 2015 at 4:46 AM Michael Klishin notifications@github.com\nwrote:\n\n@videlalvaro https://github.com/videlalvaro this is ready for another\nQA round.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/303#issuecomment-144601282\n.\n. Apart from the comments about duplicated code, and compiler warnings. The code seems to always do the same:\n\nerlang\n    [AggregatorPid ! {Ref, Msg} ||\n        Msg <- Generator],\n    AggregatorPid ! {Ref, finished},\n    ok.\nI think this has to be abstracted into a function that takes something like a message builder and a generator builder.\n. It's worth pointing out that this code first retrieves all the items to be listed, and then emits them one by one. So the difference with the current implementation on master is that while mapping, items are emitted, vs, emitting them at the end of the mapping. \nI thought this would be using something like Mnesia cursors and so on.\n@michaelklishin @dumbbell what do you think?\n. By cursor I meant a Mnesia cursor wherever we access data from mnesia. If there are 1000 queues, this  code will first query Mnesia for the 1000 queues, and then it will start providing output. \nI agree this implementation is an improvement over what we already have.\n. @Ayanda-D what @michaelklishin said.\nYou have a thing that produces a list, whether it's obtained from Mnesia or somewhere, then for each message on that list, you send a message to the AggregatorPid. Finally, once the iteration over the list is done, you send a finished message.\nI would like that to be refactored out. Otherwise the smallest change on the current code, would mean modifying the code everywhere. We want to avoid that.\n. Could you rebase your branch with the current master? Since right now it's quite hard to test due to error logger breakage since 18.1\n. @Ayanda-D I wasn't thinking about that. There you just move the sending of messages into a couple of functions, but you are not abstracting the repeated pattern.\nThe pattern I'm talking about is: \"Send a message to a pid, tagging it with a Ref, for every item of a list; once done, send a {Ref, finished} message to said Pid\".\nWhich I think can be implemented as something like:\nerlang\nemitting_map(Pid, Ref, Fun, List) ->\n    [AggregatorPid ! {Ref, Fun(Item)] ||\n        Item <- List],\n    Pid ! {Ref, finished},\n    ok.\nDepending on each case, will be what Fun is doing with each item that's received from the list.\nThen you need to see how to integrate the rabbit_misc:filter_exit_map into that emitting map, depending on each circumstance.\n. With the emitting_map this looks better, but we still have lots of duplicated code, like this for example:\n``` erlang\nlist_users() ->\n    [internal_user_filter(U) ||\n        U <- mnesia:dirty_match_object(rabbit_user, #internal_user{ = ''})].\nlist_users(Ref, AggregatorPid) ->\n    rabbit_control_main:emitting_map(\n      AggregatorPid, Ref,\n      fun(U) -> internal_user_filter(U) end,\n      mnesia:dirty_match_object(rabbit_user, #internal_user{ = ''})).\n```\nThe only difference here, besides the use of AggregatorPid, Ref is the mapping function. This problem happens all over the place on this PR.\n. Looking at one file of this code might seem right, looking at the whole PR, I'm not sure.\nJust in that case, tomorrow someone needs to change the Mnesia query, it has to be changed all over.\nOr this other case, from this same PR:\n``` erlang\nlist_permissions(Keys, QueryThunk) ->\n    [user_permission_filter(Keys, U) ||\n        %% TODO: use dirty ops instead\n        U <- rabbit_misc:execute_mnesia_transaction(QueryThunk)].\nlist_permissions(Keys, QueryThunk, Ref, AggregatorPid) ->\n    rabbit_control_main:emitting_map(\n      AggregatorPid, Ref, fun(U) -> user_permission_filter(Keys, U) end,\n      rabbit_misc:execute_mnesia_transaction(QueryThunk)).\n```\nThe TODO comment is gone on list_permissions/4. Is the TODO solved? Was it forgotten during code duplication? What happened?\nFor me, when I start seeing that I have to change the exact same code, for every file I need to modify on this PR, then I start getting suspicious that there has to be a better way.\n. I understand that Erlang doesn't support curry'ing and so on, so things might not be that easy to abstract away in this case, specially since we have AggregatorPid and Ref all over the place. I would prefer even resorting to an Erlang Macro instead of having code that's duplicate the same pattern all over\n. Closing and re-opening later once the erlang.mk changes have been addressed\n. Welp sorry. Maybe I pushed this one too quick.\nOn Wed, Sep 9, 2015 at 11:39 PM Michael Klishin notifications@github.com\nwrote:\n\nRunning make lite hangs because there's an unhandled exception in a queue\nprocess:\n=INFO REPORT==== 10-Sep-2015::00:30:52 ===\naccepting AMQP connection <0.17976.1> (127.0.0.1:51142 -> 127.0.0.1:5672)\n                       {<<\"x-message-ttl\">>,signedint,5000}],\n                               <0.17958.1>,[],[],[],undefined,[],[],live},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,1441834252222868,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,running}\n* Reason for termination ==\n* {function_clause,\n       [{gb_trees,is_defined,[9,0],[{file,\"gb_trees.erl\"},{line,221}]},\n        {rabbit_variable_queue,is_msg_in_pending_acks,2,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1095}]},\n        {rabbit_variable_queue,'-betas_from_index_entries/4-fun-0-',4,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1076}]},\n        {lists,foldr,3,[{file,\"lists.erl\"},{line,1275}]},\n        {rabbit_variable_queue,betas_from_index_entries,4,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1066}]},\n        {rabbit_variable_queue,maybe_deltas_to_betas,2,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,2016}]},\n        {rabbit_variable_queue,init,7,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1179}]},\n        {rabbit_priority_queue,init,3,\n            [{file,\"src/rabbit_priority_queue.erl\"},{line,146}]}]}\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/305#issuecomment-139053494\n.\n. Yes. That other issue fixed it.\nOn Wed, Jan 20, 2016 at 3:35 PM Michael Klishin notifications@github.com\nwrote:\nThe title isn't very specific, I'll assume this is addressed by #566\nhttps://github.com/rabbitmq/rabbitmq-server/pull/566.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/319#issuecomment-173222123\n.\n. I'd assume nobody is parsing that log message, so changing it for something more readable should help. At the same time we could add a disclaimer somewhere on the website that the \n\nerlang\n{handshake_error,opening,0,\n                 {amqp_error,access_refused,\n                             \"access to vhost 'skhskjdhfsdf' refused for user 'guest'\",\n                             'connection.open'}}\npart of log messages might change without notice. ie: the Erlang terms part,\n. My colleague @dumbbell is working on a fix for this.\n. This user had the same problem way before we released 3.5.5 so I'd guess 18.1 is the culprit here: http://stackoverflow.com/questions/32751556/startup-error-writing-to-to-var-log-rabbitmq-rabbitrab04-log\n. These changes to the error logger in erlang introduced a a new st record: https://github.com/erlang/otp/commit/003091a1fcc749a182505ef5675c763f71eacbb0\nSo instead of the tuple {Fd, File, PrevHandler} now the state is: \nerlang\n-record(st,\n    {fd,\n     filename,\n     prev_handler}).\nWhich breaks when RabbitMQ tries to call event handlers: https://github.com/erlang/otp/commit/003091a1fcc749a182505ef5675c763f71eacbb0#diff-d9a19ba08f5d2b60fadfc3aa1566b324R108 and functions that use the server state.\n. @michaelklishin @gmr I've just pushed branch rabbitmq-server-324 would you mind giving it a try? If it works we need to do some small performance improvements to handle the new depth introduced in 18.1. \n. This file https://github.com/erlang/otp/blob/maint/lib/sasl/src/sasl_report_file_h.erl was not changed on 18.1 but we have to monitor it just in case, since if they decide to perform similar changes, we will have the same problem.\n. This change here also breaks log rotation: https://github.com/erlang/otp/commit/ad7bb40d88acd0de5bdad9b64f8d8dd5c303fa48#diff-d9a19ba08f5d2b60fadfc3aa1566b324L97\nBefore terminate was returning an empty list [], now it returns ok assuming file:close(Fd) succeeded.\n. @moshezvi perhaps as on the Erlang mailing list/irc channel\n. Note that until this PR is merged, RabbitMQ's Erlang unit tests will fail in 18.1\n. @michaelklishin updated the PR\n. ready for another round @michaelklishin \n. For 1 million messages, the current implementation takes about 60 seconds to synchronize queues. During this time the queue is unresponsive.\nAn initial proof of concept shows that by using a batch size of 20000 msgs, 1 million messages can be synchronized in 6 seconds. Of course tunning batch size will depend on message size and net_ticktime\n. any reason why you run RabbitMQ this way on Mac, and not use the standalone package for Mac or homebrew?\n. Please ask these kind of questions on the rabbitmq-users mailing list https://groups.google.com/forum/#!forum/rabbitmq-users\n. This has been answered many times on the mailing list.\nFirst you start the node as sudo, creating an .erlang.cookie for the user root: sudo rabbitmq-server -detached\nThen you try to contact the server with your default user: ./rabbitmqctl status that won't work since users don't share .erlang.cookies\n. it could be improved when many rejects are sent at the same time, but I'm not sure the gains will be too big\n. To test mirror sync you can use these commands:\nmake -j test FILTER=eager_sync and make -j test FILTER=sync_detection\n. @michaelklishin ready for another round. bugs discovered/fixed were:\n- improper handling of batch_publish_delivered accumulators\n- re-ordering of messages due to how publish/publish_delivered msgs were partitioned\n- when there were delivered messages in the batch, then not all messages were sync'ed\n- the test eager_sync_cancel would always fail, when SyncBatchSize > ?MSG_COUNT, since the sync would finish before the test is able to cancel it\n. Initial tests with ./runjava.sh com.rabbitmq.examples.PerfTest -p -x 1 -y 1 -s 1000 -q 1000 -f persistent show that lazy queues have similar performance as default queues when messages are published as persistent and there are consumers retrieving message from the queue.\nDuring the test one can dynamically change policies by calling:\n./scripts/rabbitmqctl set_policy lazy \"\" '{\"queue-mode\":\"lazy\"}' \nor\n./scripts/rabbitmqctl set_policy lazy \"\" '{\"queue-mode\":\"default\"}'\n. Comparing RAM usage between queue modes without consumers shows the following:\nlazy queue:    7'805'517 msgs, 75 MB of RAM for the whole node, 19 MB used by the queue.\ndefault queue: 4'440'778 msgs, 1.6 GB of RAM for the whole node, 762 mb of RAM used by the queue.\nIn both cases messages were published by running the following during 360 seconds:\n./runjava.sh com.rabbitmq.examples.PerfTest -e sdmp -u sdmp_queue -f persistent -s 1000 -x1 -y0\nThe default queue accepted less messages because since there are no consumers, from time to time it has to page messages to disk.\n. As you can see here, basic.reject in AMQP doesn't support a parameter where you could set a reason.\n. As I remember the PR, the empty atom that this thing returns is what's expected by the management plugin\n. It's worth noting that the state of the server when crashing has this value: {true,{shutdown,ring_shutdown}}\nThe problem seems to be that the current server cannot find itself in the view dictionary, which seems natural if the whole ring is shutting down.\nPerhaps this case should be handled when dealing with 'DOWN' messages?\nThe following patch seems to solve the issue:\ngm.txt\n. Assigning @dumbbell since this seems to be related to the changes introduces here: https://github.com/rabbitmq/rabbitmq-server/commit/ed2766564f5e2cc23e62e139d435e2ea511d12dd#diff-2f42b9f7c62fc89a03a3b4acb1a1a2f0\n. @dumbbell by running the test on the bug description\n. Yes. Arguments have the x- prefix because they are protocol extensions.\nPolicies don't use that.\nOn Thu, Oct 22, 2015 at 12:47 PM Michael Klishin notifications@github.com\nwrote:\n\n@videlalvaro https://github.com/videlalvaro so we'd still keep the\nqueue.declare arguments key prefixed with an x-? So far I've updated the\ndocs to only mention setting locator via policies, to not have to explain\nthis difference.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/369#issuecomment-150177780\n.\n. @michaelklishin once I'm able to run the tests I will QA this and other PRs\n. Fixed via https://github.com/rabbitmq/rabbitmq-server/pull/390 and related tickets\n. @michaelklishin why would this go into 3.5.7 considering that it adds new things to CTL? Why not wait for 3.6.0 which anyway is happening soon?\n. Because there are version numbers for a reason? Other wise what's the meaning of them if we don't respect them? We could use just one increasing integer\u2026\n\nAlso, what's the rush?\n. This should be merged together with:\nhttps://github.com/rabbitmq/rabbitmq-website/pull/99 and https://github.com/rabbitmq/rabbitmq-test/pull/5 https://github.com/rabbitmq/rabbitmq-common/pull/14\n. Can I assume that a memory alarm would cause the same symptoms?\n. > Observe publisher connections leaving blocked state despite 1 node still in a resource alarm state\nSo what happens if we unblock publishers and then they publish a message that has to be replicated to the node that still has the resource alarm?\n. During boot we should stop the node if the queues cannot be recovered. If we know the amount of free file handles the node has, then we know in advance if this function will succeed or not:\nhttps://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_amqqueue.erl#L253\nWe can count the number of queues that need recovering, if that value exceeds the amount of available file handles, then I think there's no point in continuing booting the server.\nAnother approach could be here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_queue_index.erl#L285 to ask init_clean and init_dirty to close the journal handle once they are done recovering the queue.\nIn that way we could at least boot, then the QI will take care of opening the Journal Handle on demand (this is handled automatically already). The problem still remains of what happens when the user has more queues than file handles. We could for example release idle file handles after N seconds.\nWhat's for me is clear is that we can't boot a node without recovering all it's queues.\n. @michaelklishin @dumbbell any decision on this?\n. @dumbbell I think both solutions are rather easy to implement. The Queue Index opens the Journal Handle on demand anyway, so that wouldn't be a problem.\nNow, if the user starts using all these queues at the same time (via publishes/acks or whatever) then at some point it will still run out of file descriptors.\nStopping the server from booting and logging a clear error message will at least let the user fix the problem immediately\n. @michaelklishin should I assign this to myself then, and bring up a PR?\n. @michaelklishin @dumbbell what should we do if the FHC reports and unknown ulimit like here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/file_handle_cache.erl#L1523 ?\nThe init function there handles the unknown case like this: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/file_handle_cache.erl#L1089, assuming 1024 handles.\nShall I replicated what's done there? Assuming 1024 handles might still be unsafe\n. @michaelklishin OK, so at least now we know that RabbitMQ might hang like this during startup, so in the worst case somebody will come with this problem, but now we know what's going on\n. So this is not so simple: it's not just about counting the number of queues and comparing it to the number of available FHs. Queues that don't have recovery terms won't consume file handles, but eventually as the broker is used later on, they will. \nSafe solution: still alert the user when number_of_queues > number_of_file_handles, and stop booting.\nUnsafe solution: if number_of_queues_with_recovery terms > number_of_file_handles, warning and stop booting.\nWhich one you prefer @michaelklishin @dumbbell ?\n. Thanks for investigating, I think the spawn process should send the errors back, and probably also be monitored just in case.\n. @michaelklishin since this also change the BQ API, it should ship with 3.6.0\nRelevant tests:\nbash\nmake tests FILTER=rabbit_priority_queue_test\nmake tests FILTER=eager_sync\n. @gigablah is correct. \n. Merged, thanks!\n. I think the problem might be related to this line: https://github.com/rabbitmq/rabbitmq-server/blob/master/scripts/rabbitmq-server#L164\nWe should store in a variable the last exit code and probably return that once the wait exits\n. Here we track how many messages were delivered to queues: https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_channel.erl#L1849\nAnd here we already check the message size: https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_channel.erl#L925\nA channel belongs to a vhost, so tracking this information shouldn't be that intrusive.\n. So, this tracking I've pointed out is not for individual queues, but for messages -> exchanges -> [queues], but it's scoped per channel, so as you say, not quite useful, so I agree that having a global counter somewhere would hurt performance.\n. Is this going to query the permissions DB for every message passing via the topic exchange? If this is the case, how do you prevent these queries if the user doesn't have a need for this feature?\n. You probably want to take a look at what this plugin is doing: https://github.com/airboxlab/rabbitmq-topic-authorization\n. > While it sounds ridiculous, we've observed such a crash on a production server )\nThanks for reporting this @binarin \n. @michaelklishin see the line note I've left. The credit flow docs and the variable queue docs should hint at why that might be wrong.\nWhat to keep in mind when reviewing this:\nlazy-queues start with every message paged to disk, this can happen when:\n- The queue is declared, so every new message will go straight to disk.\n- The queue is converted from default mode to lazy mode, in this case every message should be paged to disk. I'm not convinced this patch achieves that.\nA lazy queue should only have messages in memory if at any point in its life a consumer fetched messages from it. In that case, around 16k messages will be loaded from disk. See SEGMENT_ENTRY_COUNT. That's why I've told @dumbbell that I don't think it make sense to run those slow tests this patch is trying to address with a duration = 0, because that's effectively asking the queue to load 16k msgs from disk, flush them, load them, flush them, etc. This is a similar problem to the queue length = 20 reported by some other user, which doesn't make any sense for a lazy-queue. duration = 0 does't affect a default queue, because a default queue never pages every message to disk, as seen in the functions @dumbbell is modifying.\n. the key name here doesn't match the name in the config ^^\n. I wouldn't have so many options, either flow/noflow or true/false\n. can you use SELF instead of THIS. THIS sounds too OOP'ish\n. I think you are missing a zero.\n. do you need to test for is_list here when you are matching already for a list?\n. Do you need the guard here? In this line https://github.com/rabbitmq/rabbitmq-server/pull/153/files#diff-e2043d9f095f7dc4abc052dd9fae7599R109 the list passed there must have at least one element for the case to match.\n. Perhaps this could be refactored to not need to duplicate the fun body in both clauses?\nAlso I hate to make these comments but the indentation here doesn't match what Emacs would give, which I think is what we use all over the place.\n. As the other comment down below, here you can push the fun to the next line and then just use Emacs indent like we do here for example https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_msg_store.erl#L2019\n. here fun (Info, Acc) -> could be fun ({table, Info}, Acc) -> and I think you don't need the list comprehension above\n. You could make maybe_append_to_event_group return false -> [{table, Info} | Acc] to remove this comprehension as well.\n. lists:partition/2 could return an empty list which could cause a exception error: no case clause matching [] in the case there\n. I think you could pass a sorted list of Infos to the foldl, so every time you call lists:partition/2 you can discard a whole bunch of infos, if I understand this correctly, which would remove the need for maybe_append_to_event_group which goes over the Acc all the time.\n. You could sort the list inside group_by_queue_and_reason before calling the fold. I think currently by calling lists:any all the time the complexity of this function could go quite up since you are always traversing the Accumulator for every iteration of the foldl. Sorting the input before calling foldl removes that\n. are there cases where InitialVal might be less than one? If yes, the caller will get an undef error\n. perhaps since you use a set, why not use the set as the actual accumulator, and then do http://erlang.org/doc/man/sets.html#to_list-1\nThis is more of a taste comment than a requirement. \n. Then what's the point of having those InitialVal guards?\nOn Mon, May 11, 2015 at 12:03 PM Michael Klishin notifications@github.com\nwrote:\n\nIn src/rabbit_dead_letter.erl\nhttps://github.com/rabbitmq/rabbitmq-server/pull/153#discussion_r30026279\n:\n\nend,\n         rabbit_misc:set_table_value(Headers, <<\"x-death\">>, array,\n-              [{table, rabbit_misc:sort_field_table(Info1)} | Others])\n-                [{table, rabbit_misc:sort_field_table(Info1)} | Others])\n-    end.\n  +\n  +ensure_xdeath_event_count({table, Info}, InitialVal) when InitialVal >= 1 ->\n\nNo. If we have an event, the number of times it happened is >= 1.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/153/files#r30026279.\n. how does moving this earlier in the boot process interacts with the use of rabbit_log here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_alarm.erl#L212 ?\n. why is there the \"infinity\" case?\n. Can you move the lines 201-210 to an inform_timeout function?\n. The problem is that the Mod we pass to the first call to find_prioritisers/1 is not the same Mod passed in the second call. The bug was precisely there, not calling find_prioritisers/1 for the new Mod that was swapped by the gen_server2 callback.\n\nrabbit_prequeue starts as the gen_server2 callback being Mod, then Mod:init/1 is called, which returns a new module, in our case it could be rabbit_amqqueue_process. The module rabbit_amqqueue_process will become the new gen_server2, so we have to call find_prioritisers/1 again\n. wouldn't the empty string be a problem with most commands actually? I think for that case we get an error like this:\nListing permissions for user [] ...\nError: no_such_user:\nwhich at least tells me something about the problem, not like function_clause before\n. Just a pet peeve, but can you make the empty list case the first function clause? It seems to be the style favoured in this file as well: https://github.com/rabbitmq/rabbitmq-server/blob/rabbitmq-server-196/src/file_handle_cache.erl#L763\nSame comment for the rest of the changes\n. Why do you need to run this code in the context of the backing queue? clear_process_read_cache/0 will call put in the context of the backing queue Pid if I'm not mistaken, so that process dictionary will be used.\n. Let me guess\u2026\u00a0a quick glance at the code tells the file_handle_cache module is the one doing the put/2 and what not, but it's being called from the context of the queue process\u2026\n. Do we need to clear the cache on the slave Pids? Wouldn't that clear caches on a separate node?\n. can you apply Emacs Erlang's formatting to all these changes?\n. processo small \"spanish\" typo :-)\n. This call here seems to come from rabbit_trace so I think is not related to the interceptor, but the interceptor state only needs to be updated when plugins are enabled/disabled, since we don't stop the broker for that anymore. \nWhat would happen if a channel loaded an interceptor provided by a plugin, but later that interceptor is gone, since the plugin is removed, but the channel is still open?\n. this assumes only one interceptor is defined or?\n. I don't understand the point of this partitioning \n. Mod:init is an interceptor callback right? What is expected of said function? What state does it tracks?\n. We usually add specs to behaviours, see: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_exchange_type.erl#L19\n. oops, sorry, I was looking at the wrong file. Please disregard my previous comment\n. this is not used, besides it's the same as http://www.erlang.org/doc/man/timer.html#sleep-1\n. we don't use author tags in our code, nor the Created one bellow, or the @doc one like bellow. We do have doc headers, see gm.erl for an example.\nAuthor and Date are stored on git anyway.\n. why do we have this callback if the code seems to return ok on all instances?\n. Is this an assertion? Otherwise this might crash since Node1 on 283 could not match the value on line 278\n. Perhaps there's no need for a Temp variable and this can just return {ok, node()}\n. key is the routing key used for creating the binding, it's not the queue name.\nerlang\n-record(binding, {\n    source, %% exchange where the binding was issued\n    key, %% routing key for the binding\n    destination, %% queue or exchange destination of the binding, see also e2e RabbitMQ extension\n    args = [] %% args used during binding creation\n}).\n. on the channel interceptor we also check this: code:which(M) =/= non_existing to make sure the module actually exists. Keep in mind plugins might add their own queue locator modules, which might contain errors\n. wouldn't it be better if these get_location_* functions return the module that implements the strategy, and then CB:queue_master_location/1 is called in just one place?\n. I think the key to understand that is the comment here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_amqqueue.erl#L262 \nThere are cases where the Master node might change. I suggest to see what's going on on the current version of rabbit vs. this branch when mirroring is in place and the ha-params policy is in place\n. what are these two getters used for? I'd assume on a future interceptor?\n. Now interceptors are allowed to modify the content sent by the user, right?\n. what's the point of all these function clauses if they all end up in intercept_in1/3\n. since now the interceptor modifies the Content as well, don't we need to validate that's the returned value is correct as well? See rabbit_types:content() and rabbit_channel:do/3 spec.\n. So instead of refreshing the state cache we just handle the module going away right? I think it's fine since channels might not be so long-lived to have this be a frequent issue.\n. Can this be made tail recursive/refactored? I really have a hard time wrapping my head around what's going on on this function\n. what if instead of any we make another type called interceptor_state or something like that so in the next callback intercept/3 we specify we expect said state, it'd be better as documentation in the future.\n. Do you think there could be a reason for an interceptor to update its state as well? by returning {Method2, Content1, NewIState} for example?\n. OK, considering that now IState is proplist of {Mod, State}, then I don't think my previous comments make sense, without overcomplicating the interceptor\n. in both these getters State is not used and it gives a Warning while compiling\n. expand_shortcuts might throw an error, so this here makes the channel crash when no queue name is specified for things like basic.consume or queue.bind https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_channel.erl#L745\n. I'm on it :)\n. I don't think we need to remove this comment\n. I'll change it to queue_index_embed_msgs_below\n. we need a usable index state, since after BQ:purge/1 the index state might keep being used by future publishes and so on.\n. This function got split in 2. The part that collects pending acks, and the part that handles the removal of those pending acks/msgs depending on the KeepPersistent flag.\n. purge_pending_ack_delete_and_terminate/1 shoud work like the old purge_pending_ack/2 when KeepPersistent = false, but instead of acking/delivering messages to the index, we just call rabbit_queue_index:delete_and_terminate/1\n. What I did here was to try to simplify remove/3 by having two function bodies, one for when AckRequired=true the other one for when it's false.\n. backing_queue_module has the full module word, so maybe use that here as well\n. I think it would be better if hash_password would take the HashingMod variable, for referential transparency and easier testing.\n. can this be extracted into a function to clarify a bit code intention. After looking at it I do understand that you are matching ConsumerInfoKeys with their values, but still, I can't get that info at first sight without stoping at inspecting the code.\n. don't forget to keep spaces between variables and the = sign\n. wouldn't this call here affect other nodes net_ticktime?\n. Please fix variable and = spacing here and in other places of the PR\n. please remove this line\n. is there a reason why we have all these spawn_link calls, and also the new call/4 down below? Is call/4 meant to be used everywhere? \nAlso there's seems to be a pattern here of spawn_link then wait_for_info_messages. Perhaps this can be refactored?\n. is there a reason why here there's a specialized function for the mapping, while in other places the mapping is done inline?\n. could you move the tests to their own file?\n. Got it. Can you still refactor the rpc_calls to not have duplicated code?\n. For me it's kinda weird that everywhere there's the spawn_link, then wait_for_info_messages. This seems like a pattern that could be extracted into its own function. Also spawn_link always gets rabbit_cli, rpc_call, with the only difference being the arguments. wait_for_info_messages seems to always get the same arguments, except the third one, so I would expect a function that takes 2 arguments and abstracts all this spawn_link/wait_for_info_messages away.\n. can you also print here the command that was being executed?\n. About the need to call list_to_binary_utf8/1 in some cases, and in some cases not, can this be refactored as well? There seem to be only 3 cases where this special call that handles list_to_binary_utf8/1 is required, so I don't think it would be difficult to use call/4 in a way that handles both cases.\n. it's \"we\"\n. what's the point of this timer:sleep here?\nIf this is trying to wait until the connection is established, then what would happen if connection establishment takes more than 100ms? \n. Why is this code duplicating the code from here? https://github.com/rabbitmq/rabbitmq-server/blob/rabbitmq-server-62/src/rabbit_amqqueue.erl#L619\n. Compiler warning:\nsrc/rabbit_amqqueue.erl:596: Warning: the result of the expression is ignored (suppress the warning by assigning the expression to the _ variable)\n. Compiler warning:\nsrc/rabbit_control_main.erl:766: Warning: function call/3 is unused\n. Before I was doing a code review just looking at the diff here on Github. Inspecting the full code tells me all the \n\"async\" functions just duplicated code from the module. Is my assessment correct?\nhttps://github.com/rabbitmq/rabbitmq-server/blob/rabbitmq-server-62/src/rabbit_auth_backend_internal.erl#L324\n. Why? Who was going to refactor the duplicated code afterwards?\n. I feel I need to add this here\n\n. This is clearly wrong, since it will report 19.0 as being lower than 18.1\n. mostly from the docs \n\nfoldl/3 is tail recursive and would usually be preferred to foldr/3.\n\nhttp://erlang.org/doc/man/lists.html#foldl-3\n. Aslo AFAIK list reverse is a BIF\n. I'm fine to change this to foldr if you think it's required\n. Yes, process as a verb. Your explanation is clearer tho.\n. You are right. I think this constant came to life on my first POCs, but is not required anymore. Probably not used in the code.\n\nOn Oct 10, 2015, at 3:28 PM, Michael Klishin notifications@github.com wrote:\nIn src/rabbit_mirror_queue_misc.erl:\n\n[policy_validator, <<\"ha-promote-on-shutdown\">>, ?MODULE]}},\n  {requires, rabbit_registry},\n  {enables, recovery}]}).\n+%% For compatibility with versions that don't support sync batching.\n+-define(DEFAULT_BATCH_SIZE, 1).\nAre we talking about pre-3.6.0 versions here? Mixed 3.6.0/3.5.x clusters are not allowed, so we can use a different default.\n\n\u2014\nReply to this email directly or view it on GitHub.\n. Looking at the code I remembered the original purpose. The idea is for it to be 1, so you either use non-batch sync, or batched sync in case the policy has been set. The logic that assumes policy batch size either 1 or > 1 is here: https://github.com/rabbitmq/rabbitmq-server/blob/rabbitmq-server-336/src/rabbit_mirror_queue_sync.erl#L212\n. @michaelklishin what errors do you get?\n. The problem is finding the right batch size. 16k for big messages is too much. It can even cause a network partition (reason why we have max msg size 2Gb in the first place). Finding the right value depends on workload (this is explained on the related rabbitmq-website PR), but if we provide a default, I think it has to be lower.\n. shouldn't this be mapping list_down instead?\n. how is this function a filter? \n. can we have call/4 and call/5? In this way, callers to this function don't need to be seeing this true/false parameters all over.\n\nWith the code as it is, later, once I had forgotten what call/5 five arguments are doing, I will have to always look at what that true/false flag is. \n. :+1: \n. please don't import functions that are not going the be used later: default_options/0 and expand_options/2 are not used.\n. please rename to rabbit_control_main_timeout_tests or something appropriate. There are many things that can be interpreted as \"timeouts\" inside the broker.\n. this code only lists one channel\n. Can you make the tests more thorough? Right now this is just testing that info_action runs and that's all. There's a bug right now were channels info_all/3 only returns the first channel on the list and nothing else. So I would expect first that the code above creates more than one channel, more than one queue, and so on. \nThen instead of asserting that the function info_action returns OK, I would assert that the channels I created above are actually listed when I call list_channels, just to name an example. The test should be automatic, I shouldn't need to read the screen in order to see if it passed.\nPerhaps you have to call this function instead: https://github.com/rabbitmq/rabbitmq-server/pull/303/files#diff-0791814e0c4904b21e083a7378194669R773 in order to make your tests become the aggregator Pid or see how to mock that.\n. info_all/2 calls map/2 which in turns calls rabbit_misc:filter_exit_map/2. Why is your code not replicating that behaviour?\n. this code only lists one connection, same problem as with channels\n. why is this code not ordering the policies like list_formatted/1? I understand that it has to emit things as they come, but at the same time it has to wait for rabbit_runtime_parameters:list/2 to produce the whole results, so why not order them as well?\n. Looks like it, thanks @javiereguiluz \n. as a nitpick, we don't use these kind of @doc tags\n. We have to take into account credit_flow from channels to slaves, back to channels and what would happen if we change things here\n. as discussed on slack with @dumbbell what would happen when node A that doesn't have an alarm, receives a publish that has to be routed or replicated to node B (who has a disk alarm for instance). What to do here?\n. See https://github.com/erlang/otp/blob/1523be48ab4071b158412f4b06fe9c8d6ba3e73c/lib/kernel/src/pg2.erl#L263\n. it should be automatically\n. It should be SASL as in the rest of the file\n. it should be RabbitMQ as in the rest of the file\n. what's the purpose of the Handlers variables?  Why not just use SaslHandlers?\n. Lines should be of 80 chars max\n. I think this problem deserves a deeper solution. For example by creating a behaviour that's related to the rabbit_registry. Also not sure taking the common registered and unregistered names is a good idea, since many modules might have those functions and not be related to the rabbit registry.\n@michaelklishin @dumbbell you should provide input here\n. why do you care about the IoBatchSize?\n. ",
    "carlhoerberg": "Sorry, wrong project\n. We at CloudAMQP currently have 2 customers who run HiPE in production. \n1) uptime 150 days. RabbitMQ 3.3.4, Erlang 17.1. avg 1msg/s, ~3 conns, amqp, 1x t2.micro, single node\n2) uptime 60 days. RabbitMQ 3.4.2, Erlang 17.4. avg 100msgs/s, ~400 conns, amqps, 2x c3.large, clustered\nSo very few of our users have enable HiPE, but the uptime on the once that have looks good and we haven't got a support email regarding HiPE since May last year (before we'd enabled Erlang 17).  \nDid some benchmarks from an aws c3.2xlarge to a t2.micro instance. RabbitMQ 3.4.4 and Erlang 17.4.\n```\nWithout HiPE\nStartup time: 2 seconds\namqp, 1 producer, 1 consumer, autoack\nmoving avg 1min 16,551 msgs/s\namqps, 1 producer, 1 consumer, autoack\nmoving avg 1min 5,270 msgs/s\namqp, 1 producer, 1 consumer, manual ack\nmoving avg 1min pub 8,232 msgs/s\nmoving avg 1min sub 8,089 msgs/s\n```\n```\nWith HiPE\nStartup time: 2 minutes\namqp, 1 producer, 1 consumer, autoack\nmoving avg 1min 27,817 msgs/s (70% faster)\namqps, 1 producer, 1 consumer, autoack\nmoving avg 1min 6,203 msgs/s (18% faster) \namqp, 1 producer, 1 consumer, manual ack\nmoving avg 1min pub 13,219 msgs/s (61% faster)\nmoving avg 1min sub 12,421 msgs/s (53% faster)\n```\nAMQPS TLS connection properties\nProtocol Version    tlsv1\nKey Exchange Algorithm  rsa\nCipher Algorithm    aes_256_cbc (no difference with aes_128_cbc)\nHash Algorithm      sha\nSummary: Get an up to 70% speed increase with HiPE for a 2min startup time penalty. \n. Are you thinking that it would work like a proxy for CoAP servers or? \n. Is the hash automatically updated on successful login? If the md5 hash matches you could hash the plain-text password with the new algorithm and store it. \n. This can already be done with the rabbitmq-auth-backend-http plugin: https://github.com/rabbitmq/rabbitmq-auth-backend-http\n. \ud83d\ude0d\n. According to my observations connections were still marked as \"blocked\" in the connections mgmt view, if that gives a clue or rules something out.. \n. @binarin's suggestion looks good to me! Much better then looking for /^Error: (.*)/ like we do today.. \n. It should be a server option, eg. {proxy_protocol, true}, normal clients should not be able to use the PROXY protocol, only load balancers. If the proxy protocol is enabled no normal client connections should be allowed, that could be a security issue. \n. Yes, no client lib modification is required, this is a server-side-only thing. http://www.haproxy.org/download/1.5/doc/proxy-protocol.txt\nThe server has to be configured to be behind a proxy and then accept the PROXY protocol, otherwise the IP could be spoofed, if any client could connect. From the spec:\nThe receiver MUST be configured to only receive the protocol described in this\nspecification and MUST not try to guess whether the protocol header is present\nor not. This means that the protocol explicitly prevents port sharing between\npublic and private access. Otherwise it would open a major security breach by\nallowing untrusted parties to spoof their connection addresses. The receiver\nSHOULD ensure proper access filtering so that only trusted proxies are allowed\nto use this protocol.\n(And this is not ELB only, HAProxy, Nginx, Stunnel, Stud etc all implement the PROXY protocol)\n. @uvzubovs with the new PROXY protocol support coming up you will get the real source IP, not the proxy's . Do we have any ideas on how to go forward here? Btw, this happens both when the messages are in the index and when they aren't. We routinely have very long queues on some servers, which works great as long as the server is up and running. But if the server has to be restarted it can't be done without adding tons of RAM (that's not then needed once up and running, but costs a small fortune). . In our isolated tests we can start instances with more messages than RAM (even with a huge number of queues) with message index embedding disabled (but not with the default value).\nWe do however have real-world cases where it still hasn't been possible to start the server before it runs out of RAM, even with queue index embedding disabled, so more research has be to performed there on our side. \nWe've been deploying servers with queue index embedding disabled and lazy queue-mode enabled by default on all servers for the past year or so. In our world it's much preferred to have stability and reliability over some extra msgs/s throughput. . IMHO it's a pretty valid \"feature request\", or bug report, depending on how you look at it. Let the server set a max-prefetch. Anyone can pretty easily crash a RabbitMQ server by not setting the prefetch limit (or set it to 0) and then just start consuming a long queue. Either a rough client or by mistake. . Ok, thank you ! :)\nOn Oct 20, 2017 18:25, \"Michael Klishin\" notifications@github.com wrote:\n\n@carlhoerberg https://github.com/carlhoerberg I will rename the key to\nrabbit.connection_max (we already have channel_max and frame_max) and\nupdate the docs. Thank you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/1400#issuecomment-338255507,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAK_Tpj9MGN7YoRqsQOZuc4CBDxJIkD_ks5suMmSgaJpZM4QAzFa\n.\n. The rabbitmq user doesn't not have permissions to privileged ports (<1024).\nUse iptables to redirect the port instead.\n\nOn Nov 16, 2017 11:14, \"Yoosef pp\" notifications@github.com wrote:\n\nthis is my log info\nError description:\n{could_not_start,rabbitmq_management,\n{{could_not_start_listener,\n[{port,443},\n{ssl,true},\n{ssl_opts,\n[{cacertfile,\"/home/rtm-aimpl-ca.crt\"},\n{certfile,\"/home/rtm-aimpl.crt\"},\n{keyfile,\"/home/rtm-aimpl.key\"}]}],\n{shutdown,\n{failed_to_start_child,ranch_acceptors_sup,\n{listen_error,rabbit_web_dispatch_sup_443,eacces}}}},\n{gen_server,call,\n[rabbit_web_dispatch_registry,\n{add,rabbit_mgmt,\n[{port,443},\n{ssl,true},\n{ssl_opts,\n[{cacertfile,\"/home/rtm-aimpl-ca.crt\"},\n{certfile,\"/home/rtm-aimpl.crt\"},\n{keyfile,\"/home/rtm-aimpl.key\"}]}],\nFun>,\n[{'_',[],\n[{[<<\"api\">>,<<\"overview\">>],\n[],rabbit_mgmt_wm_overview,[]},\n{[<<\"api\">>,<<\"cluster-name\">>],\n[],rabbit_mgmt_wm_cluster_name,[]},\n{[<<\"api\">>,<<\"nodes\">>],[],rabbit_mgmt_wm_nodes,[]},\n{[<<\"api\">>,<<\"nodes\">>,node],\n[],rabbit_mgmt_wm_node,[]},\n{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>],\n[],rabbit_mgmt_wm_node_memory,\n[absolute]},\n{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,\n<<\"relative\">>],\n[],rabbit_mgmt_wm_node_memory,\n[relative]},\n{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>],\n[],rabbit_mgmt_wm_node_memory_ets,\n[absolute]},\n{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,\n<<\"relative\">>],\n[],rabbit_mgmt_wm_node_memory_ets,\n[relative]},\n{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,\nfilter],\n[],rabbit_mgmt_wm_node_memory_ets,\n[absolute]},\n{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,\nfilter,<<\"relative\">>],\n[],rabbit_mgmt_wm_node_memory_ets,\n[relative]},\n{[<<\"api\">>,<<\"extensions\">>],\n[],rabbit_mgmt_wm_extensions,[]},\n{[<<\"api\">>,<<\"all-configuration\">>],\n[],rabbit_mgmt_wm_definitions,[]},\n{[<<\"api\">>,<<\"definitions\">>],\n[],rabbit_mgmt_wm_definitions,[]},\n{[<<\"api\">>,<<\"definitions\">>,vhost],\n[],rabbit_mgmt_wm_definitions,[]},\n{[<<\"api\">>,<<\"parameters\">>],\n[],rabbit_mgmt_wm_parameters,[]},\n{[<<\"api\">>,<<\"parameters\">>,component],\n[],rabbit_mgmt_wm_parameters,[]},\n{[<<\"api\">>,<<\"parameters\">>,component,vhost],\n[],rabbit_mgmt_wm_parameters,[]},\n{[<<\"api\">>,<<\"parameters\">>,component,vhost,name],\n[],rabbit_mgmt_wm_parameter,[]},\n{[<<\"api\">>,<<\"global-parameters\">>],\n[],rabbit_mgmt_wm_global_parameters,[]},\n{[<<\"api\">>,<<\"global-parameters\">>,name],\n[],rabbit_mgmt_wm_global_parameter,[]},\n{[<<\"api\">>,<<\"policies\">>],\n[],rabbit_mgmt_wm_policies,[]},\n{[<<\"api\">>,<<\"policies\">>,vhost],\n[],rabbit_mgmt_wm_policies,[]},\n{[<<\"api\">>,<<\"policies\">>,vhost,name],\n[],rabbit_mgmt_wm_policy,[]},\n{[<<\"api\">>,<<\"connections\">>],\n[],rabbit_mgmt_wm_connections,[]},\n{[<<\"api\">>,<<\"connections\">>,connection],\n[],rabbit_mgmt_wm_connection,[]},\n{[<<\"api\">>,<<\"connections\">>,connection,\n<<\"channels\">>],\n[],rabbit_mgmt_wm_connection_channels,[]},\n{[<<\"api\">>,<<\"channels\">>],\n[],rabbit_mgmt_wm_channels,[]},\n{[<<\"api\">>,<<\"channels\">>,channel],\n[],rabbit_mgmt_wm_channel,[]},\n{[<<\"api\">>,<<\"consumers\">>],\n[],rabbit_mgmt_wm_consumers,[]},\n{[<<\"api\">>,<<\"consumers\">>,vhost],\n[],rabbit_mgmt_wm_consumers,[]},\n{[<<\"api\">>,<<\"exchanges\">>],\n[],rabbit_mgmt_wm_exchanges,[]},\n{[<<\"api\">>,<<\"exchanges\">>,vhost],\n[],rabbit_mgmt_wm_exchanges,[]},\n{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange],\n[],rabbit_mgmt_wm_exchange,[]},\n{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,\n<<\"publish\">>],\n[],rabbit_mgmt_wm_exchange_publish,[]},\n{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,\n<<\"bindings\">>,<<\"source\">>],\n[],rabbit_mgmt_wm_bindings,\n[exchange_source]},\n{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,\n<<\"bindings\">>,<<\"destination\">>],\n[],rabbit_mgmt_wm_bindings,\n[exchange_destination]},\n{[<<\"api\">>,<<\"queues\">>],[],rabbit_mgmt_wm_queues,[]},\n{[<<\"api\">>,<<\"queues\">>,vhost],\n[],rabbit_mgmt_wm_queues,[]},\n{[<<\"api\">>,<<\"queues\">>,vhost,queue],\n[],rabbit_mgmt_wm_queue,[]},\n{[<<\"api\">>,<<\"queues\">>,vhost,destination,\n<<\"bindings\">>],\n[],rabbit_mgmt_wm_bindings,\n[queue]},\n{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"contents\">>],\n[],rabbit_mgmt_wm_queue_purge,[]},\n{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"get\">>],\n[],rabbit_mgmt_wm_queue_get,[]},\n{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"actions\">>],\n[],rabbit_mgmt_wm_queue_actions,[]},\n{[<<\"api\">>,<<\"bindings\">>],\n[],rabbit_mgmt_wm_bindings,\n[all]},\n{[<<\"api\">>,<<\"bindings\">>,vhost],\n[],rabbit_mgmt_wm_bindings,\n[all]},\n{[<<\"api\">>,<<\"bindings\">>,vhost,<<\"e\">>,source,dtype,\ndestination],\n[],rabbit_mgmt_wm_bindings,\n[source_destination]},\n{[<<\"api\">>,<<\"bindings\">>,vhost,<<\"e\">>,source,dtype,\ndestination,props],\n[],rabbit_mgmt_wm_binding,[]},\n{[<<\"api\">>,<<\"vhosts\">>],[],rabbit_mgmt_wm_vhosts,[]},\n{[<<\"api\">>,<<\"vhosts\">>,vhost],\n[],rabbit_mgmt_wm_vhost,[]},\n{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"permissions\">>],\n[],rabbit_mgmt_wm_permissions_vhost,[]},\n{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"connections\">>],\n[],rabbit_mgmt_wm_connections_vhost,[]},\n{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"channels\">>],\n[],rabbit_mgmt_wm_channels_vhost,[]},\n{[<<\"api\">>,<<\"users\">>],[],rabbit_mgmt_wm_users,[]},\n{[<<\"api\">>,<<\"users\">>,user],\n[],rabbit_mgmt_wm_user,[]},\n{[<<\"api\">>,<<\"users\">>,user,<<\"permissions\">>],\n[],rabbit_mgmt_wm_permissions_user,[]},\n{[<<\"api\">>,<<\"whoami\">>],[],rabbit_mgmt_wm_whoami,[]},\n{[<<\"api\">>,<<\"permissions\">>],\n[],rabbit_mgmt_wm_permissions,[]},\n{[<<\"api\">>,<<\"permissions\">>,vhost,user],\n[],rabbit_mgmt_wm_permission,[]},\n{[<<\"api\">>,<<\"aliveness-test\">>,vhost],\n[],rabbit_mgmt_wm_aliveness_test,[]},\n{[<<\"api\">>,<<\"healthchecks\">>,<<\"node\">>],\n[],rabbit_mgmt_wm_healthchecks,[]},\n{[<<\"api\">>,<<\"healthchecks\">>,<<\"node\">>,node],\n[],rabbit_mgmt_wm_healthchecks,[]},\n{[<<\"api\">>,<<\"reset\">>],[],rabbit_mgmt_wm_reset,[]},\n{[<<\"api\">>,<<\"reset\">>,node],\n[],rabbit_mgmt_wm_reset,[]},\n{[],[],cowboy_static,\n{file,\n\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/\nrabbitmq_management-3.6.9/priv/www/index.html\"}},\n{[<<\"api\">>],\n[],cowboy_static,\n{file,\n\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/\nrabbitmq_management-3.6.9/priv/www/api/index.html\"}},\n{[<<\"cli\">>],\n[],cowboy_static,\n{file,\n\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/\nrabbitmq_management-3.6.9/priv/www/cli/index.html\"}},\n{[<<\"mgmt\">>],[],rabbit_mgmt_wm_redirect,\"/\"},\n{['...'],\n[],rabbit_mgmt_wm_static,\n[\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/\nrabbitmq_management-3.6.9/priv/www\"]}]}],\n{[],\"RabbitMQ Management\"}},\ninfinity]}}}\nLog files (may contain more information):\n/var/log/rabbitmq/rabbit@v189205.log\n/var/log/rabbitmq/rabbit@v189205-sasl.log\n{\"init terminating in do_boot\",{could_not_start,\nrabbitmq_management,{{could_not_start_listener,[{port,443}\n,{ssl,true},{ssl_opts,[{cacertfile,\"/home/rtm-aimpl-\nca.crt\"},{certfile,\"/home/rtm-aimpl.crt\"},{keyfile,\"/home/\nrtm-aimpl.key\"}]}],{shutdown,{failed_to_start_child,ranch_\nacceptors_sup,{listen_error,rabbit_web_dispatch_sup_443,\neacces}}}},{gen_server,call,[rabbit_web_dispatch_registry,{\nadd,rabbit_mgmt,[{port,443},{ssl,true},{ssl_opts,[{\ncacertfile,\"/home/rtm-aimpl-ca.crt\"},{certfile,\"/home/rtm-\naimpl.crt\"},{keyfile,\"/home/rtm-aimpl.key\"}]}],#Fun>,[{'\n',[],[{[<<\"api\">>,<<\"overview\">>],[],rabbit_mgmt_wm_overview,[]},{[<<\"api\">>,<<\"cluster-name\">>],[],rabbit_mgmt_wm_cluster_name,[]},{[<<\"api\">>,<<\"nodes\">>],[],rabbit_mgmt_wm_nodes,[]},{[<<\"api\">>,<<\"nodes\">>,node],[],rabbit_mgmt_wm_node,[]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>],[],rabbit_mgmt_wm_node_memory,[absolute]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"relative\">>],[],rabbit_mgmt_wm_node_memory,[relative]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>],[],rabbit_mgmt_wm_node_memory_ets,[absolute]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,<<\"relative\">>],[],rabbit_mgmt_wm_node_memory_ets,[relative]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,filter],[],rabbit_mgmt_wm_node_memory_ets,[absolute]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,filter,<<\"relative\">>],[],rabbit_mgmt_wm_node_memory_ets,[relative]},{[<<\"api\">>,<<\"extensions\">>],[],rabbit_mgmt_wm_extensions,[]},{[<<\"api\">>,<<\"all-configuration\">>],[],rabbit_mgmt_wm_definitions,[]},{[<<\"api\">>,<<\"definitions\">>],[],rabbit_mgmt_wm_definitions,[]},{[<<\"api\">>,<<\"definitions\">>,vhost],[],rabbit_mgmt_wm_definitions,[]},{[<<\"api\">>,<<\"parameters\">>],[],rabbit_mgmt_wm_parameters,[]},{[<<\"api\">>,<<\"parameters\">>,component],[],rabbit_mgmt_wm_parameters,[]},{[<<\"api\">>,<<\"parameters\">>,component,vhost],[],rabbit_mgmt_wm_parameters,[]},{[<<\"api\">>,<<\"parameters\">>,component,vhost,name],[],rabbit_mgmt_wm_parameter,[]},{[<<\"api\">>,<<\"global-parameters\">>],[],rabbit_mgmt_wm_global_parameters,[]},{[<<\"api\">>,<<\"global-parameters\">>,name],[],rabbit_mgmt_wm_global_parameter,[]},{[<<\"api\">>,<<\"policies\">>],[],rabbit_mgmt_wm_policies,[]},{[<<\"api\">>,<<\"policies\">>,vhost],[],rabbit_mgmt_wm_policies,[]},{[<<\"api\">>,<<\"policies\">>,vhost,name],[],rabbit_mgmt_wm_policy,[]},{[<<\"api\">>,<<\"connections\">>],[],rabbit_mgmt_wm_connections,[]},{[<<\"api\">>,<<\"connections\">>,connection],[],rabbit_mgmt_wm_connection,[]},{[<<\"api\">>,<<\"connections\">>,connection,<<\"channels\">>],[],rabbit_mgmt_wm_connection_channels,[]},{[<<\"api\">>,<<\"channels\">>],[],rabbit_mgmt_wm_channels,[]},{[<<\"api\">>,<<\"channels\">>,channel],[],rabbit_mgmt_wm_channel,[]},{[<<\"api\">>,<<\"consumers\">>],[],rabbit_mgmt_wm_consumers,[]},{[<<\"api\">>,<<\"consumers\">>,vhost],[],rabbit_mgmt_wm_consumers,[]},{[<<\"api\">>,<<\"exchanges\">>],[],rabbit_mgmt_wm_exchanges,[]},{[<<\"api\">>,<<\"exchanges\">>,vhost],[],rabbit_mgmt_wm_exchanges,[]},{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange],[],rabbit_mgmt_wm_exchange,[]},{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,<<\"publish\">>],[],rabbit_mgmt_wm_exchange_publish,[]},{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,<<\"bindings\">>,<<\"source\">>],[],rabbit_mgmt_wm_bindings,[exchange_source]},{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,<<\"bindings\">>,<<\"destination\">>],[],rabbit_mgmt_wm_bindings,[exchange_destination]},{[<<\"api\">>,<<\"queues\">>],[],rabbit_mgmt_wm_queues,[]},{[<<\"api\">>,<<\"queues\">>,vhost],[],rabbit_mgmt_wm_queues,[]},{[<<\"api\">>,<<\"queues\">>,vhost,queue],[],rabbit_mgmt_wm_queue,[]},{[<<\"api\">>,<<\"queues\">>,vhost,destination,<<\"bindings\">>],[],rabbit_mgmt_wm_bindings,[queue]},{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"contents\">>],[],rabbit_mgmt_wm_queue_purge,[]},{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"get\">>],[],rabbit_mgmt_wm_queue_get,[]},{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"actions\">>],[],rabbit_mgmt_wm_queue_actions,[]},{[<<\"api\">>,<<\"bindings\">>],[],rabbit_mgmt_wm_bindings,[all]},{[<<\"api\">>,<<\"bindings\">>,vhost],[],rabbit_mgmt_wm_bindings,[all]},{[<<\"api\">>,<<\"bindings\">>,vhost,<<\"e\">>,source,dtype,destination],[],rabbit_mgmt_wm_bindings,[source_destination]},{[<<\"api\">>,<<\"bindings\">>,vhost,<<\"e\">>,source,dtype,destination,props],[],rabbit_mgmt_wm_binding,[]},{[<<\"api\">>,<<\"vhosts\">>],[],rabbit_mgmt_wm_vhosts,[]},{[<<\"api\">>,<<\"vhosts\">>,vhost],[],rabbit_mgmt_wm_vhost,[]},{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"permissions\">>],[],rabbit_mgmt_wm_permissions_vhost,[]},{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"connections\">>],[],rabbit_mgmt_wm_connections_vhost,[]},{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"channels\">>],[],rabbit_mgmt_wm_channels_vhost,[]},{[<<\"api\">>,<<\"users\">>],[],rabbit_mgmt_wm_users,[]},{[<<\"api\">>,<<\"users\">>,user],[],rabbit_mgmt_wm_user,[]},{[<<\"api\">>,<<\"users\">>,user,<<\"permissions\">>],[],rabbit_mgmt_wm_permissions_user,[]},{[<<\"api\">>,<<\"whoami\">>],[],rabbit_mgmt_wm_whoami,[]},{[<<\"api\">>,<<\"permissions\">>],[],rabbit_mgmt_wm_permissions,[]},{[<<\"api\">>,<<\"permissions\">>,vhost,user],[],rabbit_mgmt_wm_permission,[]},{[<<\"api\">>,<<\"aliveness-test\">>,vhost],[],rabbit_mgmt_wm_aliveness_test,[]},{[<<\"api\">>,<<\"healthchecks\">>,<<\"node\">>],[],rabbit_mgmt_wm_healthchecks,[]},{[<<\"api\">>,<<\"healthchecks\">>,<<\"node\">>,node],[],rabbit_mgmt_wm_healthchecks,[]},{[<<\"api\">>,<<\"reset\">>],[],rabbit_mgmt_wm_reset,[]},{[<<\"api\">>,<<\"reset\">>,node],[],rabbit_mgmt_wm_reset,[]},{[],[],cowboy_static,{file,\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/index.html\"}},{[<<\"api\">>],[],cowboy_static,{file,\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/api/index.html\"}},{[<<\"cli\">>],[],cowboy_static,{file,\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/cli/index.html\"}},{[<<\"mgmt\">>],[],rabbit_mgmt_wm_redirect,\"/\"},{['...'],[],rabbit_mgmt_wm_static,[\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www\"]}]}],{[],\"RabbitMQ\nManagement\"}},infinity]}}}} init terminating in do_boot\n({could_not_start,rabbitmq_management,{{could_not_start_listener,[{},{\n},{}],{shutdown,{}}},{gen_server,call,[rabbit_web_dispatch_registry,{\n},infinity]}}})\nCrash dump is being written to: erl_crash.dump...done\ni did not bind 443 port no another process.i already verified\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1426#issuecomment-344877519,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAK_TqkNot730QRgLvUGtJKPLVvZTyjcks5s3AsWgaJpZM4QgILN\n.\n. AFAIK opening and listening to a UDP port is not required to write to another UDP port, and not with syslog either. testing this i can see that writing syslog data to the udp port erlang is opening forwards it to the local syslog server. \n\nStarting eg. cat | logger -d does not open another listening UDP port. . Why are you doing a POC with several years old RabbitMQ and Erlang versions? Upgrade to latest, and don\u2019t open GitHub issues for this kind of stuff, use the mailing list. \n\nOn 5 Feb 2019, at 04:10, Niro23 notifications@github.com wrote:\nUpdate:\nRabbitMQ Version :\"3.6.9\"\nErlang_version: 19.0.4\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/rabbitmq/rabbitmq-server/issues/1864#issuecomment-460497846, or mute the thread https://github.com/notifications/unsubscribe-auth/AAK_TgKn8JneXjHsYerNBmioEQz52g5mks5vKPY5gaJpZM4aicV9.\n\n\n. ",
    "techmng": "Hi \nI am unable to post messages always seeing {routed:false} in response. Can someone help how I can post message using curl and then view it\n[cloud-user@manojp-centos-7 rabbitmq]$ sudo curl -i -u guest:guest -H \"content-type:application/json\"  -XPOST -d '{\"properties\":{},\"routing_key\":\"queue_test\",\"payload\":\"message test\",\"payload_encoding\":\"string\"}' http://localhost:15672/api/exchanges/%2f/manoj_exchange/publish\nHTTP/1.1 200 OK\nServer: MochiWeb/1.1 WebMachine/1.10.0 (never breaks eye contact)\nDate: Tue, 15 Mar 2016 17:57:52 GMT\nContent-Type: application/json\nContent-Length: 16\nCache-Control: no-cache\n{\"routed\":false}[cloud-user@manojp-centos-7 rabbitmq]$\n. ",
    "Ayanda-D": "The output is sorted. See rabbit_control_main:display_info_list. Question is, do we keep it as is, and display output only after we've received all info and sorted it, or, remove the sorting and display things as they happen?\n. Okay thanks @michaelklishin . Below is the current solution proposal and implementation steps I'll be following. Idea is to make responses from the rabbit nodes independent of the direct rpc:call's from rabbit_cli.erl module, allowing things to be displayed as they happen.\n1. rabbitmqctl command operates by starting a temporary node upon execution, from which remote calls to rabbit nodes are executed from.\n2. The rabbit_cli:rpc_call will be a spawned process, separate from the process running on the shell.\n3. The user defined timeout will be passed as part of the rpc:call, as already done in #181. In addition to this timeout being passed, the PID of the calling process i.e. self(), will also be included as part of the rpc:call params.\n4. After spawning rabbit_cli:rpc_call, the calling process will enter a receive loop, with a timeout equal to the user defined timeout, and wait to display responses it receives.\n5. At the backend, most operations make use of list comprehensions during data acquisition from mnesia for example. These will be updated to send message replies to the calling process upon execution of each element/step.\n6. The calling process will come out of the receive loop when it receives a finished message or when it times out. After this, program flow proceeds and the temporary node terminates.\n7. Any delayed messages from the rabbit nodes will be handled as in the current implementation; unhandled basically. They'll be sent to a nonexistant process which would have terminated at the end of the rabbitmqctl operation in (6), or be in the message queue of the spawned process in (2) if the temporary node hasn't terminated.\nThere was also concern about delayed messages sent to the process doing the rpc:call, from operations such as list_queues and list_connections, which do gen_server calls behind the scenes with timeouts set to infinity. However these won't be of concern as mentioned in (7).\n. @michaelklishin okay great. I'll proceed with the implementation.\n. @michaelklishin I tried replicating this problem,  and rabbitmqctl wait <PidFile> seems to work fine on my ubuntu-12.04 vm, with rabbit server running as a service. If this issue manifests in an unpredictable & random manner depending on the OS (similar to, or a manifestation of #92), can we add an OS type check for rabbitmqctl wait operation, similar to how rabbit_disk_monitor does this, which will fail and return a user friendly error indicating that the OS is not supported for this operation?\n. @michaelklishin okay cool. well not disable it and proceed (like rabbit disk monitor #91), but just do the same OS type check and give a more meaningful error/exception than Error: process_not_running, as reported in the gist. But I guess we'd have to be 100% certain that OS type is reason behind this operation failing.\n+ RABBITMQ_USE_LONGNAME=true exec erl -pa /usr/lib/rabbitmq/lib/rabbitmq_server-3.5.0/sbin/../ebin -noinput -hidden -boot start_clean -sasl errlog_type error -mnesia dir \"/var/lib/rabbitmq/mnesia/rabbit@rails-rabbitmq-68bc38.sfo01.justin.tv\" -s rabbit_control_main -nodename rabbit@rails-rabbitmq-68bc38.sfo01.justin.tv -extra wait /var/run/rabbitmq/pid\nWaiting for 'rabbit@rails-rabbitmq-68bc38.sfo01.justin.tv' ...\npid is 13046 ...\nError: process_not_running\nroot@rails-rabbitmq-68bc38:~# echo $?\n2\nroot@rails-rabbitmq-68bc38:~# cat /var/run/rabbitmq/pid\n13046\nroot@rails-rabbitmq-68bc38:~# ps ax|grep erl\n13046 ?        Sl     0:04 /usr/lib/erlang/erts-6.3/bin/beam.smp -W w -K true -A30 -P 1048576 -- -root /usr/lib/erlang -progname erl -- -home /var/lib/rabbitmq -- -pa /usr/lib/rabbitmq/lib/rabbitmq_server-3.5.0/sbin/../ebin -noshell -noinput -s rabbit boot -name rabbit@rails-rabbitmq-68bc38.sfo01.justin.tv -boot start_sasl -config /etc/rabbitmq/rabbitmq -kernel inet_default_connect_options [{nodelay,true}] -rabbit tcp_listeners [{\"auto\",5672}] -sasl errlog_type error -sasl sasl_error_logger false -rabbit error_logger {file,\"/var/log/rabbitmq/rabbit@rails-rabbitmq-68bc38.sfo01.justin.tv.log\"} -rabbit sasl_error_logger {file,\"/var/log/rabbitmq/rabbit@rails-rabbitmq-68bc38.sfo01.justin.tv-sasl.log\"} -rabbit enabled_plugins_file \"/etc/rabbitmq/enabled_plugins\" -rabbit plugins_dir \"/usr/lib/rabbitmq/lib/rabbitmq_server-3.5.0/sbin/../plugins\" -rabbit plugins_expand_dir \"/var/lib/rabbitmq/mnesia/rabbit@rails-rabbitmq-68bc38.sfo01.justin.tv-plugins-expand\" -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir \"/var/lib/rabbitmq/mnesia/rabbit@rails-rabbitmq-68bc38.sfo01.justin.tv\" -kernel inet_dist_listen_min 25672 -kernel inet_dist_listen_max 25672\n13064 ?        S      0:00 /usr/lib/erlang/erts-6.3/bin/epmd -daemon\n13463 pts/2    S+     0:00 grep --color=auto erl\n. @michaelklishin, question, for the min-masters policy, do we count & compare bound queues across all vhosts per rabbit node? and not specifically the vhosts of the queue being declared? \n. Okay. Thank you.\n. hi @michaelklishin. we've been running some tests to reproduce this issue. We're able to reproduce it when mirror nodes come back up simultaneously / in a near simultaneous manner. Frequency of occurance is very low and nondeterministic. We want to find out if we should still spend time fixing this in light of these test outcomes?\n. Okay. Thanks.\n. With all the concerns on implications on service when delegates timing out, or similar elements carrying out cross-node calls, some ideas on how we could actually make this feasible, in much a safer manner. I think our main limitation is the use of the gen_server behaviour to implement delegates. We have little control on setting any rules on when, and when not, a timeout should occur. I think re-implementing delegates to retain the same functionality, but using proc_lib, could give us more flexibility on adding rules relating to service, prior to a delegate exiting due to a timeout. For example;\n- Timeout only if certain a alarm (or control message) from a queue has been received - queues could be updated to send some control message to the invoking delegate when subject to certain conditions\n- Timeout if the current delegate's message queue is empty (avoiding problems regarding preceding calls and casts)\n- Timeout if the remote node is unreachable and we have no preceding calls/casts\n(could be more)\nOtherwise, if a timeout exit is going to be detrimental to service, rather the delegate resets it timeout and continues running. Another option is to update the gen_server2 behaviour, to allow handling of timeout rules from the callback, to determine whether the delegate (or any other gen_server2 process) terminates, or continues running. \nJust another design option before taking this feature into full swing. \n@michaelklishin @binarin \n. @binarin regarding the helper process you're referring to, are you implying that we spawn and wait for it complete? Then how would that help in determining whether the delegate continues alive or terminates upon a timeout? Bear in mind we only have 16 reusable delegates to work with, which is why a timeout of infinity has been appropriate. Also, having 16 delegates seems to have been a good idea in limiting the number of cross-node calling processes, compared to using rpc, which would spawn an intermediary process for each and every request.\nIf we add a timeout, more requests from channels could be inbound during the delegate's termination. So my problem with gen_server is, once we exit and the terminate function is called, we have no means of keeping the delegate alive, if for instance, the queue is busy (or for any other rules we define). We can sure spawn a helper process, but what happens to the messages held in the terminating delegate's message queue? And again, all this is for synchronous calls. Does the caller get a timeout exception, whilst the helper process continues with proceedings (if it had been spawned asynchronously)? \n. @michaelklishin the delegate to use for a multi_call is acquired by hashing the caller's pid and count of the number of delegate processes for a particular remote node. So if a delegate is referenced by more than one caller, and thus has some requests pending in its message queue, and times out and terminates, what then happens to remaining messages in its message queue? \n. I think the implications on queues when delegates do timeout are just too extreme. With the duration of a queue to complete processing being nondeterministic, I think its best leave out ANY queue related calls using infinity, despite quest to ensure cross-node timeouts \"in as many places as possible\". Also don't think there's any timeout policy we can define to entirely fulfil any requirement placed on a queue, even if it means re-implementing delegates, or improving them in any manner. Retaining infinity is safest, and we put emphasis of introducing timeouts only to cross-node calls to non-service affecting parts of the system.\n. @michaelklishin yes it is, and https://github.com/rabbitmq/rabbitmq-test/pull/1.\n. @michaelklishin thanks for the review and feedback. I've updated the header and config key. Please have a look to make sure everything is okay.\n. @videlalvaro its to ensure we calculate min-masters from bound queues only, across all vhosts.\n. @videlalvaro you mean this will filter out queues that have bindings, group them by node where they live, then count them? yes.\n. @videlalvaro cool. just clarifying the context of \"them\". but its all good :-)\n. @michaelklishin, from the updates so far, error code 530 (not_allowed) will be returned for direct broker connections only, i.e. amqp_direct_connection. For network connections, problem is rabbit_reader throws an exception when check_vhost_access fails. Socket is closed and nothing is sent back to help classify errors at the client end, and only the expected response is used. So access_refused will be returned to the client regardless of the error from rabbit_access_control:check_vhost_access/3. \nWould adding a handle_exception clause to rabbit_reader for such AMQP errors, avoiding the thrown exception, and sending #'connection.close'{reply_code = 530, reply_text = <<\"not allowed\">>, ...} back to the client before closing the socket be a way around this? or will this be violating amqp?\n. > Ideally to propagate the specific timeout reason to the client, we'd need to introduce a new protocol status code that would mean \"a timeout has occurred\".\nSome suggestions with regards to what error we propagate back to the client. I think we have two options here;\n1. Introduce a distinct timeout reason to return to the client as @michaelklishin suggests.\n2. Or, we could classify timeout exceptions as benign exits. \nThe latter would mean that delegate timeouts triggered by AMQP calls would result in either not_found or absent errors being returned to the client, depending on whether the queue is in the db that is. & note, all channel calls in question here eventually call rabbit_misc:with_exit_handler.\nWhat'd you guys think?\n. & another thing, @michaelklishin, i think our idea of caching channel_operation_timeout in the channel's process dictionary will only work if we're 100% certain that all functions in rabbit_amqqueue, which will be accessing timeout from this dictionary entry, will only be from channels. \nI'm now concerned with the initial idea, seeing that rabbit_amqqueue is not exclusive to any process, and can be used by anyone/anywhere. Otherwise we'll have to make do with accessing the timeout from application controller for every channel call :(\n. @michaelklishin okay cool. if performance won't be compromised, then its okay. thanks for that. \n. See implementation of delegate timeouts, thus far, along with rabbitmq-common/rabbitmq-server-248.  Any feedback welcome.\n. Seperating this issue from #166. Some concerns regarding end-to-end side effects on service, when delegates do timeout. Changes for delegate timeouts to be added as part of #166, as an ongoing feature with more analysis and tests of potential side effects to continue. \nFocusing this issue only on timing out when rabbit_amqqueue:notify_down_all/2 is called by the channel (as initially planned).\n. @dumbbell oh, I see the change which addressing this was reverted https://github.com/rabbitmq/rabbitmq-common/commit/02752f1c4c0b3f9226c30ad2811e01d24f481a36. Okay, we'll keep the channel state update in notify_queues/1 then, and only remove the strict match in #'channel.close' handler :)\n. @michaelklishin QA can wait for now, but you can review the updates and give feedback. I'll ping you when ready for QA. Thanks.\n. @michaelklishin this is ready for QA. Note, only successful timeout tests have been implemented here.\n. @michaelklishin this is now ready.\n. @videlalvaro question, for\nerlang\n    [AggregatorPid ! {Ref, Msg} ||\n        Msg <- Generator],\n    AggregatorPid ! {Ref, finished},\n    ok.\nare you assuming Generator to have been evaluated into a list? Because this wouldn't work as per initial requirement of the issue. Results wouldn't be sent as they happen, but after everything has been evaluated. \n. @videlalvaro okay cool. But just to make sure we're on the same wavelength, messages to AggregatorPid are only sent as the list is being produced, not after the list is produced. \nBut to add some extra level of abstraction, and your idea of passing a Generator, I've just added 2 functions to rabbit_misc.  Please confirm if you're okay with this;\nfor example in\nmap(List, fun(E) -> rabbit_misc:send_loop_result(AggregatorPid, Ref,  fun() -> generate(E) end ) \n          end),\nrabbit_misc:send_finished(AggregatorPid, Ref).\n& in rabbit_misc.erl;\n```\nsend_loop_result( AggregatorPid, Ref, Generator ) ->\n    AggregatorPid ! {Ref, Generator()}.\nsend_finished(AggregatorPid, Ref) ->\n    AggregatorPid ! {Ref, finished}.\n``\n. @michaelklishin okay, will keep inctlnamespace.\n. @videlalvaro okay cool. \n. @videlalvaro this is ready for another review and QA. \n. Currently having a look at this. Produced the same problem with a 2 node cluster (e.g.node1andnode2) and simulating theofflinenode (wsabove) by stopping the rabbit application (& not terminating the node);$> rabbitmqctl -n rabbit@node1 stop_app(which essentially is the effect ofpause_minority), and executingforget_cluster_nodefrom theofflinenode;$> rabbitmqctl -n rabbit@node1 forget_cluster_node --offline rabbit@node2. \n. findings thus far;\n- whenforget_cluster_node --offlineis executed, an attempt to impersonate the offline node (node1orwsfrom above examples) is done.\n- this by first stopping thetempnode started byrabbitmqctlor switching to non-distributed, i.e. resulting innonode@nohost- default shell, followed by an attempt to impersonate the executing offline node usingnet_kernel:start([Name, NameType]). Expected result of this action is a change fromnonode@nohostto start up of the impersonation node where all native operations to forget the specified cluster node (node2orws-clone) will be executed from.\n- problem is failure in starting the impersonating node with net_kernel:start/1.\n- anofflinenode, for example resulting frompause_minoritypartition handling, is not a terminated node. its just not visible to other cluster nodes and the rabbit application won't be running.\n- this means that theepmdprocess on the host of the offline node still has the node name of theofflinenode registered, and attempts to impersonate this node by executingnet_kernel:start/1result in a conflicts.\n- for now, i think for a node to be truelyoffline, even outside the scope of rabbit cluster nodes & connecting clients, also means having its name not registered by local runningepmdprocess - however no conclusions drawn yet\n. hello @dumbbell. can you please have a read at what I have so far for this issue, and share your thoughts and advice. i'm not sure how impersonating an offline node on its host would be possible if its name is still registered by theepmddaemon. this seems to be the cause of the problem.\n. @dumbbell thanks for this. & okay, opening separate issue for rabbitmqctl to check if node to be impersonated is  actually running or not, despite status of rabbit application (which will make what's been reported here a forbidden operation).\n. @michaelklishin #470 should be a sufficient fix for this issue. We can go ahead and close this :)\n. @michaelklishin we had some discussions on idea of listing down nodes, as part of./rabbitmqctl cluster_statusresponse, under adown_nodestag. WDYT? Or shall we have a separate command for listing down nodes?\n. Changing this into two separate features. Leaving this for listing down nodes.\n. Replicating [this thread](https://groups.google.com/forum/#!topic/rabbitmq-users/cslVlwhz2uA) is pretty straight forward, however providing error messges of more context when erlang distribution fails would involve making [diagnose_connect/2](https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_nodes.erl#L151) provide more information than just [ok](https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_nodes.erl#L157) when the [TCP connection succeeds but Erlang distribution fails](https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_nodes.erl#L156). Currently exploring some possible operations to carry out prior to [closing the socket](https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_nodes.erl#L156), which can help provide a more informative return than just [ok](https://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_nodes.erl#L157).\n. @michaelklishin this can go into3.5.7`. Rebasing #471 a bit of overkill for this fix. Recreated branch off stable & resubmitted PR #476. \n. Heads up, to update rabbit_webmachine_error_handler.erl then check for other places where client IP address logging would be relavant, and update if necessary.\n. Similar to the feeedback given on JIRA, VESC-574, the same cause and blocking effects of this problem were reported in https://github.com/rabbitmq/rabbitmq-federation/issues/7, of which investigations have been carried out, and a patch been developed. The procedures and conditions mentioned above are reproducing and manifesting the same cluster blocking effect, however not based on the same channel error which was reported, i.e.\n=ERROR REPORT==== 14-Dec-2015::16:44:40 ===\nChannel error on connection <0.5462.0> (192.168.245.3:53044 -> 192.168.245.5:5672, vhost: '/', user: 'rmq_nova_user'), channel 1:\noperation queue.bind caused a channel exception not_found: \"no exchange 'reply_5f360ace6a4d4a9e83c9d276877e4d4b' in vhost '/'\"\nThe patch which has been done directly addresses and fixes this problem; of channel errors caused by exchanges not being found in certain vhosts, which ultimately result in the rabbit cluster being unreachable. Shortly to follow is the progress of integrating this patch to rabbitmq-server for the next scheduled release.\n. @mageddo this was fixed as of 3.6.1. See #533 and #544.\n. @mageddo thats interesting. If you have further questions concerning this, raise them on rabbitmq-users, along with all your logs. This will be dealt with separately, as we've moved on from this particular issue.\nNB: RabbitMQ uses GitHub issues for specific actionable items engineers can work on, not questions.\n. Could be related to #530.\n. Might be a bit off direction to where discussion is going so far, but how fine-grain is the validation going be? And at the moment application_controller will only complain about the .config file format, but playing around with the config values still allows the application startup. Like specifying an unsupported cluster partition handling mechanism. I would fail the startup on such a case. To validate config file outside application_controller, I would probably have two boot steps, one to start a rules process which would load some predefined rules from file (could be in any format, XML for example) to ETS, then second to validate config, against rules held in ETS. If the .config is native Erlang terms (as current implementation), I'd use something like file:consult/1 read the file, which also validates format, then loop through the [{Tag1, Value1},{Tag2, Value2}, ...], validating each Value using rules in ETS, and throw an exception like {error, {badconfig,Value,LineNum}} to the user, to terminate application startup. If config is in a different format, like YAML as suggested, we could also add some mediation using the Erlang YAML parser/generator on the second boot step I've mentioned. My thoughts on this sofar. \n. @michaelklishin after carrying more tests, we've concluded that using -detached flag with erl does indeed seem problematic on a Windows environment. Currently, our options are;\n- Fix erl to handle -detached as expected (might take a while),\n- Update the rabbitmq-server.bat file to use the werl[1] command whenever the detached flag is used.\n- Use default erlsrv[2], as recommended by some folks from the OTP team for running an Erlang VM on Windows.\nwhich option is best going forward in resolving this problem?\n[1] http://erlang.org/doc/man/werl.html\n[2] http://erlang.org/doc/man/erlsrv.html\n. Okay, as discussed, going with second option for 3.6.x, of updating the rabbitmq-server.bat to use werl command whenever the -detached flag is passed. To investigate erlsrv for future releases.\n. @michaelklishin note; above feedback/findings from @rtraschke an internal ESL team member, who are currently focused on this issue. Just waiting for more feedback from them on how they're looking to get this to work.\n. @michaelklishin using the -detached flag still fails in an unpredictable manner when using werl. The team has filed an OTP bug, and once this is fixed, and shipped with an erlang release (which we'll confirm), rabbitmq-server.bat script will be updated to cater for the -detached flag. \n. @johnfoldager seems like your log setting is not defined in your LDAP configuration. Can you just check that this is defined. & if it was purposefully removed to not have logging, rather define it, and set it to false.\n. Although i agree. This mustn't fail destructively.\n. hmm, I think we can close this, in favour of rabbitmq/rabbitmq-cli#145. Thank you @Gsantomaggio! :). Okay. I suppose if we can classify them, and if defined policies belong to the same class, they'd be applied in a non-deterministic manner. \n. Okay cool. Maybe we should document this, that for example, if you want use a lazy queue or queue master distribution strategy, with mirroring policies, to only use declare arguments (and/or rabbitmq.config for queue master location). Although I'd have expected setting these as policies of equal priorities to also be possible, if possible via declare args. \n. @michaelklishin good question, hence terming this as \"Consider\" (if feasible). Not sure if we can somehow keep record/relation of which apps are started as part of a plugin's enable, and attempt to stop them if used once on disable. \nAnd I agree, drilling down to \"which other apps\" can get tricky. But the unnecessary/undesired overhead left on the node doesn't seem right when a plugin is no longer required, even node resource wise.\n. @michaelklishin sure, working on it. Minimal handling of this case seems to have been overlooked in the current implementation, and shouldn't be the cause of some of the problems regarding the GM. & as I've mentioned already, the case is rare and just needs to be handled appropriately, so I doubt simulating this scenario in a test case is feasible.\n. True. Usability wise from an non-Erlang perspective wouldn't seem like an improvement, but, it does give a true reflection of the current state of their instance. So it depends on what we're prioritising, users who might confuse this, or a true reflection of the server state.. +1. And users are most likely to be unaware of which queues are residing on the node being removed, in case any are critical, and the exercise of checking this could be tedious (as queue counts increase). Brainstorming a possibility of queue migration policies, putting onus on users to decide how migration is carried out, and for which queues in particular (durable/transient);\n- migrate-to-none - simply delete all non-mirrored queues on removed node\n- migrate-to-any - move matching non-mirrored queues to any other cluster node\n- migrate-to-node - move matching non-mirrored queues to a specific cluster node\n- migrate-to-min-master - move matching non-mirrored queues to node with least queue masters\nDurability definitely a big factor. Migration would also slow down duration of node removal procedure - but, it would/could be the price to pay, in order to retain any non-mirrored queues.. Shouldn't we draw the same conclusion as when its undefined? And just log a warning/error message that a default module is being used, and the custom module was not found/does not exist. Seems too light a reason for a node boot failure.. Okay understood. Although I still think this check needs to be carried out, and if a boot failure in such a scenario is what you guys prefer, then we at least fail safe, instead of a noisy failure. Personally I'd still prefer a fallback to a default module and just log an error message - but...your call :). hi @lukebakken - we've been working on a plugin[1] that addresses this issue. We define an ideal state in which cluster nodes possess an acceptable level of balance (which we term Queue Equilibrium ), which the plugin (FSM based) works towards achieving. We also take into account the risks involved in moving queues which aren't mirrored but yet hold messages, and leave onus on users to setup an HA-policy of choice (since variation of deployments beyond our control), prior plugin usage.\nVet it out, and see if it fits as a possible/complimenting solution for this issue. Some of our customers are already making use of this \ud83d\udc4d \ncc, @michaelklishin @hairyhum @Gsantomaggio \n[1] https://github.com/Ayanda-D/rabbitmq-queue-master-balancer. \ud83d\ude06 . Interesting. Although a stricter all_running clause which verifies both from mnesia, and is_running per acquired node would be good, for instances like these where we expect the vhost's supervision tree to already been setup. i,e. rabbit running already, not just the node. As it stands, context of running in rabbit_nodes module doesn't make sense. Something for the future \ud83d\udc4d . & in context of info calls such as that from the UI or CTL doing rabbit_nodes:all_running/0 only without checking the application as well. Now that doesn't make sense.. For the directory names, in case of an audit and question on \"who\" or \"what\" can temper with messages  stores for example? Wouldn't that be considered as vulnerable? And a more secure hash full-proof any chance of access attempt external to rabbit-server. Then for the cookie-hash, thats a definite security threat as things stand.. @michaelklishin sure, we can give reasons and detailed explanations on why and how not certain aspects are vulnerable security-wise. And I understand, as there's a level of defence for certain products if anything is deemed as a loop-hole. However, and unfortunately for all of us here, regulations have to be met in strict environments in which audits are carried out in varied methods, and usage of non-validated hash functions which are already listed as a deciding factor on whether certain systems are permitted to be provisioned in such environments, or not, can prove as the main limiting factor. So where applicable, lets improve the \"reputation\", and where its unnecessary, we can avoid certain elements which may be deemed as vulnerable . ah, solving this weird case https://groups.google.com/forum/#!topic/rabbitmq-users/-X19dq8cYaY thank you! \ud83d\udc4d . @michaelklishin ok, this was reported to us as well, but haven't been able to to reproduce it either :/ seemed the error handling could be improved as well? When binding lookup  in rabbit_route is done and not found, but exists in rabbit_durable_route, before immediately returning not_found to the channel, maybe do/attempt a re-sync from rabbit_durable_route, to update rabbit_route and rabbit_semi_durable_route? And only return not_found if binding still doesn't exist after a \"failed re-synch\" attempt (if such a case can ever occur)? coz if its in rabbit_durable_route then surely the binding exists, and returning not_found here doesn't sound right. Some thoughts, guess this PR resolving the issue on recovery then maybe this case may never be hit on runtime. We'll see \ud83d\ude05 . Ok, sounds good \ud83d\udc4d . @michaelklishin yeah it can, and would be even more efficient. Would be straight forward if we're doing this for check_write_permitted_on_topic/4 only. But there's check_read_permitted_on_topic/4 as well, which is called in binding_action/9, which will mean a propagating this channel \"source\" through from handle_method/5, which doesnt hold #ch{} (as opposed to handle_method/3). Happy to use channel state at your green light - process dictionary was the least affecting immediate fix.. Updated this to localise the source notification to the channel implementation.. @michaelklishin yep, indeed - i also cant think-of/see anywhere else its used besides rabbit_mgmt_util, we can always hardcode the source on there. Ok so store & read source from state it is. I'll update this \ud83d\udc4d. @michaelklishin sure, ready for review, now based on process state \ud83d\udc4d . merged, kwel! @michaelklishin ok i'll double check \ud83d\udc4d . crosschecked v3.7.x branch, did QA/ran some tests, and it's ok. thanks! \ud83d\udc4d . is this okay for the last few lines?\n%%%\n%% Developed by Erlang Solutions Ltd on behalf of GoPivotal, Inc.\n%% Copyright (c) 2007-2015 GoPivotal, Inc.\u2002 All rights reserved.\n%%\n. @michaelklishin please also look at my conversations with @videlalvaro on the slack room.\n. @michaelklishin please have a look at our conversations on 17/07/2015 and 20/07/2015 on my understanding of this. For the changes you're commenting on, @videlalvaro also already had a look at them. Just the test code which hadn't gone through PR. \n. @michaelklishin cool, no problem. @videlalvaro did mention how critical this part of the system is, so the more the reviews on this, the better. keep us posted.\n. Yes, I think thats what I mentioned on 20/07/2015 [6:36 PM], on slack. Just needed confirmation from you guys. If that's what we're concluding on this, then cool.\n. Okay noted. AggregatorPid sounds fine, & appropriate. \n. Okay noted. I'll update & bear that in mind. And yes, purpose is to wait for info massages. \n. @videlalvaro here the net_ticktime of the temporary node started when rabbitmqctl is executed is being set to match that of the remote node. This is the same logic of the current implementation.\n. @videlalvaro there are some list_* calls which need only spawn rabbit_cli:rpc_call directly, and some which first need to apply list_to_binary_utf8/1 to their arguments before spawning rabbit_cli:rpc_call, which is the purpose of call/4. This serves same purpose with call/3 non-spawned rabbit_cli:rpc_calls. So not all list_* operations make use of of call/4.\n. @videlalvaro for your second question, refactor how exactly, into one function? I kept these two separate based on their unrelatedness, and just to make things a bit more readable.\n. @videlalvaro thanks for the comments, and cool, I'll move spawn_link/wait_for_info_messages to the call/4 function. I'm busy working on this.\n. @videlalvaro yes its to buy a bit of time for the connection to be established. However list_connections proceeds if 100ms had elapsed without any connection established (empty list message replies to the shell are ignored).\nI'll update this to verify connection first, after timer:sleep(100), before it proceeds to executing list_connections. \n. @videlalvaro okay i'll update this. had avoided combining any new changes for this PR with the original backend implementations, but will do.\n. @videlalvaro I added a check for this, so it ensures that when list_connections is executed, a connection already exists. but what could cause the scenario you've mentioned to occur? i mean both the temp client node & rabbit node are started on the same host whenever the tests are run, and/or are likely to have through a number of amqp connection tests by the time they reach this point\n. Okay its extracting. How does extract_user_permission_params/2, and extract_internal_user_params sound?\n. @michaelklishin message sending to a Pid never fails, so that error will never occur. You're probably better off validating with is_process_alive(Conn) instead, prior to attempting to reregister, if you want to log any error here.\n. is a case statement is necessary here? if Cs = [], then nothing to evaluate in the list comprehension, and you can just return {ok, State}.\n. this might not lie on the critical path, but use of proplists:get_value/2 seems quite extensive. how about using rabbit_misc:pget/2 instead, considering that it was optimised in https://github.com/rabbitmq/rabbitmq-common/pull/51\n. @michaelklishin you're going through 2 lists just to calculate number of connections? imagine how much this will impact performance (per connection open request) as cluster size increases? can you use lists:foldl/3 only please, with connection count as your accumulator. there's no need for lists:map/2 here.\n. you can also make Chunks your accumulator in lists:foldl/3 for both your listing functions (unless you were planning something else)\n. on coding style, can we stick to our general indentation rule, of the default Erlang mode for Emacs? your indentation also seems to differ from function to function. for example boot/0 and ensure_tracked_connections_table_for_node/1 appear to use different indentation.\n. also on coding style, please stick to the max 80 chars per line convention.\n. consider referencing your table name creating functions using macros. would make code a bit more easier to read. \n. please also use record_info for specifying tracked_connection_per_vhost table attributes. otherwise if ever we decide to add more attributes to the table/record, then we'd have to hard code them here as well.\n. Okay, your preference. But usually, using macros for table names, or directly specifying the table name atom, is general practice. I think ?TRACKED_CONN_PER_VHOST_TABLE(N) would read better.\n. then why are we still retaining this rule in our header comments? and besides, most of the code base still follows this convention. \n. @dcorbacho minor thoughts here; how about a single function which takes desired env variable and calls rabbit_misc:get_env/3, to avoid what appears like code replication in both case clauses. Also, could be worth adding documentation for queue_explicit_gc_run_operation_threshold as part of this PR (unless queue_explicit_gc_run_operation_threshold configuration audience is not intended for general rabbit users).. i've added network_arbitrary_channel_source test case to illustrate why relying on the supervisor for setting the channel source may be a problem. rabbit_channel_sup:start_link may still be executed by any entity. Network/tcp based parent connections may still be amqp_params info compliant. e.g. amqp_gen_connection based connections, and if passed as part of a channel's start args (even for test purposes), then it should still work as expected. have a look at the test case . with such a shim function in the channel, the preprocessor will always expand ?MODULE to rabbit_channel, causing a reflection something that's incorrect in state. Rather the source has to explicitly provided. And announcing source isn't mandatory. If rabbit_channel:source/2 is missed, source defaults/remains as undefined in state and is always ignored for this purpose. Unless we're anticipating another critical connection implementation aside rabbit_reader to consider? . ",
    "jmcmeek": "Its not the pacemaker agent that requires the directory.  The problem is using the agent on RHEL 7 with the systemd systemctl command.  The RHEL 7 systemd scripts expect /var/run/rabbitmq to be created during install of the rabbitmq-server RPM.  Since the pacemaker resource agent deletes the directory, you cannot later run \"systemctl start rabbitmq-server\" without first recreating that directory and setting proper permissions.\n. This is in https://github.com/rabbitmq/rabbitmq-server/blob/master/packaging/common/rabbitmq-server.ocf\nin rabbit_stop:\n302  $RABBITMQ_CTL stop  <=  add $RABBITMQ_PID_FILE\nWhen I get approval from our legal folks to do so, I can look at submitting a pull request.\n. ",
    "falfaro": "What's the difference between using the OCF resource script and a plain LSB resource in pacemaker? As far as I can see, the OCF resource does not seem to figure out the right order in which cluster members shall be started.\n. Does this bug affect RabbitMQ 3.6.1?\n. ",
    "gerhard": "We've been using rabbitmqctl wait PID_FILE for a long time in cf-rabbitmq-release, the command works well, especially since #1217 . Feel free to re-open if this is still relevant.. Just came across something very similar in 3.6.11-rc.3 running on OTP 20.0.2\nAug 16 04:08:59 localhost kernel: [61139.180699] 2_scheduler[15162]: segfault at 5c ip 00000000005ba760 sp 00007f8fea1ffc40 error 4 in beam.smp[400000+333000]\nThe last logged line by RMQ, before Erlang VM segfaulted:\n=INFO REPORT==== 16-Aug-2017::04:08:57 ===\nclosing AMQP connection <0.24230.9> (10.0.16.35:42562 -> 10.0.16.23:5672, vhost: '/', user: 'admin')\nLast item logged in SASL, before Erlang VM segfaulted:\n=SUPERVISOR REPORT==== 16-Aug-2017::04:06:48 ===\n     Supervisor: {<0.24441.9>,amqp_channel_sup_sup}\n     Context:    shutdown_error\n     Reason:     shutdown\n     Offender:   [{nb_children,1},\n                  {name,channel_sup},\n                  {mfargs,\n                      {amqp_channel_sup,start_link,\n                          [direct,<0.24440.9>,\n                           <<\"<rabbit@rmq0-rmq-gcp-36.1.24440.9>\">>]}},\n                  {restart_type,temporary},\n                  {shutdown,infinity},\n                  {child_type,supervisor}]\nProcess flags (restarted after segfault)\n10010 /var/vcap/packages/erlang-20.0.2/lib/erlang/erts-9.0.2/bin/beam.smp -W w -A 64 -P 1048576 -t 5000000 -stbt db -zdbbl 128000 -K true -- -root /var/vcap/packages/erlang-20.0.2/lib/erlang -progname erl -- -home /home/vcap -- -pa /var/vcap/jobs/rabbitmq-server/packages/rabbitmq-server/ebin -noshell -noinput -s rabbit boot -sname rabbit@rmq0-rmq-gcp-36 -boot start_sasl -config /var/vcap/jobs/rabbitmq-server/rabbitmq -kernel inet_default_connect_options [{nodelay,true}] -sasl errlog_type error -sasl sasl_error_logger false -rabbit error_logger {file,\"/var/vcap/sys/log/rabbitmq-server/rabbit@rmq0-rmq-gcp-36.log\"} -rabbit sasl_error_logger {file,\"/var/vcap/sys/log/rabbitmq-server/rabbit@rmq0-rmq-gcp-36-sasl.log\"} -rabbit enabled_plugins_file \"/var/vcap/jobs/rabbitmq-server/packages/rabbitmq-server/etc/rabbitmq/enabled_plugins\" -rabbit plugins_dir \"/var/vcap/jobs/rabbitmq-server/packages/rabbitmq-server/plugins\" -rabbit plugins_expand_dir \"/var/vcap/store/rabbitmq-server/mnesia/rabbit@rmq0-rmq-gcp-36-plugins-expand\" -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir \"/var/vcap/store/rabbitmq-server/mnesia/rabbit@rmq0-rmq-gcp-36\" -kernel inet_dist_listen_min 25672 -kernel inet_dist_listen_max 25672\nLinux kernel\n4.4.0-83-generic #106~14.04.1-Ubuntu SMP Mon Jun 26 18:10:19 UTC 2017 x86_64 GNU/Linux\nReporting to erlang-questions@erlang.org. @michaelklishin should this be implemented in 3.6.7 as well? I'm thinking yes.. Implemented in both v3.6 and v3.7 CLI, making its way through the pipeline.. Not merging into stable since it's new rabbitmqctl related.. If this environment is expected to be secure, I'm pretty sure running commands as root go against this directive. Furthermore, enabling RabbitMQ plugins from the command-line mutate the deployment, which also doesn't sound like a good idea in a security strict environment.\nWould it be acceptable to enable plugins via configuration rather than the command line? If so, the infrastructure automation tool of your choice could simply create RABBITMQ_ENABLED_PLUGINS_FILE specific to your platform with the correct permissions which would contain e.g. [rabbitmq_management,rabbitmq_top].. @uvzubovs would it be enough if we logged the vhost?. Could not reproduce on a clean 3 node 3.6.8, it must have been some left-behind state that was causing this issue.. We have decided to introduce a new rabbitmqctl shutdown command as per #1166 . This allows us to get the improved shutdown behaviour without making any changes to the stop command which has been working well for many years now.. In the interest of keeping all scripts consistent, I would suggest that we change all /bin/sh/ to /usr/bin/env bash . This also means that we need to specify bash as a package dependency for all distros that we maintain.. The wait workaround for Dash sounds most reasonable since we cannot introduce a dependency on bash in 3.6. Long-term we really want to simplify all scripts, including rabbitmq-server.. We have determined that set -e in Dash will prevent traps from executing. Therefore, the problem is a bit more complicated and requires us to review the use-case for set -e.\nGiven a happy path scenario, we are thinking of forcing wait to always succeed. We are leaning towards wait $! || true which is a variation of your suggestion.\nThe unhappy paths will be a bit more complicated since we cannot use set -e in Dash as it prevents any traps from executing.\nOn SIGINT rabbitmq-server exits with status code 130 (128 + 2). We depend on this behaviour in makefile recipes. Wrapping wait in an if statement intercepts wait's exit code, so we can't use the PR as it stands. We always have to explicitly exit in all traps:\n```sh\n!/bin/sh\nset -ex\ndo_things() {\n  sleep 10\n}\ntrap \"echo SIGINTing; exit 130\" INT\ntrap \"echo SIGTERMing; exit 0\" TERM\ntrap \"echo SIGHUPing; exit 0\" HUP\ndo_things &\ndo_things_pid=\"$!\"\nwait $do_things_pid || true\n```\nWhat do you think?. It sounds reasonable with sufficient real-world implementations backing this approach, I cannot think of any reason why we shouldn't implement it in 3.7. I'm in favour of explicit vars, not abbreviations. Makes sense otherwise.. I would assume that such usernames would not be allowed to enter the system. Confident code with guards at the edges and all that.. Should there be a different flag to disable kernel polling?. In which case, updating the documentation which is outdated should be sufficient.. \ud83d\udc4d . It's actually worse than we thought:\n| Nodename | ps (bytes) | erlang (bytes) | Off by (%) |\n|-|-|-|-|\n| rabbit@rmq0-rmq-36-20170608 | 441290752 | 175692152 | 61% |\n| rabbit@rmq1-rmq-36-20170608 | 663388160 | 327849080 | 51% |\n| rabbit@rmq2-rmq-36-20170608 | 335593472 | 165004064 | 51% |\n```sh\nps_bytes_mem() {\n  KiB=$(ps -p $(pgrep beam) -o rss=)\n  echo $((KiB * 1024))\n}\nerlang_bytes_mem() {\n  rabbitmqctl eval 'erlang:memory(total).'\n}\ndiff() {\n  ps=$(ps_bytes_mem)\n  erl=$(erlang_bytes_mem)\n  echo $((100 - erl * 100 / ps))\n}\necho \"\n| Nodename | `ps` (bytes) | `erlang` (bytes) | Off by (%) |\n|-|-|-|-|\n| $RABBITMQ_NODENAME | $(ps_bytes_mem) | $(erlang_bytes_mem) | $(diff)% |\n\"\n```\n@dumbbell can you sanity check this? The difference seems too big to me.. @michaelklishin feel free to use this in the release notes:\nAs of this release, RabbitMQ will display the memory usage as reported by the operating system, not by the Erlang VM. According to the official Erlang documentation, total [memory] value is not supposed to be equal to the total size of all pages mapped to the emulator. Also, because of fragmentation and pre[-]reservation of memory areas, the size of the memory segments containing the dynamically allocated memory blocks can be much larger than the total size of the dynamically allocated memory blocks. This means that in some cases, RabbitMQ would significantly under-report the used memory. For example, Erlang VM would report 167MB used memory while the OS would report 420MB used by the Erlang VM process.\nWhile it may appear that as of this version RabbitMQ is using more memory, it's just a result of switching from Erlang VM memory reporting to OS memory reporting.\nSince memory usage is used to trigger memory alarms, publishers will be blocked more often. This change will improve stability since RabbitMQ is less likely to be killed by the OS due to out-of-memory errors.\nOS memory reporting is now enabled by default. To disable, set {vm_memory_use_process_rss, false} in the rabbit config.. @michaelklishin @dumbbell ready for QA. Good point, happy to change the flag.\nWhat should we do when a user sets an unsupported flag? I would prefer defaulting to rss & logging a warning over crashing.. That's good to know, glad that cuttlefish guards against incorrect configuration in 3.7\nWill default to rss & log a warning in 3.6. @michaelklishin how about now?. You're too good (sounds better than we're terrible). Thanks for spotting this, will go over it more thoroughly.. All failing tests are now passing. Running all broker tests, ok to QA again.. @michaelklishin just PR-ed management-agent, anything else that you can think of?. Config property has changed to vm_memory_calculation_strategy and can be (rss|erlang): https://github.com/rabbitmq/rabbitmq-website/commit/970bca75d0b9866dbc54bb2e52e95459cae94ae8. @carlhoerberg @Gsantomaggio can you please confirm if setting queue_index_embed_msgs_below to 0 solves your problem?. We've confirmed that setting queue_index_embed_msgs_below to 0 does not fix the problem. Having talked to @Gsantomaggio this is what we've concluded:\nIt is unexpected and undesired for lazy queues to load messages in memory during RabbitMQ node startup. The primary reason for using lazy queues is to keep messages on disk as long as possible, which makes  the current lazy queue behaviour on node startup unexpected. Furthermore, this results in RabbitMQ crashing if there are many lazy queues on a RabbitMQ node and the messages are below 4096 bytes in size even if queue_index_embed_msgs_below is set to 0. Currently, on node startup, lazy queues will load in memory 16384 messages. This number comes from SEGMENT_ENTRY_COUNT: https://github.com/rabbitmq/rabbitmq-server/blob/5d45fc999335451eeee3f9dd4c6f87b9fbfe4f3f/src/rabbit_queue_index.erl#L140 . In the current implementation, if the first message is 4096 bytes or lower it will be loaded into memory, as well as all other following messages, up to 16384, if they are 4096 bytes of lower. If the first message is 4096 bytes and the second messages is above 4096 bytes, only the first message will be loaded into memory.\nEven if queue_index_embed_msgs_below is set to 0, therefore Queue Index embedding is disabled, messages will still be loaded into memory as described above.\nThe expected and desired lazy queues behaviour would be to not have messages loaded in memory on node startup.\nDid we understand the problem correctly?\n  . @michaelklishin are we saying that it's a matter of documenting that SEGMENT_ENTRY_COUNT worth of messages will be loaded into memory on node startup, regardless of message size? This is what @Gsantomaggio observed & reported.\nIf this is the expected behaviour and nothing will be done about it until the queue index is replaced by something else, we should at least document it on https://www.rabbitmq.com/lazy-queues.html\nGiven our current team focus and the upcoming themes, I believe that a proper fix will need to wait until we start working on the message store level replication (a.k.a. The Replicator), which is at least a few months away. What do you think @michaelklishin ?\ncc @dumbbell @kjnilsson . @carlhoerberg There are some useful links in https://github.com/rabbitmq/workloads/tree/master/lqs-ha-confirm-multiack.\nEvery 24 hours, VMs get preempted, so if this is an issue, RabbitMQ nodes should fail to start. As you will notice, producers are consistently outpacing consumers, so once the queue limits will be hit, if everything works as it should, then the cluster should remain in equilibrium, with VMs coming and going.. I can see that messages will be loaded into memory on node startup even if queue_index_embed_msgs_below is set to 0:\n\n\nNotice how much memory message store index is using on the restarted node even though queue_index_embed_msgs_below is set to 0.\nThis is the state of the cluster after rmq0 gets restarted:\n\nThe memory alarm is getting triggered intermittently. No new messages are being published. While all channels are idle, all publisher connections are blocking. Most queues are idle, few queues are running.\nThe RabbitMQ cluster that I'm basing all the above observations on is public.\nAs a next step, we need to reach consensus within the team that the above observations are correct. Will let you know when we know more.. All observations point to the fact that mirrored lazy queues are a bad idea:\nQueues are sync-ing for a really long time, all queues keep messages on disk, nothing is loaded in memory:\n\nrmq0 has all the messages & it's sync-ing very slowly with rmq1 & rmq2:\n\nrmq1 has triggered the memory alarm, most memory is used by message store index & binaries:\n\nIn conclusion, mirrored lazy queues are pretty bad in practice. Will set up a single-node environment with many lazy queues to make public the actual behaviour.. These are some excellent points @dcorbacho, I'm updating the lazy queues docs to capture these learnings.\nJust to be clear, when we say that lazy queues are not efficient with max-length, do we mean:\n\nuse these 2 settings with care, performance of operations A, B, etc. will degrade, or\nuse these 2 settings with care, they may introduce cluster instability under certain conditions, e.g. A, B, etc. or\n\ndo not use these 2 settings together, they are not supported & you may experience unexpected problems. I continued looking into this and can confirm that simple lazy queues do not load messages into memory on node startup if queue_index_embed_msgs_below is set to 0. This is on RabbitMQ 3.7.2 & Erlang 20.2.2. Things that don't matter:\n\n\nlazy mode definition via policy / queue arg\n\nMessage body size:  I tried with 100 byte & 1000 byte messages\n\nCan you please confirm my observations?\nRabbitMQ lazy queue memory & disk usage.xlsx\n. We've shared our learnings in https://www.rabbitmq.com/lazy-queues.html?t=1#behaviour, please re-open this issue if you find the behaviour of Lazy Queues on node startup inconsistent with our latest learnings. This  is in particular relevant:\n\nTo clarify, changing the queue_index_embed_msgs_below property after messages have already been embedded in the queue index will not prevent lazy queues from loading messages into memory on node startup.. Letting the benchmark run for 12h before it's ready to merge.\n\n\n\n. Having run the benchmark for 18h, we are confident that high message redelivery rates for mirrored queues no longer affect the cluster stability:\n\nEven though there is a noticeable memory fluctuation after 11h, it's not significant. Memory usage grows from 400MB to 1000MB, it remains stable for 4h, then drops back to 400MB before another short increase. Eventually, memory returns back to 400MB and remains stable for the rest of the 18h. We observe the same behaviour on all nodes. Since the memory returns back to normal and since the cluster is stable throughout, we are happy to just take note of this and not investigate further.. @michaelklishin @dcorbacho ready to review & merge. We've left rmq-148892851 around, in case you need to use it for further benchmarks.. The Erlang VM spends 44.80% in busy_wait. Setting +sbwt none via RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS significantly reduces CPU utilisation - a few % for both user & system spaces.\nRead more and discuss on the mailing list thread\ncc @michaelklishin @dcorbacho . Once the missing properties are added to rabbitmq.config.example, I was thinking that we should order all config properties alphabetically. It requires the least cognitive overhead, both when adding or looking properties up. What do y'all think?. This is now ready to review & merge. @michaelklishin how about now?. Having run this PR against 3.7.0-rc.1 for ndq-1kb-autoack, we can see an 18% improvement (50660 msg/s vs 42930 msg/s) when looking at the average throughput over 4 hours (purple is this PR, blue is 3.7.0-rc.1):\n\n. After about 30-40 runs, I am yet to see one that did not drain within the hour. Typically, the queue gets drained within 10' - 30'\nBefore this change\nNotice that the queue is still being drained after 20 hours\n\nAfter this change\nThe queue is drained in 28 minutes:\n\n\n. ## Other runs after this change\n\n\n\n. ## Other runs with CONSUMER_BIAS, 2.0\n\n\n\n                                          ... and the battery is drained \ud83d\ude09  vvv\n\n. ## Artefacts\nndq-small-ack-36-optimised.zip\n. Apparently, the number of CPUs (number of Erlang schedulers) is a decisive factor in whether a queue can be drained or not:\n\nI am running exactly the same bechmark on n1-highcpu-16 (was using n1-highcpu-4 before) and the queue is not draining... Not done after all.. The queue drained after about 6 hours...\n\nInvestigating further.... This is with the current erl flags, no changes in that respect. In our deployments, since hosts are dedicated to RabbitMQ, we are able to achieve maximum efficiency if we only run publishing channels, meaning no queues, just exchanges (see attached screenshot). I'm profiling rabbit_amqqueue_process with eflame, still early days.\n\n. @michaelklishin would it be worth writing a blog post about this? After all: https://www.rabbitmq.com/blog/2014/04/10/consumer-bias-in-rabbitmq-3-3/. @michaelklishin Like so?. Great improvement!\nPage out behaviour before this change:\n\n\nPage out behaviour after this change:\n\nThis is what used to happen when filling & draining a queue (no paging to disk):\n\n\nThis is what happens now when filling & draining a queue (no paging to disk):\n\n\n. A rabbitmqctl shutdown can fail due to the following reasons:\n\nErlang cookie does not match\nRabbitMQ is booting\nRabbitMQ is stopping\nErlang VM is crashing (in some cases, it was observed that this can take many hours)\nThe Erlang VM is running but  the distribution is not working correctly - this happens more often than you think in hostile environments, like almost every enterprise, multi-tenant RabbitMQ cluster.\n\nIn all the above scenarios, the thing that manages RabbitMQ fails because it believes that RabbitMQ is stopped (i.e. shutdown succeeded) while in actual fact the beam process is still running.\nHow should we communicate to the thing that manages RabbitMQ that the Erlang VM failed to stop?. This means that rabbitmqctl shutdown can fail, since the exit code will not be 0. We need to reach team consensus on this matter before we move on. (cc @lukebakken & @hairyhum)\nA few points to remember:\n we already use exit codes from sysexits (3) (cc @dumbbell)\n the original behaviour of rabbitmqctl shutdown is still captured in the rabbitmqctl docs. We should discuss if we've learned that this is no longer correct. It might help to refer to the original context in #142699247 & #142699191. Without this, killing rabbit_sup app results in 500 errors on the Management UI. It also leaves the Erlang node running, heart-beating as if nothing happened. This definitely addresses the issues that we've been seeing in the wild.. Following #1378, we have observed that when using PerfTest, always prioritising consumers results in publishers being blocked most of the time. This is less than ideal. We have settled on making consumers 2x as fast as publishers.\nNo consumer bias\n\n\n2x consumer bias\n\n. \ud83c\udf89 . Maybe interesting for #1492. Having talked to @essen about this scenario, this is what we're thinking:\n\nUsers should be able to delete queues in an undefined state via the Management UI. The DELETE /api/queues/vhost/name  API endpoint should do the right thing and delete the queue even if the queue is in an undefined state.\nAMQP queue.delete operation should do the right thing and delete the queue even if the queue is in an undefined state.\nWhen this state is reached, Mirrored queue 'manualsync-non_durable-2018.Feb.09-08:57:11.185' in vhost '/': Stopping all nodes on master shutdown since no synchronised slave is available, we should also clean up the queue row from rabbit_queue Mnesia table\nTo guard against unclean shutdowns that do not get a chance of running the above step, on node startup, we should delete all non-durable queues for which no processes exist. This is a bit tricky since it might put too much pressure on Mnesia and will result in longer boot times. We are thinking to leave this step out and implement the first 3. If this issue is still mentioned, maybe consider implementing this step at that point in time.\n\nWhat do you think?. Is it sensible to expect that when the node running the master shuts down, and there is no synchronised slave for a non-durable & auto-delete queue, that queue will be deleted and all messages will be lost?\nTo address your concern of the master coming back up quickly, maybe this operation could run on the node that used to be master and is now booting, meaning that it's either recovering from a crash or simply starting up normally, and therefore it was not partitioned.. OK, that makes sense. What are your thoughts on promoting out-of-sync mirrors to masters? Should this be a manual operation? What information should the users have in order to make the best decision?\nTo me it makes sense to automatically promote the slave with the most messages, or the oldest slave if the number of messages is the same. What do others think?. Excellent! Should we promote this option by adding it to Add / update a policy > Definition > HA ?\n\n. I've added the ha-promote-on-shutdown: always option to https://ghost-queues.gcp.rabbitmq.com/ (user: demo & pass: demo), we do not expect anymore ghost queues for ^manualsync-non_durable queues.. I agree. As long as we list HA promote on shutdown ? in the HA list of options, I'm happy.. Yes, that sounds right to me. It's basically the first 2 points that I mentioned:\n\nUsers should be able to delete queues in an undefined state via the Management UI. The DELETE /api/queues/vhost/name API endpoint should do the right thing and delete the queue even if the queue is in an undefined state.\nAMQP queue.delete operation should delete the queue even if the queue is in an undefined state.. When rmq0 restarts and wants to sync rabbit_durable_route table, Mnesia running on rmq1 cannot send it because this table has an exclusive write lock - read locks cannot be claimed. This table is being cleaned up by first selecting all entries in the table, and then deleting the matching entry - there is only 1. Reading the entire table just to find and delete this 1 entry is highly inefficient when the table has many entries. We will start by making this cleanup operation more efficient.\n\nerlang\n              {mnesia_info,\"Restarting transaction ~w: in ~wms ~w~n\",\n                  [{tid,1936542,<6502.25051.32>},\n                   31742,\n                   {cyclic,'rabbit@rmq1-mnesia-contention',\n                       {rabbit_durable_route,'______WHOLETABLE_____'},\n                       write,write,\n                       {tid,1936530,<6502.27196.42>}}]}}\nProcess that loads all rabbit_durable_route entries to delete 1 entry - <6502.27196.42>\n\n\nMnesia controller worker process that continuously restarts its transaction because of the above process' write lock on rabbit_durable_route table  - <6502.25051.32>\n\nmnesia_controller that manages the above process\n\n\n. Yes, we've come across some helpful comments that confirmed our observations and suspicions.\nWe ended up with a couple of improvements, yet to be committed. You can observe the behaviour of these improvements in https://mnesia-contention.gcp.rabbitmq.com/ (user demo & pass demo). In this environment, nodes are set to restart every 10 minutes in an alternating fashion (e.g. restart rmq0, wait 10 minutes, restart rmq1, wait 10 minutes etc.). With our current changes, nodes restart faster, but there is still a noticeable locking behaviour on the entire cluster - more work to be done.. Of course, I wouldn't expect it any other way: ship early and often \ud83d\ude80 . We tried merging merge-before-1525 into master with @dumbbell, but couldn't figure out how to resolve this merge conflict - can you help @michaelklishin?\n```\ntest/unit_inbroker_parallel_SUITE.erl\n<<<<<<< HEAD\n    try\n        rabbit_auth_backend_internal:user_login_authentication(Username,\n                                                              [{key, <<\"value\">>}]),\n        ?assert(false)\n    catch exit:{unknown_auth_props, Username, [{key, <<\"value\">>}]} ->\n            ok\n    end,\n||||||| merged common ancestors\nuser_management(Config) ->\n    passed = rabbit_ct_broker_helpers:rpc(Config, 0,\n      ?MODULE, user_management1, [Config]).\nuser_management1(_Config) ->\n%% lots if stuff that should fail\n{error, {no_such_user, _}} =\n    control_action(delete_user,\n      [\"user_management-user\"]),\n{error, {no_such_user, _}} =\n    control_action(change_password,\n      [\"user_management-user\", \"user_management-password\"]),\n{error, {no_such_vhost, _}} =\n    control_action(delete_vhost,\n      [\"/user_management-vhost\"]),\n{error, {no_such_user, _}} =\n    control_action(set_permissions,\n      [\"user_management-user\", \".*\", \".*\", \".*\"]),\n{error, {no_such_user, _}} =\n    control_action(clear_permissions,\n      [\"user_management-user\"]),\n{error, {no_such_user, _}} =\n    control_action(list_user_permissions,\n      [\"user_management-user\"]),\n{error, {no_such_vhost, _}} =\n    control_action(list_permissions, [],\n      [{\"-p\", \"/user_management-vhost\"}]),\n{error, {invalid_regexp, _, _}} =\n    control_action(set_permissions,\n      [\"guest\", \"+foo\", \".*\", \".*\"]),\n{error, {no_such_user, _}} =\n    control_action(set_user_tags,\n      [\"user_management-user\", \"bar\"]),\n\n%% user creation\nok = control_action(add_user,\n  [\"user_management-user\", \"user_management-password\"]),\n{error, {user_already_exists, _}} =\n    control_action(add_user,\n      [\"user_management-user\", \"user_management-password\"]),\nok = control_action(clear_password,\n  [\"user_management-user\"]),\nok = control_action(change_password,\n  [\"user_management-user\", \"user_management-newpassword\"]),\n\nTestTags = fun (Tags) ->\n                   Args = [\"user_management-user\" | [atom_to_list(T) || T <- Tags]],\n                   ok = control_action(set_user_tags, Args),\n                   {ok, #internal_user{tags = Tags}} =\n                       rabbit_auth_backend_internal:lookup_user(\n                         <<\"user_management-user\">>),\n                   ok = control_action(list_users, [])\n           end,\nTestTags([foo, bar, baz]),\nTestTags([administrator]),\nTestTags([]),\n\n%% user authentication\nok = control_action(authenticate_user,\n  [\"user_management-user\", \"user_management-newpassword\"]),\n{refused, _User, _Format, _Params} =\n    control_action(authenticate_user,\n      [\"user_management-user\", \"user_management-password\"]),\n\n%% vhost creation\nok = control_action(add_vhost,\n  [\"/user_management-vhost\"]),\n{error, {vhost_already_exists, _}} =\n    control_action(add_vhost,\n      [\"/user_management-vhost\"]),\nok = control_action(list_vhosts, []),\n\n%% user/vhost mapping\nok = control_action(set_permissions,\n  [\"user_management-user\", \".*\", \".*\", \".*\"],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(set_permissions,\n  [\"user_management-user\", \".*\", \".*\", \".*\"],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(set_permissions,\n  [\"user_management-user\", \".*\", \".*\", \".*\"],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(list_permissions, [],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(list_permissions, [],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(list_user_permissions,\n  [\"user_management-user\"]),\n\n%% user/vhost unmapping\nok = control_action(clear_permissions,\n  [\"user_management-user\"], [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(clear_permissions,\n  [\"user_management-user\"], [{\"-p\", \"/user_management-vhost\"}]),\n\n%% vhost deletion\nok = control_action(delete_vhost,\n  [\"/user_management-vhost\"]),\n{error, {no_such_vhost, _}} =\n    control_action(delete_vhost,\n      [\"/user_management-vhost\"]),\n\n%% deleting a populated vhost\nok = control_action(add_vhost,\n  [\"/user_management-vhost\"]),\nok = control_action(set_permissions,\n  [\"user_management-user\", \".*\", \".*\", \".*\"],\n  [{\"-p\", \"/user_management-vhost\"}]),\n{new, _} = rabbit_amqqueue:declare(\n             rabbit_misc:r(<<\"/user_management-vhost\">>, queue,\n                           <<\"user_management-vhost-queue\">>),\n             true, false, [], none),\nok = control_action(delete_vhost,\n  [\"/user_management-vhost\"]),\n\n%% user deletion\nok = control_action(delete_user,\n  [\"user_management-user\"]),\n{error, {no_such_user, _}} =\n    control_action(delete_user,\n      [\"user_management-user\"]),\n\n=======\nuser_management(Config) ->\n    passed = rabbit_ct_broker_helpers:rpc(Config, 0,\n      ?MODULE, user_management1, [Config]).\nuser_management1(_Config) ->\n%% lots if stuff that should fail\n{error, {no_such_user, _}} =\n    control_action(delete_user,\n      [\"user_management-user\"]),\n{error, {no_such_user, _}} =\n    control_action(change_password,\n      [\"user_management-user\", \"user_management-password\"]),\n{error, {no_such_vhost, _}} =\n    control_action(delete_vhost,\n      [\"/user_management-vhost\"]),\n{error, {no_such_user, _}} =\n    control_action(set_permissions,\n      [\"user_management-user\", \".*\", \".*\", \".*\"]),\n{error, {no_such_user, _}} =\n    control_action(clear_permissions,\n      [\"user_management-user\"]),\n{error, {no_such_user, _}} =\n    control_action(list_user_permissions,\n      [\"user_management-user\"]),\n{error, {no_such_vhost, _}} =\n    control_action(list_permissions, [],\n      [{\"-p\", \"/user_management-vhost\"}]),\n{error, {invalid_regexp, _, _}} =\n    control_action(set_permissions,\n      [\"guest\", \"+foo\", \".*\", \".*\"]),\n{error, {no_such_user, _}} =\n    control_action(set_user_tags,\n      [\"user_management-user\", \"bar\"]),\n\n%% user creation\nok = control_action(add_user,\n  [\"user_management-user\", \"user_management-password\"]),\n{error, {user_already_exists, _}} =\n    control_action(add_user,\n      [\"user_management-user\", \"user_management-password\"]),\nok = control_action(clear_password,\n  [\"user_management-user\"]),\nok = control_action(change_password,\n  [\"user_management-user\", \"user_management-newpassword\"]),\n\nTestTags = fun (Tags) ->\n                   Args = [\"user_management-user\" | [atom_to_list(T) || T <- Tags]],\n                   ok = control_action(set_user_tags, Args),\n                   {ok, #internal_user{tags = Tags}} =\n                       rabbit_auth_backend_internal:lookup_user(\n                         <<\"user_management-user\">>),\n                   ok = control_action(list_users, [])\n           end,\nTestTags([foo, bar, baz]),\nTestTags([administrator]),\nTestTags([]),\n\n%% user authentication\nok = control_action(authenticate_user,\n  [\"user_management-user\", \"user_management-newpassword\"]),\n?assertMatch({refused, _User, _Format, _Params},\n    control_action(authenticate_user,\n      [\"user_management-user\", \"user_management-password\"])),\n?assertMatch({refused, _User, _Format, _Params},\n    control_action(authenticate_user,\n      [\"user_management-user\", \"\"])),\n\n%% vhost creation\nok = control_action(add_vhost,\n  [\"/user_management-vhost\"]),\n{error, {vhost_already_exists, _}} =\n    control_action(add_vhost,\n      [\"/user_management-vhost\"]),\nok = control_action(list_vhosts, []),\n\n%% user/vhost mapping\nok = control_action(set_permissions,\n  [\"user_management-user\", \".*\", \".*\", \".*\"],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(set_permissions,\n  [\"user_management-user\", \".*\", \".*\", \".*\"],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(set_permissions,\n  [\"user_management-user\", \".*\", \".*\", \".*\"],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(list_permissions, [],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(list_permissions, [],\n  [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(list_user_permissions,\n  [\"user_management-user\"]),\n\n%% user/vhost unmapping\nok = control_action(clear_permissions,\n  [\"user_management-user\"], [{\"-p\", \"/user_management-vhost\"}]),\nok = control_action(clear_permissions,\n  [\"user_management-user\"], [{\"-p\", \"/user_management-vhost\"}]),\n\n%% vhost deletion\nok = control_action(delete_vhost,\n  [\"/user_management-vhost\"]),\n{error, {no_such_vhost, _}} =\n    control_action(delete_vhost,\n      [\"/user_management-vhost\"]),\n\n%% deleting a populated vhost\nok = control_action(add_vhost,\n  [\"/user_management-vhost\"]),\nok = control_action(set_permissions,\n  [\"user_management-user\", \".*\", \".*\", \".*\"],\n  [{\"-p\", \"/user_management-vhost\"}]),\n{new, _} = rabbit_amqqueue:declare(\n             rabbit_misc:r(<<\"/user_management-vhost\">>, queue,\n                           <<\"user_management-vhost-queue\">>),\n             true, false, [], none),\nok = control_action(delete_vhost,\n  [\"/user_management-vhost\"]),\n\n%% user deletion\nok = control_action(delete_user,\n  [\"user_management-user\"]),\n{error, {no_such_user, _}} =\n    control_action(delete_user,\n      [\"user_management-user\"]),\n\n\n\n\n\n\n\n\nmerge-before-1525\n\n\n\n\n\n\n\nok = rabbit_auth_backend_internal:delete_user(Username, <<\"acting-user\">>),\n\n<<<<<<< HEAD\n||||||| merged common ancestors\nruntime_parameters(Config) ->\n    passed = rabbit_ct_broker_helpers:rpc(Config, 0,\n      ?MODULE, runtime_parameters1, [Config]).\nruntime_parameters1(Config) ->\n    dummy_runtime_parameters:register(),\n    Good = fun(L) -> ok                = control_action(set_parameter, L) end,\n    Bad  = fun(L) -> {error_string, } = control_action(set_parameter, L) end,\n%% Acceptable for bijection\nGood([\"test\", \"good\", \"\\\"ignore\\\"\"]),\nGood([\"test\", \"good\", \"123\"]),\nGood([\"test\", \"good\", \"true\"]),\nGood([\"test\", \"good\", \"false\"]),\nGood([\"test\", \"good\", \"null\"]),\nGood([\"test\", \"good\", \"{\\\"key\\\": \\\"value\\\"}\"]),\n\n%% Invalid json\nBad([\"test\", \"good\", \"atom\"]),\nBad([\"test\", \"good\", \"{\\\"foo\\\": \\\"bar\\\"\"]),\nBad([\"test\", \"good\", \"{foo: \\\"bar\\\"}\"]),\n\n%% Test actual validation hook\nGood([\"test\", \"maybe\", \"\\\"good\\\"\"]),\nBad([\"test\", \"maybe\", \"\\\"bad\\\"\"]),\nGood([\"test\", \"admin\", \"\\\"ignore\\\"\"]), %% ctl means 'user' -> none\n\nok = control_action(list_parameters, []),\n\nok = control_action(clear_parameter, [\"test\", \"good\"]),\nok = control_action(clear_parameter, [\"test\", \"maybe\"]),\nok = control_action(clear_parameter, [\"test\", \"admin\"]),\n{error_string, _} =\n    control_action(clear_parameter, [\"test\", \"neverexisted\"]),\n\n%% We can delete for a component that no longer exists\nGood([\"test\", \"good\", \"\\\"ignore\\\"\"]),\ndummy_runtime_parameters:unregister(),\nok = control_action(clear_parameter, [\"test\", \"good\"]),\n\n=======\n%% -------------------------------------------------------------------\n%% rabbit_auth_backend_internal\n%% -------------------------------------------------------------------\nlogin_with_credentials_but_no_password(Config) ->\n    passed = rabbit_ct_broker_helpers:rpc(Config, 0,\n      ?MODULE, login_with_credentials_but_no_password1, [Config]).\nlogin_with_credentials_but_no_password1(_Config) ->\n    Username = \"login_with_credentials_but_no_password-user\",\n    Password = \"login_with_credentials_but_no_password-password\",\n    ok = control_action(add_user, [Username, Password]),\ntry\n    rabbit_auth_backend_internal:user_login_authentication(Username,\n                                                          [{key, <<\"value\">>}]),\n    ?assert(false)\ncatch exit:{unknown_auth_props, Username, [{key, <<\"value\">>}]} ->\n        ok\nend,\n\nok = control_action(delete_user,\n  [Username]),\n\npassed.\n\n%% passwordless users are not supposed to be used with\n%% this backend (and PLAIN authentication mechanism in general)\nlogin_of_passwordless_user(Config) ->\n    passed = rabbit_ct_broker_helpers:rpc(Config, 0,\n      ?MODULE, login_of_passwordless_user1, [Config]).\nlogin_of_passwordless_user1(_Config) ->\n    Username = \"login_of_passwordless_user-user\",\n    Password = \"\",\n    ok = control_action(add_user, [Username, Password]),\n?assertMatch(\n   {refused, _Message, [Username]},\n   rabbit_auth_backend_internal:user_login_authentication(Username,\n                                                          [{password, <<\"\">>}])),\n\n?assertMatch(\n   {refused, _Format, [Username]},\n   rabbit_auth_backend_internal:user_login_authentication(Username,\n                                                          [{password, \"\"}])),\n\nok = control_action(delete_user,\n  [Username]),\n\npassed.\n\nruntime_parameters(Config) ->\n    passed = rabbit_ct_broker_helpers:rpc(Config, 0,\n      ?MODULE, runtime_parameters1, [Config]).\nruntime_parameters1(Config) ->\n    dummy_runtime_parameters:register(),\n    Good = fun(L) -> ok                = control_action(set_parameter, L) end,\n    Bad  = fun(L) -> {error_string, } = control_action(set_parameter, L) end,\n%% Acceptable for bijection\nGood([\"test\", \"good\", \"\\\"ignore\\\"\"]),\nGood([\"test\", \"good\", \"123\"]),\nGood([\"test\", \"good\", \"true\"]),\nGood([\"test\", \"good\", \"false\"]),\nGood([\"test\", \"good\", \"null\"]),\nGood([\"test\", \"good\", \"{\\\"key\\\": \\\"value\\\"}\"]),\n\n%% Invalid json\nBad([\"test\", \"good\", \"atom\"]),\nBad([\"test\", \"good\", \"{\\\"foo\\\": \\\"bar\\\"\"]),\nBad([\"test\", \"good\", \"{foo: \\\"bar\\\"}\"]),\n\n%% Test actual validation hook\nGood([\"test\", \"maybe\", \"\\\"good\\\"\"]),\nBad([\"test\", \"maybe\", \"\\\"bad\\\"\"]),\nGood([\"test\", \"admin\", \"\\\"ignore\\\"\"]), %% ctl means 'user' -> none\n\nok = control_action(list_parameters, []),\n\nok = control_action(clear_parameter, [\"test\", \"good\"]),\nok = control_action(clear_parameter, [\"test\", \"maybe\"]),\nok = control_action(clear_parameter, [\"test\", \"admin\"]),\n{error_string, _} =\n    control_action(clear_parameter, [\"test\", \"neverexisted\"]),\n\n%% We can delete for a component that no longer exists\nGood([\"test\", \"good\", \"\\\"ignore\\\"\"]),\ndummy_runtime_parameters:unregister(),\nok = control_action(clear_parameter, [\"test\", \"good\"]),\n\n\n\n\n\n\n\n\nmerge-before-1525\n```. Testing now with 3 nodes, each running 100k connections, 100k channels & 50k queues.. We have finished testing 3 nodes with 100k connections, 100k channels & 50k queues each. After all connections were established and queues created:\n\n\n\n\n\n\n\nHow long does it take for rabbit:stop/0 to return?\n76 seconds\nWhat is the queue delete rate?\n|  | Per second |\n|---|---|\n| Node stopping | ~100 |\n| Node stopped | 760, 688, 600, 447, 243 |\nIt's interesting that the fewer queues remain, the slower the queue delete rate. We are assuming that this is due to both running nodes attempting to delete the same queues since rabbit_amqqueue:on_node_down/1 runs on all the remaining nodes in the cluster.\nHow long does it take for GET /api/nodes to return?\n|  | Duration |\n|---|---|\n| Node stopping | 40ms - 70ms |\n| Node stopped | 5000ms  |\nAfter a node stops and the remaining nodes start running rabbit_node_monitor:handle_dead_rabbit/2, they fail to respond to rabbit_node_monitor:partitions/1 calls. This results in the call to the gen_server timing out after 5s (introduced in this PR) meaning that Management UI shows stale node information. While 50k queues are being cleaned up by 2 nodes concurrently, information about them is not returned for ~90s.\nSince the overview page has 2 XHR requests, GET /api/nodes & GET /api/overview and they both timeout after 5s, the UI refreshes every 15s instead of every 5s. This is preferred to it not refreshing at all while a node runs rabbit_node_monitor:handle_dead_rabbit/2.. Steps to test this fix:\n\nGiven there are 3 nodes in the cluster with 100k connections, 100k channels & 50k queues each (start clients on one node at a time, otherwise rabbit_event will get overloaded)\nStop node 1\nQueues are being deleted slowly while the node is stopping (~100 ops/s)\nNode 1 stops\nQueues will start deleting faster, ~760 ops/s\nAs fewer queues remain, ops slow down by ~100 ops/s every 15 seconds, until eventually reaching 250 ops/s\nManagement UI refreshes slower (once every 15s) and the node information is stale, but global counts and totals are accurate and Management UI is usable\n50k queues will be cleaned after 90s\n\nNext, we would would like to test what happens when the stopped node gets started while the cleanup is in progress.\nAs a last test, we would like to capture the timings when there are only 2 nodes in the cluster (vs 3 nodes).. Addressing rabbit_node_monitor:handle_dead_rabbit/2 is a minor concern compared to the rabbit_event behaviour. We agree that both these optimisations are out of scope.\nFor rabbit_event, we have observed the process using up to 12GB of memory and having ~9 million messages in its mailbox when clients were opening connections and declaring queues on all 3 nodes at the same time. The clients were timing out in queue.declare and it was almost impossible to reach the full load. We ended up starting clients on 1 node at a time. We will not look into addressing this within this PR.. While testing the queue cleanup in a 3-node cluster running Erlang 20.3.1, we've managed to trigger segfaults in both nodes running rabbit_node_monitor:handle_dead_rabbit/2 at roughly the same time, twice in a row. We didn't have core dumps enabled, this is all that was logged:\n```\nrmq1-mnesia-contention\n2018-03-20T16:52:37.825216+00:00 localhost kernel: [31235.399444] 15_scheduler[5096]: segfault at 7f0891db4ee8 ip 00000000005613d7 sp 00007f0891db4ed0 error 6 in beam.smp[400000+33b000]\n...\n2018-03-20T17:20:13.909216+00:00 localhost kernel: [32891.429912] 8_scheduler[8102]: segfault at 7faf17cfaee8 ip 00000000005613d7 sp 00007faf17cfaed0 error 6 in beam.smp[400000+33b000]\n```\n```\nrmq2-mnesia-contention\n2018-03-20T16:52:43.051532+00:00 localhost kernel: [31240.951173] 8_scheduler[1202]: segfault at 7fafc8ebaee8 ip 00000000005613d7 sp 00007fafc8ebaed0 error 6 in beam.smp[400000+33b000]\n...\n2018-03-20T17:20:01.651463+00:00 localhost kernel: [32879.398546] 15_scheduler[4598]: segfault at 7f602c1f4ee8 ip 00000000005613d7 sp 00007f602c1f4ed0 error 6 in beam.smp[400000+33b000]\n```\nWe want to enable core dumps - rabbitmq/rabbitmq-server-boshrelease#20 - so that we can understand the issue better, and maybe raise with the OTP team.\nWe suspect that these segfaults are either related to Mnesia since this component is under most pressure, or the new ageffcbf memory allocation strategy that we've been using in today's tests.. We seem to have come across a legit Erlang bug: ERL-592. This also seems to affect 20.1.3, possibly other versions.\nThe PR isn't complete just yet, we want to make sure that this works correctly when multiple nodes perform the cleanup operation. The bug feels related to rabbitmq/rabbitmq-common#258, we're testing an alternative implementation. Your hunch to be suspicious was correct \ud83d\ude09 . ## Host specs\n| | |\n| - | - |\n| CPU | 16 x Intel(R) Xeon(R) @ 2.60GHz |\n| RAM | 60GB DDR3 |\n| DISK | 100GB SSD |\n| NET | 32Gbit/s |\nRabbitMQ client specs\n\nRuby 2.4.3\nBunny 2.9.1\n\nRabbitMQ deployment configuration\nTODO\nBenchmark profile\n| Description | per node | per cluster |\n| - | - | - |\n| RabbitMQ nodes | - | 3 |\n| Connections | 100k | 300k |\n| Channels | 100k | 300k |\n| Queues | 50k | 150k |\nBefore & after on Erlang 20.3.2\n| Description | 3.7.4 | 3.7.5-pr.1526 |\n| - | - | - |\n| Time to achieve full load when loading 1st node | 155s | 165s |\n| Time for rabbit_event to finish processing all messages in its mailbox on 1st node | 155s + 35s | 165s + 35s |\n| rabbit_event memory use after it processes all messages in its mailbox | 4GB - 13GB | 4KB |\n| Time to achieve full load when loading 2nd node | 185s | 205s |\n| Time for rabbit_event to finish processing all messages in its mailbox on 2nd node | 185s + 30s | 205s + 30s |\n| Time to achieve full load when loading 3rd node | 235s | 265s |\n| Time for rabbit_event to finish processing all messages in its mailbox on 3rd node | 235s + 30s | 265s + 40s |\n| Time to achieve full load when loading first 2 nodes at once | TODO | 360s (continuation timeout increased to 60s) |\n| Time to achieve full load when loading all 3 nodes at once | TODO | 620s (continuation timeout increased to 60s) |\n| Time to stop 1 out of 3 nodes | 75s | 80s |\n| Time for 2 nodes to clean up when 1 node goes away | gave up after waiting 1800s | 95s |\n| Time to stop 2 out of 3 nodes | TODO | 90s |\n| Time for 1 nodes to clean up when 2 nodes go away | gave up after waiting 1800s | 50s |\n| Time for 1 stopped node to start & re-join the other 2 fully loaded nodes, after all its queues were cleaned up | TODO | 15s |\n| Time for 2 stopped nodes to start & re-join the remaining fully loaded nodes, after all their queues were cleaned up | TODO | 15s |\n| Time for 1 node to restart & re-join the other 2 fully loaded nodes, as these are cleaning up | TODO TODO stopTODO start | 285s 75s stop210s start |\n| Time for 2 nodes to restart & re-join the remaining fully loaded node, as this is cleaning up | TODO TODO stopTODO start | 270s 80s stop190s start |\n| Time before Management API starts reporting the node status for the 2 nodes that are cleaning up after 1 node that went away | N/A | 195s |\n| Time before Management API responds | gave up after waiting 1800s | N/A |\n| Memory use for a fully loaded node | 15GB - 16GB | 15GB - 16GB |\n| Memory use for a node that was fully loaded for 24 hours | 23GB - 29GB | 23GB |\nBefore & after on Erlang 19.3.6.7\n| Description | 3.7.4 | 3.7.5-pr.1526 |\n| - | - | - |\n| Time to achieve full load when loading 1st node | TODO | 170s |\n| Time for rabbit_event to finish processing all messages in its mailbox on 1st node | TODO | 170s + 40s |\n| rabbit_event memory use after it processes all messages in its mailbox | TODO | 4GB - 5GB |\n| Time to achieve full load when loading 2nd node | TODO | 175s |\n| Time for rabbit_event to finish processing all messages in its mailbox on 2nd node | TODO | 175 + 25s |\n| Time to achieve full load when loading 3rd node | TODO | 200s |\n| Time for rabbit_event to finish processing all messages in its mailbox on 3rd node | TODO | 200s + 35s |\n| Time to achieve full load when loading first 2 nodes at once | TODO |TODO|\n| Time to achieve full load when loading all 3 nodes at once | TODO | TODO |\n| Time to stop 1 out of 3 nodes | TODO | 85s |\n| Time for 2 nodes to clean up when 1 node goes away | gave up after waiting 1800s | 75s |\n| Time to stop 2 out of 3 nodes | TODO | TODO |\n| Time for 1 nodes to clean up when 2 nodes go away | gave up after waiting 1800s | TODO |\n| Time for 1 stopped node to start & re-join the other 2 fully loaded nodes, after all its queues were cleaned up | TODO | TODO |\n| Time for 2 stopped nodes to start & re-join the remaining fully loaded nodes, after all their queues were cleaned up | TODO | TODO |\n| Time for 1 node to restart & re-join the other 2 fully loaded nodes, as these are cleaning up | TODO TODO stopTODO start | 285s 75s stop210s start |\n| Time for 2 nodes to restart & re-join the remaining fully loaded node, as this is cleaning up | TODO TODO stopTODO start | 270s 80s stop190s start |\n| Time before Management API starts reporting the node status for the 2 nodes that are cleaning up after 1 node that went away | N/A | 195s |\n| Time before Management API responds | gave up after waiting 1800s | N/A |\n| Memory use for a fully loaded node | TODO | 32GB - 37GB |\n| Memory use for a node that was fully loaded for 24 hours | TODO | TODO |\nObservations\n\nThe more records exist in Mnesia, the longer it takes to add new records. The 2nd node takes 35s longer to become fully loaded after the 1st node is fully loaded, and the 3rd node takes 55s longer than the 2nd node.\nrabbit_event is now ~10% slower, since it has to scan all referenced items on every GC run, but it releases all memory when its done processing the mailbox, vs referencing GBs of unused memory.\nEven though the Erlang VM has allocated 23GB of memory, and rabbitmq_event is using 4.2GB, the kernel reports the RSS as 19GB. We don't understand why, but we want to wrap up this PR and investigate memory allocators separately.\n\nReminders\n\nmeasure rabbit_event when it consumes all messages, not when it GCs - it doesn't GC in 3.7.4. This PR & https://github.com/rabbitmq/rabbitmq-common/pull/258 are ready to review & merge.\n\nThe only remaining thing for us to do is to benchmark 3.7.4 next. We do not expect the benchmark to complete.. @lukebakken can you retry your load profile without the fullsweep_after optimisation in rabbit_event.erl? https://github.com/rabbitmq/rabbitmq-common/pull/258/files#diff-a88222ce6107ede0841395c2ba6cb963\n@michaelklishin yay for CI!. We've addressed ets:select_replace/2 & gen_event:start_link/2, the changes in rabbit_common are now compatible with Erlang 19.3, this PR is ready for QA once again.\nOne surprising aspect of testing against Erlang 19.3.6.7 is the difference in the memory use. We were forced to change the memory allocation strategy for the eheap_alloc from ageffcbf to aoffcbf (the former was introduced in Erlang 20.2.3). Node memory usage went from 16GB to 32GB once fully loaded, and after a few hours it reached 41GB. Since we are meant to investigate memory allocators next, we are just mentioning this unexpected find.. :shipit: . @lukebakken interesting. I think the improvements are pretty good as it stands, but since connections are @essen's favourite area, I'm happy to indulge a quick fly-by if he's up for it.. Proposing to add this to v3.6.x & v3.8.x. Yes, I was actually thinking about RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS and thought that it was weird to have some erl properties such as +zddbl & +stbt configurable explicitly, and others such as +P & +t only configurable as overrides via $RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS.. The BOSH release supports both RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS & specific properties, so it's not an issue - both approaches will yield the same result. This PR is driven by making all properties in SERVER_ERL_ARGS equally configurable.. We used to have the RABBITMQ_ prefix since #873. The sudo logic used to be:\nif RABBITMQ_SCHEDULER_BIND_TYPE env is set\nthen use this value in SERVER_ERL_ARGS\notherwise set RABBITMQ_SCHEDULER_BIND_TYPE to DEFAULT_SCHEDULER_BIND_TYPE\nWas this incorrect?. @michaelklishin got it. So we basically need to support both RABBITMQ_var_name & var_name defined in rabbitmq-env.conf. I will update the tests to cover both cases.. @michaelklishin should be ready to merge.. @lukebakken this comment applies here as well. Pretty sure the original env vars which started with RABBITMQ_ were correct.. Just to address a point raised in an ad-hoc discussion, this solution will not automatically delete queues, this action needs to happen explicitly.. Great catch, we totally missed this. Back at it next Tuesday \u26d1 . Ta for the merge! I suppose that we are expecting for the pipeline to pass for master before we merge this into v3.7.x. I can see that build-test-package-debian#128 failed, meaning that no alpha artefact was produced from this merge.\nAnything we can do to help?\ncc @dumbbell . I see what you mean. What are your plans on getting it un-stuck? Anything I can do to help?. That is excellent news! I'm waiting for a build to go through the pipeline so that I can share the artefacts. Let me know if there is anything that I can help with!. I could definitely watch this Concourse TV Show \ud83c\udf7f \n3.7.5-alpha.23 is missing some commits that made it into master but not v3.7.x. Let's chat in person, this is turning into a high latency game of ping-pong comments.. Thank you for the ping @michaelklishin, great resolution, fully agree with @dumbbell . I've noticed that connection_max defaults to infinity. Would it make sense to pick a sane default here as well, possibly the same one?. OK, in which case I can see 2 options:\n\nDefault connection_max to 100000\nLeave connection_max unchanged and rely on the OS file descriptor limit, even if this is shared between actual files (I'm thinking queue segments) and TCP sockets.\n\nThis is what other stateful services are doing:\n PostgreSQL has  max_connections with a default of 100\n Cassandra has native_transport_max_concurrent_connections with a default of -1\n IBM MQ has TCPChannels with a default of 200\n ActiveMQ has maximumConnections with a default of 2^31-1. This is very cool, looking forward to it shipping!. Started testing this against various workloads, all running the following:\n\nn1-highcpu-8\nUbuntu 16.04.4 LTS  4.15.0-22-generic x86_64\nErlang/OTP 20.3.6\nRabbitMQ v3.7.5\nperf-test v2.1.2\n\nndq-autoack\n|  | Before | After | Difference |\n|-|-|-|-|\n| 1KB Messages/s, 5min avg | 56k | 54k | -3% |\n| RSS, 5min avg | 121 MiB | 112 MiB | -7% |\n| GC/s, 5min avg | 27K | 33K | +22% |\n| Run queue | ~2 | ~2 | N/A |\n\nndq-multiack\n|  | Before | After | Difference |\n|-|-|-|-|\n| 1KB Messages/s, 5min avg | 53k | 54k | +2% |\n| RSS, 5min avg | 125 MiB | 111 MiB | -11% |\n| GC/s, 5min avg | 26K | 26K | 0% |\n| Run queue | ~1 | ~1 | N/A |\n\nndq-10q-multiack\nWe are trading off throughput for memory: we use 25% less memory at the cost of 6% less throughput.\n|  | Before | After | Difference |\n|-|-|-|-|\n| 1KB Messages/s, 5min avg | 84k | 79k | -6% |\n| RSS, 5min avg | 271 MiB | 203 MiB | -25% |\n| GC/s, 5min avg | 39K | 30K | -23% |\n| Run queue | ~30 | ~30 | N/A |\n\nndq-20q-multiack\nIf we double the number of queues, connections & channels, we observe a similar hit on throughput, but with a lower memory saving: we use 17% less memory at the cost of 5% less throughput.\n|  | Before | After | Difference |\n|-|-|-|-|\n| 1KB Messages/s, 5min avg | 84k | 80k | -5% |\n| RSS, 5min avg | 257 MiB | 213 MiB | -17% |\n| GC/s, 5min avg | 33K | 30K | -9% |\n| Run queue | ~70 | ~70 | N/A |\n\nndq-50q-multiack\nIf we again double the number of queues, connections & channels, we observe a slightly higher  hit on throughput, but with an impressive memory saving: we use 47% less memory at the cost of 7% less throughput.\nNotice how the Erlang run queue increases to 180, which feels too high considering there are just 8 schedulers.\n|  | Before | After | Difference |\n|-|-|-|-|\n| 1KB Messages/s, 5min avg | 81k | 75k | -7% |\n| RSS, 5min avg | 563 MiB | 299 MiB | -47% |\n| GC/s, 5min avg | 36K | 34K | -5% |\n| Run queue | ~180 | ~180 | N/A |\n\nndq-100q-multiack\nOnce there are 100 queues, 100 producers & 100 consumers with 1 connection & 1 channel each, the throughput is almost identical, but the memory saving remains at 45%. This matches our initial observations.\n|  | Before | After | Difference |\n|-|-|-|-|\n| 1KB Messages/s, 5min avg | 76k | 75k | -1% |\n| RSS, 5min avg | 871 MiB | 471 MiB | -45% |\n| GC/s, 5min avg | 38K | 31K | -5% |\n| Run queue | ~360 | ~360 | N/A |\n\nDoes this impact raw ingress?\nWe wanted to double-check whether this affects throughput, and if so by how much. Given 10 publishers with 1 connection & 1 channel each, going at full speed, with no queue bound to the exchange, the performance degradation is 3%:\n|  | Before | After | Difference |\n|-|-|-|-|\n| 1KB Messages/s, 5min avg | 362k | 352k | -3% |\n| RSS, 5min avg | 174 MiB | 148 MiB | -15% |\n| GC/s, 5min avg | 98K | 98K | 0% |\n| Run queue | ~13 | ~13| N/A |\n. Even though we haven't benchmarked the impact of this on durable, mirrored or lazy queues, are the above benchmarks against non-durable queues sufficient to make a call whether this change is a good default?. Are we planning to merge this into 3.7.6?\nHow about 3.6.16? #1612 . I can look into that on Monday, the LRE deployments are very outdated at this point. Does anyone want to join the fun or should I Hans Solo it?. I was wondering whether it would be worth mentioning in the release notes how to go back to the previous defaults, +MBas aoffcbf +MHas aoffcbf +MBlmbcs 5120 +MHlmbcs 5120 +MMmcs 10. Worth keeping an eye on this: Node not responding to init:stop() - Erlang/OTP 21. Ouch! That was my ego, definitely not a cool mistake.\nThank you Chris for spotting this & @michaelklishin for turning it around so quickly.. @twillouer thank you for confirming that the inconsistent high CPU utilisation is present even when there is a single node in the cluster.\n@lukebakken inter-node traffic was always the same in my observations, regardless how hard the CPUs were working\n@kitsirota all queue masters are always concentrated on a single node, high CPU utilisation is not linked to queue master location. The question is why do nodes have unpredictable CPU utilisation, regardless whether they run slave or master queue processes?. Happy for @lukebakken to take this over \ud83d\udc4d . Yes, we went with ps mainly because it works the same across both Linux & BSD.\nI would prefer to go with ps to begin with, then improve the approach as/when we learn about its limitations.\nWould it be worth doing some benchmarks?. Is it acceptable to fallback to Erlang memory calculation for RabbitMQ installations on French Windows?. I believe that contextual information is highly valuable when dealing with configuration properties. I agree that guides are essential for explaining how multiple configuration properties work together, but I strongly believe that we should keep essential details alongside configuration properties, in the config file context, without requiring users to jump out.\nWhen I look at a canonical config example, I'm thinking:\n\nWhat configs exist?\nWhat do they mean?\nWhy would I want to change a config?\n\nThe Cassandra config file example captures my sentiment well, as does this less-known nginx config.\nI would really like to know what others think re the above.\nGiven the big picture, I will reduce the scope of this PR and just sync PROJECT_ENV with rabbitmq.config.example. Single lines are in order, especially since I am yet to understand what every configuration property does.. Should nodelay be removed from PROJECT_ENV? . My bad, definitely a typo.. Does BOOT_STATUS_CHECK_INTERVAL read better to you too? What are your thoughts on lowering this value to 30 seconds? Or maybe 60 seconds? 100 seconds feels awkward.. 12 hours feels super long, especially if there is no feedback during this period. Maybe it's out of scope, but it would be magnificent if we could return the current boot step, especially if there is something that can take this long.... If that is true, and Iterations = div(BOOT_START_TIMEOUT, BOOT_STATUS_SAMPLING_INTERVAL) in do_wait_for_boot_to_Start/2, then I read BOOT_START_TIMEOUT & BOOT_START_TIMEOUT to be in milliseconds, not seconds.. ",
    "PritamUpadhyay": "Version is being displayed in \"Add Or Remove Programs\" from control panel,\" \nDid you check this?\n\n. I am using node-amqp. I am building web based chat application.\nLog file:\n=ERROR REPORT==== 2-Apr-2015::12:15:25 ===\n* Generic server <0.354.0> terminating\n* Last message in was {'$gen_cast',\n                           {method,\n                               {'queue.bind',0,<<\"LoadJs_817\">>,\n                                   <<\"demo-pre\">>,\n                                   <<\"demo$817\">>,false,[]},\n                               none,noflow}}\n* When Server state == {ch,running,rabbit_framing_amqp_0_9_1,1,<0.348.0>,\n                            <0.352.0>,<0.348.0>,\n                            <<\"192.168.0.58:55677 -> 192.168.0.58:5672\">>,\n                            {lstate,<0.353.0>,false},\n                            none,1,\n                            {[],[]},\n                            {user,<<\"abc\">>,\n                                [administrator],\n                                [{rabbit_auth_backend_internal,none}]},\n                            <<\"RMQ\">>,<<\"LoadJs_817\">>,\n                            {dict,0,16,16,8,80,48,\n                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                 []},\n                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                  [],[]}}},\n                            {state,\n                                {dict,0,16,16,8,80,48,\n                                    {[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]},\n                                    {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                      [],[],[]}}},\n                                erlang},\n                            {dict,0,16,16,8,80,48,\n                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                 []},\n                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                  [],[]}}},\n                            {dict,0,16,16,8,80,48,\n                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                 []},\n                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                  [],[]}}},\n                            {set,0,16,16,8,80,48,\n                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                 []},\n                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                  [],[]}}},\n                            <0.349.0>,\n                            {state,fine,5000,#Ref<0.0.0.1918>},\n                            false,1,\n                            {{0,nil},{0,nil}},\n                            [],\n                            {{0,nil},{0,nil}},\n                            [],none,0,none}\n* Reason for termination == \n** {function_clause,\n       [{presence_exchange,deliver,\n            [{delivery,false,false,<0.354.0>,\n                 {basic_message,\n                     {resource,<<\"RMQ\">>,exchange,\n                         <<\"demo-pre\">>},\n                     [<<\"demo\">>],\n                     {content,60,\n                         {'P_basic',undefined,undefined,\n                             [{<<\"exchange\">>,longstr,\n                               <<\"demo-pre\">>}],\n                             undefined,undefined,undefined,undefined,\n                             undefined,undefined,undefined,undefined,\n                             undefined,undefined,undefined},\n                         none,none,\n                         [<<\"{\\\"action\\\":\\\"bind\\\",\\\"channel\\\":\\\"demo\\\",\\\"uuid\\\":\\\"817\\\",\\\"timestamp\\\":\\\"1427957125\\\"}\">>]},\n                     <<170,129,170,61,121,106,56,40,71,186,50,9,114,74,111,72>>,\n                     false},\n                 undefined,noflow}],\n            [{file,\"src/presence_exchange.erl\"},{line,186}]},\n        {presence_exchange,add_binding,3,\n            [{file,\"src/presence_exchange.erl\"},{line,217}]},\n        {rabbit_binding,x_callback,4,\n            [{file,\"src/rabbit_binding.erl\"},{line,561}]},\n        {rabbit_binding,'-add/3-fun-0-',3,\n            [{file,\"src/rabbit_binding.erl\"},{line,193}]},\n        {rabbit_channel,binding_action,9,\n            [{file,\"src/rabbit_channel.erl\"},{line,1471}]},\n        {rabbit_channel,handle_cast,2,\n            [{file,\"src/rabbit_channel.erl\"},{line,326}]},\n        {gen_server2,handle_msg,2,[{file,\"src/gen_server2.erl\"},{line,1034}]},\n        {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,237}]}]}.\n. Can you please elaborate:  \"You are using  presence_exchange . It needs updating for 3.5.0.\"\nWhat kind of updates do I need to make in presence_exchange ?\nAnd also can you tell me knows bugs of using node-amqp ?\n. Okay. Got it..\nThanks.\n. ",
    "CaseyBurnsSv": "That is correct, it is displayed in the Version column as expected. However, I am requesting that the version number is included in the name to simplify deployment automation.\n. ",
    "dcorbacho": "@michaelklishin This is solved in https://github.com/rabbitmq/rabbitmq-server/tree/rabbitmq-server-84\nThe original function wasn't exported, so it doesn't break any previous API. Do you want the pull request on stable or master?\n. The -on_load() directive cannot be used, as doing an external call to the module itself causes a deadlock http://www.erlang.org/doc/reference_manual/code_loading.html#id88557. This happens, at least, when trying to load the new module.\nAdding an application behaviour to rabbit_common is a very complex issue, because of rabbit startup. The scripts try to use rabbit_common without trying to initialise the application. Thus, it would require touching the release, packages, etc.\nisis:rabbit dianacorbacho$ make run-broker PLUGINS='rabbitmq_management' GEN    dist DIST   plugins/rabbit_common-0.0.0.ez\n GEN    dist\n GEN    /var/folders/5q/vkjhc27d4w9_hc69p7q3vd1w0000gp/T//rabbitmq-test-instances/rabbit/enabled_plugins\ninit terminating in do_boot ()\n{\"init terminating in do_boot\",{undef,[{erlang,monotonic_time,[],[]},{rabbit_nodes,random,1,[{file,\"src/rabbit_nodes.erl\"},{line,209}]},{rabbit_nodes,ensure_epmd,0,[{file,\"src/rabbi\nt_nodes.erl\"},{line,217}]},{rabbit_cli,start_distribution,1,[{file,\"src/rabbit_cli.erl\"},{line,133}]},{rabbit_cli,main,3,[{file,\"src/rabbit_cli.erl\"},{line,52}]},{init,start_it,1,[]\n},{init,start_em,1,[]}]}}\ninit terminating in do_boot ()\nmake: *** [/var/folders/5q/vkjhc27d4w9_hc69p7q3vd1w0000gp/T//rabbitmq-test-instances/rabbit/enabled_plugins] Error 1\nI found a much easier solution that only implies a little bit of coding:\nhttps://github.com/rabbitmq/rabbitmq-common/tree/rabbitmq-server-346\nThe time_compat module will dynamically re-load itself the first time one of the affected functions is called, which in the general case is on startup. This module contains three functions for each affected function, for example:\n```\nmonotonic_time() ->\n    code_version:update(?MODULE),\n    time_compat:monotonic_time().\nmonotonic_time_post_18() ->\n    erlang:monotonic_time().\nmonotonic_time_pre_18() ->\n    erlang_system_time_fallback().\n```\nThe code_version then reads the abstract code, modifies, compiles and loads it. If we are in Erlang 18, it will delete monotonic_time and monotonic_time_pre_18, and rename monotonic_time_post_18 to monotonic_time. It will also remove the unused exports.\nThis solution works also doing dynamic load of modules, which would be more complex if it were an application doing it.\n1> time_compat:monotonic_time().\n-576460747892153029\n2> l(time_compat).\n{module,time_compat}\n3> time_compat:monotonic_time().\n-576460743998363885\nPerformance wise, it shouldn't affect the system at all.\n@michaelklishin @dumbbell What do you think of this approach? If you agree with it, I'll add the documentation and finish the changes to time_compat itself.\n. PR for both #346 and #347 in https://github.com/rabbitmq/rabbitmq-common/pull/42\n. I only can think of those additional try/catch and the fact that is not hipe-compiled.\n. See hipe compilation in https://github.com/rabbitmq/rabbitmq-server/pull/572\n. @dumbbell That sounds reasonable, I'll change it now.\n. Done\n. PR for both #346 and #347 in https://github.com/rabbitmq/rabbitmq-common/pull/42\n. @michaelklishin You can use this branch for merge, it includes meck as a dep: https://github.com/rabbitmq/rabbitmq-test/tree/rabbitmq-server-91\n. I implemented a first version of the rabbitmqctl command in: https://github.com/rabbitmq/rabbitmq-server/tree/rabbitmq-server-398.\nHowever, there is an issue if we want to include what the aliveness check HTTP API does, as it requires the amqp_client application. Neither rabbit nor rabbit_common depend on it, and I don't think we want to add that dependency.\n@michaelklishin what are your thoughts now?\n. This change is made of 3 PRs:\nhttps://github.com/rabbitmq/rabbitmq-management/pull/139\nhttps://github.com/rabbitmq/rabbitmq-common/pull/60\nhttps://github.com/rabbitmq/rabbitmq-server/pull/652\nI moved rabbit_cli:call_rpc to the rabbit_common application so it can be used by the common health checks code. If that is fine, I can squash all rabbitmq_server commits into one to have a cleaner history.\nI only have a doubt about this code. If it is doing a single node check, should it call the list_channels only in the given node? That is, use rabbit_channel:list_local/0 (although it is documented as 'internal') instead of rabbit_channel:info_all/1. \n. Formatting done in https://github.com/rabbitmq/rabbitmq-management/commit/0715d48adb6ad41ba13b6396f654cbd2274e408c\n. I've been investigating what [net_adm:ping/1'](https://github.com/erlang/otp/blob/maint/lib/kernel/src/net_adm.erl#L61) does, and it's simplygen:call({net_kernel, Node}, '$gen_call', {is_auth, node()}, infinity). The call itself will always return 'yes' if it reached the other node, but the trick is on how the connection is established. [gen:call/4](https://github.com/erlang/otp/blob/maint/lib/stdlib/src/gen.erl#L154) userserlang:monitor/2anderlang:send/3. These are NIFs, so I didn't follow them. But, somehow it triggers [inet_tcp_dist:setup/5](https://github.com/erlang/otp/blob/maint/lib/kernel/src/inet_tcp_dist.erl#L268) and the difference there between a connection that succeeds (but might fail on the handshake before of the cookies) and one that fails to connect is the call toerl_epmd:port_please/2`.\nHere we have a failed attempt to reach the remote node (it is down):\n<0.1353.0>) call erl_epmd:port_please(\"rabbit2\",{127,0,0,1})\n(<0.1353.0>) call erl_epmd:port_please(\"rabbit2\",{127,0,0,1},infinity)\n(<0.1353.0>) returned from erl_epmd:port_please/3 -> noport\nAnd here is succeeds, but will later fail on the handshake (wrong cookie):\n(<0.1362.0>) call erl_epmd:port_please(\"rabbit2\",{127,0,0,1})\n(<0.1362.0>) call erl_epmd:port_please(\"rabbit2\",{127,0,0,1},infinity)\n(<0.1362.0>) returned from erl_epmd:port_please/3 -> {port,25673,5}\n(<0.1362.0>) returned from erl_epmd:port_please/2 -> {port,25673,5}\nerl_epmd:port_please/2 yields the same info as diagnose_connect/2, so I'll try to find if we can use any of the next steps on the handshake.\n. I've been going around it and there is nothing we can do from the Rabbit side. I'll work on a contribution to increase logs in OTP.\n. There is one way to improve the rabbit logs with the potential OTP patch.\n- Part 1: Add error reasons to the logs as done for TLS here: https://github.com/legoscia/otp/commit/5f49de9d6e8ae247b10e37c085bf1d1dc9945ac8 These detailed logs can be generated by setting the verbose level in net_kernel:verbose/1\n- Part 2: Set the logs to verbose and install an error_logger handler in rabbit_nodes:diagnostics/1. The error_handler will select the relevant net_kernel logs that could be then printed with the output of rabbitmqctl. These logs could be parsed and pretty-printed for the final user, or refer to Rabbit documentation for details.\nSee it working below. I need to detect the TLS errors, but hostname and cookie should be already captured.\n```\nClustering node rabbit@isis with rabbit@rabbit1 ...\nError: unable to connect to nodes [rabbit@rabbit1]: nodedown\nDIAGNOSTICS\nattempted to contact: [rabbit@rabbit1]\nrabbit@rabbit1:\n  * connected to epmd (port 4369) on rabbit1\n  * epmd reports node 'rabbit' running on port 25672\n  * TCP connection succeeded but Erlang distribution failed\n  * suggestion: is the Erlang distribution using TLS?\n  * Hostname mismatch \"rabbit@t-srv-rabbit01\"\ncurrent node details:\n- node name: 'rabbitmq-cli-38@isis'\n- home dir: /Users/dianacorbacho\n- cookie hash: 9gfN63OW21CerY4A3v1jEQ==\n```\n@michaelklishin what do you think of this?\nIt would require some periodic updates with new Erlang releases (and would not work with old ones), but I don't think it is likely to change often and the patch might easily be accepted as the TLS one.\n. I still have to do work on the TLS information, but here is how the previous proposal could look like:\nhttps://github.com/rabbitmq/rabbitmq-common/tree/rabbitmq-server-401\nhttps://github.com/dcorbacho/otp/tree/dist_util_logs\n. Erlang/OTP PR in https://github.com/erlang/otp/pull/980\n. The OTP PR is merged into master, it should be available on the next Erlang release\n. I thought it would be the next minor release, but it is in fact for the next major release Erlang/OTP 19.\nmaster branch -> major release\nmaint branch -> minor release\n. Just renamed the parameter.\nGood point in the persistence, you're right, I will look at it. Maybe the runtime-parameters are not such a good idea? I used that because was the most similar structure already in place.\n. I recall discussing this ticket with @michaelklishin, but I am not sure about that part. I guess that it refers to mirrored supervisors, as supervising workers in another nodes is not a common OTP pattern (not fully supported either, if the node is down the supervisor wouldn't know what to do).\n. I believe that the enoent error on Debian with nodes in network partition comes from the prim_file:open call in:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/rabbitmq_v3_6_0/src/file_handle_cache.erl#L752\nIt seems it tries to reopen a handle that for some reason has been deleted.\n. I did some investigation on this issue while working on #676, #714 and #749. While I was tracing the queue processes, I saw how the queue would receive the delete_and_terminate call and remove the indexes, but just after that it would receive a publish message that will cause this crash. That is understandable, as the index have been removed, but I could not get to the root cause of how a publish is processed when the queue is being terminated. At some point, when I increased the logs to get more details, the issue did not happen anymore. It might be related to timings.\n. #863 (derived from #802) solves the overlap between stop/start of slaves which can trigger this bug. The stopping slave process removes the queue index, while the new incarnation of the slave tries to access to it and thus the badmatch with enoent or the opposite with eexist.\n. I can reproduce this on my MacBook with PerfTest and rabbit 3.6.1. \nsh runjava.sh com.rabbitmq.examples.PerfTest -y25 -x 25 -u myqueue.test -f persistent -p\n\nI'll investigate the issue.\n. All the I/O average time per operation times are miscalculated. The management agent performs the calculations of the averages and notifies them as such in the events, but the event collector treats them as any other simple metric and calculates rates per second instead. That also led to negative rates and samples.\nI'm finalising the testing, but with the correction of the calculations the seek times seem stable under a constant load:\n\n. After some further testing I improved the previous fix to provide instant rates in the samples (not accumulated), which are used to plot the graphics. In this way, we provide a view that truly reflects the current state of the system. The seek operation no longer increases, but keeps a global average of 0.09ms in my test (vs 4ms in the old implementation).\n\n\n\n\nPRs https://github.com/rabbitmq/rabbitmq-management/pull/150 and https://github.com/rabbitmq/rabbitmq-management-agent/pull/13\n. @falfaro this issue is solved in 3.6.1.\n675, referenced above, is a different issue which might be triggered in similar circumstances.\n. Solved in #655, list_permissions was also affected.\nIt avoids escaping again the full output of the selected commands, but avoids deep changes in the code at this point.\n. I think it is a good idea to add that option. In a particular case, I found a good performance improvement in a load testing tool by using +sbt db. However, care must be taken on the implementation of the flag in the rabbit side, because the default may not be optimal in some platforms (add a warning?) or the option not allowed (if my memory doesn't fail, OS X doesn't accept the +sbt db flag and fails on startup). \n. Thanks @hairyhum, I hadn't notice that option before!\n. @michaelklishin Just added the unix error codes to rabbitmqctl\n. One of the Mnesia issues was fixed in Erlang 18.3 (OTP-13284 http://erlang.org/download/otp_src_18.3.readme).\nThe second issue was reported at the same time this bug was filled, and is going through the last round of testing right now https://github.com/rabbitmq/rabbitmq-server/issues/676#issuecomment-202804626. I will see if #675 still happens with this patch or needs further investigation.\n. The underlying problem is in fact a mnesia bug triggered in node rabbit05.\nThe maybe_stuck output on that node shows that a mnesia transaction is always blocked in mnesia_tm:rec_acc_pre_commit, thus other transactions trying to access the locked tables are permanently restarted (mnesia_tm:restart):\n2016-03-03 08:31:42 [{pid,<5588.4603.0>},\n                     {registered_name,[]},\n                     {current_stacktrace,\n                         [{mnesia_tm,rec_acc_pre_commit,8,\n                              [{file,\"mnesia_tm.erl\"},{line,1550}]},\n                          {mnesia_tm,apply_fun,3,\n                              [{file,\"mnesia_tm.erl\"},{line,834}]},\n                          {mnesia_tm,execute_transaction,5,\n                              [{file,\"mnesia_tm.erl\"},{line,808}]},\n                          {rabbit_misc,\n                              '-execute_mnesia_transaction/1-fun-0-',1,\n                              [{file,\"src/rabbit_misc.erl\"},{line,534}]},\n                          {worker_pool_worker,'-run/2-fun-0-',3,\n                              [{file,\"src/worker_pool_worker.erl\"},\n                               {line,81}]}]},\nThe mnesia process is waiting for a 'mnesia_down' message after the network partition is triggered, but that message never arrives https://github.com/erlang/otp/blob/maint/lib/mnesia/src/mnesia_tm.erl#L1548\nThis bug has been reported and debugged with help of the OTP team (thanks @dgud!), which provided us with a patch for it. The patch is going through a last round of testing with RabbitMQ, and will be linked here once it is publicly available. \nAs a curiosity, the bug is triggered when using mirrored queues: rabbit_mirror_queue_slave:handle_go and rabbit_mirror_queue_slave:record_synchronised are the functions triggering the issue. Both rabbit_mirror_queue_slave functions run almost the same mnesia transaction:\n1. Read a queue from rabbit_queue table\n2. Call rabbit_mirror_queue_misc:store_updated_slaves\n. Hi @noahhaon, this issue will require an OTP upgrade. We will publish the details here whenever the OTP patch is available. \n. @noahhaon we don't have any workaround in RabbitMQ. \nIf upgrade is not a possibility, you might want to evaluate the option of backporting the Mnesia patch and build your own Erlang release. It is always a bit delicate, but there have been few changes in Mnesia lately.\n. The internal OTP bug tracker number is OTP-13423, the patch will be hopefully available this week.\n. OTP patch release 18.3.1 is out https://github.com/erlang/otp/tree/OTP-18.3.1\n```\nPatch Package:           OTP 18.3.1\nGit Tag:                 OTP-18.3.1\nDate:                    2016-04-04\nTrouble Report Id:       OTP-13417, OTP-13418, OTP-13419, OTP-13420,\n                         OTP-13423, OTP-13424, OTP-13446, OTP-13452\nSeq num:\nSystem:                  OTP\nRelease:                 18\nApplication:             erts-7.3.1, inets-6.2.1, mnesia-4.13.4\nPredecessor:             OTP 18.3\nCheck out the git tag OTP-18.3.1, and build a full OTP system\n including documentation. Apply one or more applications from this\n build as patches to your installation using the 'otp_patch_apply'\n tool. For information on install requirements, see descriptions for\n each application version below.\n\n--- erts-7.3.1 ------------------------------------------------------\n ---------------------------------------------------------------------\nThe erts-7.3.1 application can be applied independently of other\n applications on a full OTP 18 installation.\n--- Fixed Bugs and Malfunctions ---\nOTP-13418    Application(s): erts\n           process_info(Pid, last_calls) did not work for Pid /=\n           self().\n\nOTP-13419    Application(s): erts\n           Make sure to create a crash dump when running out of\n           memory. This was accidentally removed in the erts-7.3\n           release.\n\nOTP-13420    Application(s): erts\n           Schedulers could be woken by a premature timeout on\n           Linux. This premature wakeup was however harmless.\n\nOTP-13424    Application(s): erts\n               Related Id(s): OTP-10336\n           A process communicating with a port via one of the\n           erlang:port_* BIFs could potentially end up in an\n           inconsistent state if the port terminated during the\n           communication. When this occurred the process could\n           later block in a receive even though it had messages\n           matching in its message queue.\n\n           This bug was introduced in erts version 5.10 (OTP\n           R16A).\n\nOTP-13446    Application(s): erts\n           The reference count of a process structure could under\n           rare circumstances be erroneously managed. When this\n           happened invalid memory accesses occurred.\n\nOTP-13452    Application(s): erts\n           Fix race between process_flag(trap_exit,true) and a\n           received exit signal.\n\n           A process could terminate due to exit signal even\n           though process_flag(trap_exit,true) had returned. A\n           very specific timing between call to process_flag/2 and\n           exit signal from another scheduler was required for\n           this to happen.\n\nFull runtime dependencies of erts-7.3.1: kernel-4.0, sasl-2.4,\n stdlib-2.5\n\n--- inets-6.2.1 -----------------------------------------------------\n ---------------------------------------------------------------------\nThe inets-6.2.1 application can be applied independently of other\n applications on a full OTP 18 installation.\n--- Fixed Bugs and Malfunctions ---\nOTP-13417    Application(s): inets\n           Mend ipv6_host_with_brackets option in httpc\n\nFull runtime dependencies of inets-6.2.1: erts-6.0, kernel-3.0,\n mnesia-4.12, runtime_tools-1.8.14, ssl-5.3.4, stdlib-2.0\n\n--- mnesia-4.13.4 ---------------------------------------------------\n ---------------------------------------------------------------------\nThe mnesia-4.13.4 application can be applied independently of other\n applications on a full OTP 18 installation.\n--- Fixed Bugs and Malfunctions ---\nOTP-13423    Application(s): mnesia\n           Mnesia transactions could hang while waiting on a\n           response from a node who had stopped.\n\nFull runtime dependencies of mnesia-4.13.4: erts-7.0, kernel-3.0,\n stdlib-2.0\n\n--- Thanks to -------------------------------------------------------\n ---------------------------------------------------------------------\nAndrey Mayorov, Lukas Larsson\n```\n. @binarin The change seems a good idea, I didn't verify the times but I'll trust you on that. \nThe calls that determine if the data will be emitted from one or several nodes are indeed in the remote node. Any rabbitmqctl command that doesn't need this optimisation, would be the equivalent to your changes with Chunks == 1. I think that you can do all that work with minimal changes to the code:\n- rabbit_control_main:call_emitter is not needed if rabbit_control_main:call is expanded to accept one more parameter that is the list of nodes. It would default to the current target node, so any other command would still work.\n- Then, it would only require a function that  appends Pid, Ref and Nodes to the Args if length(nodes) > 1, just before call rpc_cli:rpc_call/7.\n- rabbit_control_misc:wait_for_info_messages receives always the number of chunks (1-N)\nIn this way, we only have to maintain one set of rabbit_control_main:call functions\n. Hi @bharris47, the endless sync loop could be caused by #676. That issue requires Erlang 18.3.1, as it is a bug in Mnesia.\nI can check for you if that is happening in your cluster. Could you share with us the output of rabbitmqctl eval 'rabbit_diagnostics:maybe_stuck().' in all nodes when the endless loop happens? You can send it through Michael.\nWe we'll continue investigating the [{rabbit_mirror_queue_slave,update_delta,2 crash, as that is not a known symptom of #676.\n. We are still investigating the issue, but we wanted to post a quick update.\nWe are reproducing the issue using three nodes: bunny, hare and rabbit. Bunny and hare are slaves, while rabbit holds the master of the mirrored queues. Hare is restarted in the middle of the test, while load is being injected into the system, and after it rejoins the cluster, the node bunny crashes as reported in this ticket.\nThe crash happens processing the depth instruction: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_mirror_queue_slave.erl#L937\nWhen hare rejoins the cluster, it triggers the depth call:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_mirror_queue_slave.erl#L140\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_mirror_queue_coordinator.erl#L365\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_mirror_queue_master.erl#L542\nBunny has messages stored by rabbit_variable_queue in the qi_pending_ack queue, which seem to have been removed from the master, as the master's depth is 0.\nBunny crashes two seconds after hare is detected down, while the messages still in bunny have gone through drain_confirmed/2 in the master node four seconds before the crash: https://github.com/rabbitmq/rabbitmq-server/blob/stable/src/rabbit_mirror_queue_master.erl#L340. The messages are stored in the master seen_status with the tag error.\nBest guess is that some notifications get lost, or the down notification is not properly handled in bunny when hare goes down. We continue working on this.\n. This crash is caused by an improper handling of the acknowledgments (automatic or manual) in the priority queues, bringing nodes down is not the cause of the issue. We can reproduce the crash without bringing any node down, simply by triggering request_depth in the GM at any time. Some messages stay in the slaves queue index waiting for acknowledge, but are not registered in the master anymore.\n. The original problem is easy reproducible with the Java client using channel.basicQos(5);.\nThe crash happens with messages that arrive at the slave by the synchronisation, because the ordering of the messages was reverted during the handling of the priority queues. Thus, acknowledgments could not be processed as those messages had not yet been dropped.\nPR #842 closes this.\n. @amulyas we are able to reproduce a similar issue without using priority queues, so although the crash seems the same it has a different root cause. You can follow it up on #944\n. The logs report several and plenty errors related to the GM (see below), which seem to be the cause of this error. As we have previously seen, a 'master' retrieves an incorrect database state where it is a slave too.\nThis test environment has 3 nodes clustered with HA queues: rabbit04, rabbit05, rabbit06. A partial partition is caused by dropping the connection from rabbit05 to rabbit04. \nFollowing the error traces and with some additional logs, we can see the changes on the GM group:\nT0 ----------------T1----------------------------- T2 --------------------- T3\nrabbit04     All                All                           All                     All -> Crashes!\nrabbit05     All   partition    {death, r04}, r05, r06        pause_minority          pause_minority\nrabbit06     All                {death, r04} through r05      {death,r05}, r06         r06\nrabbit05 detects the death of rabbit04 (through partial partition) and stores the new GM group as r05 and r06. rabbit06 sees it and instants later detects rabbit05 in pause_minority, updating the GM group to r06 only. During the next gm:check_neighbours call that rabbit04 executes, it will crash as the GM group that retrieves from mnesia doesn't contain r04. Indeed, r04 is alive and connected to r06, so it gets all database updates. \nIt seems the handling of 'DOWN' messages on partial network partitions, is not properly handled (or recovered afterwards) and creates an inconsistent state across the cluster. See here.\n=ERROR REPORT==== 31-Mar-2016::07:24:37 ===\n** Generic server <0.917.0> terminating\n** Last message in was {'$gen_cast',{'$gm',5,check_neighbours}}\n** When Server state == {state,\n                         {0,<0.917.0>},\n                         {{0,<21912.1217.0>},#Ref<0.0.2.709>},\n                         {{1,<21910.1249.0>},#Ref<0.0.2.799>},\n                         {resource,<<\"/\">>,queue,<<\"myQuueue_a_5\">>},\n                         rabbit_mirror_queue_coordinator,\n                         {2,\n                          [{{0,<0.917.0>},\n                            {view_member,\n                             {0,<0.917.0>},\n                             [],\n                             {0,<21912.1217.0>},\n                             {1,<21910.1249.0>}}},\n                           {{0,<21912.1217.0>},\n                            {view_member,\n                             {0,<21912.1217.0>},\n                             [],\n                             {1,<21910.1249.0>},\n                             {0,<0.917.0>}}},\n                           {{1,<21910.1249.0>},\n                            {view_member,\n                             {1,<21910.1249.0>},\n                             [],\n                             {0,<0.917.0>},\n                             {0,<21912.1217.0>}}}]},\n                         1617,\n....\n** Reason for termination == \n** {function_clause,\n       [{orddict,fetch,\n            [{0,<0.917.0>},\n             [{{1,<21910.1249.0>},\n               {view_member,\n                   {1,<21910.1249.0>},\n                   [],\n                   {1,<21910.1249.0>},\n                   {1,<21910.1249.0>}}}]],\n            [{file,\"orddict.erl\"},{line,80}]},\n        {gm,check_neighbours,1,[{file,\"src/gm.erl\"},{line,1267}]},\n        {gm,change_view,2,[{file,\"src/gm.erl\"},{line,1439}]},\n        {gm,handle_cast,2,[{file,\"src/gm.erl\"},{line,649}]},\n        {gen_server2,handle_msg,2,[{file,\"src/gen_server2.erl\"},{line,1049}]},\n        {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}\nand\n=ERROR REPORT==== 31-Mar-2016::07:46:33 ===\n** Generic server <0.14462.0> terminating\n** Last message in was {'DOWN',#Ref<0.0.2.59115>,process,<21908.8418.0>,\n                               noconnection}\n** When Server state == {state,\n                            {55,<0.14462.0>},\n                            {{60,<21908.8418.0>},#Ref<0.0.2.59115>},\n                            {{60,<21908.8418.0>},#Ref<0.0.2.59116>},\n                            {resource,<<\"/\">>,queue,<<\"myQuueue_a_2\">>},\n                            rabbit_mirror_queue_coordinator,\n                            {62,\n                             [{{55,<0.14462.0>},\n                               {view_member,\n                                   {55,<0.14462.0>},\n                                   [],\n                                   {60,<21908.8418.0>},\n                                   {60,<21908.8418.0>}}},\n                              {{60,<21908.8418.0>},\n                               {view_member,\n                                   {60,<21908.8418.0>},\n                                   [],\n                                   {55,<0.14462.0>},\n                                   {55,<0.14462.0>}}}]},\n                            0,\n                            [{{55,<0.14462.0>},{member,{[],[]},-1,-1}},\n                             {{60,<21908.8418.0>},{member,{[],[]},30,30}}],\n                            [<0.14589.0>],\n                            {[],[]},\n                            [],0,undefined,\n                            #Fun<rabbit_misc.execute_mnesia_transaction.1>,\n                            false}\n** Reason for termination == \n** {function_clause,\n       [{orddict,fetch,\n            [{55,<0.14462.0>},\n             [{{62,<21909.16396.0>},\n               {view_member,\n                   {62,<21909.16396.0>},\n                   [],\n                   {43,<21908.7319.0>},\n                   {43,<21908.7319.0>}}}]],\n            [{file,\"orddict.erl\"},{line,80}]},\n        {gm,check_neighbours,1,[{file,\"src/gm.erl\"},{line,1267}]},\n        {gm,change_view,2,[{file,\"src/gm.erl\"},{line,1439}]},\n        {gm,handle_info,2,[{file,\"src/gm.erl\"},{line,760}]},\n        {gen_server2,handle_msg,2,[{file,\"src/gen_server2.erl\"},{line,1049}]},\n        {proc_lib,wake_up,3,[{file,\"proc_lib.erl\"},{line,250}]}]}\nand\n=ERROR REPORT==== 31-Mar-2016::07:27:15 ===\n** Generic server <0.10502.0> terminating\n** Last message in was {'$gen_cast',\n                           {gm_deaths,[<21908.4277.0>,<21908.4108.0>]}}\n** When Server state == {state,\n                            {amqqueue,\n                                {resource,<<\"/\">>,queue,<<\"myQuueue_a_8\">>},\n                                true,false,none,[],<0.10466.0>,[],[],[],\n                                [{vhost,<<\"/\">>},\n                                 {name,<<\"all\">>},\n                                 {pattern,<<>>},\n                                 {'apply-to',<<\"all\">>},\n                                 {definition,\n                                     [{<<\"ha-mode\">>,<<\"all\">>},\n                                      {<<\"ha-sync-mode\">>,<<\"automatic\">>}]},\n                                 {priority,0}],\n                                [{<21909.6330.0>,<21909.6326.0>}],\n                                [],live},\n                            <0.10467.0>,\n                            {state,\n                                {dict,2,16,16,8,80,48,\n                                    {[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]},\n                                    {{[],[],[],[],[],[],[],[],[],[],[],\n                                      [[<21909.6570.0>|#Ref<0.0.2.36114>]],\n                                      [],\n                                      [[<21908.850.0>|#Ref<0.0.1.37197>]],\n                                      [],[]}}},\n                                erlang},\n                            #Fun<rabbit_mirror_queue_master.7.98604090>,\n                            #Fun<rabbit_mirror_queue_master.8.98604090>}\n** Reason for termination == \n** {{case_clause,{ok,<21909.6678.0>,[],[]}},\n    [{rabbit_mirror_queue_coordinator,handle_cast,2,\n         [{file,\"src/rabbit_mirror_queue_coordinator.erl\"},{line,354}]},\n     {gen_server2,handle_msg,2,[{file,\"src/gen_server2.erl\"},{line,1049}]},\n     {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}\n. I have a patch currently under testing, that solves the deadlock and apparently allows the cluster to recover.\nIn this patch, every GM process verifies if it is still on the group before call check_neighbours. It it is not, it stops. This allows the process to stop gracefully and rejoin the cluster afterwards so the synchronization of the queues and master/slave election happens as usually. \nThe patch needs longer rounds of testing, as now it eventually hits #545 (on Ubuntu) which seems to cause mismatches on the delta calculations (as seen in https://groups.google.com/forum/#!topic/rabbitmq-users/3QKj-UBqz-g by @johnfoldager). The delta error has been seen before, so it might be an independent problem. Also, during the terminate of the queues (those stopped) it can lead to:\n** Reason for termination == \n** {function_clause,\n       [{rabbit_variable_queue,d,\n            [{delta,21045,3504,21045}],\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1100}]},\n        {rabbit_variable_queue,maybe_deltas_to_betas,2,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,2469}]},\n        {rabbit_variable_queue,purge_betas_and_deltas,2,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1667}]},\n        {rabbit_variable_queue,purge1,2,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1638}]},\n        {rabbit_variable_queue,purge_and_index_reset,1,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,1619}]},\n        {rabbit_variable_queue,delete_and_terminate,2,\n            [{file,\"src/rabbit_variable_queue.erl\"},{line,545}]},\n        {rabbit_priority_queue,delete_and_terminate,2,\n            [{file,\"src/rabbit_priority_queue.erl\"},{line,181}]},\n        {gen_server2,terminate,3,[{file,\"src/gen_server2.erl\"},{line,1146}]}]}\n** In 'terminate' callback with reason ==\n** normal\nI will try to publish the patch next week once I am more confident on it.\n. I have seen the same message store index increasing while fixing the mentioned bugs - without any messages in the system - but that memory stops increasing and drops every 4 hours.\n\n. The problem is raised when the next situation happens:\n- A -> B -> C is a GM group ring\n- A partial partition happens between A and B\n- A erases B as a member and updates Mnesia\n- A few seconds later, B is not aware of this change and records A as dead, adding itself to the group again. \n- C sees these changes of the left member in the ring, but continues as usual assuming it is normal. In fact, a dead member is back to life. An inconsistency processing the queues triggers {gm,find_common,3,[{file,\"src/gm.erl\"},{line,1369}]}. Basically, when the process is comparing both queues, some unexpected elements show up in the middle of the common block.\nI'm still investigating if this is a race condition in the synchronization of the database (B must read the updates of A through the node C) or an implementation problem. Both nodes hosting A and B are indeed stopping, but the GM continues processing events until the stopped is finally reflected on the logs.\nMaybe we could do something with the unexpected elements in the queue? I don't know yet.\n. I created a PR that solves the crash, but not the root cause. I couldn't find a way to solve the broken ring, the nodes into partial partition can't see the update through the 'live' node. Maybe is a timing issue or  mnesia might already be inconsistent and don't update anymore, as the inconsistent database event is triggered just afterwards.\nIt might happen that a tiny amount of messages do not end up in all queues with this change, but the system is still functional. @michaelklishin thoughts? \n. This bug happens with persistent queues and can be reproduced by constantly re-applying the HA policy, as it causes queues to be started/stopped. It does not happen on its own, but as a consequence of other bugs that do not allow queues to clear up their state (index folder and journal stay on disk).\nIn stable, it can be a consequence of #803. Inmaster, it happens after getting this crash:\n** {{badmatch,true},[{rabbit_queue_index,init,3,[{file,\"src/rabbit_queue_index.erl\"},{line,286}]},{rabbit_variable_queue,init,6,[{file,\"src/rabbit_variable_queue.erl\"},{line,477}]},{rabbit_priority_queue,'-init/3-lc$^0/1-1-',3,[{file,\"src/rabbit_priority_queue.erl\"},{line,157}]},{rabbit_priority_queue,'-init/3-lc$^0/1-1-',3,[{file,\"src/rabbit_priority_queue.erl\"},{line,157}]},{rabbit_priority_queue,init,3,[{file,\"src/rabbit_priority_queue.erl\"},{line,157}]},{rabbit_mirror_queue_slave,handle_go,1,[{file,\"src/rabbit_mirror_queue_slave.erl\"},{line,124}]},{rabbit_mirror_queue_slave,handle_cast,2,[{file,\"src/rabbit_mirror_queue_slave.erl\"},{line,245}]},{gen_server2,handle_msg,2,[{file,\"src/gen_server2.erl\"},{line,1049}]}]}\nDuring initialization, this check fails https://github.com/rabbitmq/rabbitmq-server/blob/stable/src/rabbit_queue_index.erl#L281 as the directory exists on disk. This can be reproduced quickly with queues with large priorities (i.e. 256), but takes longer to happen with small priorities (i.e 5) and could not reproduce it with normal queues. The issue comes from the async stop/start of the queues, as the initialization of the queues can overlap with reset_state in rabbit_queue_index, when the previous instance of the queues is still terminating. Large priorities imply a larger number of variable queues (i.e 256), thus the stop command takes much longer to be processed.\nAt the same time, I believe that unrelated to the previous issues, I'm getting this error:\n2016-06-20 08:01:26.889 [error] <0.1163.0> ** Generic server <0.1163.0> terminating\n** Last message in was {'$gen_cast',{run_backing_queue,rabbit_mirror_queue_master,#Fun<ra\nbbit_mirror_queue_master.9.50136656>}}\n** When Server state == {q,{amqqueue,{resource,<<\"/\">>,queue,<<\"@prio_p_a_21_ 10\">>},true\n,false,none,[{<<\"x-max-priority\">>,signedint,124}],<0.1163.0>,[<28871.8320.0>,<28868.9145\n.0>],[],['rabbit@t-srv-rabbit05','rabbit@t-srv-rabbit06'],undefined,[{<28868.9154.0>,<288\n68.9145.0>},{<28871.8323.0>,<28871.8320.0>},{<0.8447.0>,<0.1163.0>}],[],live,15},none,fal\nse,rabbit_priority_queue,{state,rabbit_variable_queue,[{124,{vqstate,{0,{[],[]}},{0,{[],[\n]}},{delta,undefined,0,undefined},{0,{[],[]}},{5,{[{msg_status,4,<<13,193,241,204,131,208\n,54,32,69,23,31,190,108,97,144,121>>,{basic_message,{resource,<<\"/\">>,exchange,<<>>},[<<\"\n@prio_p_a_21_ 10\">>],{content,60,{'P_basic',undefined,undefined,undefined,2,124,undefined\n,undefined,undefined,undefined,undefined,undefined,undefined,undefined,undefined},<<24,0,\n2,124>>,rabbit_framing_amqp_0_9_1,[<<0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\n,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,...>>]},...},...},...],...}},...}},...]},...}\n** Reason for termination == ** {function_clause,[{rabbit_priority_queue,invoke,[rabbit_mirror_queue_master,#Fun<rabbit_mirror_queue_master.9.50136656>,{state,rabbit_variable_queue,[{124,{vqstate,{0,{[],[]}},{0,{[],[]}},{delta,undefined,0,undefined},{0,{[],[]}},{5,{[{msg_status,4,<<13,193,241,204,131,208,54,32,69,23,31,190,108,97,144,121>>,{basic_message,{resource,<<\"/\">>,exchange,<<>>},[<<\"@prio_p_a_21_ 10\">>],{content,60,{'P_basic',undefined,undefined,undefined,2,124,undefined,undefined,undefined,undefined,undefined,undefined,undefined,undefined,undefined},<<24,0,2,124>>,rabbit_framing_amqp_0_9_1,[<<0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\nI will investigate later if it is independent and then create a new issue for it.\nI'm also getting:\n2016-06-20 08:02:46.519 [warning] <0.11268.0> (<0.11268.0>) rabbit_priority_queue delete_and_terminate {{badmatch,{error,enoent}},[{rabbit_queue_index,append_journal_to_segment,1,[{file,\"src/rabbit_queue_index.erl\"},{line,836}]},{rabbit_queue_index,'-flush_journal/1-fun-0-',2,[{file,\"src/rabbit_queue_index.erl\"},{line,819}]},{lists,foldl,3,[{file,\"lists.erl\"},{line,1262}]},{rabbit_queue_index,segment_fold,3,[{file,\"src/rabbit_queue_index.erl\"},{line,1014}]},{rabbit_queue_index,flush_journal,1,[{file,\"src/rabbit_queue_index.erl\"},{line,811}]},{rabbit_variable_queue,handle_pre_hibernate,1,[{file,\"src/rabbit_variable_queue.erl\"},{line,838}]},{rabbit_priority_queue,foreach1,3,[{file,\"src/rabbit_priority_queue.erl\"},{line,488}]},{rabbit_priority_queue,foreach1,2,[{file,\"src/rabbit_priority_queue.erl\"},{line,486}]}]}\nwhich seems to be #545 and the next one which may be another manifestation of it:\n2016-06-20 08:02:45.537 [warning] <0.11866.0> (<0.11866.0>) rabbit_priority_queue delete_and_terminate {{badmatch,{error,eexist}},[{rabbit_queue_index,get_journal_handle,1,[{file,\"src/rabbit_queue_index.erl\"},{line,852}]},{rabbit_queue_index,publish,6,[{file,\"src/rabbit_queue_index.erl\"},{line,382}]},{rabbit_variable_queue,maybe_write_index_to_disk,3,[{file,\"src/rabbit_variable_queue.erl\"},{line,1886}]},{rabbit_variable_queue,publish1,7,[{file,\"src/rabbit_variable_queue.erl\"},{line,1721}]},{rabbit_variable_queue,publish,6,[{file,\"src/rabbit_variable_queue.erl\"},{line,570}]},{rabbit_priority_queue,pick1,3,[{file,\"src/rabbit_priority_queue.erl\"},{line,496}]},{rabbit_mirror_queue_slave,process_instruction,2,[{file,\"src/rabbit_mirror_queue_slave.erl\"},{line,847}]},{rabbit_mirror_queue_slave,handle_cast,2,[{file,\"src/rabbit_mirror_queue_slave.erl\"},{line,254}]}]}\n. The branch https://github.com/rabbitmq/rabbitmq-server/commits/rabbitmq-server-802 contains now two patches.\nThe invoke crash is solved in https://github.com/rabbitmq/rabbitmq-server/commit/5f1ffc6b2834aca32674121ed723e6b7e632a54e, retrieving the max priority from the internal state. The rabbit_amqqueue_process, rabbit_mirror_queue_master and rabbit_mirror_queue_slave are unaware of the backing queue they are using, thus will never provide the priority in the invoke call.\nThe {badmatch,true} is solved  in https://github.com/rabbitmq/rabbitmq-server/commit/b2fd8734280f59247d2e0e503ca689aad406f53a, by calling BQ:delete_crashed/1 to ensure indexes are deleted by the backing queue after a crash and before start the new slave. Thus, each backing queue needs to know where and how many indexes has to delete. However, this not yet solves the original {badmatch,{error,writer_exists}}, as it will now happen under new circumstances.\nPreviously, the queue would not start because the index was still present. As the race condition is still present, the reset of the index can happen once the slave has started and thus delete the new index:\n** {{badmatch,{error,enoent}},[{rabbit_queue_index,get_journal_handle,1,[{file,\"src/rabbi\nt_queue_index.erl\"},{line,840}]},{rabbit_queue_index,flush_pre_publish_cache,1,[{file,\"sr\nc/rabbit_queue_index.erl\"},{line,358}]},{rabbit_queue_index,flush_pre_publish_cache,2,[{f\nile,\"src/rabbit_queue_index.erl\"},{line,351}]},{rabbit_variable_queue,ui,1,[{file,\"src/ra\nbbit_variable_queue.erl\"},{line,2603}]},{rabbit_variable_queue,batch_publish,4,[{file,\"sr\nc/rabbit_variable_queue.erl\"},{line,578}]},{rabbit_priority_queue,pick1,3,[{file,\"src/rab\nbit_priority_queue.erl\"},{line,488}]},{lists,foldl,3,[{file,\"lists.erl\"},{line,1262}]},{r\nabbit_mirror_queue_sync,batch_publish,4,[{file,\"src/rabbit_mirror_queue_sync.erl\"},{line,\n405}]}]}\nThis needs to be solved before the patch is merged. A fix for this will solve part of #545.\n. These bugs seems to be solved with the following patches, not able to reproduce in master any longer.\n- stable https://github.com/rabbitmq/rabbitmq-server/pull/857\n- master https://github.com/rabbitmq/rabbitmq-server/tree/rabbitmq-server-802-master\nThe rationale behind the last one:\nWhen the mirroring is stopped, it only waits for a certain amount of time for the slaves to shutdown https://github.com/rabbitmq/rabbitmq-server/blob/stable/src/rabbit_mirror_queue_master.erl#L217. If the 'DOWN' messages doesn't arrive on time, it continues and removes all slaves from the amqqueue record. The slaves might be busy synchronising or stopping the many queues on each priority. If mirroring is restarted shortly afterwards, we can have an old incarnation of the slave still doing things (maybe resetting it's old index or processing data), while the new slave is being started. So the operations of both incarnations over the same queue will overlap and cause havoc. \nWe don't have a way to find out about those old incarnations, unless we check every single queue supervised by an rabbit_amqqueue_sup, under the main rabbit_amqqueue_sup_sup.\nThus, a new field has been added on amqqueue to store those pids. This change can only go in master (future 3.7)\n. The node receiving the set_policy command generates the start/stop_mirroring call based in the previous and new policy as stored by rabbit_policy. It generates then a delegate:cast message to the master/owner of the queue, in order to change the mirroring status. \nHowever, the policy in rabbit_policy might not match the current status of the system. For example, if the queues have a large amount of messages to synchronise it will take long time to process the policy change, and when a stop_mirroring command arrives the queues can still be synchronising. Policy change messages can then built up in the message queue and produce a continuous start/stop of the mirroring, long after the user has issued the set policy commands.\nAt the same time, the internal processes issuing the delegate:call command change every time, thus there is no guarantee the messages are received in order by the master/owner of the queue. This causes the reported crash, where two start or stop commands can be received consecutively.\n. @michaelklishin why not use {one_for_one, 1000, 1}? Otherwise, it is allowing 10k in 1s\n. @alaendle thanks for the patch.\nThis bug can be easily reproduced with the Java client using a durable queue, the new tests are in https://github.com/rabbitmq/rabbitmq-java-client/pull/154\n. @harlowja Thanks for your PR. I verified it in Fedora 24 and CentOS 7.\nOnce the stale if is removed, it is ready to merge.\n. The new packages seem good to me, built on Ubuntu and tested on Fedora. \ud83d\udc4d \n. One of the causes for this crash is the coexistence of several slaves for the same queue on the same node. This bug is probably caused during the partial partitions and restarts by autoheal, when several master can be alive on different nodes at the same time as these nodes are disconnected. Mnesia updates could propagate views of the cluster where the first slave disappears, thus a second one is allowed to start. See logs below\n(note: the warning is a debug message added for testing only)\n```\n=INFO REPORT==== 16-Sep-2016::10:31:58 ===\nMirrored queue 'test_71' in vhost '/': Adding mirror on node 'rabbit@ubuntu-c1': <0.2078.2>\n=INFO REPORT==== 16-Sep-2016::10:32:06 ===\nMirrored queue 'test_71' in vhost '/': Slave rabbit@ubuntu-c1.2.2078.2 saw deaths of mirrors rabbit@ubuntu-c2.1.2396.3\n=INFO REPORT==== 16-Sep-2016::10:32:06 ===\nMirrored queue 'test_71' in vhost '/': Adding mirror on node 'rabbit@ubuntu-c1': <0.2998.2>\n=INFO REPORT==== 16-Sep-2016::10:32:10 ===\nMirrored queue 'test_71' in vhost '/': Slave rabbit@ubuntu-c1.2.2078.2 saw deaths of mirrors rabbit@ubuntu-c2.1.3333.3\n=INFO REPORT==== 16-Sep-2016::10:32:11 ===\nMirrored queue 'test_71' in vhost '/': Slave rabbit@ubuntu-c1.2.2078.2 saw deaths of mirrors rabbit@ubuntu-c2.1.4421.3\n=INFO REPORT==== 16-Sep-2016::10:32:30 ===\nMirrored queue 'test_71' in vhost '/': Slave rabbit@ubuntu-c1.2.2078.2 saw deaths of mirrors rabbit@ubuntu-c2.1.5311.3\n=INFO REPORT==== 16-Sep-2016::10:32:47 ===\nMirrored queue 'test_71' in vhost '/': Slave rabbit@ubuntu-c1.2.2998.2 saw deaths of mirrors rabbit@ubuntu-c2.1.6173.3\n=INFO REPORT==== 16-Sep-2016::10:32:48 ===\nMirrored queue 'test_71' in vhost '/': Slave rabbit@ubuntu-c1.2.2078.2 saw deaths of mirrors rabbit@ubuntu-c3.1.3811.0\n=INFO REPORT==== 16-Sep-2016::10:32:49 ===\nMirrored queue 'test_71' in vhost '/': Promoting slave rabbit@ubuntu-c1.2.2998.2 to master\n=WARNING REPORT==== 16-Sep-2016::10:32:49 ===\n(<0.2998.2>) MASTER on promote_backing_queue_state broadcasting 0 for {resource,\n                                                                       <<\"/\">>,\n                                                                       queue,\n                                                                       <<\"test_71\">>}\n=INFO REPORT==== 16-Sep-2016::10:32:49 ===\nMirrored queue 'test_71' in vhost '/': Synchronising: 0 messages to synchronise\n=INFO REPORT==== 16-Sep-2016::10:32:49 ===\nMirrored queue 'test_71' in vhost '/': Synchronising: batch size: 4096\n=INFO REPORT==== 16-Sep-2016::10:32:49 ===\nMirrored queue 'test_71' in vhost '/': Synchronising: all slaves already synced\n=ERROR REPORT==== 16-Sep-2016::10:32:49 ===\n Generic server <0.2078.2> terminating\n Last message in was {'$gen_cast',{gm,{depth,0}}}\n** When Server state == {state,\n                         {amqqueue,\n                          {resource,<<\"/\">>,queue,<<\"test_71\">>},\n                          true,false,none,[],<0.2998.2>,[],[],[],\n...\n Reason for termination ==\n {{badmatch,-2322},\n    [{rabbit_mirror_queue_slave,update_delta,2,\n                                [{file,\"src/rabbit_mirror_queue_slave.erl\"},\n                                 {line,991}]},\n     {rabbit_mirror_queue_slave,process_instruction,2,\n                                [{file,\"src/rabbit_mirror_queue_slave.erl\"},\n                                 {line,945}]},\n     {rabbit_mirror_queue_slave,handle_cast,2,\n                                [{file,\"src/rabbit_mirror_queue_slave.erl\"},\n                                 {line,260}]},\n     {gen_server2,handle_msg,2,[{file,\"src/gen_server2.erl\"},{line,1032}]},\n     {proc_lib,wake_up,3,[{file,\"proc_lib.erl\"},{line,257}]}]}\n=WARNING REPORT==== 16-Sep-2016::10:32:57 ===\n(<0.2998.2>) MASTER terminate(shutdown) {resource,<<\"/\">>,queue,<<\"test_71\">>}\n=WARNING REPORT==== 16-Sep-2016::10:32:57 ===\nMirrored queue 'test_71' in vhost '/': Stopping all nodes on master shutdown since no synchronised slave is available\n``\n. When two slaves are alive on the same node one of them can be promoted to master, and thedepthnotification of this master reach the other local slave. As this slave is synced with other master in other node, it will crash. The presence of multiple masters in the cluster during a partial partition can also cause slaves to synchronise with one master and receivedepth` notifications from a different one. This is also probably causing #959.\nSlaves should detect they have been removed from the slaves list and do a clean stop. This should avoid at least the majority of the crashes, and allow the queue to eventually reach a consistent state. Note that it is possible that messages are lost in this situation.\n. This clause can created duplicated slaves in other nodes, which is happening at the same time all other crashes take place. It might or might not be part of the root cause.\n. At least many of the crashes have been introduced by #914 in here. During partial partitions several nodes can contain an active master, that on rejoining the cluster stop with reason normal causing the queue to be deleted. Termination reason should be shutdown to keep the existing queue.\nThere might be some other cases where the termination reason needs to be reviewed.\n. Disconnecting from remote nodes during a partial partition can also cause mirrored queue records to be deleted by rabbit_amqqueue:on_node_down/1, as it only checks for slaves and apparently there are none.\n```\n=ERROR REPORT==== 14-Sep-2016::10:36:53 ===\n Node 'rabbit@ubuntu-c1' not responding \n Removing (timedout) connection \n=INFO REPORT==== 14-Sep-2016::10:36:53 ===\nnode 'rabbit@ubuntu-c1' down: net_tick_timeout\n=ERROR REPORT==== 14-Sep-2016::10:36:53 ===\nPartial partition detected:\n * We saw DOWN from rabbit@ubuntu-c1\n * We can still see rabbit@ubuntu-c3 which can see rabbit@ubuntu-c1\nWe will therefore intentionally disconnect from rabbit@ubuntu-c3\n=INFO REPORT==== 14-Sep-2016::10:36:54 ===\nrabbit on node 'rabbit@ubuntu-c3' down\n``\n. Another case where the queue is deleted is if the GM detects membership lost (as [here](https://github.com/rabbitmq/rabbitmq-server/blob/master/src/gm.erl#L758)), so it terminates with reasonnormalcausing the master to terminate in such way. Thus, deleting the record.\n. The termination of the GM just after startup can bring down the coordinator before it finishes theinit, causing the calling master function to crash. It can be better handled by propagating the error to therabbit_amqqueue_process`, so it can gracefully stop. This is done in https://github.com/rabbitmq/rabbitmq-server/commit/4005a5a12cffea117e4bc59a64ce4ec06b93ef7e (pending more testing).\n** Generic server <0.14022.0> terminating\n** Last message in was {'$gen_cast',init}\n** When Server state == {q,{amqqueue,\n                               {resource,<<\"/\">>,queue,<<\"test_3\">>},\n                               true,false,none,[],<0.14022.0>,\n                               [<0.13384.0>],\n                               [],\n                               ['rabbit@ubuntu-c2'],\n                               [{vhost,<<\"/\">>},\n                                {name,<<\"ha-all\">>},\n                                {pattern,<<\".*\">>},\n                                {'apply-to',<<\"all\">>},\n                                {definition,\n                                    [{<<\"ha-mode\">>,<<\"all\">>},\n                                     {<<\"ha-sync-mode\">>,<<\"automatic\">>}]},\n                                {priority,0}],\n                               [{<0.13385.0>,<0.13384.0>},\n                                {<24589.1116.0>,<24589.1115.0>}],\n                               [],live,0},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,-576460143926141,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,none,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,0,\n                           running}\n** Reason for termination ==\n** {{badmatch,{error,shutdown}},\n    [{rabbit_mirror_queue_master,init_with_existing_bq,3,\n                                 [{file,\"src/rabbit_mirror_queue_master.erl\"},\n                                  {line,104}]},\n     {rabbit_mirror_queue_master,init,3,\n                                 [{file,\"src/rabbit_mirror_queue_master.erl\"},\n                                  {line,99}]},\n     {rabbit_amqqueue_process,init_it2,3,\n                              [{file,\"src/rabbit_amqqueue_process.erl\"},\n                               {line,196}]},\n     {gen_server2,handle_msg,2,[{file,\"src/gen_server2.erl\"},{line,1032}]},\n     {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,247}]}]}\n. As in #944, partial partitions cause the coexistence of several masters in the same cluster. When the nodes get reconnected, the master exchange messages with existing slaves - expecting them to be newly started slaves - but those have just been synchronised or received messages from other master. Thus, message queues get out of sync and status do not match.\nThis requires an enhanced consensus algorithm to avoid the root cause. \n. As in #944, partial partitions cause the coexistence of several masters in the same cluster. When the nodes get reconnected, the master exchange messages with existing slaves - expecting them to be newly started slaves - but those have just been synchronised or received messages from other master. Thus, message queues get out of sync and status do not match.\nThis requires an enhanced consensus algorithm to avoid the root cause. \n. The root cause is not changes in master/slave status as I originally thought, but the network partition causing remote channels (in a different node) to be removed from the queue. If a message has been delivered to a remote channel and immediately after the queue process receives a DOWN message from the channel, the messages pending acknowledgment for this channel are requeued. Thus, later delivered to a different channel.\nIf the other node comes back shortly after (a few seconds on my tests), the queue might end up receiving two acknowledgments for the same ack tag: one from the channel considered down and one from the redelivery channel. The second ack causes the crash as the tag cannot be found.\n. Note that this is not exclusively related to HA queues, as it can happen without them.\n. @michaelklishin The failure is related to the change in rabbit_common  https://github.com/rabbitmq/rabbitmq-common/pull/133\nHaven't figured out why yet, still looking at the other problems in #953 \n. There is bug in gm:link_view/2, or more precisely on the calling functions. If link_view receives a list of members with duplicates - the alive generated here` - the ring gets broken as the function doesn't finish the cycle.\nCompare the output from:\n(rabbit@MacBook-Pro)2> Members = [{32, pid(0,1,0)}, {39, pid(0, 2, 0)}, {35, pid(0, 3, 0)}, {39, pid(0, 2, 0)}].\n[{32,<0.1.0>},{39,<0.2.0>},{35,<0.3.0>},{39,<0.2.0>}]\n(rabbit@MacBook-Pro)3>  gm:link_view(Members ++ Members ++ Members, gm:blank_view(22)).\n{22,\n [{{35,<0.3.0>},\n   {view_member,{35,<0.3.0>},[],{39,<0.2.0>},{39,<0.2.0>}}},\n  {{39,<0.2.0>},\n   {view_member,{39,<0.2.0>},[],{32,<0.1.0>},{35,<0.3.0>}}}]}\nwith:\n(rabbit@MacBook-Pro)4> Members2 = [{32, pid(0,1,0)}, {39, pid(0, 2, 0)}, {35, pid(0, 3, 0)}].\n[{32,<0.1.0>},{39,<0.2.0>},{35,<0.3.0>}]\n(rabbit@MacBook-Pro)5>  gm:link_view(Members2 ++ Members2 ++ Members2, gm:blank_view(22)).\n{22,\n [{{32,<0.1.0>},\n   {view_member,{32,<0.1.0>},[],{35,<0.3.0>},{39,<0.2.0>}}},\n  {{35,<0.3.0>},\n   {view_member,{35,<0.3.0>},[],{39,<0.2.0>},{32,<0.1.0>}}},\n  {{39,<0.2.0>},\n   {view_member,{39,<0.2.0>},[],{32,<0.1.0>},{35,<0.3.0>}}}]}\nThen, add_aliases  can't find the missing member.\n. @michaelklishin The management UI needs to be updated to capture the errors while changing password and notify the users. At the moment, the password change can fail but no error is given by the UI. See https://github.com/rabbitmq/rabbitmq-management/blob/master/src/rabbit_mgmt_wm_user.erl#L90 and related.\nSimilar thing goes for add_user, but in this case the user is first created with a fake password and then changed. Thus, the operator will see an user in the UI but won't be able to use it as the password is not the given one.  See https://github.com/rabbitmq/rabbitmq-management/blob/master/src/rabbit_mgmt_wm_user.erl#L119\n(<0.724.0>) call rabbit_auth_backend_internal:add_user(<<\"short\">>,<<\"tmp-b7jZmNtXkpCt3c2yaEa_kQ\">>)\n(<0.724.0>) returned from rabbit_auth_backend_internal:add_user/2 -> ok\n(<0.724.0>) call rabbit_auth_backend_internal:change_password(<<\"short\">>,<<\"short\">>)\n(<0.724.0>) returned from rabbit_auth_backend_internal:change_password/2 -> {error,\n                                                                             \"minimum required password length is 8\"}. Why triggering a full GC when it is know which queue is being synchronised? If we want to show the queue changes instantly, it would be more effective to GC only the named queue.. Queues will load a segment on memory from the index on startup, and when delivering messages. With the current design this cannot be avoided, as reading single elements from the index causes an  unacceptable performance degradation (throughput three orders of magnitude lower). If the memory usage is too high, users could consider if the messages they are embedding on the index are too big (see queue_index_embed_msgs_below configuration).\n. The message store index still requires 664-696 Bytes per message when queue_index_embed_msgs_below is set to 0, which is approximately 14Gb of RAM for those 21M messages on the previous example.\nThus, number of messages x 696 Bytes is the potential min memory requirement for the system to store that number of messages even using lazy queues and queue_index_embed_msgs_below = 0. Note that not all segments might be in memory at the same time, thus the footprint might be smaller. \n| Mem used by msg index (Bytes) | Message payload (Bytes) |\n| - | - |\n| 664 | 0 |\n| 672 | 1-8 |\n| 680 | 9-16 |\n| 688 | 17-24 |\n| 696 | 25 and above |\n  . Note also that the lazy queue setting is known to be not particularly efficient whenmax-length is used, and both are configured in @gerhard deployment. Thus, we must be careful with conclusions drawn from those results.. @michaelklishin This PR doesn't introduce any (negative) impact on the throughput, in fact the speed up is over 3x! See below, first rate line belongs to a queue with drop-head and a length of 300. Second line is a queue with reject-publish and length of 300.\n\n. Merge with master done\n. @hairyhum This change will not work. If we remove all the config from makefiles and rabbitmq-env (which is missing in this PR) - ra will start with the default of current working dir as the application starts before the boot steps are executed. After discussion with @kjnilsson we won't implement this now, as it would require delay startup of the rabbit application until boot steps are executed. This is a way bigger change.. @kjnilsson @michaelklishin Message size is recalculated every time the messages is unack or returned to the queue. Should we add it to the message properties we handle in rabbit_fifo? Any possible downside to this?. I also agree with @michaelklishin in that we shouldn't require manual intervention. It works fine, but I think we could improve the cleanup that we do in startup and include these data folders not linked to an existing queue. . Note that this call is now broken\n. Not that I'm aware, but I set it here the same way IO_THREAD_POOL_SIZE is defined just above\n. This endif doesn't match any if. Could you please remove it?\n. These error checks are too closely tied to the implementation by verifying the exact message. For long term maintenance, it would be better to match only the error part in the tuple. \n. offline/online/local are not strictly queueinfoitems, but are listed as such in the help. What if we add those details just below usage ?\nlist_queues [-p <vhost>] [--offline|--online|--local] [<queueinfoitem> ...]\n...\n<queueinfoitem> must be a member of the list [[--offline], [--online],\n[--local], name, durable, auto_delete, arguments, policy, pid, owner_pid,\nexclusive, exclusive_consumer_pid, exclusive_consumer_tag, messages_ready,\n. @michaelklishin I guess this io:format slipped through from dev, but maybe an info log would be useful? The same below. \n. Well spotted, not anymore. I split them originally to parallelize more groups, but that didn't work (or became slower). I should have reverted also this part.. Indentation, also other logs below. you could use get_env/3. This is why #1233 was reported. Units are specified just above: https://github.com/rabbitmq/rabbitmq-server/blob/1b976358b62db552f70a0f6c273431895e3cddb0/docs/rabbitmq.config.example#L230. The problem is that we report the wrong unit back to the user here: https://github.com/rabbitmq/rabbitmq-server/blob/master/src/vm_memory_monitor.erl#L240, using MB when the conversion applied for that log message is MiB.. This is 83 min (5000 seconds), 5 min would be 300. Use an interval from config, so the default could be easily changed.. rabbit application can never call anything on rabbitmq_management or rabbitmq_management_agent as those plugins might or might not be active. The dependency is the other way around.\n=ERROR REPORT==== 3-Jul-2017::14:07:12 ===\nError in process <0.631.0> on node 'rmq-ct-cluster_tests-1-21000@localhost' with exit value:\n{undef,[{rabbit_mgmt_gc,run_gc_soon,[],[]},\n        {rabbit_mirror_queue_sync,syncer_loop,3,\n                                  [{file,\"src/rabbit_mirror_queue_sync.erl\"},\n                                   {line,266}]}]}. This part of the test (call rabbit_mgmt_gc) could never work on the rabbit app, as the management application is not started. \n```\n=== Ended at 2017-07-03 14:07:12\n=== Location: [{rabbit_ct_broker_helpers,rpc,967},\n              {rabbit_core_metrics_gc_SUITE,cluster_queue_metrics,383},\n              {test_server,ts_tc,1533},\n              {test_server,run_test_case_eval1,1053},\n              {test_server,run_test_case_eval,985}]\n=== Reason: undefined function rabbit_mgmt_gc:run_gc_soon/1\n  in function  rpc:'-handle_call_call/6-fun-0-'/5 (rpc.erl, line 187)\n\n=ERROR REPORT==== 3-Jul-2017::14:07:12 ===\nError in process <0.631.0> on node 'rmq-ct-cluster_tests-1-21000@localhost' with exit value:\n{undef,[{rabbit_mgmt_gc,run_gc_soon,[],[]},\n        {rabbit_mirror_queue_sync,syncer_loop,3,\n                                  [{file,\"src/rabbit_mirror_queue_sync.erl\"},\n                                   {line,266}]}]}\n``. Wait until everything is started. Why trigger it manually? Then the test will never verify that the synchronisation is doing the clean up. Moreover, setting a configurable default will allow to reduce the wait time. Why do we try once if the queue is stopped?. is this a future change or pending for this PR?. If cleanup is properly implemented, thesequeue.deleteshouldn't be needed at the beginning of the test. Or am I missing something? I think insteadend_per_groupthese changes requireend_per_testcase.. Same here. Do we really need this lookup if we always callleave_all_groupswith the samePid? The threecaseclauses do exactly the same. Or am I missing something?. It was addressed, but the comment was left behind. Fixing it now.. I think it was done here for a reason, I'm trying to remember why.. Kind of. It lists all the quorum followers on the current node. . What do you mean? They're currently considered mirrored.. Looking at the code I don't think that guard is right, I wonder if something changed while implementing it and that's a leftover. @kjnilsson any idea why this guard would be there?. We really can't be more explicit, unless we carry around the queuetypeapart from thepidfield. If we store it as a tagged tuple or similar, it means to change all code that uses theamqqueuerecord.. We could, but all the listing ops are inrabbit_amqqueue. If we move specific functions out, then we should also consider moving all the mirror (HA) synchronisation ones - which currently reside here - to some mirroring module.. In this case, the 'is_mirrored' definition means that there could be another incarnation of the queue in a different node (may it be HA mirror or quorum follower). Despite being an exported function, it's only used insiderabbit_amqqueueto determine which queues to delete on node down and when to retryrabbit_amqqueue:with` operations.. Clarified in the comments: https://github.com/rabbitmq/rabbitmq-server/pull/1706/commits/f10ec8d09df4a94bb8daaf93435e8362cce2ddea. I removed the erlang monitor in https://github.com/rabbitmq/rabbitmq-server/pull/1706/commits/4de27b65cd8e2091507b7664405a5cb73f376d30 and use the 'eol' event instead. Everything seems to work.. I just introduced some macros to distinguish between quorum/classic queues. It's only cosmetic, but might help: https://github.com/rabbitmq/rabbitmq-server/pull/1706/commits/80c85833bcab905c11c98b839b4cfb8bf1a2db27\n?IS_QUORUM(QPid) and ?IS_CLASSIC(QPid). I don't know if there is much we can do, except maybe to retry. The supervisor should restart the crashed process, so if that's not happening it's because something went terribly wrong. I don't think crashing here would be an acceptable behaviour, as the delivery itself is a cast (https://github.com/rabbitmq/rabbitmq-server/blob/master/src/rabbit_amqqueue.erl#L1242). Anything that went wrong there we wouldn't know anyway.. Well spotted, forgot to commit and push it. Available now in https://github.com/rabbitmq/ra/pull/26. should we ignore it then?. Nothing, they only drop the messages from the queue. done\n. ",
    "bgou": "ah that makes sense. I've updated my code to retrieve it for future reference.\n``` java\n    private String getDeadletterRoutingKey(Delivery delivery) {\n        Map deathParams = getDeadletterParams(delivery);\n        ArrayList routingKeys = (ArrayList) deathParams.get(\"routing-keys\");\n    if (routingKeys.isEmpty())\n        return null;\n\n    return routingKeys.get(0).toString();\n}\n\n```\nThanks!\n. ",
    "vespakoen": "Thanks for the heads up!\n. ",
    "johnfoldager": "+1\n. +1\nOn Tue, May 10, 2016 at 2:13 PM, hosup choi notifications@github.com\nwrote:\n\n+1\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/89#issuecomment-218139411\n. Yes the node that accepts the message from client.\nOkay with the not-per-protocol configuration.\nOkay with the low-priority.\n. Cool. Thanks!\nMichael, you added this for 3.6.1 milestone but the GitHub page says it is \"This plugin targets RabbitMQ 3.6.0 and later versions.\"\nAlso on the GitHub page at the bottom it says \"...process in order to add the current timestamp as seen by the broker.\" - notice \"timestamp\" which be broker node or something similar.\n. Ok. Will it be downloadable as a .ez file from somewhere and/or be available in the RabbitMQ 3.6.1 release when released?\n. Can see the duplicate for #104 for connections, but what about the rest?\n. Okay. Cool.... I wasn't aware that I could use arguments for something custom. Thanks\n. Would it be possible to have the patch/fix for 3.5.7 as well?\n. The above error happens many, many times after each other. After some time I see these errors which might be related?\n\n```\n=ERROR REPORT==== 28-Apr-2016::14:55:37 ===\n Generic server <0.5509.0> terminating\n Last message in was connect\n When Server state == {<0.5508.0>,\n                         {amqp_params_direct,<<\"USERNAME\">>,\n                             <<\"PASSWORD\">>,\n                             <<\"init\">>,'rabbit@ts-mb-eur101a',\n                             {amqp_adapter_info,\n                                 {0,0,0,0,0,65535,49320,2566},\n                                 15679,\n                                 {0,0,0,0,0,65535,23120,39997},\n                                 42744,\n                                 <<\"10.0.0.2:42744 -> 192.168.10.6:15679\">>,\n                                 {'STOMP',\"1.2\"},\n                                 [{channels,1},\n                                  {channel_max,1},\n                                  {frame_max,0},\n                                  {client_properties,\n                                      [{<<\"product\">>,longstr,\n                                        <<\"STOMP client\">>}]},\n                                  {ssl,true},\n                                  {ssl_protocol,'tlsv1.2'},\n                                  {ssl_key_exchange,ecdhe_rsa},\n                                  {ssl_cipher,aes_128_gcm},\n                                  {ssl_hash,null}]},\n                             []}}\n Reason for termination ==\n** {function_clause,\n       [{amqp_gen_connection,terminate,\n            [{{case_clause,\n                  {badrpc,\n                      {'EXIT',\n                          {{badmatch,undefined},\n                           [{rabbit_auth_backend_ldap,env,1,\n                                [{file,\"src/rabbit_auth_backend_ldap.erl\"},\n                                 {line,418}]},\n                            {rabbit_auth_backend_ldap,log,2,\n                                [{file,\"src/rabbit_auth_backend_ldap.erl\"},\n                                 {line,507}]},\n                            {rabbit_auth_backend_ldap,\n                                user_login_authentication,2,\n                                [{file,\"src/rabbit_auth_backend_ldap.erl\"},\n                                 {line,59}]},\n                            {rabbit_access_control,try_authenticate,3,\n                                [{file,\"src/rabbit_access_control.erl\"},\n                                 {line,91}]},\n                            {rabbit_access_control,\n                                '-check_user_login/2-fun-0-',4,\n                                [{file,\"src/rabbit_access_control.erl\"},\n                                 {line,77}]},\n                            {lists,foldl,3,[{file,\"lists.erl\"},{line,1262}]},\n                            {rabbit_direct,connect0,5,\n                                [{file,\"src/rabbit_direct.erl\"},{line,85}]},\n                            {rpc,local_call,3,\n                                [{file,\"rpc.erl\"},{line,330}]}]}}}},\n              [{amqp_direct_connection,connect,4,\n                   [{file,\"src/amqp_direct_connection.erl\"},{line,135}]},\n               {amqp_gen_connection,handle_call,3,\n                   [{file,\"src/amqp_gen_connection.erl\"},{line,171}]},\n               {gen_server,try_handle_call,4,\n                   [{file,\"gen_server.erl\"},{line,629}]},\n               {gen_server,handle_msg,5,[{file,\"gen_server.erl\"},{line,661}]},\n               {proc_lib,init_p_do_apply,3,\n                   [{file,\"proc_lib.erl\"},{line,240}]}]},\n             {<0.5508.0>,\n              {amqp_params_direct,<<\"USERNAME\">>,\n                  <<\"PASSWORD\">>,\n                  <<\"init\">>,'rabbit@ts-mb-eur101a',\n                  {amqp_adapter_info,\n                      {0,0,0,0,0,65535,49320,2566},\n                      15679,\n                      {0,0,0,0,0,65535,23120,39997},\n                      42744,<<\"10.0.0.2:42744 -> 192.168.10.6:15679\">>,\n                      {'STOMP',\"1.2\"},\n                      [{channels,1},\n                       {channel_max,1},\n                       {frame_max,0},\n                       {client_properties,\n                           [{<<\"product\">>,longstr,<<\"STOMP client\">>}]},\n                       {ssl,true},\n                       {ssl_protocol,'tlsv1.2'},\n                       {ssl_key_exchange,ecdhe_rsa},\n                       {ssl_cipher,aes_128_gcm},\n                       {ssl_hash,null}]},\n                  []}}],\n            [{file,\"src/amqp_gen_connection.erl\"},{line,230}]},\n        {gen_server,try_terminate,3,[{file,\"gen_server.erl\"},{line,643}]},\n        {gen_server,terminate,7,[{file,\"gen_server.erl\"},{line,809}]},\n        {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]} \n=ERROR REPORT==== 28-Apr-2016::14:55:37 ===\nSTOMP error frame sent:\nMessage: \"Processing error\"\nDetail: \"Processing error\"\nServer private detail: {{function_clause,\n                         [{amqp_gen_connection,terminate,\n                           [{{case_clause,\n                              {badrpc,\n                               {'EXIT',\n                                {{badmatch,undefined},\n                                 [{rabbit_auth_backend_ldap,env,1,\n                                   [{file,\"src/rabbit_auth_backend_ldap.erl\"},\n                                    {line,418}]},\n                                  {rabbit_auth_backend_ldap,log,2,\n                                   [{file,\"src/rabbit_auth_backend_ldap.erl\"},\n                                    {line,507}]},\n                                  {rabbit_auth_backend_ldap,\n                                   user_login_authentication,2,\n                                   [{file,\"src/rabbit_auth_backend_ldap.erl\"},\n                                    {line,59}]},\n                                  {rabbit_access_control,try_authenticate,3,\n                                   [{file,\"src/rabbit_access_control.erl\"},\n                                    {line,91}]},\n                                  {rabbit_access_control,\n                                   '-check_user_login/2-fun-0-',4,\n                                   [{file,\"src/rabbit_access_control.erl\"},\n                                    {line,77}]},\n                                  {lists,foldl,3,\n                                   [{file,\"lists.erl\"},{line,1262}]},\n                                  {rabbit_direct,connect0,5,\n                                   [{file,\"src/rabbit_direct.erl\"},{line,85}]},\n                                  {rpc,local_call,3,\n                                   [{file,\"rpc.erl\"},{line,330}]}]}}}},\n                             [{amqp_direct_connection,connect,4,\n                               [{file,\"src/amqp_direct_connection.erl\"},\n                                {line,135}]},\n                              {amqp_gen_connection,handle_call,3,\n                               [{file,\"src/amqp_gen_connection.erl\"},\n                                {line,171}]},\n                              {gen_server,try_handle_call,4,\n                               [{file,\"gen_server.erl\"},{line,629}]},\n                              {gen_server,handle_msg,5,\n                               [{file,\"gen_server.erl\"},{line,661}]},\n                              {proc_lib,init_p_do_apply,3,\n                               [{file,\"proc_lib.erl\"},{line,240}]}]},\n                            {<0.5508.0>,\n                             {amqp_params_direct,<<\"0080F4CA0036\">>,\n                              <<\"WOY02fkQfd95b8yK7TkRxe28qEcVdn4W3uny62Yo6Po2M8G1G1Cv4oN5ZR3V1DG6\">>,\n                              <<\"init\">>,'rabbit@ts-mb-eur101a',\n                              {amqp_adapter_info,\n                               {0,0,0,0,0,65535,49320,2566},\n                               15679,\n                               {0,0,0,0,0,65535,23120,39997},\n                               42744,\n                               <<\"90.80.156.61:42744 -> 192.168.10.6:15679\">>,\n                               {'STOMP',\"1.2\"},\n                               [{channels,1},\n                                {channel_max,1},\n                                {frame_max,0},\n                                {client_properties,\n                                 [{<<\"product\">>,longstr,<<\"STOMP client\">>}]},\n                                {ssl,true},\n                                {ssl_protocol,'tlsv1.2'},\n                                {ssl_key_exchange,ecdhe_rsa},\n                                {ssl_cipher,aes_128_gcm},\n                                {ssl_hash,null}]},\n                              []}}],\n                           [{file,\"src/amqp_gen_connection.erl\"},{line,230}]},\n                          {gen_server,try_terminate,3,\n                           [{file,\"gen_server.erl\"},{line,643}]},\n                          {gen_server,terminate,7,\n                           [{file,\"gen_server.erl\"},{line,809}]},\n                          {proc_lib,init_p_do_apply,3,\n                           [{file,\"proc_lib.erl\"},{line,240}]}]},\n                        {gen_server,call,[<0.5509.0>,connect,infinity]}}\n```\n. If I set it to false I probably won't see anything. So I'd rather not do that. And yes... it shouldn't fail so destructively ;o)\n. Next error right after the above is the following which is probably related?\n=ERROR REPORT==== 29-Apr-2016::05:04:29 ===\nError in process <0.1752.0> on node 'rabbit@ts-mb-eur101a' with exit value:\n{{nocatch,{gen_tcp_error,enotconn}},\n [{eldap,send_the_LDAPMessage,3,[{file,\"eldap.erl\"},{line,991}]},\n  {eldap,do_unbind,1,[{file,\"eldap.erl\"},{line,955}]},\n  {eldap,loop,2,[{file,\"eldap.erl\"},{line,567}]}]}\n. rabbitmq_auth_backend_ldap.log is already set to false so it shouldn't be undefined!?\n. I've tried the following (removing all other plugins and using colon instead):\n```\n$ sudo rabbitmq-plugins list\n Configured: E = explicitly enabled; e = implicitly enabled\n | Status:   * = running on rabbit@ts-mb-eur101a\n |/\n:\n[E*] rabbitmq_auth_backend_ldap        3.6.1.906\n:\n$ sudo rabbitmq-plugins disable rabbitmq_auth_backend_ldap\nThe following plugins have been disabled:\n  rabbitmq_auth_backend_ldap\nApplying plugin configuration to rabbit@ts-mb-eur101a... stopped 1 plugin.\n$ sudo rabbitmq-plugins list\n Configured: E = explicitly enabled; e = implicitly enabled\n | Status:   * = running on rabbit@ts-mb-eur101a\n |/\n:\n[  ] rabbitmq_auth_backend_ldap        3.6.1.906\n:\n$ sudo rabbitmq-plugins enable rabbitmq_auth_backend_ldap\nThe following plugins have been enabled:\n  rabbitmq_auth_backend_ldap\nApplying plugin configuration to rabbit@ts-mb-eur101a... failed.\nError: {error,already_present}\n$ sudo rabbitmq-plugins list\n Configured: E = explicitly enabled; e = implicitly enabled\n | Status:   * = running on rabbit@ts-mb-eur101a\n |/\n:\n[E ] rabbitmq_auth_backend_ldap        3.6.1.906\n:\n```\nAs you can see plugin was loaded as E* then disabled, enabled (but with error), now just E.\n. Hmmm... needed to disable and then re-enable using the --online parameter:\n$ sudo rabbitmq-plugins enable --online rabbitmq_auth_backend_ldap\nand now it is in E* state again.\n. Error is still being logged.... but a little bit different:\n```\n=ERROR REPORT==== 29-Apr-2016::09:57:57 ===\n Generic server <0.13568.26> terminating\n Last message in was connect\n When Server state == {<0.13567.26>,\n                         {amqp_params_direct,<<\"USERNAME\">>,\n                             <<\"PASSWORD\">>,\n                             <<\"init\">>,'rabbit@ts-mb-eur102a',\n                             {amqp_adapter_info,\n                                 {0,0,0,0,0,65535,49320,2568},\n                                 15679,\n                                 {0,0,0,0,0,65535,49919,1242},\n                                 46745,\n                                 <<\"194.255.4.218:46745 -> 192.168.10.8:15679\">>,\n                                 {'STOMP',\"1.2\"},\n                                 [{channels,1},\n                                  {channel_max,1},\n                                  {frame_max,0},\n                                  {client_properties,\n                                      [{<<\"product\">>,longstr,\n                                        <<\"STOMP client\">>}]},\n                                  {ssl,true},\n                                  {ssl_protocol,'tlsv1.2'},\n                                  {ssl_key_exchange,ecdhe_rsa},\n                                  {ssl_cipher,aes_256_gcm},\n                                  {ssl_hash,null}]},\n                             []}}\n Reason for termination ==\n** {function_clause,\n       [{amqp_gen_connection,terminate,\n            [{{case_clause,\n                  {badrpc,\n                      {'EXIT',\n                          {noproc,\n                              {gen_server2,call,\n                                  [ldap_pool,\n                                   {next_free,<0.13568.26>},\n                                   infinity]}}}}},\n              [{amqp_direct_connection,connect,4,\n                   [{file,\"src/amqp_direct_connection.erl\"},{line,135}]},\n               {amqp_gen_connection,handle_call,3,\n                   [{file,\"src/amqp_gen_connection.erl\"},{line,171}]},\n               {gen_server,try_handle_call,4,\n                   [{file,\"gen_server.erl\"},{line,629}]},\n               {gen_server,handle_msg,5,[{file,\"gen_server.erl\"},{line,661}]},\n               {proc_lib,init_p_do_apply,3,\n                   [{file,\"proc_lib.erl\"},{line,240}]}]},\n             {<0.13567.26>,\n              {amqp_params_direct,<<\"USERNAME\">>,\n                  <<\"PASSWORD\">>,\n                  <<\"init\">>,'rabbit@ts-mb-eur102a',\n                  {amqp_adapter_info,\n                      {0,0,0,0,0,65535,49320,2568},\n                      15679,\n                      {0,0,0,0,0,65535,49919,1242},\n                      46745,<<\"194.255.4.218:46745 -> 192.168.10.8:15679\">>,\n                      {'STOMP',\"1.2\"},\n                      [{channels,1},\n                       {channel_max,1},\n                       {frame_max,0},\n                       {client_properties,\n                           [{<<\"product\">>,longstr,<<\"STOMP client\">>}]},\n                       {ssl,true},\n                       {ssl_protocol,'tlsv1.2'},\n                       {ssl_key_exchange,ecdhe_rsa},\n                       {ssl_cipher,aes_256_gcm},\n                       {ssl_hash,null}]},\n                  []}}],\n            [{file,\"src/amqp_gen_connection.erl\"},{line,230}]},\n        {gen_server,try_terminate,3,[{file,\"gen_server.erl\"},{line,643}]},\n        {gen_server,terminate,7,[{file,\"gen_server.erl\"},{line,809}]},\n        {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,240}]}]}\n=ERROR REPORT==== 29-Apr-2016::09:57:57 ===\nSTOMP error frame sent:\nMessage: \"Processing error\"\nDetail: \"Processing error\"\nServer private detail: {{function_clause,\n                         [{amqp_gen_connection,terminate,\n                           [{{case_clause,\n                              {badrpc,\n                               {'EXIT',\n                                {noproc,\n                                 {gen_server2,call,\n                                  [ldap_pool,\n                                   {next_free,<0.13568.26>},\n                                   infinity]}}}}},\n                             [{amqp_direct_connection,connect,4,\n                               [{file,\"src/amqp_direct_connection.erl\"},\n                                {line,135}]},\n                              {amqp_gen_connection,handle_call,3,\n                               [{file,\"src/amqp_gen_connection.erl\"},\n                                {line,171}]},\n                              {gen_server,try_handle_call,4,\n                               [{file,\"gen_server.erl\"},{line,629}]},\n                              {gen_server,handle_msg,5,\n                               [{file,\"gen_server.erl\"},{line,661}]},\n                              {proc_lib,init_p_do_apply,3,\n                               [{file,\"proc_lib.erl\"},{line,240}]}]},\n                            {<0.13567.26>,\n                             {amqp_params_direct,<<\"USERNAME\">>,\n                              <<\"PASSWORD\">>,\n                              <<\"init\">>,'rabbit@ts-mb-eur102a',\n                              {amqp_adapter_info,\n                               {0,0,0,0,0,65535,49320,2568},\n                               15679,\n                               {0,0,0,0,0,65535,49919,1242},\n                               46745,\n                               <<\"194.255.4.218:46745 -> 192.168.10.8:15679\">>,\n                               {'STOMP',\"1.2\"},\n                               [{channels,1},\n                                {channel_max,1},\n                                {frame_max,0},\n                                {client_properties,\n                                 [{<<\"product\">>,longstr,<<\"STOMP client\">>}]},\n                                {ssl,true},\n                                {ssl_protocol,'tlsv1.2'},\n                                {ssl_key_exchange,ecdhe_rsa},\n                                {ssl_cipher,aes_256_gcm},\n                                {ssl_hash,null}]},\n                              []}}],\n                           [{file,\"src/amqp_gen_connection.erl\"},{line,230}]},\n                          {gen_server,try_terminate,3,\n                           [{file,\"gen_server.erl\"},{line,643}]},\n                          {gen_server,terminate,7,\n                           [{file,\"gen_server.erl\"},{line,809}]},\n                          {proc_lib,init_p_do_apply,3,\n                           [{file,\"proc_lib.erl\"},{line,240}]}]},\n                        {gen_server,call,[<0.13568.26>,connect,infinity]}}\n```\nI didn't make the installation but it was done using the deb package on a Ubuntu Linux 14.04 LTS server. After installation a script is run in order to set up a cluster, add exchanges, queues, users, virtual hosts, enable plugins, etc.\n. I have the problem on 4 RabbitMQ nodes in a cluster. If I restart one of the nodes then it works for that specific node. Does this help?\nAnyway, I don't like that it can get into this state.\n. It can be a workaround for now. However, if the machines are restarted due to hosting maintenance or a power outage then they will turn on automatically at a later time when noone might monitor it, so we will have a cluster that seems to work but fails on all requests. So this is not a \"solution\" we can live with.\nIt has been installed from the 3.6.2 RC1 deb package after a complete un-installation/removal of the previous version.\nIn our setup we have a backend LDAP server which has unlimited idle timeout for connections. But in-between RabbitMQ nodes and LDAP we are using Microsoft Azure Load Balancer which might have a connection idle timeout.\nSo my question would now be if the LDAP plugin has a connection state monitoring and/or what the connection idle time might be for this plugin? Maybe we can set the idle timeout on the load balancer to something higher than what LDAP plugin requires?\n. The plugins were enabled using the rabbitmq_plugins enable command.\n. Does the LDAP plugin maintain and check that the open connections are still working?\nIf a Load Balancer or the end-point-service times outs the connection will the LDAP plugin notice and recreate the connection?\n. The logs contains too many connections and Erlang dumps containing usernames and passwords for me to manually go through, so I can't upload entire logs.\n. Well... we have approximately 200 devices that continuously connects. That might not be a lot for RabbitMQ in a test environment but it quickly generate a lot of log entries.\n. I'll look into this next week and see if I can put together a Docker image or something with an isolated environment.\n. Can you add an option to make a delay in milliseconds between each force-closing?\nIf I force-close all connections on a production system today then if their logic is to try to reconnect it will generate a connection storm which can lead to other problems. By specifying a delay I'm able to control this reconnection a bit.. Maybe also add an option to specify how many connections (randomly) you want to force-close. This should take into account the time the connection was made to make sure only to force-close connections that was made before this command was issued (in case reconnects happens with the execution time).. I see these two options as really nice and I can see their usage in several scenarios. I guess the latter will be implemented in a loop, so maybe just set a maximum count (instead of the random approach) which wouldn't be a big change in logic. I would really like this feature in order to be able to disconnect a number of clients and make them reconnect to the load balancer in front of RabbitMQ in order to re-distribute the load one several RabbitMQ nodes.\nRegarding the vhost and deletiong of all connections: That makes sense!. ",
    "gfoiani": "+1\n. ",
    "BlueBeN82": "+1\n. ",
    "michaelplaing": "The gotthardp COAP implementation would be more useful if it used bindings on a common topic exchange rather than distinct exchanges for topics. The current COAP implementation makes it difficult to interoperate with the RabbitMQ MQTT implementation, a desirable goal IMHO.\n. done :)\nOn Fri, Dec 18, 2015 at 7:52 PM, Michael Klishin notifications@github.com\nwrote:\n\n@michaelplaing https://github.com/michaelplaing please post this to\nrabbitmq-users or file an issue so that @gotthardp\nhttps://github.com/gotthardp can see your feedback ;)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/89#issuecomment-165928814\n.\n. I would recommend doing it the simplest way. I like the 'per exchange' approach.\n. If going down this route, one might consider a white list / black list\napproach with wildcards to cover more use cases.\n\nOn Wed, Jan 6, 2016 at 2:25 AM, Michael Klishin notifications@github.com\nwrote:\n\n@uvzubovs https://github.com/uvzubovs right, just like rabbitmqctl\nset_permissions does.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/505#issuecomment-169247227\n.\n. Here's a simple demo of white / black in python.\n\nIt uses MQTT syntax but is easily translatable / generalizable to AMQP.\nThere are four lists of ACLs: W/B for the topic plus W/B for the retain\nflag (illustrating ACL chaining).\nhttps://gist.github.com/michaelplaing/2f6f209b5449212a4984\nOn Wed, Jan 6, 2016 at 6:25 AM, Laing, Michael michael.laing@nytimes.com\nwrote:\n\nIf going down this route, one might consider a white list / black list\napproach with wildcards to cover more use cases.\nOn Wed, Jan 6, 2016 at 2:25 AM, Michael Klishin notifications@github.com\nwrote:\n\n@uvzubovs https://github.com/uvzubovs right, just like rabbitmqctl\nset_permissions does.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/505#issuecomment-169247227\n.\n. Hmm.\n\n\nThe simple way is to document the correspondence I guess and send newer\nusers to a UI.\nOn Wed, Jan 6, 2016 at 10:47 AM, Michael Klishin notifications@github.com\nwrote:\n\nThis issue is not about MQTT specifically but there is one MQTT-specific\naspect that's hard to ignore: the wildcards. We have a way to translate\nthem into AMQP 0-9-1 ones. The question is, how should this be exposed to\nend user, given that rabbitmqctl is currently not extendable from plugins.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/505#issuecomment-169363656\n.\n. Seems good to me!\n\nOn Fri, Dec 16, 2016 at 12:18 PM, Michael Klishin notifications@github.com\nwrote:\n\nFor topic pattern definitions, what if instead of AMQP 0-9-1 patterns we\nused regular expressions? We already use them for permission patterns\nanyway. They are not perfect but more protocol-neutral and I expect that in\nmost cases basic topic prefixing would be sufficient.\nOn the upside for us, it would be easy to implement and the matching\nfunction is already quite heavily optimized in the standard library.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/505#issuecomment-267645846,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAxl04OOKeaEpaIhV7flHSPoBsBVi84uks5rIsf6gaJpZM4G3mix\n.\n. \n",
    "mwmw7": "+1\n. ",
    "willejs": ":+1:\n. @michaelklishin are there plans to implement this any time soon? I am very interested in it!\n. Cheers @michaelklishin its good enough that this is on the horizon :smile: . Adding this issue to the 3.6.0 milestone would be cool :wink: \n. @michaelklishin sorry to keep bothering you! Is it on a roadmap? Is there anyway to speed this up, apart from learning erlang? ;) I'l donate to the project in any way if i can!\n. I understand, hence apologising! July would be great!\n. @michaelklishin me again :) Hows this looking?\n. ",
    "vitaly-krugl": "Good point @videlalvaro, I just noticed that simply running rabbitmqctl status produces a crash dump.\n. ",
    "noahhaon": "Hi - I am the user that reported the issue initially.  Recently, we were experiencing rmq bug # 26527 with 3.4.4 which prompted the upgrade to 3.5.0.  \nI have included the charts below, they reflect the positive increments to the publish counts for our queues over two timespans, one over the past 18 hours or so with 3.5.0 and one showing more typical behavior, and a 3 week period prior showing more typical load.  Please let me know if you wish to track any more stats.  We will likely downgrade to 3.4.4 on Monday.\n3.5.0:\n\n3.4.4:\n\n. I would also note that the cluster is recovering from the downtime during the upgrade and exchange publish rates are very high, on the order of 20k/s - 30k/s.  Typical for us is around 10k/s - 15k/s\n. I will need to check with our ops team to see if we were collecting I/O and CPU on the nodes during that period, unfortunately I think we may have experienced an outage on the local collectors during that time :/  \nI don't think we ever had network traffic to consumers.\nQueue ack rates:\n3.5.0\n\n3.4.4\n\n(Sorry getting the same time series in circonus is maddening, I had to do it on the date boundary.)\n. Updated graphs\n. We tried upgrading to 3.5.1 today and quickly ran into the same issue after a couple mirrored queues grew to around 3MM messages. \n. More details: this occurs with or without mirrored queues.  Publisher channels had publish confirmations enabled, and consumer channels had manual acknowledgements. \n. We can reliably reproduce this with HA queues on 3.5.1\n. We haven't had time to set up a different environment for this, but it happens reliably in our production 8 node environment with RMQ 3.5.1 and Erlang 17.3\n- This happens with a policy of:\nha-mode:    exactly\nha-params:  2\n- Currently idle with several million messages stuck in queues :(  Prior to this about ~10mbps between nodes\n- We've yet to reproduce this under light load.  It only seems to happen under heavy load with several million messages in queues.  We have seen it with the HA policy applied prior to publishing, and seen it happen shortly after HA policy gets applied to running queues\n- Attempting to remove the ha policy doesn't work for about half of the queues, the old policy remains applied.  \nPlease let me know if there is anything else I can provide?\n. - It is when they back up\n- Yes, millions of messages in the queues is not unusual when recovering from downtime - which we've had plenty of recently due to RMQ upgrades/downgrades :(\n- Not particularly high I/O according to the RMQ management plugin.  I/O is quiescent currently and the cluster is completely unresponsive.\nI have emailed you the output of maybe_stuck from one of the nodes which had a process with a huge dictionary of ctags.\n. Sent logs from all the nodes.  I suspect this is something related to pub confirms, but I can't say for sure.  Note that all publisher connections were closed when these logs were generated.  I tried closing the consumer connections, but it caused the cluster to become unresponsive. \n. This has lasted for 10 hours so far.  Is there any way to dump the contents of a queue via the erlang console?  We have 10s of millions of messages we are at risk of losing now.\n. I just saw the huge number of ctags in the process dictionaries in the maybe stuck logs and though it could be related.\n. Hmmm.. I tried closing the consumer connections via the management api, and the management plugins on all the nodes became, and remain unresponsive.\n. re: making mirror flow control optional, I don't see anything in the logs indicating that they are blocking?  We switched to 3.5.x for the mirror flow control feature, as we were regularly hitting high memory watermark on our mirror nodes. \n. I agree.  We are going to have to start on disaster recovery here soon.  Is there anything else you would like me to run on the cluster while it is in this state?\n. @michaelklishin sent - if you think of anything else you need, please let me know.\n. Thanks - as this issue has caused data loss for us, we will try to get a separate environment set up to test with and may be a week or so before we are able to get back to you,\n. @dumbbell just a follow up, we have seen far worse performance in the rabbitmq-management stats collector under 3.6.0, to the point of consistently reaching backlogs in the tens of millions and rapidly hitting the memory high water mark or suffering an allocation failure and crash.  \nAfter conversations w/ @michaelklishin on rabbitmq-users we suspect it may be due to this change.  Other work here https://github.com/rabbitmq/rabbitmq-management/pull/101 will hopefully mitigate the issue, but I thought you may be interested given your concerns on this issue.\n. @michaelklishin Sorry, but you made that speculation yourself on the newsgroup.  There had been no other significant changes between 3.5.7 and 3.6.0 that could have caused this.  We are actively trying to engage a support contract with Pivotal and will ensure this issue is resolved in pre-sales.\n. @michaelklishin as @essen stated, the most important property of a salt is that it is unique for each user - the point is to prevent precompute \"rainbow table\" style attacks.  \nI think that your description of the salt algorithm in master meets that criteria, I don't see any value in changing it or otherwise making it pluggable.\n. I have also observed case 2 mentioned by @binarin above. :+1: \n. @michaelklishin done - #379 \n. > Can I assume that a memory alarm would cause the same symptoms?\n\nSo what happens if we unblock publishers and then they publish a message that has to be replicated to the node that still has the resource alarm?\n\nI seem to recall running into this case with earlier versions of RMQ, and it leading to node crashes as the publishers would become unblocked and memory pressure on the affected node would continue to increase. I believe in that case it was for a slave of a mirrored queue.  We still need to verify the recent improvements/fixes to mirrored queue performance, but I suspect those issues were the root cause of the memory exhaustion.\nThe publishers may enter a flow control state, but that seems like it was incidental to the alarm. \n. Great - but what is being indexed on an brand new node that has no queues?  Seems like it is synching something with the rest of the cluster but it is unclear exactly what, and it takes hours to do.\n. @michaelklishin Hi, no there are no publishers, no messages.  The cluster is entirely quiescent. \n. will take this to rmq users, https://groups.google.com/forum/#!topic/rabbitmq-users/2oExWl_ogZw\n. Related: nodes show up in cluster_status running_nodes prior accepting connections or the \"rabbit\" application showing as started.  It would be nice to either change the behavior of cluster_status or include status that indicates whether or not that node has fully started. \n. We solve this using any number of monitoring tools that can consume JSON via the management interface.  The management plugin has a very comprehensive REST API and emits JSON which just about anything can consume.  You can even do this with curl jq and bash if need be. \n. I am confused, does the resolution for this issue require an OTP upgrade, or will RabbitMQ be mitigating the Mnesia bug in a later release?\n. @dcorbacho thank you for the quick response!  I assume it is not possible/practical to mitigate this in the RabbitMQ application?  We have issues upgrading to Erlang 18 due to openssl incompatibilities. \n. @dcorbacho Probably better to resolve our OpenSSL issues to get the other Mnesia fixes then.  Mnesia has been a sore point for us at scale, so it would likely be worth the pain of an upgrade.  Thanks!\n. I think you misunderstand, the message digest is weak, you can keep your key you just need to sign using SHA2.\n. Also the latest LTS version of Ubuntu will not install packages signed with SHA1 keys.\n. If you can tell me what build tools you use to sign your debs, I would be happy to investigate this further.\n. Hi, sorry I didn't get an earlier notification on this.  I have some ideas and will do some digging today.\n. @michaelklishin Unfortunately I think this change is most easily handled in your build environment.  You would need to add the following to the the .gnupg/gpg.conf file in the build user's homedir:\ndigest-algo sha256 \nDisappointingly, reprepro appears to have no options to passing arguments to gpg as it uses an embedded version, libgpgme.  However, you can set the environment variable GNUPGHOME to point to the location of the directory housing gpg.conf, and drop the modified config file via make or similar.  Not ideal, but at least something you could get into git.\nRelevant bits for build the deb repo appear to live here:\nhttps://github.com/rabbitmq/rabbitmq-server/tree/master/packaging/debs/apt-repository\nc.f.\nhttps://manpages.debian.org/cgi-bin/man.cgi?query=reprepro&apropos=0&sektion=0&format=html&locale=en\n. @michaelklishin from what I understand of the issue, there should be no reason for you to generate a new key.  \nOnce you get the fix in, you should be able to verify the signature digest algorithm that reprepro used:\nwget http://www.rabbitmq.com/debian/dists/kitten/Release\nwget http://www.rabbitmq.com/debian/dists/kitten/Release.gpg\ngpg -v Release.gpg\n. BTW I don't see any release artifacts (debs, etc) mentioned in these repos:\nhttp://www.rabbitmq.com/debian/dists/kitten/Release\nhttp://www.rabbitmq.com/debian/dists/testing/Release\n. @dumbbell I was unaware that the signing key was DSA, which would require a key migration :( Your proposed course of action for the transition looks perfect - thanks for your work on this!\nSome more background from the Debian folks here:\nhttps://wiki.debian.org/Teams/Apt/Sha1Removal\n\nFixing half-broken repositories\nThe repository owner needs to pass --digest-algo SHA512 or --digest-algo SHA256 (or another SHA2 algorithm) to gpg when signing the file. Repositories with DSA keys need to be migrated to RSA first.\n. Sometimes it's better to just rip off the band-aid :)  \n\nLooks like Debian is not going to enforce the SHA1 deprecation until Jan 1, 2017 , and apparently Ubuntu 16.04 is throwing a warning so you have a bit of time.\nThanks again!\n. @jianping-roth Hi, we've hit this issue a few times after a partition on 3.5.7 and Erlang 17.5.  Pivotal support was unable to find the root cause.   Deleting and recreating the queue did not resolve the issue for us, we had to create a similar binding with a wildcard to avoid a cluster restart.\nOtherwise, stopping and starting the entire cluster fixes the problem - which is our SOP now for \"handling\" partitions in a RMQ cluster, as we frequently see a variety of stability issues after a partition.  Best bet is just to restart the whole thing.\n. ",
    "RonTsai": "Hi,   Michaelklishin ,  This is Ron from VMware.\n My client have opened bug26527, and also open an issue like this.\nI can easy to reproduce this issue with two node in one cluster with mirror mode.\n{ha-mode:all ,  ha-sync-mode:automatic}\nHere are the reproduce procedure on RMQ3.5.\n1. First queued 1G messages on queue in Master node( each message 3Kb, total 300000 msgs)\n   sh runjava.sh com.rabbitmq.examples.PerfTest -y0 -s 3000 -r 2 -x1000  -C 3000 -e exchange -f persistent -b 11 -p \u2014uri amqp://user:pwd@host:5672/fdc\n2. Then use perf tool to do stress test.\n   Rabbitmq setting: Net_ticktime 90s\n   #28MB/s,  producer on Master node\n   sh runjava.sh com.rabbitmq.examples.PerfTest -y0 -s 14000 -r 2 -x1000  -e exchange -f persistent -b 11 -p \u2014uri amqp://user:pwd@masterhost:5672/fdc\n   #100 consumer in Master node\n   sh runjava.sh com.rabbitmq.examples.PerfTest -y100  -u f12a.fdc.topic.exchange -p \u2014uri amqp://user:pwd@masterhost:5672/fdc\n3. Here we can see it\u2019s soon to consume 300000 msg within 60~70 seconds.\n4. But after 10mins around , the connection start to have flow control / block publish, and\n   memory kept on around 3G/10G (master/slave)\nIn the same perftool setting with 28MB/s producer and 100 consumer, and without the stuck messages. It will be no flow control, and the master/slave memory are keep around 400MB/100MB .\nAlso I can reproduce this on RMQ 3.2.1.\nthis will cause memory increasing without flow control, and result in over memory watermark.\nThe difference between 3.5. It will have flow control step in while memory increased over 3G/10G (master/slave). \nI think it is the native issue fd RMQ.  under high throughput, the mirror node within cluster will have this issue.I have test the same procedure under 18MB/s throughput, it will keep memory as normal, and won't have flow control. \nBy the way I also done test of queuing message from 300000 to 6000 msg, it all have this issue, but under 5000 msg, it keep normal on memory and no flow control.\nThanks for your help.\n. Hi,  MK\nCould you also provide the tarball with a fix to me?\nMy client also need this patch to test.\nThanks!\nRon\n. Hi, sorry to miss your response.\nMy email: rtsai@vmware.com\nThanks\n. ",
    "dallasmarlow": "@michaelklishin you beat me to it, updating description now\n. @michaelklishin thanks, do you think these should be configurable?\n. @rade i'm not sure, but after reading the interval_operation code and seeing the effects of a manual gc during these events it looked like as gc times got slower, the next interval would always get pushed back making the amount of gc work needed next time even more (with a constant load). that was my impression after seeing the graphs and stats after several of these events, and memory usage has been stable and never above 10G after patching.\n. @michaelklishin sounds good, thanks again.\n. ",
    "mrcaptainoak": "hi, all,\nI want to know why rabbitmq Binaries in memory is too high?  And after removing  all devices the Binaries is still too high, why not release memory?\nRabbitmq 3.6.2   erlang:18.3\n. ",
    "AlessandroEmm": "@dumbbell Sorry for the stupid title. I posted it here to because I thought I may haven fallen into a bug or something. \nI did issue a reset multiple times (as it says in the doc) hence the nodes in the cluster cluster_nodes.config. One thing i noticed is that the clusters name we're different - could that be my issue? Or is there a minimum amount of nodes needed for autojoining to work?\nI'm on Ubuntu Vivid btw.\n. @dumbbell thanks! I will join the list then. You can close this thread. :-)\n. ",
    "Freakachoo": "Is that possible to add connection_name property to response in a list of channel on /api/channels request?\nThere is property: connection_details, and would be very useful to have custom connection name there too.. ",
    "gotthardp": "There is no need to change the Erlang client nor MQTT/STOMP. The information is available to the server already. It just doesn't pass them to the authentication function. See my e-mail on the mailing list. Hence I think this is a correct repository.\n. ",
    "Gsantomaggio": "In this rabbitmq-users gruop thread, you can find more info about EPMD and IPV6 . Should we update also Windows ? \nCurrently it uses 30\n. Great! Thank you @hairyhum  ! :+1: \n. @dumbbell yes of corse. I will try.\n. I'm testing the issue using the version 3.6.0.\n=INFO REPORT==== 14-Jan-2016::14:24:49 ===\nStarting RabbitMQ 3.6.0 on Erlang 17.3\nCopyright (C) 2007-2015 Pivotal Software, Inc.\nLicensed under the MPL.  See http://www.rabbitmq.com/\nAnd I had this error:\n** Reason for termination == \n** {{badmatch,{error,{\"c:/rmqdata/db/rabbit@WIN-NCNMMKRPVMR-mnesia/queues/D3P5UZWQOW7DKGCO3YXAZ6WWO\",\n                      eexist}}},\n    [{rabbit_queue_index,reset_state,1,\n                         [{file,\"src/rabbit_queue_index.erl\"},{line,276}]},\n     {rabbit_variable_queue,reset_qi_state,1,\n                            [{file,\"src/rabbit_variable_queue.erl\"},\n                             {line,1646}]},\n     {rabbit_variable_queue,purge_and_index_reset,1,\n                            [{file,\"src/rabbit_variable_queue.erl\"},\n                             {line,1620}]},\n     {rabbit_variable_queue,delete_and_terminate,2,\n                            [{file,\"src/rabbit_variable_queue.erl\"},\n                             {line,545}]},\n     {rabbit_priority_queue,delete_and_terminate,2,\n                            [{file,\"src/rabbit_priority_queue.erl\"},\n                             {line,181}]},\n     {gen_server2,terminate,3,[{file,\"src/gen_server2.erl\"},{line,1146}]},\n     {proc_lib,wake_up,3,[{file,\"proc_lib.erl\"},{line,247}]}]}\nSeems exactly this https://github.com/rabbitmq/rabbitmq-server/issues/341\nI tried to put also the Debian nodes in network partition, and I (sometimes) had this error:\n** Reason for termination ==\n** {{badmatch,{error,enoent}},\n    [{rabbit_queue_index,append_journal_to_segment,1,\n                         [{file,\"src/rabbit_queue_index.erl\"},{line,831}]},\n     {rabbit_queue_index,'-flush_journal/1-fun-0-',2,\n                         [{file,\"src/rabbit_queue_index.erl\"},{line,816}]},\n     {lists,foldl,3,[{file,\"lists.erl\"},{line,1261}]},\n     {rabbit_queue_index,segment_fold,3,\n                         [{file,\"src/rabbit_queue_index.erl\"},{line,1000}]},\n     {rabbit_queue_index,flush_journal,1,\n                         [{file,\"src/rabbit_queue_index.erl\"},{line,808}]},\n     {rabbit_variable_queue,handle_pre_hibernate,1,\n                            [{file,\"src/rabbit_variable_queue.erl\"},\n                             {line,838}]},\nThe stack it different form windows, but enoent sounds like a problem during the creation and destroy the queue directory. It looks like similar to Windows situation.\nFor what have seen until now the problem seems not related only to Windows. Unfortunately the error doesn't occur systematically both in Windows and Debian.\n. I found other conditions where this error is raised as:\n** Reason for termination == \n** {{badmatch,{error,{\"c:/Users/gabriele/AppData/Roaming/RabbitMQ/db/RABBIT~1/queues/1Y30DXXRLPQ8IQFHDELJQGHP5\",\n                      eacces}}},\n    [{rabbit_queue_index,delete_and_terminate,1,\n                         [{file,\"src/rabbit_queue_index.erl\"},{line,305}]},\n     {rabbit_variable_queue,purge_pending_ack_delete_and_terminate,1,\nAll the conditions seem to lead to the call  rabbit_file:recursive_delete \n. Fixed by https://github.com/rabbitmq/rabbitmq-server/pull/674\n.  Here the steps to reproduce the issue:\n1 - RabbitMQ cluster with 3 nodes\n- Node 1 - 10.100.0.101\n- Node 2 - 10.100.0.102\n- Node 3 - 10.100.0.103\nSetup this policy:\nrabbitmqctl set_policy all \"\" '{\"ha-mode\":\"all\",\"ha-sync-mode\":\"automatic\"}'\n2 - Execute the following  script to one node, ex:  Node 3\n```\nwhile true; do\necho \"START\"\nsleep 5\nrabbitmqctl list_channels\necho \"sync 1 \"\nrabbitmqctl list_queues -q synchronised_slave_pids\nsleep 2\nrabbitmqctl stop_app\nsleep 2\necho \"stop 3\"\nrabbitmqctl reset\nrabbitmqctl join_cluster rabbit@node1\nsleep 3\nrabbitmqctl start_app\nsleep 5\necho \"sync 2 \"\nrabbitmqctl list_queues -q synchronised_slave_pids\nrabbitmqctl list_channels\niptables -A OUTPUT -d 10.100.0.101  -j DROP\necho \"blocked\"\nsleep 25\niptables -F\necho \"unblocked\"\ndone\n```\n3 -  execute this python or/and this java scripts\n4 - after a few minutes you will see the command rabbitmqctl list_channels blocked.\nOn the management UI you will see channels with unknown connection \n. Hi @jcasale \nThank you for reporting. \nI retried the Windows installer 3.6.1 RC1 on Windows 7, just to be sure, and it works correctly:\n\nI'm going to try with Windows 2008 R2. \n. @hairyhum Yes, thank you! I will.\n. Ok I have the same problem.\nI noticed that there is and error during the setup :\n\nI think that the installation didn't finish correctly and the registry is not correct:\n\nI stopped the service and put it as manual and I restarted the machine.\nI got the same error using rabbitmq-server.bat as: \nC:\\Program Files\\RabbitMQ Server\\rabbitmq_server-3.6.0.901\\sbin>rabbitmq-server.\nbat\nThe filename, directory name, or volume label syntax is incorrect.\nThe filename, directory name, or volume label syntax is incorrect.\nThe filename, directory name, or volume label syntax is incorrect.\nERROR: epmd error for host Protocol: inet_tcp: register/listen error: econnrefus\ned: nxdomain (non-existing domain)\nNOTE: This error appears only the first execution, I retried to execute rabbitmq-server.bat it worked:\n```\nC:\\Program Files\\RabbitMQ Server\\rabbitmq_server-3.6.0.901\\sbin>rabbitmq-server.\nbat\n          RabbitMQ 3.6.0.901. Copyright (C) 2007-2016 Pivotal Software, Inc.\n\n##  ##      Licensed under the MPL.  See http://www.rabbitmq.com/\n  ##  ##\n  ##########  Logs: C:/Users/ADMINI~1/AppData/Roaming/RabbitMQ/log/RABBIT~1.LOG\n  ######  ##        C:/Users/ADMINI~1/AppData/Roaming/RabbitMQ/log/RABBIT~2.LOG\n  ##########\n              Starting broker... completed with 0 plugins.\n```\nAt this point I tried to uninstall RabbitMQ service re-install it again without reboot the machine and the setup worked correctly:\n\nalso the TCP listener is up:\nC:\\Program Files\\RabbitMQ Server\\rabbitmq_server-3.6.0.901\\sbin>netstat -na | find \"5672\"\n  TCP    0.0.0.0:5672           0.0.0.0:0              LISTENING\n  TCP    0.0.0.0:25672          0.0.0.0:0              LISTENING\nbasically I think that we have two issues:\n1. ERROR: epmd error for host Protocol only to the first boot.\n2. The setup should stop if there is an error \n. Yes @hairyhum the error is when epmd.exe is not running. Thanks.\n. Using https://github.com/rabbitmq/rabbitmq-server/commit/0f5111cab23e900ccc6794b53f220de594a11c80 it works. \nI tested it using rabbitmq-service.bat and rabbitmq-server.bat \n. @michaelklishin just tried and it works correctly. \nBefore close I'd wait also @jcasale feedback. \n. Fixed by https://github.com/rabbitmq/rabbitmq-server/pull/636\n. Thanks @hairyhum.\n@dumbbell  I tested the new setup, and it woks correctly. \nif you want you can assign the PR to me.\nhere is the setup rabbitmq-server-3.6.0.626.exe.zip\n. Tried by killing erl.exe and the process restart correctly. \nJust for more clarification:\n1.  OnFail flag ignores the Windows service recovery setting , right?\n2.  By default now  rabbitmq-service.bat uses:\nif \"!RABBITMQ_SERVICE_RESTART!\"==\"\" (\n     set RABBITMQ_SERVICE_RESTART=restart\n)\nWhich are the other values for RABBITMQ_SERVICE_RESTART ? \nWe should document it here https://www.rabbitmq.com/configure.html \n. Thank you @hairyhum\n. Fixed by https://github.com/rabbitmq/rabbitmq-server/pull/660\n. I made two tests\n1. Setup using the standard configuration and the rabbitmq.configis located inside the folder C:\\Users\\gabriele\\AppData\\Roaming\\RabbitMQ\\.  OK\n2.  removed all and reinstalled the setup using RABBITMQ_BASEas c:\\test_rmq and the file rabbitmq.config has been created inside  C:\\Users\\gabriele\\AppData\\Roaming\\RabbitMQ\\\n=INFO REPORT==== 3-Mar-2016::16:14:25 ===\nnode           : rabbit@windowsdev\nhome dir       : C:\\Windows\nconfig file(s) : c:/test_rmq/rabbitmq.config (not found)\nIn this case the file should be created inside c:/test_rmq/, isn't it ? \n. @hairyhum Yes I think it should be moved.\nI'd add that when RABBITMQ_BASEis configured the directory AppData\\Roaming\\RabbitMQ shouldn't be created.\nEven if RABBITMQ_BASEis configured,  the setup creates:\n```\nDirectory di C:\\Users\\gabriele\\AppData\\Roaming\\RabbitMQ\n03/03/2016  16:14              .\n03/03/2016  16:14              ..\n03/03/2016  16:14              db\n03/03/2016  16:14              log\n```\nThe directories are empty. \n. @hairyhum what do you think ? \n. @hairyhum I got it, thanks. \nI'd move the file creation to rabbitmq-service.bat to close the this issue. \nFor the other points I will refer to the #626\n. echo []. > !RABBITMQ_CONFIG_FILE!.config raises The system cannot find the path specified. if the variable RABBITMQ_CONFIG_FILE  points to a directory that does not exist.\nI tried leaving the default configuration and setting  RABBITMQ_CONFIG_FILE=C:\\test_pull660_1\\test_file_conf, where  C:\\test_pull660_1\\ does not exist. I got the error during the setup, and the error inside the log:\nconfig file(s) : c:/test_pull660_1/test_file_conf.config (not found)\nI read the comment, but I want to be sure that this is the expected behavior or we should try to create the directory as we do for RABBITMQ_BASE \n. > RABBITMQ_CONFIG_FILE  is The path to the configuration file.\nI think that most of the users use the default configuration or they change RABBITMQ_BASE.\nSo, we have two options:\n1. Ignore this case, and we suppose that who changed this setting knows what is doing.\n2. Find a way to create the directory form. \nExcept for this particular situation, the setup works correctly now. \n. @dls314 do you mean  RABBITMQ_BASE in Windows ?\nmaybe this https://github.com/rabbitmq/rabbitmq-server/issues/433\n. looks good to me\n. hi @    skizunov \nThank you for reporting, yes I think there is a bug here: https://github.com/rabbitmq/rabbitmq-server/blob/rabbitmq-server-553/packaging/windows-exe/rabbitmq_nsi.in#L265 \nWe could:\n1. Remove always the ERLANG_HOME key and re-setup it each time, by removing this code  https://github.com/rabbitmq/rabbitmq-server/blob/rabbitmq-server-553/packaging/windows-exe/rabbitmq_nsi.in#L243  and make sure to call again findErlang \n2. Avoid to remove the key during the setup \nThe first option should be the best.\n. Changed with:\n=WARNING REPORT==== 27-Apr-2016::16:17:07 ===\nReceived a 'DOWN' message from rabbit1@mac but still can communicate with it\n. To avoid to use the ERLANG_HOME the cmd tools have to query the windows registry.\nI tried to implement it, but (obviously) works only as admin. The RabbitMQ cmd tools work also with the standard users, so it is not possible to apply the fix I tried.\nFor now I'd leave the setup as it.\nThoughts ?\nSince I spent time on it, I report it here, maybe it is useful for the future.\n```\ncall :finderlang \"/reg:64\" KEY_64\nif \"%KEY_64%\"==\"\" ( \n   call :finderlang \"/reg:32\" KEY_32\n   echo KEY 32 \"%KEY_32%\"\nif \"%KEY_32%\"==\"\" ( \n      echo \"not found\" \n   ) else (\n   ERLANG_HOME = %KEY_32%\n   ) \n) else  ERLANG_HOME = %KEY_64%\n:finderlang\nsetlocal\nfor /f \"skip=2\" %%a in ('reg query HKEY_LOCAL_MACHINE\\Software\\Ericsson\\Erlang %1 ') do (\n  ( endlocal\n  FOR /F \"tokens=2*\" %%L IN ('reg query %%a %1 /v \"\" ') DO (\n       set \"%~2=%%M\"\n    )\n   )\n )\nexit /b\n``\n. During the tests we also had a crash node\nIt causedcrashed` queues as:\n\n. Hi @cenekzach\nI can reproduce the issue in this  way:\n1. Publish messages with an high rate, until the queues go in flow status:\nrabbitmqctl list_queues name arguments state\n   test_850_3  []  flow\n   test_850_11 []  flow\n   test_850_15 []  flow\n   test_850_27 []  flow\n   test_850_14 []  flow\n   test_850_18 []  flow\n   test_850_28 []  flow\n   test_850_1  []  flow\n   test_850_7  []  flow\n   test_850_10 []  flow\n1. apply the policy rabbitmqctl set_policy Lazy \"\" '{\"queue-mode\":\"lazy\"}' --apply-to queues \n2. stop publish.\nWhat happen:\nThe queues enter in wait_for_msg_store_credit\nThe credit_flow:blocked() should return true but gets always false as:\n(rabbit@mac)6>\n(rabbit@mac)6> dbg:start().\n{ok,<0.3806.0>}\n(rabbit@mac)7> dbg:tracer().\n{ok,<0.3806.0>}\n(rabbit@mac)8> dbg:tpl(credit_flow, x).\n{ok,[{matched,rabbit@mac,14},{saved,x}]}\n(rabbit@mac)9>\n(rabbit@mac)9> dbg:p(all, c).\n(<0.3540.0>) call credit_flow:blocked()\n(<0.3540.0>) returned from credit_flow:blocked/0 -> false\n(<0.3446.0>) call credit_flow:blocked()\n{ok,[{matched,rabbit@mac,310}]}\n(<0.3446.0>) returned from credit_flow:blocked/0 -> false\n(<0.3540.0>) call credit_flow:blocked()\n(rabbit@mac)10> (<0.3471.0>) call credit_flow:blocked()\n(<0.3522.0>) call credit_flow:blocked()\n(<0.3446.0>) call credit_flow:blocked()\n(<0.3540.0>) returned from credit_flow:blocked/0 -> false\n(<0.3471.0>) returned from credit_flow:blocked/0 -> false\nIt enters in a loop\n. Currently the rabbitmq-server-xxx.noarch.rpm works under Centos 6 and Centos 7. \nAfter merged this PR, will it work on Centos 6 ?  Since systemd  is only for Centos 7. \nI made a quick test:\n```\ncat /etc/redhat-release\nCentOS release 6.8 (Final)\nsudo rpm -i rabbitmq-server-3.6.5.777-1.el7.centos.noarch.rpm\nerror: Failed dependencies:\n    systemd is needed by rabbitmq-server-3.6.5.777-1.el7.centos.noarch\n```\nAm I missing something ?\nThanks\n. @harlowja you are right, I created the package using Centos 6 and works correctly.\nAt this point, do we need two RPM versions ? \nrabbitmq-server-3.6.5.888-1.el6.noarch.rpm\nand\nrabbitmq-server-3.6.5.888-1.el7.noarch.rpm\n? \n. @diaolanshan The new parameter lazy_queue_explicit_gc_run_operation_threshold will not solve this issue, but part of it.\nPlease see https://github.com/rabbitmq/rabbitmq-server/pull/978.\n. we looked in deep into the issue and just to clarify: \nThe memory does not grow up always when the lazy queues get in flow status, but only under certain conditions. \nHere is two tests that explain in detail the problem:\nTest1 - 5 Connections - 10 Channels -  Body size 51200 bytes\n\u279c  scripts git:(rabbitmq-server-973) \u2717 ./rabbitmqctl list_queues message_bytes message_bytes_ready message_bytes_ram name state policy\nListing queues ...\n9145088000  9145088000  0 lazy.t_2  flow  Lazy\n9145088000  9145088000  0 lazy.t_0  flow  Lazy\n9145088000  9145088000  0 lazy.t_1  flow  Lazy\n\u279c  scripts git:(rabbitmq-server-973) \u2717 ./rabbitmqctl list_connections name state | grep -v running\nListing connections ...\n127.0.0.1:61419 -> 127.0.0.1:5672 flow\n127.0.0.1:61420 -> 127.0.0.1:5672 flow\n127.0.0.1:61422 -> 127.0.0.1:5672 flow\n127.0.0.1:61423 -> 127.0.0.1:5672 flow\n127.0.0.1:61424 -> 127.0.0.1:5672 flow\n\u279c  scripts git:(rabbitmq-server-973) \u2717 ./rabbitmqctl list_channels name state | grep -v running\nListing channels ...\n127.0.0.1:61419 -> 127.0.0.1:5672 (1) flow\n127.0.0.1:61424 -> 127.0.0.1:5672 (2) flow\n127.0.0.1:61423 -> 127.0.0.1:5672 (1) flow\n127.0.0.1:61419 -> 127.0.0.1:5672 (2) flow\n127.0.0.1:61422 -> 127.0.0.1:5672 (1) flow\n127.0.0.1:61424 -> 127.0.0.1:5672 (3) flow\n127.0.0.1:61419 -> 127.0.0.1:5672 (3) flow\n127.0.0.1:61422 -> 127.0.0.1:5672 (2) flow\n127.0.0.1:61423 -> 127.0.0.1:5672 (2) flow\n127.0.0.1:61420 -> 127.0.0.1:5672 (1) flow\nIt works correctly. Queues, Connections and Channels get in flow status\nTest2 - 1050 connections and 1050 channels -  Body size 51200 bytes\n\u279c  scripts git:(rabbitmq-server-973) \u2717 ./rabbitmqctl list_queues message_bytes message_bytes_ready message_bytes_ram name state policy\nListing queues ...\n162304000 75264000  0 lazy.t_2 flow  Lazy\n349286400 180326400 0 lazy.t_1 flow  Lazy\n\u279c  scripts git:(rabbitmq-server-973) \u2717 ./rabbitmqctl list_connections name state | grep -v running\nListing connections ...\n\u279c  scripts git:(rabbitmq-server-973) \u2717 ./rabbitmqctl list_channels name state | grep -v running\nListing channels ...\nHere the queues get in flow status but connections and channels don''t.\nIn this case, the channels continue to publish the messages always in the same rate. \n_How to solve this situation?_\nThere are a few configurations to tune.\nServer Side:\n- reduce the credit_flow_default_credit  in this case the channels get in flow status first.\n- increase msg_store_file_size_limit\n- in same situation, can be useful to disable fhc_*_buffering\n{fhc_read_buffering,false},\n  {fhc_write_buffering,false},\n- Increase the file descriptors (ulimit -n)\n- Staring from 3.6.6 also this parameter can be tuned \n- using a cluster configuration can certainly helps, but the queues should be distributed equally across the nodes, to distribute the writes.\nClient Side:\n- for big string buffers (typically string serialization as XML or JSON  ), can be useful compress the buffers before publish it and decompress it after dequeue. \n. @michaelklishin \nyes eacces is hard to understand\nbut when the port is used, it is enough clear with eaddrinuse :\n```\n=ERROR REPORT==== 24-Jan-2017::14:10:51 ===\nFailed to start Ranch listener {acceptor,{0,0,0,0,0,0,0,0},1883} in ranch_tcp:listen([{port,\n      1883},\n     {ip,\n      {0,\n       0,\n        0,\n        0,\n       0,\n        0,\n        0,\n       0}},\n     inet6,\n     {backlog,\n      128},\n     {nodelay,\n      true}]) for reason eaddrinuse (address already in use)\n````\n. (After internal conversation )\nMaking the enabled_plugins file world-readable is the easier ( and safest) way. \n. Let's recap the problem:\nThe file enabled_plugins is created by rabbitmq-plugins inside the dir /etc/rabbitmq\nNow the directory has the following permissions:\nroot@jessie:/home/vagrant# ls -la /etc/rabbitmq/\ndrwxr-xr-x  2 root root 4096 May 25 10:44 .\ndrwxr-xr-x 91 root root 4096 Jun 20 17:23 ..\nSo the rabbitmq group can't access on this file:\nroot@jessie:/home/vagrant# rabbitmq-plugins enable rabbitmq_management\n...\nApplying plugin configuration to rabbit@jessie... failed.\nError: {cannot_read_enabled_plugins_file,\"/etc/rabbitmq/enabled_plugins\",\n           eacces}\n(Note: the error we see here is from RabbitMQ server and not from rabbitmq-plugins ) \nWe have two solutions:\n1 - Modify rabbitmq-plugins adding umask 0022 as suggested here.\n2 - Modify the setup and give the access to /etc/rabbitmq also to the rabbitmq group:\nchgrp rabbitmq /etc/rabbitmq ; chmod g+s /etc/rabbitmq\nIn this way the RabbitMQ service can access without problems. \nWe ( Me and @dumbbell) prefer the second solution, we'd like to avoid to modify the umask inside rabbitmq-plugins \nWe exclude this solution, because the the owner of enabled_plugin file is root ( in this case ) so copy the rights won't solve the problem. \nImportant note:\ninside /etc/rabbitmq there is also the rabbitmq.config file, it can be created manually or by tools, so the problem is not only rabbitmq-plugins.\nThe problem can be solved using the solution 2:\nroot@jessie:/# ls -la /etc/rabbitmq/\ndrwxr-sr-x  2 root rabbitmq  4096 Jun 20 19:48 .\ndrwxr-xr-x 91 root root      4096 Jun 20 19:36 ..\n-rw-r-----  1 root rabbitmq    23 Jun 20 19:48 enabled_plugins\n-rw-r-----  1 root rabbitmq 22321 Jun 20 19:47 rabbitmq.config \n. @selivan @michaelklishin :\nPackages uploaded here: https://github.com/rabbitmq/rabbitmq-server-release/pull/30#issuecomment-310658554\n . @selivan Thank you for your test, and glad to hear that it fixes your case.\nAt this point I'd merge the PR https://github.com/rabbitmq/rabbitmq-server-release/pull/30, then we can investigate how to handle stricter umask like 0077.\n. Hi,\nI tried different solutions as\n- set the permissions on the rabbitmq-plugins code.\nChange/play with permissions inside the code can be dangerous, we have to considerer other platforms and file-system types. \n\navoid to recreate the file\n  Even if this could be a solution, it requires a manual configuration to set the right permissions to the file, so I'd exclude it.\n\nIn my option working with umask as 077 can be solved in different ways, for example:\n- run the command with another umask\n``` \numask\n0077\n(umask 007; rabbitmq-plugins enable rabbitmq_federation)\nThe following plugins have been enabled:\n  amqp_client\n  rabbitmq_federation\nApplying plugin configuration to rabbit@jessie... started 2 plugins.\n```\n\n\nusing the ACL:setfacl -d -m group:rabbitmq:r /etc/rabbitmq/ \n\n\nand other permissions configurations\n\n\nIn conclusion, I'd avoid to change the code/setup for this specific situation, we can document it by showing the problem and the possible solutions\n.  I agree with it. @michaelklishin Before QA,  I'd like some tests, as here\npeer_discovery.tests.rabbitmq.net it is only ipv4 . @michaelklishin ready. \nTested also with an unreachable dns:\n2017-04-24 17:44:21.973 [info] <0.160.0> Addresses discovered via A records of discovery.eng.example.local.doesnot.exist.io:\n2017-04-24 17:44:22.650 [info] <0.160.0> Addresses discovered via AAAA records of discovery.eng.example.local.doesnot.exist.io:\n2017-04-24 17:44:22.650 [info] <0.160.0> Discovered no peer nodes to cluster with\n. Hi @501956430 \nPlease post this question and this information to the rabbitmq-users mailing list. GitHub is used for bugs and other development-related items, not for general discussion or diagnosis.\n. Tried on:\ncat /etc/redhat-release\nCentOS Linux release 7.2.1511 (Core)\nWith:\n-rw-rw-r--. 1 vagrant vagrant 18580960 Aug 15  2016 erlang-19.0.4-1.el7.centos.x86_64.rpm\n-rw-rw-r--. 1 vagrant vagrant  4963465 Aug 16 13:34 rabbitmq-server-3.6.11-1.el7.noarch.rpm\nIt works without problems.\nsudo systemctl status rabbitmq-server\n\u25cf rabbitmq-server.service - RabbitMQ broker\n   Loaded: loaded (/usr/lib/systemd/system/rabbitmq-server.service; disabled; vendor preset: disabled)\n   Active: active (running) since Tue 2017-08-22 11:41:05 UTC; 5min ago\n Main PID: 12767 (beam)\n   Status: \"Initialized\"\n   CGroup: /system.slice/rabbitmq-server.service\n           \u251c\u250012767 /usr/lib64/erlang/erts-8.0.3/bin/beam -W w -A 64 -P 1048576 -t 5000...\n           \u251c\u250012909 /usr/lib64/erlang/erts-8.0.3/bin/epmd -daemon\n           \u251c\u250013008 erl_child_setup 1024\n           \u251c\u250013015 inet_gethost 4\n           \u2514\u250013016 inet_gethost 4\nIt'd be useful to have your logs, and the OS details  . @564064202 \nThank you for your time\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on.\nPlease post this to rabbitmq-users.. @564064202 I already posted an answer on stackoverflow:\nhttps://stackoverflow.com/questions/46049536/how-i-can-make-rabbitmq-server-auto-start-at-boot-timecentos-7\nIf you can't use the google group, please use stackoverflow\nThank you\n . hi @AceHack \nThere is this example here: https://github.com/rabbitmq/rabbitmq-autocluster/tree/master/examples/compose_consul_haproxy \nAnd also there are examples here: https://github.com/Gsantomaggio/rabbitmqexample/tree/master/cluster_docker_compose\nAbout the second one, soon we will move the examples in the official repository.\n. @lukebakken thanks \nbut I forgot to mention that:\n$ cat etc/rabbitmq/rabbitmq.conf\nvm_memory_high_watermark.relative = 0.3\ninet_dist_listen_min = 50000\ninet_dist_listen_max = 50000\ndoes not work :)\nsee:\n\u279c  rabbitmq_server-3.7.11 sbin/rabbitmqctl report | grep inet_dist_listen_min\n      {inet_dist_listen_min,25672}, \nthis because of the RABBITMQ_DIST_PORT is assigned during the rabbit_prelaunch \nIn 3.6.x it worked because of the value was read directly from the conf file\nin 3.7.x is suppose to read RABBITMQ_CONFIG_ARG_FILE\n. @lukebakken I think the same, I was focusing on the advanced.config because  I get this message:\nUnable to update config for app kernel from a .conf file. The app is already running. \nUse advanced.config instead. . @michaelklishin I will.\nI was looking for the stable branch, but I think you changed the repositories organization :)! \n@lukebakken \nI am going to remove this configuration from the rabbitmq.conf.example file\n. I had  to move this code here (Didn't commit yet). Because here the service is already installed.\nOn my tests work better, Makes sense for you? \n. I removed the double quotes also here, for the same reason I did here\n. this else if  should be else if exist \"!RABBITMQ_CONFIG_FILE!.conf\" as rabbitmq-server.bat\n. I am not sure this set is necessary. \nIn rabbitmq-server.bat is not present. \n. Yes, as first choice we wrote a message similar to  yours:\n\nDisconnection from node ~p but the node is still running\n\nI decided to change it, because this call checks if the node x is running.  It does not receive the DOWN message. \nBTW maybe for the user is most clear another kind of the message. I'm going to change it\n. directoy should be directory . check a substring because they could change the sub-domain\nrabbit_peer_discovery_dns:discover_hostnames(\"www.v6.facebook.com\", true).\n[\"v6-prod6-shv-09-frc3.facebook.com\"]. Should we add supports_registration() to rabbit_peer_discovery_backend ? \n. @lukebakken should  the old TargetRamCount be restored after convert_to_lazy ?. ",
    "yottatsa": "I found that https://github.com/yandex/inet64_tcp fixes clustering over IPv6 if you talking about it.\n. ",
    "gsogol": "The queue that the client creates should be on a node where the specified strategy meets the criteria.\n. Thank you\n. if it's done, then yeah, If I specified 2, it should be kept at 2.\n. Thanks\n. IMHO, the definition of done should be whether or not it always works. We are experiencing this even with 3.5. Your testing from most production setups may be different. Please test with after your rework to Raft but it should work 100% of the time, not 99%. \n. I agree that it wouldn't make sense to do this now if you're planning big changes in 3.6. Waiting makes sense. Just wanted to clarify that low occurrences are still had. \n\nOn Aug 10, 2015, at 8:22 AM, Michael Klishin notifications@github.com wrote:\n@gsogol we completely agree that it should work correctly 100% of the time. Our question really is, should we try to solve this with an ad-hoc protocol (something we're moving away from) or after we introduce Raft into the core.\n\u2014\nReply to this email directly or view it on GitHub.\n. So the main ask is to union or merge the 2 clusters when autoheal happens. You can't just discard data. These message could have million $ orders. I'd hate to lose them :)\n. The request is about a UI plugin. I know about shovels and federation. There are scenarios where you just tell the admin or in devops world a developer and s/he just goes in and manually moves the messages. There are many cases where you want to move messages from a DLQ back to a specific exchange to be processed because you fixed a bug on the subscriber side and want those messages replayed.\n. \n",
    "jemc": "I've seen lots of discussion about this, and just wanted to step in and +1 this feature request.\nSpecifically, I'd think it would be most useful to focus on merging messages rather than conceptual server objects (like queues and exchanges), which in most use cases seem to be fairly steady state.  This may simplify the problem space enough to make it feasible.\nI'll also add in some relevant links I've seen of discussion on this topic:\nhttp://www.refactorium.com/distributed_systems/messaging/rabbit/#conclusion\nhttps://aphyr.com/posts/315-call-me-maybe-rabbitmq#comment-1930\nhttp://comments.gmane.org/gmane.comp.networking.rabbitmq.general/33032\n. Is there anywhere you can point me to read more about the new schema data store?. Yeah, I saw that link in your earlier post, thanks.\nAre there any mailing list discussions, issue tickets, or code commits you can point me to that I can read about it in more detail?. Thanks!. ",
    "tianon": "Sorry, :+1:; su does strange things.\n. :heart:!\n. @dumbbell oh cool, I'll take a look!\n. So, the first issue I get testing that is:\ntrap: SIGHUP: bad trap\nLooking at the change (which isn't too bad with ?w=1: https://github.com/rabbitmq/rabbitmq-server/compare/master...rabbitmq-server-234?w=1), I'm also a little bit concerned that the strategy for fixing this is just to keep the wrapping /bin/sh alive and have it trap the signals instead -- that's essentially what our tini solution already does, albeit without the attempt to run rabbitctl stop.  If that's the way this needs to be fixed, it's OK, it just feels like a workaround. :smile:\n. Seems to work OK. :+1:\nI'm still a little leary of having the shell stay resident, but if this is the best path, sounds good enough! :metal:\n. :+1: fair enough!\n. Nice, sounds good!  I left total_memory_available_override_value commented out as a TODO for now. :+1: :heart:. ",
    "mattmaniflaf": "Just wanted to add some more information to this bug, as I am now also hitting it.\nAfter reading thread here and on rabbtimq-users list https://groups.google.com/forum/#!topic/rabbitmq-users/Gca8vW52gB8. It was deemed to be a 32 vs 64 bit issue.\nI don't believe this to be the case, but rather a SPARC vs x86 issue.\nOn s11u3 x86:\n  1> math:exp(-1162.102134881488).\n   0.0\nOn s11u3 SPARC:\n  1> math:exp(-1162.102134881488).\n   ** exception error: an error occurred when evaluating an arithmetic expression\n        in function  math:exp/1\n           called as math:exp(-1162.102134881488)\nBoth systems are running 64-bit Erlang 17.5 in a 64-bit environment. \n. ",
    "maggyero": "The maximum message size in RabbitMQ was 2 GiB up to version 3.7:\n%% Trying to send a term across a cluster larger than 2^31 bytes will\n%% cause the VM to exit with \"Absurdly large distribution output data\n%% buffer\". So we limit the max message size to 2^31 - 10^6 bytes (1MB\n%% to allow plenty of leeway for the #basic_message{} and #content{}\n%% wrapping the message body).\n-define(MAX_MSG_SIZE, 2147383648).\nReference: https://github.com/rabbitmq/rabbitmq-common/blob/v3.7.13/include/rabbit.hrl#L279\nIt is now 512 MiB from version 3.8:\n%% Max message size is hard limited to 512 MiB.\n%% If user configures a greater rabbit.max_message_size,\n%% this value is used instead.\n-define(MAX_MSG_SIZE, 536870912).\nReference: https://github.com/rabbitmq/rabbitmq-common/blob/master/include/rabbit.hrl#L255. ",
    "jakauppila": "Thanks, let me know if there's any questions/concerns.\nOne thing I wasn't 100% sure on was simply the location of the file. It may be worth defining a new environment variable for it's location, if it doesn't exist there, check %RABBITMQ_BASE%, if it doesn't exist there, check the default location !APPDATA!\\RabbitMQ.\n. Sure, that makes sense. There will just have to be additional logic on every environment variable check:\nbatch\nif \"!RABBITMQ_NODENAME!\"==\"\" (\n    if \"!NODENAME!\"==\"\" (\n        set RABBITMQ_NODENAME=rabbit@!COMPUTERNAME!\n    ) else (\n        set RABBITMQ_NODENAME=!NODENAME!\n    )\n)\nIf that looks fine, I can continue making the additional changes.\nJust another note on the default location, I know on *nix it defaults to /etc/rabbitmq/rabbitmq-env.conf so it would make sense to default to !APPDATA!\\RabbitMQ\\rabbitmq-env.bat on Windows.\nSince I would like the ability to override it could be done indirectly via !RABBITMQ_BASE!\\rabbitmq-env.bat like I have above, but that seems a little silly since that environment variable isn't configured with an install. Is this fine since it's more of an advanced feature/config or should it check that location and then the default if it doesn't exist?\n. Hi @dumbbell, sorry for the late response, very extended weekend.\nThat looks great to me; it'll be nice to have the same logic/functionality on both sides.\nAs far as putting these changes together, want me to take a stab at it or do you?\n. @michaelklishin Will do, thanks.\n. Sorry for the delay. Finally got some time to work on it and committed the changes.\nFor the *nix scripts it's mimicking for rabbitmq-defaults.bat and rabbitmq-env.bat, I commented out those lines and put the batch equivalents below it for easier review.\n. @uvzubovs At a preliminary glance it looks like you can use an iRule on the Virtual Server to implement Proxy Protocol v1; but there isn't anything baked into the appliance to enable the functionality.. Do you like to have associated issues for PRs even when they're something small like this? Or is just the PR fine?. ",
    "rjrizzuto": "Using RabbitMQ broker version 3.4.4 with my .Net app (using RabbitMQ.Client 3.4.3) I was able to run for about 1/2 an hour, and publish 46GB of data (24 million messages) to the queue using persistent messages.  The publisher did eventually crash, but there is no unexpected_frame message in the broker log. The last item in the broker log is:\n```\n=INFO REPORT==== 15-May-2015::10:35:29 ===\nvm_memory_high_watermark set. Memory used:3419764032 allowed:3405098188\n=WARNING REPORT==== 15-May-2015::10:35:29 ===\nmemory resource limit alarm set on node rabbit@CYAN199B.\n\n Publishers will be blocked until this alarm clears \n**********\n```\nThe erl.exe process is at 3.4G working set.  The connection shows as blocked in the web page, the channel shows as idle.\nThe exception the publisher logged is:\nUnhandled Exception: System.IO.IOException: Unable to write data to the transport connection: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because\ns.SocketException: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n   at System.Net.Sockets.Socket.Send(Byte[] buffer, Int32 offset, Int32 size, SocketFlags socketFlags)\n   at System.Net.Sockets.NetworkStream.Write(Byte[] buffer, Int32 offset, Int32 size)\n   --- End of inner exception stack trace ---\n   at System.Net.Sockets.NetworkStream.Write(Byte[] buffer, Int32 offset, Int32 size)\n   at System.IO.BufferedStream.Flush()\n   at System.IO.BinaryWriter.Flush()\n   at RabbitMQ.Client.Impl.SocketFrameHandler.WriteFrame(Frame frame)\n   at RabbitMQ.Client.Impl.Command.Transmit(Int32 channelNumber, Connection connection)\n   at RabbitMQ.Client.Impl.SessionBase.Transmit(Command cmd)\n   at RabbitMQ.Client.Impl.ModelBase.ModelSend(MethodBase method, ContentHeaderBase header, Byte[] body)\n   at RabbitMQ.Client.Framing.Impl.Model._Private_BasicPublish(String exchange, String routingKey, Boolean mandatory, Boolean immediate, IBasicProperties basicProperties, Byte[] body)\n   at RabbitMQ.Client.Impl.ModelBase.BasicPublish(String exchange, String routingKey, Boolean mandatory, Boolean immediate, IBasicProperties basicProperties, Byte[] body)\n   at RabbitMQ.Client.Impl.ModelBase.BasicPublish(String exchange, String routingKey, IBasicProperties basicProperties, Byte[] body)\n   at rabbitPublisher.rabbitPublisher.Main(String[] args) in c:\\Users\\rizzuto\\Documents\\Visual Studio 2012\\Projects\\queuing\\rabbitPublisher\\rabbitPublisher.cs:line 55\nMy guess is that the memory is used up due to the metadata/indexes that the broker keeps in memory, causing the connection to block and the publisher to die.  If so, this may just be expected behavior, but I wanted to provide all the details in case they are pertinent.\n. Great.  It looks like I might be able to handle that in code by using the Connection.Blocked and Unblocked event handlers, and not publishing while blocked.\nIn any event, I'll use 3.4.4 for my testing for now.  \nOut of curiosity, does 3.5.2 use as much memory for indexes?\n. Results from rabbitmqctl status seem to confirm that:\nStatus of node rabbit@CYAN199B ...\n[{pid,8552},\n {running_applications,\n     [{rabbitmq_management,\"RabbitMQ Management Console\",\"3.4.4\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.4.4\"},\n      {webmachine,\"webmachine\",\"1.10.3-rmq3.4.4-gite9359c7\"},\n      {mochiweb,\"MochiMedia Web Server\",\"2.7.0-rmq3.4.4-git680dba8\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.4.4\"},\n      {rabbit,\"RabbitMQ\",\"3.4.4\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.12.5\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.3.1\"},\n      {sasl,\"SASL  CXC 138 11\",\"2.4.1\"},\n      {inets,\"INETS  CXC 138 49\",\"5.10.6\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.4.4\"},\n      {xmerl,\"XML parser\",\"1.3.7\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"2.4\"},\n      {kernel,\"ERTS  CXC 138 10\",\"3.2\"}]},\n {os,{win32,nt}},\n {erlang_version,\n     \"Erlang/OTP 17 [erts-6.4] [64-bit] [smp:8:8] [async-threads:30]\\n\"},\n {memory,\n     [{total,3411540720},\n      {connection_readers,9064},\n      {connection_writers,2704},\n      {connection_channels,9104},\n      {connection_other,28024},\n      {queue_procs,6671128},\n      {queue_slave_procs,0},\n      {plugins,511352},\n      {other_proc,13991432},\n      {mnesia,65512},\n      {mgmt_db,184856},\n      {msg_index,2935088024},\n      {other_ets,415194904},\n      {binary,14415344},\n      {code,19975441},\n      {atom,703377},\n      {other_system,4690454}]},\n {alarms,[memory]},\n {listeners,[{clustering,25672,\"::\"},{amqp,5672,\"127.0.0.1\"}]},\n {vm_memory_high_watermark,0.4},\n {vm_memory_limit,3405098188},\n {disk_free_limit,50000000},\n {disk_free,806677446656},\n {file_descriptors,\n     [{total_limit,8092},\n      {total_used,6},\n      {sockets_limit,7280},\n      {sockets_used,2}]},\n {processes,[{limit,1048576},{used,196}]},\n {run_queue,0},\n {uptime,6686}]\n. Have you had any success reproducing this at least?  It seems it is sensitive to the amount of data published to the queue, not the speed, since the python script I wrote published at ~300 messages/sec vs 9000 for .net.\n. I'm glad to hear that you were able to reproduce it.  Hopefully the fact that it is more reproducible in 3.5.x will make it easier to fix.\n. @dumbbell Keep in mind the same symptoms show up at the server with either a Python or C# publisher.  Unless they copied from each other, it seems more likely that the issue is in common code, i.e. the RabbitMQ server component.\nI was also trying to look at the packet capture with wireshark, but having some difficulty, so I am not sure how much to trust it.\nOne question I had was how the client libraries handle a connection that is in flow control.  Does a tcp send return e_wouldblock or the like, and does the client lib retry later, or skip that send and go on to the next?\n. It seems to me that sending Basic.Publish+Content-header+Content-Body in one atomic send may \"fix\" the issue with the server receiving an invalid sequence, but it would break the implied contract of reliable delivery with the publisher.  Retrying would be necessary to adhere to the contract.\n. Thanks for the update.  I'm glad you've got a handle on this.\nOne thought I had is that it might be helpful to have some logging in the client libraries for exception cases like this.  For .Net, something like Common.Logging would work, and allow the user to decide where to log to.\n. In general, we try to log exception/error cases only so as not to impact performance.  We do use debug level logging for development purposes, and set the log filtering in production to only include info and higher.\nHere's a perfect case in point from AutorecoveringConnection.cs:\ncatch (Exception e)\n                    {\n                        // TODO: logging\n                        Console.WriteLine(\"BeginAutomaticRecovery() failed: {0}\", e);\n                    }\nWith common.logging, I'd do the following:\ncatch (Exception e)\n                    {\n                        logger.Warn(\"BeginAutomaticRecovery() failed\", e);\n                    }\nThe logger would log all the details, including stack trace (configurable in the logging config file) about the exception.\n. I have moved the logging suggestion.\n. Have you had any luck finding the issue in the .Net client?  I only tested with Python to see if it was the client lib or the message broker, and apparently that was a red herring anyway.\n. Thanks, that is great news!\nOn Mon, Jun 15, 2015, 2:14 PM Michael Klishin notifications@github.com\nwrote:\n\nYes, we understand what the issue is. Will fix it for .NET after s few\nother .NET issues are merged.\n\nOn 15/6/2015, at 19:11, rjrizzuto notifications@github.com wrote:\nHave you had any luck finding the issue in the .Net client? I only\ntested with Python to see if it was the client lib or the message broker,\nand apparently that was a red herring anyway.\n\u2014\nReply to this email directly or view it on GitHub.\n\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/156#issuecomment-112159328\n.\n. If the single write fails, does that mean the data is lost?\n\nIf you have a binary, I can give it a try.\n. What I mean is that if the write times out due to flow control, and there is an exception, is the frame set lost, or retried later?  I'm concerned about being in flow control and having the publisher quietly losing data.  In my application, I'd prefer to see the publisher retry or block till the flow control state is lifted.  Of course, other applications may prefer a different policy in such a case.\n. Thanks for clarifying\nOn Thu, Jun 25, 2015 at 8:25 PM Michael Klishin notifications@github.com\nwrote:\n\nBlocking or retrying is the plan (this is what other clients do).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/156#issuecomment-115441023\n.\n. I'm out of the office this week, but will give it a try when I get back.\n. I'm having trouble getting the .Net library.  The company I work at blocks file sharing sites, such as google drive, as well as most email attachments.  It's quite annoying.\n. You already did, but my company strips binary attachments.\n. I'm working with our tech department to see if they can get the file from the link you posted.\n. After running for a while (1.2 million message of size 1024), I get a System.IO.IOException that says \"Unable to write data to the transport connection: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.\".  Here's the stack trace:\n\nStackTrace  \"   at System.Net.Sockets.NetworkStream.Write(Byte[] buffer, Int32 offset, Int32 size)\\r\\n   at System.IO.BufferedStream.Flush()\\r\\n   at System.IO.BinaryWriter.Flush()\\r\\n   at RabbitMQ.Client.Impl.SocketFrameHandler.WriteFrameSet(IList`1 frames)\\r\\n   at RabbitMQ.Client.Framing.Impl.Connection.WriteFrameSet(IList`1 f)\\r\\n   at RabbitMQ.Client.Impl.Command.TransmitAsFrameSet(Int32 channelNumber, Connection connection)\\r\\n   at RabbitMQ.Client.Impl.Command.Transmit(Int32 channelNumber, Connection connection)\\r\\n   at RabbitMQ.Client.Impl.SessionBase.Transmit(Command cmd)\\r\\n   at RabbitMQ.Client.Impl.ModelBase.ModelSend(MethodBase method, ContentHeaderBase header, Byte[] body)\\r\\n   at RabbitMQ.Client.Framing.Impl.Model._Private_BasicPublish(String exchange, String routingKey, Boolean mandatory, Boolean immediate, IBasicProperties basicProperties, Byte[] body)\\r\\n   at RabbitMQ.Client.Impl.ModelBase.BasicPublish(String exchange, String routingKey, Boolean mandatory, Boolean immediate, IBasicProperties basicProperties, Byte[] body)\\r\\n   at RabbitMQ.Client.Impl.ModelBase.BasicPublish(String exchange, String routingKey, Boolean mandatory, IBasicProperties basicProperties, Byte[] body)\\r\\n   at RabbitMQ.Client.Impl.ModelBase.BasicPublish(String exchange, String routingKey, IBasicProperties basicProperties, Byte[] body)\\r\\n   at RabbitMQ.Client.Impl.AutorecoveringModel.BasicPublish(String exchange, String routingKey, IBasicProperties basicProperties, Byte[] body)\\r\\n   at rabbitPublisher.rabbitPublisher.Main(String[] args) in c:\\\\Users\\\\rizzuto\\\\Documents\\\\Visual Studio 2012\\\\Projects\\\\queuing\\\\rabbitPublisher\\\\rabbitPublisher.cs:line 68\"  string\nIs that expected?  Do I need to handle retry logic in my app for the message that was not sent, or is that done in the library?\nI can confirm that the connection is not dropping.\n. I split out handling of System.IO.IOExceptionto (just print a message) from System.Exception.  I am also getting a System.ObjectDisposedException {\"Cannot access a disposed object.\\r\\nObject name: 'System.Net.Sockets.Socket'.\"} with this call stack:\nStackTrace  \"   at System.Net.Sockets.Socket.Poll(Int32 microSeconds, SelectMode mode)\\r\\n   at RabbitMQ.Client.Impl.SocketFrameHandler.WriteFrameSet(IList`1 frames)\\r\\n   at RabbitMQ.Client.Framing.Impl.Connection.WriteFrameSet(IList`1 f)\\r\\n   at RabbitMQ.Client.Impl.Command.TransmitAsFrameSet(Int32 channelNumber, Connection connection)\\r\\n   at RabbitMQ.Client.Impl.Command.Transmit(Int32 channelNumber, Connection connection)\\r\\n   at RabbitMQ.Client.Impl.SessionBase.Transmit(Command cmd)\\r\\n   at RabbitMQ.Client.Impl.ModelBase.ModelSend(MethodBase method, ContentHeaderBase header, Byte[] body)\\r\\n   at RabbitMQ.Client.Framing.Impl.Model._Private_BasicPublish(String exchange, String routingKey, Boolean mandatory, Boolean immediate, IBasicProperties basicProperties, Byte[] body)\\r\\n   at RabbitMQ.Client.Impl.ModelBase.BasicPublish(String exchange, String routingKey, Boolean mandatory, Boolean immediate, IBasicProperties basicProperties, Byte[] body)\\r\\n   at RabbitMQ.Client.Impl.ModelBase.BasicPublish(String exchange, String routingKey, Boolean mandatory, IBasicProperties basicProperties, Byte[] body)\\r\\n   at RabbitMQ.Client.Impl.ModelBase.BasicPublish(String exchange, String routingKey, IBasicProperties basicProperties, Byte[] body)\\r\\n   at RabbitMQ.Client.Impl.AutorecoveringModel.BasicPublish(String exchange, String routingKey, IBasicProperties basicProperties, Byte[] body)\\r\\n   at rabbitPublisher.rabbitPublisher.Main(String[] args) in c:\\\\Users\\\\rizzuto\\\\Documents\\\\Visual Studio 2012\\\\Projects\\\\queuing\\\\rabbitPublisher\\\\rabbitPublisher.cs:line 69\"    string\nThat seems like an issue.\n. I think the System.ObjectDisposedException exception was related to a heartbeat timeout while paused in the debugger.\nThe System.IO.IOException seems to be the result of flow control, and perhaps that is expected behavior, but if so it may need to be documented, especially for retry scenarios.\n. Is this change going into 3.5.4?\n. ",
    "greggwon": "We have a single connection to rabbit with both readers and writers using the same connection.  How much contention should we expect in that case.  Also, we seem to see that when the above situation occurs, all readers on a blocked connection seem to stop receiving messages.  Is this expected behavior in this case?  Should we have a single connection per client per queue?\n. Both purge and delete need to be asynchronous actions.   Delete needs to hide the queue as it is deleted in the background.  It should be possible to have a huge non-durable queue happen because a service is not running, and be able to delete that queue and start the service immediately.  Huge outages get created when non-durable queues have transient traffic in them due to some error which you'd like to fix \"right now\", but are stuck waiting for 10s of minutes for millions of records to be deleted from a queue that you just deleted.. ",
    "cressie176": "Deleting the x-death header between prior to republishing resolves the issue. I'd rather not do this as it's valuable information. It's easy for me to produce a test case if you don't mind running a node application.\n. ",
    "c-datculescu": "Ok, thank you very much. I guess i can try to learn erlang a little. Adding the header when message is routed is a fine option as well, didn't really consider it.\nThank you very much for the feedback.\n. @michaelklishin \nOne solution that crossed my mind is to enable tracing, but i am not sure about the performance downsides. At the moment the number of dispatched messages is not extremely high.\nThat would allow me to catch the time when message got dispatched as well.\nAny thoughts about using this?\n. I would not enable tracing full time. my idea was to enable it just enough to troubleshoot the issue and disable it afterwards.\nRegarding dead-lettering and headers, we hit a funny issue in production. if a message is dead lettered more than 498 times, as i remember, we start getting some errors. this is an edge case that we dealt with by limiting the number of times we dead-letter, but it was a little tricky to discover the reason.\nThanks for the answers.\n. After some tinkering we managed to get rid of the \"ghost\" queue. One node from the cluster was blocking the entire operation on that queue plus on a shovel generated queue. Removing the node fixed the issue for now but this led to message loss across the entire cluster.\n. Hello. We are having this issue since months as well, but while seek operations are atm shown at around 4-6 seconds we do not see any degradation in service.\nWould be cool to fix it though.\n. ",
    "Scratch-net": "Any quick workaround for this? Or better wait for 3.5.4? \n. Ok, will try. I just don't see commits referring #180\n. Yep, seems to be working after I fixed #222\n. Submitted #223 to fix that\n. ",
    "codingconcepts": "Hi guys,\nFollowing on from @scratch-net's comment, rather than having to wait for 3.5.4 or rolling a release candidate into a production environment, is there a config tweak I can make to reduce the amount of logging to the -sasl.log file?\nI've made the following modification which completely prevents the -sasl.log file from being created but A) it'd be useful to see errors and B) the .log file starts to log out a badarg error (see below):\nFROM:  -rabbit sasl_error_logger {file,\\\"\"!SASL_LOGS:\\=/!\"\\\"} ^\nTO:  -rabbit sasl_error_logger false ^\nError in .log file:\n=ERROR REPORT==== 17-Jul-2015::10:37:27 ===\n* Generic server rabbit_mgmt_external_stats terminating \n* Last message in was emit_update\n* When Server state == {state,8192,\n                               [{{io_read,bytes},0},\n                                {{io_read,count},0},\n                                {{io_read,time},0},\n                                {{io_reopen,count},0},\n                                {{io_seek,count},0},\n                                {{io_seek,time},0},\n                                {{io_sync,count},0},\n                                {{io_sync,time},0},\n                                {{io_write,bytes},0},\n                                {{io_write,count},0},\n                                {{io_write,time},0},\n                                {{mnesia_disk_tx,count},5},\n                                {{mnesia_ram_tx,count},45},\n                                {{msg_store_read,count},0},\n                                {{msg_store_write,count},0},\n                                {{queue_index_journal_write,count},0},\n                                {{queue_index_read,count},0},\n                                {{queue_index_write,count},0}],\n                               undefined,\n                               {set,0,16,16,8,80,48,\n                                    {[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]},\n                                    {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                      [],[],[]}}}}\n* Reason for termination == \n** {badarg,[{erlang,list_to_binary,[undefined],[]},\n            {rabbit_mgmt_external_stats,log_location,1,[]},\n            {rabbit_mgmt_external_stats,'-infos/2-lc$^0/1-0-',2,[]},\n            {rabbit_mgmt_external_stats,'-infos/2-lc$^0/1-0-',2,[]},\n            {rabbit_mgmt_external_stats,emit_update,1,[]},\n            {rabbit_mgmt_external_stats,handle_info,2,[]},\n            {gen_server,try_dispatch,4,[{file,\"gen_server.erl\"},{line,593}]},\n            {gen_server,handle_msg,5,[{file,\"gen_server.erl\"},{line,659}]}]}\nCheers,\nRob\n. Hmmm, that's strange, I made that change a little earlier but must have missed something before, it all looks good now!  Thanks Michael.\n. A bit more information as a follow-up:\n- The publishing nodes are running version 3.5.1 of the RabbitMQ server and the subscriber nodes are running 3.5.1 and 3.5.4.\n- Issue affects both a clustered and non-clustered setup\n- I use a max-length for all queues but the issue occurs whether the policy is present or not\n- I was affected by the SASL Excessive Logging issue but have applied a fix to all affected machines\n- While all nodes are in the same data centre at the moment, the federation connection are all SSL.  This doesn't slow things down too drastically though, the publishers manage to send 50,000 messages in ~5 seconds.\nThanks guys,\nRob\n. ",
    "pinepain": "Same for body-payload (message body) which covered by *OCTET grammar.\n. I double checked all my tests and environment and I found that I've made a stupid mistake during issue testing.\nI was testing the case in which client library (php-amqp) truncating message and I used the same queue name each time I run the test case and target queue was declared as persistent. And I didn't use auto-ack and didn't acked received message. So when I made proper fix to client lib code, I still received that fist truncated message, even already sending proper message.\nI'm terribly sorry that I bothered you.\n. ",
    "BlackRider97": "@michaelklishin  Thanks it worked now :-)\n. ",
    "weisslj": "OK! Updating the type does not remove support for R13B03. The way it is now all versions >= R16B03 get this warning when calling make dialyze:\nUnknown types:\n  digraph:digraph/0\nAfter this patch versions < R16B03 would get a Dialyzer warning. To support all versions, a configure-like check for the stdlib version or diagraph type would be required to set a preprocessor define.\nFeel free to close the pull request if you want to keep the old type instead.\n. Thanks!\n. ",
    "bogdando": "@dumbbell thank you for a very thorough review and time spent. I will address all of the vital issues\n. @dumbbell, addressed your comments for the bash related things. The checkbashisms tool from the devscript, which I used to make this OCF script POSIX compatible, should not show warnings now.\n. @michaelklishin , @dumbbell addressed the pidfile and FQDN related comments.\n. Addressed the rest of the changes, but cosmetic ones. Let's please keep them to be fixed in next patch sets after the this one accepted. I believe in this huge file, there are many cosmetic things to be polished :)\n. This contains major and minor bug fixes and several features, listed in the full change log.\nI tested this change manually. I suggest to sync fixes this way, unless we'll get CI here...\n. @michaelklishin There would be no harm to backport it as well. There were no any usage of this yet.\n. okay, will resubmit to stable as well!\n. But Is it OK to submit backport ahead of the master?..\n. Thank you too. More users of this OCF script we will manage to get, the better rabbitmq clustering UX for the community and other projects like OpenStack as well.\n. related master branch target https://github.com/rabbitmq/rabbitmq-server/pull/363\n. There was a change lost while I was syncing fixes...\n. There is related bug discovered in Fuel https://launchpad.net/bugs/1501731 and this patch addresses that issue.\n+1 LGTM for me\n. Please don't merge w/o @dmitrymex review\n. related to https://github.com/rabbitmq/rabbitmq-server/pull/480\n. Looks valid if we want no mirroring queues out of box, though this would have docs impact like: \"in order to enable mirroring queues, copy this example file as ...\"\n. right, I missed it has all lines commented out :)\n. I tested it on the vagrant box ( https://atlas.hashicorp.com/bogdando/boxes/rabbitmq-cluster-ocf ), seems working for me\n. tested the same way, vagrant box...\n. Misplaced sorry! Should be a stable\n. Resubmitted to stable as https://github.com/rabbitmq/rabbitmq-server/pull/525\n. @dmitrymex \n. should go to the stable\n. @dmitrymex\n. You wellcome! I found a completely new bug case for the beam process running unresponsive (like kill -STOP). Hence, new PR incoming ...\n. @dmitrymex \n. So I tested with the vagrant box against the test case described in the title\n. @dmitrymex \n. Please resubmit to the stable branch\n. Thank you! LGTM. @dmitrymex ?\n. LGTM, thank you\n. Yes, I liked the idea, @binarin did you test this?\n. I will test this as well, please hold for a while\n. works for me. It provides things like this:\n\"per-node explanation: [{rabbit@n2,ok,0},{rabbit@n1,ok,0}]\"\nWhich is: a node, ok-if all channels' status has been checked, how many channels has been checked.\nAnd if reported not ok for a non local node, that would mean that local node has been killed being innocent :)\n. yes please\n. https://github.com/rabbitmq/rabbitmq-server/pull/600 is the correct one\n. please hold for a while, I'm testing this\n. LGTM, thank you!\n. @dmitrymex \n. @michaelklishin please take a look\n. Fuel CI tests looks good, so LGTM, thank you!\n. Waiting for the patch test results from Fuel CI infra.\nJust wondering, could we think of the separate Travis CI job to this repo, see an example job  https://groups.google.com/forum/#!original/rabbitmq-users/BnoIQJb34Ao/eZmrONnpBQAJ\n. LGTM\n. LGTM, good point, thank you!\n. to stable please\n. LGTM\n. LGTM, the https://github.com/rabbitmq/rabbitmq-server/pull/657 must go first\n. @dmitrymex \n. @michaelklishin please take a look. I tested it for my fork and it works for me...\n. Note, it relies on a docker image bogdando/rabbitmq-cluster-ocf-wily\nand a Vagrantfile with provisioning scripts from an external repo https://github.com/bogdando/rabbitmq-cluster-ocf-vagrant.git.\nShould we rework those externals to be the part of the rabbitmq-server project as well?\nFor example, the docker image may be rebuilt and resubmitted to the rabbitmq-server project space at dockerhub. And the vagrantfile + scripts may be moved to the rabbitmq-* github space as well... Or we can just leave things as I proposed and do not consider the check as a voting one. Thoughts?\nAnd please also consider enabling git web hooks to see Travis build results?\n. An example PR https://github.com/bogdando/rabbitmq-server/pull/5 , which Travis CI check GROUP=2 (see the 45.1 gate only):\n- shall fail https://travis-ci.org/bogdando/rabbitmq-server/jobs/116131980\n- shall pass (ignore as no changes to the OCF RA) https://travis-ci.org/bogdando/rabbitmq-server/builds/116134576\n- shall pass (with changes made to the OCF RA) https://travis-ci.org/bogdando/rabbitmq-server/jobs/116135836\n. @dmitrymex \n. No one's interested? ;(\n. Thank you! Will you enable web hooks to see job results?\n. @dmitrymex \n. ready for review\n. @dumbbell please take a look\n. I'm not sure which default path to use. Could be as well a relative path, like just a \"...=set_rabbitmq_policy\". But I'm not a fan of relative paths for binaries :/ So, what do you think?\n. Could you elaborate the exact place for \"ocf_run does $(\"$@\")\" please?..\n. LGTM, thank you!\n@michaelklishin could you enable travis ci for the stable branch? I'd like to see results of the rabbit OCF gate for pull requests\n. @dmitrymex \n. LGTM, but yes please submit to the stable\n. @dmitrymex @relusek\n. LGTM, I will take a TODO item for me as well, which is update docs, with something like : The \"rabbitmqctl\" control plane's consequent timeouts can be detected by setting the max_rabbitmqctl_timeouts>0 (default is 3). Requires a Pacemaker >=1.1.14, otherwise it does noting. Set the max_rabbitmqctl_timeouts=0 to enable it in a simplest detection mode for older versions. Note: A simplest mode makes monitors to report failed resources after a very first encountered rabbitmqctl timeout.\n. +1, but please remove DO NOT MERGE if you think it's done and it works for you\n. Submitted to stable\n. LGTM\n. We had a long discussion on that topic, I checked the changes against network partitions (by Jepsen's Nemesis) and for all of the cases the cluster was auto recovered. Dmitry also prepared docs update (UML flow changes). So nothing to add from my side, LGTM. Thank you for a hard work, folks!\n. I'm not sure which problem you solve by replacing  eval \"mnesia:system_info(running).\" to \"lists:member('%s', rabbit_mnesia:cluster_nodes(running)).\". And how to test this.\n. LGTM and the standard check I do with jepsen (https://github.com/bogdando/rabbitmq-cluster-ocf-vagrant) works for me\n. :+1: \n. @binarin @dmitrymex \n. This doesn't help to fix the bug\n. I've reproduced this error having the patch in place:\nMnesia(rabbit@n2): ** ERROR ** (core dumped to file: \"/var/lib/rabbitmq/MnesiaCore.rabbit@n2_1473_324370_784463\")\n ** FATAL ** Failed to merge schema: Bad cookie in table definition rabbit_user_permission: rabbit@n2 = {cstruct,rabbit_user_permission,set,[],[rabbit@n2,rabbit@n1],[],[],0,read_write,false,[],[],false,user_permission,[user_vhost,permission],[],[],[],{{1473324142071425597,-576460752303421855,1},rabbit@n2},{{3,2},{rabbit@n2,{1473,324194,33515}}}}, rabbit@n1 = {cstruct,rabbit_user_permission,set,[],[rabbit@n1],[],[],0,read_write,false,[],[],false,user_permission,[user_vhost,permission],[],[],[],{{1473324350438460524,-576460752303423231,1},rabbit@n1},{{2,0},[]}}\nshall it clean up /var/lib/rabbitmq/MnesiaCore as well?\n. Ok, it should be working with the 3.6.6 only, yet released, or with the MOS downstream 3.6.5, so I ran tests against wrong version. I hope it works for you folks! :+1: \n. :+1: \n. @dmitrymex \n. Do you mean inserting ocf as source5 and renaming all shifted source5 to source6 ? This would introduce more unneeded changes, what is the point?\n. We have much bashisms in this script, I suggest to address refactoring to the \"sh\" by separate changes. Hopefully once we would have tests and maybe CI for this script as well\n. @dumbbell : If we wanted to cover this script with bats unit tests, should we want to keep it written as \"#!/bin/bash\" or make it 100% POSIX compatible won't hurt?\n. will do\n. should I introduce an OCF parameter like use_fqdn?\n. sounds reasonable\n. Understood. Please note, that OCF scripts in pacemaker seems always importing an ocf-shellfuncs \"library\", which has a bash shebang. Although, I have the version of this OCF script 100% /bin/sh compatible, I doubt it makes sense to stick with a \"sh\" shebang here because of the aforementioned ocf-shellfuncs... But I will update this patch set as well to be shell compatible :)\n. Done\n. done\n. done\n. done\n. done\n. Done\n. Done\n. Done\n. Done\n. This seems quite a cosmetic change, if you don't mind I will skip this for now.\n. This seems quite a cosmetic change, if you don't mind I will skip this for now.\n. This seems a cosmetic fix, although I agree with you. Let's keep this for the next change set please\n. This seems a cosmetic fix, although I agree with you. Let's keep this for the next change set please\n. This seems a cosmetic fix, although I agree with you. Let's keep this for the next change set please\n. Done\n. Done\n. will do\n. done\n. done\n. done\n. done\n. what is the point of this change, please elaborate?\nAh, I see there is a case when rc_check will be checked against uninitialized value. Could you please move its initializtaion to the line 1364? And resubmit the patch to the stable branch to get it to the 3.6.x fixes as well.\nNote, quotes aren't required as OCF_* return codes are numbers\n. yes, $OCF_SUCCESS\n. there is also 124 exit code we may want to inspect more precisely\n. Makes sense, although any version of the git and wget will fit out needs, the update is mostly required for the proper docker version\n. this ^^ so , which path? or just OCF_RESKEY_policy_file_default=\"set_rabbitmq_policy\" ?\n. It is not, it is distributed as an example in docs\n. how to update the [ -f foo ] test then?\n. I think the example file must do something if copied to /usr/sbin ?\n. Okay, if folks want package it, they may use OCF param instead.\nSo, let's keep the change simple, which is an absolute path with a simple -f test instead of complex PATH lookups\n. thank you! fixed\n. Perhaps we should add EnvironmentFile=/etc/rabbitmq/rabbitmq-env.conf ?\n. Shall we as well use this approach in the check_need_join_to()? Your change seems like the get_running_nodes() is not good :(\n. Or may be we need update get_nodes__base()'s eval \"mnesia:system_info(${infotype}).\" as well?\n. is it \"${seen_as_running}\" != \"true\" ?\n. Could you capitalize to MNESIA_SCHEMA_FILES? Please also take into account the ./MnesiaCore.rabbit@n2_1473_324370_784463 (that's an example name from my rmq 3.6.5 env, so please update filter). F.e. this filter worked for me:\np=\"/var/lib/rabbitmq/Mnesia*.rabbit@n3*\"\nrm -f $p\n. ",
    "marcelog": "That was a quick :) I just sent you the signed CA. \nThanks!\n. Hi guys,\nThanks for your comments! Both have valid points.\nI settled on this change because as Michael states, the function is available in the minimum OTP version required by RabbitMQ, and that seemed like a reasonable decision that didn't require to refactor more code. That same document also states that:\n\nIf you want the same format as returned by erlang:now/0, use erlang:timestamp/0.\n\nThe idea of the change was to keep the rest of the code \"as is\", since it depends on the result format. \nThoughts?\n. Hi @dumbbell,\nThanks for stepping in!\nIt's true that os:timestamp/0 is not monotonic, that's a good point. It is also true that the behavior of erlang:now/0 would make the system stall to compensate time differences, and might have a nasty effect too.\nI agree then, and it makes sense that perhaps one with a deeper understanding of the code works in a patch for each one of the use cases. Feel free to close this pull if you feel you want to go this way.\nThanks for the enriching discussion :)\n. ",
    "robsonpeixoto": "@michaelklishin Is possible to notify the memory using environment variable ?. ",
    "mthenw": "Awesome, thanks!\n. ",
    "occho": "What does this issue mean?\nIs it still an open issue?\n. OK, maybe I understand.\nMy first thought was the issue is CONF_ENV_FILE location cannot be overridden in any way due to some logic that depends on the default value (/etc/rabbitmq/rabbitmq-env.conf).\nI'm guessing you mean that we cannot override CONF_ENV_FILE variable by setting a corresponding environment variable like RMQ_CONF_ENV_FILE, right?\nWe currently changes the CONF_ENV_FILE value directly in rabbitmq-defaults.\n. ",
    "tiredpixel": "I can confirm this issue, which is also happening for us after upgrading from RabbitMQ 3.5.0 to 3.5.3 on Ubuntu Server 14.04 LTS, running in AWS EC2. Initially, I was running dist Erlang R16B03, but now I'm running Erlang R18. Interaction with RabbitMQ is using Ruby, through library Bunny 1.7.0, (with library amq-protocol 1.9.2).\nOur logic is similar to that described by @riyad, where messages are published through a dead-letter exchange potentially multiple times. I reinstalled our broker yesterday, and all was running fine until this afternoon, when errors were reported in RabbitMQ logs, after which CPU immediately started increasing fairly steadily, until the Erlang beam.smp process reported almost maxing out the CPUs. Messages being published or consumed are currently very low, around 1 message per 5 minutes. Number of messages in all queues is low, under 20 total. We're using durable queues and persistent messages. Memory is low, far below the watermark.\nPeace,\ntiredpixel\n\nStatus of node 'rabbit@i-x.example.com' ...\n[{pid,596},\n {running_applications,\n     [{rabbitmq_management,\"RabbitMQ Management Console\",\"3.5.3\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.5.3\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.5.3\"},\n      {rabbit,\"RabbitMQ\",\"3.5.3\"},\n      {webmachine,\"webmachine\",\"1.10.3-rmq3.5.3-gite9359c7\"},\n      {mochiweb,\"MochiMedia Web Server\",\"2.7.0-rmq3.5.3-git680dba8\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.5.3\"},\n      {xmerl,\"XML parser\",\"1.3.8\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.13\"},\n      {inets,\"INETS  CXC 138 49\",\"6.0\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.4\"},\n      {sasl,\"SASL  CXC 138 11\",\"2.5\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"2.5\"},\n      {kernel,\"ERTS  CXC 138 10\",\"4.0\"}]},\n {os,{unix,linux}},\n {erlang_version,\n     \"Erlang/OTP 18 [erts-7.0] [source] [64-bit] [smp:2:2] [async-threads:30] [kernel-poll:true]\\n\"},\n {memory,\n     [{total,383717416},\n      {connection_readers,1325456},\n      {connection_writers,7694952},\n      {connection_channels,125132600},\n      {connection_other,85150160},\n      {queue_procs,19523888},\n      {queue_slave_procs,0},\n      {plugins,20670368},\n      {other_proc,7088664},\n      {mnesia,1589408},\n      {mgmt_db,34545264},\n      {msg_index,450864},\n      {other_ets,3566880},\n      {binary,44084160},\n      {code,20075250},\n      {atom,785313},\n      {other_system,12034189}]},\n {alarms,[]},\n {listeners,[{clustering,25672,\"::\"},{amqp,5672,\"::\"}]},\n {vm_memory_high_watermark,0.4},\n {vm_memory_limit,6416846028},\n {disk_free_limit,50000000},\n {disk_free,1036529143808},\n {file_descriptors,\n     [{total_limit,16284},\n      {total_used,133},\n      {sockets_limit,14653},\n      {sockets_used,99}]},\n {processes,[{limit,1048576},{used,26879}]},\n {run_queue,1193},\n {uptime,87068}]\n\nCluster status of node 'rabbit@i-x.example.com' ...\n[{nodes,[{disc,['rabbit@i-x.example.com']}]},\n {running_nodes,['rabbit@i-x.example.com']},\n {cluster_name,<<\"rabbit@i-x.example.com\">>},\n {partitions,[]}]\n\n```\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\n Generic server <0.755.0> terminating\n Last message in was {drop_expired,1}\n When Server state == {q,\n                         {amqqueue,\n                          {resource,<<\"x_messenger\">>,queue,\n                           <<\"ha.x_messenger.app_x_queue_error\">>},\n                          true,false,none,\n                          [{<<\"x-dead-letter-exchange\">>,longstr,\n                            <<\"ha.x_messenger.app_x_queue\">>},\n                           {<<\"x-message-ttl\">>,signedint,300000}],\n                          <0.755.0>,[],[],[],undefined,[],[],live},\n                         none,false,rabbit_priority_queue,\n                         {passthrough,rabbit_variable_queue,\n                          {vqstate,\n                           {0,{[],[]}},\n                           {0,{[],[]}},\n                           {delta,undefined,0,undefined},\n                           {0,{[],[]}},\n                           {2,\n                            {[{msg_status,16,\n                               <<145,236,191,185,197,127,102,104,95,63,130,119,\n                                 35,220,135,98>>,\n                               {basic_message,\n                                {resource,<<\"x_messenger\">>,exchange,\n                                 <<\"ha.x_messenger.app_x_queue_error\">>},\n                                [<<\"ha.x_messenger.app_x_queue\">>],\n                                {content,60,\n                                 {'P_basic',<<\"application/octet-stream\">>,\n                                  undefined,\n                                  [{<<\"x-death\">>,array,\n                                    [{table,\n                                      [{<<\"count\">>,signedint,1},\n                                       {<<\"reason\">>,longstr,<<\"expired\">>},\n                                       {<<\"queue\">>,longstr,\n                                        <<\"ha.x_messenger.app_x_queue_error\">>},\n                                       {<<\"time\">>,timestamp,1436273833},\n                                       {<<\"exchange\">>,longstr,\n                                        <<\"ha.x_messenger.app_x_queue_error\">>},\n                                       {<<\"routing-keys\">>,array,\n                                        [{longstr,\n                                          <<\"ha.x_messenger.app_x_queue\">>}]}]}]}],\n                                  2,0,undefined,undefined,undefined,\n                                  <<\"79d42774-1346-4bc9-a08d-3ba45c61a79a\">>,\n                                  undefined,undefined,undefined,undefined,\n                                  undefined},\n                                 <<184,128,24,97,112,112,108,105,99,97,116,105,\n                                   111,110,47,111,99,116,101,116,45,115,116,\n                                   114,101,97,109,0,0,1,25,7,120,45,100,101,97,\n                                   116,104,65,0,0,1,12,70,0,0,1,7,5,99,111,117,\n                                   110,116,73,0,0,0,1,6,114,101,97,115,111,110,\n                                   83,0,0,0,7,101,120,112,105,114,101,100,5,\n                                   113,117,101,117,101,83,0,0,0,59,104,97,46,\n                                   100,99,95,109,101,115,115,101,110,103,101,\n                                   114,95,112,114,111,100,46,118,49,46,116,114,\n                                   97,110,115,102,111,114,109,101,114,95,100,\n                                                                      99,95,97,114,99,104,105,118,101,115,95,112,\n                                   97,114,116,115,95,101,114,114,111,114,4,116,\n                                   105,109,101,84,0,0,0,0,85,155,204,169,8,101,\n                                   120,99,104,97,110,103,101,83,0,0,0,59,104,\n                                   97,46,100,99,95,109,101,115,115,101,110,103,\n                                   101,114,95,112,114,111,100,46,118,49,46,116,\n                                   114,97,110,115,102,111,114,109,101,114,95,\n                                   100,99,95,97,114,99,104,105,118,101,115,95,\n                                   112,97,114,116,115,95,101,114,114,111,114,\n                                   12,114,111,117,116,105,110,103,45,107,101,\n                                   121,115,65,0,0,0,58,83,0,0,0,53,104,97,46,\n                                   100,99,95,109,101,115,115,101,110,103,101,\n                                   114,95,112,114,111,100,46,118,49,46,116,114,\n                                   97,110,115,102,111,114,109,101,114,95,100,\n                                   99,95,97,114,99,104,105,118,101,115,95,112,\n                                   97,114,116,115,2,0,36,55,57,100,52,50,55,55,\n                                   52,45,49,51,52,54,45,52,98,99,57,45,97,48,\n                                   56,100,45,51,98,97,52,53,99,54,49,97,55,57,\n                                   97>>,\n                                 rabbit_framing_amqp_0_9_1,\n                                 [<<133,162,116,114,171,99,114,97,119,108,95,\n                                    115,116,101,112,115,166,116,114,95,115,\n                                    101,116,131,166,112,114,101,102,105,120,\n                                    169,99,114,97,119,108,46,51,54,54,174,\n                                    116,114,97,110,115,102,111,114,109,101,\n                                    100,95,97,116,183,50,48,49,53,45,48,55,\n                                    45,48,55,32,48,50,58,53,56,58,51,48,32,\n                                    85,84,67,172,116,114,97,110,115,102,111,\n                                    114,109,95,105,100,210,138,95,31,211,166,\n                                    105,110,112,117,116,115,129,168,99,114,\n                                    97,119,108,95,105,100,205,1,110,164,112,\n                                    97,114,116,132,164,105,100,95,102,184,53,\n                                    53,57,97,98,48,56,57,54,57,50,100,51,53,\n                                    51,51,97,49,101,56,48,52,48,48,164,105,\n                                    100,95,116,184,53,53,57,97,98,49,101,51,\n                                    54,57,50,100,51,53,51,51,99,100,56,52,48,\n                                    54,48,48,164,105,100,95,111,205,35,40,\n                                    164,105,100,95,105,10,172,99,97,108,108,\n                                    98,97,99,107,95,107,101,121,131,168,112,\n                                    105,101,99,101,95,105,100,205,15,183,177,\n                                    116,114,97,110,115,102,111,114,109,97,\n                                    116,105,111,110,95,105,100,205,2,12,181,\n                                    116,114,97,110,115,102,111,114,109,97,\n                                    116,105,111,110,95,115,101,116,95,105,\n                                    100,205,1,6>>]},\n                                <<145,236,191,185,197,127,102,104,95,63,130,\n                                  119,35,220,135,98>>,\n                                true},\n                               true,false,false,true,queue_index,\n                               {message_properties,1436279640057485,true,\n                                270}}],\n                             [{msg_status,15,\n                               <<168,84,203,115,201,232,154,241,51,33,92,99,\n                                 248,170,24,198>>,\n                               {basic_message,\n                                {resource,<<\"x_messenger\">>,exchange,\n                                                                 <<\"ha.x_messenger.app_x_queue_error\">>},\n                                [<<\"ha.x_messenger.app_x_queue\">>],\n                                {content,60,\n                                 {'P_basic',<<\"application/octet-stream\">>,\n                                  undefined,\n                                  [{<<\"x-death\">>,array,\n                                    [{table,\n                                      [{<<\"count\">>,signedint,1},\n                                       {<<\"reason\">>,longstr,<<\"expired\">>},\n                                       {<<\"queue\">>,longstr,\n                                        <<\"ha.x_messenger.app_x_queue_error\">>},\n                                       {<<\"time\">>,timestamp,1436238931},\n                                       {<<\"exchange\">>,longstr,\n                                        <<\"ha.x_messenger.app_x_queue_error\">>},\n                                       {<<\"routing-keys\">>,array,\n                                        [{longstr,\n                                          <<\"ha.x_messenger.app_x_queue\">>}]}]}]}],\n                                  2,0,undefined,undefined,undefined,\n                                  <<\"788e03ca-672a-4c46-8133-6a2fc2c4f4f4\">>,\n                                  undefined,undefined,undefined,undefined,\n                                  undefined},\n                                 <<184,128,24,97,112,112,108,105,99,97,116,105,\n                                   111,110,47,111,99,116,101,116,45,115,116,\n                                   114,101,97,109,0,0,1,25,7,120,45,100,101,97,\n                                   116,104,65,0,0,1,12,70,0,0,1,7,5,99,111,117,\n                                   110,116,73,0,0,0,1,6,114,101,97,115,111,110,\n                                   83,0,0,0,7,101,120,112,105,114,101,100,5,\n                                   113,117,101,117,101,83,0,0,0,59,104,97,46,\n                                   100,99,95,109,101,115,115,101,110,103,101,\n                                   114,95,112,114,111,100,46,118,49,46,116,114,\n                                   97,110,115,102,111,114,109,101,114,95,100,\n                                   99,95,97,114,99,104,105,118,101,115,95,112,\n                                   97,114,116,115,95,101,114,114,111,114,4,116,\n                                   105,109,101,84,0,0,0,0,85,155,68,83,8,101,\n                                   120,99,104,97,110,103,101,83,0,0,0,59,104,\n                                   97,46,100,99,95,109,101,115,115,101,110,103,\n                                   101,114,95,112,114,111,100,46,118,49,46,116,\n                                   114,97,110,115,102,111,114,109,101,114,95,\n                                   100,99,95,97,114,99,104,105,118,101,115,95,\n                                   112,97,114,116,115,95,101,114,114,111,114,\n                                   12,114,111,117,116,105,110,103,45,107,101,\n                                   121,115,65,0,0,0,58,83,0,0,0,53,104,97,46,\n                                   100,99,95,109,101,115,115,101,110,103,101,\n                                   114,95,112,114,111,100,46,118,49,46,116,114,\n                                   97,110,115,102,111,114,109,101,114,95,100,\n                                   99,95,97,114,99,104,105,118,101,115,95,112,\n                                   97,114,116,115,2,0,36,55,56,56,101,48,51,99,\n                                   97,45,54,55,50,97,45,52,99,52,54,45,56,49,\n                                   51,51,45,54,97,50,102,99,50,99,52,102,52,\n                                   102,52>>,\n                                 rabbit_framing_amqp_0_9_1,\n                                 [<<133,162,116,114,171,99,114,97,119,108,95,\n                                    115,116,101,112,115,166,116,114,95,115,\n                                    101,116,131,166,112,114,101,102,105,120,\n                                    169,99,114,97,119,108,46,51,54,54,174,\n                                    116,114,97,110,115,102,111,114,109,101,\n                                                                        100,95,97,116,183,50,48,49,53,45,48,55,\n                                    45,48,55,32,48,50,58,53,56,58,51,48,32,\n                                    85,84,67,172,116,114,97,110,115,102,111,\n                                    114,109,95,105,100,210,138,95,31,211,166,\n                                    105,110,112,117,116,115,129,168,99,114,\n                                    97,119,108,95,105,100,205,1,110,164,112,\n                                    97,114,116,132,164,105,100,95,102,184,53,\n                                    53,57,97,98,53,102,53,54,57,50,100,51,53,\n                                    51,53,48,49,54,53,48,55,48,48,164,105,\n                                    100,95,116,184,53,53,57,97,98,55,51,57,\n                                    54,57,50,100,51,57,54,50,49,98,55,48,48,\n                                    54,48,48,164,105,100,95,111,205,50,200,\n                                    164,105,100,95,105,14,172,99,97,108,108,\n                                    98,97,99,107,95,107,101,121,131,168,112,\n                                    105,101,99,101,95,105,100,205,15,187,177,\n                                    116,114,97,110,115,102,111,114,109,97,\n                                    116,105,111,110,95,105,100,205,2,12,181,\n                                    116,114,97,110,115,102,111,114,109,97,\n                                    116,105,111,110,95,115,101,116,95,105,\n                                    100,205,1,6>>]},\n                                <<168,84,203,115,201,232,154,241,51,33,92,99,\n                                  248,170,24,198>>,\n                                true},\n                               true,false,false,true,queue_index,\n                               {message_properties,1436279449181213,true,\n                                270}}]}},\n                           17,\n                           {0,nil},\n                           {0,nil},\n                           {0,nil},\n                           {qistate,\n                            \"/var/lib/rabbitmq/mnesia/rabbit@i-x.example.com/queues/69RXCRA0YQFLNESM4YE1XHF0G\",\n                            {{dict,0,16,16,8,80,48,\n                              {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                               []},\n                              {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                []}}},\n                             [{segment,0,\n                               \"/var/lib/rabbitmq/mnesia/rabbit@i-x.example.com/queues/69RXCRA0YQFLNESM4YE1XHF0G/0.idx\",\n                               {array,16384,0,undefined,100000},\n                               2}]},\n                            #Ref<0.0.1572866.70861>,0,65536,\n                            #Fun,\n                            #Fun,\n                            {0,nil},\n                            {0,nil}},\n                           {{client_msstate,msg_store_persistent,\n                             <<13,24,107,90,238,140,22,42,193,193,247,117,63,\n                               25,47,49>>,\n                             {dict,0,16,16,8,80,48,\n                              {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                               []},\n                              {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                []}}},\n                             {state,225357,\n                              \"/var/lib/rabbitmq/mnesia/rabbit@i-x.example.com/msg_store_persistent\"},\n                                                           rabbit_msg_store_ets_index,\n                             \"/var/lib/rabbitmq/mnesia/rabbit@i-x.example.com/msg_store_persistent\",\n                             <0.240.0>,229454,221260,233551,237648},\n                            {client_msstate,msg_store_transient,\n                             <<101,74,79,173,74,7,174,242,180,223,118,182,139,\n                               198,89,135>>,\n                             {dict,0,16,16,8,80,48,\n                              {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                               []},\n                              {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                []}}},\n                             {state,204872,\n                              \"/var/lib/rabbitmq/mnesia/rabbit@i-x.example.com/msg_store_transient\"},\n                             rabbit_msg_store_ets_index,\n                             \"/var/lib/rabbitmq/mnesia/rabbit@i-x.example.com/msg_store_transient\",\n                             <0.233.0>,208969,200775,213066,217163}},\n                           true,0,2,540,0,2,540,infinity,2,0,0,540,0,0,\n                           {rates,0.005228732197132713,\n                            1.7370371540135685e-303,1.7370371540135685e-303,\n                            1.7370371540135685e-303,\n                            {1436,279341,778502}},\n                           {0,nil},\n                           {0,nil},\n                           {0,nil},\n                           {0,nil},\n                           0,0,0,17}},\n                         {state,\n                          {queue,[],[],0},\n                          {inactive,1436238631386029,41942114405,1.0}},\n                         undefined,undefined,undefined,undefined,\n                         {state,fine,5000,undefined},\n                         {0,nil},\n                         300000,\n                         {erlang,#Ref<0.0.3932161.80107>},\n                         1436279449181213,\n                         {state,\n                          {dict,2,16,16,8,80,48,\n                           {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},\n                           {{[],[],[],[],\n                             [[<0.3455.6>|#Ref<0.0.3932161.80106>]],\n                             [],[],[],[],[],[],[],\n                             [[<0.3479.6>|#Ref<0.0.3932161.86474>]],\n                             [],[],[]}}},\n                          delegate},\n                         {resource,<<\"x_messenger\">>,exchange,\n                          <<\"ha.x_messenger.app_x_queue\">>},\n                         undefined,undefined,undefined,1,running}\n Reason for termination ==\n** {{case_clause,{value,{<<\"count\">>,signedint,1}}},\n    [{rabbit_dead_letter,x_death_event_key,3,[]},\n     {rabbit_dead_letter,increment_xdeath_event_count,1,[]},\n     {rabbit_dead_letter,update_x_death_header,2,[]},\n     {rabbit_dead_letter,'-make_msg/5-fun-2-',8,[]},\n     {rabbit_basic,map_headers,2,[]},\n     {rabbit_dead_letter,make_msg,5,[]},\n     {rabbit_dead_letter,publish,5,[]},\n          {rabbit_amqqueue_process,'-dead_letter_msgs/4-fun-0-',7,[]}]}\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\nRestarting crashed queue 'ha.x_messenger.app_x_queue_error' in vhost 'x_messenger'.\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\n Generic server <0.7229.6> terminating\n Last message in was {'$gen_cast',init}\n When Server state == {q,{amqqueue,\n                               {resource,<<\"x_messenger\">>,queue,\n                                   <<\"ha.x_messenger.app_x_queue_error\">>},\n                               true,false,none,\n                               [{<<\"x-dead-letter-exchange\">>,longstr,\n                                 <<\"ha.x_messenger.app_x_queue\">>},\n                                {<<\"x-message-ttl\">>,signedint,300000}],\n                               <0.7229.6>,[],[],[],undefined,[],[],crashed},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,1436279449185300,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,running}\n Reason for termination ==\n** {{case_clause,{value,{<<\"count\">>,signedint,1}}},\n    [{rabbit_dead_letter,x_death_event_key,3,[]},\n     {rabbit_dead_letter,increment_xdeath_event_count,1,[]},\n     {rabbit_dead_letter,update_x_death_header,2,[]},\n     {rabbit_dead_letter,'-make_msg/5-fun-2-',8,[]},\n     {rabbit_basic,map_headers,2,[]},\n     {rabbit_dead_letter,make_msg,5,[]},\n     {rabbit_dead_letter,publish,5,[]},\n     {rabbit_amqqueue_process,'-dead_letter_msgs/4-fun-0-',7,[]}]}\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\nRestarting crashed queue 'ha.x_messenger.app_x_queue_error' in vhost 'x_messenger'.\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\n Generic server <0.7231.6> terminating\n Last message in was {'$gen_cast',init}\n When Server state == {q,{amqqueue,\n                               {resource,<<\"x_messenger\">>,queue,\n                                   <<\"ha.x_messenger.app_x_queue_error\">>},\n                               true,false,none,\n                               [{<<\"x-dead-letter-exchange\">>,longstr,\n                                 <<\"ha.x_messenger.app_x_queue\">>},\n                                {<<\"x-message-ttl\">>,signedint,300000}],\n                                                               <0.7231.6>,[],[],[],undefined,[],[],live},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,1436279449191151,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,running}\n Reason for termination ==\n** {{case_clause,{value,{<<\"count\">>,signedint,1}}},\n    [{rabbit_dead_letter,x_death_event_key,3,[]},\n     {rabbit_dead_letter,increment_xdeath_event_count,1,[]},\n     {rabbit_dead_letter,update_x_death_header,2,[]},\n     {rabbit_dead_letter,'-make_msg/5-fun-2-',8,[]},\n     {rabbit_basic,map_headers,2,[]},\n     {rabbit_dead_letter,make_msg,5,[]},\n     {rabbit_dead_letter,publish,5,[]},\n     {rabbit_amqqueue_process,'-dead_letter_msgs/4-fun-0-',7,[]}]}\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\nRestarting crashed queue 'ha.x_messenger.app_x_queue_error' in vhost 'x_messenger'.\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\n Generic server <0.7233.6> terminating\n Last message in was {'$gen_cast',init}\n When Server state == {q,{amqqueue,\n                               {resource,<<\"x_messenger\">>,queue,\n                                   <<\"ha.x_messenger.app_x_queue_error\">>},\n                               true,false,none,\n                               [{<<\"x-dead-letter-exchange\">>,longstr,\n                                 <<\"ha.x_messenger.app_x_queue\">>},\n                                {<<\"x-message-ttl\">>,signedint,300000}],\n                               <0.7233.6>,[],[],[],undefined,[],[],live},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,1436279449196370,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                                                                    delegate},\n                           undefined,undefined,undefined,undefined,0,running}\n Reason for termination ==\n** {{case_clause,{value,{<<\"count\">>,signedint,1}}},\n    [{rabbit_dead_letter,x_death_event_key,3,[]},\n     {rabbit_dead_letter,increment_xdeath_event_count,1,[]},\n     {rabbit_dead_letter,update_x_death_header,2,[]},\n     {rabbit_dead_letter,'-make_msg/5-fun-2-',8,[]},\n     {rabbit_basic,map_headers,2,[]},\n     {rabbit_dead_letter,make_msg,5,[]},\n     {rabbit_dead_letter,publish,5,[]},\n     {rabbit_amqqueue_process,'-dead_letter_msgs/4-fun-0-',7,[]}]}\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\nRestarting crashed queue 'ha.x_messenger.app_x_queue_error' in vhost 'x_messenger'.\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\n Generic server <0.7235.6> terminating\n Last message in was {'$gen_cast',init}\n When Server state == {q,{amqqueue,\n                               {resource,<<\"x_messenger\">>,queue,\n                                   <<\"ha.x_messenger.app_x_queue_error\">>},\n                               true,false,none,\n                               [{<<\"x-dead-letter-exchange\">>,longstr,\n                                 <<\"ha.x_messenger.app_x_queue\">>},\n                                {<<\"x-message-ttl\">>,signedint,300000}],\n                               <0.7235.6>,[],[],[],undefined,[],[],live},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,1436279449201433,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,running}\n Reason for termination ==\n** {{case_clause,{value,{<<\"count\">>,signedint,1}}},\n    [{rabbit_dead_letter,x_death_event_key,3,[]},\n     {rabbit_dead_letter,increment_xdeath_event_count,1,[]},\n     {rabbit_dead_letter,update_x_death_header,2,[]},\n     {rabbit_dead_letter,'-make_msg/5-fun-2-',8,[]},\n     {rabbit_basic,map_headers,2,[]},\n     {rabbit_dead_letter,make_msg,5,[]},\n     {rabbit_dead_letter,publish,5,[]},\n     {rabbit_amqqueue_process,'-dead_letter_msgs/4-fun-0-',7,[]}]}\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\nRestarting crashed queue 'ha.x_messenger.app_x_queue_error' in vhost 'x_messenger'.\n=ERROR REPORT==== 7-Jul-2015::14:30:49 ===\n Generic server <0.7237.6> terminating\n Last message in was {'$gen_cast',init}\n When Server state == {q,{amqqueue,\n                               {resource,<<\"x_messenger\">>,queue,\n                                   <<\"ha.x_messenger.app_x_queue_error\">>},\n                               true,false,none,\n                               [{<<\"x-dead-letter-exchange\">>,longstr,\n                                 <<\"ha.x_messenger.app_x_queue\">>},\n                                {<<\"x-message-ttl\">>,signedint,300000}],\n                               <0.7237.6>,[],[],[],undefined,[],[],live},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,1436279449206805,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,running}\n Reason for termination ==\n** {{case_clause,{value,{<<\"count\">>,signedint,1}}},\n    [{rabbit_dead_letter,x_death_event_key,3,[]},\n     {rabbit_dead_letter,increment_xdeath_event_count,1,[]},\n     {rabbit_dead_letter,update_x_death_header,2,[]},\n     {rabbit_dead_letter,'-make_msg/5-fun-2-',8,[]},\n     {rabbit_basic,map_headers,2,[]},\n     {rabbit_dead_letter,make_msg,5,[]},\n     {rabbit_dead_letter,publish,5,[]},\n     {rabbit_amqqueue_process,'-dead_letter_msgs/4-fun-0-',7,[]}]}\n``\n. Sorry, I don't have a sample I'm able to post easily at present. But I can add to the above that it does indeed seem to berabbit_dead_letter.erlincrement_xdeath_event_countcase x_death_event_key(Info, <<\"count\">>, long) ofline; when I patched that to disable the attempt to read and increment<<\"count\">>(which we're not currently using), instead following theundefined` branch of the case, the broker freed up and CPU usage went right back down with existing messages being processed. My apologies for not being able to post a test case code sample.\nPeace, tiredpixel\n. @michaelklishin, our apps don't set or use the count header at all. It's only ever touched by RabbitMQ on dead-lettering. I noted that <<\"count\">> seems to be signedint at that point, but I wasn't able to spot why it's such rather than a long. As far as I got (before patching RabbitMQ as above) was that this seems to be for messages which have gone through the dead-letter exchange a few times, by which time <<\"count\">> appears to be already set\u2014I suspect following update_x_death_header Info1 [{table, M}] matches branch being followed, if that helps.\n. @michaelklishin, understood; thank you for looking into it. :)\n@riyad, thank you for contributing a test case to this investigation. :)\nPeace, tiredpixel\n. @michaelklishin, oh, such a speedy patch\u2014very much obliged to you! :) I'll look forward to 3.5.4 RC. :)\n. ",
    "riyad": "You can provoke the crash of the queue with the Python script found in https://gist.github.com/riyad/7439eb545dabf287bcc9.\nIt requires you to install the Puka library and update the AMQP_URL in the script to point to your RabbitMQ.\n. FYI: I fixed a small issue with the demo script. It was missing a wait() when publishing the first message. :sweat_smile:\nsee: https://gist.github.com/riyad/7439eb545dabf287bcc9/revisions\n. Basically there should be an additional x-death entry for every time the message is deadlettered.\nIf there is a message in the queue at the end of the script with all the x-deaths everything should be OK.\nThe important part is, the message must not be lost.\n. @michaelklishin To be sure it would be nice if you could provide the output of the script with an empty queue.\n. Looks good :)\n. Hmm ... so you're saying Puka is to blame? Am I correct that the issue is that header fields were encoded with the \"wrong\" type when republishing (even when they're superficially the same)?\n. Thank a lot for fixing this so quickly. :smile: \n. ",
    "seetharamireddy540": "hi,\nI am using Rabbit MQ 3.5.4 and experiencing the same issues ?\n=ERROR REPORT==== 14-Mar-2016::20:04:20 ===\n* Generic server <0.1073.0> terminating\n* Last message in was {'$gen_cast',init}\n\nundefined,undefined,undefined,undefined,0,running}\n* Reason for termination == \n* {{case_clause,{<<\"x-death\">>,longstr,\n                  <<\"[{reason=expired, \nCan you pls suggest me how i can delete the queue ? I am not able to delete this queue from Admin Console.\n. ",
    "pdoreau": "Hello. I'm using v3.6.5 (also tested from 3.5.6) and retry queues (task => retry1 (dl/ttl) => task => retry2 (dl/ttl) => task => retry3 (dl/ttl) => dl).\nDead lettering the 2nd time causes this error :\n=ERROR REPORT==== 14-Nov-2016::11:09:53 ===\n ** Generic server <0.932.0> terminating\n ** Last message in was {drop_expired,1}\n...\n=CRASH REPORT==== 14-Nov-2016::11:09:53 ===\n   crasher:\n     initial call: rabbit_prequeue:init/1\n     pid: <0.932.0>\n     registered_name: []\n     exception exit: {function_clause,\n                      [{rabbit_dead_letter,\n                        '-queue_and_reason_matcher/2-fun-1-',\n                        [{array,\n                          [{array,[{longstr,<<\"l\">>},{longstr,<<\"1\">>}]},\n                           {array,[{longstr,<<\"S\">>},{longstr,<<\"expired\">>}]},\n                           {array,\n                            [{longstr,<<\"S\">>},\n                             {longstr,\n                              <<\"mytask_retry_1\">>}]},\n                           {array,\n                            [{longstr,<<\"T\">>},{longstr,<<\"1479121758\">>}]},\n                           {array,[{longstr,<<\"S\">>},{longstr,<<\"retry\">>}]},\n                           {array,\n                            [{longstr,<<\"A\">>},\n                             {array,\n                              [{longstr,\n                                <<\"mytask_retry_1\">>}]}]}]}],\n                        [{file,\"src/rabbit_dead_letter.erl\"},{line,178}]},\n                       {lists,partition,4,[{file,\"lists.erl\"},{line,1302}]},\n                       {rabbit_dead_letter,update_x_death_header,2,\n                        [{file,\"src/rabbit_dead_letter.erl\"},{line,127}]},\n                       {rabbit_dead_letter,'-make_msg/5-fun-2-',8,\n                        [{file,\"src/rabbit_dead_letter.erl\"},{line,65}]},\n                       {rabbit_basic,map_headers,2,\n                        [{file,\"src/rabbit_basic.erl\"},{line,258}]},\n                       {rabbit_dead_letter,make_msg,5,\n                        [{file,\"src/rabbit_dead_letter.erl\"},{line,68}]},\n                       {rabbit_dead_letter,publish,5,\n                        [{file,\"src/rabbit_dead_letter.erl\"},{line,34}]},\n                       {rabbit_amqqueue_process,'-dead_letter_msgs/4-fun-0-',\n                        7,\n                        [{file,\"src/rabbit_amqqueue_process.erl\"},\n                         {line,848}]}]}\n       in function  gen_server2:terminate/3 (src/gen_server2.erl, line 1143)\n     ancestors: [<0.931.0>,rabbit_amqqueue_sup_sup,rabbit_sup,<0.140.0>]\n     messages: []\n     links: [<0.931.0>]\n     dictionary: [{{credit_to,<0.1089.0>},49},\n                   {{xtype_to_module,direct},rabbit_exchange_type_direct},\n                   {process_name,\n                       {rabbit_amqqueue_process,\n                           {resource,<<\"myvhost\">>,queue,\n                               <<\"mytask_retry_2\">>}}},\n                   {rand_seed,\n                       {#{max => 288230376151711743,\n                          next => #Fun<rand.8.41921595>,\n                          type => exsplus,\n                          uniform => #Fun<rand.9.41921595>,\n                          uniform_n => #Fun<rand.10.41921595>},\n                        [239153173037508270|193202472509615284]}},\n                   {guid,{{2457094499,2660964459,2278303215,3311440446},1}},\n                   {credit_flow_default_credit,{200,50}}]\n     trap_exit: true\n     status: running\n     heap_size: 2586\n     stack_size: 27\n     reductions: 4190\n   neighbours:\n...\nAny way to make it work ?\n. @michaelklishin I posted a message on the user list but it doestn't appear.\nMay be not so usefull to post a snippet as I rely on several layers (swarrot, amqplib).\nHowever, I noticed nested arrays are given to Amqplib to publish the retry2 message:\n...\n[\"x-death\"]=>\n      array(2) {\n        [0]=>\n        string(1) \"A\"\n        [1]=>\n        array(1) {\n          [0]=>\n          array(6) {\n            [\"count\"]=>\n            array(2) {\n              [0]=>\n              string(1) \"l\"\n              [1]=>\n              string(1) \"1\"\n            }\n            [\"reason\"]=>\n            array(2) {\n              [0]=>\n              string(1) \"S\"\n              [1]=>\n              string(7) \"expired\"\n            }\n            [\"queue\"]=>\n            array(2) {\n              [0]=>\n              string(1) \"S\"\n              [1]=>\n              string(36) \"mytask_retry_1\"\n            }\n            [\"time\"]=>\n            array(2) {\n              [0]=>\n              string(1) \"T\"\n              [1]=>\n              string(10) \"1479142401\"\n            }\n            [\"exchange\"]=>\n            array(2) {\n              [0]=>\n              string(1) \"S\"\n              [1]=>\n              string(5) \"retry\"\n            }\n            [\"routing-keys\"]=>\n            array(2) {\n              [0]=>\n              string(1) \"A\"\n              [1]=>\n              array(1) {\n                [0]=>\n                string(36) \"mytask_retry_1\"\n              }\n            }\n          }\n        }\n      }\n...\nCan that be the cause ?\n. ",
    "ChrisCGH": "This sounds similar to an issue I raised on the rabbitmq-users list: https://groups.google.com/forum/#!topic/rabbitmq-users/Ja0iyfF0Szw\n. ",
    "aungkoko96": "Which command is used to get logging which worker is hitting ? plz let me know. I have tested some tutorials of rabbitmq.I just want to know the status of connection (close or open) when i run send.php in work queue tutorials.\n. ",
    "gmr": "I'm thinking this might be even more significant the larger the amount of data. In my production issue today my consumers could not keep up with the publishing velocity. Messages are generally >50KB, but can be larger. Consumers would only be delivered at a rate that was roughly the disk read rate, but there was no IOWait on any node in the cluster. \nFor example, where I might be able to consume @1k per sec normally, when the transient message storage was around 20GB in size, it would only deliver at 10 messages per second. \nWhen it was ~1GB on disk, it started to go back to normal speeds\nIt seems that the larger the transient message store becomes, the disk read rate disproportionately slows down. All this while there are not any indicators at the OS level as to why.\n\n. Here's a quick in-progress update to what I found as I've been working to validate/invalidate #226 as a fix, along with other settings changes. \ntl;dr version: by updating to RabbitMQ 3.5.4.60818, turning of FHC, and queue_index_embed_msgs_below set to 0, RabbitMQ's memory usage is much better and I am having to work a lot harder to get messages to page to disk in the first place.\nI am evaluating the changes on the same RabbitMQ cluster I ran into the problem with, using the nightly build from the 18th (3.5.4.60818). \nThis is on our \"production\" cluster for a new service running in EC2 on 4 i2.2xlarge instances. \nI am using CoreOS and RabbitMQ is running inside one docker container on each node. The /var/lib/rabbitmq directory is mounted to the OS level for persistence of data across restarting of my docker containers.\nIn the issue from last week, once I got above ~12GB in queue size (almost all messages paged to disk at that point), I started to run into the issue.\nThe queue is HA with the following settings applied via policy:\nha-mode: exactly\nha-params: 3\nIn trying to address the problem and validate tuning changes, I added the following entries to my config, with the values previously being non-configured defaults:\nerlang\n{fhc_read_buffering, false},\n{fhc_write_buffering, false}\n{queue_index_embed_msgs_below, 0}\nI am having a much harder time getting to memory pressure... I'm currently sitting on 19GB of messages in the queue, with 14GB in memory and only 5GB on disk. My plan is to keep pushing until I have 19GB on disk before I try and dequeue and see if I can maintain a sustained rate above 2k messages per consumer, which is what I get when I don't have any issues.\nI'm currently rolling out a change to set:\nerlang\n{queue_index_max_journal_entries, 16384}\nand will resume filling the queue once I've updated all the nodes and resynced the queue so it's back up on three nodes. More updates as I have more findings.\n. Would the \"fix\" here be to warn on missing plugins instead of crash?\n. Agreed with storing the hash type in the new format, defaulting to sha256 and setting to md5 on conversion. Might be overkill, but you might want to consider a configurable salt as well for the new password hashing implementation.\n. A string you can define in rabbitmq.config. Based upon the code, it appears the auto-generated salt is stored with the user record. I'm not sure what value that provides, security wise, since you're providing the all the data required to validate the data with the data. OTOH, one could argue that you could get at that value via application:get_env/2 if it were configurable and thus is not more secure than the dynamically computed salt.\n. Yeah should point out that it was not RabbitMQ per se.. 18.1 was the clear change in my testing.\n. @hairyhum I think that's a good way to go.\n. ",
    "klutarevich": "What's rabbitmq-users? Google group?\n. ",
    "tonal": "```\n$ sudo rabbitmq-plugins list                                                       \n[sudo] password for tonal:                                                                                          \nWARNING - plugins currently enabled but missing: [rabbitmq_delayed_message_exchange]                                  \nConfigured: E = explicitly enabled; e = implicitly enabled\n | Status:   [failed to contact rabbit@hius2 - status not shown]\n |/\n[e ] amqp_client                       3.5.4\n[  ] cowboy                            0.5.0-rmq3.5.4-git4b93c2d\n[e ] mochiweb                          2.7.0-rmq3.5.4-git680dba8\n[  ] rabbitmq_amqp1_0                  3.5.4\n[  ] rabbitmq_auth_backend_ldap        3.5.4\n[  ] rabbitmq_auth_mechanism_ssl       3.5.4\n[  ] rabbitmq_consistent_hash_exchange 3.5.4\n[  ] rabbitmq_federation               3.5.4\n[  ] rabbitmq_federation_management    3.5.4\n[E ] rabbitmq_management               3.5.4\n[e ] rabbitmq_management_agent         3.5.4\n[  ] rabbitmq_management_visualiser    3.5.4\n[  ] rabbitmq_mqtt                     3.5.4\n[  ] rabbitmq_shovel                   3.5.4\n[  ] rabbitmq_shovel_management        3.5.4\n[  ] rabbitmq_stomp                    3.5.4\n[  ] rabbitmq_test                     3.5.4\n[  ] rabbitmq_tracing                  3.5.4\n[e ] rabbitmq_web_dispatch             3.5.4\n[  ] rabbitmq_web_stomp                3.5.4\n[  ] rabbitmq_web_stomp_examples       3.5.4\n[  ] sockjs                            0.3.4-rmq3.5.4-git3132eb9\n[e ] webmachine                        1.10.3-rmq3.5.4-gite9359c7\n``\n. rabbitmq_delayed_message_exchange locate in/usr/lib/rabbitmq/lib/rabbitmq_server-3.5.3/pluginsrabbitmq_delayed_message_exchange-0.0.1-rmq3.5.x-9bf265e4.ez\nafter move plugin to/usr/lib/rabbitmq/lib/rabbitmq_server-3.5.4/plugins` rabbitmq-server starts\n. Hi @dumbbell!\nYes. I manuali install rabbitmq_delayed_message_exchange plugin to 3.5.3\n. Also for version 3.5.5-3\n```\n$ cat startup_err \nCrash dump was written to: erl_crash.dump\ninit terminating in do_boot ()\n$ cat startup_log \nBOOT FAILED\nError description:\n   {error,{\"no such file or directory\",\n           \"rabbitmq_delayed_message_exchange.app\"}}\nLog files (may contain more information):\n   /var/log/rabbitmq/rabbit@hius2.log\n   /var/log/rabbitmq/rabbit@hius2-sasl.log\nStack trace:\n   [{app_utils,load_applications,2,[]},\n    {app_utils,load_applications,1,[]},\n    {rabbit,start_apps,1,[]},\n    {rabbit,broker_start,0,[]},\n    {rabbit,start_it,1,[]},\n    {init,start_it,1,[]},\n    {init,start_em,1,[]}]\n{\"init terminating in do_boot\",{error,{\"no such file or directory\",\"rabbitmq_delayed_message_exchange.app\"}}}\n```\nAfter move delayed_message plagin start Ok.\nuser@node:/usr/lib/rabbitmq/lib/rabbitmq_server-3.5.4/plugins$ sudo mv rabbitmq_delayed_message_exchange-0.0.1-rmq3.5.x-9bf265e4.ez ../../rabbitmq_server-3.5.5/plugins/\n. May be simple copy between minor subversions?\n. ",
    "andrewmichaelsmith": "Just a heads up for anyone running >=3.5.5, installing from the .deb and scratching their head at this issue. \nYou want to call /usr/lib/rabbitmq/bin/rabbitmq-server not /usr/sbin/rabbitmq-server\n. ",
    "Plorax": "Hi guys, it appears this file 'rabbitmq-env-conf.bat' does not exist anymore.\n. ",
    "midas": "I am most certainly willing to take this course of action, but can you tell me why you consider this a question and not a bug report? I did not ask a question, I simply reported an issue.\n. ",
    "priviterag": "```\n=ERROR REPORT==== 30-Jul-2015::15:19:11 ===\nRestarting crashed queue 'priority' in vhost '/'.\n=ERROR REPORT==== 30-Jul-2015::15:19:11 ===\n Generic server <0.419.0> terminating\n Last message in was {'$gen_cast',init}\n When Server state == {q,{amqqueue,\n                               {resource,<<\"/\">>,queue,<<\"priority\">>},\n                               false,false,none,\n                               [{<<\"x-max-priority\">>,signedint,0}],\n                               <0.419.0>,[],[],[],undefined,[],[],live},\n                           none,false,undefined,undefined,\n                           {state,\n                               {queue,[],[],0},\n                               {active,1438265951336055,1.0}},\n                           undefined,undefined,undefined,undefined,\n                           {state,fine,5000,undefined},\n                           {0,nil},\n                           undefined,undefined,undefined,\n                           {state,\n                               {dict,0,16,16,8,80,48,\n                                   {[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                    [],[]},\n                                   {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]}}},\n                               delegate},\n                           undefined,undefined,undefined,undefined,0,running}\n Reason for termination ==\n** {badarith,\n       [{rabbit_priority_queue,'-info/2-fun-1-',5,\n            [{file,\"src/rabbit_priority_queue.erl\"},{line,381}]},\n        {rabbit_priority_queue,fold0,3,\n            [{file,\"src/rabbit_priority_queue.erl\"},{line,413}]},\n        {rabbit_amqqueue_process,'-infos/2-lc$^0/1-0-',2,\n            [{file,\"src/rabbit_amqqueue_process.erl\"},{line,808}]},\n        {rabbit_amqqueue_process,'-infos/2-lc$^0/1-0-',2,\n            [{file,\"src/rabbit_amqqueue_process.erl\"},{line,808}]},\n        {rabbit_amqqueue_process,emit_stats,2,\n            [{file,\"src/rabbit_amqqueue_process.erl\"},{line,882}]},\n        {rabbit_event,if_enabled,3,[{file,\"src/rabbit_event.erl\"},{line,143}]},\n        {rabbit_amqqueue_process,init_it2,3,\n            [{file,\"src/rabbit_amqqueue_process.erl\"},{line,171}]},\n        {gen_server2,handle_msg,2,\n            [{file,\"src/gen_server2.erl\"},{line,1035}]}]}\n```\n. this ruby/bunny code also crashes the queue: https://gist.github.com/priviterag/1643d0af72b413fa507e\nsteps to reproduce the error:\nfrom the main rabbitmq-public-umbrella\ncd rabbitmq-stomp\nmake run-in-broker\nrun the ruby script\n. ",
    "spatula75": "Just to throw an idea out: would there be any value in the additional complexity of a timeout/retry mechanism in the event of a transient failure?  In the Amazon EC2 world, this is probably the most common troublemaking scenario we experience-- short failures that last maybe a few seconds, but which are enough to cause chaos.\nOne could just as easily argue it would be better to kick it back up to the client and let it decide if a retry is warranted, of course.\n. I do not see the same oscillation on our cluster without any load, nor do I see any further rise without load; rather, it's holding steady at the size to which the index had grown.\n. I also haven't (yet) seen the index grow beyond 26MB.  Not sure if that's a magic number or just coincidental, of course.\n. 26 turns out not to be a magic number.  It also looks like we might be seeing the index size creep at least as far back as our production 3.5.7 cluster, but anecdotally it looks vaguely like the creep might be faster in 3.6.2M3, but that's just a general observational sense, not anything I can prove with hard data.\n. Just FYI, we still see this in the 3.6.2 final release when rates mode is set to 'basic' but it does not seem to be significant when rates mode is set to 'none.'  Also I noticed just now that should have opened this against rabbitmq-management rather than rabbitmq-server.\n. One thing I've verified does not seem to affect the rate of increase is just creating a lot of channels and closing them (with basic mode enabled).  If I have some time, I'll see if publishing lots of messages affects it today or tomorrow.  (I somewhat suspect it's message that are doing it.)\n. I think I might be conflating two things here too now that I re-read the history.  Most of the increase we're seeing is on the management database side of things; the message store increase is insignificant by comparison.\n. ",
    "fenec": "@michaelklishin thanks for quick help!\nin case of anyone finding this, database dir for brew install is here:\n/usr/local/var/lib/rabbitmq/mnesia\nsource: https://www.rabbitmq.com/relocate.html\n. ",
    "essen": "The acceptor pools are quite similar.\nConfiguration is basically the same (Ranch is filtering out a few options, like SSL's depth option or TCP's exit_on_close option, but adding those to Ranch is trivial).\nConcepts of listener and acceptors are very similar, it should be fairly straightforward to replace them.\nThe supervision tree for connections in RabbitMQ is a bit complex but I don't think it would be a problem. The ranch_conns_sup supervisor would become rabbit_tcp_client_sup (registered name) handled by rabbit_client_sup module. The difference would be mainly that ranch_conns_sup is a custom supervisor, while rabbit_client_sup is supervisor2. The supervision tree would otherwise stay unchanged.\nI estimate this would take about two days to get something up and running, and probably some time after that to resolve unforeseen issues.\n. Rather a correct wording would be: Ranch validates options (and sets some required defaults at the same time) and the list of allowed options was done on a use case basis.\nSo if depth isn't there yet, it's that no Ranch user has ever used it. :-)\nBut it's a two lines patch to add options so it won't take me long.\n. Alright.\n. OK putting some notes here mostly so I don't forget things:\n- [x] Ranch is missing some TCP and SSL options\n- [x] Ranch does not support callbacks for oninit and onterminate; we do not need to add those, we can just keep the tcp_listener process and let it do the callback handling like it does today\n- [x] When starting the listener, use ranch:child_spec instead of what we currently do; put the Ranch child spec side by side with tcp_listener, and start them in that order; put them under tcp_listener_sup\n- [x] Make sure rabbit_tcp_client_sup starts before the listeners (this is already the case, just a remainder); we need it to stop after the listeners on shutdown (see below)\n- [x] The reader process will be supervised by the ranch part of the supervision tree (but still under the rabbit application)\n- [x] The connection related processes (queue, apparently 2 hearbeat processes and the channel_sup_sup) can still sit under rabbit_connection_sup; however rabbit_connection_helper_sup is not needed anymore\n- [x] Link the reader process and the rabbit_connection_sup process so that they both die together (basically simulating what having them under the rabbit_connection_sup supervisor would do)\nIt's interesting that the process that is supervised and the process that receives ownership of the socket are not the same. I will open a ticket about that and think about it for Ranch 2. We can then later on have everything in a proper supervision tree (not needing rabbit_tcp_client_sup anymore either). In the meantime, we need to use the link trick to have the same behavior as we had previously.\nDoing the integration first, options can be added to Ranch when everything else starts working.\n. ninenines/ranch#117, ninenines/ranch#121 and ninenines/ranch#122 are required changes for this ticket to be resolved, though neither will prevent me from finishing the proof of concept.\n. The branch: https://github.com/rabbitmq/rabbitmq-server/tree/rabbitmq-server-260\nThe current WIP commit: https://github.com/rabbitmq/rabbitmq-server/commit/ad9753aac418b7ddebff12a2c503851d6f2d2181\n. - [x] Check that we still do what ninenines/ranch#122 talks about, regardless of that ticket being closed.\nNo changes needed in Ranch, but need to make sure we still do this in RabbitMQ itself.\n. - [x] Move the rest of the connection-related processes under the ranch listener's supervision tree once #119 is done.\n. Damn I meant Ranch's 122, sorry for the spam. :-)\n. Branch has been updated and works against current Ranch master but there's still work to do. :-)\n. Most recent commit is 89a51ca2030f4ede32f300e4125921158d32a51f.\nA few more things left to do, tests to be tested and then integrating that in the build properly.\n. Pushed another update in 23421ecd7f6812ec484d74f8ac3f3003070be2e7. I'm at the point where I did everything I could do on my own and I'll need help to integrate in the build system to finish. There's also an open ended question in the diff that will involve questions or investigation (search for \"todo\" if curious).\n. For future reference: I will also need to update the AMQP 1.0 plugin when the time comes (stumbled upon a compile issue while doing other things).\n. Woo!!\n. With regards to security, the key point of salting is that there's a unique salt per user. Storing it separately or not does very little. Having a single salt in a configuration file does very little security-wise.\nPeople here explain better than me: https://stackoverflow.com/questions/1219899/where-do-you-store-your-salt-strings\n. When the project switches to Ranch, those options will be enforced at the listening socket level: [binary, {active, false}, {packet, raw}, {reuseaddr, true}]\nSetting one of these options, or an unknown option, will trigger a warning in the logs.\n. 18.1 has changed the format of some error messages sent by the emulator, this is most likely what happens here.\n. Just found out about https://github.com/rabbitmq/rabbitmq-server/commit/d85792bd6fe579b291533deb35cc7166510cac40 (there's already a rabbitmq-server-343 branch it seems). The good news is that what I did is basically the same so I guess I'm on the right tracks. I need to experiment with calling update index twice though. I'll push/open a PR tonight but I'll want to do more testing next week.\n. I want to make sure all tests pass, then make Ranch 1.2 official, before 3.6.0 is released (will be just renaming the version if I did my job properly). Right now it's depending on 1.2.0-rc.1.\n. All (working) Erlang tests pass on my machine.\nThe Java client test suite also passes on my machine: https://gist.github.com/essen/b9b4cc5e9d0a3b433833\nWill see if there's more interesting things to test (maybe inside plugins) and finalize the PRs.\n. Probably binary.\nThough it's an issue that I fixed in Ranch 1.2.0-rc.1 so check that you got the right version pulled in.\n. Good to know thanks. Will continue on Tuesday. If MQTT is really all that's left, I'll most likely also be done on the same day.\n. MQTT is done. I will update Ranch to 1.2.0, review the PRs one more time and we should be good.\n. OK. Please review!\n. OK I refreshed my memory a bit on that.\nBefore, there was two parts of the supervision tree: one for the listener/acceptor, and one for connections. The latter was one big supervisor handling all connections, regardless of the listener they were coming from.\nWith Ranch, each listener has its own branch in the supervision tree. Inside it are then two more branches, one for the acceptors and one for the connections.\nTo fix this I should be able to just get all connection sups pids from Ranch (there's no API to do this directly without knowing the listeners name but there's a tiny ETS table I can use to get these for now) and put them in the list.\nWill try this and confirm (and reuse the text above in the commit message on success :-).\n. I created an issue in Ranch to provide a better way to access the list of connection sups at https://github.com/ninenines/ranch/issues/131\n. Guess it can happen if the function is called before the table was created, I'll check on Tuesday.\n. It appears I also removed the \"tune_buffer_size\" functionality.\n. I spent a lot of time on tune_buffer_size, perhaps more than I should have, but now I understand why it was there much better. The best solution would be to have a callback in Ranch to perform this operation right after accepting (like it was before the switch to Ranch). I have opened a ticket for Ranch at https://github.com/ninenines/ranch/issues/118 but I don't expect it to be done before the end of the week.\nFor the time being I will do a solution that is not the best but will work.\nEDIT: forget what I said about accept_ack.\n. This is OK for review.\n. Ready for review/merge.\n. The parse_transform is only useful so that lager can include file and line numbers automatically. This implies that you call lager:error/info/.. functions directly in the source files where you log. This doesn't seem at all compatible with the direction the PR has taken. The parse_transform will probably not do anything, considering you don't call those functions.\n. Depending on the machine the ideal number tends to be between 10 and 100. A default of 10 is probably fine, these processes don't tend to take much memory.\nI know it is already configurable in some places (web stomp i think?) so I will check everywhere and make it an option where it is missing.\n. Ready for review.\n. In addition to the previous comments:\nI vote 18 for maps, because the syntax is more complete, and large maps are fast.\nGoing to 18 opens the way to eventually switching to Cowboy 2 (currently WIP) which would allow me to delete a lot of code in various parts of RabbitMQ and would also allow supporting HTTP/2.\n18.2 would mean having some extra fixes for SSL that makes it easier to avoid it taking insane amounts of memory under specific scenarios. But 18.2 was a little broken on Windows and FreeBSD, so we need to require 18.2.1 at a minimum on those platforms (easy enough for Windows, since it's the last release listed on erlang.org; for FreeBSD we need to check if patches are in).\n. The plugin list is here http://erlang.mk/guide/plugins_list.html and the Elixir plugin here https://github.com/botsunit/elixir.mk\n. I have updated to current stable, pushed and this can be merged.\nI have done a few more tests, in particular in a cluster with a HA queue, and was not able to see much of a difference before/after this patch. Still this brings this part of the code in line with the rest of the module and should have no negative impact.\n. Yes I am open to adding checks to catch errors early/provide better messages. 1.3.0 improved eaddrinuse and another but not the ones mentioned in this ticket. If you have more cases I'll be happy to add further checks.. Can you show me what's the corresponding code in 3.7.0?. It's a little more complex depending on user needs. Just whitelisting will allow using sni options, but still require a default cert defined when sni is not used. Is that good enough?\n. Pushed a PR updating Ranch to 1.3.0 which has proper SNI support.. Closing because bad target branch.\n. Cheers!\n. You don't need a Google account. IIRC you need to send an email to rabbitmq-users+subscribe@googlegroups.com to subscribe. I'm not sure how to unsubscribe after that but I'm guessing using +unsubscribe instead would work. Email title and content don't matter.\n. Ranch 1.3 has started making some errors simpler (initially contributed by @binarin btw). We should probably add this in Ranch directly and issue a 1.3.1 release.\nEdit: and eaddrinuse is already done in 1.3.. Worth nothing:\nChecking on a specific status code is not a good practice, you only really should care about receiving a 2xx. The only reason you would look for a special status code is if you need to handle it differently than others of the same class of status codes (401 for example). If you do a request and get a 2xx and don't need to differentiate between the different 2xx codes, then don't match specifically against it.. Yes... :-(. The log doesn't provide me with enough information. The configuration and log files would certainly help.. This is an example function returning trace patterns: https://github.com/rabbitmq/rabbitmq-server/pull/1225/files#diff-9cd14a2bfbc44ba926e2fd2c4219e589R38\nAll patterns from Looking Glass can be set here so no extra changes will be necessary moving forward.. After these PRs get merged some extra work will be necessary to add more functions for the various important parts of RabbitMQ and plugins.. I cannot reproduce these on a fresh master. It sounds like the lz4 app is a plugin in your environment, when it shouldn't be (that's what part of the rabbit-common patch prevents). I'm not sure what could be the issue.\nI will rename the module in a few minutes.. That one is done. I'll have some LG improvements tomorrow around noon that I couldn't finish tonight, got disturbed.. OK first to be clear, you do not need to specify the PLUGINS variable. Just RABBITMQ_TRACER. LG cannot be loaded as a plugin since it is a NIF. The proper configuration is setup just by using RABBITMQ_TRACER.\nSecond is curious, let me try on an OSX environment (that's OSX right?).. You need to install the LZ4 library (brew install lz4).. Done requested changes in other PR and rebased to current master.. There's a little more work I think, though it's nearly there.\nTesting involves connecting tons of clients with tons of queues, and killing or stopping a node and seeing how the other behaves. Also seeing how the other node behaves when the killed node is restarted during or after the queue cleanup is completed.. Yes it seems like a stack overflow in the code that runs ets:select_replace/2. I guess while Erlang can't do stack overflows, the VM still can hehe.. > According to Observer data, there's definitely some room to optimize stats collection\nAnd many other things we noted while testing but left for another time. :-) Memory next!. If I understand https://github.com/rabbitmq/rabbitmq-server/pull/1526#issuecomment-376983039 correctly this is not the issue we were working to fix but another issue. Namely, that when you have many connections stopping on their own, they all have to go through terminate to clean up individually, and this implies at least one Mnesia transaction (and therefore locks) per connection being terminated. It can certainly be improved, possibly by having a process do batch cleanups, but it's for another time.\nWe've optimized for the case where in a cluster of 2+ nodes, 1 node goes down and the remaining nodes have to cleanup. 2+ node cluster, Ctrl+C twice a node, check how fast the cleanup is done and/or start the node again, see how long it takes for it to be able to start working again. This code is never triggered in a single node scenario.. @dumbbell Please give it a small look and tell us if we're going the wrong way about this.\nI also have not been able to test on Windows (I wrote a small script testing for memory allocators, tested it and then incorporated it into rabbitmq-server.bat which I then didn't test, I'm hoping you have a CI environment for Windows).. @dumbbell On Windows you can install different versions side by side. Then it's just a matter of pointing to the right version. I change the PATH variable in my CI plugin: https://git.ninenines.eu/ci.erlang.mk.git/tree/early-plugins.mk#n119. I have added the changes to rabbitmq-service.bat.\nShould I work on a separate PR to include it in 3.6.16?. I have tested the patch on R16B03 on the branch v3.6.x on Linux and it works as intended. I am unable to test on Windows.. Woo!!. This was tested against R16B03-1 on Linux only.. Depends on https://github.com/rabbitmq/rabbitmq-common/pull/282 + a few tiny changes.. Oh geez. Yes it was indeed missing from the non-proxy protocol branch. Sorry about that.\nAbout accept_ack, it's now handshake that performs  the equivalent operation in Ranch. The only difference is a new socket is returned; it mirrors the interface changes in the ssl application. In Ranch 2.0 the only way to obtain the socket will be through handshake.. It does. Current Ranch has both the old accept_ack and the new handshake, and RabbitMQ switched to the new function. You can call either of them (not both!) until Ranch 2.0.. Once Ranch has accepted a socket it needs to start a connection process and gives controlling_process to this new process. The accept_ack waits for that to happen. For ssl it also performs the TLS handshake.\nThe principle is the same with handshake except it was made to work like ssl's handshake function, which it calls when the transport is ssl. There was a slow move toward handshake over the past few releases which culminate in accept_ack now being just a wrapper around handshake, but the functionality hasn't changed; only the interface is being phased out: https://github.com/ninenines/ranch/blob/1.7.0/src/ranch.erl#L231. The above comment about the merge/spec position should not apply anymore.. OK one warning left to go.. Dialyzer passes on my machine! Please review/merge and enable Dialyzer on CI.. I will send a PR for that also.\n. I need to test it to make sure but downloading/compiling something is a matter of adding it to BUILD_DEPS and setting the appropriate dependent targets. I'm not sure if cuttlefish works well with Erlang.mk though (I know it compiles, I don't know if it produces a working escript). I'll experiment.\n. Right, it doesn't build a proper escript out of it right now (rebar escript options are ignored). If you don't care too much about having a \"cuttlefish\" executable you can have it in BUILD_DEPS and call the erlang functions directly through $(erlang ...), otherwise I can fix it in Erlang.mk.\n. I opened https://github.com/ninenines/erlang.mk/issues/503 to track this. Feel free to remind me if it's not done when it's time to merge, it shouldn't take too long to do.\n. none() and no_return() should be equivalent. But somehow only no_return() fixes the Dialyzer warning. If anyone has an explication.... ",
    "jingHW": "{rabbit,\"RabbitMQ\",\"3.4.3\"}\nmore:\n     [{rabbitmq_tracing,\"RabbitMQ message logging / tracing\",\"3.4.3\"},\n      {rabbitmq_management,\"RabbitMQ Management Console\",\"3.4.3\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.4.3\"},\n      {webmachine,\"webmachine\",\"1.10.3-rmq3.4.3-gite9359c7\"},\n      {mochiweb,\"MochiMedia Web Server\",\"2.7.0-rmq3.4.3-git680dba8\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.4.3\"},\n      {rabbit,\"RabbitMQ\",\"3.4.3\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.3\"},\n      {inets,\"INETS  CXC 138 49\",\"5.10.3\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.12.3\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.4.3\"},\n      {xmerl,\"XML parser\",\"1.3.7\"},\n      {sasl,\"SASL  CXC 138 11\",\"2.4.1\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"2.2\"},\n      {kernel,\"ERTS  CXC 138 10\",\"3.0.3\"}]},\n. two nodes in cluster. \n. ",
    "baoyonglei": "I happen to meet the similar problem.\nI have 2 rabbit nodes, and have a script running on the both nodes which just restart the rabbitmq then sleep 25 seconds.  after a while, maybe  30~60miniuts, the problem happened.  after the problem happen , some queue declaration will block ,never return.\nI debug this , and find that, the queue pid is not alive. \nEshell V6.2  (abort with ^G)\n(rabbit@rabbitmqNode0)1> rd(resource, {virtual_host, kind, name}).\nresource\n(rabbit@rabbitmqNode0)2> rd(amqqueue, {name, durable, auto_delete, exclusive_owner = none,arguments,pid,slave_pids, sync_slave_pids,down_slave_nodes,policy,gm_pids,decorators,state}).\namqqueue\n(rabbit@rabbitmqNode0)3> QN2= #resource{virtual_host= <<\"/\">>, kind=queue,name= <<\"q-servicechain-plugin\">>}. \nresource{virtual_host = <<\"/\">>,kind = queue,\n      name = <<\"q-servicechain-plugin\">>}\n\n(rabbit@rabbitmqNode0)4> \n(rabbit@rabbitmqNode0)4> rabbit_misc:dirty_read({rabbit_queue, QN2}).\n{ok,#amqqueue{name = #resource{virtual_host = <<\"/\">>,\n                               kind = queue,name = <<\"q-servicechain-plugin\">>},\n              durable = false,auto_delete = false,exclusive_owner = none,\n              arguments = [],pid = <3079.1517.0>,\n              slave_pids = [<0.823.0>],\n              sync_slave_pids = [],down_slave_nodes = [],\n              policy = [{vhost,<<\"/\">>},\n                        {name,<<\"ha_length_ttl\">>},\n                        {pattern,<<\"^(?!metering.sample).+\">>},\n                        {'apply-to',<<\"queues\">>},\n                        {definition,[{<<\"ha-mode\">>,<<\"all\">>},\n                                     {<<\"ha-sync-mode\">>,<<\"automatic\">>},\n                                     {<<\"max-length\">>,59600},\n                                     {<<\"message-ttl\">>,86400000}]},\n                        {priority,1}],\n              gm_pids = [{<0.828.0>,<0.823.0>}],\n              decorators = [],state = live}}\n(rabbit@rabbitmqNode0)5> rabbit_misc:is_process_alive(pid(3079,1517,0)).\nfalse \n. Then I have a look to rabbit-server log:\n=INFO REPORT==== 27-Sep-2015::04:34:13 ===\nMirrored queue 'q-servicechain-plugin' in vhost '/': Adding mirror on node rabbit@rabbitmqNode1: <0.1517.0>\n=INFO REPORT==== 27-Sep-2015::04:34:38 ===\nMirrored queue 'q-servicechain-plugin' in vhost '/': Slave <0.1517.0> saw deaths of mirrors[[32,\n                                                                                             <3078.751.0>]]\n=INFO REPORT==== 27-Sep-2015::04:34:38 ===\nMirrored queue 'q-servicechain-plugin' in vhost '/': Promoting slave <0.1517.0> to master\n=INFO REPORT==== 27-Sep-2015::04:34:38 ===\nMirrored queue 'q-servicechain-plugin' in vhost '/': Synchronising: 0 messages to synchronise\n=INFO REPORT==== 27-Sep-2015::04:34:38 ===\nMirrored queue 'q-servicechain-plugin' in vhost '/': Synchronising: all slaves already synced\n=WARNING REPORT==== 27-Sep-2015::04:34:45 ===\nMirrored queue 'q-servicechain-plugin' in vhost '/': Stopping all nodes on master shutdown since no synchronised slave is available, pid=<0.1517.0>\nThe last log is the key, I think.  If the queue have not been synchoronized, and the master have to terminate, it will not  fresh the mnesia db now.\nAm I right? \n. ",
    "AVGP": "I have a different use case, but it would be great to have the feature, because I think message groups are what I'd need for it at the moment.\nSo, I'm having a task that usually sits on the queue for a quite some time (i.e. minutes or even hours) and while it's on the queue other users may request the exact same task.\nInstead of adding a duplicate I'd like the duplicates to be handled as if one single job was created.\nCurrently I solve this with an additional storage that keeps track of things that have already been submitted... but I'd rather not have to do that.\n. ",
    "petercuichen": "@michaelklishin My original issue is like this, we producer many message like (id, status), which can be (134378578, 0) or (231394533, 1), to one exchange, and so on to one queue, but I have several consumers listening this one queue. Here comes the problem, I need to make sure the consumers consume the messages with the same 'id' in order. \nFor example, in queue, message A (134378578, 1) comes after message B (134378578, 0) , so I need message A to be processed also after message B, there is no problem with one consumer to one queue, but with several consumers, we can't be sure because of uncertain network delay, so what I am thinking is to make sure messages with the same 'id' be processed by the same consumer. So 'Message Grouping' may solve my problem.\nI saw some solutions in exchange level, but in my case, I need something like sharding according to the message info in queue->consumer level.\nI shall be in great grateful if you can give me some other solutions in RabbitMQ, if not Message Grouping.\nThanks again for your rapidly response.\n. @videlalvaro Good point, thanks for providing the idea. Yeah, I considered it before, but I have a few concern below:\n- If I use one consumer for a queue, how could I do to make the message still be consumed when the one consumer down? I used multi-consumer for one queue before.\n- May it lose some or even one message when binding a new queue to the exchange? (also unbinding). Cause in my application, the message is so important that we shouldn't even lose one.\n- It may be a bit heavy for me cause when there is 20 consumers, I should create 20 queues to bind the exchange, and do it again every time I change the consumers (for the consumer in the application could be changed sometimes), and the OPS will kill me...\nThanks for your response.\n. @michaelklishin Okay, got that. I won't do it again.\nThanks for your response.\n. ",
    "yairogen": "Any updates on this? I would also like to this feature.\n. ",
    "funwun": "Any updates on this? . ",
    "ducdigital": "+1 for this. just looking around and it seem pretty important feature to have . ",
    "z0mb1ek": "+1. ",
    "kbrumer": "+1. ",
    "jpackagejasonc": "thanks for the quick response! is there a yum repository method that is preferred over packagecloud?\n. it would be nice to have a rabbitmq yum repo analogous to the deb repos already provided on the downloads site (at least for my use case anyway). I'll use packagecloud for now and keep an eye out for any changes here. thanks!\n. ",
    "ryandzink": "To correct the dialog box, it appears the installer needs to have an /SD switch added to the MessageBox so the OK button is selected by default. There's documentation here: http://nsis.sourceforge.net/Docs//Chapter4.html#silent\nAs far as the service not getting installed, it looks like it is failing to install because the installer detects the environment variable ERLANG_HOME is not present. I see when the uninstall runs it removes ERLANG_HOME variable and when the install runs in non-silent mode it creates the ERLANG_HOME variable, but maybe there is a timing issue that prevents the environment variable from being created when it has been removed in the past second or so as the uninstall/reinstall occurs.\n. @michaelklishin Unfortunately there isn't a standard way to do that. NSIS has a lot of limitations in that department. However I think passing parameters to the uninstall process could work. If there was a parameter that indicated the Uninstall was temporary, it could provide a workaround. I'll see what I can dig up.\n. I don't know if this is an issue that can be remedied, but I'd like to add that we have found that the NSIS installer run with the /S argument for silent mode install cannot be run as the system account \"NT AUTHORITY\\SYSTEM\". When it is run this way, some of the functionality of the installer is bypassed. In particular, the installer ignores the installer configuration environment variables listed here, specifically RABBITMQ_BASE, leading to the failure to create the db and logs folders.\n. Yes, I had to use that line as the language looks like it is set otherwise in your build process and neglected to remove it. I have removed it and pushed the changes to my branch, however I am seeing this in the new version of the installer:\n\nThe final page of the installer looks like it is missing some sort of content. I validated this across a few different machines. I don't believe my changes impacted that but I wanted to know if that is something that you are seeing on your side as well.\n. My branch is ready for testing. I have compiled and the installer silently successfully but am wondering if you were able to replicate the issue I posted above with the white installer pages...\n. ",
    "emqplus": "@michaelklishin, I am forking now, will submit PR minutes later. \n. @videlalvaro, no I did not check gen_server2 module in older releases. I submit a PR: https://github.com/rabbitmq/rabbitmq-server/pull/269\n. ",
    "freemansoft": "The salt should be somehow configurable also.\n\nPA-DSS requires that each password must have a unique input variable that is concatenated with the password before the cryptographic algorithm is applied\n. \n",
    "baiyusysu": "In previous RabbitMQ, the password hashing algorithm is that to store the password as :4 bytes salt + md5(4 bytes salt + password).Is this Algorithm changed like salt + sha-256(4 bytes salt + password)?\n. ",
    "pnovotnak": "Sorry for my ignorance! I had seen the RABBITMQ_NODENAME variable in the documentation and tried using it, but was not setting up my container correctly.\n. ",
    "nishan": "Thanks @michaelklishin. I have raised a new pull request on stable branch.\nhttps://github.com/rabbitmq/rabbitmq-server/pull/276\n. Oops! Sorry.  Let's try one more time?\nhttps://github.com/rabbitmq/rabbitmq-server/pull/277\n. Closing this. I should check if the target system is Linux based one before using \"-r\".  Please don't merge this change.\n. ",
    "sukinsky": "Hi @michaelklishin!\nI'm using MS Windows Server 2008R2 with RABBITMQ_BASE=D:\\Work\\ESB\\rabbitmq_base.\nUsed your config and run the server, port didn't changed:\n\nMade a typo with RABBITMQ_NODE_PORT.\n. @michaelklishin, config seems to be ok, I can change ssl and management ports.\nCreated env variable RABBITMQ_CONFIG_FILE=D:\\Work\\ESB\\rabbitmq_base\\rmq.\nSpecially changed config file name. Reinstalled service and port didn't changed:\n\n. @michaelklishin, made another test with this config:\n[\n    {rabbit, \n        [ \n            {tcp_listeners, [35000]},\n            {ssl_listeners, [35001]}\n        ]\n    },\n    {rabbitmq_management, \n        [\n            {listener, [{port, 35002}]}\n        ]\n    }\n].\nAnd here is what I get:\n\nIf I make a mistake in tcp_listeners port number or smth else - service will not start, so config is loading correct.\nLogs are clean:\n``=INFO REPORT==== 26-Aug-2015::15:39:54 ===\nStarting RabbitMQ 3.5.4 on Erlang 18.0\nCopyright (C) 2007-2015 Pivotal Software, Inc.\nLicensed under the MPL.  See http://www.rabbitmq.com/\n=INFO REPORT==== 26-Aug-2015::15:39:54 ===\nnode           : rabbit@srv\nhome dir       : C:\\Windows\nconfig file(s) : d:/Work/ESB/rabbitmq_base/rmq.config\ncookie hash    : BizMGvvRTtTkxYoG3yBwAA==\nlog            : D:/Work/ESB/rabbitmq_base/log/rabbit@srv.log\nsasl log       : D:/Work/ESB/rabbitmq_base/log/rabbit@srv-sasl.log\ndatabase dir   : d:/Work/ESB/rabbitmq_base/db/rabbit@srv-mnesia\n=WARNING REPORT==== 26-Aug-2015::15:39:54 ===\nKernel poll (epoll, kqueue, etc) is disabled. Throughput and CPU utilization may worsen.\n=INFO REPORT==== 26-Aug-2015::15:39:55 ===\nMemory limit set to 44232MB of 110581MB total.\n=INFO REPORT==== 26-Aug-2015::15:39:55 ===\nDisk free limit set to 50MB\n=INFO REPORT==== 26-Aug-2015::15:39:55 ===\nLimiting to approx 8092 file handles (7280 sockets)\n=INFO REPORT==== 26-Aug-2015::15:39:55 ===\nPriority queues enabled, real BQ is rabbit_variable_queue\n=INFO REPORT==== 26-Aug-2015::15:39:55 ===\nManagement plugin: using rates mode 'basic'\n=INFO REPORT==== 26-Aug-2015::15:39:55 ===\nmsg_store_transient: using rabbit_msg_store_ets_index to provide index\n=INFO REPORT==== 26-Aug-2015::15:39:55 ===\nmsg_store_persistent: using rabbit_msg_store_ets_index to provide index\n=INFO REPORT==== 26-Aug-2015::15:39:56 ===\nstarted TCP Listener on [::]:5672\n=INFO REPORT==== 26-Aug-2015::15:39:56 ===\nstarted TCP Listener on 0.0.0.0:5672\n=INFO REPORT==== 26-Aug-2015::15:39:56 ===\nstarted SSL Listener on [::]:35001\n=INFO REPORT==== 26-Aug-2015::15:39:56 ===\nstarted SSL Listener on 0.0.0.0:35001\n=INFO REPORT==== 26-Aug-2015::15:39:56 ===\nManagement plugin started. Port: 35002\n=INFO REPORT==== 26-Aug-2015::15:39:56 ===\nStatistics database started.\n=INFO REPORT==== 26-Aug-2015::15:39:56 ===\nServer startup complete; 6 plugins started.\n * rabbitmq_management\n * rabbitmq_web_dispatch\n * rabbitmq_management_agent\n * amqp_client\n * webmachine\n * mochiweb`\n```\n. @michaelklishin here is 3.5.3 with everything same:\n\n. 3.5.3 report is here https://gist.github.com/sukinsky/89f01de898b6c322f6ca\n3.5.4 https://gist.github.com/sukinsky/55b2d651e3c3e4041bdc\ndifference:\n3.5.4 {tcp_listeners,[{\"auto\",5672}]},\n3.5.3 {tcp_listeners,[35000]},\n. @michaelklishin updated my previous post\n. @michaelklishin I used installers otp_win64_18.0.exe and rabbitmq-server-3.5.4.exe on a clean WS2008R2.\nNext I made an env variable RABBITMQ_BASE=D:\\Work\\ESB\\rabbitmq_base\nSo config is here: D:\\Work\\ESB\\rabbitmq_base\\rabbitmq.config\nIn that configuration Rabbit finds config file by itself. I can change any settings besides tcp_listeners.\nYou asked me to set env var for config file explicit like  'RABBITMQ_CONFIG_FILE=D:\\Work\\ESB\\rabbitmq_base\\rabbitmq.config` but didn't help \nI tried again on my local PC. Installed OTP and Rabbit. Didn't change any env vars, just used the same config. Setting tcp_listeners didn't work in 3.5.4, in 3.5.3 -works.\n. @michaelklishin yes\n. @michaelklishin good, thank you\n. @michaelklishin it's ok, will wait for 3.5.5\n. ",
    "lishuai87": "After reduce_memory_use, lots of garbage generated. Paging will continue if no implicit GC happen. I think it's better to do explicit GC after most msg paged, and reduce memory_monitor and update_ram_duration timer (DEFAULT_UPDATE_INTERVAL and RAM_DURATION_UPDATE_INTERVAL). \nAlso is there any meaning to do reduce_memory_use for every msg(rabbit_variable_queue:publish/6)?  maybe just let set_ram_duration_target do that will better?\n. awesome. no paging, no speed drop :-)\n. can't start 3.6.0 RC2 with hipe:\n```\nError description:\n   badarg\nLog files (may contain more information):\n   /data/rabbitmq/log/rabbit.log\n   /data/rabbitmq/log/rabbit-sasl.log\nStack trace:\n   [{erlang,get_module_info,[rabbit_reader,native],[]},\n    {rabbit_reader,module_info,1,[]},\n    {rabbit,'-hipe_compile/0-lc$^0/1-0-',1,\n            [{file,\"src/rabbit.erl\"},{line,288}]},\n    {rabbit,hipe_compile,0,[{file,\"src/rabbit.erl\"},{line,282}]},\n    {rabbit,'-boot/0-fun-0-',0,[{file,\"src/rabbit.erl\"},{line,357}]},\n    {rabbit,start_it,1,[{file,\"src/rabbit.erl\"},{line,386}]},\n    {init,start_it,1,[]},\n    {init,start_em,1,[]}]\n{\"init terminating in do_boot\",badarg}\n.\nErlang/OTP 17 [erts-6.4] [source] [64-bit] [smp:8:8] [async-threads:10] [hipe] [kernel-poll:false]\n```\n. Linux 2.6.32.43\n. does credit_flow affect consume speed?  if there are a few consumers and small prefetch_count.. ",
    "evanccnyc": "Couple of things:\n1) Is there documentation on the website for what to do if the stats db becomes unreachable? I didnt see anything in my brief look. Perhaps that should be documented there?\n2) Even though you shouldnt have to disable/enable the plugin, shouldnt the fact that it crashed while doing this be a bit of problem?\n. Thanks #2 makes sense. I still think #1 should be documented on the website somewhere, I was searching for a while and couldnt find it. \n. Thanks. Ah next time I just submit a pull request for that.\n. ",
    "ben-page": "I changed this to commit against stable.\nhttps://github.com/rabbitmq/rabbitmq-server/pull/317\n. Sorry for the extraneous pull requests. I'm still getting the hang of git.\n. You're right. My PR broke that. I was just about to create a PR to fix it, but @CVTJNII beat me to it.\n. ",
    "degorenko": "rabbitmqctl -q list_users\n/usr/sbin/rabbitmqctl: 19: [: Linux: unexpected operator\nguest   [administrator]\n. No, 3.5.5 has this, but anyway broken.\n. We need backport this:\nhttps://github.com/rabbitmq/rabbitmq-server/commit/67c24aa1ccf5209660d54ad49033a1d1e3cc0502\n. Michael,\nOk, thanks, just for information and bug log.\n. ",
    "EmilienM": "It's actually fixed in master by 6964e8ac1559cf76b9245394b46aaec5822f4178\nWe might need a new release.\n. Right, we need a 3.5.6 as soon as possible.\n. Thank you guys for your responsiveness. Much appreciated.\n. ",
    "moshezvi": "When trying to downgrade erlang-nox, it keeps bringing all the erlang 1:18.1 dependencies. Any idea how to downgrade the deps as well?\n. ",
    "apachesun": "I tried to install rabbitmq on CentOS 7 and I'm still seeing many errors in the startup_log:\nError in log handler\nEvent: {info_report,<0.29.0>,\n                    {<0.7.0>,progress,\n                     [{application,xmerl},{started_at,'rabbit@vm-centos-7'}]}}\nError: function_clause\nStack trace: [{error_logger_file_h,write_event,\n                  [{<0.48.0>,\"/var/log/rabbitmq/rabbit@vm-centos-7.log\",[]},\n                   {info_report,<0.29.0>,\n                       {<0.7.0>,progress,\n                        [{application,xmerl},\n                         {started_at,'rabbit@vm-centos-7'}]}}],\n                  [{file,\"error_logger_file_h.erl\"},{line,113}]},\n              {error_logger_file_h,handle_event,2,\n                  [{file,\"error_logger_file_h.erl\"},{line,78}]},\n              {rabbit_error_logger_file_h,safe_handle_event,3,[]},\n              {gen_event,server_update,4,[{file,\"gen_event.erl\"},{line,522}]},\n              {gen_event,server_notify,4,[{file,\"gen_event.erl\"},{line,504}]},\n              {gen_event,server_notify,4,[{file,\"gen_event.erl\"},{line,506}]},\n              {gen_event,handle_msg,5,[{file,\"gen_event.erl\"},{line,266}]},\n              {proc_lib,init_p_do_apply,3,[{file,\"proc_lib.erl\"},{line,239}]}]\nI tried various combination of erlang package and rabbitmq packages, such as\n- Erlang R16B_03 from epel,\n- Erlang 18.3.1 and 18.3.3\n- rabbitmq server 3.5.4, 3.5.7-1, 3.6.1\nThe error occurred in each combination.\nExactly in which combination this error is gone? Thanks.\n. ",
    "igreenfield": "@michaelklishin #121 only apply to new queues.\nif I already have many masters on one node how I can rebalance it?. Can you point me to a card to track this? it is very important to me organization.. I write same in python: rebalance. ",
    "Itxaka": "@michaelklishin Hi! Is there any update on this? has any planning has been done or is there any spec that we can look at? Just wondering if this may be in for 3.8/4.0 :)\nThanks!. thank you guys for the comments, much appreciated! \ud83d\udc4d . ",
    "dbl001": "Not really.  Just learning.\nSo, install from rabbitmq-server-mac-standalone-3.5.5.tar.gz http://www.rabbitmq.com/releases/rabbitmq-server/v3.5.5/rabbitmq-server-mac-standalone-3.5.5.tar.gz\nas per:\nhttp://www.rabbitmq.com/install-standalone-mac.html http://www.rabbitmq.com/install-standalone-mac.html\nOr, is there a Macport package?\n\nOn Sep 28, 2015, at 7:58 PM, Alvaro Videla notifications@github.com wrote:\nany reason why you run RabbitMQ this way on Mac, and not use the standalone package for Mac or homebrew?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/rabbitmq/rabbitmq-server/issues/337#issuecomment-143929422.\n. ```\n$ sudo rabbitmq-server -detached\nPassword:\nWarning: PID file not written; -detached was passed.\n\n$ ./rabbitmqctl status\nStatus of node 'rabbit@David-Laxers-MacBook-Pro' ...\nError: unable to connect to node 'rabbit@David-Laxers-MacBook-Pro': nodedown\nDIAGNOSTICS\nattempted to contact: ['rabbit@David-Laxers-MacBook-Pro']\nrabbit@David-Laxers-MacBook-Pro:\n  * connected to epmd (port 4369) on David-Laxers-MacBook-Pro\n  * epmd reports node 'rabbit' running on port 25672\n  * TCP connection succeeded but Erlang distribution failed\n  * suggestion: hostname mismatch?\n  * suggestion: is the cookie set correctly?\n  * suggestion: is the Erlang distribution using TLS?\ncurrent node details:\n- node name: nonode@nohost\n- home dir: /Users/davidlaxer\n- cookie hash: 92vmrXD/iLl+OKWxh4sEhQ==\n```\nIs this an Erlang issue?\n\nOn Sep 28, 2015, at 10:40 PM, Michael Klishin notifications@github.com wrote:\nThere was but we no longer support Macpirts. Use Homebrew.\n\nOn 29 sept 2015, at 11:17, dbl notifications@github.com wrote:\nOr, is there a Macport package?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/rabbitmq/rabbitmq-server/issues/337#issuecomment-143951002.\n. \n\n",
    "codenaked": "@videlalvaro thanks much! How often do you put out release builds?\n. A windows installer would be perfect. Thanks!\n. Sure it's at the top of this thread, but here it is also: https://groups.google.com/forum/#!msg/rabbitmq-users/eDgz6nlC_hg/qRFmwaeAAwAJ\nThanks!\n. ",
    "rtraschke": "Ah, shall I add the old 2-tuple version back into the callback list?\n. Done.\n. One way to get started on this road, would be to make use of the SSL verify_fun hook that allows checking details of certificates.\nAs a motivating example, here's how you could create a static check for whitelisted client peer cert serial numbers.\nThe config file needs to include the verify_fun entry in the ssl_options list, something like this:\nerlang\n[\n    {rabbit, [\n        {ssl_listeners, [5671]},\n        {ssl_options, [\n            {cacertfile, \"/Users/rtr/Downloads/rabbitmq_server-3.5.6/testca/cacert.pem\"},\n            {certfile, \"/Users/rtr/Downloads/rabbitmq_server-3.5.6/server/cert.pem\"},\n            {keyfile, \"/Users/rtr/Downloads/rabbitmq_server-3.5.6/server/key.pem\"},\n            {verify, verify_peer},\n            {fail_if_no_peer_cert, false},\n            % check if peer client cert has valid serial number\n            {verify_fun, {ssl_verify, check, [1, 2, 3, 4, 5, 6, 14410040307426354783]}}\n        ]}\n    ]}\n].\nThis makes every SSL cert check call the function ssl_verify:check/3 with the user defined list of serial numbers as one of the arguments (plus the cert and event triggering the check).\nThis function may be implemented along these lines:\n``` erlang\n-module(ssl_verify).\n-export([check/3]).\n-include_lib(\"public_key/include/public_key.hrl\").\n% default verify_fun option in verify_peer mode,\n% with added failure on non-whitelisted client cert serial number\n% (see http://www.erlang.org/doc/man/ssl.html about verify_fun)\ncheck(, {bad_cert, } = Reason, SerialNumberWhiteList) ->\n    {fail, Reason};\ncheck(, {extension, }, SerialNumberWhiteList) ->\n    {unknown, SerialNumberWhiteList};\ncheck(, valid, SerialNumberWhiteList) ->\n    {valid, SerialNumberWhiteList};\ncheck(OtpCert, valid_peer = Event, SerialNumberWhiteList) ->\n    % rabbit_log:info(\"ssl_verify( ~p, ~p, ~p ).~n\", [OtpCert, Event, SerialNumberWhiteList]),\n    Serial = OtpCert#'OTPCertificate'.tbsCertificate#'OTPTBSCertificate'.serialNumber,\n    case lists:member(Serial, SerialNumberWhiteList) of\n        true -> {valid, SerialNumberWhiteList};\n        false -> {fail, invalid_serial}\n    end.\n```\nThe compiled ssl_verify.beam would need to be available in the code path of Rabbit MQ.\n. I think we need to little bit deeper understanding of what rabbit_mirror_queue_misc:initial_queue_node/2 does. Because we're now passing in a potentially different node.\n. ",
    "moriyoshi": "I was under the same circumstances (was stuck with exactly the same error message) and there shouldn't have been such a conflict that is described in https://github.com/rabbitmq/rabbitmq-server/issues/348#issuecomment-148151726, because there were no running nodes (i.e. no epmd processes were running could not be reached) other than the last survived node.\nLooking at the thread in rabbitmq-users likely started by @mr-yuri,  the author has had the identical problem, which is different from the one addressed eventually in #470. \n. ",
    "dims": "@binarin do we ask to reopen this bug or log a new one?\n. cc @binarin \n. @binarin i see the same in http://fossies.org/dox/glibc-2.22/misc_2sysexits_8h.html so +1 from me to use some of the standard ones from that list\n. ",
    "Dzol": "So, the goal is to whitelist clients through their certificates (as opposed to what revocation lists would normally do)?\n. ## Trust-Store\nA verify function would become a default SSL setting (for listeners from every plugin) if rabbit is configured with a valid trusted_certificates tuple. Essentially, a procedure (say whitelisted/1) will interface with the plugin, which keeps hold of trusted certificate information, i.e. in a generic server. The issuer name and serial number being those attributes which uniquely identify certificates [1]. The entry in the configuration file would look something like:\nErlang\n[\n    {rabbit, [\n       ...\n        {trusted_certificates, \"/path/to/directory/\"},\n       ...\n    ]}\n].\nReferences\n[1] PKI Certificate & CRL Profile, 4.1.2.2. Serial Number: https://tools.ietf.org/html/rfc5280#section-4.1.2.2\n. > @Dzol where /path/to/them assumes a /path/to/directory, correct?\nYes, that's correct, a path to a directory: edited that part.\nThis should be 3.6.* compatible: the only change rabbit and plugins will need is to introduce the {verify_fun, {fun (), term ()}} option on an SSL socket if necessary. It can fall back to path validation as @uvzubovs suggests.\nWhat should we be aware of w.r.t. 3.6.* compatibility?\n. > While I was looking I noticed in http://www.erlang.org/doc/man/ssl.html that there is crl_check along with crl_cache. Are those not usable somehow? (Disclaimer: I know -><- this much about Erlang)\n@michaelklishin & @bodgit Those are great for CRL, but they wouldn't make it into a 3.6.* release because they're a recent addition in 18.0, that is if CRL is still on the table.\n. Update: I have plugin boilerplate in place and will make a start on tests.\nAfter some research today, I found that Lo\u00efc Hoguin has done some very helpful work with certificates and testing, which I'm sure will prove to be a real convenience + time saver.\n. > @hairyhum yes but it also prints a ton of other stuff. So this can still be useful.\nHow about printing something like this (alarms on the last line):\n$ ./deps/rabbit/scripts/rabbitmqctl cluster_status\nCluster status of node rabbit@dizzy ...\n[{nodes,[{disc,[rabbit@dizzy]}]},\n {running_nodes,[rabbit@dizzy]},\n {cluster_name,<<\"rabbit@dizzy\">>},\n {partitions,[]},\n {alarms, []}]\n$\nThis commit https://github.com/rabbitmq/rabbitmq-test/commit/a9056ea475a180520c0dd2ded1fb962a4cecdab1#diff-991a3fd99f60cf5a5188bf184e9422b1R494 tests things as they currently are.\nPresumably the output should be similar to the alarms tuple in the status output?\n. This is what the output looks like (following this change):\nCluster status of node a@dizzy ...\n[{nodes,[{disc,[a@dizzy,b@dizzy]}]},\n {running_nodes,[b@dizzy,a@dizzy]},\n {cluster_name,<<\"a@dizzy\">>},\n {partitions,[]},\n {alarms,[{b@dizzy,[]},{a@dizzy,[]}]}]\nThe test here could be improved. E.g.:\n- test that the alarms tuple does in fact contain information for each node\n- test that if an alarm is raise it is printed\n@michaelklishin: please advise before I make a pull request.\n. The above pull requests should close this issue with the proposed improvements.\nThough https://github.com/rabbitmq/rabbitmq-test/commit/1895c74fc931dbd91d273c9b0224bb7b5dab3608#diff-991a3fd99f60cf5a5188bf184e9422b1R633 is a little crude, the alternative would cut off the first line of output a scan & parse the rest, then test against the corresponding term that comes out.\n. To distinguish a node which hasn't completed start-up yet, from a node which failed to create its PID file, we could consider the node to have failed after retrying or timing out.\nSpecifying a timeout on the command-line would be a friendly way to give the user control.\nBut the danger is that the broker may complete start-up after we consider it to have failed.\n. @dumbbell & @michaelklishin: To clarify what I meant above: if the script doesn't get a response from the server (in a given time) saying that it has started successfully, then the script could exit with status/code 1, otherwise 0.\n. There have been some recent fixes related to synchronizing mirrored queues. Issue #714 solved a deadlock in their synchronization which might have been the cause here. The crash shown might not be a direct symptom of it but caused by wrong syncing through the GM module. We will continue investigating. Our other finding concerns the folds, over several backing queues, which are specific to priority queues. This could cause the bad match in the call to update_delta/2.\n. @bharris47: We can reproduce the error reports from the logs but seeing the same symptoms on the management console is proving more difficult. We'll investigate the reason behind those error reports and return to the issue of reproducing the symptoms on the management console in the near future.\n. ",
    "uvzubovs": "CRL doesn't work for us. It's an availability risk (happened a few times). \nWhat works is how Java, .NET, DataPower, and other enterprise systems handle this: there is a trust store (Java Key Store, folder, file, etc.) that contains the public key certificates that match the private keys of the authorized clients. Only those clients are allowed to connect regardless of the CA. This seems to work very well, and has been tested both functionally and secure over the past decade having integrated with dozens of business partners. \nWhite-listing clients may not be palatable when the list is very large, i.e. public Internet and IoT, which is for what CRL was invented. Systems typically implement both strategies: first, see if the cert is in the trust store; if not, then check with the CA.\n. Thank you. For backward compatibility you may implement the strategy to check the trust store first, and if the cert is not in the trust store then check if it was issued by a particular CA, i.e. fallback to the current implementation. If someone wants to disable the latter (like us), they would upload a cert that is not a CA, i.e. of a non-existent CA.\n. MQTT is often used with the clients deployed outside of the trust zone. Therefore, providing them with the precise error description is typically frowned upon for security reasons. Closing the connection on the authorization failure, but logging the true reason on the server correlated to the client id, seems reasonable.\n. Since authorization decides access to a resource by an actor, where the actor is the logged in user and  the resource is the topic, the pattern (in the proposed permission command) should be able to refer to the logged in user. You may see example of this in Mosquitto (look for acl_file in http://mosquitto.org/man/mosquitto-conf-5.html). Such approach addresses the performance concern present in https://github.com/airboxlab/rabbitmq-topic-authorization. \n. With respect to permission scoping, either virtual host or exchange would work since the MQTT plug-in uses single exchange in a virtual host. If you would like topic authorization to apply beyond MQTT, then exchange is certainly the way to go. However, since the MQTT client cannot specify the virtual host like the AMQP client can (prefixing username with virtual host on connect is not always possible as in some cases we use client SSL certificate instead of user credentials), perhaps, you would consider specifying the default virtual host on a per-MQTT listener rather than for the entire plug-in, i.e. \n{rabbitmq_mqtt, [\n      %% {vhost, <<\"MQTT\">>},\n      {tcp_listeners, [{1883,\"vhost1\"}, {1884,\"vhost2\"}]}\n]}\n. I am not skilled with the proxy protocol. Will this work with F5 LTM (local traffic manager, a.k.a. the load balancer/IP-sprayer)?. How would this work with client auth SSL? Proxy needs to terminate SSL to add client info to the connect frame, but it does not have client's SSL cert when connecting to Rabbit; it will present it's own. Will Rabbit's client auth SSL logic be updated to recognize certain client SSL cert as that of a proxy, and so then get the true client SSL cert name from the client info in the connect frame?. When I loaded the queue with messages having empty body but tons of data in the headers, management UI showed queue taking 750MB+ of RAM even though total message body size remained zero. Doesn't it mean that Rabbit already has some idea of how much space is consumed by the headers?\n. I understand. Please consider the following -- If the policy has max-length-bytes but queue memory usage starts to exceed this, isn't it already an indication that the queue is over the limit without calculating anything additional?\n. Second point -- let's assume calculating message header size has non-trivial throughput effect, even though I do not understand why it would (we are adding up a dozen of integers). What is wrong with that? Maybe I would like to trade performance for reliability.\n. Correct. So, is max-length-bytes-include-headers an option? \nAlthough I would think that you should be able to take accurate measurement of the message size (body and headers included) without overhead/performance hit before the message is parsed per the AMQP spec (into the body and headers). No? I am certainly not informed of Rabbit internals, but from a general perspective.\n. We can wait and use max-length in tandem with max-length bytes in the meantime, \n. Sorry, what does this mean?\n. Thank you very much!\n. You got it, Karl! Thank you very much for your insight.\n. Would you please clarify how this is expected to work on the consumer end? The regex that controls which routing keys are delivered seems to be applied to the consumer user and exchange, but consumer is connected to a queue. Will all messages go to the queue but only some dispatched to the consumer? What happens to the remaining messages? And, why are there 2 regular expressions in the rabbitmqctl command for the consumer?. From your response, https://github.com/rabbitmq/rabbitmq-server/issues/1085 (MQTT: The check when subcribing to a topic should use the new check_topic_access/4 callback), and https://github.com/rabbitmq/rabbitmq-website/commit/de19f01ca6cc7affef0dba5415f7e092f641b061 do I understand correctly that consumer permission is tested in the MQTT plug-in, i.e. if MQTT consumer attempts to subscribe to MQTT topic \"foo\" then MQTT plugin will only create queue with binding key \"foo\" if such permission exists, and that is all that controls message delivery to the MQTT consumer?. Ok. Thank you for confirming.\nOne authorization use-case requires restricting the MQTT consumers to \"own\" topics, i.e. have one permission apply to all users. Mosquitto solves this problem via %u substitution, i.e. \"data/%u\" permission would allow user \"foo\" to subscribe to the \"data/foo\" topic and user \"bar\" to \"data/bar\" topic. Is this supported?. Do you think you could accept this as an enhancement? Otherwise, topic authorization may not be practical when the number of user is too large to list.. Thank you. Mosquitto also supports %c substitution for the client id. This is important when consumer needs to know from which client the message came in case it needs to respond back to that client -- kind of like request/reply -- and, so, client publishes a message and subscribes to the topic that reflects that client's MQTT client id. Please refer to the requirements in https://github.com/rabbitmq/rabbitmq-mqtt/issues/95. Before 3.7.0 meaning in 3.6.x or before 3.7.0 ships?. Can vhost and user id be logged when connection name is logged? . It would help if everything is logged together, if possible, please. Otherwise, we have to do quite a bit of acrobatics to collate entries on source IP/port.. I'll take what I can get  and as much as you can give ;). Thank you so much for getting this done. Is this available for testing?. ",
    "david-mohr": "+1 for CRL.  Riak has implemented this already: https://github.com/basho/riak_api/pull/39/files.  Exporting a function similar to validate_function would allows users to opt in for CRL support by using verify_fun.\n. ",
    "bodgit": "+1 for CRL and/or OCSP support. I'm creating a large number of client certificates on the fly and so putting them into a directory on the server just isn't a feasible solution. When a client certificate is revoked I'm removing the matching ACL from RabbitMQ so if a CRL is temporarily unavailable the user can technically connect to the server, but they can't log in anymore.\nWhile I was looking I noticed in http://www.erlang.org/doc/man/ssl.html that there is crl_check along with crl_cache. Are those not usable somehow? (Disclaimer: I know -><- this much about Erlang)\n. ",
    "biiiipy": "Just wanted to chime in that this feature is critical and I completely agree with what @uvzubovs said - certificate whitelist in a trust store (specifically windows certificate store) would be the best fit for our use case. CRLs work for global scale, not so good for a managed application.\n. This part is very problematic on Windows, so we invested a lot of time in creating a custom distribution (with predefined folders and patched .bat files) which: \n1) doesn't require install\n2) doesn't touch any user's folders\n3) doesn't use environment variables (they are really bad, believe me)\nFor real enterprise deployments these things are critical, so any improvements would be very welcome.\n. I didn't go into specifics, but if you asked... This is our scenario and works for us, maybe someone will find this usefull.\nWe distribute erlang and rabbitmq together in zip file with a folder structure:\n- appdata\n- erl\n  - rabbitmq_server\nThis folder structure is deployed to server to a specific folder, for example C:\\dist\nWe bundle all the plugins and already configured enabled_plugins file, and we have to patch erl.ini files in erlang dist to point to the actual folder erl will be deployed:\n- Bindir=C:\\dist\\erl\\erts-7.2.1\\bin\n- Rootdir=C:\\dist\\erl\nWe also patch rabbitmq-env.bat to call our .bat at the beginning, where we set following env variables:\n- RABBITMQ_BASE=\"C:\\dist\\appdata\"\n- ERLANG_HOME=\"C:\\dist\\erl\"\nand we don't want to use .erlang.cookie from user's or c:\\windows folder, that's why we store it in appdata folder and set environment variables:\nCOOKIE_VALUE=\"C:\\dist\\appdata.erlang.cookie\"\nRABBITMQ_SERVER_ERL_ARGS=%RABBITMQ_SERVER_ERL_ARGS% -setcookie %COOKIE_VALUE%\nRABBITMQ_CTL_ERL_ARGS=%RABBITMQ_CTL_ERL_ARGS% -setcookie %COOKIE_VALUE%\nWith this, we just unzip this folder and install service with rabbitmq-service.bat. (of course we configure it, set cookie and other things). If it's done the very first time, you must also execute:\n\"C:\\dist\\erl\\install.exe -s\".\nThe end result:\n1) doesn't require install\n2) doesn't touch any user's folders\n3) doesn't use (system's) environment variables\n4) uses only one folder anywhere on disk with everything in it\nAnd removing dist (or playing with other versions side-by-side) is as easy as uninstalling service and deleting C:\\dist.\nWe obviously didn't want to do all these custom things, but now our deployment is very easy. If you have a multi environment clustered setup, this helps a lot.\n. I'm not sure I understand the case, but basically you don't really need a .erlang.cookie, you just have to provide value with -setcookie which you get from somewhere. In my case, I read the cookie's value from a file (I still call it .erlang.cookie, but it could be anything).\nhere's an example of bat files:\n(before that put your cookie in C:\\dist\\appdata\\.erlang.cookie)\nrabbitmq-env.bat (part)\n``\n...\nREM SCRIPT_DIR=dirname $SCRIPT_PATH`\nREM RABBITMQ_HOME=\"${SCRIPT_DIR}/..\"\nset SCRIPT_DIR=%TDP0%\nset SCRIPT_NAME=%1\nfor /f \"delims=\" %%F in (\"%SCRIPT_DIR%..\") do set RABBITMQ_HOME=%%~dpsF%%~nF%%~xF\ncall \"%SCRIPT_DIR%\\custom-rabbitmq-env.bat\"\nREM If ERLANG_HOME is not defined, check if \"erl.exe\" is available in\n...\n```\ncustom-rabbitmq-env.bat (my bat skills are not so good)\n```\n@echo off\nif \"!ERLANG_HOME!\"==\"\" (\n    set ERLANG_HOME=\"C:\\dist\\erl\"\n)\nif \"!RABBITMQ_BASE!\"==\"\" (\n    set RABBITMQ_BASE=\"C:\\dist\\appdata\"\n)\nif not exist \"C:\\dist\\appdata.erlang.cookie\" (\n    echo. NOT FOUND C:\\dist\\appdata.erlang.cookie\n    exit /B 1\n)\nset /p COOKIE_VALUE=<\"C:\\dist\\appdata.erlang.cookie\"\nif \"!COOKIE_VALUE!\"==\"\" (\n    set COOKIE_VALUE=\"ERROR_COOKIE_NOT_SET\"\n)\necho.%RABBITMQ_SERVER_ERL_ARGS% | findstr /C:\"setcookie\" 1>nul\nif errorlevel 1 (\n  set \"RABBITMQ_SERVER_ERL_ARGS=%RABBITMQ_SERVER_ERL_ARGS% -setcookie %COOKIE_VALUE%\"\n)\necho.%RABBITMQ_CTL_ERL_ARGS% | findstr /C:\"setcookie\" 1>nul\nif errorlevel 1 (\n  set \"RABBITMQ_CTL_ERL_ARGS=%RABBITMQ_CTL_ERL_ARGS% -setcookie %COOKIE_VALUE%\"\n)\n`\n. We have the same issue. @lukebakken worked for me, but I had to add that config torabbitmq.config`` file \n@oneiros-de check your log, if it says lager_error_logger_h dropped XX messages in the last second that exceeded the limit of 100 messages/sec then that means that you haven't successfully changed default 100 message limit. @lukebakken no, we're just setting RABBITMQ_BASE variable. Not sure why advanced.config didn't work. ",
    "ghost": "Ah snap. A breaking protocol change is no good :/\n. I posted this issue here as a suggestion for the team to work on it. Your product is really great and I think it would become bigger if retry capabilities are present. Thank you!\n. I can reproduce this with RabbitMQ 3.7.8. ",
    "jiajunsu": "For getting down nodes, there is a method by rabbitmqctl eval \"rabbit_mnesia:cluster_nodes(all)--rabbit_mnesia:cluster_nodes(running).\" \nBut this method may not work if all running nodes have been restarted(stop_app then start_app) after several nodes down. (cause restart may flush the 'all' nodes)\n. I think it's better adding a rabbitmqctl cli instead the management aliveness check.\nIf server's health-check depend on management plugin, when management got crash for an internal error, we could not know whether the server is fine.\n. ",
    "smee": "There is no problem in the single node case, only in clusters.\n. Ok, so the problem is known, thanks. As requested, here are the results using Erlang 18.1 64bit and RabbitMQ 3.5.6: Archive\nIndeed, the exceptions are the same as in #267.\n. @michaelklishin Thank you! I've never seen such low latencies in any open source related communication as when interacting with you/the RMQ team. Keep up the good work :smile:\n. ",
    "lukebakken": "@belesev - please post your question to the mailing list. Thanks.. @ben-spiller \n\nIt's a big limitation that when queue limit is reached the only available options involve throwing messages away\n\nFurther up in the discussion you find a link to #995, which shipped in 3.7.0. This feature gives you the option of having nacks returned when a queue's limit is reached (docs).. @ben-spiller - have you tried doing a JMS send to a full queue while using the reject-publish option? RabbitMQ will send a basic.nack back to the JMS client.\n@acogoluegnes would probably know off the top of his head what happens \ud83d\ude04 . @Misiu @Soarc - please use the mailing list for discussions.. @wong000 - why do you ask? Did you see the same thing? Do you have reliable steps to reproduce this issue?\nI did some research and it looks like there is no guaranteed way to reproduce this issue. If you can provide one, please do.. @megwill4268 RabbitMQ implements topic authorization as of 3.7.0. Read about it here.. @firewave - this is due to the x509 implementation in Erlang itself. I modified the test CA that the RabbitMQ team uses with the same change you made here: michaelklishin/tls-gen#8. This is the code in OTP that does the filtering.\nUnfortunately, every x509 implementation has different keyUsage interpretation.\nOur SSL guide does use digitalSignature in its CA configuration, but it may not be the starting point for everyone.. @gujun4990 - please read the responses in this issue, several people list a fix that works. If you wish to discuss, please use the mailing list. @ben-spiller - I chatted with @acogoluegnes to see what would happen in this scenario and the scenario where I commented here. It turns out that the RabbitMQ JMS client does not use publisher confirms so it would not know whether or not a published message was rejected.\nAs I'm sure you know, JMS specifies an API but not an implementation, and this would be a change to our implementation. If you would like to submit a pull request with this feature here, it would be appreciated. Thanks.. @cbjjensen - you can see here that the installer queries the registry to find Erlang.\nAt this point, can you check the HKLM\\Software\\Ericsson\\Erlang registry key to see what the value is? That would be valuable information.\nAfter doing that, I recommend that you fully uninstall Erlang and RabbitMQ, remove the ERLANG_HOME environment variable and reboot your VM.\nThen, log in and start an administrative command prompt, and run the Erlang installer from there. Once Erlang is installed, check to see if the HKLM\\Software\\Ericsson\\Erlang registry key is present and valid. If so, install RabbitMQ from the administrative command prompt.\nI've done quite a few \"from scratch\" installations of RMQ in Windows VMs and haven't run into this issue \ud83e\udd37\u200d\u2642\ufe0f . Thanks for the info. If you're not using an administrative account while installing Erlang, it won't have permission to write to HKLM. Apparently that installer doesn't warn about that, either. Let me know how the rest of your testing goes, and thanks again for the report.. @cbjjensen I will make sure the docs are very clear about that. Thanks!. @cocowalla - what method were you using to set the RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS environment variable? Just out of curiosity, thanks.. OK, and in %SCRIPT_DIR%\\custom-rabbitmq-env.bat you had a statement like this:\nset RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=\"-setcookie COOKIE\"\nand then changed it to this:\nset \"RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS=-setcookie COOKIE\"\nor did you leave out the double quotes entirely?. Thanks for the info.. I fixed this issue in rabbitmq/rabbitmq-mqtt#149. This fix will ship in the next RabbitMQ minor release (3.6.15) @lukas-krecan-lt . Hi @ryanvgates I believe this is the discussion:\nhttps://groups.google.com/d/topic/rabbitmq-users/MzPje5n_U7A/discussion. @bitnitdit - I believe the \"CLI tools\" to which @michaelklishin refers are commands like rabbitmqctl, rabbitmq-plugins and not the rabbitmq-server start script.\n@fiksn if you would like to use rabbit@IP style node names, you can follow the directions I posted here.. We could change the os:cmd call to this:\ncase os:cmd(\"systemctl show --property=ActiveState -- '\" ++ Unit ++ $') of\nThat would end the option list with -- and quote Unit using single quotes. I suspect if your unit name contains a single quote you are intentionally asking for problems.\nWorks on my workstation:\n[root@shostakovich ~]# systemctl show --property=ActiveState -- '-.slice'\nActiveState=active. @binarin this will at least address the problem of passing invalid arguments to systemctl.\n\nThe only real solution is to re-implement notifications using unix-domain socket support which is now available in Erlang for some time\n\nHow will that be any different than what is now happening? RabbitMQ's startup correctly informs systemd of its PID. If systemctl status PID doesn't return the correct unit, how is that RabbitMQ's fault?\nI tested both the original code and my changes on CentOS 7 and Ubuntu 16. Both used to work fine, and work fine with my changes. The bug reported here seems to be out of scope.. @rmoriz in your docker environment, what is the output of this command?\nsystemctl status rabbitmq-server.service\nThanks!. @binarin thanks. I noticed that Erlang has unix domain socket support last October and added the change you described to our internal tracker back then.. @tbennett6421 - please see this comment - /sys/fs/cgroup must be available.. @tbennett6421 are you seeing the same output from systemctl status rabbitmq-server.service as reported initially?\nApr 22 14:49:30 01b3d8c9ee66 rabbitmq-server[3169]: systemd unit for activation check: \"-.slice\"\nApr 22 14:49:30 01b3d8c9ee66 rabbitmq-server[3169]: Unexpected status from systemd \"systemctl: invalid option -- '.'\\n\". @michaelklishin - I thought I'd provide details of how I tried to reproduce this issue:\n\nBuilt RabbitMQ and the MQTT plugin using the umbrella project & deps checked out on the rabbitmq_v3_6_4 tag\nStarted via cd deps/rabbitmq_mqtt; make run-broker. In order to aid in debugging, I also added redbug to the code path via the console.\nRan this sample code, only modifying the connection to use localhost and the Subscribe() / Publish() arguments to specify 1 for QoS. Everything ran fine, as expected.\nModifed the code in this branch to send more messages, not wait for responses, and use the same MessageID value. I thought that perhaps this value is the same that is inserted here. Running this modified code doesn't reproduce the issue, though.\n\nI used redbug to ensure that {1, 1} is returned by delivery_qos:\n    ```\n    (rabbit@localhost)3> redbug:start(30000, 25, \"rabbit_mqtt_processor:delivery_qos->return\").\n    {214,1}\n% 13:29:39 <0.398.0>({rabbit_mqtt_reader,init,1})\n% rabbit_mqtt_processor:delivery_qos(<<\"amq.ctag-7XdwG_ahNl3-sKR0s85v1w\">>, [{<<\"x-mqtt-publish-qos\">>,byte,1},{<<\"x-mqtt-dup\">>,bool,false}], {proc_state,#Port<0.7412>,\n% TRIMMED\n% 13:29:39 <0.398.0>({rabbit_mqtt_reader,init,1})\n% rabbit_mqtt_processor:delivery_qos/3 -> {1,1}\n``\n*next_msg_id()does in fact increment the current message id in the process state, so at this point I'm left scratching my head. I wonder if there is something about the client library used by @yckim1 that does not follow the MQTT protocol spec. You can see in the attached screenshot that QoS has a value of1, which requires a message id ([\"packet identifier\"](http://www.hivemq.com/blog/mqtt-essentials-part-4-mqtt-publish-subscribe-unsubscribe)) .. FWIW, I always assumeMBmeans2^20(the power-of-two value) unless docs say otherwise. I think the values should remain power-of-two (kiB,MiB,GiB`) and if the power-of-ten abbreviations are displayed, they could be switched to the IEC abbreviations.\n\n\nCheck out the man page for dd as yet another example of potential confusion:\nN and BYTES may be followed by the following multiplicative suffixes: c =1, w =2, b =512, kB =1000, K =1024, MB =1000*1000, M =1024*1024, xM =M, GB =1000*1000*1000, G =1024*1024*1024, and so on for T, P, E, Z, Y.. Hi @lskbr -\nWithout a core dump or exact means to reproduce this issue, we can't move forward. GitHub issues are used when an actionable piece of work has been defined.\nPlease post your findings to rabbitmq-users, ideally once you are able to produce a core file.\nOther valuable information:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information. When we have enough details and evidence we'd be happy to file a new issue.\nThanks!\nLuke. @lskbr - could you also please provide the rabbit.log file?. @Pkuutn - yes, those segfaults are weird. The source code for the Erlang VM is available here: https://github.com/erlang/otp\nDebugging a segfault requires building a debug release of Erlang and using that, capturing the core file, and analyzing it with gdb or other tools.\nBefore doing this, I recommend building the absolute latest version of Erlang from source (21.1.3 at this time) and seeing if that resolves your issue.\nThe RabbitMQ team also produces pre-packaged Erlang releases:\nhttps://github.com/rabbitmq/erlang-rpm/releases/tag/v21.1.3\nhttps://github.com/rabbitmq/erlang-debian-package\nI also recommend checking your system logs or dmesg (Linux) / system event log (Windows) for clues.. @michaelklishin I'll keep an eye on this and will update this PR accordingly.. @michaelklishin done!. It could be specific to cmd.exe in Windows 10. Certainly, I can get old VMs here. Confirmed on Windows 7 and 8. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. The ha-count policy setting is retrieved here. The policy/2 function returns none which is passed to suggested_queue_nodes and used in an arithmetic statement, causing the badarith exception.\nIt is unlikely that ha-params contained anything other than a non-zero integer due to the validation here.\nSince there is no reliable way to reproduce, we can't investigate further.. @carlhoerberg - the team spent a lot of time investigating solutions but couldn't find anything satisfactory. I will bring your concern to the team's attention. Today and tomorrow are US holidays. Thanks!. Thanks @michaelklishin - I should be able to generalize the build script to be consistent across our projects.. Riak used sudo which caused trouble due to the packaging requirements as you point out. In addition, multiple customers complained about having the command present at all.. See #289, #290 and #339 for garbage_collect() in rabbit_variable_queue. FWIW, tests still pass\n\n. @kjnilsson @michaelklishin - I've confirmed that this fixes both the case where a node is killed in a cluster as well as the case where a partition causes the issue (reported in the support channel today, reproduced using my configuration and pf rules here). We do have partition tests so I'm going to see about adding one.. Reproduction steps:\n\nChange this line to return false\nStart up two-node cluster, add policy where ha-mode for all queues (.*) is all\nDeclare exclusive queue and optionally publish messages to it.\nKill the node to which exclusive queue publisher is connected.\nRestart node. You'll see NaN queues.\nIf you run ct-simple_ha t='cluster_size_2:clean_up_exclusive_queues' the test will fail.\n\nRestore changes from this PR and queues will be cleaned up, test passes.. @marscher - I think there's a hint in the error output:\n\"myuser\",\n\"'.*\",\n\".\",\n\"..\",\n\".mkdir\",\n\".*'\"\nNotice that the second element in that list is \"'.*\" - with a single quote, and that the last element is \".*'\". I recommend double-checking that you are using actual double-quote characters and haven't copied something like U+201C and U+201D.\nOr, change your quoting to use single-quotes:\nrabbitmqctl set_permissions -p myvhost myuser '.*' '.*' '.*'. OK. As @michaelklishin said, there must be something awry with your shell or terminal. If this were an issue with rabbitmqctl be assured we would hear about it non-stop as .* is a very common regex.. Hello!\nIn the future, post questions with information to the rabbitmq-users mailing list. GitHub is used for bugs and other development-related items, not for general discussion or diagnosis.\nHaving said that, you should be able to delete this queue using this command:\nrabbitmqctl eval 'rabbit_amqqueue:internal_delete({resource,<<\"prod\">>,queue,<<\"x1\">>}).'\nThanks -\nLuke. Hello!\nPlease post this question and this information to the rabbitmq-users mailing list. GitHub is used for bugs and other development-related items, not for general discussion or diagnosis.\nThanks -\nLuke. No idea, we'll have to investigate.. Commit with change to wmic - b5b529d2df4bd9ebb74a97e3dc616812d589d857. @lasfromhell thanks for that output. I just installed 3.6.11 on Windows 8.1 and don't see this behavior. I will try on Win 10 next.\nWhat version of Erlang are you using?. @lasfromhell - wmic is used to calculate the memory used by the Erlang VM process and is run once per second. It is more accurate than using the erlang memory calculation strategy.\nI have 3.6.11 running on OTP 19.3 on Windows 10 running in a VirtualBox VM with 2GB ram and 2 vCPUs. As reported by Task Manager, total system memory used remains constant at 1.2GB, and CPU usage doesn't exceed 20%. I'll keep this VM running a while to see if that changes.\n\n. My environment has remained stable as shown in the above screenshot. I'll close this issue now but if any additional reproduction information can be provided please re-open it.. @atroxes - which version of Windows and what patch level? Thanks!. @Gmcourtney - thanks for the detailed report.\n\n\nWould you mind sharing links to discussions around this issue? None of the RabbitMQ team members have been able to duplicate this on their workstations. Personally I have tried both Windows 8.1 and 10 Pro, and see processes cleaned up correctly like you have. I don't have access to any Windows server versions at this time.\n\n\nIf you use the old approach for memory calculation, it is likely that the amount of memory RabbitMQ thinks it is using is less than what it actually is using. This could lead to scenarios where RMQ exhausts memory before its internal memory alarms are triggered.\n\n\n@michaelklishin @hairyhum - It's starting to look like we should revert back to the previous method on Windows.. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nThanks,\nLuke. >  Is there a reason not to?\nI suspect it is because there are (rare) failure situations where auto-restart is not a good solution, or that an auto-restart may clobber data that could be used to diagnose the failure.\n@michaelklishin probably has more historical information about this.. We may want to use StartLimitIntervalSec= and StartLimitBurst= to prevent too many restarts.\nThere is some discussion here.. At this time, I don't see a way in the systemd documentation to limit the total number of restart attempts. Additionally, I don't like the idea of auto-restarting RabbitMQ as that is likely to hide an issue that an operator may want to investigate.. I'll look into it more, though I'm pretty sure systemd is always going to run the ExecStop command. Now that I've read more of the docs the exit code of that command is possibly ignored.. See #693 for some history.. Closing the issue because it is not an accurate description of the problem that needs to be addressed.. @stawiu could you please provide a definitions file to reproduce this issue? Thanks!. @michaelklishin - all set. I'll add the same text to the configuration that ships, one moment.. @michaelklishin on it. @michaelklishin ready for round two. @michaelklishin OK that's good to know ... exactly is working differently for you than me \ud83d\ude04 I'll take that into account.. @michaelklishin all set for round three. Thanks!. @michaelklishin - yeah, on my run I thought of one other thing ... ha-mode exactly needs to take the min-masters strategy into account when picking the nodes to use, which is what you're seeing. For whatever reason I didn't run into that locally.. @michaelklishin alright, turns out some smarter behavior around the initial queue master node was needed. I was able to reproduce the failure you saw and can confirm that I can run the queue_master_location test suite 30 times in a row without error now.. @ransford you can run the following to check the environment RabbitMQ is using:\nrabbitmqctl eval 'os:getenv(\"PATH\").'\nrabbitmqctl eval 'os:cmd(\"which sysctl\").'. @gerhard this is with the default scheduler bind and +spp settings? Just curious! This is fascinating.. I'll check out the shutdown command, thanks.. @michaelklishin @hairyhum this change just makes the badrpc_multi case behave the same way as badrpc.. @michaelklishin I can get the changes in master once this is merged, if you'd like.. OK I'll re-target it for master once rabbitmq-server-batch-betas is merged \ud83d\ude04 . @hairyhum thanks for noticing that the new callback is not necessary.. > Every time queue process handles the bump message it should resume, but not every time it resumes it should clear the flag.\n@hairyhum got it. Thanks again for the careful reviews.. @Jleagle - we're working on improving the error reporting in situations like that. Those messages come from an external program that translates the rabbitmq.conf file into a format that RabbitMQ can use and we may not be able to change the message in the case you describe.. @michaelklishin OK I've restored the code for the general case. I don't think error should be matched explicitly because the only successful return from stop_and_halt is ok ... anything else is a problem.. Of course I didn't test running the command with the node not running ... one second .... @michaelklishin thanks for the reviews. @Ayanda-D hey that is really slick, I will give it a try. Thanks!. Thank you for your time.\nTeam RabbitMQ uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Thank you @Xanhou. Since this issue shows up in search results, I will provide the following steps for validating the syntax of the rabbitmq.config or advanced.config file. These are the Erlang term style configuration files, not the ini-style file that uses the .conf extension.\n\nSave your configuration in /etc/rabbitmq/rabbitmq.config\nRun the following command:\n\nerl -noshell -eval 'io:format(\"~p~n\", [file:consult(\"/etc/rabbitmq/rabbitmq.config\")]).' -eval 'init:stop().'\nYou will either see output that begins with {ok, ...} indicating success, or a syntax error message.. Hello -\nCould you please post this question to the RabbitMQ users mailing list? The RabbitMQ team does not use GitHub issues to answer questions or do issue analysis.\nWhen you post your question, please be sure to include the following information -\n\nRabbitMQ version and Erlang version\nClient library used and version\nOperating system platform and version.\nComplete code example to reproduce your issue (ideally).\n\nThanks -\nLuke. Thanks Aur\u00e9lien for the detailed description. I can reproduce what you report and am investigating. It looks to be specific to set_parameter in rabbitmqctl. @shengis - this is definitely an issue with the rabbitmqctl command, so I opened rabbitmq/rabbitmq-cli#232\nAs a workaround, you should be able to execute this command:\nrabbitmqctl eval 'J=\"{\\\"uri\\\":\\\"amqp://user:password@server/prd\\\",\\\"ack-mode\\\":\\\"on-publish\\\",\\\"trust-user-id\\\":true,\\\"message-ttl\\\":300000}\", rabbit_runtime_parameters:parse_set(<<\"prd\">>, <<\"federation-upstream\">>, \"node01\", J, \"guest\").'\nI ran that command and verified it:\n$ rabbitmqctl list_parameters -p prd\nListing runtime parameters for vhost \"prd\" ...\nfederation-upstream     \"node01\"        {\"ack-mode\":\"on-publish\",\"message-ttl\":300000,\"trust-user-id\":true,\"uri\":\"amqp://user:password@server/prd\"}. Hi Thiago -\nThe RabbitMQ team does not use GitHub issues for discussions or to analyze issues. Please start a discussion on the rabbitmq-users mailing list and we can take it from there.\nPlease be sure to provide as much information as you can. For instance, you do not state what the AMQP 1.0 destination is - what software and what version.\nThanks -\nLuke\n. @rolatsch - please post your question to the mailing list,. @oneiros-de @biiiipy - could you please create the /etc/rabbitmq/advanced.config file with this content and restart your node?\n[\n    {lager, [\n        {error_logger_hwm, 1024}\n    ]}\n].\nCould you confirm that, with this setting, messages are not dropped when your node boots? The reason I ask is that I can't reproduce the dropped message scenario even with debug logging enabled. I am working on raising the error_logger_hwm limit temporarily during startup.. @biiiipy are you setting the configuration file name via an environment variable or via /etc/rabbitmq/rabbitmq-env.conf? The reason I ask is that RabbitMQ 3.7 and beyond uses an ini-compatible configuration format, as well as the old-style Erlang term format file (https://www.rabbitmq.com/configure.html).. Hi @cachiama -\nIf you decide to continue discussion on the mailing list, please also reproduce this issue outside of a Docker environment and report on what you observe. Also, I'm assuming that you are not using OS X in production so details about if you see this in your production environment and what operating system you use there will be valuable as well.\nThanks!. @michaelklishin thanks for the review and catching that missing \\ character. @michaelklishin I forgot the Windows changes. I'll get them now.. Thanks @michaelklishin . @michaelklishin - The other option to fix this is to use the unit name directly instead of via a PID, as the name is well-known (rabbitmq-server.service). Thank you for your time.\nThe RabbitMQ team uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.\n. Latest findings - using long names with 3.7.3 on Windows does not work with rabbitmqctl. I suspect it has to do with the fact that that name chosen when disterl starts is not a FQDN:\nhttps://github.com/rabbitmq/rabbitmq-cli/blob/v3.7.x/lib/rabbitmq/cli/core/distribution.ex#L81-L104\nIf longnames are being used, then something like this should be used to in the case where inet:get_rc() does not return a domain value:\n{ok, Hostname} = inet:gethostname(),\n{ok,{hostent,FullHostname,[],inet,_,[_]}} = inet:gethostbyname(Hostname). Turns out the above issue was due to setting USE_LONGNAME=\"true\" rather than USE_LONGNAME=true - rabbitmq/rabbitmq-cli#244 will address that.. Thanks. I tested this on Win 8.1 and will test the build artifact there and Win 10 when it's ready.. Issue is in rabbitmq-management. Would you look at that, it's even in the comments.. I'll be very interested to see the code changes ... it's what I like to call \"Italianate\" at the moment :spaghetti: . Thank you for the report and the detailed (and correct) reproduction steps.\nhttps://gist.github.com/lukebakken/7459fbb4fcbfa1062198c36ca0d8fd41. ## Test scenario -\n\nOne node running on Linux workstation tuned for many thousands of connections\nRun consumer.go which creates 61440 connections, one channel per connection with an exclusive queue per channel\nStop the app via CTRL-C, then immediately restart it\n\nconsumer.go.txt\nOld behavior\nAfter immediately restarting consumer.go, the first two batches of connections would take about 10 seconds each to be created, then subsequent batches would take less than a second.\nBehavior with #1525 and rabbitmq/rabbitmq-common#259\nAfter immediately restarting consumer.go, the first batch of connections takes 20 - 30 seconds to be created, then subsequent batches take less than a second.. @gerhard I finally had time to follow up on your comment. I changed rabbit_event.erl to use gen_event:start_link/2 but I see the same behavior - a long delay as the first batch of connections tries to establish while previous queues are deleted and cleaned up.. I'd say apply this to v3.7.x and master since RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS provides this function for v3.6.x.. OK I tested my latest changes by creating a rabbitmq-env.conf file with these contents:\nlbakken@shostakovich ~/issues/rabbitmq/rabbitmq-server/gh-1528\n$ cat rabbitmq-env.conf \nSCHEDULER_BIND_TYPE=u\nDISTRIBUTION_BUFFER_SIZE=65536\nMAX_NUMBER_OF_PROCESSES=131072\nMAX_NUMBER_OF_ATOMS=196608\nThen running from this branch with this command:\nmake RABBITMQ_NODENAME='rabbit@shostakovich' RABBITMQ_CONF_ENV_FILE=$HOME/issues/rabbitmq/rabbitmq-server/gh-1528/rabbitmq-env.conf run-broker\nps -ef output shows that the settings were applied:\n$ ps -ef|fgrep beam.smp\nlbakken  25122 22593 15 11:55 pts/2    00:00:02 /home/lbakken/development/erlang/installs/19.3/erts-8.3/bin/beam.smp -W w -A 128 -P 131072 -t 196608 -stbt u -zdbbl 65536 -K true -- -root /home/lbakken/development/erlang/installs/19.3 -progname erl -- -home /home/lbakken -- -kernel shell_history enabled -pa /home/lbakken/development/rabbitmq/rabbitmq-server/ebin -s rabbit boot -sname rabbit@shostakovich -boot start_sasl -config /tmp/rabbitmq-test-instances/test -kernel inet_default_connect_options [{nodelay,true}] -sasl errlog_type error -sasl sasl_error_logger false -rabbit lager_log_root \"/tmp/rabbitmq-test-instances/rabbit@shostakovich/log\" -rabbit lager_default_file \"/tmp/rabbitmq-test-instances/rabbit@shostakovich/log/rabbit@shostakovich.log\" -rabbit lager_upgrade_file \"/tmp/rabbitmq-test-instances/rabbit@shostakovich/log/rabbit@shostakovich_upgrade.log\" -rabbit enabled_plugins_file \"/tmp/rabbitmq-test-instances/rabbit@shostakovich/enabled_plugins\" -rabbit plugins_dir \"/home/lbakken/development/rabbitmq/rabbitmq-server/plugins\" -rabbit plugins_expand_dir \"/tmp/rabbitmq-test-instances/rabbit@shostakovich/plugins\" -os_mon start_cpu_sup false -os_mon start_disksup false -os_mon start_memsup false -mnesia dir \"/tmp/rabbitmq-test-instances/rabbit@shostakovich/mnesia/rabbit@shostakovich\" -kernel inet_dist_listen_min 25672 -kernel inet_dist_listen_max 25672\nTesting Windows next.. Tested on Windows 8.1 using this alpha build with all of the scripts/*.bat files copied from this branch into sbin. After doing the copy, you must run rabbitmq-service.bat remove then rabbitmq-service.bat install for erlsrv.exe to be installed with the correct arguments for erl.exe. Contents of %AppData%\\RabbitMQ\\rabbitmq-conf-env.bat:\nset SCHEDULER_BIND_TYPE=u\nset DISTRIBUTION_BUFFER_SIZE=65536\nset MAX_NUMBER_OF_PROCESSES=131072\nset MAX_NUMBER_OF_ATOMS=196608. @michaelklishin that's correct and is also something I just noticed. Addressing it as well.. Thank you for your time.\nThe RabbitMQ team uses GitHub issues for specific actionable items engineers can work on. This assumes two things:\n\nGitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, etc (as defined by this team)\nWe have a certain amount of information to work with\n\nWe get at least a dozen of questions through various venues every single day, often quite light on details.\nAt that rate GitHub issues can very quickly turn into a something impossible to navigate and make sense of even for our team. Because of that questions, investigations, root cause analysis, discussions of potential features are all considered to be mailing list material by our team. Please post this to rabbitmq-users.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're\nobserving, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used\nServer logs\nA code example or terminal transcript that can be used to reproduce\nFull exception stack traces (not a single line message)\nrabbitmqctl status (and, if possible, rabbitmqctl environment output)\nOther relevant things about the environment and workload, e.g. a traffic capture\n\nFeel free to edit out hostnames and other potentially sensitive information.\nWhen/if we have enough details and evidence we'd be happy to file a new issue.\nThank you.. Please, post your question to the mailing list. If you would like to provide your code, creating a GitHub repository with it works great. Just be sure that everything that is necessary to build and run your code is in the repository.. Thanks @michaelklishin. I changed P to Priority because it wasn't immediately clear what that part of the tuple represents.. rabbit_peer_discovery_classic_config requires a list of nodes in its configuration.\nHelp the RabbitMQ team help you and please provide enough instructions or a script to exactly reproduce your environment:\n\nComplete RabbitMQ configuration files\nHost name values\nDocker container version\n\nIdeally we'd have instructions to reproduce this outside of docker to remove that as a variable. Thank you.. @lemenkov I would like to get your input as well, especially if you can reproduce the issue and test this out.. @lemenkov thank you for following up.. Nice work @hairyhum . Testing with channel_max set to 2048 and trying to create 2050 channels produces this expected error condition:\n2018/05/09 10:33:08 Channel: Exception (504) Reason: \"channel id space exhausted\"\nexit status 1. Please post this to rabbitmq-users. In addition, this question appears to be out-of-scope for RabbitMQ - please provide details as to exactly how Apache 2 and mod_wsgi fit in.. Thanks for running those @michaelklishin ... I forgot to. Running them now to see if I can reproduce.. I see the same test failures in unit_log_config_SUITE. 1b38416 fixed tests for me, thanks. @michaelklishin what about RABBITMQ_ADDITIONAL_PLUGINS_PATH which would be a list of additional directories used to discover plugins. We do have other environment variables in the form *_ADDITIONAL_* so this could be considered \"familiar\".\nUsing _PATH rather than _DIR or _DIRS might make it more self-evident that it's a list of directories.. That would be up to the user to decide, right?\nI think the \"official\" recommendation for recent Windows versions would be C:\\ProgramData\\RabbitMQ\\X.Y.Z\\plugins for plugins that ship with RMQ and something like C:\\ProgramData\\RabbitMQ\\plugins as a default place to look for 3rd-party plugins.. > Paths that include versions can be impossible for installers to compute\nWhose installers? Third-party? They would use the \"generic\" directory C:\\ProgramData\\RabbitMQ\\plugins. We still need a version-specific directory to prevent mixing shipping plugins from third-party ones so that un-installations don't delete 3rd-party plugins. Unless I'm missing something here :smile: . I see. Now I'm curious where the plugin archives are expanded to .... @Alex-Schiegl this is either an issue with your certificates and RabbitMQ configuration or with how you are configuring your Java application to use TLS. If you can provide more details or share your certificates and Java code on the mailing list, that will facilitate us helping out. Thanks.. > but there is error when the rabbitmq-server started\nIt would be helpful to let us know what the error is. I suspect it has something to do with using a fully-qualified hostname as your node name - i.e. myhost.com instead of a short name like myhost.\nIf you have suggestions for improving the documentation, we would gladly accept a pull request here. Thanks.. Hello! Just FYI the RabbitMQ team does not use GitHub to have discussions or diagnose issues like this one. In the future, please post to the rabbitmq-users mailing list.\nI'm not sure why you are using this configuration:\n{auth_mechanisms, ['EXTERNAL']},\nUsing that in conjunction with usernames / password but only configuring the internal auth backend won't work. This is why adding PLAIN fixed your issue. You can read more about it here: https://www.rabbitmq.com/authentication.html\nI'm not sure why you don't think TLS is working in your scenario. You have RabbitMQ configured to only listen via TLS so there's no other option. If you'd like, post to the mailing list and include a packet capture demonstrating that TLS isn't actually being used.\nThanks for using RabbitMQ.. Good catch @michaelklishin I didn't even notice the RMQ and Erlang versions. @postables thank you for providing that info, though, many people don't bother.. Hello!\nThis is the sort of question that should be posted to the rabbitmq-users mailing list. I'll respond here but if any further discussion is needed, please start a new message thread on the list.\nThe error is right in front of you:\n21:50:53.791 [error] ** System running to use fully qualified hostnames **\n** Hostname node1 is illegal **\nnode1 is not considered to be fully qualified by Erlang. Either remove the USE_LONGNAME setting or use hostnames that are fully qualified and can be resolved by DNS.. Searching Google with site:rabbitmq.com log console brings up this document as the first hit.\nIf that is not sufficient, please post follow-up questions to rabbitmq-users.. Thank you for your time and for using RabbitMQ.\nThe RabbitMQ team uses GitHub issues for specific actionable items engineers can work on. This means that GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, and so on.\nPlease post your question or issue to the rabbitmq-users mailing list.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're observing, or at least sharing as much relevant information as possible on the list:\n\nOperating system used, and version\nRabbitMQ version and details on how it was installed\nErlang version and details on how it was installed\nPlugins enabled and their versions\nClient library and versions used.\nServer logs.\nA code example or terminal transcript that can be used to reproduce.\nFull exception stack traces (not a single line message).\nIf applicable, rabbitmqctl status (and, if possible, rabbitmqctl environment output). Redirect output to files and attach the files to your message.\nOther relevant information about the environment and workload, e.g. a traffic capture.\n\nFeel free to edit out hostnames and other potentially sensitive information.\nIf enough details and evidence are provided that demonstrate an issue we'd be happy to file one on GitHub.\nThanks again!\nLuke. @currycan - more than likely your Erlang cookies are not synchronized. Please see this document: https://www.rabbitmq.com/clustering.html#erlang-cookie. This works fine in my local environment:\n$ ./sbin/rabbitmqctl -n rabbit3@MESSIAEN stop_app; ./sbin/rabbitmqctl -n rabbit3@MESSIAEN join_cluster --ram rabbit@MESSIAEN; ./sbin/rabbitmqctl -n rabbit3@MESSIAEN start_app\nStopping rabbit application on node rabbit3@MESSIAEN ...\nClustering node rabbit3@MESSIAEN with rabbit@MESSIAEN\nStarting node rabbit3@MESSIAEN ...\n completed with 0 plugins.. For your first item, please provide the rabbitmq.conf file where you tried to set log.syslog.ip, log.syslog.transport and log.syslog.protocol. As you can see here and here, it should be supported.. Yep, I'm surprised too. I'll check this out when I get some time.. I can't reproduce this using the following configuration files for RabbitMQ and syslog-ng:\nhttps://gist.github.com/lukebakken/cb11303fd8c66384dccd039c2936ad69\nI tested this using Erlang 21.0.2 and RabbitMQ 3.7.7 via the generic-unix package. I started RabbitMQ with this command run from the root of the extracted archive:\nRABBITMQ_NODENAME='rabbit@shostakovich' \\\nRABBITMQ_ALLOW_INPUT=true \\\nRABBITMQ_CONFIG_FILE=/home/lbakken/issues/rabbitmq/rabbitmq-server/gh-1639/rabbitmq \\\n./sbin/rabbitmq-server\nAt this point my guess is that there is something up with your installation.\n@michaelklishin do we want to open a separate issue for the suggestions that @Garagoth made in point #3?. I can't reproduce this with 20.3.8.2, either. syslog-ng output is as follows:\n[2018-07-12T13:32:18.951307] Outgoing message; message='Jul 12 13:32:18 localhost rabbit[18530]: [info] <0.5.0> Server startup complete; 5 plugins started.\\x0a'\n[2018-07-12T13:32:18.951312] Outgoing message; message='Jul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_top\\x0a'\n[2018-07-12T13:32:18.951316] Outgoing message; message='Jul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_management\\x0a'\n[2018-07-12T13:32:18.951320] Outgoing message; message='Jul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_web_dispatch\\x0a'\n[2018-07-12T13:32:18.951323] Outgoing message; message='Jul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_stomp\\x0a'\n[2018-07-12T13:32:18.951327] Outgoing message; message='Jul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_management_agent\\x0a'\nJul 12 13:32:18 localhost rabbit[18530]: [info] <0.5.0> Server startup complete; 5 plugins started.\nJul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_top\nJul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_management\nJul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_web_dispatch\nJul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_stomp\nJul 12 13:32:18 localhost rabbit[18530]:  * rabbitmq_management_agent\nJust FYI, the RabbitMQ core team doesn't maintain those docker images. At this point, this issue is most likely in your environment or in the docker image itself. If you can reproduce this outside of Docker we can continue investigating, or if you can provide a Dockerfile and instructions on how to reproduce this using your configuration file, I can try that out.. Closing, but I will be notified if the information I requested here is provided.. Thanks for the review, and it's very interesting and helpful to know what you were thinking before starting it.. Thanks! @dumbbell @michaelklishin this appears to be the only section of the web site that mentions this command (link). Should that link to the html version of this manual page?\nFinally ... should rabbitmqctl be changed to rename these directories for the user?. Thanks for that additional info.. I can't say I completely understand your situation, because IMHO your load balancer should not have any effect on RabbitMQ clustering.\nI believe you should be able to create an erl_inetrc file with the following contents:\n{host, {192,168,1,1}, [\"4c01db0b339c\"]}.\n{host, {192,168,1,2}, [\"d7886598dbe2\"]}.\n{host, {192,168,1,3}, [\"d894ede837d2\"]}.\n{lookup, [file, native]}.\nIt is basically an /etc/hosts file for the Erlang VM. This file should be readable in each RabbitMQ instance. Create the /etc/rabbitmq/rabbitmq-env.conf file with the following contents:\nexport ERL_INETRC=/path/to/erl_inetrc\nThen, your RabbitMQ configuration would be as follows:\n[\n  {rabbit, [\n    {cluster_nodes, {['rabbit@4c01db0b339c', 'rabbit@d7886598dbe2', 'rabbit@d894ede837d2'], disc}}\n  ]}\n].. Please see the following code:\nhttps://github.com/rabbitmq/rabbitmq-common/blob/master/src/rabbit_nodes_common.erl#L73-L81\nAnd this explanation:\nhttps://groups.google.com/d/msg/rabbitmq-users/YgtY4R_heI0/AZFs5RPTCgAJ\nos:find_executable is returning false.. @michaelklishin - ready for re-review, both log.syslog.ip and log.syslog.host are supported now :smiley_cat: . Thanks for the review @michaelklishin . > So how can I find out the parameters the exe(both the erlang and the rabbitmq) can support?\nBoth the Erlang and RabbitMQ installer are built with NSIS, which only supports a few command-line arguments: http://nsis.sourceforge.net/Docs/Chapter3.html#installerusage. I can reproduce this using 3.7.7. This isn't the correct repository for this issue, but I'll link back here.. @mundus08 for what it's worth, I could not reproduce this using Debian Stretch and the following RabbitMQ + Erlang combinations:\n\n3.7.8-rc3 and Erlang 21\n3.6.16 and Erlang 20.3.6\n\nThis is the Vagrantfile I used for the second test: \nVagrantfile.txt\nIn both cases RabbitMQ restarted just fine after rebooting the virtual machine.. Thanks @michaelklishin - cherry-picked:\nhttps://github.com/rabbitmq/rabbitmq-server/commit/6ae7f68954c8d931eebf7df5585a96ff1c99b2f4. @michaelklishin what do you think about redirecting from http://www.rabbitmq.com/debian/ to bintray?. Hi @noxdafox - do you have an example of a crash with a stack trace or a set of steps to reproduce a crash?. @noxdafox good enough explanation for me.. OK!. @hairyhum before I merge this into config_files_check I want to chat about how many conflicts it may create. I have quite a bit of refactoring and tests to write still.. @michaelklishin roger that \ud83d\udc4d . @hairyhum the check for false was added by #1320. Should something different be done if rabbit_mnesia:is_process_alive(QPid) returns true?. Apparently this assertion existed prior to 7c76ca3 - check out this line. A user reported that this assertion failed in 3.6.16 here. See these lines in the included gist.. Please post this to the mailing list, as there is no evidence of a RabbitMQ bug nor do you provide a way to reproduce what you're seeing.\nWhen providing the output of commands, if the output is more than a line or two, please attach the output in a text file, not pasted like you have done here (which is very difficult to read - I have edited your text to make it easier to read).\nYou should also provide the output (in a file!) of the rabbitmqctl environment and rabbitmqctl report commands.\nFinally, handshake_timeout,handshake is evidence that your sensu client is not doing the TLS negotiation correctly. Since you have upgraded Erlang and OpenSSL, there are probably differences in how TLS negotiation is done that need to be taken into account with sensu. You should try the following:\n\nConfigure sensu to connect to the non-TLS RabbitMQ port (5672) and verify that works. Be sure to disable TLS in sensu.\nIf that works, re-enable TLS in sensu and increase debug output as much as possible, hopefully you will see something useful logged.\nRemove or comment-out the following three TLS options from RabbitMQ to see if it makes a difference. Restart RMQ and re-try connecting from sensu:\n\n{versions, ['tlsv1.2']},\n    {verify,verify_peer},\n    {fail_if_no_peer_cert,true}\nAgain, please follow up on the mailing list, thanks..  @hairyhum do you think the following sequence of events could trigger the badmatch:\n\nwith/4 is called on a stopped mirrored queue here\nThe thunk F fails, so we fall into the error handler here\nDuring that time, the queue pid becomes active\nSince the queue is mirrored, we enter this code block, and trigger a badmatch. We know that retry_wait/4 is only called if F exits with one of these reasons:\n\nnoproc\nnoconnection\nnodedown\nnormal\nshutdown\nLet me know what you think of 9d9f624c9622531849f4cd05a3689c33483a241c. I special-cased the scenario where the queue is mirrored and the pid is alive. Then, in the other case, rather than matching against false the error handler func is called.. Another instance of this error: https://groups.google.com/d/topic/rabbitmq-users/u9URsZ2ova8/discussion. @hairyhum @michaelklishin @kjnilsson I'm not letting this one go! My latest change is simpler, and tries to do something sane where a crash would have happened.. Thanks for the review @hairyhum . Backported to v3.7.x. Hello,\nThe RabbitMQ team does not use GitHub issues for issue diagnosis. Please provide the following in a message to the rabbitmq-users mailing list and we'll help out there:\n\nPython version\nPika version\nComplete set of code that reproduces this issue. This is important!\n\nThanks. Hello,\nThanks for using RabbitMQ and taking the time to provide this information. In general, the RabbitMQ team doesn't use GitHub issues to do diagnosis of an issue. You've provided some information but it's not enough to try to discover the root cause of this issue.\nIn order to get to try and figure out what happened, could you please review this document and answer as many questions as possible? Also, please run the rabbitmq-collect-env script on your node (I am assuming it is Linux / Unix) and provide the generated archive somewhere.\nFor instance, I see that you have a /data directory - are you running in a cloud environment? If so, which one? What sort of storage device is mounted there? The error suggests there may have been an issue with storage.. @Haster2004 @ctapmex - please take the time to review these questions and answer them, then follow up on the mailing list. I also asked a couple other questions in my original response here.\nProviding some logs and a few sentences of information isn't going to help, especially since no way to reproduce has been provided.. @ctapmex I don't see anything in the logs that could explain the error you saw, but I do see some items that require your attention. The following are log entries from msk-mbus-rmq51.mxxx.ru.\n\n2018-09-04 23:00:06.625 [error] <0.12401.97> Channel error on connection <0.28036.21> (10.199.82.102:35390 -> 10.199.82.27:5672, vhost: '/', user: 'cis'), channel 168:\noperation basic.consume caused a channel exception access_refused: queue 'ps.cis.payment_events.1' in vhost '/' in exclusive use\nExclusive queues can only be used by the connection/channel that declares them. The above log message indicates a programming error where your code is trying to access an exclusive queue that it doesn't \"own\". This could be a threading issue, or the queue ps.cis.payment_events.1 may have been declared exclusive when it shouldn't have been.\n\n2018-09-04 23:00:33.742 [error] <0.6888.96> Channel error on connection <0.25118.17> (10.199.82.102:29840 -> 10.199.82.27:5672, vhost: '/', user: 'cis'), channel 647:\noperation basic.ack caused a channel exception precondition_failed: unknown delivery tag 185\nThe above error may be related to the first one. Basically, the wrong channel is trying to acknowledge a message. This points to a threading issue or an issue where channel objects are being shared in your code when they shouldn't be.\n\nI also see a Mnesia overload message during the same time period as the badmatch error -\n2018-09-05 02:16:50.208 [warning] <0.152.0> Mnesia('rabbit@msk-mbus-rmq51'): ** WARNING ** Mnesia is overloaded: {dump_log,write_threshold}. > Mnesia overload - Is not this the result of a error badmatch?\nNo, I don't think so at this time, but I'm also not 100% sure. If the cluster's performance were degrading at the time of the badmatch, there may have been a race condition between compaction and when the code that raised the badmatch executed. It is expecting there to be 0 readers but 1 was still alive, maybe in the process of shutting down. This is just speculation.\n\nwe use one exclusive queue and several consumers for reservation. If one falls, one of the others picks up\n\nThis won't work as described. By definition an exclusive queue can only have one consumer, and when that consumer stops (either gracefully or due to failure) the exclusive queue will be deleted. So, new consumers should re-declare their own exclusive queue and bind it to the exchange to pick up delivery.\nLike @michaelklishin said, without a clear indication of why this error happened we can't file a bug. We should pick up discussion on the mailing list (link). Thanks. Please follow up via this existing thread:\nhttps://groups.google.com/d/topic/rabbitmq-users/eSAn-nOz_bE/discussion. Thank you for using RabbitMQ!\nPlease note that both the version of Erlang and RabbitMQ you are using are out-of-date.\nI can't reproduce using RabbitMQ 3.6.16 and Erlang 20.3.8.6. Please see the attached Vagrantfile for setup.\nVagrantfile.txt\n. FWIW, this is some output after running systemctl stop rabbitmq-server. Note that it stopped gracefully.\n```\nroot@UBUNTU-18:~# systemctl status rabbitmq-server\n\u25cf rabbitmq-server.service - RabbitMQ broker\n   Loaded: loaded (/lib/systemd/system/rabbitmq-server.service; enabled; vendor preset: enabled)\n   Active: inactive (dead) since Sat 2018-09-08 15:25:59 UTC; 11s ago\n  Process: 4437 ExecStop=/bin/sh -c while ps -p $MAINPID >/dev/null 2>&1; do sleep 1; done (code=exited, status=0/SUCCESS)\n  Process: 4289 ExecStop=/usr/lib/rabbitmq/bin/rabbitmqctl stop (code=exited, status=0/SUCCESS)\n  Process: 3636 ExecStart=/usr/lib/rabbitmq/bin/rabbitmq-server (code=exited, status=0/SUCCESS)\n Main PID: 3636 (code=exited, status=0/SUCCESS)\n   Status: \"Initialized\"\nSep 08 15:25:28 UBUNTU-18 rabbitmq-server[3636]:   ######  ##        /var/log/rabbitmq/rabbit@UBUNTU-18-sasl.log\nSep 08 15:25:28 UBUNTU-18 rabbitmq-server[3636]:   ##########\nSep 08 15:25:28 UBUNTU-18 rabbitmq-server[3636]:               Starting broker...\nSep 08 15:25:30 UBUNTU-18 rabbitmq-server[3636]: systemd unit for activation check: \"rabbitmq-server.service\"\nSep 08 15:25:30 UBUNTU-18 systemd[1]: Started RabbitMQ broker.\nSep 08 15:25:30 UBUNTU-18 rabbitmq-server[3636]:  completed with 0 plugins.\nSep 08 15:25:56 UBUNTU-18 systemd[1]: Stopping RabbitMQ broker...\nSep 08 15:25:56 UBUNTU-18 rabbitmqctl[4289]: Stopping and halting node 'rabbit@UBUNTU-18'\nSep 08 15:25:56 UBUNTU-18 rabbitmq-server[3636]: Gracefully halting Erlang VM\nSep 08 15:25:59 UBUNTU-18 systemd[1]: Stopped RabbitMQ broker.\n``. Also, the recommended version of RabbitMQ is3.7.7.3.6.16is the last supported version in the3.6.X` series and will not be supported indefinitely.. I will try to reproduce using Vagrant / VirtualBox.\n@nbu - could you please provide the exact links used for the Erlang and RabbitMQ RPM files? Thanks.. Reproduced on CentOS 6 and 7. This is due to the service not starting on installation which is why this isn't seen in Ubuntu or other Debian-based distros. I'll put a PR together.. Hello, thanks for using RabbitMQ and for including all of that information. I have opened rabbitmq/rabbitmq-management#609 to track this since that's the repository where this code lives.. This will be addressed by rabbitmq/rabbitmq-management#610, thanks.. This may have been addressed by #1691 - @dcorbacho what do you think?\n@lazedo we have 3.7.8 RC builds available, would you mind testing one? You can find the latest build at this link:\nhttps://github.com/rabbitmq/rabbitmq-server/releases/tag/v3.7.8-rc.4. Hello and thanks for using RabbitMQ.\nThe RabbitMQ team doesn't use GitHub issues to answer question but only for tracking work related to features and bugs. Could you please post this to the rabbitmq-users mailing list and we will follow up there?\nWhen you post to the mailing list, please provide all of your data in a text file as an attachment. In addition to what you have provided (which is comprehensive, thank you), please provide the ouput of these commands from each node:\n\nepmd -names\nhostname and hostname -f\ndig $HOSTNAME output for each hostname in this cluster (dig db1.testdomain.com)\n\nSince you are using long names, the host/domain part of the node names you are using for RabbitMQ (db1.testdomain.com for instance) must resolve in DNS to the expected IP address (192.168.190.10)\nThanks,\nLuke. @domhaas - as @michaelklishin explained, it is working by design.. I tried to specify ciphers in advanced.config but everything else in rabbitmq.conf and sure enough the values aren't merged :confounded: \nconf.zip\n. @mohag - is that causing an issue for you? Does that order actually affect how ciphers are chosen in your environment?. Great, that's what I figured is happening. Thanks for the evidence. You'll have to reverse the order of ciphers in rabbitmq.conf for now, or use the old-style Erlang term format file.. @mohag please see https://github.com/rabbitmq/rabbitmq-server/issues/1814 - a fix will ship in 3.7.10. It would be interesting to see if the amount of inter-node traffic is directly related to this.. Thanks @gerhard that's an interesting fact.. Erlang/OTP always binds a UDP socket after opening it (source). When only sending UDP datagrams this step isn't necessary and the call to bind/3 is why the socket accepts data (not strictly \"listening\" since that's a TCP thing).\nOptions -\n\nOpen a PR in https://github.com/schlagert/syslog to open, send, and close a UDP socket for every log message. This might come at an unacceptable performance cost.\nOpen an issue at bugs.erlang.org requesting that client-only UDP be supported.. > testing this i can see that writing syslog data to the udp port erlang is opening forwards it to the local syslog server\n\n@carlhoerberg - could you show how you are testing? I can't seem to reproduce this. I am using the attached RabbitMQ and syslog-ng configurations (config.zip). As you can see, RMQ has udp port 61712 open:\n$ netstat -pan 2>/dev/null | egrep 'udp.*beam.smp'\nudp        0      0 0.0.0.0:61712           0.0.0.0:*                           2786/beam.smp\nAnd I send a syslog messages using these commands:\n```\n$ echo -n 'test message via RabbitMQ port' | nc -4u -w1 localhost 61712\n$ echo -n 'test message via syslog-ng port' | nc -4u -w1 localhost 5140\n```\nThe second message is echoed by syslog-ng:\n[2018-10-02T09:34:27.674754] Incoming log entry; line='test message via syslog-ng port'\n[2018-10-02T09:34:27.674795] Initial message parsing follows;\n[2018-10-02T09:34:27.674813] Setting value; name='PROGRAM', value='test', msg='0x7f9294003820'\n[2018-10-02T09:34:27.674826] Setting value; name='LEGACY_MSGHDR', value='test ', msg='0x7f9294003820'\n[2018-10-02T09:34:27.674843] Setting value; name='MESSAGE', value='message via syslog-ng port', msg='0x7f9294003820'\n[2018-10-02T09:34:27.674865] >>>>>> Source side message processing begin; instance='0.0.0.0', location='/home/lbakken/issues/rabbitmq/rabbitmq-server/gh-1639/syslog-ng.conf:4:5', msg='0x7f9294003820'\n[2018-10-02T09:34:27.674884] Setting value; name='HOST_FROM', value='localhost', msg='0x7f9294003820'\n[2018-10-02T09:34:27.674900] Setting value; name='HOST', value='localhost', msg='0x7f9294003820'\n[2018-10-02T09:34:27.674917] Setting value; name='SOURCE', value='s_local', msg='0x7f9294003820'\n[2018-10-02T09:34:27.675182] <<<<<< Source side message processing finish; instance='0.0.0.0', location='/home/lbakken/issues/rabbitmq/rabbitmq-server/gh-1639/syslog-ng.conf:4:5', msg='0x7f9294003820'\n[2018-10-02T09:34:27.675419] Outgoing message; message='Oct  2 09:34:27 localhost test message via syslog-ng port\\x0a'\n[2018-10-02T09:34:27.675469] Window size adjustment; old_window_size='999', window_size_increment='1', suspended_before_increment='FALSE', last_ack_type_is_suspended='FALSE'\n[2018-10-02T09:34:27.675497] LogSource window is empty;\nOct  2 09:34:27 localhost test message via syslog-ng port\nSo it seems as though data sent to the UDP port that Erlang has open via schlagert/syslog is just dropped.. FWIW, the following configuration will set the udp socket to bind to 127.0.0.1: \nrabbitmq.config.txt\nIf a user doesn't set this pretty obscure setting we could default to 127.0.0.1. Ideally the socket would not be bound to a local address but that requires a change in Erlang/OTP.. Thanks @hairyhum! I will review this today.. I'm having trouble reproducing the issue that @csernazs reports in php-amqplib/php-amqplib#597. However, that user can reproduce this issue and reports that these changes did not fix it. I'm going to continue investigating today.. @csernazs reports that this PR fixes php-amqplib/php-amqplib#597. I'll do a review today but I think we're good. Nice job @hairyhum . @dcorbacho awesome!! \ud83c\udf89 . Hi Valentin,\nThank you for your time and for using RabbitMQ.\nThe RabbitMQ team uses GitHub issues for specific actionable items engineers can work on. This means that GitHub issues are not used for questions, investigations, root cause analysis, discussions of potential issues, and so on.\nPlease post your question or issue to the rabbitmq-users mailing list.\nGetting all the details necessary to reproduce an issue, make a conclusion or even form a hypothesis about what's happening can take a fair amount of time. Our team is multiple orders of magnitude smaller than the RabbitMQ community. Please help others help you by providing a way to reproduce the behavior you're observing, or at least sharing as much relevant information as possible on the list:\n\nServer, client library and plugin (if applicable) versions used.\nServer logs.\nA code example or terminal transcript that can be used to reproduce.\nFull exception stack traces (not a single line message).\nIf applicable, rabbitmqctl status (and, if possible, rabbitmqctl environment output). Redirect output to files and attach the files to your message.\nOther relevant information about the environment and workload, e.g. a traffic capture.\n\nFeel free to edit out hostnames and other potentially sensitive information.\nIf enough details and evidence are provided that demonstrate an issue we'd be happy to file one on GitHub.\nThanks again!\nLuke. @michaelklishin that output is probably due to lager starting at a different time than prior to this change. Let me see if I can figure it out.. @michaelklishin FYI I undid the merge commit you mention in this comment. @michaelklishin ready for round #2. This issue pointed me in the right direction: https://github.com/erlang-lager/lager/issues/481. Previous error text :face_with_head_bandage: \n```\n              Starting broker...\n{\"Kernel pid terminated\",application_controller,\"{application_start_failure,rabbit,{bad_return,{{rabbit,start,[normal,[]]},{'EXIT',{{case_clause,{error,{{shutdown,{failed_to_start_child,{ranch_listener_sup,{acceptor,{0,0,0,0,0,0,0,0},5672}},{shutdown,{failed_to_start_child,ranch_acceptors_sup,{listen_error,{acceptor,{0,0,0,0,0,0,0,0},5672},eaddrinuse}}}}},{child,undefined,'rabbit_tcp_listener_sup_:::5672',{tcp_listener_sup,start_link,[{0,0,0,0,0,0,0,0},5672,ranch_tcp,[inet6,{backlog,128},{nodelay,true},{linger,{true,0}},{exit_on_close,false}],rabbit_connection_sup,[],{rabbit_networking,tcp_listener_started,[amqp,[{backlog,128},{nodelay,true},{linger,{true,0}},{exit_on_close,false}]]},{rabbit_networking,tcp_listener_stopped,[amqp,[{backlog,128},{nodelay,true},{linger,{true,0}},{exit_on_close,false}]]},10,\\\"TCP Listener\\\"]},transient,infinity,supervisor,[tcp_listener_sup]}}}},[{rabbit_networking,start_listener0,5,[{file,\\\"src/rabbit_networking.erl\\\"},{line,230}]},{rabbit_networking,'-start_listener/5-lc$^0/1-0-',5,[{file,\\\"src/rabbit_networking.erl\\\"},{line,221}]},{rabbit_networking,start_listener,5,[{file,\\\"src/rabbit_networking.erl\\\"},{line,222}]},{rabbit_networking,'-boot_tcp/1-lc$^0/1-0-',2,[{file,\\\"src/rabbit_networking.erl\\\"},{line,128}]},{rabbit_networking,boot_tcp,1,[{file,\\\"src/rabbit_networking.erl\\\"},{line,128}]},{rabbit_networking,boot,0,[{file,\\\"src/rabbit_networking.erl\\\"},{line,121}]},{rabbit_boot_steps,'-run_step/2-lc$^1/1-1-',1,[{file,\\\"src/rabbit_boot_steps.erl\\\"},{line,49}]},{rabbit_boot_steps,run_step,2,[{file,\\\"src/rabbit_boot_steps.erl\\\"},{line,52}]}]}}}}}\"}\nKernel pid terminated (application_controller) ({application_start_failure,rabbit,{bad_return,{{rabbit,start,[normal,[]]},{'EXIT',{{case_clause,{error,{{shutdown,{failed_to_start_child,{ranch_listener\nCrash dump is being written to: /tmp/rabbitmq-test-instances/rabbit@shostakovich/log/erl_crash.dump...done\nmake: *** [/home/lbakken/development/rabbitmq/rabbitmq-server/deps/rabbit_common/mk/rabbitmq-run.mk:226: run-broker] Error 1\n```\nWith this patch:\n```\n              Starting broker...\n{\"Kernel pid terminated\",application_controller,\"{application_start_failure,rabbit,{{could_not_start_listener,\\\"::\\\",5672,eaddrinuse},{rabbit,start,[normal,[]]}}}\"}\nKernel pid terminated (application_controller) ({application_start_failure,rabbit,{{could_not_start_listener,\"::\",5672,eaddrinuse},{rabbit,start,[normal,[]]}}})\nCrash dump is being written to: /tmp/rabbitmq-test-instances/rabbit@shostakovich/log/erl_crash.dump...done\nmake: *** [/home/lbakken/development/rabbitmq/rabbitmq-server/deps/rabbit_common/mk/rabbitmq-run.mk:226: run-broker] Error 1\n```\nI'd call that a big improvement :+1: . Thanks for taking the time to report this. Please see rabbitmq/rabbitmq-server-release#87. PR with fix: https://github.com/rabbitmq/rabbitmq-server-release/pull/88. Thanks for using RabbitMQ.\nTo start, this should have been posted as a question to the rabbitmq-users mailing list. The RabbitMQ team doesn't use GitHub for answering questions - only for tracking issues that represent actionable work.\nHaving said that, you should re-read the federation docs. You can't specify a JSON array of upstreams, only one.\nThanks -\nLuke. Hello, thanks for reporting this. Please see https://github.com/rabbitmq/rabbitmq-auth-backend-ldap/issues/97 as that is the repository where this issue will be tracked and fixed.. @Docjones - Your LDAP server is returning a referral to another server for that user (docs). We may be able to take these referrals into account in a future version of RabbitMQ, but for the time being you need to configure your LDAP server to not do this, or ensure the user being authenticated does not return a referral as the result of a simple bind.. I'm pretty sure your RabbitMQ configuration is OK, it's just what the server is returning. I recommend using a tool like ldapsearch (part of OpenLDAP) or ldp.exe (on Windows) to see what happens when you try to do the same search as what is done here:\nsearch request = {'SearchRequest',\"(CN=SW_Entwicklung,OU=Gruppen,OU=Kleinostheim,DC=ids,DC=net)\",baseObject,derefAlways,0,0,false,{equalityMatch,{'AttributeValueAssertion',\"member\",\"CN=Marc Rink,OU=User,OU=Kleinostheim,DC=ids,DC=net\"}},[\"objectClass\"]}\nThe search is, basically, does the member attribute of this group:\n(CN=SW_Entwicklung,OU=Gruppen,OU=Kleinostheim,DC=ids,DC=net)\ncontain the following DN:\nCN=SW_Entwicklung,OU=Gruppen,OU=Kleinostheim,DC=ids,DC=net\nThe result of that search is returning the referral.. Hello, the files whose names end with .example are meant to have every setting commented out. Users should take these files and remove comments where necessary.. Thanks!. For RabbitMQ itself, if NODENAME is unset, then yes, the system's hostname is used from the HOSTNAME variable and is passed via either the -sname or -name argument.\nFor rabbitmqctl and other CLI tools, this has never been necessary until this point. I'll probably implement something like what was shown here.. Decided to resolve this in the CLI tool code itself: https://github.com/rabbitmq/rabbitmq-cli/issues/270. I'll move this to rabbitmq-cli since the presence of RABBITMQ_NODENAME can be used there. Thanks for the reviews!. @peteroneil - you are using an invalid value for ackmode. Allowed values are:\nack_requeue_false\nack_requeue_true\nreject_requeue_false\nreject_requeue_true\nThis is documented in the HTTP API docs: link. Thanks for reporting this. We're aware of this issue and are trying to reach the app's owner. This is not something maintained by the core RabbitMQ team.\nFWIW, the mailing list should be used in the future for something like this. Have a good day.. @abunch this is now resolved.. @RokkerRuslan - you are using an ancient version of RabbitMQ (3.5.7). Please use the latest version.\nIn addition, I can't reproduce this with RabbitMQ 3.7.8 / Erlang 21.1.1 on OS X using your latest script from here. Basically, I see a small delay in the ACK after every publish, as you would expect, considering that publishing takes work.\nport-5672-csv.txt\n. > I can reproduce this with RabbitMQ 3.7.8\nHave you tried on a different system? Are you using a VM? Have you tried running RabbitMQ on one system and your application on another (i.e. not localhost connections)? Have you compared with the behavior of a different application, like PerfTest?. @michaelklishin sounds good. @RokkerRuslan I'll keep an eye on the mailing list.. Hello,\nPlease post this to rabbitmq-users and we can discuss there. If an actual issue is found, we will then file an issue on GitHub.\nPlease provide exact steps to reproduce, including the application code you are using to publish messages.. Hello and thanks for using RabbitMQ.\nPlease remember that RabbitMQ is maintained by a small team of engineers who do not have time to put together code to try and reproduce issues. The issue you describe \"smells\" like something specific to your application since we know that RabbitMQ correctly delivers messages, in publishing order, to consumers.\nIf you would like to continue discussing this issue, please post a message to the rabbitmq-users mailing list with code that we can run. Ideally this code would only use the Java client and no other components. Thank you.. I ran your code snippets past the maintainer of the RabbitMQ Java client (@acogoluegnes), and the snippets appear to be correct.\nOne thing you can do to investigate is to do the following:\n\nRun your test scenario\nWhile it is running, run the following command on your RabbitMQ node:\n\nrabbitmqctl list_consumers\nYou should only see one consumer registered, like this:\nListing consumers on vhost / ...\nqueue_name      channel_pid     consumer_tag    ack_required    prefetch_count  arguments\namq.gen-iB4VTjl9u469x1ggQHLw2Q  <rabbit@shostakovich.2.624.0>   amq.ctag-uFeAs5hOYf55vjv3irE1-Q true    0       []\nIf you see more than one consumer, it mean that the @Bean configuration you have provided is not being applied correctly. This could be an issue in your application (likely) or a bug in Spring Boot (less likely). If you post a complete set of code to rabbitmq-users, I will make sure that a member of the Spring team sees it.. > P.S. list_consumers returns a single consumer, just like your example\nI'm surprised by that result, so thanks for letting us know. I will keep an eye out on the mailing list for your code example.. [sigh] good find. Tested with mandoc version 1.14.4. Ensure that version of mandoc is in your PATH, then run the following to generate the HTML web pages:\nmake web-manpages. Backported to v3.7.x. Testing instructions:\n\nUse the following rabbitmq.conf file:\n\nsysmon_handler.thresholds.busy_processes = 15\nsysmon_handler.thresholds.busy_ports = 1\nsysmon_handler.triggers.process.garbage_collection = 1ms\nsysmon_handler.triggers.process.long_scheduled_execution = 1ms\nsysmon_handler.triggers.process.heap_size = 1KB\nsysmon_handler.triggers.port = on\nsysmon_handler.triggers.distribution_port = on\n\n\nStart up a two node-cluster using the above config and RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS='+zdbbl 1'. Add a policy to mirror all queues to all nodes.\n\n\nStart up PerfTest using a large file as the message body:\n\n\n--rate 5 --exclusive --producers 1 --consumers 1 --body /home/lbakken/issues/misc/random-64MiB.txt\nYou should see busy_dist_port, large_heap and maybe other messages as warning lines in the log file.. > About accept_ack, it's now handshake that performs the equivalent operation in Ranch.\nYep, I saw that in the docs. But, accept_ack is marked as deprecated in ranch ... shouldn't it still work \"the old way\" until it's actually removed? My concern is about deprecated functions actually not doing what they used to in previous versions.. I probably don't understand how accept_ack is supposed to work, but this ranch 1.5.0 code will call Transport:accept_ack, whereas versions 1.6 and later never call Transport:accept_ack, instead calling handshake/1. It was the call to Transport:accept_ack that RabbitMQ depended on to increment the socket stat as well as tune the socket buffers. Were we using accept_ack incorrectly?. Backported to v3.7.x. Thanks for using RabbitMQ.\nWhen a process in the Erlang VM crashes, its state is logged. Controlling access to your log files is important.\nPlease use the mailing list to discuss this, if you'd like to continue.. Hello and thanks for using RabbitMQ.\nUnfortunately, you haven't provided enough information. Please post the following to the rabbitmq-users mailing list:\n\nRabbitMQ, Erlang and operating system version\nList of plugins enabled\nComplete RabbitMQ log file\nDescription of the problem. What does \"fails\" mean? Does the event exchange simply stop publishing messages? Do your consumers report errors?. @dumbbell @michaelklishin - another fix would be to always copy the rabbit.schema file when RabbitMQ starts. The reason that may be a better fix that something in packaging is that we don't know if RABBITMQ_SCHEMA_DIR is set during install / upgrade, correct?. I don't know the history behind making the schema dir configurable :man_shrugging: . Windows users will appreciate this new command since a pid file isn't created.. Thanks!. Hello and thanks for using RabbitMQ.\n\nPlease be aware that the RabbitMQ team does not use GitHub issues for anything but actionable work, which means that questions like these should be addressed to the rabbitmq-users mailing list.\nNote that there is no guarantee that any rabbitmqctl eval command will work between RabbitMQ versions. If you need to declare an exchange from the command line, use the rabbitmqadmin tool or use the HTTP API directly.. Thank you @michaelklishin . Hello and thank you for using RabbitMQ.\nPlease post this to the mailing list and follow the instructions here which show how to provide enough information that people can help you.\nPlease do NOT paste the output of commands like rabbitmqctl status into your mailing list post. Instead, redirect the output to a file and attach the file.\nYou need to let us know exactly how the error message was produced - what command was run, how RabbitMQ was configured, etc.\nThank you.. I made this change in Pika and here's what it looks like:\n\n. Please read what @michaelklishin said here and do not open a new issue on GitHub.\nRead this, then provide the necessary information for us to help in a message posted to the mailing list. Do not paste large amounts of text - please attach configuration and log files to your message:\nhttps://groups.google.com/forum/#!forum/rabbitmq-users. @Avivsalem -\n\nOur method of reproduction is very simple...Set up a 3 node cluster with durable queues.\n\n\nHow many queues?\nAny other queue properties other than \"durable\"?\nWhat policies are in place? Are the queues mirrored?\nIs there any other RabbitMQ configuration in place? What plugins are used?\n\nThese details matter, and the lack of them is why we still have yet to reproduce this issue.. @Avivsalem thanks, that's the sort of detail we need.. @Avivsalem \nUnfortunately, your reproduction steps don't work on my local workstation using RabbitMQ 3.7.11 and Erlang 21.2.5. When RabbitMQ starts and I publish messages to the NULL exchange, everything works fine.\nSee my reproduction steps and scripts I used here:\nhttps://gist.github.com/lukebakken/602dafac58d2d3cc20028199b618af23. @Avivsalem - thanks for reviewing my test. For the mandatory flag, all I had to do was add a callback for Basic.Return here and ensure that the code processes events for a while before exiting, in case a Basic.Return comes in \"late\". Delivery confirmations would also be an option but they don't have an affect on the mandatory flag (comment). I'll keep that in mind, but a basic test to the default exchange with routing key foobarbaz produces the expected result -\n$ python ./test-publish.py \nINFO       2019-02-14 08:01:33,795 __main__                       <module>                             13  : pika version: 0.13.0\nINFO       2019-02-14 08:01:33,797 pika.adapters.base_connection  _create_and_connect_to_socket        237 : Pika version 0.13.0 connecting to ::1:5672\nINFO       2019-02-14 08:01:33,799 pika.adapters.blocking_connection __init__                             1201: Created channel=1\nINFO       2019-02-14 08:01:33,800 __main__                       <module>                             27  : Exiting in 5 seconds...\nERROR      2019-02-14 08:01:33,800 __main__                       on_return_callback                   16  : Basic.Return: (<Basic.Return(['exchange=', 'reply_code=312', 'reply_text=NO_ROUTE', 'routing_key=foobarbaz'])>,)\nINFO       2019-02-14 08:01:33,801 pika.adapters.blocking_connection close                                712 : Closing connection (200): Normal shutdown\nINFO       2019-02-14 08:01:33,801 pika.channel                   close                                529 : Closing channel (200): 'Normal shutdown' on <Channel number=1 OPEN conn=<SelectConnection OPEN socket=('::1', 64730, 0, 0)->('::1', 5672, 0, 0) params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>\nINFO       2019-02-14 08:01:33,801 pika.channel                   _on_closeok                          1090: Received <Channel.CloseOk> on <Channel number=1 CLOSING conn=<SelectConnection OPEN socket=('::1', 64730, 0, 0)->('::1', 5672, 0, 0) params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>>\nINFO       2019-02-14 08:01:33,801 pika.connection                close                                1309: Closing connection (200): Normal shutdown\nINFO       2019-02-14 08:01:33,801 pika.connection                _on_terminate                        2095: Disconnected from RabbitMQ at localhost:5672 (200): Normal shutdown\nINFO       2019-02-14 08:01:33,802 pika.adapters.blocking_connection _flush_output                        480 : Connection closed; result=BlockingConnection__OnClosedArgs(connection=<SelectConnection CLOSED socket=None params=<ConnectionParameters host=localhost port=5672 virtual_host=/ ssl=False>>, reason_code=200, reason_text='Normal shutdown'). Well, the test still passes in that no messages are returned, with or without confirm_delivery (gist is updated).\n\n. Right, it's probably more instantaneous locally. I might try adding small delays between the first stop_app and the other two.. I could see about using something like toxiproxy to introduce some delay in distributed Erlang connections. What's probably happening in your environment is that one node does in fact appear to \"go down\" before the other two (either due to slight delays in ssh or distributed Erlang communication) so some part of partition detection happens.\nWe never clarified if your environment is VM-based or bare metal, and what kind of network connects nodes.. @Gsantomaggio advanced.config is only read if rabbitmq.conf is also present. You can use /etc/rabbitmq/rabbitmq.conf to set those values (schema):\ninet_dist_listen_min = 50000\ninet_dist_listen_max = 50000\nOr, you can create the /etc/rabbitmq/rabbitmq.config file with your original, Erlang-term configuration.. @Gsantomaggio - I can reproduce this, but the fix will have to take rabbitmq.conf into account somehow. Let me see what we can come up with.. OK, since we generate config from rabbitmq.conf after the VM starts, we can't set these kernel values at that time. I'll take that into account too.. @Gsantomaggio I think your change will suffice, but I'm doing some testing and will remove the settings from the rabbit.schema file. I'm also testing net_ticktime since I don't trust that it will work.. You can change net_ticktime after the VM starts, but not the dist ports. We would have to run cuttlefish as part of the startup scripts to support that.\nAlso see https://github.com/rabbitmq/rabbitmq-website/pull/687\nThanks @Gsantomaggio . Backported to v3.7.x. When you click \"New Issue\" in this repository, the following text is shown. Please, rather than ignoring it, read it carefully and follow the instructions given. Thank you.\n\nThank you for using RabbitMQ.\nSTOP NOW AND READ THIS BEFORE OPENING A NEW ISSUE ON GITHUB\nUnless you are CERTAIN you have found a reproducible problem in RabbitMQ or\nhave a specific, actionable suggestion for our team, you must first ask\nyour question or discuss your suspected issue on the mailing list:\nhttps://groups.google.com/forum/#!forum/rabbitmq-users\nTeam RabbitMQ does not use GitHub issues for discussions, investigations, root\ncause analysis and so on.\nPlease take the time to read the CONTRIBUTING.md document for instructions on\nhow to effectively ask a question or report a suspected issue:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/CONTRIBUTING.md#github-issues\nFollowing these rules will save time for both you and RabbitMQ's maintainers.\nThank you.. Backported to v3.7.x. Backported to v3.7.x. When you clicked \"New Issue\", you were presented with the following text. Did you read it? Please post this as a question to the rabbitmq-users mailing list. This is not appropriate for GitHub.\n\nThank you for using RabbitMQ.\nSTOP NOW AND READ THIS BEFORE OPENING A NEW ISSUE ON GITHUB\nUnless you are CERTAIN you have found a reproducible problem in RabbitMQ or\nhave a specific, actionable suggestion for our team, you must first ask\nyour question or discuss your suspected issue on the mailing list:\nhttps://groups.google.com/forum/#!forum/rabbitmq-users\nTeam RabbitMQ does not use GitHub issues for discussions, investigations, root\ncause analysis and so on.\nPlease take the time to read the CONTRIBUTING.md document for instructions on\nhow to effectively ask a question or report a suspected issue:\nhttps://github.com/rabbitmq/rabbitmq-server/blob/master/CONTRIBUTING.md#github-issues\nFollowing these rules will save time for both you and RabbitMQ's maintainers.\nThank you.\n. What do you think of https://github.com/rabbitmq/rabbitmq-server/pull/1912/commits/13f41179bdb3e67fa2491aa45dc1a2de4f74e717 ?. I see what @dumbbell describes. HWM is 50 to start, then running set_log_level debug increases it to 5000. Running set_log_level info sets it back to 50.\nConfig file is as follows:\n```\n$ cat ~/issues/misc/rabbitmq.conf \nLogging\nlog.console = true\nlog.console.level = debug\nlog.file.level = debug\n``. @michaelklishin I'm just now testing withmake run-brokerand I see something similar to what you report. I'm investigating.. False alarm,run-broker` is affected by this change: https://github.com/rabbitmq/rabbitmq-common/commit/c04944235b8ea477735b78d3ed2d7138a93d9343. \u26a0\ufe0f nitpick ahead!\nSince this value is parsed via rabbit_resource_monitor_misc:parse_information_unit, 5000MB will be interpreted as the powers-of-10 value, rather than powers-of-2. The user may not expect this behavior, I'm not sure.\nRelated to rabbitmq/rabbitmq-server#1233 and rabbitmq/rabbitmq-server#1235. Oh I see. What do you think about changing the comment text to 5000MiB or 5GiB so it's clear to the user that those abbreviations are supported?. @dcorbacho thanks for pointing that out. I should have read the entire file rather than just this diff.. Reading from /proc/PID/smaps is an option, I'll code up an example.. https://gist.github.com/lukebakken/03d92dcdaecce8bd48d1639f9545295d\nThe only reason I'm suggesting this is that reading from /proc should be faster than starting up a sub-process.\nOutput:\n```\n$ ./get-rss.escript $$\nRSS: 8588 KiB\n$ ps -h -o rss -p $$\n 8588\n``. OK I didn't know how often this stat is collected \ud83d\ude04 . Should this match the value inPROJECT_ENVin theMakefile(true)?. @dumbbell good find. I'll look into this today.. Prior to these changes, after switching tolazymode viaconvert_to_lazy,target_ram_countwould be set back to the initial value saved inTargetRamCountwhich would beinfinity. This value doesn't really make sense in the case oflazy` queues as we do not want to have any message values in RAM.\nSetting the value to 0 does not seem to have any ill effects, but I will re-re-review that change and take it into consideration as I test some more scenarios today.. Should this be info rather than error?. TRef =/= undefined could be is_reference(TRef) but it doesn't really matter \ud83d\ude04 . stderr instead maybe?. Rather than down should this atom be vhostdown or vhost_down to describe it better? I saw the changes here and didn't know what down meant without coming to this line of code.. The other policy names use dash characters instead of underscores. Should these be drop-head and reject-publish to be consistent? They could still be used as atoms elsewhere if single-quoted.. Just to confirm ... this is for future work and not necessary here?. This is for future work and not necessary in this PR?. Instead of waiting, I think the value here should be true otherwise the skip-case won't be hit on future calls that are waiting for the message. Also see #1393. Got it. 5th time is a charm \ud83d\ude04 . Nitpick: whitespace alignment for opening \" character in multi-line string.. I think this should be\nPassword = <<\"\">>,\notherwise it's not a password-less user.... See this comment. I totally missed that, assignment vs comparison. Fixing it now and figuring out why make bats didn't catch it. Rather than use the process dictionary, could we instead use sys:get_status(self()) to get the current process state and extract client_properties from that?. Thanks for the explanation!. I wanted to do that, but since they both set syslog.dest_host, it doesn't work.. Hm there may be options if the value can be transformed, maybe. Let me check.. What do you think about combining the configuration setup code into its own .bat file to be shared between rabbitmq-server.bat and rabbitmq-service.bat?. Good catch @michaelklishin . It does, I will make a note to do that.. I've removed them when they are not needed. You only need to use curly braces to disambiguate the variable name from another character that is a valid variable character, or when you're using the % or # trick to do string manipulation.\nNeither of these cases need curly braces:\necho \"$RABBITMQ_MNESIA_DIR foobarbaz\"\necho \"$RABBITMQ_MNESIA_DIR-foobarbaz\"\nThis case does because RABBITMQ_MNESIA_DIRfoobarbaz and RABBITMQ_MNESIA_DIR_foobarbaz are valid variable names:\necho \"${RABBITMQ_MNESIA_DIR}foobarbaz\"\necho \"${RABBITMQ_MNESIA_DIR}_foobarbaz\". Yep consistency! / isn't a valid character to use in a variable name so it's all good here.. We can merge this now, and then we'll be working on the same branch (which is A-OK), or I can get all the TODOs done and then merge. Just let me know.. That is surprising. There is a set -e in the _rmq_env_initialize function but only if RABBITMQ_SCRIPTS_DIR is unset. Let me change the code to check the result of rabbitmq_config. Let's wait for future powershell work. Note: printf is POSIX sh compatible. yep! at least one command is consistent. It's 100ms. I committed a comment to that effect but I'm not sure why you didn't see that?. The names / comments could be clearer, for sure.. Rather than do this, check out rabbit_channel_sup:start_link. It differentiates between tcp (i.e. rabbit_reader) and direct which could be passed as an argument to rabbit_channel:start_link.. Those tests clarify things. I still am concerned that at some point the call to rabbit_channel:source/2 will be missed, and the resulting bug hard to diagnose. I'm not sure how likely that scenario is (@michaelklishin ?)\nCould ?MODULE be added as an argument to start_link, or an additional argument like Opts that would specify amqp_params_supported?. I'm fine. @michaelklishin I'm curious why this case ... end existed to clean up the values in the first place? Did this have to do with configuration values?. @michaelklishin if we're going to catch these errors, how about moving try ... catch into rabbit_connection_tracking:register_connection itself?. OK. @michaelklishin I don't think that this line will be reached in a real {error, closed} situation as ranch:handshake should exit here. But, there's no harm in leaving this exit call in.. ",
    "aboroska": "Thanks!\n. ",
    "venikkin": "Hi michaelklishin, \nI can reproduce this issue on RabbitMQ 3.5.6, except step 2) requires 5000+ connections to make RabbitMQ unresponsive. \nCPU usage becomes 100% and memory usage rises almost double after step 4)\nCan we re-open this issue please?\n. ",
    "sazary": "How do you know this is not a bug? Is it a known issue?\nI reviewed CONTRIBUTING.md file before reporting issue; maybe you should mind adding that rabbitmqctl report is required.\n. ",
    "cesarmun": "Is there an approximate timeline to fix this?\n. ",
    "jmoney8080": "Could this make it into the 3.6.0 release? I've hit this quite a bit and its rather annoying to have to ctrl-c and kill the process. \n. Thanks! If you need an external source to test an RC with it I can help. This issue causes a lot of annoyances when our automation runs(not all the time but when it does happen to causes issues it's annoying to fix). \n. ",
    "Subito": "Nice, thanks!\n. ",
    "dmitrymex": "@michaelklishin, thank you for suggestion. Yep, I want that change to be in 3.5.7 and so I have ported that pull request to the stable branch - https://github.com/rabbitmq/rabbitmq-server/pull/480\n. @michaelklishin, a newbie question: will that change be ported to master by someone or how will it get to the next major release?\n. @michaelklishin: got it, thank you\n. @bogdando: +1 from me, thank you. @michaelklishin: could you please merge this?\n. @michaelklishin: thank you\n. @michaelklishin : yep, sounds absolutely reasonable to me\n@dumbbell, thank you for your fix!\n. @bogdando: agree, +1\n. @michaelklishin: yep, the change looks good to me\n. @bogdando: +1, agree with the change\n. @bogdando : +1\n. @bogdando: yep, looks good to me\n. @bogdando : hey, what do you think regarding this PR?\n. Looks good to me, the change only adds more debug output in case list_channels times out, so it should be safe. @bogdando ?\n. @bogdando : hey, what do you think about it?\n. @bogdando +1, looks good to me\n. @bogdando: please take a look\n@michaelklishin: am I right that stable will go into 3.6.1? If yes, then I suggest postpone merging the fix until 3.6.1 is release. The fix is not that important.\n. @bogdando hey, could you please take a look at this?\n. @michaelklishin: oh, good point, thank you! You are right, that should be in stable\n. Created a new pull request for stable here - https://github.com/rabbitmq/rabbitmq-server/pull/647/files\n. @bogdando: please take a look at this\n. I have created a replacement for that PR - #654 \n. @bogdando: hello, please take a look\n. @michaelklishin, no, these changes are independent. I suggest to merge current change right now.\n. @michaelklishin: err, that was unexpected. Do you have an idea what I did wrong? I have just checked my branch https://github.com/dmitrymex/rabbitmq-server/commits/increase-timeout from which I have created current pull request and it does not contain commit from #647 \n@bogdando: regarding #647 , I suggest it to leave merged right now, as I have some confidence in it. Lets decide its further destiny once you finish reviewing / testing it. We will either roll it back or incorporate your suggestions as a separate commit.\n. @michaelklishin @bogdando  I understand now what I did wrong. Before publishing #650 I have force pushed the same commit to dmitrymex:stable. So apparently that changed my #647, and it contains the same commit as #650. I did not expect github to automatically republish my PR on branch change.\nThe outcome is that #647 is not merged, and I will republish it for further review.\n. @bogdando: please take a look\n. @bogdando: please take a look, that is replacement for spoiled #647 \n. @bogdando : right, silly mistake\n. Opened #658  instead\n. please note that this pull request is based on #654, which must be merged first\n. @bogdando : could you please take a look?\n. @michaelklishin : Thanks for letting us know. These are non-critical enhancements, so it is not that important for them to get into 3.6.1\n. Update: that PR is based on #658 , the previous PR - #654 was made to master\n. @bogdando : agree, it makes sense to revert that commit\n. As for me, I am ok with the current approach. I only wish somebody who understands travis configuration to look into the PR as well to verify we do not break it\n. @bogdando: I've left a nit suggestion, but I am not insisting on changing that. Otherwise LGTM\n. @binarin: lgtm, thank you!\n. @bogdando : frankly, I don't like us to perform any action in monitor. But since we don't have better idea how to fix this bug, we will have to go with that workaround. So +1 from me.\n. @bogdando, @michaelklishin: looks good to me, thank you guys\n. @binarin aside from that minor suggestion (ignore it if you don't like it) the change looks good to me. @bogdando ?\n. @binarin now it even better, thank you :-) . Still waiting for input from Bogdan.\n. @michaelklishin: Bogdan is on vacation for now. Can we merge it without his review?\n. @michaelklishin: thank you!\n. @binarin: overall patch looks good to me, it looks like it should remove load done by our current monitoring.\n. @bogdando, @binarin: what do you think about it?\n. @binarin: yep, probably a good idea :-) Fixed that.\n. That is a version of commits which passed through several iterations of review in downstream Fuel in https://review.openstack.org/#/c/324646/ and https://review.openstack.org/#/c/324647/.\n@bogdando, @binarin : guys, what do you think?\n. @bogdando: WDYT ^^?\n. @bogdando: could you please review it?\n. @bogdando: I have performed 10 Jepsen runs as you requested, as well as rebooted the whole cluster a couple times. Each time cluster was restored back successfully, except one time when a tricky partitioning occurred which OCF script failed to fix. The problem is unrelated to the current fix and I will investigate it separately.\n. @bogdando : looks good to me, thank you! \n. Here\ndebian/rabbitmq-server.ocf \nshould be\ndebian/set_rabbitmq_policy.sh\n. Should not we instead do 'apt-get update' before the first install in line 15?\n. It is a general advice I know to do update before any install. The reasoning is nicely outlined in that answer - http://askubuntu.com/a/147029\n. If you change that PR one more time, I suggest to put the exact example name here: set_rabbitmq_policy.sh.example, so that people know what to search\nBTW since you removed .sh suffix from the script, it might be removed from the example as well\n. Seems like you have forgotten '$' sign here\n. I would call it 'ocf-update-private-attr' as it always updates with '-p'\n. Here should be something like (rc_timeouts == 0 AND rc != 0) because if (rc_timeouts == 1 AND rc == 137), that is a timeout which should be ignored\n. Is there a reason to use timeout built-into rabbitmqctl? I think we can use only /usr/bin/timeout as we do right now just for simplicity.\n. And BTW if you agree here, there is no need to check \"$op_rc -ne 75\" in check_timeouts above\n. Here and above you can use type=\"boolean\", like it is done for 'debug' parameter, for example. Though I am not sure if it will affect anything at all.\n. I would prefer to keep mnesia_schema_location lowercase - it is just a local variable and we have them in lowercase everywhere. I'd rather make MNESIA_FILES a local variable later and make it lowercase as well.\nRegarding files like MnesiaCore.rabbit@n2_1473_324370_784463, I don't think we should remove them. These are core dumps from Mnesia and they might help us to perform RCA in case Mnesia fails. In my case just removing schema directory helped to restore RabbitMQ cluster.\n. ",
    "vgkiziakis": "I agree that limiting the number of queues per vhost is a viable solution however it would unfairly penalise vhosts that want to create tons of queues but not fill them up. They would hit the queue limit despite using very little memory or disk.\nAs for the connection limit; It's definitely nice to be able to configure an overall per-vhost connection limit node-wide but it would be more convenient if different vhosts were able to have different limits.\n. @michaelklishin making the numbers of connections and queues per vhost configurable using policies sounds good to me.\nIt does sound like having a vhost-wide byte count is much more intrusive and perhaps not worth the extra complexity. Per queue byte limit + per vhost number of queues limit is sufficient.\n. @michaelklishin not at all, go ahead. Thanks.\n. In this case the publisher would effectively be declaring that they are happy for the entire connection to be blocked if any queue that is bound to any exchange they send a message to hits its length limit.\nIf this is hard/impossible to track perhaps your suggestion for a protocol extension could work. Perhaps this could work similarly to the mandatory or immediate flags. A new type of message flag could indicate that the message should be returned to the sender if it ends up being routed to any queue that has hit its length limit. Is this feasible ?\n. So, if I've understood this correctly, it could work in the following way:\n- Publisher connects to the broker and declares in the \"capabilities\" table that they have the \"queue-blocking\" capability.\n- A queue is declared with the \"blocking\" setting and bound to an exchange.\n- Publisher then uses some variant of basic.publish to send messages to that exchange. The fact that they use this method indicates that they want to be blocked if a \"blocking\" queue attached to that exchange reaches its limit.\n- If the publisher is blocked they receive a connection.blocked event.\nBoth the publisher and the queue should indicate they want to be blocked. I think it's somewhat dangerous to only require this to be a queue setting because it would allow consumers to declare tiny blocking queues and effectively block unsuspecting publishers.\nDo you think this makes sense ?\n. Also very interested in this feature. It's invaluable in scenarios where there is a \"poisonous\" message that for some reason is causing consumers to crash or behave erratically.\nAre there any plans for adding this in a future release ?\n. ",
    "pranjaljain": "Even after putting all these limits, what are the alternate ways in which a single virtual host can take up a lot of memory/disk/CPU?. ",
    "chadrik": "I'd love to see this feature.  @hairyhum did your state machine refactor pave the way for this change?. Between this and #995, I think I currently prefer the latter, because A) the client may want to do something other than block and B) it could be used as the basis for implementing blocking on the client-side.  \nCurious what others think.\n. +1. ",
    "fake-name": "I've been hitting some issues that'd also really benefit from this. . My install is a few years old. It worked up until yesterday.\nThere was no notification about that particular apt repo being out of date, and it still seems to resolve.. > FTR, this repo has nothing to do with packaging. [rabbitmq-server-release](https://github.com/rabbitmq/rabbitmq-server-release/] is where all packaging bits reside.\nWhoops, too many repos. Sorry about that.\nAlso, mailing lists are where discussions go to die. At least use a forum.. ",
    "marksteward": "995 leaves it to the publisher to decide when to retry. If a publisher doesn't know when to throttle messages, can it be expected to be able to do the opposite?\n(To give some background, I imagined this behaviour as alarms but more granular. It would be useful in situations where a badly-behaved app is at risk of filling queues suddenly, and has no other back off mechanism. Not something that should happen often, but ideally wouldn't require additional coding when it does.). ",
    "ben-spiller": "+1 for this. It's a big limitation that when queue limit is reached the only available options involve throwing messages away. Every other JMS provider I've seen either implements publisher blocking or throws an exception from send() to let the publisher know the send didn't succeed - either of those would be fine, but having the send() complete successfully but throw away messages makes it very hard to build a reliable application. It's an even bigger issue with the JMS API as some of the Rabbit-specific confirmation API options aren't available. . @lukebakken\nThanks for the follow-up. We're using RabbitMQ using the JMS API (which isn't mentioned in that sections of the docs). Is there any way for the publisher to get an indication that a JMS send has failed in this case (i.e. an exception from the JMS send or commit method)?. How does this look to a JMS publisher? Would it get an easy-to-handle exception?. ",
    "mikljohansson": "@hairyhum do you know if the delivery-count feature is still slated for inclusion in rabbitmq-server 3.7? \nThe reason I'm asking is because we see the poisonous-message problem every once in a while. Latest incident was last week and caused a system degradation in our own components, before the problem could be found and circumvented. \nWe'd very much like a robust broker-supported way of detecting messages that consistently trigger a severe problem in the client\n. ",
    "plechev": "This is a much needed feature to enhance the robustness of message (re)delivery mechanism. We are dealing with the \"poisonous messages\" problem which is currently handled by the client, but it will be  more elegant if we can use the broker for this. \nThumbs up!\n. ",
    "mattheworiordan": "\nAn update: this won't be in 3.7.0 because we had quite a few out-of-cycle and new features in 3.7.0 and we need to limit the scope at some point. But we are contemplating it for 3.8.0\n\n@michaelklishin out of interest, how do your minor release cycles work i.e. realistically if it made it into 3.8.0, when could we expect that release?\nSeparately, is there anything we could do to help if it was a priority for us?\n. Hi @michaelklishin, do you mind if I ask if any progress has been made on this?  Running RabbitMQ on behalf of customers means RabbitMQ is quite susceptible to overload from one customer (unless we rate limit them which is not ideal) simply because of a faulty code loop.. > It is on the roadmap for 3.8.0.\nGreat to hear, thanks Michael.. > The status is that it has a chance of getting into 3.8.0 but we make no promises.\n\ud83d\ude4f . @avivsalem, I think you were meaning to ask @michaelklishin?  . Thanks @michaelklishin for reviewing and pointing me in the right direction\n. ",
    "rrmayer": "Do we know what the status is on this issue? We continue to see questions about this on Stack Overflow, so it appears that there is at least a reasonable demand for this feature.. ",
    "jlcd": "Also interested. This feature would be a life saver, as we won't have to create workarounds to stop requeueing messages that aren't processable.\nAny updates whether it will be included into 3.8.0?. ",
    "Misiu": "I'm really new to RabbitMQTT but I also think this would be a lifesaver.\nNormally when querying external API and getting an exception I'd like to try to make a request for same data couple of times. I'm aware I have Redelivered flag, but I would like to know how many times the same message was redelivered. This way I could reject it when I hit 5 tries. This could be even done on the server.\nI have one additional question. Would it be possible to delay redelivery?\nI want the same message to be redelivered after a specified time, for example, I want second try after 10 seconds, third after a minute.\nCan this be done without publishing a new message?\nIn C# I can use BasicReject method, but this allows me to flag message to redelivery, but not to delay it.\nCan https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/ be used somehow to accomplish this?\n. ",
    "Soarc": "@Misiu to achieve this, i ended up with using MassTransit  over RabbitMQ. Something like this\n```\n            _bus = Bus.Factory.CreateUsingRabbitMq(sbc =>\n            {\n                var host = sbc.Host(new Uri($@\"rabbitmq://{_host}/\"), h =>\n                {\n                    h.Username(_userName);\n                    h.Password(_password);\n                });\n                sbc.ExchangeType = \"direct\";\n                sbc.AutoDelete = false;\n                sbc.Durable = true;\n                sbc.Exclusive = false;\n            sbc.ConfigurePublish(PublishCallBack);\n            sbc.ReceiveEndpoint(host, _queueName, ep =>\n            {\n                foreach (var suber in _handlerSubscribators)\n                    suber(ep);                    \n            });\n            sbc.UseRetry(retryConfig => retryConfig.Immediate(_retryCount)); // <----- This line achives what you want\n        });\n\n```. ",
    "Avivsalem": "@mattheworiordan,\nDid it make the cut for 3.8.0?. Yup... My mistake. So @michaelklishin what do you say? Did it make it to 3.8.0?\nBecause in my production environment, if a message causes a consumer crash, it will be poison forever... . I'm encountering a similar bug using the last version (3.7.11)\nAfter changing a policy from ha-params=exactly 2 to exactly 1 rapidly, the queue goes into a \"frozen\" state. \nIt shows on the management ui, but with no state. Can't be deleted or used in anyway. \nThe only way to \"unlock\" it is to restart the master node for this queue... \nIs there any fix coming to this? . Thanks for the reply!\nI solved this for now by adding a sleep between policies... \nWaiting for quorum queues... (and generally for 3.8.0, for multiple reasons) . Thank you for response!\n2 things:\n1. Our method of reproduction is very simple... \nSet up a 3 node cluster with durable queues. \nSimultaneously (we were using multi exec in moba xterm) call rabbitmqctl stop_app on all three nodes. \nWhen cluster starts up again, the symptoms appear on some queues. \n\nYou said that in 3.8.0, there will be no more bindings to the default exchange... Will it still be possible to use the default exchange as alternate exchange? (because in my symptoms, when no binding exists to the default exchange, the alternate exchange routing to it stops working...) . @lukebakken, @michaelklishin \ni'm sorry, but i cannot export defenitions and share them... but here is my setup:\nHW: 3 nodes, 8 CPU, 8GB ram, 500GB ssd storage each.\n\nSW: Centos 7 + rabbitmq 3.7.11 + erlang 21.2.3\n\n\nrabbit:\n3 nodes\ncluster_partition_policy: pause_minority\n170 durable queues (no other properties) - not binded to any exchange (but the default...)\nsingle policy on all queues:\n  ha-mode: excatly\n  ha-params: 2\n  ha-sync-mode: manual\n  ha-promote-on-failure: when_synced\none internal durable direct exchange called NULL with alternate-exchange=''\nno bindings from NULL\n\n\nall 170 queues are empty and synced.\nrabbitmqctl stop_app is called simultaniously on all 3 nodes.\nwhen cluster starts up again, some queues (~100) are not bound to the default exchange...\npublish directly on default exchange with routing_key=QUEUE works.\npublish on NULL with routing_key=QUEUE works for queues which are bound to default exchange.\nfor queues which are not bound to default exchange, we get UnroutableError\nhope that's more helpfull...\nif i've missed something you need to know, let me know\n. I'll Check. @hairyhum\nWe have lots of \"queue <> failed to initialise\"\nIn the logs. \nBut not the other stuff you wrote. . @michaelklishin\nThanks! \nI use rpm for centos 7.\nAlthough I doubt that anyone will let me install a non stable version on prod environment \ud83d\ude1d. It would require me to set up a test cluster with the same specs... I'm not optimistic.. \nSend me the version and I'll see what I can do... . I notices that you forgot to call channel.confirm_delivery()\nOn the publishing channel... You have to do it once, after creation. Without it, you won't get unroutable errors... \n. Also, you can call /api/bindings and see if all the queues appear bound to default exchange . And one more thing... My fault...\nI forgot to tell you that the policy also states \nQueue-mode:lazy\nQueue-master-locator:min-masters... \nSorry! . Still. In our code we call confirm_delivery on the channel First... So, if you aim to reproduce, you might wanna add that anyway... \ud83d\ude1d. The only thought that crosses my mind that it might be something to do with the network delays we have, since we run the nodes on different machines...\nWhile your nodes all run on same machine. \nBut other than that, I ran out... \nThanks for all the effort! . No... You got it all wrong...\nWe call the stop_app simultaneously on all three machines (not via script, but via mutli exec on ssh... Actually hitting enter at the same time on all three terminals) \nWhat I meant is, that the nodes negotiation on shutdown is somewhat delayed by the network, maybe causing the race. . 3 VMs. Regular network. Nothing special . ",
    "acogoluegnes": "Initial implementation idea was to add a options field to the resource record and let check_resource_access/3 the same, but it appears the field addition implies huge changes in Mnesia (resource is used in keys).\nSo the plan is to keep resource record the same and add a check_topic_access/4 callback in the rabbit_authz_backend behavior.. I tested by listing cluster nodes in the configuration file (the old way) and it works as before. LGTM.\n. Configuration file with encrypted entries:\n``` erlang\n[\n  {rabbit, [\n      {default_user, <<\"guest\">>},\n      {default_pass, {encrypted,<<\"cPAymwqmMnbPXXRVqVzpxJdrS8mHEKuo2V+3vt1u/fymexD9oztQ2G/oJ4PAaSb2c5N/hRJ2aqP/X0VAfx8xOQ==\">>}},\n      {decoder_config, [\n             {passphrase, <<\"mypassphrase\">>}\n         ]}\n    ]},\n{rabbitmq_shovel,\n    [ {shovels, [ {my_first_shovel,\n                    [ {sources,\n                        [ {brokers, [ {encrypted,<<\"RI2z8WNpHS1PReI8ADwAWAZXWOXlQsAi12+me8Brirhwr7odQdMv9+iFNO421AKGw4XEOo5CL8IhSegXE4cLsrLyedRoS+uzeEylHNKVi2c/OozcCjdXnBZi8DXdCXe4\">>}\n                                    ]} \n                        ]}\n                    , {destinations,\n                        [ {broker, \"amqp://\"}                      \n                        ]}\n                    , {queue, <<\"shovel.from\">>}\n                    , {publish_fields, [ {exchange, <<\"shovel.to\">>}\n                                       , {routing_key, <<\"from_shovel\">>}\n                                       ]}\n                    ]}\n                ]}\n    ]}\n].\n```\nSome usage examples of rabbitmqctl encode:\n$ rabbitmqctl encode --list-ciphers\n[des3_cbc,des_ede3,des3_cbf,des3_cfb,aes_cbc,aes_cbc128,aes_cfb8,aes_cfb128,\n aes_cbc256,aes_ige256,des_cbc,des_cfb,blowfish_cbc,blowfish_cfb64,\n blowfish_ofb64,rc2_cbc]\n$ rabbitmqctl encode --list-hashes\n[sha,sha224,sha256,sha384,sha512,md5]\n$ rabbitmqctl encode '\"amqp://fred:secret@host1.domain/my_vhost\"' mypassphrase\n{encrypted,<<\"RI2z8WNpHS1PReI8ADwAWAZXWOXlQsAi12+me8Brirhwr7odQdMv9+iFNO421AKGw4XEOo5CL8IhSegXE4cLsrLyedRoS+uzeEylHNKVi2c/OozcCjdXnBZi8DXdCXe4\">>}\n$ rabbitmqctl encode --decode '{encrypted,<<\"RI2z8WNpHS1PReI8ADwAWAZXWOXlQsAi12+me8Brirhwr7odQdMv9+iFNO421AKGw4XEOo5CL8IhSegXE4cLsrLyedRoS+uzeEylHNKVi2c/OozcCjdXnBZi8DXdCXe4\">>}' mypassphrase\n\"amqp://fred:secret@host1.domain/my_vhost\"\n$ rabbitmqctl encode --decode '<<\"RI2z8WNpHS1PReI8ADwAWAZXWOXlQsAi12+me8Brirhwr7odQdMv9+iFNO421AKGw4XEOo5CL8IhSegXE4cLsrLyedRoS+uzeEylHNKVi2c/OozcCjdXnBZi8DXdCXe4\">>' mypassphrase\n\"amqp://fred:secret@host1.domain/my_vhost\"\n$ rabbitmqctl encode '<<\"guest\">>' mypassphrase\n{encrypted,<<\"cPAymwqmMnbPXXRVqVzpxJdrS8mHEKuo2V+3vt1u/fymexD9oztQ2G/oJ4PAaSb2c5N/hRJ2aqP/X0VAfx8xOQ==\">>}\n$ scripts/rabbitmqctl encode --decode '{encrypted,<<\"cPAymwqmMnbPXXRVqVzpxJdrS8mHEKuo2V+3vt1u/fymexD9oztQ2G/oJ4PAaSb2c5N/hRJ2aqP/X0VAfx8xOQ==\">>}' mypassphrase\n<<\"guest\">>\n. Closed by https://github.com/rabbitmq/rabbitmq-server/pull/1039.. There are 3 pieces of info in a topic permission (the exchange, the type of the permission [write | read], and the value of the permission). If we map them to a binding:\n * exchange: the exchange targeted by the binding (only if it's a topic-typed exchange)\n * type of permission: write or read? configure would also make sense for this\n * value of permission: regex match against the binding routing key\nIt could also make sense to apply the check to exchange.{bind, unbind}.. Not sure a new configure permission type would make sense, at least the way it's working now with topics in MQTT and STOMP. When a MQTT/STOMP client starts listening on a topic, the topic security check is done upstream in the respective plugin, against read. At the AMQP level, a binding between the topic exchange and an exclusive queue is created. So if there was another security check against configure when the binding is created, the read and configure value (regular expression to check against the binding routing key) would have to be the same, which doesn't make much sense.\nSo the most suited permission type seems to be read.\n. @michaelklishin Yes, it is!. I just noticed the lookup of the exchange breaks an integration test in the Java client. The test tries to unbind non-existing exchanges, so basic.unbind now fails instead of silently doing nothing.\nI think we'd better off checking the exchange exists and skip the topic permission check if it does not, to stick to the previous behavior.. Looks like there's no one-size-fits-all solution, so maybe a new queue argument can provide a hint so the queue uses the appropriate strategy. It means we need to know the load beforehand, but it sounds reasonable for a \"degenerate\" case. WDYT @benmmurphy @michaelklishin?. I created a few tests to experiment and avoid regressions as well. I need to cover other scenarios like re-enqueueing to see if there are assumptions we can rely on to optimize the current algorithm or pick another one.. I changed the internal FIFO queue to a list and used sorting + binary search for single ack. The worse-case scenario is a bit faster, but the common scenario is much slower than before now. This is WIP though, I need to profile more thoroughly.. @benmmurphy I applied only the map for substract_acks part of your patch and it does fix the Bunny check. I haven't looked into the acks collection part yet. I'll do more tests and benchmarks with the map solution to see if we can include it.. I included the map-based ack-ing and some tests (2000 messages to ack, 1000 iterations). Tests can be run with make ct-unit_rabbit_queue_consumers t=performance_tests. Reverse-order ack is much faster, but other cases are slower:\n```\n User 2018-02-20 15:45:53.881 \nQueue single ack fifo 782 ms\n User 2018-02-20 15:45:55.282 \nMap single ack fifo 1401 ms\n User 2018-02-20 15:49:51.164 \nQueue single ack lifo 235882 ms\n User 2018-02-20 15:49:52.597 \nMap single ack lifo 1433 ms\n User 2018-02-20 15:49:53.133 \nQueue multiple ack lifo 536 ms\n User 2018-02-20 15:49:54.412 \nMap multiple ack lifo 1279 ms\n. I tried a multiple ack with \"ack in the middle\", I got:\n User 2018-02-20 16:27:08.161 \nQueue multiple ack in the middle 523 ms\n User 2018-02-20 16:27:09.224 \nMap multiple ack in the middle 1063 ms\nSo not much different than multiple acking at the very end.\n. Not sure we'll come up with a data structure that will fix the worse case scenario without affecting the most common ones. We may try to detect we're hitting the degenerate case by peeking the rear of the queue after a few failed attempts to find the passed-in tag?. Not sure this is doable, as the `kernel` application is started and runs before the `rabbit` application. `rabbit_config` [needs to unload an application to update its configuration](https://github.com/rabbitmq/rabbitmq-server/blob/da988060c5557d453dd3de99a2e2f48f7f31fdfc/src/rabbit_config.erl#L68-L93), it logs this warning:\nUnable to update config for app kernel from *.conf file. App is already running. Use advanced.config instead.\n```\nIf there's no easy workaround, we'd better off keeping kernel settings in the advanced configuration and remove this line from the rabbitmq.conf.example file.. As per discussion with the team, we'll treat kernel.net_ticktime in a special way, avoiding the warning message for this entry in the kernel application configuration.. With the new connection_user_provided_name:\n\n\n. Key is now user_provided_name and only added if necessary:\n\n\n. This has been backported to 3.7.x: b2097218c564e859f7d957a0e869621d16200593.. @hairyhum @michaelklishin Renaming makes sense. I'll resolve the conflicts ASAP.. @hairyhum @michaelklishin OK, renaming and merging done. It's ready for QA. Note there may be some follow-up stories: implement single active consumer in quorum queues, policy support, and maybe changes in the management UI, but I wouldn't mind some feedback about what's been done so far.. Steps to test: https://github.com/acogoluegnes/rabbitmq-proxy-protocol. I reproduced on a single node, with a sequence similar to what @michaelklishin described.\n\nNo deliveries as well, but the list of consumers becomes normal after a while:\n\nThe consumers list from the CLI isn't consistent, but the list of channels is:\n$ escript/rabbitmqctl list_consumers\nListing consumers on vhost / ...\nqueue_name  channel_pid consumer_tag    ack_required    prefetch_count  arguments\nsac.qq.consumers.leak   <rabbit@acogoluegnes-inspiron.2.3627.0> 9   false   0   []\n$ escript/rabbitmqctl list_channels \nListing channels ...\npid user    consumer_count  messages_unacknowledged\n<rabbit@acogoluegnes-inspiron.2.16712.0>    guest   1   0\n<rabbit@acogoluegnes-inspiron.2.16717.0>    guest   1   0\n<rabbit@acogoluegnes-inspiron.2.16722.0>    guest   1   0\nInvestigating.. https://github.com/rabbitmq/rabbitmq-stomp/pull/107. https://github.com/rabbitmq/rabbitmq-mqtt/pull/126. Sorry, I don't get it. Here we get the protocol on the fly from the connection info (e.g. {'MQTT', '3.1.1'} and then build the module name from it (e.g. rabbit_mqtt_connection_info) and check if the module exists.. Any reason for not using the MAX_SUPPORTED_PRIORITY constant here?. Same as previously, any reason for not using the MAX_SUPPORTED_PRIORITY constant here?. Ditto.. I've tried sys:get_status(self()) but as @hairyhum mentioned it hangs :-(. Makes sense. I'll make the change to not report a single active consumer as an exclusive consumer and we may add a new info item for single active consumer in a follow-up story.. This is fixed, single active consumer don't use exclusive consumer info items anymore.. Done in the body of the function, the function clause was getting beefy. . Good idea, done.. Done. This needed to declare a new consumer type, just after the record declaration.. Done.. Don't the effects need to contain {aux, inactive} if no new active consumer could have been set up? (same behavior as the next clause). ",
    "tasmaniski": "Yes, it was clear.. With older version RabbitMQ (3.5.7) works fine (as before release) so I removed \ndeb http://www.rabbitmq.com/debian/ testing main \nin exchange for hard-coded version of .deb package. \n. Yeah, sure.. I solved a problem immediately, before post it here. \nI just wanted to point out for others folks that default \"installation guide\" doesn't works anymore.\nProbably that should be part of some other issue board, sorry on that, my mistake :) \n. Yap sure. My install shell script is:\nInstall Erlang\ncd /tmp/\nwget https://packages.erlang-solutions.com/erlang-solutions_1.0_all.deb\nsudo dpkg -i erlang-solutions_1.0_all.deb\nsudo apt-get update -y\nsudo apt-get install erlang -y\nInstall RabbitMQ from source\ncd /tmp/\necho 'echo \"deb http://www.rabbitmq.com/debian/ testing main\" >> /etc/apt/sources.list' | sudo -s\nwget https://www.rabbitmq.com/rabbitmq-signing-key-public.asc\nsudo apt-key add rabbitmq-signing-key-public.asc\nsudo apt-get update -y\nsudo apt-get install rabbitmq-server -y\n. ",
    "rodvela": "Hello tasmaniski how did you installed an older version of RabbitMQ?\nThanks\n. ",
    "mohammednaseef": "As mentioned in the rabbitmq group,\nCan an auto-forward functionality be added to the federation plugin for messages from downstream to upstream (presently the same happens only for bindings).\nThis would be useful if we want to use federation for logical distribution of nodes. \neg, Exchange EMaster can be federated into ESlave1 and ESlave2. All requests sent to ESlave1 & ESlave2 should be forwarded automatically to EMaster, whereas responses received by EMaster should be sent only to the corresponding slaves based on their subscription.\nIn exchange federation, presently only bindings are forwarded from downstream to upstream. Can the same be done for messages also.\n. ",
    "tenor": "My Erlang is not good enough to submit a PR. :smile:\n. I'll send a PR shortly.\nThe direct_reply_to capability is needed so the client knows if it needs to fall back to the older technique of creating/disposing callback queues.\n. Thanks!\n. I don't know what the numbers are but I think users that are mostly interested in transient messaging will be unfairly penalized by a default lazy queue.\nI'd like to propose a default \"hybrid\" mode:\n1. All queues are created as the current 'default' mode. \n2. When the backlog in a queue hits a certain size limit , then the queue is converted to a 'lazy' one.\nFor non-durable queues this involves paging all current messages (a one time performance hit) and processing all new incoming messages lazily. For durable queues, all current messages will also be paged to disk to ensure non-persistent messages are also saved to disk.\nalternatively\nFor both durable and non-durable queues, all current messages are still kept in the cache, but new messages (and the message that caused the cache limit to be hit) are processed lazily. The cache is never paged to disk since it never gets larger than a few MB.\n3. Once a queue has been converted to a lazy queue, it stays lazy until it is empty, then it is reconverted to a 'default' queue.\nI think this proposal preserves the performance gains RabbitMQ brings to transient messaging while still meeting the goals set for lazy queueing.\n. @michaelklishin I see your point. \nMessages are committed to disk every 100ms or so. There may not be a significant performance drop for transient queues since many messages might go straight to the consumer and escape being committed to disk.\n. ",
    "vikinghawk": "Yep there would have been a topic exchange to header exchange binding on the system. If you can get me a rpm build I can test it. Thanks!\n. Any guesses on if this change could improve the performance of min-masters? We have noticed that with a min-masters policy in place, durable queue declare time grows linearly with the number of durable queues on the cluster. With ~20k durable queues existing, a single queue declare time was 100-200 millis. But with 80k+ durable queues on the cluster it slowed down to 2-6 seconds for a single queue declare. With the nature of what this policy is doing it makes sense that it slows down as more queues are added.\nWhen I get some free time I'll look to recreate my test using 3.7.4 and report back. Was just wondering in the meantime if you guys had any guesses on if this change would help?. Are there any updates on this investigation or is it on hold for other priorities?. ",
    "Michael-Ekkert": "Hi Michael,\nWhy would this be closed when it\u2019s a rendering defect in RabbitMQ 3.6.0 when used with IE 11..\nThe page shows all of the JS as shown below but displays as a blank page..\n[cid:image001.png@01D14264.2093B950]\nFrom: Michael Klishin [mailto:notifications@github.com]\nSent: Tuesday, December 29, 2015 6:05 PM\nTo: rabbitmq/rabbitmq-server\nCc: Michael Ekkert\nSubject: Re: [rabbitmq-server] RabbitMQ 3.6.0 and IE 11 - Login Broken (#522)\nClosed #522https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_rabbitmq_rabbitmq-2Dserver_issues_522&d=AwMCaQ&c=4aA2H3GyBw6it6SRFhMbbIJGKZA51o1h2BweFpsEY5k&r=r4TdAGcGHG4CylHqiq049JZSALSTkxt7aMFgrbgvlmM&m=reDvjIq3dsqn-uhZKGqh-raZD4yX_14erWi4_WN-gnY&s=AwfIawHEdgheZ0cyx8_oRm-388QJ0VEhq10JeY0m_Zw&e=.\n\u2014\nReply to this email directly or view it on GitHubhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_rabbitmq_rabbitmq-2Dserver_issues_522-23event-2D502487226&d=AwMCaQ&c=4aA2H3GyBw6it6SRFhMbbIJGKZA51o1h2BweFpsEY5k&r=r4TdAGcGHG4CylHqiq049JZSALSTkxt7aMFgrbgvlmM&m=reDvjIq3dsqn-uhZKGqh-raZD4yX_14erWi4_WN-gnY&s=q5tlosI1D_i8meD8MAr35ibocv8rZn_J6fqODh90uSs&e=.\n. I included the error that was rendered from main.js.  This issue came up once I upgraded to 3.6.0 and not in the prior release that I upgraded from..  The below is what is returned when requesting the login page:\n\n\nRabbitMQ Management\n\n\n\n\n\n\n\n\n\n\n\n\n\n<link href=\"css/main.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<link href=\"favicon.ico\" rel=\"shortcut icon\" type=\"image/x-icon\"/>\n[if lte IE 8]>\n    <script src=\"js/excanvas.min.js\" type=\"text/javascript\"></script>\n    <link href=\"css/evil.css\" rel=\"stylesheet\" type=\"text/css\"/>\n<![endif]\n\n\n\n\n\n\n\nThe error presented in the console of IE developer tools is:\nSCRIPT1004: Expected ';'\nFile: main.js, Line: 1068, Column: 32\nFrom: Michael Klishin [mailto:notifications@github.com]\nSent: Tuesday, December 29, 2015 6:13 PM\nTo: rabbitmq/rabbitmq-server\nCc: Michael Ekkert\nSubject: Re: [rabbitmq-server] RabbitMQ 3.6.0 and IE 11 - Login Broken (#522)\nThere is no evidence that it is a RabbitMQ issue. We do not use issues for discussions. See what markup is returned and what JS errors there may be. Management is easily the most popular plugin, this issue hasn't been reported to date.\n\u2014\nReply to this email directly or view it on GitHubhttps://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_rabbitmq_rabbitmq-2Dserver_issues_522-23issuecomment-2D167903361&d=AwMCaQ&c=4aA2H3GyBw6it6SRFhMbbIJGKZA51o1h2BweFpsEY5k&r=r4TdAGcGHG4CylHqiq049JZSALSTkxt7aMFgrbgvlmM&m=tslU_IP3db39UuZUjrp3AI0oogzmL6MAyi8zERtQcsc&s=M-chmCBTi4wBiqI0VOxfOhLEWEqnJqbbFAXBCK2oTbo&e=.\n. ",
    "beermattuk": "I'm also having this issue.  What exacerbates the problem is by default RabbitMQ only accepts logins from localhost, so I can't even log into it from another machine with a different browser.  I don't want to have to install a 3rd party browser on the server.  This needs re-opening!\n. Great thanks\n. ",
    "Dixeat": "Hi, we have the same problem in ubuntu on RabbitMQ V3.6.0-1.\nDo you know, how can I fix it ?\n. ",
    "mageddo": "Not solution for it?\n. Is funny because I had a problem like that but the cause was because the queues weren't created\n. ",
    "Caberset": "Just for be sure, take a look to your local network\nip add\nIf there's no \"lo\" network, the you should enable it:\nifconfig lo up\nThen restart the server again and let's see if it works again now\nsystemctl start rabbitmq-server. ",
    "firewave": "I made some screenshots - I hope those help.\nlast 10 minutes:\n\n\nlast hour:\n\n\nlast 8 hours:\n\n\nThe server gets about 150k messages every 3 minutes and processes them and the queue is always empty afterwards. But as you can see in the charts the seek times increase and persist even if there are no messages in the queue at all.\nThe values is back to two-digit milliseconds since I had to restart it because of a configuration change\nTell me if you need more or different data.\n. Since it doesn't seem to impact the performance at all no matter what the value says it appears to me that it might just be a display/calculation error.\n. Another screenshot that shows the increase and how it doesn't seems to affect the performance at all:\n\n. As mentioned in the thread I did specify explicit ciphers in the rabbitmq.conf and they are aavialble in Erlang but are not being picked up by RabbitMQ. According to the SSL article on the RabbitMQ website this should be working so it is indeed a bug.\n. We had to dig into this some more since because of the ROBOT attack the server with this issue have no more ciphers left to use.\nIt turned out, that this was caused by the server certificate lacking the \"Digital Signature\" extension.  Here's the old certificate information:\nX509v3 extensions:\n            X509v3 Basic Constraints: \n                CA:FALSE\n            X509v3 Key Usage: \n                Key Encipherment\n            X509v3 Extended Key Usage: \n                TLS Web Server Authentication\nAdding that made the server return all the ciphers.\nStrangely the same OpenSSL configuration is used for the creation of Apache httpd certificates which is not behaving in this way and returns all the ciphers. So I am not sure who is actually at fault here since I don't have that much knowledge about certificates in general.\nUpdate: The initial issue was confirmed with Erlang 20.2 and RabbitMQ 3.6.15.. ",
    "randylcain": "I'll investigate this more. It is repeatable but could be that the Windows installer is not completing the installation. I have 4 new servers with new 2012 installs that I will use for the testing. I have noticed that after the remove/install steps using the batch file have enabled proper operation, I am no longer seeing the problem, even after uninstalling and reinstalling both RabbitMQ and Erlang.\nErlang 18.2RabbitMQ 3.6Windows 2012 R2\nSent from Yahoo Mail on Android \nOn Tue, Jan 19, 2016 at 10:09 PM, Michael Klishinnotifications@github.com wrote:\nClosed #561.\n\u2014\nReply to this email directly or view it on GitHub.\n. ",
    "AVVS": "Some ugly code, which was put on top of tests, but would be sufficient enough to reproduce it.\nhttps://github.com/makeomatic/ms-amqp-transport/blob/tests/latency/test/amqp-transport.js#L66-L109\nRequires node.js 5.x and npm 3.x to run\n. ",
    "rpadovani": "I used tail to checking all files in /var/log/rabbitmq/\nThe only two with some outputs are rabbit@artemis.log:\n```\n=INFO REPORT==== 20-Jan-2016::23:54:46 ===\nStopping RabbitMQ\n=INFO REPORT==== 20-Jan-2016::23:54:46 ===\nstopped TCP Listener on [::]:5672\n=INFO REPORT==== 20-Jan-2016::23:54:47 ===\nStopped RabbitMQ application\n=INFO REPORT==== 20-Jan-2016::23:54:47 ===\nHalting Erlang VM\n```\nand /var/log/rabbitmq/shutdown_log:\nStopping and halting node rabbit@artemis ...\nMaybe I need to change a conf to have more output?\n. I use the deb http://www.rabbitmq.com/debian/ testing main PPA, and I installed it on wily with sudo apt-get install rabbitmq-server when wily was in development.\nI upgraded to xenial back in November. I do not remember if I had trouble installing it.\n. ",
    "ejogee": "Hi\n@michaelklishin  I dont think this is a duplicate. The problem relates specifically to the current versions of ubuntu.\n@rpadovani How did you manage to get rabbitmq installed on ubuntu 16.04. Installation fails as rabbitMQ attempts to use upstart but Ubuntu has switched to systemd. \nIf you see the error that you you have on stopping rabbitMQ it is the same error i get on installation. RabbitMQ script is attempting to use upstart but upstart is no longer available on ubuntu by default.\nSee error below: \nsystemctl status rabbitmq-server.service\n\u25cf rabbitmq-server.service - LSB: Manages RabbitMQ server\n   Loaded: loaded (/etc/init.d/rabbitmq-server; bad; vendor preset: enabled)\n   Active: failed (Result: exit-code) since Fri 2016-05-20 10:50:10 SAST; 6min ago\n     Docs: man:systemd-sysv-generator(8)\n  Process: 11725 ExecStart=/etc/init.d/rabbitmq-server start (code=exited, status=1/FAILURE)\n    Tasks: 76 (limit: 512)\n   CGroup: /system.slice/rabbitmq-server.service\n           \u251c\u250011944 /bin/sh -e /usr/lib/rabbitmq/bin/rabbitmq-server\n           \u251c\u250012223 /usr/lib/erlang/erts-7.3/bin/beam.smp -W w -A 64 -P 1048576 -K true -B i -- -root /usr/lib/erlang -progname erl --\n           \u251c\u250012364 inet_gethost 4\n           \u2514\u250012365 inet_gethost 4\nMay 20 10:50:08 ebrahim-VirtualBox su[11807]: + ??? root:rabbitmq\nMay 20 10:50:08 ebrahim-VirtualBox su[11807]: pam_unix(su:session): session opened for user rabbitmq by (uid=0)\nMay 20 10:50:08 ebrahim-VirtualBox su[11976]: Successful su for rabbitmq by root\nMay 20 10:50:08 ebrahim-VirtualBox su[11976]: + ??? root:rabbitmq\nMay 20 10:50:08 ebrahim-VirtualBox su[11976]: pam_unix(su:session): session opened for user rabbitmq by (uid=0)\nMay 20 10:50:10 ebrahim-VirtualBox rabbitmq-server[11725]: initctl: Unable to connect to Upstart: Failed to connect to socket /com/ub\nMay 20 10:50:10 ebrahim-VirtualBox systemd[1]: rabbitmq-server.service: Control process exited, code=exited status=1\nMay 20 10:50:10 ebrahim-VirtualBox systemd[1]: Failed to start LSB: Manages RabbitMQ server.\nMay 20 10:50:10 ebrahim-VirtualBox systemd[1]: rabbitmq-server.service: Unit entered failed state.\nMay 20 10:50:10 ebrahim-VirtualBox systemd[1]: rabbitmq-server.service: Failed with result 'exit-code'.\n. @michaelklishin Im installing via the deb package rabbitmq-server_3.6.2-1_all.deb\n. ",
    "luckydogchina": "Can the question  be solved?. ",
    "eurake": "in 3.6.9  have . ",
    "bdshroyer": "Is there a particular form that we need to support for plugin status commands, or are plugins supposed to supply their own commands?\n. On second thought, this belongs in rabbitmq-test.\n. @michaelklishin I can fix the timestamp copy in the README.\n. @michaelklishin Is there any additional functionality that would go into a rabbitmq-diagnostics tool, or is it just the report contents?\n. Looks good to me. Those diagnostic messages are great!\n. Okay. I'll open a new issue then.\n. Tested in this PR.\n. ",
    "panchenko": "Definitely this should check if partitions is empty.\n. ",
    "damoxc": "@michaelklishin It doesn't require any client library modifications, it's only server side in the communication between loadbalancer -> backend, the client -> loadbalancer connection doesn't include any modification in how the connection works. It would prevent a client from directly connecting to the server, although you could delegate how the connection is handled based on the actual source IP of the connection, configure proxy_ips that will then read the proxy header.\n. ",
    "williamsandrew": "Has there been anymore discussion about this feature? I think this would be really useful for those of us running Rabbits behind any kind of proxy.. ",
    "davidhiston": "Re-opening as had not meant to close it.\n. ",
    "benlaplanche": "The problem statement to is, we'd like the ability to do:\n- Rolling upgrade deployments across a cluster\n- with no downtime\n- across both major & minor lines of RabbitMQ\n- client applications to continue operating with no impact (apart from reconnecting to nodes as they are restarted which is application specific behavior)\n  \n  We can currently do this with patch versions - so we'd expect this behaviour to continue.\nIt could be acceptable for certain features to not be available during the upgrade window, such as features impacted by protocol changes, but for the RabbitMQ cluster itself to protect against this. \n. ",
    "rwkarg": "Old issue, but we're currently working to design around the same problem.\nOne part not brought up yet is handling federation during the upgrade. \nEx. BrokerA1 is the old broker that has an upstream to BrokerB\nBrokerA2 is created with a newer version\nEITHER we add an upstream from BrokerA2 to BrokerB and then remove the upstream from BrokerA1 or we do that in the reverse order. This leads to either duplicated or missed messages, respectively.\nWe're currently looking at potential duplicated messages since we are already working in the \"at least once\" delivery mindset just by using RabbitMQ.. ",
    "wyardley": "See also #692. The Puppet module (and presumably modules for other config management tools) may manage multiple versions of RabbitMQ, which I would think might be the reason they're pulling it from the admin interface?\nI wasn't suggesting having it as a separate package, but including the file in the server package would be fantastic, and I imagine would make things easier.\n. Sorry, what I meant is (in terms of it being drastic), is just concern about causing an unintended consequence to the user when we start mucking around with default env in modules. For Puppet, at least, it should be only in the calls related to the module, but still I worried that it could cause unpredictable behavior (not to mention the political issue of what specific locale would be the default).\nEither way, it's good to have confirmation that setting it to en_US.UTF-8 shouldn't cause an issue -- I didn't know whether users could create resources for which this might cause a problem in certain corner cases.. ",
    "wmoran-uatc": "With the current setup, an application with a connection leak can crash RabbitMQ when it runs out of file descriptors. If a per-user connection limit could be enforced, it could be guaranteed that one application with a connection leak won't crash Rabbit (thus causing problems for all applications that use it). This would provide a more stable backend for systems that are developed rapidly enough that bugs occasionally sneak through.. While per-vhost is an excellent feature, it still fails to provide protection in the case where multiple teams are creating software that connects to the same vhost. With per-vhost, a single buggy app could eat up all the connections for a particular vhost, thus negatively impacting other applications that need to use that vhost. With per-user limits, each application could be limited independently of any other.. ",
    "cwhsu1984": "I would love to see rabbitmq to have this feature, I currently plan to let users connect to rabbitmq through Web STOMP. However, I just want to have only one connection per user to rabbitmq to prevent users from draining the resources(cpu, memory) on the rabbitmq node. Although, I might be able to do this by closing user connection when a user tries to login to our server, still, it would be great if rabbitmq support this feature.. ",
    "jesferman": "Is there any planning to include this feature in some RabbitMQ release?. ",
    "barryz": "What's going on?. ",
    "neilneely": "Having just experienced the scenario @wmoran-uatc described above - where a new single buggy app was leaking connections and caused us considerable grief in production -  I'd like to reiterate that a per-user connection limit would be a very helpful mitigation strategy when the whole purpose of a given MQ cluster is supporting a single high volume vhost.  . ",
    "hanej": "I too would like to put my vote in for per-user level limits.  We have several groups that will be connecting to the same vhost and we don't want one group to exhaust all resources on the machine.  Limiting them to a few connections would be the right way to go.. ",
    "sega-yarkin": "@hairyhum db now is equal to thread_no_node_processor_spread that spreads threads over one NUMA node only.\nIf we have system with 2 NUMA nodes (for example 16GB RAM per node) and we use db and we want to use all available memory (32GB) threads will have access to non-local NUMA node (and this access will be slower than access to local NUMA node). But at the same time we'll use only half of available CPU cores.\nSo, if we use only local NUMA memory it'll be good to use only its CPU cores, but if we use all system memory there no reason to not use all CPU cores.\n. I've tested behavior of Erlang with different values of +stbt parameter.\nMy system:\n```\nnumactl -H\navailable: 2 nodes (0-1)\nnode 0 cpus: 0 1 2 3\nnode 0 size: 16383 MB\nnode 0 free: 11142 MB\nnode 1 cpus: 4 5 6 7\nnode 1 size: 16384 MB\nnode 1 free: 10289 MB\nnode distances:\nnode   0   1\n  0:  10  20\n  1:  20  10\nlscpu\nArchitecture:          x86_64\nCPU op-mode(s):        32-bit, 64-bit\nByte Order:            Little Endian\nCPU(s):                8\nOn-line CPU(s) list:   0-7\nThread(s) per core:    1\nCore(s) per socket:    4\nSocket(s):             2\nNUMA node(s):          2\nVendor ID:             GenuineIntel\nCPU family:            6\nModel:                 37\nModel name:            Intel(R) Xeon(R) CPU E5-2630 0 @ 2.30GHz\nStepping:              1\nCPU MHz:               2300.000\nBogoMIPS:              4600.00\nHypervisor vendor:     VMware\nVirtualization type:   full\nL1d cache:             32K\nL1i cache:             32K\nL2 cache:              256K\nL3 cache:              15360K\nNUMA node0 CPU(s):     0-3\nNUMA node1 CPU(s):     4-7\n```\nRMQ works without async I/O threads (RABBITMQ_IO_THREAD_POOL_SIZE=0).\nThe first run was without +stbt parameter and I had:\n```\nps -T -H -o spid --no-headers --ppid cat /var/run/rabbitmq/pid | xargs -n1 taskset -p\npid 3105's current affinity mask: ff\npid 3112's current affinity mask: ff\npid 3113's current affinity mask: ff\npid 3114's current affinity mask: ff\npid 3116's current affinity mask: ff\npid 3117's current affinity mask: ff\npid 3118's current affinity mask: ff\npid 3119's current affinity mask: ff\npid 3120's current affinity mask: ff\npid 3121's current affinity mask: ff\npid 3122's current affinity mask: ff\npid 3123's current affinity mask: ff\npid 3124's current affinity mask: ff\npid 3353's current affinity mask: ff\npid 9488's current affinity mask: ff\n```\nThe second one with +stbt ts\npid 9488's current affinity mask: ff\npid 20035's current affinity mask: ff\npid 20049's current affinity mask: ff\npid 20061's current affinity mask: ff\npid 20063's current affinity mask: ff\npid 20073's current affinity mask: 1\npid 20074's current affinity mask: 2\npid 20075's current affinity mask: 4\npid 20076's current affinity mask: 8\npid 20077's current affinity mask: 10\npid 20078's current affinity mask: 20\npid 20079's current affinity mask: 40\npid 20080's current affinity mask: 80\npid 20081's current affinity mask: ff\npid 20101's current affinity mask: ff\n+stbt db:\npid 9488's current affinity mask: ff\npid 20648's current affinity mask: ff\npid 20658's current affinity mask: ff\npid 20661's current affinity mask: ff\npid 20675's current affinity mask: ff\npid 20677's current affinity mask: 1\npid 20678's current affinity mask: 2\npid 20679's current affinity mask: 4\npid 20680's current affinity mask: 8\npid 20681's current affinity mask: 10\npid 20682's current affinity mask: 20\npid 20683's current affinity mask: 40\npid 20684's current affinity mask: 80\npid 20685's current affinity mask: ff\npid 20950's current affinity mask: ff\n+stbt nnps:\npid 9488's current affinity mask: ff\npid 28959's current affinity mask: ff\npid 28966's current affinity mask: ff\npid 28967's current affinity mask: ff\npid 28968's current affinity mask: ff\npid 28969's current affinity mask: 1\npid 28970's current affinity mask: 2\npid 28971's current affinity mask: 4\npid 28972's current affinity mask: 8\npid 28973's current affinity mask: 10\npid 28974's current affinity mask: 20\npid 28975's current affinity mask: 40\npid 28976's current affinity mask: 80\npid 28977's current affinity mask: ff\npid 29481's current affinity mask: ff\nI dunno why, but behavior is the same)\n. @michaelklishin yes, sure:\n{\n        \"rabbit_version\": \"3.7.0-beta.19\",\n        \"vhosts\":[ { \"name\": \"/\" } ],\n        \"users\": [\n                { \"name\": \"admin_user\", \"password\": \"admin_user\", \"tags\": \"administrator\" },\n                { \"name\": \"mon_user\" , \"password\": \"mon_user\", \"tags\": \"monitoring\" }\n        ],\n        \"permissions\":[\n                { \"user\": \"admin_user\", \"vhost\": \"/\", \"configure\": \".*\", \"write\": \".*\", \"read\": \".*\" },\n                { \"user\": \"mon_user\", \"vhost\": \"/\", \"configure\": \"\"  , \"write\": \"\"  , \"read\": \"\"   }\n        ],\n        \"policies\": [\n                { \"name\": \"ha\", \"vhost\": \"/\", \"pattern\": \".*\", \"apply-to\": \"all\", \"definition\": {\n                        \"ha-mode\": \"all\",\n                        \"ha-sync-mode\": \"automatic\",\n                        \"queue-mode\": \"lazy\"\n                }}\n        ]\n}. According to logs it loads/creates everything well but fails after at the first start.. ",
    "jurajseffer": "Thanks Michael. It's not only a question but a report since the guide is at odds with what you say is expected. If you look at my output above you'll see that I used the -n option but it still failed.\n. ",
    "darshanmehta10": "@michaelklishin thanks for the prompt reply :) \nDoes this apply for policies as well? I tried setting some policies based on queue name pattern but none of the policies gets applied when queue gets created.\n. Thanks for the info. So, to analyze the behavior a bit, I tried with the following examples:\n- headers : {'auto-delete' : false, 'persistent' : true , 'id' : 'unique_id'}\nThis creates a queue with name stomp-subscription-*, it has durable set to true and consumer has 'T_unique_id' tag.\nI am able to connect and stream the messages. On killing the script, messages get queued up on queue, and on re-connection they get drained but none reaches to my client.\n- headers : {'auto-delete' : false, 'persistent' : true , 'id' : 'unique_id', 'ack' : 'client'}\nIn this case, everything is same except that when I reconnect, it gets disconnected in 10 seconds without getting any messages (although there are 1000s of messages queued up). As the script has retry mechanism, it waits for 10 seconds and tries connecting again. This goes on forever and no message is received.\nSo, in NONE of these cases, I am able to get the queued messages back. As per the Stomp documentation, I have tried using 'durable' header instead of 'persistent', but no luck. Also, please note that, this only happens if the number of queued messages is more than 500, in other cases, the messages do come back. \n. @michaelklishin should I open another issue in rabbitmq-web-stomp then?\nthanks\n. ",
    "cbandy": "Shall I open another PR?\n. - http://www.freebsd.org/cgi/man.cgi?query=re_format&sektion=7\n- https://developer.apple.com/library/mac/documentation/Darwin/Reference/ManPages/man7/re_format.7.html\n. ",
    "juliocr-ciandt": "Hi guys, \nWe continue with this problem happening in version 3.6.12,  3 node configured with 2GB, 1800MiB of watermark and 1,500,000 messages in the queue, when a node need to be sync, this one exceeds the WM several times until the node crash the node. Can some one help? thank you\n. ",
    "ttmonica": "@Ayanda-D Thanks for your sharing information! wait for the fix or new updates!\n. ",
    "GauravSharmaDA": "Hi, Any update on this? I am still getting this issue on our rabbitmq. \nIf I do \nRabbitMq-Server.bat start \nThen \nRabbitMQCtl cluster_status  works fine.\nIf I do\nRabbitMq-Server.bat start -detached\nI get\n_Cluster status of node rabbit@DARBTCH01 ...\nError: unable to connect to node rabbit@DARBTCH01: nodedown\nDIAGNOSTICS\nattempted to contact: [rabbit@DARBTCH01]\nrabbit@DARBTCH01:\n  * connected to epmd (port 4369) on DARBTCH01\n  * epmd reports: node 'rabbit' not running at all\n                  no other nodes on DARBTCH01\n  * suggestion: start the node\ncurrent node details:\n- node name: 'rabbitmq-cli-18@DARBTCH01'\n- home dir: C:\\Users\\GauravS_admin.DIRECTAXIS\n- cookie hash: Kec5RbLAXMvcIzxRRzRjpQ==\nCluster status of node rabbit@DARBTCH01 ...\nError: unable to connect to node rabbit@DARBTCH01: nodedown\nDIAGNOSTICS\nattempted to contact: [rabbit@DARBTCH01]\nrabbit@DARBTCH01:\n  * connected to epmd (port 4369) on DARBTCH01\n  * epmd reports: node 'rabbit' not running at all\n                  no other nodes on DARBTCH01\n  * suggestion: start the node_\nPlease help!!. ",
    "camelpunch": "I'm presuming that you'd want this to use on boxes that didn't have too much functionality in their tools for security reasons. Is it worth considering git-style delegation to sub commands to retain the existing interface from rabbitmqctl?\n. We found a small sequence of policies that, when applied, cause inconsistencies between policy and actual state of HA on the queue.\nFor example, consider a cluster of two nodes, A and B, and the following two policies applied in a row:\n1. Replicate queue on nodes A and B:\nerlang\n   [{<<\"ha-mode\">>, <<\"nodes\">>}, {<<\"ha-params\">>, [A, B]}]\n2. Replicate queue on node A only:\nerlang\n   [{<<\"ha-mode\">>, <<\"nodes\">>}, {<<\"ha-params\">>, [A]}]\nA slave should be started on node B, and then stopped when the second policy is applied.\nWhat sometimes happens is that the slave is slow to start up and registers itself after policy 2 has been applied. This means that the slave is registered, the queue is replicated, but this contravenes the last policy applied.\nEvents in shorthand:\n- set policy 1\n- start slave (... takes time...)\n- set policy 2\n- read #amqqueue from Mnesia -> Mnesia record has no slaves\n- add_mirror -> does nothing, queue already on node A\n- drop_mirror -> no slave recorded, no op\n- slave finishes startup -> records itself in rabbit_queue\nIn database: #amqqueue's policy says \"no replication\", but there is a slave\n. We have a proposal for a new algorithm that deals with quick slave addition / removal in a short time window.\nCurrent state, and what we are trying to fix\nWhen we want a slave:\n| State | Current Behaviour | Bug? | Expected Behaviour |\n| --- | --- | --- | --- |\n| Slave running | Do nothing |  |  |\n| Slave pending startup | Start slave (pending slave not registered yet) | YES | Do nothing |\n| Slave pending shutdown | Do nothing (pending slave is still registered) | YES | Wait for exiting slave, start a new slave |\n| No slave | Start slave |  |  |\nWhen we no longer want a slave:\n| State | Current Behaviour | Bug? | Expected Behaviour |\n| --- | --- | --- | --- |\n| Slave running | Remove slave |  |  |\n| Slave pending startup | Do nothing | YES | Stop slave |\n| Slave pending shutdown | Remove slave again (probably harmless) | MAYBE | Do nothing (easier in new world) |\n| No slave | Do nothing |  |  |\nShort-lived scheduler\nWhen a change is required for a particular queue, such as mirroring to a new slave, we intend to start a scheduler process on the master node. This process will be started from the queue master process, and its pid will be recorded in the #amqqueue record in mnesia.\nThe master will continue to compute the new master and list of slaves. But, it will delegate slaves' lifecycle management to the scheduler process (starting, stopping).\nThe state of master and slave processes must be more detailed inside mnesia. We want to record whether a slave is pending startup or shutdown. This way, the scheduler process can be stateless.\nEach scheduler process will live until the whole cluster reaches the target state for the queue. It will monitor slaves pending start/stop, and verify if a policy is fully applied each time a slave is ready or fully exited. If the cluster is fine, the process exits.\nThe master process and the scheduler process are linked together. If the scheduler loses its link to the master, it exits (to avoid having two schedulers running, which would start multiple identical slaves). If the master loses its link to the scheduler, it spawns a new scheduler.\nWhen a master starts and there is already a scheduler process pid in the database, it links it.\nMnesia schema change\nWe need to modify the #amqqueue record, and thus the schema. So, we'd like to take this opportunity to consolidate all the lists of slaves into a single proplist or map. This allows future changes to be made to this area of the schema without requiring a major upgrade (cluster restart / plugin changes).\nBefore:\n``` erlang\namqqueue{\nslaves = [PidA, PidB, PidC, PidD],\n  sync_slaves = [PidC, PidD],\n  slaves_pending_shutdown = [PidD] % only in master\n}\n```\nAfter:\n``` erlang\namqqueue{\ntarget_slaves = [ NodeA, NodeB, NodeC ],\n  slaves = [\n    { PidA, starting },\n    { PidB, out_of_sync },\n    { PidC, synced },\n    { PidD, exiting }\n}\n```\nApproach\n\nDo this in master. Leave pending tests as they are in stable. This avoids too much change on stable.\nAdd functions to read / write slave data. Use them to implement existing behaviour.\nImplement the above\nRe-enable pending tests on master\n. Does this mean we can continue building on Debian?\n. Please feel free to submit a PR for content changes like this. Please base it on the live branch of rabbitmq-website: https://github.com/rabbitmq/rabbitmq-website/tree/live\n. Closing as it's not a server issue.\n. \n",
    "atuljangra": "Yes agreed that this should be done in separate app or plugin.\nThanks for the responses.\n. ",
    "edmorley": "I think it would be good to add a test for this, since this caused breakage for our Heroku app, after updating to 3.6.0  and using our managed rabbitmq instance's \"rotate password\" feature.\nShall I file a new issue for adding the test? \nMany thanks :-)\n. Many thanks :-) \n. Could we revisit that spec design choice?\nIt may also be good to mention that auth failures are expected to result in the socket being closed on http://www.rabbitmq.com/authentication.html - and link to the auth-notification plugin page from there.\n. Ah I see!\nIn many other places \"extensions\" are used as a synonym for \"add-ons\" or \"plugins\", so on reading http://www.rabbitmq.com/auth-notification.html I'd interpreted the parent category name \"Our Extensions\" to mean \"Official plugins\" rather than \"Additional features compared to the AMQP spec\".\n. That's perfect - thank you for your help! :-)\n. ",
    "jcasale": "@Gsantomaggio I saw the same error in the output window while installing RC1 however the window disappears before you can copy any of it. It might be good to fail the installation if any of the steps return invalid exit codes. In this case, the installation reports success even though you can fix it afterwords it is misleading.\n@michaelklishin Once I see a new build, I will give it a spin.\nThanks everyone.\n. I can confirm for the following cases with Erlang 18.2.1 x64 and rabbitmq-server-3.6.0.625 with Windows Server 2008 R2:\n- Installation in the default locations when SystemDrive=C:\u0000 works, the service installs and starts correctly.\n- Installation in an alternate drive and path such as the original user who had the problem at [https://groups.google.com/forum/#!topic/rabbitmq-users/Wa_4TJ8ozxE] works, the service installs and starts correctly.\nThanks.\n. Do you think that location is maybe better defined as ${install_prefix}/var/lib/rabbitmq/schema? Seems its less config oriented but actually state oriented and a byproduct of configuration.\n. Right,\nDoesn't that mean the enabled_plugins file drives the resulting population and therefor it is \"statefull\" beling in the ${install_prefix}/var/lib/rabbitmq/* location?\n. ",
    "mishrsud": "For the benefit of future visitors, I got hit by the same issue while trying to run any script from the sbin directory after installing RabbitMQ 3.6.1. \nThe issue in my case was that my HOMEDRIVE and HOMESHARE environment variables were set to a network drive and Rabbit Server didn't like this. It kept on telling me that it failed to write the Erlang cookie to H: (which was mapped to the network drive and set as my HOMEDRIVE)\nResolution: Set your HOMEDRIVE to a local drive\nset HOMEDRIVE=C:\nset HOMESHARE=C:\\users\nAfter this, run the RabbitMQ installer and things should be fine!\nThis may be helpful to set these options at startup: http://superuser.com/questions/246731/how-do-i-change-homedrive-homepath-and-homeshare-in-windows-xp\n. ",
    "grovesNL": "I had the same issue. I was able to use the fix provided by @mishrsud successfully.\n. ",
    "tr00st": "Alternative solution pulled from various sources for me was to patch the rabbitmq-env to pull the cookie from the network drive. Can't comment on whether this is ideal or works in all scenarios, but it's proven more robust for me, and doesn't require changing the home directory. Patch in gist:\nhttps://gist.github.com/tr00st/c4609ca35121253dd0a6b25921c23417\n. ",
    "haragarden3": "Thanks @biiiipy \nYour description above helped me a lot and I\u2019m almost there. I\u2019m though still having problem with the erlang cookie (.erlang.cookie). \nI created a bat file that adds \u201c-setcookie %COOKIE_VALUE%\u201d to the RABBITMQ_SERVER_ERL_ARGS and  RABBITMQ_CTL_ERL_ARG environment variables and I call that bat file at the end of the rabbitmq-env.bat file. My %COOKIE_VALUE% is earlier sat to my %RABBITMQ_BASE%.erlang.cookie (for now my %RABBITMQ_BASE% is simply C:\\RabbitMQ\\appdata).\nWhen I run the rabbitmq-service.bat to install the service I see that my \u201c-setcookie \u2026..\u201d (added an echo) is there and correct. And then I start the RabbitMQ service. RabbitMQ is working and I can post messages and receive.\nBut\u2026. I get no cookie in my %RABBITMQ_BASE% folder and neither in the, other vice standard, windows folder. The only cookie created is in my %home% directory (in my case redirected by some policy or other to K:). \nI\u2019m also unable to use rabbitmqctl.bat that reports that the node is down and \u201cAuthentication failed (rejected by the remote node), please check the Erlang cookie\u201d. But\u2026. RabbitMQ IS working and I\u2019m able to administrate it through the web interface. I have also tried to copy the cookie from %home% to %RABBITMQ_BASE% (where it should be) and to the Windows directory but the rabbitmqctl message is always the same.\nAm I missing something here\u2026? All hints appreciated! \n. Thanks a lot. You saved my day! In your bat I noticed \u201cset /p COOKIE_VALUE=<\"C:\\dist\\appdata.erlang.cookie\", that is you put the content of the cookie file in your variable :). What I was doing was giving the -setcookie the path to the cookie file and that didn\u2019t work.\nNow I modified according to your example (read the file content into the variable) and now everything works wonderfully :)\nI completely misunderstood how this -setcookie and the whole cookie business worked.\nMany thanks!\n. ",
    "ramonsmits": "When running the installer the program should go to %ProgramFiles% and the database should be created in %ProgramData% that is what the default paths should be on Windows. \n. Ok, for people that land here, the solution is to set the ENVVAR prior to running the installer, then logout, and login again and run the RabbitMQ installer. Now RabbitMQ will not store the data anymore in a user folder when installed as a windows service but in the C:\\ProgramData* folder where it should store its data.\nRABBITMQ_BASE=C:\\ProgramData\\RabbitMQ\n. ",
    "lefremova": "Done: https://github.com/rabbitmq/rabbitmq-server/pull/639.\nShould I close this one?\n. Ok. Thank you!\n. Done.\nhttps://github.com/rabbitmq/rabbitmq-server/pull/794\n. ",
    "dbjsy": "Hi guys,\nI have the same problem here. None of the queued messages are fetched on connection. Has that been reported anywhere? - if yes please, advise.\nIf this is not a bug, what is the solution to this?. ",
    "wong000": "Has this problem been solved?. I have just met a very similar phenomenon.But it can't be reproduced.\nThere are two nodes in the cluster. Node1 dies\uff08unknown reason\uff09\uff0cNode2 provides services.\uff0cBut one consumer can't register with node2. The queue q-reports-plugin can not find this consumer\uff0cso node2 can not push the message to it.\n. this question maybe  probabilistic. now  i am trying to reappear it.\n. ",
    "quxiaojing": "Thanks ! \n. ",
    "dls314": "@Gsantomaggio, @hairyhum do you know if there is/was another issue about having the installer use environment values for RABBITMQ_HOME?\nI'm glad for this discussion regardless, I just spent a bit finding out/confirming that it doesn't :-\\\n. @Gsantomaggio I do mean in Windows. Eventually I installed the service by hand to fix my problems and unchecked those options (services and links in the start menu) from the installer.\nI'll spin up a new issue I think, #443 seems to be more about the plugins and batch files, and they all seem to work correctly.\n. ",
    "divick": "@michaelklishin I see that this issue has been closed but it could be helpful, if you could mention that if runtime is unable to allocate memory then what could be the reason for that. I am running rabbitmq via its official docker image (alpine variant 3.7.7) on aws and even though there is quite some memory available to it, but still the rabbitmq got killed. What could be the reason for this and how to go about figuring it out?\nWould appreciate any help.. ",
    "megwill4268": "Hi @michaelklishin , excited to have the topic-based authorizations implemented. This feature would be extraordinarily useful to my organization (and personally I could stop maintaining an application to hack around this) is this still in sight to implement?  . @michaelklishin yes, I was thanking you for the topic authorization feature. I was asking if you were still looking at implementing vhost level administration of users and permissions. https://github.com/rabbitmq/rabbitmq-management/issues/153 . ",
    "Morthy": "Also occurring for us, with nearly the same setup/config but OS is Debian 7\n. ",
    "epretha": "Hi,\nWe were not able to reproduce the issue with the information provided. It would be really helpful if you could kindly share with us the configuration applied and any script that you might have to reproduce the issue.\nCould you also kindly send us the full log files for all the nodes in the cluster?\nThis will help us to understand the context under which the problem occurred.\nThank you very much in advance.\n. ",
    "bharris47": "@epretha do you guys have any private channels where we could send the logs and other details as opposed to this public Github forum.\n. @michaelklishin did my logs shed any light on this issue? It continues to be a problem for us.\n. @michaelklishin this is only around a day of logs. Is there anything specific we can grep for or filter to get a more concise log file for you?\n. Some further notes as we've been researching this issue.\nWe have the same setup as mentioned initially. A priority queue is in an unsynchronized state with a slave node. I press \"Synchronise\" in an attempt to mirror the queue to the unsynchronized node. Tailing the logs on the master node yields:\n```\n=INFO REPORT==== 27-Apr-2016::01:18:35 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: mirrors ['rabbit@data-rabbitmq-test-test-01-04.***.info'] to sync\n=INFO REPORT==== 27-Apr-2016::01:18:36 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: 36000 messages\n=INFO REPORT==== 27-Apr-2016::01:18:37 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: 70000 messages\n=INFO REPORT==== 27-Apr-2016::01:18:38 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: 102000 messages\n=INFO REPORT==== 27-Apr-2016::01:18:39 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: 122000 messages\n=INFO REPORT==== 27-Apr-2016::01:18:40 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: 151000 messages\n=INFO REPORT==== 27-Apr-2016::01:18:41 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: 189000 messages\n=INFO REPORT==== 27-Apr-2016::01:18:42 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: 214000 messages\n=INFO REPORT==== 27-Apr-2016::01:18:42 ===\nMirrored queue 'rabbit_killer_celery.1' in vhost 'test': Synchronising: complete\n```\nIt seems as if the master node believes that all the messages have been synchronized. \nThe unsychronised node still shows as unsychronised and logs the following:\n=ERROR REPORT==== 27-Apr-2016::01:24:26 ===\n** Generic server <0.19982.0> terminating\n** Last message in was {'$gen_cast',\n                           {gm,{ack,\n                                   [<<9,243,150,47,97,42,180,240,71,211,\n                                      233,164,163,224,247,187>>]}}}\n** When Server state == {state,\n                         {amqqueue,\n                          {resource,<<\"test\">>,queue,\n                           <<\"rabbit_killer_celery.1\">>},\n                          true,false,none,\n                          [{<<\"x-max-priority\">>,signedint,10}],\n...\nThis sample is followed by a large dump of what appears to be message data and the same crash log as posted in my initial message.\n. Thanks for the heads up @Dzol. Unfortunately, the issue persists in RabbitMQ 3.6.2 RC1.\n. ",
    "amulyas": "Milestone is confusing is this fix available in 3.6.3? \n. we are using 3.6.3 and having this issue .. pivotal support saying they can reproduce this issue\n. we are hitting the same issue .. we have around 200 vHost.. 3 vHost have around 3K Qs each but other are around 3 to 10 Qs ... and all of a sudden we started experiencing this issue.. there is no way to fix it for now .. only way is to turn off HA .. if we turn on HA we can not bring the cluster back .. slave even kill the master .. if we turn off HA on Qs we can bring the cluster back up and running .. but as soon as we turn on HA On Qs cluster go down again .. \n. my understanding is its just metadata about the Qs get stored in mnesia DB not the Q or .. so what process actually sync the Qs is it rabbitmq-server .. ?\n. ",
    "WeiBanjo": "We haven't seen this issue on 3.6.5.\n. ",
    "skizunov": "I like option 1 as well. The problem is that the remove ERLANG_HOME happens after the add ERLANG_HOME in the upgrade scenario. This is because the add ERLANG_HOME logic is in \"Function findErlang\" and this is called before the uninstall logic. To properly implement option 1, you will probably need to split \"Function findErlang\" into 2 functions, the second being something like \"Function setErlangHome\" which will be invoked after the uninstall logic.\n. ",
    "FrancescoErmini": "I don't change dash (bin/sh -> dash as default). But I change the first line of \n/etc/init.d/rabbitmq-server from #!/bin/sh to #!/bin/bash as I get the same error.\nI didn't try the unix binary build, but I will. \nAnyway if you hadn't problems, it mean there is an error on my enviroment....so it is not an issue\n. Ok, the distribution it's an Ubuntu server that run on VM. Tomorrow (or tonight) I will create a new, fresh VM and try to install the package again, then I'll update.\n. Sorry for the late reply. I did a fresh installation of ubuntu and then install rabbitmq-server via apt package manager and everything just work fine.\nThanks! it's ok now to close the issue?\n. ",
    "Giri-Chintala": "I know rabbitmqctl forget_clister_node solves the probelm. My question was why not automatically run this command on running nodes when a node tries to join cluster before making cluster.\nIt will help us automate the thinings on AWS deployment.\n. Agreed. But if joining node tells it got reset , there is no harm in ignoring the fact that it was a cluster mememebr.\n. MK, do you want me re-open this issue as you are willing to consider it for future enhancements ? \n. @michaelklishin  and @binarin  Thanks for making rabbitmqctl join_cluster idempotent!! \nit reduces lot of complexity in my anisble scripts. \n@michaelklishin\nI think main issue is not solved yet. A node is still not able to re-join Cluster after it got reset, Cluster thinks this node is already member but off-line.   \n{\"init terminating in do_boot\",{function_clause,[{rabbit_control_misc,print_cmd_result,[join_cluster,already_member],[{file,\"src/rabbit_control_misc.erl\"},{line,92}]},{rabbit_cli,main,3,[{file,\"src/rabbit_cli.erl\"},{line,89}]},{init,start_it,1,[]},{init,start_em,1,[]}]}}\ninit terminating in do_boot ()\n. ",
    "davidvanlaatum": "While stripping is better could also cause issues for things that inspect the queues and then don't find a match. I would personally error at them\n. ",
    "naag": "How about tabs? We've seen some cases where users copy pasted queue names from the management interface into the \"Move messages\" > \"Destination queue\" field and ended with queues with tabs in front of the queue name. If the queue name is longer than the display length of the input field, the user will not see that there's a tab at the beginning.. @michaelklishin Ok, thread started at https://groups.google.com/forum/#!topic/rabbitmq-users/gtXVqQtCbb4.. ",
    "hammett": "Ok, thanks. The way I understood the semantics of durable, auto-delete and exclusive it would make sense for a queue to exist (durable) and store the messages routed to it, but with a guarantee that at most 1 consumer (exclusive) can consume from it (no shared workers) \n. Exclusive consumer is exactly what I was looking for. thanks again!\n. ",
    "shernshiou": "+1\n. ",
    "GMTA": "As @noahhaon stated, Ubuntu 16.04 LTS users will be unable to install packages. AFAIK there is no workaround, apart from downloading and installing the package and all its dependencies by hand.\n. ",
    "gsker": "Why is this closed if the InRelease file digest hash is  still SHA1?\nI still get the warning after all these months.\n```\n    wget -O - http://www.rabbitmq.com/debian/dists/testing/InRelease |& grep Hash\n    Hash: SHA1\nAnd the output from apt update:\n    W: http://www.rabbitmq.com/debian/dists/testing/InRelease: Signature by key\n        0A9AF2115F4687BD29803A206B73A36E6026DFCA uses weak digest algorithm (SHA1)\n```\nThis was never about the Signing key. It was about that Warning.\n. ",
    "Hydrochoerus": "I had an interesting problem with the TLS/SSL options. For some reason it seems that RabbitMQ/Erlang does not verify that the specified cacertfile actually exists. I actually removed my cacert and restarted. The server was listening on the TLS port 5671, but didn't talk back to clients. No log entries were made with the default log levels. I'm running RabbitMQ 3.6.5 with Erlang 19.\n. ",
    "mydiemho": "@Hydrochoerus I had a similar problem with an incorrect certfile path and spent 2 days trying to figure out why it's not working.\n. ",
    "Algent": "I just spent one work day trying to figure out why amqps didn't work. It kept getting stuck trying to establish connection.\nThe keyfile had the wrong permissions (which probably caused some empty string to be used). It was my fault but it took me far too long to find out (testing with openssl s_server was fine since I had permissions).\nThis could have been avoided by a proper error in the logs. Server probably shouldn't even start in that case.. ",
    "evollu": "@michaelklishin I followed steps on https://www.rabbitmq.com/ssl.html but rabbit crashed on init\nHere is the log. It doesn't stat what is wrong with the pem file. Can you help me find the issue?\nI'm using v3.6.9 on ubuntu 16.04 BTW\n```\n=SUPERVISOR REPORT==== 1-Jun-2017::01:40:39 ===\n     Supervisor: {<0.262.0>,tcp_listener_sup}\n     Context:    start_error\n     Reason:     {shutdown,\n                     {failed_to_start_child,ranch_acceptors_sup,\n                         {listen_error,\n                             {acceptor,{0,0,0,0,0,0,0,0},5671},\n                             {options,{certfile,'/tmp/server/cert.pem'}}}}}\n     Offender:   [{pid,undefined},\n                  {id,{ranch_listener_sup,{acceptor,{0,0,0,0,0,0,0,0},5671}}},\n                  {mfargs,\n                      {ranch_listener_sup,start_link,\n                          [{acceptor,{0,0,0,0,0,0,0,0},5671},\n                           1,ranch_ssl,\n                           [{port,5671},\n                            {ip,{0,0,0,0,0,0,0,0}},\n                            {max_connections,infinity},\n                            {ack_timeout,5000},\n                            {connection_type,supervisor},\n                            inet6,\n                            {backlog,128},\n                            {nodelay,true},\n                            {linger,{true,0}},\n                            {exit_on_close,false},\n                            {versions,['tlsv1.2','tlsv1.1',tlsv1]},\n                            {cacertfile,'/tmp/testca/cacert.pem'},\n                            {certfile,'/tmp/server/cert.pem'},\n                            {keyfile,'/tmp/server/key.pem'},\n                            {verify,verify_peer},\n                            {fail_if_no_peer_cert,false}],\n                           rabbit_connection_sup,[]]}},\n                  {restart_type,permanent},\n                  {shutdown,infinity},\n                  {child_type,supervisor}]\n=CRASH REPORT==== 1-Jun-2017::01:40:39 ===\n  crasher:\n    initial call: application_master:init/4\n    pid: <0.144.0>\n    registered_name: []\n    exception exit: {bad_return,\n                     {{rabbit,start,[normal,[]]},\n                      {'EXIT',\n                       {{case_clause,\n                         {error,\n                          {{shutdown,\n                            {failed_to_start_child,\n                             {ranch_listener_sup,\n                              {acceptor,{0,0,0,0,0,0,0,0},5671}},\n                             {shutdown,\n                              {failed_to_start_child,ranch_acceptors_sup,\n                               {listen_error,\n                                {acceptor,{0,0,0,0,0,0,0,0},5671},\n                                {options,\n                                 {certfile,'/tmp/server/cert.pem'}}}}}}},\n                           {child,undefined,\n                            'rabbit_tcp_listener_sup_:::5671',\n                            {tcp_listener_sup,start_link,\n                             [{0,0,0,0,0,0,0,0},\n                              5671,ranch_ssl,\n                              [inet6,\n                               {backlog,128},\n                               {nodelay,true},\n                               {linger,{true,0}},\n                               {exit_on_close,false},\n                               {versions,['tlsv1.2','tlsv1.1',tlsv1]},\n                               {cacertfile,'/tmp/testca/cacert.pem'},\n                               {certfile,'/tmp/server/cert.pem'},\n                               {keyfile,'/tmp/server/key.pem'},\n                               {verify,verify_peer},\n                               {fail_if_no_peer_cert,false}],\n                              rabbit_connection_sup,[],\n                              {rabbit_networking,tcp_listener_started,\n                               ['amqp/ssl',\n                                [{backlog,128},\n                                 {nodelay,true},\n                                 {linger,{true,0}},\n                                 {exit_on_close,false},\n                                 {versions,['tlsv1.2','tlsv1.1',tlsv1]},\n                                 {cacertfile,'/tmp/testca/cacert.pem'},\n                                 {certfile,'/tmp/server/cert.pem'},\n                                 {keyfile,'/tmp/server/key.pem'},\n                                 {verify,verify_peer},\n                                 {fail_if_no_peer_cert,false}]]},\n                              {rabbit_networking,tcp_listener_stopped,\n                               ['amqp/ssl',\n                                [{backlog,128},\n                                 {nodelay,true},\n                                 {linger,{true,0}},\n                                 {exit_on_close,false},\n                                 {versions,['tlsv1.2','tlsv1.1',tlsv1]},\n                                 {cacertfile,'/tmp/testca/cacert.pem'},\n                                 {certfile,'/tmp/server/cert.pem'},\n                                 {keyfile,'/tmp/server/key.pem'},\n                                 {verify,verify_peer},\n                                 {fail_if_no_peer_cert,false}]]},\n                              1,\"SSL Listener\"]},\n                            transient,infinity,supervisor,\n                            [tcp_listener_sup]}}}},\n                        [{rabbit_networking,start_listener0,5,\n                          [{file,\"src/rabbit_networking.erl\"},{line,294}]},\n                         {rabbit_networking,'-start_listener/5-lc$^0/1-0-',5,\n                          [{file,\"src/rabbit_networking.erl\"},{line,282}]},\n                         {rabbit_networking,start_listener,5,\n                          [{file,\"src/rabbit_networking.erl\"},{line,282}]},\n                         {rabbit_networking,'-boot_ssl/1-lc$^0/1-0-',3,\n                          [{file,\"src/rabbit_networking.erl\"},{line,142}]},\n                         {rabbit_networking,boot_ssl,1,\n                          [{file,\"src/rabbit_networking.erl\"},{line,142}]},\n                         {rabbit_networking,boot,0,\n                          [{file,\"src/rabbit_networking.erl\"},{line,128}]},\n                         {rabbit_boot_steps,'-run_step/2-lc$^1/1-1-',1,\n                          [{file,\"src/rabbit_boot_steps.erl\"},{line,49}]},\n                         {rabbit_boot_steps,run_step,2,\n                          [{file,\"src/rabbit_boot_steps.erl\"},{line,49}]}]}}}}\n      in function  application_master:init/4 (application_master.erl, line 134)\n    ancestors: [<0.143.0>]\n    messages: [{'EXIT',<0.145.0>,normal}]\n    links: [<0.143.0>,<0.7.0>]\n    dictionary: []\n    trap_exit: true\n    status: running\n    heap_size: 2586\n    stack_size: 27\n    reductions: 290\n  neighbours:\nand this is my /tmp/server folder\n-rw-rw-r--  1 llu  llu  1070 Jun  1 01:35 cert.pem\n-rw-rw-r--  1 llu  llu     0 Jun  1 01:16 index.txt\n-rw-rw-r--  1 llu  llu  2349 Jun  1 01:36 keycert.p12\n-rw-rw-r--  1 llu  llu  1675 Jun  1 01:34 key.pem\n-rw-rw-r--  1 llu  llu   924 Jun  1 01:34 req.pem\n```. Oh stupid mistake. I'm new to erlang. It is working now!\nThanks for your timely help \ud83d\udc4d . ",
    "palaiya": "@michaelklishin But this seems issue to me in RabbitMQ as suggested here in https://www.rabbitmq.com/dlx.html documentation. It says x-dead-letter-exchange and x-dead-letter-routing-key  args are being used to publish the dead-lettered messages to another queue. And after ttl message should get republished to the original queue. Which is not happening currently. Dead-letter queue silently drops the messages. So, more detail kindly read that stack overflow link.\n. Ok, If, it disallows by the design then how come failed messages get back to the WorkQueue from RetryQueue after ttl. When instead of using the x-dead-letter-exchange and x-dead-letter-routing-key for publishing the messages from WorkQueue to RetryQueue. I explicitly publish the message to the RetryQueue. Is there any difference in both the way messages get published to the RetryQueue\n. ",
    "HW-Siew": "@michaelklishin, I am so sorry. I didnt notice your suggestion regards to post question elsewhere. Thank you very much for your advice and information. I will post question to where your suggested\nApologize for inconveniences caused. \n. ",
    "sylvainhubsch": "I did, but I don't have a summary because there are many cases (clearly more efficient on bindings with long list of headers). Maybe more substantiation would be better, I understand; so I will work on producing some benchmarks results in the meantime.\nThanks you.\n. Sorry I did not have much time these days; will post some stats on monday/tuesday (main problem is with big bindings which take \"long time\" to be retrived from mnesia so that stats seems not relevant).\nThank you.\n. Stats_on_PR.pdf.txt\nHi,\nHere are values from my tests; maybe it's a bit messy.. sorry about that.\nPS : I had to rename file .pdf.txt because github didn't let me name it .pdf\nAt my business Cie, we work a lot with headers exchanges; what I've done\ntoo is a \"deep match\" on headers (not only first level) through table and\narray to catch values of x-death messages to stop the loop of recycling\nmessages definitively not handled by consumers (rerouted via DLX).\nTo achieve this, x-death messages (the 3 types) should be separated (new\nheader not named \"x-*\" ??) and in a mid time I incorporated comparing\nheader values with message when binding key starts with \">= \" or \"< \" etc...\nSo in a word, I prepare another PR that will modify that code (but I don't\nknow exactly when)...and I would not blame you if you refuse my current PR.\nBut I'm pretty sure that comparing relaxed-types is \"overkill\" and that it\nmissed the two lines added in this PR :)\nLong life to the Rabbit and thanks for all your work!\nSylvain.\n2016-06-01 0:49 GMT+02:00 Michael Klishin notifications@github.com:\n\n@sylvainhubsch https://github.com/sylvainhubsch any numbers you can\nshare? It'd help us prioritise this issue. Thank you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/760#issuecomment-222844582,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AC2K_ssh_RF63JEjQn8DwMhaSWXUUABqks5qHLr2gaJpZM4ILB8f\n.\n. Hi Michael,\nI have already wrote that feature some day; so would you consider to assign me this issue ?. @michaelklishin Thank you Michael.\nFrom what I have understood, those new headers are only set the first time, as keys names imply ? If this is the case, that would imply to treat such message the first time it has been dead. Would you consider valuable to track at the first level too, the count for each reasons ? I used to run such Rabbit versions at work (without exchange and queue info), which permits to write something like << if message has been killed 20 times because of rejected, then.... >>; that way, Rabbit has the ability to \"detect\" infinite loop of messages not consumed.\nAnd by the way, from what I see, all headers beginnig with \"x-\" can't be matched at all (historically); for that, what I've done is to create a new operator \"x-?eq K\" where K CAN begin with anything (specially x-). But this is another subject :). PS : I just wanted to share what I did : https://github.com/sylvainhubsch/rabbitmq-server/commit/72d54225c33c58fd422ef04f782338c463a59ab4\n(from my point of view, like discussed is #78, the \"cycle detection problem\" should be only based on loop counters given by death reasons; what could be improved, is to add the LAST (instead of the first/original) queue/exchange ! So that if we want to handle the \"first\" death, it is tantamount to what you propose...)\n(I hope my english is good enough to be understood :/ )\n. The big change is that now a specific table (rabbit_headers_bindings) handles headers' type exchange bindings instead of the generic one (rabbit_route). When bindings are added, they are processed before being written so that we do not more job when routing message... Secondly, when a message goes in exchange, a single read is done to read all bindings related to the current exchange..\nThose 2 facts make perfs better.. Thanks Michael for this first feedback.\nI thought tables' upgrade would be made via add_binding function, but this effectively can't be the case ! I will try to add upgrade funs for that point; may you consider this PR not to be rejected for now ?\nThanks again.\n\n. I've just added function rabbit_exchange_type_headers:upgrade_headers_bindings/0 (which call add_binding/2 itself) but I don't know if I can call it from rabbit_upgrade_functions:headers_bindings/0... Actually I really don't know how to test my code by triggering some 'real' migration processes, that's so bad.. :/\nAgain, thanks for your help and your time.. @michaelklishin : I'm terribly sorry for the delay.. really.\nWe can cancel this merge request, I've worked on something more \"useful\" than just improve headers's exchange; I will give details later, this week I hope (the main idea is the same, but even better I think). I'm about to submit a totally new merge request that will be immediatly ready for QA.\nOnce again, sorry for the delay; I would like to have more time to contribute to RabbitMQ and MOM systems in general..\nThank you.. @michaelklishin I perfectly understand. Thanks for your feedback.. ",
    "dhaval-gusani": "Actually I tried this on 3.5.6 and 3.6.1, and still occurred.\n. ",
    "belt-ascendlearning": "There are reasons I closed the issue. See https://github.com/saltstack/salt/issues/32875 and https://github.com/saltstack-formulas/rabbitmq-formula/issues/33\n. ",
    "palaiyacw": "Could you please answer properly what do you mean by must be upgraded in lock step and link for the official documentation for upgrading the rabbitmq-server cluster\n. ",
    "kjnilsson": "Sorry, wrong branch. resubmitted against stable.\n. @ash-lshift the only way I can reproduce your error is when I manually run rabbitmq-env.bat before rabbitmqctl.bat. Does this sound familiar at all? If so you should not need to run rabbitmq-env.bat on it's own, it is only to be used by other scripts. I tried it with and without your proposed escaping change with the same result. \nIf you open up a fresh instance of the RabbitMQ Command Prompt and only run rabbitmqctl does it work then?\n. @rhmoult your error is slightly different. It complains about a syntax error. Have you edited the config file at all?\n. Yes my erlang install directory has spaces as does my rabbitmq install directory (all the windows defaults normally have spaces) but this dump gives me something to work with. Will investigate further. Thank you @ash-lshift \n. @ash-lshift could you check the state of 8.3 name creation on your d: drive? fsutil 8dot3name query d:\n. Ok I've finally managed to reproduce this. :) I had to disable 8.3 name creation, reboot and remove and reinstall erlang and rabbitmq. \nThis replicates the reported error, however it also appears to affect the running of the rabbitmq broker itself. After install the rabbit application isn't running, and rabbitmq-service.bat start yields the same error. \nOnce I added the extra quotation marks as suggested by @ash-lshift then I was able to start the service and the rabbitmqctl.bat status command completed successfully.\n. Thanks, I should have issued it to stable.\n. Ok I\u2019ve started the service on windows with that setting and verified it using: rabbitmqctl eval 'string:tokens(binary_to_list(erlang:system_info(info)),\"\\n\").'\nIn the output I found this:\n32665\\ndepth: 10\\n=i\",\n \"dex_table:atom_tab\\nsize: 32768\\nlimit: 5000000\\ne\"\nI think I got the escaping of the token delimiter on windows a bit wrong but it does show the atom_tab limit to be of the correct value.\n. I've republished this branch into rabbitmq-server-932 to contain the additional work required so will close this PR. \n. @camelpunch yes\n. Website PR: https://github.com/rabbitmq/rabbitmq-website/pull/251\n. Just to make sure my understanding of the request is correct, the feature requested is to be able to configure a queue with a queue length limit to discard incoming messages (rather than from the head of the queue) and hook into the publisher confirm mechanism to nack  when a routable queue is unable to accept a message due to the queue being \"full\"? Similarly to what may happen if the queue process crashes during processing.\n. Whilst investigating something else I noticed this pattern on  a single rabbit node with 10000 idle queues.\n\nI haven't confirmed that it is caused by background_gc but the interval is in line with the default IDEAL_INTERVAL. Will try to test with the patch applied and background_gc disabled. . I wasn't able to reproduce it exactly as described above however it is clear that the install batch file does not check the ERRORLEVEL of the last erlsrv command and instaed just skip to the :END label that exits with a '0'. I will add a check and we can do some more testing. @michaelklishin should I submit against stable?. When testing I also noted a scenario we won't be able to handle which is this. Here erlsrv does not return a non-zero exit code as this error isn't considered severe enough.. Should validate optionally pass the username as well so that it would be possible to check that a password change for example isn't the same as before and that he usersname isn't present in the password itself?. Configuration could also include a list of regexes that would need to match. (e.g. upper/lower case non-alpha etc) as well as a a switch for disallowing the username appearing in the password.. @michaelklishin perhaps not but it would probably be enough to cover the majority of most user's password strength requirements.. Can regex be used to check if the username is contained in the password? Else we could include a switch for doing that.. I think the default user should adhere to any policy defined so perhaps failing boot is the right option? :/. I have tested this and can reproduce the issue against 3.6.6 but not against 3.6.7 (with the new management plugin). . Thank you, did you happen to inspect the rabbitmq server logs during the time you were running 3.6.7? The server logs are typically what we'd need to be able to investigate this.. An alternative to changing the init sequence is to use a permanent  and configurable pool of vhost supervisors instead. Then almost all code can stay the same apart from some additional logic to choose a suitable pool member for a newly created vhost and such.. Sadly the dynamic nature of RabbitMQ queues mean that many of the abstractions aren't as intuitive as they should be.\nCould we extend the is_duplicate callback with a return value that tells the queue process to reject the message and not just drop it silently? like true | {true, drop} | {true, reject} | false where true is backwards compatible with {true, drop}. For a newly created vhost this processes uses ~100kB before this change and ~16kB after. . I would like to see an integration test testing how this behaves when a consumer on a different node is disconnected but the consumer channel didn't crash and then comes back and wants to consume more.. This change works best with: https://github.com/rabbitmq/ra/pull/30 . @michaelklishin I suspect that you need to delete gen_batch_server and aten from deps as they are hex dependencies of Ra and may not be updated by erlang.mk automatically.. This looks fine to me but would like @hairyhum 's review if possible. . Those tables won\u2019t be cleared up until the wal rolls over. They will get\ncleared up so not a leak per se unless they are still there after a roll\nover. High churn ra clusters isn\u2019t something that\u2019s been optimised for.\nOn Thu, 10 Jan 2019 at 18:26, Michael Klishin notifications@github.com\nwrote:\n\n@michaelklishin requested changes on this pull request.\nI managed to reproduce what looks like a resource leak:\nin a loop, do 1K to 10K times:\n\nDeclare a quorum queue\nPublish 1000 messages that route to it\nDelete the queue\n\nThen navigate to node page and find hundreds of MiBs used by \"other ETS\"\ntables:\n[image: ra ets tables]\nhttps://user-images.githubusercontent.com/1090/50988618-49086580-151e-11e9-86e4-22a732260269.png\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/1823#pullrequestreview-191349548,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABIDlBPn3eEeuLlYnOV-JC9j7IFzVEs-ks5vB4XRgaJpZM4Z5Wbt\n.\n. Yes it still tries all the servers in the cluster and logs if any of them fail which is sort of expected. A bit simplistic perhaps, we could possibly filter out the server that timed out to get rid of that error message.. Actually that won't work at all as it doesn't know which nodes are down so it needs to try hare anyway. I think this succeeded in the end but that the management timeout was too low.. @michaelklishin each call to force delete a server is inside a catch expression, if it errors we log a message to say we couldn't delete that node (as unavailable) which means the ra data dir could not be removed and may need manual clean up. Servers that were removed successfully are not logged (rabbit it the scenario above).\n\nI'm not sure how we can make this a bit more intuitive. Perhaps we shouldn't automatically fall back on force deletion after a consensus server deletion times out and instead provide cli tool to do it or similar?. @michaelklishin I've pushed a commit to lower the timeout to 5s. This seems to work a lot better in my testing and should be plenty of time for a cluster to commit it's delete command in a healthy ra cluster.. rabbit_log already has a log/4 function but with a different signature but I could make it delegate to rabbit_log:log/4 perhaps?. We could do that although I'm not sure what other uses it may have at this point. Thinking about it though should logs coming from ra have their own category / sink or just use the default lager sink?. Looks generally fine but it needs to be tested through rabbit_fifo_props which I am pretty sure will fail as we need to persist the delivery count in the dehydrated state as well message bytes. Perhaps we should add the bytes to the headers map and use this as our per message dehydrated data?. Closing in favour of https://github.com/rabbitmq/rabbitmq-server/pull/1894. comment copy and paste ;)\n. could these defaults use rabbit_pbe:default_cipher rabbit_pbe:default_hash etc instead of a hard value?\n. Formatting is a bit inconsistent. E.g. this case statement is indented differently to others in the PR.\n. when entering an invalid encrypted value it crashes here with this error:\n```\n=INFO REPORT==== 12-Oct-2016::09:43:15 ===\nError description:\n   {badmatch,false}\nLog files (may contain more information):\n   /var/folders/dm/5wv5rvg17m9976tc9m24kpzr0000gq/T/rabbitmq-test-instances/rabbit/log/rabbit.log\n   /var/folders/dm/5wv5rvg17m9976tc9m24kpzr0000gq/T/rabbitmq-test-instances/rabbit/log/rabbit-sasl.log\nStack trace:\n   [{base64,decode_binary,2,[{file,\"base64.erl\"},{line,212}]},\n    {rabbit_pbe,decrypt,5,[{file,\"src/rabbit_pbe.erl\"},{line,72}]},\n    {rabbit_pbe,decrypt_term,5,[{file,\"src/rabbit_pbe.erl\"},{line,49}]},\n    {rabbit,decrypt_app,3,[{file,\"src/rabbit.erl\"},{line,519}]},\n    {rabbit,decrypt_config,2,[{file,\"src/rabbit.erl\"},{line,513}]},\n    {rabbit,start_apps,1,[{file,\"src/rabbit.erl\"},{line,473}]},\n    {rabbit,broker_start,0,[{file,\"src/rabbit.erl\"},{line,282}]},\n    {rabbit,start_it,1,[{file,\"src/rabbit.erl\"},{line,403}]}]\n```\nGiven users may get configuration wrong initially should we consider providing a friendlier error message when decryption fails?\n. I suggest reverting this indentation change to make multi-line lists in line with how they are formatted in the rest of the file.\n. 2016?\n. woudn't returning {error, Message} pretty much do the same thing? Do we need a special error_message?. spelling. In what way would queue stats and credit flow be affected?. @dcorbacho no I was hoping you would remember :) We should try to remove it.. Ok so the DOWN message unblocks the peer which may not be what we want if the peer is currently blocked. Perhaps we shouldn't include the quorum queue pids in the return value here: https://github.com/rabbitmq/rabbitmq-server/blob/b70c966c825d5d7ccffb47b65f19d8e844fda2ef/src/rabbit_amqqueue.erl#L1441\nwdyt @dcorbacho - is there any need to monitor delivering quorum queues?. I'm not sure a monitored delivering qq is handled correctly here: https://github.com/rabbitmq/rabbitmq-server/blob/b70c966c825d5d7ccffb47b65f19d8e844fda2ef/src/rabbit_channel.erl#L1675-L1695. This is where we really need to start working on a good queue type abstraction to interact with rather than switching on the type the QPid.. Can we move this to rabbit_quorum_queue?. I think the question is whether a quorum queue with a single member should be considered mirrored? I'm not sure what effect it would have to return false in this instance? what does it mean to be mirrored? Does it imply fault tolerance somehow? what are it's availability expectations? would a quorum queue with 2 members (NB: can handle no failures) be considered mirrored?. Good point. Again we'd have to defer this to a future queue type abstraction.. We'd have to test this. We had a team discussion about this change where we concluded that a per vhost dlx processes was likely to be fine but that further testing would be needed to confirm.. yes, the queue will notify all interacting processes (channels effectively) with an eol message. eol means all pending publishes should be confirmed so we need to record confirms here. Ok I've done that now. eol should issue confirms. anything else here or can we resolve?. ra_machine_config. maybe we can filter based on the state machine configuration somehow, I guess we'd need to query each one to do so or change how the ra supervision tree works. For now I think this is ok but can we add a comment @dcorbacho ?. Probably but on the other hand this is useful information. What do classic queues do?. match waiting consumers with [] rather than taking the length should be more efficient.. Formatting: line up case with end. Rather than using nth and delete you could simply match the head in the function clause [{_, _} | RemainingWaitingConsumers]. The prior function clause when it matches on the empty list should make this clause impossible.. I'm wondering if a consumer_stategy = default :: default | single_active or similar would provide a bit more future proofing than a bool.. should this be waiting_consumers = [] :: [{consumer_id(), consumer()}] ?. single_active_consumer_on would need to be added to the config() type above.. Can we name this to_map or similar?. Drop head should only be done for non-duplicate deliveries. Also we need some unit tests for the drop head logic. no need to fold here. yep :). ok I've addressed this. I think the whole aux active/inactive could do with a review as currently is seems more like art than science to get that working well with qqs.. ",
    "oliversalzburg": "I assumed it would be an issue with the packaging process, which I assumed to be part of this repo.\nAnyway, I'll post on the mailing list instead.\n. ",
    "NeMO84": "I am still running into this issue. \n. I tried. I deleted the previous key and imported the new key but still same issue.\nbash\n$ sudo apt-key del 056E8E56\n$ sudo wget -O- https://www.rabbitmq.com/rabbitmq-signing-key-public.asc | apt-key add -\n$ sudo apt-key list \n...\npub   1024D/056E8E56 2007-07-06\nuid                  RabbitMQ Release Signing Key <info@rabbitmq.com>\nsub   2048g/81181462 2007-07-06\nShouldn't the date listed be updated?\n. Maybe the key was recently reverted/updated?\nhttp\nHTTP/1.1 200 OK\nDate: Wed, 08 Jun 2016 20:17:44 GMT\nServer: Apache/2.2.22 (Debian)\nLast-Modified: Tue, 07 Jun 2016 09:35:55 GMT\nETag: \"18f65d-6a6-534acea6414c0\"\nAccept-Ranges: bytes\nContent-Length: 1702\nVary: Accept-Encoding\nContent-Type: text/plain\n. DOH! Thanks for the help. Didn't realize the file had been renamed. \nFixed! \n. ",
    "tiandavis": "Got it. Thanks!\n. ",
    "mribichich": "For what it says in the package: Robert Labrie\nChocolatey package\n. That would be great, thanks!\n. great thanks!\nOn Sat, Jun 11, 2016 at 10:04 AM robertlabrie notifications@github.com\nwrote:\n\nThanks for taking it over. You can remove me as a maintainer.\nCheers,\nRob\nOn Sat, Jun 11, 2016 at 4:55 AM, Michael Klishin <notifications@github.com\n\nwrote:\nIt needed a confirmation from me, both accounts should be confirmed now.\nThank you, Robert! We should be able to take it from here.\nOn Sat, Jun 11, 2016 at 3:00 AM, robertlabrie notifications@github.com\nwrote:\n\nHi Michael,\nOk, it's done. It says \"pending approval\", I don't know what that\nmeans,\nbut lets keep an eye on it and see if it resolves itself automatically.\nIf it's still pending on Monday then I'll drop a message on the Google\nGroup and try to find out what the scoop is.\nOn Fri, Jun 10, 2016 at 7:48 PM, Michael Klishin <\nnotifications@github.com\n\nwrote:\n@robertlabrie https://github.com/robertlabrie rabbitmq and\nmklishin-pivotal, thank you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<\n\n\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225321214\n\n\n,\nor mute the thread\n<\n\n\nhttps://github.com/notifications/unsubscribe/AEVU5YrG71ss-3Yogpo6nfx7vQYoFqz7ks5qKfe2gaJpZM4IzLn2\n\n\n.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n<\n\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225322541\n\n,\nor mute the thread\n<\n\nhttps://github.com/notifications/unsubscribe/AAAEQoL-vXGOZXuD--JcB3lUuIfu3EXyks5qKfqAgaJpZM4IzLn2\n\n.\n\n\nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225345471\n,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe/AEVU5Y222bmazvDN1dxZ83JKp4xvesefks5qKngbgaJpZM4IzLn2\n.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225360272,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AFe3SldCmUOG-advZur9bgIJgwPBju9Eks5qKrJhgaJpZM4IzLn2\n.\n. Excellent!\n\nOn Mon, Jun 13, 2016, 07:15 Michael Klishin notifications@github.com\nwrote:\n\nWe will do a Chocolatey release of 3.6.3 once it is ready.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225541978,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AFe3SjBfZ4MhuudYdiyLwGXfHiG0qZE1ks5qLS26gaJpZM4IzLn2\n.\n. \n",
    "robertlabrie": "I'm sorry I ignored everyone! Yes, I'd be happy to transfer. I changed job\nand no longer work on this.\nDoes anyone know how?\nOn Fri, Jun 10, 2016 at 2:09 PM, Matias Ribichich notifications@github.com\nwrote:\n\nThat would be great, thanks!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225254798,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AEVU5Ywi4U2UH_d8P6jYkS8m9YwfbTtwks5qKag_gaJpZM4IzLn2\n.\n. Hi Michael,\n\nI need your username(s), Choclatey doesn't lookup by email.\nAlso, I'm kind of glad to be handing this over to the vendor :)\nLet me know your usernames and I'll take care of it.\nOn Fri, Jun 10, 2016 at 5:24 PM, Michael Klishin notifications@github.com\nwrote:\n\n@robertlabrie https://github.com/robertlabrie OK, I've signed up with\nboth of the emails above. Please add us as maintainers or (if possible)\nco-owners of the package and we'll figure out the rest. Thank you!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225299473,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AEVU5aSeAZe4xNkuE5i5-tXNKu4znj2bks5qKdYagaJpZM4IzLn2\n.\n. Hi Michael,\n\nOk, it's done. It says \"pending approval\", I don't know what that means,\nbut lets keep an eye on it and see if it resolves itself automatically.\nIf it's still pending on Monday then I'll drop a message on the Google\nGroup and try to find out what the scoop is.\nOn Fri, Jun 10, 2016 at 7:48 PM, Michael Klishin notifications@github.com\nwrote:\n\n@robertlabrie https://github.com/robertlabrie rabbitmq and\nmklishin-pivotal, thank you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225321214,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AEVU5YrG71ss-3Yogpo6nfx7vQYoFqz7ks5qKfe2gaJpZM4IzLn2\n.\n. Thanks for taking it over. You can remove me as a maintainer.\n\nCheers,\nRob\nOn Sat, Jun 11, 2016 at 4:55 AM, Michael Klishin notifications@github.com\nwrote:\n\nIt needed a confirmation from me, both accounts should be confirmed now.\nThank you, Robert! We should be able to take it from here.\nOn Sat, Jun 11, 2016 at 3:00 AM, robertlabrie notifications@github.com\nwrote:\n\nHi Michael,\nOk, it's done. It says \"pending approval\", I don't know what that means,\nbut lets keep an eye on it and see if it resolves itself automatically.\nIf it's still pending on Monday then I'll drop a message on the Google\nGroup and try to find out what the scoop is.\nOn Fri, Jun 10, 2016 at 7:48 PM, Michael Klishin <\nnotifications@github.com\n\nwrote:\n@robertlabrie https://github.com/robertlabrie rabbitmq and\nmklishin-pivotal, thank you.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<\n\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225321214\n\n,\nor mute the thread\n<\n\nhttps://github.com/notifications/unsubscribe/AEVU5YrG71ss-3Yogpo6nfx7vQYoFqz7ks5qKfe2gaJpZM4IzLn2\n\n.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n<\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225322541\n,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe/AAAEQoL-vXGOZXuD--JcB3lUuIfu3EXyks5qKfqAgaJpZM4IzLn2\n.\n\n\nMK\nStaff Software Engineer, Pivotal/RabbitMQ\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/840#issuecomment-225345471,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/AEVU5Y222bmazvDN1dxZ83JKp4xvesefks5qKngbgaJpZM4IzLn2\n.\n. \n",
    "nathanejohnson": "I am building from source to contribute a package to the alpine linux aports repository.  The rabbitmq-components.mk is out of date in 100% of the source code tarballs I've downloaded from the Releases section of your github repository to date, which includes 3.6.2, 3.6.3 milestone 1, and 3.7.0 milestone 4.  Perhaps that could be an actionable item?  Also, I am not particularly interested in joining another mailing list, so I'll probably just let this fester and I'll halt my efforts.\n. Your tip about rabbitmq-common was enough to get me in the right direction, I noticed from the source rpm bundle you're using the .xz file versus the .tar.gz, which the former looks to include a rabbitmq-common inline that matches the rabbitmq-components.mk.  Using the .xz seems to have made the build happy.  Thanks for the help!  And sorry for using an inappropriate channel, but I am already subscribed to (and ignoring) too many mailing lists to keep up with at the moment, and so far IRC doesn't seem to be terribly active.\n. ",
    "cenekzach": "The messages are published as persistent, so they should be already stored in the disk. All the other queues switch to 'lazy' mode pretty fast (seconds top), the 'bad' ones does not seem to finish ever (I think I tried half an hour).\nI tried it again today, but was only able to reproduce the behavior with publisher still running. Find the attached archive containing logs, configuration, rabbitmqctl status,report output (report hanged so it is incomplete), /api/queues/ output  and 10s strace output of the 'beam' process. I tried to simplify the configuration so I removed the node from the cluster and turned off all monitoring agents we have in place.\nreport.tar.gz\nIt has been now 20 minutes since I applied the policy. 14/20 are in the 'bad' state, the server is using 800% CPU (all the CPUs available).\n. ",
    "rhmoult": "I installed RabbitMQ 3.6.3 in the default location and did the following on a 32 bit Win7 OS:\nStart an admin command prompt\ncd into the sbin folder under RabbitMQ server installation directory (e.g. C:\\Program Files\\RabbitMQ Server\\rabbitmq_server-3.6.3\\sbin)\nRun rabbitmq-service.bat remove\nI got the following results:\n\nI also have some folders with unusual names pop up:\n\n. Didn't modify anything, but I was able to fix this by uninstalling rabbit and erlang, reinstalling into c:\\programdata (no spaces) and then setting ERLANG_HOME to c:\\programdata\\erl8.0.  Everything seems to be hunky dory now!\n. ",
    "jj1bdx": "If you have compatibility issues, you can use exsplus116 here: https://github.com/jj1bdx/exsplus116\n. @michaelklishin Same algorithm, does not use anything specific for 18.0 or later.\n. ",
    "hujog": "OK. Did so.\nBut the point is: The nodename is static. The nodename tried on startup of the service is dynamically created.\nRabbitMQ just dies if the statically configured nodename (rabbitmq@hostnameOnInstallation) does not match the dynamically determined hostname (rabbitmq@currentHostName) on service startup.\nI really think it is a bug and not a usability issue to use a static nodename in one part of the solution and a dynamically created one in another part.\nIf the solution is to use current nodename at both places or static nodenames at both places: I don't know what would be better but it needs to be consistent.\n. ",
    "marcincinik": "I understand your position \nConsider it though. It's a bit un-logical that you can ensure message to be dead-lettered in every case, except for this particular one. This would really improve functionality of the product.\nThere is really no better way to address our scenario than this.\n... and thanks for so quick response - wow!! :) \n. \" can result in many millions of messages being re-published \"\nThis may also happen in case of queues with TTL .\n\"only supposed to be used when you can afford to lose all messages\".\nWe're speaking about extension here - I'm aware of that. I reckon this is a gap in the spec.\n. ",
    "kae": "Same issue here.\n```\n=ERROR REPORT==== 7-Jul-2016::16:25:03 ===\nJSON encode error: {bad_term,#{error_logger => true,kill => true,size => 0}}\nWhile encoding:\n[[{memory,14128},\n  {reductions,3628},\n  {reductions_details,[{rate,0.0}]},\n  {messages,0},\n  {messages_details,[{rate,0.0}]},\n  {messages_ready,0},\n  {messages_ready_details,[{rate,0.0}]},\n  {messages_unacknowledged,0},\n  {messages_unacknowledged_details,[{rate,0.0}]},\n  {idle_since,<<\"2016-07-07 13:22:09\">>},\n  {consumer_utilisation,''},\n  {policy,<<\"ha-all\">>},\n  {exclusive_consumer_tag,''},\n  {consumers,1},\n  {slave_nodes,[]},\n  {synchronised_slave_nodes,[]},\n  {recoverable_slaves,[]},\n  {state,running},\n  {reductions,3628},\n  {garbage_collection,[{max_heap_size,#{error_logger => true,kill => true,size => 0}},\n                       {min_bin_vheap_size,46422},\n                       {min_heap_size,233},\n                       {fullsweep_after,65535},\n                       {minor_gcs,1}]},\n  {messages_ram,0},\n  {messages_ready_ram,0},\n  {messages_unacknowledged_ram,0},\n  {messages_persistent,0},\n  {message_bytes,0},\n  {message_bytes_ready,0},\n  {message_bytes_unacknowledged,0},\n  {message_bytes_ram,0},\n  {message_bytes_persistent,0},\n  {head_message_timestamp,''},\n  {disk_reads,0},\n  {disk_writes,0},\n  {backing_queue_status,{struct,[{mode,default},\n                                 {q1,0},\n                                 {q2,0},\n                                 {delta,[delta,undefined,0,undefined]},\n                                 {q3,0},\n                                 {q4,0},\n                                 {len,0},\n                                 {target_ram_count,infinity},\n                                 {next_seq_id,0},\n                                 {avg_ingress_rate,0.0},\n                                 {avg_egress_rate,0.0},\n                                 {avg_ack_ingress_rate,0.0},\n                                 {avg_ack_egress_rate,0.0},\n                                 {mirror_seen,0},\n                                 {mirror_senders,0}]}},\n  {node,'rabbit@it-kae-rmq'},\n  {arguments,{struct,[{<<\"x-dead-letter-exchange\"...\n=ERROR REPORT==== 7-Jul-2016::16:25:03 ===\nwebmachine error: path=\"/api/queues/\"\n\"Internal Server Error\"\n```\nLogs after \ncurl -u guest:guest -X GET http://127.0.0.1:15672/api/queues/ | python -m json.tool\nOutput of command\n```\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  113k  100  113k    0     0  1658k      0 --:--:-- --:--:-- --:--:-- 1667k\n{\n    \"error\": \"JSON encode error: {bad_term,#{error_logger => true,kill => true,size => 0}}\",\n    \"reason\": \"While encoding: \\n[[{memory,14128},\\n  {reductions,3628},\\n  {reductions_details,[{rate,0.0}]},\\n  {messages,0},\\n  {messages_details,[{rate,0.0}]},\\n  {messages_ready,0},\\n  {messages_ready_details,[{rate,0.0}]},\\n  {messages_unacknowledged,0},\\n  {messages_unacknowledged_details,[{rate,0.0}]},\\n  {idle_since,<<\\\"2016-07-07 13:22:09\\\">>},\\n  {consumer_utilisation,''},\\n  {policy,<<\\\"ha-all\\\">>},\\n  {exclusive_consumer_tag,''},\\n  {consumers,1},\\n  {slave_nodes,[]},\\n  {synchronised_slave_nodes,[]},\\n  {recoverable_slaves,[]},\\n  {state,running},\\n  {reductions,3628},\\n  {garbage_collection,[{max_heap_size,#{error_logger => true,kill => true,size => 0}},\\n                       {min_bin_vheap_size,46422},\\n                       {min_heap_size,233},\\n                       {fullsweep_after,65535},\\n                       {minor_gcs,1}]},\\n  {messages_ram,0},\\n  {messages_ready_ram,0},\\n  {messages_unacknowledged_ram,0},\\n  {messages_persistent,0},\\n  {message_bytes,0},\\n  \n....\n\n```\n. ",
    "dannykopping": "From error log: error.txt\nCan confirm that I encountered the same issue.\nI pinned Erlang on my Ubuntu Precise box:\n/etc/apt/preferences.d/30erlang:\nPackage: erlang*\nPin: version 1:18.3-1\nPin-Priority: 999\nPurged erlang*, reinstalled rabbitmq-server and it's now working\n. ",
    "chrome050": "Same issue when calling /api/connections\n{\n   \"error\" : \"JSON encode error: {bad_term,#{error_logger => true,kill => true,size => 0}}\",\n   \"reason\" : \"While encoding: \\n[[{connected_at,1469090807198},\\n  {client_properties,\\n      {struct,\\n          [{<<\\\"product\\\">>,<<\\\"RabbitMQ\\\">>},\\n           {<<\\\"version\\\">>,<<\\\"3.6.3.0\\\">>},\\n           {<<\\\"platform\\\">>,<<\\\".NET\\\">>},\\n           {<<\\\"copyright\\\">>,\\n            <<\\\"Copyright (c) 2007-2016 Pivotal Software, Inc.\\\">>},\\n           {<<\\\"information\\\">>,\\n            <<\\\"Licensed under the MPL.  See http://www.rabbitmq.com/\\\">>},\\n           {<<\\\"MachineName\\\">>,<<\\\"CLEANMASCHINE\\\">>},\\n           {<<\\\"capabilities\\\">>,\\n            {struct,\\n                [{<<\\\"publisher_confirms\\\">>,true},\\n                 {<<\\\"exchange_exchange_bindings\\\">>,true},\\n                 {<<\\\"basic.nack\\\">>,true},\\n                 {<<\\\"consumer_cancel_notify\\\">>,true},\\n                 {<<\\\"connection.blocked\\\">>,true},\\n                 {<<\\\"authentication_failure_close\\\">>,true}]}},\\n           {<<\\\"connection_name\\\">>,undefined}]}},\\n  {channel_max,0},\\n  {frame_max,131072},\\n  {timeout,60},\\n  {vhost,<<\\\"devel\\\">>},\\n  {user,<<\\\"ats-test\\\">>},\\n  {protocol,<<\\\"AMQP 0-9-1\\\">>},\\n  {ssl_hash,sha},\\n  {ssl_cipher,aes_256_cbc},\\n  {ssl_key_exchange,rsa},\\n  {ssl_protocol,tlsv1},\\n  {auth_mechanism,<<\\\"PLAIN\\\">>},\\n  {peer_cert_validity,''},\\n  {peer_cert_issuer,''},\\n  {peer_cert_subject,''},\\n  {ssl,true},\\n  {peer_host,<<\\\"CLEANIP\\\">>},\\n  {host,<<\\\"CLEANIP\\\">>},\\n  {peer_port,32993},\\n  {port,5671},\\n  {name,<<\\\"CLEANIP:32993 -> CLEANIP:5671\\\">>},\\n  {node,testbench@localhost},\\n  {type,network},\\n  {garbage_collection,\\n      [{max_heap_size,#{error_logger => true,kill => true,size => 0}},\\n       {min_bin_vheap_size,46422},\\n       {min_heap_size,233},\\n       {fullsweep_after,65535},\\n       {minor_gcs,3}]},\\n  {reductions,84694},\\n  {channels,0},\\n  {state,running},\\n  {send_pend,0},\\n  {send_cnt,9},\\n  {recv_cnt,26},\\n  {recv_oct_details,[{rate,7.4}]},\\n  {recv_oct,1779},\\n  {send_oct_details,[{rate,0.0}]},\\n  {send_oct,2732},\\n  {reductions_details,[{rate,103.4}]},\\n  {reductions,84694}]]\"\n}\nCalling from Management Interface im getting:\nGot response code 500 with body {\"error\":\"JSON encode error: {bad_term,#{error_logger => true,kill => true,size => 0}}\",\"reason\":\"While encoding: \\n[{total_count,1},\\n {item_count,1},\\n {filtered_count,1},\\n {page,1},\\n {page_size,100},\\n {page_count,1},\\n {items,\\n [[{connected_at,1469090807198},\\n {client_properties,\\n {struct,\\n [{<<\\\"product\\\">>,<<\\\"RabbitMQ\\\">>},\\n {<<\\\"version\\\">>,<<\\\"3.6.3.0\\\">>},\\n {<<\\\"platform\\\">>,<<\\\".NET\\\">>},\\n {<<\\\"copyright\\\">>,\\n <<\\\"Copyright (c) 2007-2016 Pivotal Software, Inc.\\\">>},\\n {<<\\\"information\\\">>,\\n <<\\\"Licensed under the MPL. See http://www.rabbitmq.com/\\\">>},\\n {<<\\\"MachineName\\\">>,<<\\\"CLEANMASCHINE\\\">>},\\n {<<\\\"capabilities\\\">>,\\n {struct,\\n [{<<\\\"publisher_confirms\\\">>,true},\\n {<<\\\"exchange_exchange_bindings\\\">>,true},\\n {<<\\\"basic.nack\\\">>,true},\\n {<<\\\"consumer_cancel_notify\\\">>,true},\\n {<<\\\"connection.blocked\\\">>,true},\\n {<<\\\"authentication_failure_close\\\">>,true}]}},\\n {<<\\\"connection_name\\\">>,undefined}]}},\\n {channel_max,0},\\n {frame_max,131072},\\n {timeout,60},\\n {vhost,<<\\\"devel\\\">>},\\n {user,<<\\\"CLEANUSER\\\">>},\\n {protocol,<<\\\"AMQP 0-9-1\\\">>},\\n {ssl_hash,sha},\\n {ssl_cipher,aes_256_cbc},\\n {ssl_key_exchange,rsa},\\n {ssl_protocol,tlsv1},\\n {auth_mechanism,<<\\\"PLAIN\\\">>},\\n {peer_cert_validity,''},\\n {peer_cert_issuer,''},\\n {peer_cert_subject,''},\\n {ssl,true},\\n {peer_host,<<\\\"CLEANIP\\\">>},\\n {host,<<\\\"CLEANIP\\\">>},\\n {peer_port,32993},\\n {port,5671},\\n {name,<<\\\"CLEANIP:32993 -> CLEANIP:5671\\\">>},\\n {node,testbench@localhost},\\n {type,network},\\n {garbage_collection,\\n [{max_heap_size,#{error_logger => true,kill => true,size => 0}},\\n {min_bin_vheap_size,46422},\\n {min_heap_size,233},\\n {fullsweep_after,65535},\\n {minor_gcs,1}]},\\n {reductions,58807},\\n {channels,0},\\n {state,running},\\n {send_pend,0},\\n {send_cnt,5},\\n {recv_cnt,8},\\n {recv_oct_details,[{rate,0.0}]},\\n {recv_oct,1113},\\n {send_oct_details,[{rate,0.0}]},\\n {send_oct,2436},\\n {reductions_details,[{rate,94.0}]},\\n {reductions,58807}]]}]\"}\n. ",
    "xuanskyer": "anyway, thanks!\n. ",
    "jianping-roth": "thank you for your reply :)\n. ",
    "spencer1248": "I just ran into this with rabbitmq-server 3.5.4 and Erlang R16B03 on a 3 node cluster following a network partition. We resolved without taking down our cluster by deleting the exchange identified in our log. Deleting the queue did not resolve nor did manually creating a binding between the exchange and queue.. ",
    "nagas": "We also run into this issue after network partition with rabbitmq-server 3.6.6 and Erlang 19.2.. ",
    "RogerSolerV": "Same here, it worked deleting the affected exchanges and recreating everything from scratch, thanks @spencer1248 . ",
    "gujun4990": "We encounter the same issue. The version of rabbitmq is 3.6.5 and we also have cluster of 3 nodes. So anybody know the issue have been solved or how to avoid the issue.. I had read the RabbitMQ official document again and found the below contents:\nhttps://www.rabbitmq.com/ha.html\n\"in the case of publisher confirms, a message will only be confirmed to the publisher when it has been accepted by all of the mirrors.\"\nThat is to say, if the other 3 nodes don't recover, the publisher confirm mode will not work. Certainly this will guarantee the data safety for RabbitMQ. But if I want to guarantee availability for RabbitMQ, how to configure RabbitMQ for me?. Based on the second comment, I had modified RabbitMQ ha policy {\"ha-mode\": \"exactly\", \"ha-params\":2} and found that RabbitMQ dose't also reply basic.ack or basic.nack when using publisher confirm.Besides, I delete the ha policy, i.e, use non-mirror queues. The result is the same. So I am very confused what caused this problem.. I had analyzed the related code and the pause_minority configuration cause the problem. When the count of cluster nodes are less than half, the publisher confirm mode will stay pausing state for RabbitMQ. So the publisher will not receive basic.ack or basic.nack. Thank you. @lukebakken @michaelklishin . ",
    "svrx": "This issue as been found to be still affecting 3.6.12.\nBinding to queues fail, Then only deleting the exchange and recreating it seems to work.\nThis is extremely painful to deal with in production, because it may happen during OS patch restarts and will require multiple tries to bring application with right subscriptions back up.\nWould you consider reopening this bug?\nLet me know what information would you require.. ",
    "kajottnasdaq": "I was just hit with this on 3.7.4, with Erlang 20.3.4 after a switch patching reboot left 2 node cluster partitioned... We are still in test, but risking this silent loss happeing at patching as opposed to having a one node cluster fail with a clean break is making me seriously reconsider deploying a cluster in production. Right now risks seem to outweigh benefits.. ",
    "gshmu": "thanks anyway\n. ",
    "hoalequang": "I have a same problem. but in my case, they happen on all my nodes in cluster. could rabbitmq team please help me resolve this?. ",
    "tpwow": "@sepich i have the same problem with yours.  have you solved it?? need a little help or suggest. ",
    "harlowja": "I'm not to concerned, it can go in 3.6.x or 3.7.x, I'll let u guys decide :-P\n. Moved to https://github.com/rabbitmq/rabbitmq-server/pull/913 (since I'm not sure how to retarget the branch without closing, ha).\n. Hmmm, I wouldn't try installing a el7 package on el6,\nThe %if and such in the package should make it still work on older versions, bu I wouldn't recommend installing a package for a newer distro on an older one.\n. Seems like it yes, u'll need a el7 and a el6 one; though I am pretty sure this is common practice.\n. U turned into a cat! Hi there :-P\n. My first rabbitmq commit ftw :-P\n. A make package-rpm to build both is going to require a little bit more work IMHO, it will likely require something like mock but this starts to push the boundaries of my understanding of rpmbuild.\nhttps://fedoraproject.org/wiki/Mock?rd=Subprojects/Mock\n. I can try to work on said thing (time permitting) as a followup if we want (it shouldn't be impossible). \nThoughts?\n. Cools.\n. Cool thx for letting me know, I hope rabbitmq-server-932 gets merged soon.\n. Thanks guys and gals!\n. Ah, sure will fix that :)\nWonder how that snuck in :-P\n. Yup, will do\n. ",
    "jerrykuch": "Oh hey, Joshua Harlow, nice to see you... we met when I visited Yahoo for RabbitMQ-related stuff a couple of times back when you were working on Annie's team there...\n. Welcome, etc.!\nOn Thu, Aug 11, 2016 at 2:56 PM, Joshua Harlow notifications@github.com\nwrote:\n\nMy first rabbitmq commit ftw :-P\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/pull/913#issuecomment-239305647,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAFGEajpcwJY2HE60DIjLmWaTxfmYOFrks5qe5qKgaJpZM4JgmZ1\n.\n. \n",
    "mwhahaha": "I'll submit another against the stable branch as well. Thanks.\n. ",
    "smurfix": "Sorry, apparently I didn't write clearly. I do know about RabbitMQ's MQTT front-end/plug-in, but what I meant was to provide for this using the \"native\" AMQP protocol in some way.\nSwitching to MQTT doesn't work for my application, as I need too many other features which AMQP provides.\n. To repeat,\n\nthe message is delivered to a queue w/ auto-delete\nthe AMQP client reads the message, then immediately dies.\nthe queue thus gets auto-deleted.\nthe message is lost.\n\nI don't like lost messages.\nI'd like the message to be dead-lettered (either by default, or optionally).. ",
    "meverett": "I also really like this feature from the MQTT protocol and would love a more native version of the same in RabbitMQ (ideally in the AMQP spec as I believe it's a very useful feature). This is especially helpful in the world of IoT devices. Often times devices only implement one main messaging protocol like AMQP or MQTT and it speeds up/simplifies development to be able to have this functionality built in, without having to implement a secondary communication layer and additional keep alive system (like say over HTTP or MQTT), which increases firmware size and complexity.\nThere are plenty of use cases for wanting to know when a client or device has become disconnected/unavailable ungracefully; this is especially important in the world of sensor telemetry. Often times unexpected client disconnections are events that require reactionary/recovery workflows.\nThe advantages of having such a feature built in include:\n\nLess overall sockets or communication layers to manage in clients who's main protocol is already AMQP (otherwise you'd have to implement your own in a semi-redundant, alternative communication protocol just to add in connection state events that can be broadcast).\nThe server already manages connection state, so it is in a great position to be able to handle broadcasting events in the case of ungraceful client disconnects.\nSince the server is already a message bus with queuing and pub/sub it is possible to easily broadcast and handle disconnection events without having to implement a redundant pattern elsewhere.\nIn the MQTT version you can specify the topic, retain/persisted flag, QoS, and payload to be broadcast upon ungraceful disconnect, which means there are many creative and powerful ways to route the disconnection message to which allows you to handle sophisticated broadcast scenarios and workflows.\n\nHaving worked with both MQTT and AMQP I can say that there are very few things from the MQTT protocol that I prefer over AMQP, but the Last Will and Testament feature is definitely one of them.\nHaving the RabbitMQ MQTT plugin (which it sounds like implements the LWT feature) is a decent work around. But it is rather cumbersome to have to include and support an additional client library just to add in this functionality which is a pretty common use cases for telemetry.. @michaelklishin You said it's something that would be considered if there was enough interest so I was replying, expressing my interest in context to the feature request. I apologize, but you didn't specify a desired destination to express interest and I've never seen any other repository where doing so in the issue's comment thread itself was actively discouraged. I thought that was the point? Nothing in CONTRIBUTING.md, CODE_OF_CONDUCT.md, or http://www.rabbitmq.com/github.html indicated otherwise. Many projects gauge interest on issues through such a process. I'm sure the reasons exist, I've just yet to run into a project that doesn't utilize issue discussions in such a manner. Maybe consider making that desire more visible in the README or something similar with high visibility. I would have been happy to direct my comment/interest to the correct location had I known what that was and had I known it would have been discouraged in this context. I won't reply further, but I wanted to offer that feedback since I know others will share my experience of it being pretty customary to express contextual interest and thoughts in the comments of an issue, especially when finding it as one of the top results of a search as to whether or not the feature exists.. ",
    "darkgray": "The client is upgrading to latest version on Linux. Is there any way for me to get in touch with a anyone at Pivotal when we support these clients? We never get told whether the client has Pivotal support or not. \n. Thanks\n. ",
    "Esity": "I feel as though this is an issue and not a question since rabbitmq-server should allow the hostnames to have subdomains. \n. ",
    "diaolanshan": "We should wait for the next release for this or add a parameter can solve the issue?\n. ",
    "phunehehe": "As I wrote a new peak appeared at 6.3GB. At this point I'm going to just stick to 10%...\n. ",
    "jguenther-va": "Whoops. I will do that, thank you\n. ",
    "DanTup": "@michaelklishin Apologies; I couldn't understand the error so I wasn't sure if this was a bug that would need a fix (it was unexpected that a reboot would break things).\nI did change the machines name after installing; so my guess is that's related (does it seem likely/possible RabbitMQ has written this info somewhere, such that a machine rename would crash like this?)\n. ",
    "homelessnessbo": "Sorry if Im wrong, but it looks like bug and unexpected behavior of rabbitmq-server, and, according to google groups rabbitmq-team hasnt any official bug tracker that I could post it to.\nSo, you want to say that when message went into durable and direct exchange that binded to only durable queue - its shouldnt appear in that single queue without publisher confirms? And sorry if my explanation wasnt clear enough, but I see by graphs in web-ui that messages came into exchange, but after - it doesnt appears in the queue.\nOk, we will try using it with confirms, but it doesnt look like it should work that way. \n. ",
    "spgriffinjr": "Im using erlang R16B-03.17\n. yeah i just noticed it. Thanks for pointing me in the right direction. \n. ",
    "richlv": "re-reading this for the 5th time, it might have been \"(A complete rabbitmq.config) (which does this) (would look like):\"\nif so, it is probably a proper sentence... but it might also be a good idea to rephrase it a bit :)\n. Actually, it looks like normal user credentials cannot even be tested with rabbitmqadmin - it fails with \"Not management user\".. Sorry about the noise. I was checking on an older RabbitMQ version and nothing came up - turns out, authenticate_user was added in 3.6.5: https://rabbitmq.docs.pivotal.io/36/relnotes/release-notes-rabbitmq3.6.5.html\nThank you for that hint :). Thanks, but how come https://github.com/rabbitmq/rabbitmq-server/releases/tag/rabbitmq_v3_6_5 does not list the new command?. ",
    "peaksnail": "It will make sence! Thumbs up!\n. ",
    "jrlog": "Seconded. The message-discarding behavior has been universally unexpected among people I've talked with about it, and it would make sense to add this as an (at least optional) feature. \n. ",
    "hsquirrel": "Yes, please! I have systems that are far more complicated than they need to be because of the default behavior. This one feature would would be a godsend for me. Keep the default, but make it an option.\n. ",
    "smnirven": "I too would love to see this feature. ",
    "boh717": "Sorry, but I've found no answers anywhere else.\n\nThere is a field for head-of-the-queue timestamp in recent releases. It can be a very fast moving thing and I'm not sure I'd make my system depend on it.\n\nI will check it.\nWhich version introduced it?\n. ",
    "johanhaleby": "@michaelklishin Thanks, that would be really great for us! Could we expect to see this change in a minor release anytime soon?\nI understand that the intention behind tracing is not meant to be used on an ongoing basis but are there any downsides of always having it on like this (besides what would be expected in terms of increased resource allocation)?\n. ",
    "augi": "The problem is that QoS set to 0 is the default in Java client so it's not nice that the server crashes in this default settings. The expectation is that the server shouldn't close the connection, and if it's reasonable to close the connection then it should be done in a nice way (not in a way that leads to UnexpectedConnectionDriverException in Java client).\nThe surrounding log entries from main log are related to different connections and they have no effect as the crash is 100% reproducible in our environment. SASL log contains no entries related to these incidents (the last messages is more than hour before the crash).\nBtw. the connection is not closed immediately after creating the channel, the connection is closed after some time (also some messages are consumed).\n. Ok. So it must be something related to our infrastructure. Thank you for your time!. I actually wasn't able to fix this issue :( I also tried to use Lyra library without success (it should be handle much more failures than the standard RabbitMQ Java client).\nIf this kind of error occurs, we re-establish the connection.. ",
    "airuleguy": "Hey @augi, I'm getting the exact same error under the exact same circumstances. Sometimes it takes a pretty long time until I get the\n=ERROR REPORT==== 27-Apr-2017::11:23:24 ===\nclosing AMQP connection <0.14589.1> (172.16.172.183:34993 -> 10.32.98.232:5672):\n{writer,send_failed,{error,enotconn}}\nBut sooner or later, it ends up happening.\nWere you able to sort it out?\nA bit of context\nThere is one VM with 5 consumers leeching messages from a particular queue. It works just fine for a while, but then all of a sudden I get this enotconn error and all consumers are disconnected (all 5 share the same connection, different channels, so it's pretty evident that when the connection goes kaput, so will all 5 consumers).\nWhen I attempt to restart the consumer application, here's what rabbit says:\n```\n=WARNING REPORT==== 27-Apr-2017::11:35:10 ===\nclosing AMQP connection <0.10232.1> (172.16.172.183:34467 -> 10.32.98.232:5672):\nclient unexpectedly closed TCP connection\n=INFO REPORT==== 27-Apr-2017::11:36:10 ===\naccepting AMQP connection <0.22614.1> (172.16.172.183:36559 -> 10.32.98.232:5672)\n=INFO REPORT==== 27-Apr-2017::11:36:11 ===\nclosing AMQP connection <0.22614.1> (172.16.172.183:36559 -> 10.32.98.232:5672)\n=INFO REPORT==== 27-Apr-2017::11:36:17 ===\naccepting AMQP connection <0.22703.1> (172.16.172.183:36562 -> 10.32.98.232:5672)\n=INFO REPORT==== 27-Apr-2017::11:42:38 ===\naccepting AMQP connection <0.26651.1> (172.16.172.183:36705 -> 10.32.98.232:5672)\n=ERROR REPORT==== 27-Apr-2017::11:43:09 ===\nclosing AMQP connection <0.22703.1> (172.16.172.183:36562 -> 10.32.98.232:5672):\n{writer,send_failed,{error,enotconn}}\n```\nPlease if this is not the appropriate place to post the question, direct me to wherever it is.\nCheers!. Hey peeps,\nJust to let you guys know, I fixed this issue by reducing the amount of prefetchCount messages. Looks like with a big enough prefetchCount, Rabbit may end up not having room in the transport buffer to accommodate its control packets. It all goes downhill from there.\nI'm not sure exactly how that works or if it is even true, but it fixed the issue for me and for others out there. KYP on any updates I come across.\nCheers!. ",
    "oznetmaster": "There appears to be multiple contradictory pages about installing the server.  https://www.rabbitmq.com/install-windows.html says that a normal installation involves only two steps, the two that I tried and which failed.\n\"Firstly, download and run the Erlang Windows Binary File. It takes around 5 minutes.\nThen just run the installer, rabbitmq-server-3.6.5.exe. It takes around 2 minutes, and will set RabbitMQ up and running as a service, with a default configuration.\"\n. Also, even if the environment variable is defined as in the alternative installation page, it still does not work:\n```\nC:\\WINDOWS\\system32>if exist \"%ERLANG_HOME%\\bin\\erl.exe\" (echo \"yes\")\n\"yes\"\nC:\\WINDOWS\\system32>if exist \"!ERLANG_HOME!\\bin\\erl.exe\" (echo \"yes again\")\nC:\\WINDOWS\\system32>\n```\nThe rabbitmq-server.bat file fails with the error:\nINFO: Could not find files for the given pattern(s).\nThe system cannot find the path specified.\n\nERLANG_HOME not set correctly.\n\nPlease either set ERLANG_HOME to point to your Erlang installation or place the\nRabbitMQ server distribution in the Erlang lib folder.\nbecause of the above bug.\n. So if it is not commonly reported, even though it is repeatable and demonstrable with a very simple example, it is not a bug to be fixed?\nI am not a google member, and do not have a google account, so I cannot post on google groups.\n. In case anyone is wondering, my simple demo of the problem should work, even though it is not in a .bat file, because:\n[HKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Command Processor]\n\"CompletionChar\"=dword:00000009\n\"DefaultColor\"=dword:00000000\n\"EnableExtensions\"=dword:00000001\n\"PathCompletionChar\"=dword:00000009\n\"DelayedExpansion\"=dword:00000001\n. I found the bug.  The problem is that delayed expansion (!) only works on environment variables that are defined as system variables.  It does not work for environment variables defined as user variables.\nOur users are normally restricted from modifying system environment variables on their computers.\nThis needs to be documented at least, if not fixed.  Applications should not require changes to system environment variables to work.\n. Furthermore, system environment variables apparently can only be accessed from an administrative command prompt.\n. ",
    "jaime-elias": "I'm getting the same error trying to install 3.6.9 build 920 with Erlang R19B3. ",
    "vbrocket": "uninstall , restart windows,   Disable antivirus And kick other windows users if exist then re-install it will work.\n. ",
    "j0nimost": "up to date we still don't have a solution for this. ",
    "cbjjensen": "I'm unable to get the installer to work on a new VM (https://developer.microsoft.com/en-us/windows/downloads/virtual-machines)\nSystem variables:\nERLANG_HOME is set to C:\\Program Files\\erl9.1\nPath is set to include %ERLANG_HOME%\nBelow is a screenshot of everything configured\n\nSorry if this is necro'ing, but I'm a new user to this and can't get it to install. \n. I did a RUN, instead of a save when downloading it from the erlang page (not as admin). Here is the result... There isn't an entry there!\n\nI'm going to try a few more things and report back here with the results.\n. Running the erlang (otp_win64) installer as administrator has resolved the issue.. ",
    "supernomad": "ok thank you for the response.\n. I couldn't get that working for rabbitmqctl but I probably messed it up. I definitely agree that is less secure, and in my mind renders the cookie pretty much useless for any kind of security, hence my question in the first place.\n. ",
    "cocowalla": "@Supernomad I also can't get -setcookie to work with rabbitmqctl - whether I added it to RABBITMQ_CTL_ERL_ARGS or changed the .bat file, the value is ignored and an .erlang.cookie file is created at %HOMEDRIVE%%HOMEPATH%\\.erlang.cookie.\nDid you ever get this working?. @michaelklishin the issue was specifically when running as a Windows service using rabbitmq-service. I was setting RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS to  \"-setcookie MY_COOKIE\", but it was creating and using a cookie in C:\\Windows\\.erlang.cookie regardless of the setcookie value.\nI finally figured it out though - you have to make sure you don't encase the RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS value in quotes (like you did in your example; I guess that's fine for Linux, but not for the Windows Service).\nThis is the same for RABBITMQ_CTL_ERL_ARGS - you must not encase the value in quotes.. @lukebakken in rabbitmq-env.bat I added call \"%SCRIPT_DIR%\\custom-rabbitmq-env.bat\", which calls a custom script that generates a cookie and sets RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS. @lukebakken leave them out entirely. ",
    "ron819": "Will this be implemented soon?. ",
    "RafaAguilar": "@michaelklishin My purpose was notifying a Bug, not a question. I already did what you suggested, but after several exercises, peer revisions and re-read the documentation We conclude that this could be a bug and wanted to report it here. Did you read all the Issue?, because it have 1 minute from publish to closed.. ",
    "mqchau": "Ok. I'll do that. Sorry about it.. ",
    "lkdatm": "thank you. ",
    "AndrewDryga": "Actually I was using list of common typos from Wikipedia to grep over source code. After that I used to fix typos manually.\nI will retry carefully.. ",
    "knappe": "@michaelklishin This is an issue though.  Why can't I ask rabbitmq-server which version it is running?  That is a very reasonable expectation.  And I shouldn't have to invoke sudo to do it.. ",
    "mlouage": "Found a workaround. \nI disconnected all network drives and rebooted the workstation without a network cable. I made sure my HOMEDRIVE was now set to C: and HOMEPATH to \\Users\\username\nRetried the steps to remove and install the service with the command line tools and now everything worked. Because my erlang.cookie was the same in systemroot, homedrive on C:\\ and homedrive on P:\\ the setup also works when I reboot the workstation with all network cables connected and network drives mapped again.. ",
    "abra7134": "I have install v3.6.6.903 on Debian v8.7 from .deb, and began to repeat everything on steps.\nAfter sending the message to queue (the first python script), in the server log the following mistakes appear:\n```\n=ERROR REPORT==== 24-Jan-2017::11:22:19 ===\n Generic server <0.697.0> terminating\n Last message in was {'$gen_cast',\n                           {method,\n                               {'basic.publish',0,<<>>,<<\"task_queue\">>,\n                                   false,false},\n                               {content,60,none,\n                                   <<16,0,2>>,\n                                   rabbit_framing_amqp_0_9_1,\n                                   [<<\"Hello World!\">>]},\n                               flow}}\n When Server state == {ch,running,rabbit_framing_amqp_0_9_1,1,<0.687.0>,\n                            <0.695.0>,<0.687.0>,\n                            <<\"[::1]:33499 -> [::1]:5672\">>,\n                            {lstate,<0.696.0>,false},\n                            none,1,\n                            {[],[]},\n                            {user,<<\"guest\">>,\n                                [administrator],\n                                [{rabbit_auth_backend_internal,none}]},\n                            <<\"/\">>,<<\"task_queue\">>,\n                            {dict,0,16,16,8,80,48,\n                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                 []},\n                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                  [],[]}}},\n                            {state,\n                                {dict,0,16,16,8,80,48,\n                                    {[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                     [],[],[]},\n                                    {{[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                      [],[],[]}}},\n                                erlang},\n                            {dict,0,16,16,8,80,48,\n                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                 []},\n                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                  [],[]}}},\n                            {dict,0,16,16,8,80,48,\n                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                 []},\n                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                  [],[]}}},\n                            {set,0,16,16,8,80,48,\n                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                 []},\n                                {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                                  [],[]}}},\n                            <0.690.0>,\n                            {state,fine,5000,#Ref<0.0.0.4565>},\n                            false,1,\n                            {{0,nil},{0,nil}},\n                            [],\n                            {{0,nil},{0,nil}},\n                            [{<<\"connection.blocked\">>,bool,true},\n                             {<<\"authentication_failure_close\">>,bool,true},\n                             {<<\"consumer_cancel_notify\">>,bool,true},\n                             {<<\"publisher_confirms\">>,bool,true},\n                             {<<\"basic.nack\">>,bool,true}],\n                            none,0,none,flow,[]}\n Reason for termination == \n** {{cannot_compile_forms,error},\n    [{code_version,compile_forms,1,[{file,\"src/code_version.erl\"},{line,97}]},\n     {code_version,update,1,[{file,\"src/code_version.erl\"},{line,63}]},\n     {rabbit_core_metrics,ets_update_counter,4,\n                          [{file,\"src/rabbit_core_metrics.erl\"},{line,183}]},\n     {rabbit_core_metrics,channel_stats,4,\n                          [{file,\"src/rabbit_core_metrics.erl\"},{line,149}]},\n     {rabbit_channel,'-incr_stats/2-lc$^0/1-0-',2,\n                     [{file,\"src/rabbit_channel.erl\"},{line,2002}]},\n     {rabbit_channel,deliver_to_queues,2,\n                     [{file,\"src/rabbit_channel.erl\"},{line,1860}]},\n     {rabbit_channel,handle_method,3,\n                     [{file,\"src/rabbit_channel.erl\"},{line,958}]},\n     {rabbit_channel,handle_cast,2,\n                     [{file,\"src/rabbit_channel.erl\"},{line,459}]}]}\n=ERROR REPORT==== 24-Jan-2017::11:22:19 ===\nError on AMQP connection <0.687.0> ([::1]:33499 -> [::1]:5672, vhost: '/', user: 'guest', state: closing), channel 1:\n{{cannot_compile_forms,error},\n [{code_version,compile_forms,1,[{file,\"src/code_version.erl\"},{line,97}]},\n  {code_version,update,1,[{file,\"src/code_version.erl\"},{line,63}]},\n  {rabbit_core_metrics,ets_update_counter,4,\n                       [{file,\"src/rabbit_core_metrics.erl\"},{line,183}]},\n  {rabbit_core_metrics,channel_stats,4,\n                       [{file,\"src/rabbit_core_metrics.erl\"},{line,149}]},\n  {rabbit_channel,'-incr_stats/2-lc$^0/1-0-',2,\n                  [{file,\"src/rabbit_channel.erl\"},{line,2002}]},\n  {rabbit_channel,deliver_to_queues,2,\n                  [{file,\"src/rabbit_channel.erl\"},{line,1860}]},\n  {rabbit_channel,handle_method,3,\n                  [{file,\"src/rabbit_channel.erl\"},{line,958}]},\n  {rabbit_channel,handle_cast,2,\n                  [{file,\"src/rabbit_channel.erl\"},{line,459}]}]}\n=WARNING REPORT==== 24-Jan-2017::11:22:19 ===\nNon-AMQP exit reason '{{cannot_compile_forms,error},\n                       [{code_version,compile_forms,1,\n                            [{file,\"src/code_version.erl\"},{line,97}]},\n                        {code_version,update,1,\n                            [{file,\"src/code_version.erl\"},{line,63}]},\n                        {rabbit_core_metrics,ets_update_counter,4,\n                            [{file,\"src/rabbit_core_metrics.erl\"},{line,183}]},\n                        {rabbit_core_metrics,channel_stats,4,\n                            [{file,\"src/rabbit_core_metrics.erl\"},{line,149}]},\n                        {rabbit_channel,'-incr_stats/2-lc$^0/1-0-',2,\n                            [{file,\"src/rabbit_channel.erl\"},{line,2002}]},\n                        {rabbit_channel,deliver_to_queues,2,\n                            [{file,\"src/rabbit_channel.erl\"},{line,1860}]},\n                        {rabbit_channel,handle_method,3,\n                            [{file,\"src/rabbit_channel.erl\"},{line,958}]},\n                        {rabbit_channel,handle_cast,2,\n                            [{file,\"src/rabbit_channel.erl\"},{line,459}]}]}'\n=INFO REPORT==== 24-Jan-2017::11:22:19 ===\nclosing AMQP connection <0.687.0> ([::1]:33499 -> [::1]:5672)\n```\nAnd worker process (the second python script) take off with a error:\n$ ./worker.py\n [*] Waiting for messages. To exit press CTRL+C\n [x] Received 'Hello World!'\n [x] Done\nTraceback (most recent call last):\n  File \"./worker.py\", line 21, in <module>\n    channel.start_consuming()\n  File \"/usr/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 955, in start_consuming\n    self.connection.process_data_events()\n  File \"/usr/lib/python2.7/dist-packages/pika/adapters/blocking_connection.py\", line 243, in process_data_events\n    raise exceptions.ConnectionClosed()\npika.exceptions.ConnectionClosed\nAnd server log:\n```\n=INFO REPORT==== 24-Jan-2017::11:32:45 ===\naccepting AMQP connection <0.1427.0> ([::1]:34661 -> [::1]:5673)\n=ERROR REPORT==== 24-Jan-2017::11:32:45 ===\n Generic server <0.1435.0> terminating\n Last message in was {'$gen_cast',\n                        {deliver,\n                         <<\"ctag1.7b0c1c033977462d86b2e76df6a1f505\">>,true,\n                         {{resource,<<\"/\">>,queue,<<\"task_queue\">>},\n                          <30777.700.0>,0,true,\n                          {basic_message,\n                           {resource,<<\"/\">>,exchange,<<>>},\n                           [<<\"task_queue\">>],\n                           {content,60,\n                            {'P_basic',undefined,undefined,undefined,2,\n                             undefined,undefined,undefined,undefined,\n                             undefined,undefined,undefined,undefined,\n                             undefined,undefined},\n                            <<16,0,2>>,\n                            rabbit_framing_amqp_0_9_1,\n                            [<<\"Hello World!\">>]},\n                           <<129,105,138,93,63,115,23,109,185,102,170,115,223,\n                             188,88,161>>,\n                           true}}}}\n When Server state == {ch,running,rabbit_framing_amqp_0_9_1,1,<0.1427.0>,\n                         <0.1433.0>,<0.1427.0>,\n                         <<\"[::1]:34661 -> [::1]:5673\">>,\n                         {lstate,<0.1434.0>,false},\n                         none,1,\n                         {[],[]},\n                         {user,<<\"guest\">>,\n                          [administrator],\n                          [{rabbit_auth_backend_internal,none}]},\n                         <<\"/\">>,<<\"task_queue\">>,\n                         {dict,1,16,16,8,80,48,\n                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},\n                          {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                            [[<30777.700.0>|\n                              {resource,<<\"/\">>,queue,<<\"task_queue\">>}]],\n                            []}}},\n                         {state,\n                          {dict,1,16,16,8,80,48,\n                           {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},\n                           {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                             [[<30777.700.0>|#Ref<0.0.0.20659>]],\n                             []}}},\n                          erlang},\n                         {dict,1,16,16,8,80,48,\n                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},\n                          {{[],[],[],[],[],[],[],[],[],\n                            [[<<\"ctag1.7b0c1c033977462d86b2e76df6a1f505\">>|\n                              {{amqqueue,\n                                {resource,<<\"/\">>,queue,<<\"task_queue\">>},\n                                true,false,none,[],<30777.700.0>,\n                                [<0.1216.0>],\n                                [<0.1216.0>],\n                                [p5673@mq],\n                                [{vhost,<<\"/\">>},\n                                 {name,<<\"ha-all\">>},\n                                 {pattern,<<>>},\n                                 {'apply-to',<<\"all\">>},\n                                 {definition,\n                                  [{<<\"ha-mode\">>,<<\"all\">>},\n                                   {<<\"ha-sync-mode\">>,<<\"automatic\">>}]},\n                                 {priority,0}],\n                                [{<0.1217.0>,<0.1216.0>},\n                                 {<30777.703.0>,<30777.700.0>}],\n                                [],live,0},\n                               {false,1,false,[]}}]],\n                            [],[],[],[],[],[]}}},\n                         {dict,1,16,16,8,80,48,\n                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},\n                          {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                            [[<30777.700.0>|\n                              {1,\n                               {<<\"ctag1.7b0c1c033977462d86b2e76df6a1f505\">>,\n                                nil,nil}}]],\n                            []}}},\n                         {set,1,16,16,8,80,48,\n                          {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]},\n                          {{[],[],[],[],[],[],[],[],[],[],[],[],[],[],\n                            [<30777.700.0>],\n                            []}}},\n                         <0.1428.0>,\n                         {state,fine,5000,#Ref<0.0.0.20652>},\n                         false,1,\n                         {{0,nil},{0,nil}},\n                         [],\n                         {{0,nil},{0,nil}},\n                         [{<<\"connection.blocked\">>,bool,true},\n                          {<<\"authentication_failure_close\">>,bool,true},\n                          {<<\"consumer_cancel_notify\">>,bool,true},\n                          {<<\"publisher_confirms\">>,bool,true},\n                          {<<\"basic.nack\">>,bool,true}],\n                         none,1,none,flow,[]}\n Reason for termination == \n** {bad_return_value,{cannot_compile_forms,error}}\n=ERROR REPORT==== 24-Jan-2017::11:32:45 ===\nError on AMQP connection <0.1427.0> ([::1]:34661 -> [::1]:5673, vhost: '/', user: 'guest', state: running), channel 1:\n{bad_return_value,{cannot_compile_forms,error}}\n=WARNING REPORT==== 24-Jan-2017::11:32:45 ===\nNon-AMQP exit reason '{bad_return_value,{cannot_compile_forms,error}}'\n=INFO REPORT==== 24-Jan-2017::11:32:45 ===\nclosing AMQP connection <0.1427.0> ([::1]:34661 -> [::1]:5673)\n```. Ok, I have published in rabbitmq-management repo.. ",
    "ash9002": "Noted - cheers :). ",
    "lukasgeyer": "As most of the script-based libraries only support long-long-uint, but not long-long-int this minimal working example uses a patched version of pika to generate a 4c instead of a 6c. Declaring any entity with a long-long-int field value seems to trigger the exception:\n+++ data.py     2017-01-31 08:34:07.000000000 +0100\n@@ -119 +119 @@\n-        pieces.append(struct.pack('>cq', b'l', value))\n+        pieces.append(struct.pack('>cq', b'L', value))\nimport pika\nconnection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\nchannel = connection.channel()\nchannel.queue_declare(queue='queueName', auto_delete=True, arguments={'fieldName': long(3000)})\n```\n=INFO REPORT==== 31-Jan-2017::08:43:41 ===\naccepting AMQP connection <0.7099.3> (127.0.0.1:42853 -> 127.0.0.1:5672)\n=CRASH REPORT==== 31-Jan-2017::08:43:41 ===\n  crasher:\n    initial call: rabbit_reader:init/4\n    pid: <0.7099.3>\n    registered_name: []\n    exception error: no function clause matching\n                     rabbit_binary_parser:parse_table(<<9,102,105,101,108,100,\n                                                        78,97,109,101,76,0,0,\n                                                        0,0,0,0,11,184>>) (src/rabbit_binary_parser.erl, line 50)\n      in function  rabbit_framing_amqp_0_9_1:decode_method_fields/2 (src/rabbit_framing_amqp_0_9_1.erl, line 786)\n      in call from rabbit_command_assembler:process/2 (src/rabbit_command_assembler.erl, line 81)\n      in call from rabbit_reader:process_frame/3 (src/rabbit_reader.erl, line 978)\n      in call from rabbit_reader:handle_input/3 (src/rabbit_reader.erl, line 1035)\n      in call from rabbit_reader:recvloop/4 (src/rabbit_reader.erl, line 446)\n      in call from rabbit_reader:run/1 (src/rabbit_reader.erl, line 428)\n      in call from rabbit_reader:start_connection/4 (src/rabbit_reader.erl, line 386)\n    ancestors: [<0.7097.3>,<0.277.0>,<0.276.0>,<0.275.0>,rabbit_sup,\n                  <0.153.0>]\n    messages: [{'EXIT',#Port<0.28188>,normal}]\n    links: [<0.7097.3>]\n    dictionary: [{{channel,1},\n                   {<0.7107.3>,{method,rabbit_framing_amqp_0_9_1}}},\n                  {process_name,\n                      {rabbit_reader,<<\"127.0.0.1:42853 -> 127.0.0.1:5672\">>}},\n                  {{ch_pid,<0.7107.3>},{1,#Ref<0.0.262145.55082>}}]\n    trap_exit: true\n    status: running\n    heap_size: 1598\n    stack_size: 27\n    reductions: 4046\n  neighbours:\n=SUPERVISOR REPORT==== 31-Jan-2017::08:43:41 ===\n     Supervisor: {<0.7097.3>,rabbit_connection_sup}\n     Context:    child_terminated\n     Reason:     function_clause\n     Offender:   [{pid,<0.7099.3>},\n                  {name,reader},\n                  {mfargs,\n                      {rabbit_reader,start_link,\n                          [<0.7098.3>,\n                           {acceptor,{0,0,0,0,0,0,0,0},5672},\n                           #Port<0.28188>]}},\n                  {restart_type,intrinsic},\n                  {shutdown,30000},\n                  {child_type,worker}]\n=SUPERVISOR REPORT==== 31-Jan-2017::08:43:41 ===\n     Supervisor: {<0.7097.3>,rabbit_connection_sup}\n     Context:    shutdown\n     Reason:     reached_max_restart_intensity\n     Offender:   [{pid,<0.7099.3>},\n                  {name,reader},\n                  {mfargs,\n                      {rabbit_reader,start_link,\n                          [<0.7098.3>,\n                           {acceptor,{0,0,0,0,0,0,0,0},5672},\n                           #Port<0.28188>]}},\n                  {restart_type,intrinsic},\n                  {shutdown,30000},\n                  {child_type,worker}]\nI agree that it does not make sense to use a signed integer for the TTL, but there seems to be a general problem with `long-long-int` (apart from the fact that the documentation requires a signed integer to be used, see https://www.rabbitmq.com/ttl.html#per-queue-message-ttl).. The initial client was `AMQP-CPP`, but the frame is the same, regardless of the client.\ninclude \ninclude \ninclude \nint main()\n{\n    auto evbase = event_base_new();\nAMQP::LibEventHandler handler(evbase);\n\nAMQP::TcpConnection connection(&handler, AMQP::Address(\"amqp://guest:guest@localhost/\"));\n\nAMQP::Table queueArguments;\nqueueArguments.set(\"x-message-ttl\", static_cast<std::int64_t>(3000));\n\nAMQP::TcpChannel channel(&connection);\nchannel.declareQueue(\"queueName\", queueArguments);\n\nevent_base_dispatch(evbase);\nevent_base_free(evbase);\n\nreturn 0;\n\n}\n```\n\nBut perhaps we can support signed values easily and safely, needs a bit of investigation.\n\nDoes this mean that RabbitMQ does not support *-int at all, or is it just long-long-int?\nI am fine with updating the documentation of Per-Queue Message TTL (and using long-long-uint instead), but is the fact that RabbitMQ does not support long-long-int documented somewhere too (if not it most probably should, shouldn't it)?. Out of curiosity, is there any negative impact on the server when this exception is thrown (except for the related client)?. Thanks for the reference, I had a look at the 0.9.1 specification only and wondered why RabbitMQ would behave different in this case and that there must be something going on.\nI agree with your middle ground solution, this should not break any existing client and will add (standards-compliant) support for those who use L. This is definitely a solution to our use case, thanks. . ",
    "ngoossens": "Thanks for the reply @michaelklishin. I will consider my arguments and decide whether a post to rabbitmq-users is required.. ",
    "edgework": "I'm an idiot. ",
    "OdellDotson": "@hairyhum Issue should be marked as closed, no?\nThis and this seem to have addressed the issue.\n. ",
    "selivan": "@michaelklishin \n\npre-create this file and others and set up the permissions\n\nWould not work, because file is not overwritten, but deleted and created again. I can see it because permissions and inode number are changed each time I run rabbitmq-plugins.\n\nSo how would whatever piece of code has to create enabled_plugins know what user should be the owner?\n\nIdeal solution is to overwrite enabled_plugins file instead of delete and create. Sufficient workaround is to set umask 0022 in rabbitmq-plugins shell script.. @gerhard \nWe just follow the admin guide. Commands are run as root only when we deploy new server.\nSeveral workarounds can be used to fix this: setting umask before running rabbitmq-plugins, creating RABBITMQ_ENABLED_PLUGINS_FILE by other tool, etc. But I think having it working out-of-the-box is better option :)\nSo I continue to insist that using umask 0022 in rabbitmq-plugins is one suitable solution, and making it overwrite file in place instead of delete/create is even better.. @michaelklishin\n\nSwitching to rewriting the file instead of creating and deleting it\nPre-creating a file in package post-installation scripts\n\nThis will solve the issue completely.. @michaelklishin Overwriting files instead of delete and create will solve the problem for some of the users, me included. It's easy to create files with proper permissions in initial deployment process, both for pre-built distributive packages and automation tools. And it does not break anything. So I think it's a good thing to implement it.\n. @hairyhum Then it would help if backup file owner and permissions were copied from original, not created from scratch. @michaelklishin @Gsantomaggio Installed deb package from https://github.com/rabbitmq/rabbitmq-server-release/pull/30#issuecomment-310658554.\nSGID bit on /etc/rabbitmq works, rabbitmq-plugins now works fine:\n```\numask\n0027\nrabbitmq-plugins disable rabbitmq_top\nThe following plugins have been disabled:\n  rabbitmq_top\nApplying plugin configuration to rabbit@work... stopped 1 plugin.\n```\nSo this fixes my use case. But systems with stricter umask like 0077 will still have problems. Complete solution will be to preserve file permissions either by explicit saving them or by rewriting old file. If that is not possible, this one IMHO is acceptable.. ",
    "roeintense": "I have to agree. This is causing issues on my rabbitmq_management enable as well. \n```\nrabbitmq-plugins enable rabbitmq_management\nPlugin configuration unchanged.\nApplying plugin configuration to rabbit@devbox... failed.\nError: {cannot_read_enabled_plugins_file,\"/etc/rabbitmq/enabled_plugins\",\n           eacces}\n```. ",
    "lalit65": "[root@ yum.repos.d]# systemctl status rabbitmq-server.service \n\u25cf rabbitmq-server.service - RabbitMQ broker\n   Loaded: loaded (/usr/lib/systemd/system/rabbitmq-server.service; disabled; vendor preset: disabled)\n  Drop-In: /etc/systemd/system/rabbitmq-server.service.d\n           \u2514\u2500limits.conf\n   Active: failed (Result: exit-code) since Fri 2018-07-06 01:12:18 IST; 1min 42s ago\n  Process: 16270 ExecStop=/usr/lib/rabbitmq/bin/rabbitmqctl stop (code=exited, status=0/SUCCESS)\n  Process: 16155 ExecStart=/usr/lib/rabbitmq/bin/rabbitmq-server (code=exited, status=1/FAILURE)\n Main PID: 16155 (code=exited, status=1/FAILURE)\nJul 06 01:12:18 work.in rabbitmqctl[16270]: attempted to contact: [rabbit@work]\nJul 06 01:12:18 work.in rabbitmqctl[16270]: rabbit@work:\nJul 06 01:12:18 work.in rabbitmqctl[16270]: * unable to connect to epmd (port 4369) on work: timeout (timed out)\nJul 06 01:12:18 work.in rabbitmqctl[16270]: current node details:\nJul 06 01:12:18 work.in rabbitmqctl[16270]: - node name: 'rabbitmq-cli-60@work'\nJul 06 01:12:18 work.in rabbitmqctl[16270]: - home dir: /var/lib/rabbitmq\nJul 06 01:12:18 work.in rabbitmqctl[16270]: - cookie hash: 9gQg7jaSnWAiFC3JXEukCg==\nJul 06 01:12:18 work.in systemd[1]: Failed to start RabbitMQ broker.\nJul 06 01:12:18 work.in systemd[1]: Unit rabbitmq-server.service entered failed state.\nJul 06 01:12:18 work.in systemd[1]: rabbitmq-server.service failed.\n[root@work yum.repos.d]# systemctl start rabbitmq-server.service \nJob for rabbitmq-server.service failed because the control process exited with error code. See \"systemctl status rabbitmq-server.service\" and \"journalctl -xe\" for details.\n[root@work yum.repos.d]# \n. ",
    "BlaM": "That thread looks very similar. Thank you. Might be able to find my solution there.. I think the affected server is on Erlang 17.3 (according to the installed Debian package).. ",
    "robvelor": "@michaelklishin I'm seeing this issue on 3.6.9 with erlang 19 on docker image of 3.6.9.. ",
    "dcarwin-pivotal": "Invalid topic ID.  Are you referring to this thread?\n\n\nhttps://groups.google.com/d/msg/rabbitmq-users/XfQgta5v6Z0/1V6y2h8GFAAJ\n\n\nOn Wed, Mar 15, 2017 at 5:04 PM, Michael Klishin notifications@github.com\nwrote:\n\nSee https://groups.google.com/forum/m/#!topic/rabbitmq-users/XfQgta5v6Z.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1149#issuecomment-286918244,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFDVVJa41w2P8i8StswwvuVDHZ7rPrAFks5rmHyRgaJpZM4MenXO\n.\n\n\n-- \nDaniel Carwin | dcarwin@pivotal.io | 415-425-0307 |\nRabbitMQ | tc Server | Web Server | Pivotal App Suite |\n. ",
    "benauthor": "Great, thanks for the pointer to that thread, it appears to address my issue.. ",
    "jabbink": "Can confirm, installing esl-erlang 19.3 fixed it for me.. ",
    "marlier": "Appreciate the quick response, guys!  I PR'd ansible (the one specific example that I know of): https://github.com/ansible/ansible/pull/22715. ",
    "powersj": "To make sure I am following you, by parent directory is it trying to create \"/var/log/rabbitmq\"? Are there more clues for me to gather to understand why that would that fail?. ok I think what I had missing is understanding RabbitMQ's user permissions, as I was running as root.\nCompletely understand about the package scripts. The scenario in this case was using tmpfs for /var/log; I guess the answer is, \"don't do that\". Thanks!. Awesome, thanks for taking the time to help clear that up in my head :). ",
    "waleedsaud": "thank you.. ",
    "lukas-krecan-lt": "Thanks. We were sending messages using AMQP and receiving them using MQTT and did not use message ID. Adding message ID to AMQP publish helped (so far).. I am still not sure that it really helped, I will confirm it next week.. Sorry, setting message ID did not help. ",
    "saterHATER": "If I'm not mistaken, this is a working link to rabbitMQ-users. My apologies Michael I don't think your link was working.. ",
    "893271511": "{\n    \"global_prefetch_count\": 0,\n    \"messages_unconfirmed\": 0,\n    \"node\": \"rabbit@test\",\n    \"name\": \"10.32.64.120:50303 -> 10.32.55.40:5672 (6)\",\n    \"confirm\": false,\n    \"transactional\": true,\n    \"state\": \"flow\",\n    \"consumer_count\": 1,\n    \"idle_since\": \"2017-03-23 14:59:41\",\n    \"vhost\": \"/\",\n    \"acks_uncommitted\": 0,\n    \"prefetch_count\": 1,\n    \"message_stats\": {\n        \"publish_details\": {\n            \"rate\": 0.0\n        },\n        \"ack\": 3,\n        \"deliver_get\": 3,\n        \"deliver\": 3,\n        \"publish\": 511,\n        \"ack_details\": {\n            \"rate\": 0.0\n        },\n        \"deliver_details\": {\n            \"rate\": 0.0\n        },\n        \"deliver_get_details\": {\n            \"rate\": 0.0\n        }\n    },\n    \"connection_details\": {\n        \"peer_host\": \"10.32.64.120\",\n        \"name\": \"10.32.64.120:50303 -> 10.32.55.40:5672\",\n        \"peer_port\": 50303\n    },\n    \"messages_uncommitted\": 0,\n    \"number\": 6,\n    \"messages_unacknowledged\": 0,\n    \"user\": \"admin\"\n}\n\u8fd9\u662f\u7528/api/channels\u53d6\u51fa\u7684\u4fe1\u606f\uff0c\u8fd9\u4e2achannel\u5f53\u524d\u6ca1\u6709\u751f\u4ea7\u6d88\u606f \"publish_details\": {\"rate\": 0.0\uff0c\u4f46state\u4e3aflow\uff1f. ",
    "micdenny": "Sorry I had to push this in https://github.com/rabbitmq/rabbitmq-management/issues \nMaybe is a duplicate of https://github.com/rabbitmq/rabbitmq-management-agent/issues/34? but I'm using erlang 19.0 and I also tested RabbitMQ v3.6.8 with same error\nIf there's no consumer the queue is accessible.. > How can this be reproduced so that I can file a new issue there?\nI'm using EasyNetQ, just install the latest version from nuget, then this is the c# code:\n```csharp\nusing System;\nusing EasyNetQ;\nnamespace BadTermIssue\n{\n    public class Program\n    {\n        public static void Main(string[] args)\n        {\n            var bus = RabbitHutch.CreateBus(\"host=127.0.0.1\", x => x.Register(s => new EasyNetQ.Loggers.ConsoleLogger()));\n        bus.Receive<MyMessage>(\"BAD_TERM_ISSUE\", message => { });\n\n        Console.WriteLine(\"Press enter to stop the consumer and exit the application...\");\n        Console.ReadLine();\n\n        bus.Dispose();\n    }\n}\n\npublic class MyMessage\n{\n}\n\n}\n```\nWhen it's running try to access the queue -> http://localhost:15672/#/queues/%2F/BAD_TERM_ISSUE. tested and it works like a charm. ",
    "szank": "Hi, \n1. We are using 3.6.6 on FreeBSD 11. The maintained have not yet pushed out the 3.6.8\n2. I mixed up management interface and clustering interface. Sorry about that. My rabbitmq-05 node have clustering connection open to the other two nodes. \n\nIn the case I have described, a connectivity issue between two nodes, have brought both of them down, leaving single point of failure for the 3 node cluster. \n\nWe are more worried about the last node failing, and us loosing some messages, than by having a downtime. Loosing messages is much more costly for us. \nMongoDB for example handles it much better. I have filed this bug because I was expecting MongoDB-like behaviour with Rabbitmq, where problems on one node, doesn't create a single point of failure for the cluster. \nI am fine with moving it to the mailing list. \n. ",
    "NicholasMarty": "@michaelklishin I have another similiar issue where the partition handling isn't quite working as I'd expect it to work based on the description. The setup is kinda identical to the one mentioned by szank (3 nodes, each configured with pause_minority). Is this discussion continued somewhere (and if so: where can I find it?) or should I add my problem either in this issue, a new issue or somewhere else? . ",
    "pruthaMunshi": "@michaelklishin Thank you so much for your valuable response I really appreciate it :) \n. ",
    "chernser": "Why the issue is closed? The link to forum is broken. . Yes, I have to start the rabbitmq because is becomes in state \"not running\" \n. @michaelklishin I mean, that server should not crash when consumer dies and queue has alot messages. I clearly understand, that we need to have limit and they are solving the problem. \nBut, the root cause of this BUG seems in way queue is cleared or in wrong consumer termination logic (for example, when consumer dies, RMQ tries to fetch all messages from mnesia) \n. @michaelklishin the problem is in why RabbitMQ crashes when consumers disconnect. What is not clear? \nWhy RabbitMQ doesn't crash when there is no consumer? . ",
    "adfinlay": "The config file is pretty bare:\n```\n[\n{rabbit,\n[\n        {vm_memory_high_watermark, 0.65}\n]\n},\n{rabbitmq_web_stomp, [{port, 15674}]}\n].\n```\nWhen these errors appear, RabbitMQ stops accepting connections and current connections are closed, I assumed that those errors were related as they are the only thing that happens at the time of the issue. The management interface is also unreachable when this happens.. ",
    "srenatus": "@michaelklishin that's up to you -- I don't need it urgently \ud83d\ude03 . @michaelklishin I was just poking around the code when investigating various options around controlling epmd better. One option was getting rid of epmd and going the route outlined here, which would have made this necessary.\nIs that \"other case\" you've seen before documented publicly? I'd be curious about that, too \ud83d\ude03 . @michaelklishin thanks for sharing :+1:. ",
    "BugRaptor": "Ok, Sorry for the misplacement. I copied myy question on the rabbitmq-user mailing list.\n. ",
    "ryanvgates": "@BugRaptor can you include a link to the discussion on the rabbitmq-user mailing list?. ",
    "sp3c1": "Hey, well I kind of have one, yet but unrelated to that. A delayed queue of different expiration times on messages, where you would set priority to some ridiculous large int representing the UNIX timestamp so that short lived messages wont queue behind a long running one scheduled previously. Something like #1449 \nYeah, I know that I can just have a queue of set expiration and then the whole ordering thing goes away, but I am looking into having one queue per functionality, not the time frame.. I kind of hit the wall at the same place. \n~~Easiest workaround I have found so far is to queue a message with some highest priority (unavailable for other messages) and no ttl. Then to read and re queue this message in intervals (whatever will be acceptable for you). It seems to release all the expired messages with lower priorities.~~\n~~I have being reading from queue non programatically so far with http admin, but have a strong feeling that prefetch will probably work as well - with no need to re queue.~~\neh, works only if the next message would expire, so if you have two low prior long running it will still get blocked.\n. ",
    "fiksn": "Yep, rabbitmq-autocluster with k8s support works like a charm w/ 3.6.9 RabbitMQ.\nWhat was irritating me was the \".no-domain\", but it seems that is used only for CLI tools not by rabbitmq-server itself. Thanks for the links and sorry for opening the invalid ticket.\n. ",
    "bitnitdit": "Hi, @michaelklishin:\nYou've said,\"CLI node names are intentionally made unique and RABBITMQ_NODENAME is not supposed to be used by CLI tools.\"\nBut in this link: https://www.rabbitmq.com/clustering.html, section \"A Cluster on a Single Machine\", it still uses RABBITMQ_NODENAME, \nRABBITMQ_NODE_PORT=5672 RABBITMQ_NODENAME=rabbit rabbitmq-server -detached\nRABBITMQ_NODE_PORT=5673 RABBITMQ_NODENAME=hare rabbitmq-server -detached\nand the steps this section provides do not work, that is, using those steps can not setup a rabbitmq cluster on a single machine. \nIs there any other method to setup a rabbitmq cluster on a single machine? Thanks!\n. @lukebakken, @michaelklishin , thank you all very much! . ",
    "abelgo": "Sorry didn't want to publicly post the security info\nThe config appears noting out of the ordinary.\nThis is a self signed cert.\n``` erlang\n[\n{rabbit, [\n {reverse_dns_lookups, true},\n\n {ssl_listeners, [5671]},\n\n {ssl_options, [\n                {depth, 4},\n                {server_renegotiate, true},\n                {cacertfile,\"/etc/rabbitmq/cert/cacert.pem\"},\n                {certfile,\"/etc/rabbitmq/cert/cert.pem\"},\n                {keyfile,\"/etc/rabbitmq/cert/key.pem\"}\n\n               ]}\n\n]}\n   ,\n{rabbitmq_management,\n  [{listener, [{port, 15672},\n               {ssl,  true},\n               {ssl_opts, [\n                    {cacertfile,\"/etc/rabbitmq/cert/cacert.pem\"},\n                    {certfile,\"/etc/rabbitmq/cert/cert.pem\"},\n                    {keyfile,\"/etc/rabbitmq/cert/key.pem\"}\n              ]} \n              ]}\n             ]} \n   , \n{rabbitmq_web_stomp,\n      [{ssl_config, [{port,       61617},\n                     {backlog,    1024},\n                     {password,   \"testertester\"},\n                     {cacertfile,\"/etc/rabbitmq/cert/cacert.pem\"},\n                     {certfile,\"/etc/rabbitmq/cert/cert.pem\"},\n                     {keyfile,\"/etc/rabbitmq/cert/key.pem\"}\n                 ]}\n  ]}\n\n].\n```. Ok guys I will do some more testing tonight then generate some better logs to try and figure out whats happening . Perfect. I'm not particularly familiar whit Erlang or ranch the config was something on a cluster I set up almost 4 years ago. But that did that trick. Thanks for taking the time to investigate the issue. Much appreciated  . ",
    "rmoriz": "besides that, IMHO it's the wrong service, anyway:\n\u25cf rabbitmq-server.service - RabbitMQ broker\n   Loaded: loaded (/usr/lib/systemd/system/rabbitmq-server.service; enabled; vendor preset: disabled)\n   Active: activating (start) since Sat 2017-04-22 15:19:07 UTC; 11min ago\n Main PID: 202 (beam.smp)\n   CGroup: /docker/01b3d8c9ee668b5396d2d374bb279181567a5f73840432192d5bd9bb62b14eea/system.slice/rabbitmq-server.service\n           \u251c\u2500202 /usr/lib64/erlang/erts-5.10.4/bin/beam.smp -W w -A 64 -P 1048576 -t 5000000 -stbt db -zdbbl 32000 -K true -- -root /usr/...\n           \u251c\u2500426 inet_gethost 4\n           \u2514\u2500427 inet_gethost 4\n           \u2023 202 /usr/lib64/erlang/erts-5.10.4/bin/beam.smp -W w -A 64 -P 1048576 -t 5000000 -stbt db -zdbbl 32000 -K true -- -root /usr/...\nhowever:\n```\nsystemctl status 202\n\u25cf -.slice - Root Slice\n   Loaded: loaded (/usr/lib/systemd/system/-.slice; static; vendor preset: disabled)\n   Active: active since Sat 2017-04-22 15:19:06 UTC; 13min ago\n     Docs: man:systemd.special(7)\n   CGroup: /docker/01b3d8c9ee668b5396d2d374bb279181567a5f73840432192d5bd9bb62b14eea\n           \u251c\u25001 /usr/lib/systemd/systemd\n           \u2514\u2500system.slice\n             \u251c\u2500dbus.service\n             \u2502 \u2514\u250057 /bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation\n             \u251c\u2500rabbitmq-server.service\n             \u2502 \u251c\u2500202 /usr/lib64/erlang/erts-5.10.4/bin/beam.smp -W w -A 64 -P 1048576 -t 5000000 -stbt db -zdbbl 32000 -K true -- -root /...\n             \u2502 \u251c\u2500426 inet_gethost 4\n             \u2502 \u2514\u2500427 inet_gethost 4\n             \u251c\u2500system-epmd.slice\n             \u2502 \u2514\u2500epmd@0.0.0.0.service\n             \u2502   \u2514\u2500314 /usr/bin/epmd -systemd\n             \u251c\u2500system-getty.slice\n             \u2502 \u2514\u2500getty@tty1.service\n             \u2502   \u2514\u250072 /sbin/agetty --noclear tty1 linux\n             \u251c\u2500systemd-logind.service\n             \u2502 \u2514\u250067 /usr/lib/systemd/systemd-logind\n             \u251c\u2500systemd-udevd.service\n             \u2502 \u2514\u250028 /usr/lib/systemd/systemd-udevd\n             \u2514\u2500systemd-journald.service\n               \u2514\u250022 /usr/lib/systemd/systemd-journald\n```. You can easily reproduce it by using your very own chef recipes on e.g. macOS:\n```\nget docker for mac + chefdk\ndocker -v || brew cask install docker \n/opt/chefdk/bin/chef -v || brew cask install chefdk\ngit clone https://github.com/rabbitmq/chef-cookbook\ncd chef-cookbook\nKITCHEN_LOCAL_YAML=.kitchen.dokken.yml /opt/chefdk/embedded/bin/kitchen converge default-centos-72\n```\nto get a shell while this job is running/hanging, start in the same direcotry:\nKITCHEN_LOCAL_YAML=.kitchen.dokken.yml /opt/chefdk/embedded/bin/kitchen login default-centos-72. @michaelklishin I really tried to give as much hints as possible. It doesn't work for your own CI\u2026 https://travis-ci.org/rabbitmq/chef-cookbook/jobs/221400408. Srsly, did you read the issue? I provided examples, logs, path to your erlang source and even a recipe to run everything local. And all of that code is maintained by your org.. @michaelklishin I guess you need to involve someone who wrote the code and understands the systemd logic (maybe @binarin ?)\nThe issues start with looking up the current service by shelling out and looking up the pid: https://github.com/rabbitmq/rabbitmq-server/blob/e07ca0eacc0f2db77685a48253b3457a22c0e269/src/rabbit.erl#L410\n (systemctl status 202 in my example). \nThat returns '-.slice' which then is used for another shell-out without shell escaping to get the state - and that crashes and no notification is ever sent to systemd, leaving the job hanging (\"activating\").\nhttps://github.com/rabbitmq/rabbitmq-server/blob/e07ca0eacc0f2db77685a48253b3457a22c0e269/src/rabbit.erl#L433\n(see https://www.freedesktop.org/software/systemd/man/sd_notify.html for the systemd details)\nThe other issue is, that IMHO the pid (202 in my example) actually (IMHO) should belong to rabbitmq-server.service and not to the root slice (-.slice) anyway.\nI started reverseing the issue by grepping thethe journald/systemd error message in your erlang source code.. Your CI looks good, but you're using /sbin/init as pid1 command and the chef docker-setup uses  /usr/lib/systemd/systemd (not sure if this is a problem yet). I got happy news for you.  \ud83d\udc4d \nThe only issue left here is the un-escaping which may hurt if someone decides to run rabbitmq with a unit-name that requires escaping. I guess it's minor. Feel free to close or discuss/fix internally.\nThis issue covered (at least) another two issues which are a part of the chef-cookbook repo:\n\n\nLooks like Docker container need the cgroup fs binding even when they run privilged. Then rabbitmq gets the right service (not -.slice) and starts as expected (also because rabbitmq-server.service does not need escaping). \n  This is broken in the chef-cookbook CI setup but also not configured by almost all chef-cookbook systemd CI setups on GitHub yet. I guess this is not an issue because most software projects are too lazy to implement sd_notify and other \"advanced\" systemd features yet. \ud83d\ude29\n\n\nchef-cookbook tests still fail due to other reasons. I'll continue my deep journey (with PRs) in https://github.com/rabbitmq/chef-cookbook/issues/435\n\n\nSorry for the hassle\u2026\n. @lukebakken see opening post. But as I wrote, this only occurs if you don't proper mount hosts' /sys/fs/cgroup into a container. This is required and documented but was not very well known among tooling https://developers.redhat.com/blog/2016/09/13/running-systemd-in-a-non-privileged-container/\nJust to clarify, this only happens when:\n\nWithout /sys/fs/cgroup mounted, the unit will be resolved/named -.slice\nnot properly escaping (the leading) '-' causes the failure for which I opened this issue.\n\n. ",
    "axot": "Any updates for this bug?. @michaelklishin Is there any side-effects if we change Type=notify to Type=simple?\nhttps://github.com/rabbitmq/rabbitmq-server/blob/c26f0b567b09bfc4403198339cd0f2d207b83215/docs/rabbitmq-server.service.example#L8. ",
    "tbennett6421": "This is affecting our rabbitmq systems as well.. weird /sys/fs/cgroup is not empty for us. may be a different issue then.. ",
    "elw00d": "will this patch be included into the next milestone pre-release ?. Actually I'm just waiting for the next 3.6 release \ud83d\ude03 (because can't build it from master branch to deb package). And yesterday I saw that pre-release of milestone 3.6.10 is ready, but there are no IPv6 commit included to its tag.. ok, thanks for clarification. Ok.. and what about rabbitmq management plugin ? @michaelklishin . ",
    "yebangyu": "I have solved this......\nSo bad,  ./rabbitmqctl join_cluster rabbit@cluster_node1 failed\nbut\n./rabbitmqctl join_cluster rabbit@st-dz-rs761 succeeeeeed !!!!!!. ",
    "sodre": "@gerhard I can change all of them as well but it is probably not needed. The reason is the other runnable scripts, are not using \"trap\". \nLet me know if you still want the others changed and I will make it happen.. Changed it WIP, I am trying to replicate it in another system.. Folks,\nHere are the steps to replicate the bug on your own using a blank Docker container.\n```bash\ndocker run -it --rm ubuntu bash\napt-get update\napt-get install -y wget  xz-utils erlang-asn1 erlang-base erlang-corba erlang-diameter erlang-edoc erlang-eldap erlang-erl-docgen erlang-eunit erlang-ic erlang-inets erlang-mnesia erlang-nox erlang-odbc erlang-os-mon erlang-parsetools erlang-percept erlang-public-key erlang-runtime-tools erlang-snmp erlang-ssh erlang-ssl erlang-tools erlang-webtool erlang-xmerl libltdl7 libodbc1 libpopt0\nwget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.9/rabbitmq-server-generic-unix-3.6.9.tar.xz\ntar -xf rabbitmq-server-generic-unix-3.6.9.tar.xz \n./rabbitmq_server-3.6.9/sbin/rabbitmq-server\nWait until all the plugins are loaded and hit `ctrl-c`. Your terminal should look like this..\nroot@3cf4b2669034:/# ./rabbitmq_server-3.6.9/sbin/rabbitmq-server\n          RabbitMQ 3.6.9. Copyright (C) 2007-2016 Pivotal Software, Inc.\n\n##  ##      Licensed under the MPL.  See http://www.rabbitmq.com/\n  ##  ##\n  ##########  Logs: /rabbitmq_server-3.6.9/var/log/rabbitmq/rabbit@3cf4b2669034.log\n  ######  ##        /rabbitmq_server-3.6.9/var/log/rabbitmq/rabbit@3cf4b2669034-sasl.log\n  ##########\n              Starting broker...\n completed with 0 plugins.\n^Croot@3cf4b2669034:/# \nThe shell-script quit but `epmd` and `beam` are still running in the background. You can type `ps aux` to verify. After that runbash\npkill epmd && pkill beam.smp\nIf we \"apply the patch\", i.e. use `sed`, and do the same process this is what we get:\nroot@3cf4b2669034:/# sed -i 's|/bin/sh -e|/usr/bin/env bash|' ./rabbitmq_server-3.6.9/sbin/rabbitmq-server \nroot@3cf4b2669034:/# ./rabbitmq_server-3.6.9/sbin/rabbitmq-server\n          RabbitMQ 3.6.9. Copyright (C) 2007-2016 Pivotal Software, Inc.\n\n##  ##      Licensed under the MPL.  See http://www.rabbitmq.com/\n  ##  ##\n  ##########  Logs: /rabbitmq_server-3.6.9/var/log/rabbitmq/rabbit@3cf4b2669034.log\n  ######  ##        /rabbitmq_server-3.6.9/var/log/rabbitmq/rabbit@3cf4b2669034-sasl.log\n  ##########\n              Starting broker...\n completed with 0 plugins.\n^CStopping and halting node rabbit@3cf4b2669034 ...\nGracefully halting Erlang VM\nroot@3cf4b2669034:/# \n```\nThe reason I am reproducing the bug is because the dash man page says that it supports traps, and a simple example code for dash works out of the box. Unfortunately, there is something with the rabbitmq-server script that is making the code fail. Using bash is one possible fix.. I like that! I will test it on my system and change the patch later today. . We will be going with the \"wait\" trick. However, I think the answer is simpler than the second proposed patch. Using the first patch, I added a silly If statement to look at the result of wait, surprisingly it works! \nWhen you guys have time please verify it also works for you and that I am not getting things messed up. In particular testing it for FreeBSD would be great.\n```sh\n!/bin/sh\nset -e\ndo_things () {\n  sleep 20\n}\ntrap \"echo 'Got SIGINT'\" INT\ndo_things &\nif wait $!; then\n  :\nfi\n```. Michael,\n1200 is the answer to https://github.com/rabbitmq/rabbitmq-server/pull/1192#issuecomment-297187882 when you asked to rebase the patch into \"stable\" if I wanted the fix in 3.6.\n1192 is the same exact change but based in master.\nWhat I was asking for you guys to look at is if the script in https://github.com/rabbitmq/rabbitmq-server/pull/1192#issuecomment-297592851 also work for you. It was mostly out of disbelief since the only change required is this \"NoOp\" with the wait statement.\nI tested the script in my Ubuntu box that was failing before, and with the \"NoOp\"   it works well!. @gerhard,\nI think I got it now. Please review at your earliest convenience. \nI also fixed the beginning of the code so that the \"set -e\" call is consistent with the other scripts in master. This was different for the \"stable\" branch PR #1200 , the scripts there were called with #!/bin/sh -e.\nI hope that covers all you guys need. It was awesome working on this PR, thanks for giving me the chance to contribute to it.. No problem. these are all good points. I will take care of them soon.\n. ",
    "sandeeppalla": "Thank you very much for your reply michael.\nCan you help me with how to troubleshot \" times out of 10 it's due to a Erlang cookie (a shared secret) mismatch\" .\nI am new to this RabbitMQ.\nRegards,\nSandeep Palla\n. ",
    "balajik1992": "network settings related to rabbitmq are as below\nfs.file-max = 1614110\nnet.core.somaxconn = 128\nnet.ipv4.conf.default.rp_filter = 1\nnet.ipv4.ip_local_port_range = 32768    60999\nnet.ipv4.tcp_fin_timeout = 60\nnet.ipv4.tcp_max_syn_backlog = 512\nnet.ipv4.tcp_tw_reuse = 0\n. server log as below\n=INFO REPORT==== 28-Apr-2017::08:01:42 ===\nStarting RabbitMQ 3.6.9 on Erlang 19.3\nCopyright (C) 2007-2016 Pivotal Software, Inc.\nLicensed under the MPL.  See http://www.rabbitmq.com/\n=INFO REPORT==== 28-Apr-2017::08:01:42 ===\nnode           : rabbit@sd-0aad-6587\nhome dir       : /home/aimops\nconfig file(s) : /etc/rabbitmq/rabbitmq.config\ncookie hash    : yl7M4HTRmS+l2YF5Tc3RYg==\nlog            : /var/log/rabbitmq/rabbit@host.log\nsasl log       : /var/log/rabbitmq/rabbit@host.log\ndatabase dir   : /var/lib/rabbitmq/mnesia/rabbit@host\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nMemory limit set to 6351MB of 15879MB total.\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nDisk free limit set to 50MB\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nLimiting to approx 924 file handles (829 sockets)\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nFHC read buffering:  OFF\nFHC write buffering: ON\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nWaiting for Mnesia tables for 30000 ms, 9 retries left\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nWaiting for Mnesia tables for 30000 ms, 9 retries left\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nPriority queues enabled, real BQ is rabbit_variable_queue\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nStarting rabbit_node_monitor\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nmsg_store_transient: using rabbit_msg_store_ets_index to provide index\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nmsg_store_persistent: using rabbit_msg_store_ets_index to provide index\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nstarted SSL Listener on [::]:5671\n=INFO REPORT==== 28-Apr-2017::08:01:43 ===\nServer startup complete; 0 plugins started.. ",
    "ebelew": "Before this patch, the env was getting set with the following command:\nset ERL_LIBS=;C:\\PROGRA~1\\RABBIT~1\\RABBIT~1.6\\plugins\n\nAfter many trials and tribulations involving writing a custom batch script using the output of set and testing various variables, I found that if I manually set the ERL_LIBS value with:\nset ERL_LIBS=C:\\PROGRA~1\\RABBIT~1\\RABBIT~1.6\\plugins\n\nrabbitmqctl.bat, rabbitmq-server.bat and others ran correctly without causing the error erl.exe has stopped. Therefor, I added logic to remove the empty item at the beginning of the list.\nWhile this is most likely technically a bug in erlang, this workaround allowed me to complete setup and get RMQ Server running on my local windows server.\n. ",
    "LeeFowlerCU": "I'm curious as to why ~/.erlang.cookie would be written to the filesystem if the cookie is passed to the VM as a command line parameter.. @michaelklishin Thanks for your help. That does indeed seem to be the best course of action here.. It's the default behavior of the Erlang VM to create the erlang.cookie file if no -setcookie argument is supplied on the command line, which is why it's interesting that it would be both respectful of the -setcookie supplied by the RABBITMQ_SERVER_ADDITIONAL_ERL_ARGS environment variable AND also creating the .erlang.cookie file as well.  . ",
    "rain10154": "I'm so sorry for this! Thank you very much!. ",
    "jutkko": "@vlad-stoian. ",
    "xiexianbin": "when i update my rabbitmq cluster from 3.6.2-1 to 3.6.10-1, Appear again.. ",
    "quixoten": "Yeah, that output is from the list_policies command. I used\nsudo rabbitmqctl set_policy test_policy \"full_queue_name\" '{\"ha-mode\":\"nodes\",\"ha-params\":[\"node1\",\"node2\"],\"ha-sync-mode\":\"automatic\"}' --priority 200 --apply-to queues\nto apply the test policy. Here's a link to the forum post: https://groups.google.com/forum/#!topic/rabbitmq-users/lFjy51tPZsM. ",
    "pydevd": "@michaelklishin, thanks for reply. I'll use rabbitmq-users for future questions. \n\nempirical observations such as those provided above are not really evidence of write synchronisation with heartbeat frames.\n\nI understand, also, they should not be connected anyway, I understand this. My experiments just saying that in Workflow #2 example, delay before task will be read from rabbitmq connection socket will be equal to hearbeat interval. I've checked same scenario many times with different hearbeat interval values and result is always the same: delay time == hearbeat timeout.\nI will provide more details later according to your list of necessary things.\nthanks.. ",
    "thomas-riccardi": "Documentation is missing for total_memory_available_override_value at https://www.rabbitmq.com/configure.html (or anywhere else on the website it seems).. Yes indeed, but the https://www.rabbitmq.com/configure.html page says its list is the complete list of configurable variables:\n\nVariables Configurable in rabbitmq.config\nMany users of RabbitMQ never need to change any of these values, and some are fairly obscure. However, for completeness they are all listed here. \n\nIt currently is not complete.. Perfect, thanks !\nI was not aware of this repository, I did search some source link from the footer and could not find any: it could be made more visible.. ",
    "AlexCppns": "@michaelklishin Thanks for the quick reply. I have tried to disable/enable the plugins without luck. I am trying to gather more information and make a post in the Google Group\nEdit: I made a post here: https://groups.google.com/forum/#!topic/rabbitmq-users/AwTIH75b2u0. ",
    "djenriquez": "Thanks for the response @michaelklishin.\nMaybe its something I'm doing wrong, but it doesn't look like 3.6.8 works either:\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:22 ===\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: Starting RabbitMQ 3.6.8 on Erlang 19.3\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: Copyright (C) 2007-2016 Pivotal Software, Inc.\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: Licensed under the MPL.  See http://www.rabbitmq.com/\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]:               RabbitMQ 3.6.8. Copyright (C) 2007-2016 Pivotal Software, Inc.\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]:   ##  ##      Licensed under the MPL.  See http://www.rabbitmq.com/\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]:   ##  ##\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]:   ##########  Logs: tty\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]:   ######  ##        tty\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]:   ##########\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]:               Starting broker...\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:22 ===\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: node           : rabbit@ip-10-2-8-119\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: home dir       : /var/lib/rabbitmq\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: config file(s) : /usr/lib/rabbitmq/etc/rabbitmq/rabbitmq.config\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: cookie hash    : iqG7DCBA+lxNNLQq/Y6efg==\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: log            : tty\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: sasl log       : tty\nJun  1 17:35:22 ip-10-2-8-119 rabbitmq[2737]: database dir   : /var/lib/rabbitmq/mnesia\nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:24 ===\nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: autocluster: log level set to debug\nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:24 ===\nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: autocluster: Using AWS backend\nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:24 ===\nJun  1 17:35:24 ip-10-2-8-119 rabbitmq[2737]: autocluster: Delaying startup for 323ms.\nJun  1 17:35:25 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:25 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:25 ===\nJun  1 17:35:25 ip-10-2-8-119 rabbitmq[2737]: autocluster: Starting aws registration.\nJun  1 17:35:25 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:25 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:25 ===\nJun  1 17:35:25 ip-10-2-8-119 rabbitmq[2737]: autocluster: Setting region: \"eu-central-1\"\n.\n.\n.\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:26 ===\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: autocluster: Fetching autoscaling = DNS: [\"ip-10-2-11-6.eu-central-1.compute.internal\",\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]:                                           \"ip-10-2-11-82.eu-central-1.compute.internal\",\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]:                                           \"ip-10-2-8-119.eu-central-1.compute.internal\"]\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:26 ===\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: autocluster: Discovered ['rabbit@ip-10-2-11-6','rabbit@ip-10-2-11-82',\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]:                          'rabbit@ip-10-2-8-119']\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:26 ===\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: autocluster: Node is already in the cluster\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:26 ===\nJun  1 17:35:26 ip-10-2-8-119 rabbitmq[2737]: Memory limit set to 794MB of 993MB total.\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:27 ===\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: Disk free limit set to 50MB\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:27 ===\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: Limiting to approx 924 file handles (829 sockets)\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:27 ===\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: FHC read buffering:  OFF\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: FHC write buffering: OFF\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:27 ===\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: Waiting for Mnesia tables for 30000 ms, 9 retries left\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: =CRASH REPORT==== 1-Jun-2017::17:35:27 ===\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:   crasher:\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     initial call: application_master:init/4\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     pid: <0.194.0>\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     registered_name: []\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     exception exit: {{schema_integrity_check_failed,\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:                          [{table_attributes_mismatch,rabbit_listener,\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:                               [node,protocol,host,ip_address,port,opts],\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:                               [node,protocol,host,ip_address,port]}]},\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:                      {rabbit,start,[normal,[]]}}\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:       in function  application_master:init/4 (application_master.erl, line 134)\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     ancestors: [<0.193.0>]\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     messages: [{'EXIT',<0.195.0>,normal}]\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     links: [<0.193.0>,<0.31.0>]\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     dictionary: []\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     trap_exit: true\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     status: running\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     heap_size: 987\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     stack_size: 27\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     reductions: 98\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:   neighbours:\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: \nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]: =INFO REPORT==== 1-Jun-2017::17:35:27 ===\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     application: rabbit\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     exited: {{schema_integrity_check_failed,\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:                  [{table_attributes_mismatch,rabbit_listener,\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:                       [node,protocol,host,ip_address,port,opts],\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:                       [node,protocol,host,ip_address,port]}]},\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:              {rabbit,start,[normal,[]]}}\nJun  1 17:35:27 ip-10-2-8-119 rabbitmq[2737]:     type: temporary\nTrying 3.6.7. Ahhh I am doing something wrong \ud83d\ude04  thanks, i'll give that a shot. ",
    "M22an": "Thanks for your response @michaelklishin. I will use the google groups list going forward.. ",
    "oneiros-de": "Well, the page on erlang currently says \"Erlang/OTP 20 is NOT supported due to breaking changes\" - it would be nice if the packages would reflect that.\nAnd yes, we have implemented apt pinnig.. @lukebakken I tested the setting; no change. The system is a VM with one core.. ",
    "FabianPonce": "See #1253 for PR against stable branch. Thanks for the quick review, guys!. ",
    "lskbr": "rabbitmqctl environment\nApplication environment of node rabbit@chipp...\n[{amqp_client,[{prefer_ipv6,false},{ssl_options,[]}]},\n {asn1,[]},\n {compiler,[]},\n {cowboy,[]},\n {cowlib,[]},\n {crypto,[]},\n {inets,[]},\n {kernel,\n     [{error_logger,tty},\n      {inet_default_connect_options,[{nodelay,true}]},\n      {inet_default_listen_options,[{nodelay,true}]},\n      {inet_dist_listen_max,25672},\n      {inet_dist_listen_min,25672}]},\n {mnesia,[{dir,\"/var/lib/rabbitmq/mnesia/rabbit@chipp\"}]},\n {os_mon,\n     [{start_cpu_sup,false},\n      {start_disksup,false},\n      {start_memsup,false},\n      {start_os_sup,false}]},\n {public_key,[]},\n {rabbit,\n     [{auth_backends,[rabbit_auth_backend_internal]},\n      {auth_mechanisms,['PLAIN','AMQPLAIN']},\n      {background_gc_enabled,false},\n      {background_gc_target_interval,60000},\n      {backing_queue_module,rabbit_priority_queue},\n      {channel_max,0},\n      {channel_operation_timeout,15000},\n      {cluster_keepalive_interval,10000},\n      {cluster_nodes,{[],disc}},\n      {cluster_partition_handling,ignore},\n      {collect_statistics,fine},\n      {collect_statistics_interval,5000},\n      {config_entry_decoder,\n          [{cipher,aes_cbc256},\n           {hash,sha512},\n           {iterations,1000},\n           {passphrase,undefined}]},\n      {credit_flow_default_credit,{400,200}},\n      {default_permissions,[<<\".*\">>,<<\".*\">>,<<\".*\">>]},\n      {default_user,<<\"guest\">>},\n      {default_user_tags,[administrator]},\n      {default_vhost,<<\"/\">>},\n      {delegate_count,16},\n      {disk_free_limit,50000000},\n      {enabled_plugins_file,\"/etc/rabbitmq/enabled_plugins\"},\n      {error_logger,{file,\"/var/log/rabbitmq/rabbit@chipp.log\"}},\n      {fhc_read_buffering,false},\n      {fhc_write_buffering,true},\n      {frame_max,131072},\n      {halt_on_upgrade_failure,true},\n      {handshake_timeout,10000},\n      {heartbeat,60},\n      {hipe_compile,false},\n      {hipe_modules,\n          [rabbit_reader,rabbit_channel,gen_server2,rabbit_exchange,\n           rabbit_command_assembler,rabbit_framing_amqp_0_9_1,rabbit_basic,\n           rabbit_event,lists,queue,priority_queue,rabbit_router,rabbit_trace,\n           rabbit_misc,rabbit_binary_parser,rabbit_exchange_type_direct,\n           rabbit_guid,rabbit_net,rabbit_amqqueue_process,\n           rabbit_variable_queue,rabbit_binary_generator,rabbit_writer,\n           delegate,gb_sets,lqueue,sets,orddict,rabbit_amqqueue,\n           rabbit_limiter,gb_trees,rabbit_queue_index,\n           rabbit_exchange_decorator,gen,dict,ordsets,file_handle_cache,\n           rabbit_msg_store,array,rabbit_msg_store_ets_index,rabbit_msg_file,\n           rabbit_exchange_type_fanout,rabbit_exchange_type_topic,mnesia,\n           mnesia_lib,rpc,mnesia_tm,qlc,sofs,proplists,credit_flow,pmon,\n           ssl_connection,tls_connection,ssl_record,tls_record,gen_fsm,ssl]},\n      {lazy_queue_explicit_gc_run_operation_threshold,1000},\n      {log_levels,[{channel,debug},{connection,info}]},\n      {loopback_users,[<<\"guest\">>]},\n      {memory_monitor_interval,2500},\n      {mirroring_flow_control,true},\n      {mirroring_sync_batch_size,4096},\n      {mnesia_table_loading_retry_limit,10},\n      {mnesia_table_loading_retry_timeout,30000},\n      {msg_store_credit_disc_bound,{4000,800}},\n      {msg_store_file_size_limit,16777216},\n      {msg_store_index_module,rabbit_msg_store_ets_index},\n      {msg_store_io_batch_size,4096},\n      {num_ssl_acceptors,1},\n      {num_tcp_acceptors,10},\n      {password_hashing_module,rabbit_password_hashing_sha256},\n      {plugins_dir,\n          \"/usr/lib/rabbitmq/plugins:/usr/lib/rabbitmq/lib/rabbitmq_server-3.6.9/plugins\"},\n      {plugins_expand_dir,\n          \"/var/lib/rabbitmq/mnesia/rabbit@chipp-plugins-expand\"},\n      {queue_explicit_gc_run_operation_threshold,1000},\n      {queue_index_embed_msgs_below,4096},\n      {queue_index_max_journal_entries,32768},\n      {reverse_dns_lookups,false},\n      {sasl_error_logger,{file,\"/var/log/rabbitmq/rabbit@chipp-sasl.log\"}},\n      {server_properties,[]},\n      {ssl_allow_poodle_attack,false},\n      {ssl_apps,[asn1,crypto,public_key,ssl]},\n      {ssl_cert_login_from,distinguished_name},\n      {ssl_handshake_timeout,5000},\n      {ssl_listeners,[]},\n      {ssl_options,[]},\n      {tcp_listen_options,\n          [{backlog,4096},\n           {nodelay,true},\n           {linger,{true,0}},\n           {exit_on_close,false},\n           {sndbuf,196608},\n           {recbuf,196608}]},\n      {tcp_listeners,[{\"192.168.1.224\",5672}]},\n      {trace_vhosts,[]},\n      {vm_memory_high_watermark,0.6},\n      {vm_memory_high_watermark_paging_ratio,0.3}]},\n {rabbit_common,[]},\n {rabbitmq_management,\n     [{cors_allow_origins,[]},\n      {cors_max_age,1800},\n      {http_log_dir,none},\n      {listener,[{port,15672}]},\n      {load_definitions,none},\n      {management_db_cache_multiplier,5},\n      {process_stats_gc_timeout,300000},\n      {stats_event_max_backlog,250}]},\n {rabbitmq_management_agent,\n     [{rates_mode,basic},\n      {sample_retention_policies,\n          [{global,[{605,5},{3660,60},{29400,600},{86400,1800}]},\n           {basic,[{605,5},{3600,60}]},\n           {detailed,[{605,5}]}]}]},\n {rabbitmq_web_dispatch,[]},\n {ranch,[]},\n {sasl,[{errlog_type,error},{sasl_error_logger,false}]},\n {ssl,[]},\n {stdlib,[]},\n {syntax_tools,[]},\n {xmerl,[]}]\nrabbitmqctl status\nStatus of node rabbit@chipp...\n[{pid,20764},\n {running_applications,\n     [{rabbitmq_management,\"RabbitMQ Management Console\",\"3.6.9\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.6.9\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.6.9\"},\n      {rabbit,\"RabbitMQ\",\"3.6.9\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.6.9\"},\n      {rabbit_common,\n          \"Modules shared by rabbitmq-server and rabbitmq-erlang-client\",\n          \"3.6.9\"},\n      {compiler,\"ERTS  CXC 138 10\",\"7.0.4\"},\n      {cowboy,\"Small, fast, modular HTTP server.\",\"1.0.4\"},\n      {ranch,\"Socket acceptor pool for TCP protocols.\",\"1.3.0\"},\n      {ssl,\"Erlang/OTP SSL application\",\"8.1.1\"},\n      {public_key,\"Public key infrastructure\",\"1.4\"},\n      {cowlib,\"Support library for manipulating Web protocols.\",\"1.0.2\"},\n      {crypto,\"CRYPTO\",\"3.7.3\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.14.3\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.4.2\"},\n      {xmerl,\"XML parser\",\"1.3.13\"},\n      {asn1,\"The Erlang ASN1 compiler version 4.0.4\",\"4.0.4\"},\n      {syntax_tools,\"Syntax tools\",\"2.1.1\"},\n      {inets,\"INETS  CXC 138 49\",\"6.3.6\"},\n      {sasl,\"SASL  CXC 138 11\",\"3.0.3\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"3.3\"},\n      {kernel,\"ERTS  CXC 138 10\",\"5.2\"}]},\n {os,{unix,linux}},\n {erlang_version,\n     \"Erlang/OTP 19 [erts-8.3] [source-d5c06c6] [64-bit] [smp:8:8] [async-threads:128] [hipe] [kernel-poll:true]\\n\"},\n {memory,\n     [{total,1414497584},\n      {connection_readers,11462752},\n      {connection_writers,1866200},\n      {connection_channels,25246672},\n      {connection_other,22396568},\n      {queue_procs,1011424240},\n      {queue_slave_procs,0},\n      {plugins,15718072},\n      {other_proc,24336008},\n      {mnesia,223200},\n      {metrics,2163472},\n      {mgmt_db,24265776},\n      {msg_index,282584},\n      {other_ets,5048568},\n      {binary,231739096},\n      {code,24747054},\n      {atom,1033401},\n      {other_system,14704561}]},\n {alarms,[]},\n {listeners,\n     [{clustering,25672,\"::\"},{amqp,5672,\"192.168.1.224\"},{http,15672,\"::\"}]},\n {vm_memory_high_watermark,0.6},\n {vm_memory_limit,10082237644},\n {disk_free_limit,50000000},\n {disk_free,65727135744},\n {file_descriptors,\n     [{total_limit,32668},\n      {total_used,365},\n      {sockets_limit,29399},\n      {sockets_used,353}]},\n {processes,[{limit,1048576},{used,4034}]},\n {run_queue,0},\n {uptime,66222},\n {kernel,{net_ticktime,60}}]\n_usr_lib_erlang_erts-8.3_bin_beam.smp.123.crash.zip\n. Here you have the dump file\nerl_crash.zip\n. I tested with:\nesl-erlang_19.2-1~ubuntu~xenial_amd64.deb\nand \nesl-erlang_19.3-1~ubuntu~xenial_amd64.deb\nThe log file is very huge 1.8G.. Thx. Please close this issue, as I'm updating Erlang to 19.3.6.\nI will open another issue with all required files if it happens again. Thx.. I\u2019m not aware of this and I stopped using RabbitMq 1 yr ago.\nEnvoy\u00e9 de mon iPhone\n\nLe 24 nov. 2018 \u00e0 10:34, Ankita Anil Verma notifications@github.com a \u00e9crit :\nwas this issue resolved?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "ankita-anil-verma": "was this issue resolved?. ",
    "Pkuutn": "I have weird segfaults with 3.7.8/OTP 21.1 (Erlang Solutions) - i.e. no faults with perftest load, but 100% reproducable with workload (it's a pre-prod environment, so 10-200 messages per second\nCould someone point me to how to locate the RabbitMQ server source code part which is associated with segfault?\nkernel: [319434.121690] 1_scheduler[13351]: segfault at 208 ip 000055ce50cb9104 sp 00007f965ce3e9d0 error 4 in beam.smp[55ce50afa000+2dc000]. ",
    "Farit-Biktimirov": "@michaelklishin , would you kindly describe workaround for this type of BUG?. ",
    "goncalotomas": "Having installed Erlang 20, I just cloned the repository and ran make.. ",
    "choyuri": "apolgoies.. \nbut this is a bug in the rabbit_mirror_queue_mode_exactly module that prevents rabbit node from restarting in certain conditions\nthe bug is in \nfile,\"src/rabbit_mirror_queue_mode_exactly.erl\"}, {line,39}\nis this ticket closed because it's not considered a bug?\nthanks\n. ",
    "ornoone": "thanks for your help. I will take time to inspect logs and closed connection warning, and will reopen with more data if I can't find out myself. (and with a reproduction steps). ",
    "langyxxl": "When in openstack large deployment, 1000 compute node ovs-agent  will create 10K ha-queue.\nThis gc every 250ms will cause cpu usage very high. Even these queue is empty.\nhttps://groups.google.com/forum/?nomobile=true#!topic/rabbitmq-users/6jGtaHINmNM\n. ",
    "liaodalin19903": "@michaelklishin  But, however, in my other centos 7.2 host machine all they all works perfect with rabbitmq-server-3.6.5-1.el7.noarch. ",
    "marscher": "Thank you for your quick responses. I've double checked, that there are no extra quotes, just using single quotes also lead to file name expansion.. I ended up using the web interface to set the permissions.... On 08/11/2017 05:59 PM, Luke Bakken wrote:\n\nOK. As @michaelklishin https://github.com/michaelklishin said, there\nmust be something awry with your shell or terminal. If this were an\nissue with |rabbitmqctl| be assured we would hear about it non-stop as\n|.*| is a very common regex.\n\nyes, it is very likely that my shell is borked.\n. ",
    "alvinzhangproton": "Thanks Luke, I'll post it on the mailing list. \nBest,\nAlvin. ",
    "xtender": "And also lots of errors like this one:\n\n=SUPERVISOR REPORT==== 25-Aug-2017::12:26:43 ===\n     Supervisor: {<0.11544.11>,amqp_channel_sup_sup}\n     Context:    shutdown_error\n     Reason:     shutdown\n     Offender:   [{nb_children,1},\n                  {name,channel_sup},\n                  {mfargs,\n                      {amqp_channel_sup,start_link,\n                          [direct,<0.11530.11>,\n                           <<\"10.101.28.127:57984 -> 10.103.183.50:61613\">>]}},\n                  {restart_type,temporary},\n                  {shutdown,brutal_kill},\n                  {child_type,supervisor}]\n. \n",
    "lasfromhell": "@lukebakken Erlang 8.3 (OTP 19.3) x64\n@michaelklishin thanks, that helps. ",
    "atroxes": "Recently upgraded to RabbitMQ 3.6.11 (Erlang OTP 20.0) and experienced this issue as well. In a 2-node cluster, Host A and B spent a lot more CPU resources right after the upgrade, and after about a week, Host A suddenly went into a downward spiral, using 100% CPU resources. Host B did not experience this though, only higher avg. CPU usage, caused by WmiPrvSE.exe.\nThese two images show Host A from just before the upgrade on the morning of August 31st, to about a week later. Especially \"Number of Threads\" seems interesting.\nHost A CPU Utilization - https://i.imgur.com/7k3i1kN.png\nHost A Number of Threads - https://i.imgur.com/tGmpXYM.png\nThe issue has been temporarily fixed for us by using the suggestion of @michaelklishin and setting rabbit.vm_memory_calculation_strategy to erlang in the config file.\nEdit: The higher avg. CPU usage of WmiPrvSE.exe was also present using Erlang OTP 19.3.. @lukebakken - Windows 2012 R2 - Last rollup installed is KB4022726 - It's a test system :)\nWould you suggest fully updating and trying again?. @michaelklishin - I'm afraid I'm not at liberty to do that, sorry. I will however attempt to fully update the same host and see if this resolves the issue.\nUpdate: Same behaviour on a fully updated Windows 2012 R2. Excessive CPU usage from WmiPrvSE.exe as soon as RabbitMQ service is started.. @hairyhum \nConnections: 7\nChannels: 24\nExchanges: 33\nQueues: 24\nConsumers: 24\nRegarding the rest: It's a test setup, it's literally doing next to nothing, as in, I barely see any messages at all. All rates/s are bascially 0.\nSorry I can't provide more detailed information, I'm pretty new to RabbitMQ :-). @hairyhum With 3.6.12.rc3+2.g9086607, still seeing the same behaviour, WmiPrvSE.exe hovering around 3-7% CPU using the default 'rss' memory calculation strategy. 'erlang' strategy showing no issues.. @michaelklishin Sure thing! The test server is still online :). @michaelklishin Behaviour changed with 3.6.12-alpha.44.\nNow I see 'WMIC.exe' spawning every 2-3 seconds and while it's running, 'WmiPrvSE.exe' eats around 3-7% on my, admittedly, low spec'ed VM. I'm not sure if the resource usage is significant on larger systems, but it's definitely noticeable.. @michaelklishin Unfortunately I know nothing about the capabilities of Erlang, but maybe it's able to query Resource Monitor? That should be available in Windows 7+.\nThanks for looking into this though, it's not a huge issue for us luckily :). ",
    "jdahl": "We see the same behavior. On my local Win 10 machine it started to consume 25% of the cpu so a complete core on its own. Using Rabbit 3.6.12 and Erlang 19.3. On windows you really shouldn't call a command prompt every second since its a quite CPU intensive thing to do. The one I see that is spawned is \n\nSuggestions would be to either find another way to ask for the memory usage or possibly revert to the old system on windows.\n. As a windows developer I can say that you really shouldn't start any process once per second. It's quite resource intensive on windows to do. If you needs to ask WMI you should do it in process but I assume your problem is that you can't do this though erlang. The best solution is probably to revert this behavior on windows until a better solution is found.\nI personally didn't get the stuck command process as other has reported but I see the cmd executed and closing. What however took 25% cpu usage was the WMI window services. Partly because every time you asked this it among other things loaded in a resource dll from the window system. A bit tricky to track down that rabbit was the cause since the process had closed before I could look at who called the WMI service. Because of this behavior rabbit isn't seen as the cause since a number of other services relies on the WMI service that consumes all cpu. Rabbit isn't among them but update services and security services where on my machine. Might be part of the reason why you dont get many reports on this. Users thinks something else is the cause and not rabbit.\n. ",
    "Gmcourtney": "Hello,\nJust wanted to contribute that we have noticed a similar issue with the spawned cmd.exe and WMIC.exe processes.\nOur initial test environment is configured as below:\nOS- Windows Server 2008R2\nRabbitMQ- 3.6.11\nErlang- OTP20\nWe noticed that upon starting the RabbitMQ service, Windows Resource Monitor showed system memory in use growing at a rate of approx 200K per hour, however when looking at running processes nothing appeared to be growing out of control.  On a small test environment with 4GB RAM, our machine would make it 12 hours or so before before it crashed.  \nUsing RamMap.exe we identified that there were thousands of cmd.exe and WMIC.exe processes, each still holding 4K of private memory and 16K in the page file.  These zombie process handles were growing at an extremely rapid rate, which appeared to be occuring every time a process was spawned/destroyed.  This finding lead me to this bug/issue discussion thread.  \nI applied the update provided by @hairyhum (rabbitmq-server-3.6.12.rc3+2.g9086607).   The behaviour is still being seen, however at a SIGNIFICANTLY reduced rate.  Since applying this package yesterday evening, our environment has run for approx 18 hours.  Running RamMap to check for abandoned processes, I see approximately 30-40 cmd.exe and WMIC.exe processes.  So it's still a problem that leads to a memory leak, although much less impactful.\n\n\nI then switched the configuration to use {vm_memory_calculation_strategy, erlang}.  Obviously this solved my memory issue... if the processes don't spawn, there's nothing to hang.\nI have two questions:\n1) Is this problem isolated to specific versions of Windows?  I've found threads regarding similar symptoms/behaviour leaving hanging process references to terminated processes with other software running on Windows Server.  On my local development machine, which is running Windows 10 (build 1607), I see no such behavior.  This machine has been running for 3-4 days with RabbitMQ service running (although precious few messages have actually been processed).  I can watch the processes being spawned in Process Explorer, and they are all terminated and cleaned up properly.  RamMap shows no evidence of any remnants of these processes on my local developer machine.\n2) What am I giving up by switching the vm_memory_calculation_strategy to use the old approach?  This change resolves the memory pressure that results from the spawned processes, RabbitMQ appears to be stable, and my application still appears to function properly interacting with the bus.  Obviously the new approach to use WMI to get the statistics was done for a purpose; so I'm looking for some guidance with regards to any downside to using the old approach in a productive environment.\nThank you!\n. ",
    "AceHack": "Still not working on 3.6.12 even after this fix.\n##########  Logs: C:/rabbitmq/sbin/-\n  ######  ##        C:/rabbitmq/sbin/-. Okay tested with 3.6.13 and it's working great!! Thanks.. So I was able to get erlang and rabbitmq working on the latest preview of windows nanoserver insider edition. It's a bit kludgy but it's working!!!\nI guess microsoft added whatever was missing in the latest edition to make things work.\nHope this image can help someone else.\nYou can type \"build-image.cmd\" to create the docker image\n\"run-containers.cmd\" will startup two instance of rabbit one with host name \"rabbitmq\" and one with host name \"joinmq\"\nOne interesting thing here is the .erlang.cookie file. I had to do some playing to get the two rabbits to have the same .erlang.cookie file but not embed it in the image so it could be replaced at runtime.\n\"log-containers.cmd\" will show you once the rabbit images have started fully.\n\"join-cluster.cmd\" will join the two instances into a cluster.\nIt would be great to see an officially support windows docker container from rabbit\nrabbitmq-nanoserver-insider.zip\n. Thanks so much this is very helpful. When adding rabbit to things like K8s it's impossible to know when you do a deploy 10s if not 100s of deploys are happening a day automated if a network partition is occuring or not.  This functions as designed would suggest rabbit-autocluster is not K8s production ready.. ",
    "johanrhodin": "Questions like these are filed on the rabbitmq-users mailing list :\nhttps://groups.google.com/forum/?hl=en#!forum/rabbitmq-users\nOn Mon, Sep 4, 2017 at 5:52 PM, Kareem Amin notifications@github.com\nwrote:\n\nHi All,\nI have a question related to RabbitMQ upgrade from 3.6.6 to 3.6.11.\nIs it possible to upgrade directly from 3.6.6 to 11 or we should go step\nby step?\nIn the release notes, it is stated that 3.6.11 doesn't have\nincompatibility issues with 3.6.7 walking through 3.6.10, what about 3.6.6?\nhttps://github.com/rabbitmq/rabbitmq-server/releases/tag/rabbitmq_v3_6_11\nDoes anyone have prior experience with upgrading RabbitMQ?\nSorry if this is the right place to ask that question, please refer to the\nright place if it is wrong here.\nBest Regards,\nKareem\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/rabbitmq/rabbitmq-server/issues/1350, or mute the\nthread\nhttps://github.com/notifications/unsubscribe-auth/AAcGUWiUxhVDGGQRYuOJUP78wC75DsFHks5sfBzQgaJpZM4PMEk6\n.\n. Thanks guys, nice improvements! Will try when 3.6.13 is released.. \n",
    "564064202": "@Gsantomaggio I am so sorry I can't post it there,because I am in China,And our country forbidden use google domain ....\nhttp://images2017.cnblogs.com/blog/83201/201709/83201-20170905163511116-878346723.png. http://images2017.cnblogs.com/blog/83201/201709/83201-20170905163511116-878346723.png. @Gsantomaggio hello. ",
    "rgl": "Ah, those are good points! Now I'm in a catch-22 \"to restart, or not to restart, that is the question\" moment!\nBut, I'm lingering towards starting indefinitely... with  Restart=always, StartLimitIntervalSec=0 (to disable rate limiting and keep restarting forever) and RestartSec=10 (restart after 10 seconds for not burning the cpu/logs in case it really keeps restarting forever). Something like:\n```bash\ncat >/etc/systemd/system/test.service <<'EOF'\n[Unit]\nAfter=network.target\nStartLimitIntervalSec=0\n[Service]\nType=simple\nExecStart=/bin/bash -c \"date '+%F %T.%%N';exit 1;\"\nRestart=always\nRestartSec=10\n[Install]\nWantedBy=multi-user.target\nEOF\nsystemctl daemon-reload\nsystemctl enable test\nsystemctl start test\njournalctl --follow -u test\nsystemctl disable test\n```\nBut I'm not really sure what should be the default for rabbitmq :-/\n. But is there other way to keep restarting forever without disabling rate limiting with StartLimitIntervalSec=0?. Oh, I now realize that I failed to mention the reason why I've initially created this issue... I had a disk outage, freed the disk, and much latter noticed rmq was stopped, and scratched my head why rmq was never (re)started by systemd. \nIt turns out that rmq stopped due to the disk outage and was never restarted due to the systemd unit configuration. \nThis made me realize that I needed to have alerting in place and perhaps change the systemd unit to keep restarting rmq forever. Hence this issue was created.\nIn my particular case, having systemd restart (forever) rmq would have helped.. https://github.com/rabbitmq/chocolatey-package/blob/master/rabbitmq.nuspec#L6 still says 3.7.7. is that expected? does that repository contains the latest package source?. I understand. Thank you!. ",
    "gilbsgilbs": "@michaelklishin Sorry for the noise \ud83d\ude15 . Let's continue there.\nNothing special in the server logs.. ",
    "sigiesec": "@michaelklishin I can understand this is not a priority for your team. But I don't understand that you are closing the ticket. Keeping it open will allow to track that it is open, both for people from the community interested in the feature as well as maybe someone from the community who will work on it (I cannot promise but it might be that me or some colleague of mine might work on this).. ",
    "deslum": "Thank you. Asked a question here https://groups.google.com/forum/#!topic/rabbitmq-users/ti6mIBmZMuE. ",
    "ransford": "Yes, sysctl is in /usr/sbin and /usr/sbin is in the default $PATH (via /etc/paths).\nIs this a Homebrew packaging issue, then?. Thanks @lukebakken.  I patched Homebrew's rabbitmq formula to add /usr/sbin to the path, which silences the warning and error: Homebrew/homebrew-core#18580.. ",
    "lamchakchan": "I don't think this makes sense.  # allows for 0 to many words.  Using dots (.) as separators, what is the rule set that allows this operator to take in separators.  From real world observations for this case, I've seen this.\n\nbinding : a.#.b\nroutable keys : a..b, a.b, a.x.b\nnon-routable keys: ab\n\nFrom your interpretation, # should be greedy enough to eat up all separators.  I would also assumed after reading your thoughts (which makes sense), ab is allowable. . I'm also finding discrepancy between documentation and implementation for the * wild card as well.  \n\nbinding : a.*.b\nroutable keys : a..b, a.x.b\nnon-routable keys: a.b\n\nFrom section 3.1.3.3 as well\n\nThe routing pattern follows the same rules as the routing key with the addition that * matches a single word, and # matches zero or more words. Thus the routing pattern *.stock.# matches the routing keys usd.stock and eur.stock.db but not stock.nasdaq.\n\n. I'm not for fixing this as your said, this might break existing users.  I just need some clarity on syntax and the expected behavior of result within the documentation so I can base my implementation off of.  . ",
    "folt": "rabbitmqctl environment\nApplication environment of node server_node@localhost ...\n[{amqp_client,[{prefer_ipv6,false},{ssl_options,[]}]},\n {asn1,[]},\n {compiler,[]},\n {cowboy,[]},\n {cowlib,[]},\n {crypto,[]},\n {inets,[]},\n {kernel,\n     [{error_logger,tty},\n      {inet_default_connect_options,[{nodelay,true}]},\n      {inet_dist_listen_max,25672},\n      {inet_dist_listen_min,25672}]},\n {mnesia,[{dir,\"/usr/local/var/lib/rabbitmq/mnesia/server_node@localhost\"}]},\n {os_mon,\n     [{start_cpu_sup,false},\n      {start_disksup,false},\n      {start_memsup,false},\n      {start_os_sup,false}]},\n {public_key,[]},\n {rabbit,\n     [{auth_backends,\n          [rabbit_auth_backend_internal,rabbit_auth_backend_cache,\n           rabbit_auth_backend_http]},\n      {auth_mechanisms,['PLAIN','AMQPLAIN']},\n      {background_gc_enabled,false},\n      {background_gc_target_interval,60000},\n      {backing_queue_module,rabbit_priority_queue},\n      {channel_max,0},\n      {channel_operation_timeout,15000},\n      {cluster_keepalive_interval,10000},\n      {cluster_nodes,{[],disc}},\n      {cluster_partition_handling,ignore},\n      {collect_statistics,fine},\n      {collect_statistics_interval,5000},\n      {config_entry_decoder,\n          [{cipher,aes_cbc256},\n           {hash,sha512},\n           {iterations,1000},\n           {passphrase,undefined}]},\n      {credit_flow_default_credit,{400,200}},\n      {default_permissions,[<<\".\">>,<<\".\">>,<<\".*\">>]},\n      {default_user,<<\"guest\">>},\n      {default_user_tags,[administrator]},\n      {default_vhost,<<\"/\">>},\n      {delegate_count,16},\n      {disk_free_limit,50000000},\n      {enabled_plugins_file,\"/usr/local/etc/rabbitmq/enabled_plugins\"},\n      {error_logger,\n          {file,\"/usr/local/var/log/rabbitmq/server_node@localhost.log\"}},\n      {fhc_read_buffering,false},\n      {fhc_write_buffering,true},\n      {frame_max,131072},\n      {halt_on_upgrade_failure,true},\n      {handshake_timeout,10000},\n      {heartbeat,60},\n      {hipe_compile,false},\n      {hipe_modules,\n          [rabbit_reader,rabbit_channel,gen_server2,rabbit_exchange,\n           rabbit_command_assembler,rabbit_framing_amqp_0_9_1,rabbit_basic,\n           rabbit_event,lists,queue,priority_queue,rabbit_router,rabbit_trace,\n           rabbit_misc,rabbit_binary_parser,rabbit_exchange_type_direct,\n           rabbit_guid,rabbit_net,rabbit_amqqueue_process,\n           rabbit_variable_queue,rabbit_binary_generator,rabbit_writer,\n           delegate,gb_sets,lqueue,sets,orddict,rabbit_amqqueue,\n           rabbit_limiter,gb_trees,rabbit_queue_index,\n           rabbit_exchange_decorator,gen,dict,ordsets,file_handle_cache,\n           rabbit_msg_store,array,rabbit_msg_store_ets_index,rabbit_msg_file,\n           rabbit_exchange_type_fanout,rabbit_exchange_type_topic,mnesia,\n           mnesia_lib,rpc,mnesia_tm,qlc,sofs,proplists,credit_flow,pmon,\n           ssl_connection,tls_connection,ssl_record,tls_record,gen_fsm,ssl]},\n      {lazy_queue_explicit_gc_run_operation_threshold,1000},\n      {log_levels,[{connection,info}]},\n      {loopback_users,[<<\"guest\">>]},\n      {memory_monitor_interval,2500},\n      {mirroring_flow_control,true},\n      {mirroring_sync_batch_size,4096},\n      {mnesia_table_loading_retry_limit,10},\n      {mnesia_table_loading_retry_timeout,30000},\n      {msg_store_credit_disc_bound,{4000,800}},\n      {msg_store_file_size_limit,16777216},\n      {msg_store_index_module,rabbit_msg_store_ets_index},\n      {msg_store_io_batch_size,4096},\n      {num_ssl_acceptors,1},\n      {num_tcp_acceptors,10},\n      {password_hashing_module,rabbit_password_hashing_sha256},\n      {plugins_dir,\"/usr/local/Cellar/rabbitmq/3.6.9_1/plugins\"},\n      {plugins_expand_dir,\n          \"/usr/local/var/lib/rabbitmq/mnesia/server_node@localhost-plugins-expand\"},\n      {queue_explicit_gc_run_operation_threshold,1000},\n      {queue_index_embed_msgs_below,4096},\n      {queue_index_max_journal_entries,32768},\n      {reverse_dns_lookups,false},\n      {sasl_error_logger,\n          {file,\"/usr/local/var/log/rabbitmq/server_node@localhost-sasl.log\"}},\n      {server_properties,[]},\n      {ssl_allow_poodle_attack,false},\n      {ssl_apps,[asn1,crypto,public_key,ssl]},\n      {ssl_cert_login_from,distinguished_name},\n      {ssl_handshake_timeout,5000},\n      {ssl_listeners,[]},\n      {ssl_options,[]},\n      {tcp_listen_options,\n          [{backlog,128},\n           {nodelay,true},\n           {linger,{true,0}},\n           {exit_on_close,false}]},\n      {tcp_listeners,[{\"192.168.100.200\",5672}]},\n      {trace_vhosts,[]},\n      {vm_memory_high_watermark,0.4},\n      {vm_memory_high_watermark_paging_ratio,0.5}]},\n {rabbit_common,[]},\n {rabbitmq_amqp1_0,\n     [{default_user,\"guest\"},\n      {default_vhost,<<\"/\">>},\n      {protocol_strict_mode,false}]},\n {rabbitmq_auth_backend_cache,\n     [{cache_module,rabbit_auth_cache_ets},\n      {cache_module_args,[]},\n      {cache_refusals,false},\n      {cache_ttl,240000},\n      {cached_backend,rabbit_auth_backend_http}]},\n {rabbitmq_auth_backend_http,\n     [{http_method,post},\n      {resource_path,\n          \"http://192.168.100.200:8000/v1/api_rabbitmq/auth/resource/\"},\n      {topic_path,\"http://192.168.100.200:8000/v1/api_rabbitmq/auth/topic/\"},\n      {user_path,\"http://192.168.100.200:8000/v1/api_rabbitmq/auth/user/\"},\n      {vhost_path,\"http://192.168.100.200:8000/v1/api_rabbitmq/auth/vhost/\"}]},\n {rabbitmq_consistent_hash_exchange,[]},\n {rabbitmq_event_exchange,[]},\n {rabbitmq_federation,[{pgroup_name_cluster_id,false}]},\n {rabbitmq_federation_management,[]},\n {rabbitmq_jms_topic_exchange,[]},\n {rabbitmq_management,\n     [{cors_allow_origins,[]},\n      {cors_max_age,1800},\n      {http_log_dir,none},\n      {listener,[{port,15672}]},\n      {load_definitions,none},\n      {management_db_cache_multiplier,5},\n      {process_stats_gc_timeout,300000},\n      {stats_event_max_backlog,250}]},\n {rabbitmq_management_agent,\n     [{rates_mode,basic},\n      {sample_retention_policies,\n          [{global,[{605,5},{3660,60},{29400,600},{86400,1800}]},\n           {basic,[{605,5},{3600,60}]},\n           {detailed,[{605,5}]}]}]},\n {rabbitmq_management_visualiser,[]},\n {rabbitmq_message_timestamp,[]},\n {rabbitmq_mqtt,\n     [{allow_anonymous,true},\n      {default_user,<<\"guest\">>},\n      {exchange,<<\"amq.topic\">>},\n      {num_ssl_acceptors,1},\n      {num_tcp_acceptors,10},\n      {prefetch,10},\n      {retained_message_store,rabbit_mqtt_retained_msg_store_dets},\n      {retained_message_store_dets_sync_interval,2000},\n      {ssl_cert_login,false},\n      {ssl_listeners,[]},\n      {subscription_ttl,86400000},\n      {tcp_listen_options,[{backlog,128},{nodelay,true}]},\n      {tcp_listeners,[1883]},\n      {vhost,<<\"/\">>}]},\n {rabbitmq_shovel,\n     [{defaults,\n          [{prefetch_count,1000},\n           {ack_mode,on_confirm},\n           {publish_fields,[]},\n           {publish_properties,[]},\n           {reconnect_delay,5}]}]},\n {rabbitmq_shovel_management,[]},\n {rabbitmq_stomp,\n     [{default_user,[{login,<<\"guest\">>},{passcode,<<\"guest\">>}]},\n      {default_vhost,<<\"/\">>},\n      {hide_server_info,false},\n      {implicit_connect,false},\n      {num_ssl_acceptors,1},\n      {num_tcp_acceptors,10},\n      {ssl_cert_login,false},\n      {ssl_listeners,[]},\n      {tcp_listen_options,[{backlog,128},{nodelay,true}]},\n      {tcp_listeners,[61613]},\n      {trailing_lf,true}]},\n {rabbitmq_top,[]},\n {rabbitmq_tracing,\n     [{directory,\"/var/tmp/rabbitmq-tracing\"},\n      {password,<<\"guest\">>},\n      {username,<<\"guest\">>}]},\n {rabbitmq_web_dispatch,[]},\n {rabbitmq_web_mqtt,\n     [{cowboy_opts,[]},\n      {num_ssl_acceptors,1},\n      {num_tcp_acceptors,10},\n      {ssl_config,[]},\n      {tcp_config,[]}]},\n {rabbitmq_web_mqtt_examples,[{listener,[{port,15670}]}]},\n {rabbitmq_web_stomp,\n     [{cowboy_opts,[]},\n      {num_ssl_acceptors,1},\n      {num_tcp_acceptors,10},\n      {port,15674},\n      {sockjs_opts,[]},\n      {ssl_config,[]},\n      {tcp_config,[]},\n      {use_http_auth,false},\n      {ws_frame,text}]},\n {rabbitmq_web_stomp_examples,[{listener,[{port,15670}]}]},\n {ranch,[]},\n {sasl,[{errlog_type,error},{sasl_error_logger,false}]},\n {sockjs,[]},\n {ssl,[]},\n {stdlib,[]},\n {syntax_tools,[]},\n {xmerl,[]}]\n/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/-/\nrabbitmqctl status\nStatus of node server_node@localhost ...\n[{pid,5937},\n {running_applications,\n     [{rabbitmq_top,\"RabbitMQ Top\",\"3.6.9\"},\n      {rabbitmq_tracing,\"RabbitMQ message logging / tracing\",\"3.6.9\"},\n      {rabbitmq_federation_management,\"RabbitMQ Federation Management\",\n          \"3.6.9\"},\n      {rabbitmq_management_visualiser,\"RabbitMQ Visualiser\",\"3.6.9\"},\n      {rabbitmq_shovel_management,\n          \"Management extension for the Shovel plugin\",\"3.6.9\"},\n      {rabbitmq_management,\"RabbitMQ Management Console\",\"3.6.9\"},\n      {rabbitmq_web_stomp_examples,\"Rabbit WEB-STOMP - examples\",\"3.6.9\"},\n      {rabbitmq_web_stomp,\"Rabbit WEB-STOMP - WebSockets to Stomp adapter\",\n          \"3.6.9\"},\n      {rabbitmq_web_mqtt_examples,\"Rabbit WEB-MQTT - examples\",\"3.6.9\"},\n      {rabbitmq_web_mqtt,\"RabbitMQ MQTT-over-WebSockets adapter\",\"3.6.9\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.6.9\"},\n      {cowboy,\"Small, fast, modular HTTP server.\",\"1.0.4\"},\n      {cowlib,\"Support library for manipulating Web protocols.\",\"1.0.2\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.6.9\"},\n      {rabbitmq_consistent_hash_exchange,\"Consistent Hash Exchange Type\",\n          \"3.6.9\"},\n      {rabbitmq_jms_topic_exchange,\n          \"RabbitMQ JMS topic selector exchange plugin\",\"3.6.9\"},\n      {rabbitmq_shovel,\"Data Shovel for RabbitMQ\",\"3.6.9\"},\n      {rabbitmq_auth_backend_http,\"RabbitMQ HTTP Authentication Backend\",\n          \"3.6.7\"},\n      {rabbitmq_stomp,\"RabbitMQ STOMP plugin\",\"3.6.9\"},\n      {rabbitmq_federation,\"RabbitMQ Federation\",\"3.6.9\"},\n      {rabbitmq_amqp1_0,\"AMQP 1.0 support for RabbitMQ\",\"3.6.9\"},\n      {rabbitmq_event_exchange,\"Event Exchange Type\",\"3.6.9\"},\n      {rabbitmq_mqtt,\"RabbitMQ MQTT Adapter\",\"3.6.9\"},\n      {rabbitmq_message_timestamp,\"RabbitMQ Message Timestamp\",[]},\n      {rabbit,\"RabbitMQ\",\"3.6.9\"},\n      {ranch,\"Socket acceptor pool for TCP protocols.\",\"1.3.0\"},\n      {ssl,\"Erlang/OTP SSL application\",\"8.1.1\"},\n      {public_key,\"Public key infrastructure\",\"1.4\"},\n      {crypto,\"CRYPTO\",\"3.7.3\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.4.2\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.6.9\"},\n      {rabbitmq_auth_backend_cache,\"RabbitMQ Authentication Backend cache\",[]},\n      {rabbit_common,\n          \"Modules shared by rabbitmq-server and rabbitmq-erlang-client\",\n          \"3.6.9\"},\n      {compiler,\"ERTS  CXC 138 10\",\"7.0.4\"},\n      {sockjs,\"SockJS\",\"0.3.4\"},\n      {syntax_tools,\"Syntax tools\",\"2.1.1\"},\n      {asn1,\"The Erlang ASN1 compiler version 4.0.4\",\"4.0.4\"},\n      {sasl,\"SASL  CXC 138 11\",\"3.0.3\"},\n      {inets,\"INETS  CXC 138 49\",\"6.3.6\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.14.3\"},\n      {xmerl,\"XML parser\",\"1.3.13\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"3.3\"},\n      {kernel,\"ERTS  CXC 138 10\",\"5.2\"}]},\n {os,{unix,darwin}},\n {erlang_version,\n     \"Erlang/OTP 19 [erts-8.3] [source] [64-bit] [smp:4:4] [async-threads:64] [hipe] [kernel-poll:true] [dtrace]\\n\"},\n {memory,\n     [{total,154503320},\n      {connection_readers,88968},\n      {connection_writers,1488},\n      {connection_channels,17064},\n      {connection_other,80992},\n      {queue_procs,8324176},\n      {queue_slave_procs,0},\n      {plugins,4274432},\n      {other_proc,44782760},\n      {mnesia,47190536},\n      {metrics,698464},\n      {mgmt_db,124824},\n      {msg_index,536568},\n      {other_ets,8822176},\n      {binary,2360584},\n      {code,26045814},\n      {atom,1041593},\n      {other_system,10808497}]},\n {alarms,[]},\n {listeners,\n     [{clustering,25672,\"::\"},\n      {amqp,5672,\"192.168.100.200\"},\n      {mqtt,1883,\"::\"},\n      {stomp,61613,\"::\"},\n      {'http/web-mqtt',15675,\"::\"},\n      {http,15670,\"::\"},\n      {'http/web-stomp',15674,\"::\"},\n      {http,15672,\"::\"}]},\n {vm_memory_high_watermark,0.4},\n {vm_memory_limit,8939565875},\n {disk_free_limit,undefined},\n {disk_free,undefined},\n {file_descriptors,\n     [{total_limit,156},{total_used,10},{sockets_limit,138},{sockets_used,1}]},\n {processes,[{limit,1048576},{used,935}]},\n {run_queue,0},\n {uptime,481},\n {kernel,{net_ticktime,60}}\nThis situation kills the rabbit and it stops working. \nI suspect that the problem is that some customer often tries to connect to a rabbit. Either the client is triggered by a loop of connections to the rabbit that does not stop. Rabbit does not limit attempts to connect and does not handle the cycle from the client. However, these are only my guesses based on watching the rabbit's logos.. ",
    "afreetgo": "thanks\uff0ci try using the generic UNIX package\u3002. ",
    "klboke": "After setting the 755 permission, the problem is solved.. ",
    "andymartin": "Informational addenda for those who might search on this issue in the future:\n\n\nDisabling 8.3 filenames on the target volume does not solve the problem. \n\n\nUsing env variables to specify 8.3-compliant filenames also does not work. That is:\nRABBITMQ_LOGS=D:\\Logging\\rabbitmq.log\nRABBITMQ_SASL_LOGS=D:\\Logging\\sasl.log\nStill creates:\nRABBIT~3.LOG\nRABBIT~4.LOG\n\n\nWill be awaiting 3.7 release to determine whether this can be resolved with new logging framework.. ",
    "pivotal-issuemaster": "@carlhoerberg Please sign the Contributor License Agreement!\nClick here to manually synchronize the status of this Pull Request.\nSee the FAQ for frequently asked questions.. @carlhoerberg Thank you for signing the Contributor License Agreement!. @essen Please sign the Contributor License Agreement!\nClick here to manually synchronize the status of this Pull Request.\nSee the FAQ for frequently asked questions.. @essen Thank you for signing the Contributor License Agreement!. @getong Please sign the Contributor License Agreement!\nClick here to manually synchronize the status of this Pull Request.\nSee the FAQ for frequently asked questions.. @getong Thank you for signing the Contributor License Agreement!. @notque Please sign the Contributor License Agreement!\nClick here to manually synchronize the status of this Pull Request.\nSee the FAQ for frequently asked questions.. @notque Thank you for signing the Contributor License Agreement!. @noxdafox Please sign the Contributor License Agreement!\nClick here to manually synchronize the status of this Pull Request.\nSee the FAQ for frequently asked questions.. @noxdafox Please sign the Contributor License Agreement!\nClick here to manually synchronize the status of this Pull Request.\nSee the FAQ for frequently asked questions.. @lxc-vmaxx Please sign the Contributor License Agreement!\nClick here to manually synchronize the status of this Pull Request.\nSee the FAQ for frequently asked questions.. ",
    "grahamc": "Great to hear about 3.7.0! Thank you. I'm confused about -sasl though, as both logs are set to tty, and not in the tty output.. ",
    "Jleagle": "I'm running rabbitmq:3.7-management-alpine image and am getting the following errors. The files all exist in the paths specified but are owned by root. The errors don't seem to tell me this.\nerror: files specified, but missing\n  - /etc/letsencrypt/live/x/fullchain.pem (RABBITMQ_MANAGEMENT_SSL_CACERTFILE)\n  - /etc/letsencrypt/live/x/cert.pem (RABBITMQ_MANAGEMENT_SSL_CERTFILE)\n  - /etc/letsencrypt/live/x/privkey.pem (RABBITMQ_MANAGEMENT_SSL_KEYFILE). ",
    "sathishbabu96": "Hi @lukebakken right now, with the queuemaster balancer plugin, we can rebalance the queues only when we trigger it manually. Is there any possibility for enhancement or upgrade in which the plugin automatically identifies the downing of a node and rebalances the queue masters without human intervention.. ",
    "Xanhou": "Had the same issue. Solved by doing the following:\n1. Copy the config example.\n2. Delete the two %% in front of the config line you want to add to the config, but not the space.\n3. Delete the comma behind the config line\nI only needed to change one line, but my guess is that you have to only remove the comma from the last entry.\nps. Im commenting on this because it poped up high in google for me, so hopefully the next person with the same issue sees this solution.. ",
    "mctwynne": "@michaelklishin I'm running Ubuntu 16.04, and I installed rabbitmq via puppet+packagecloud as part of an OpenStack deployment. Is there any more info you need from me? What do you mean by systems version?\nAfter reading more about comments in service files it seems inline comments are not permitted by systemd: \"Empty lines and lines starting with \"#\" or \";\" are ignored. This may be used for commenting.\" Inline comments are not permitted. Since I'm doing a puppet deployment, removing the comments is easier said then done on my end since I'm deploying to lots of hosts, and the rest of the installation requires that rabbit is functional before continuing.\nThanks for looking into this.. @michaelklishin Thank you very much!!. Just FYI, systemd --version gives me: systemd 229\nI'm not sure how this would have passed tests and not reloaded the service constantly since it's unsupported usage of service files, at least with my version of systemd.. @michaelklishin I don't know if or how I can specify a specific version with puppet. I haven't found a way yet. Any chance you might know?. If you just want to know if removing the inline comments works, it does. I've deleted them and restarted the service, and no longer get the errors.. @michaelklishin I'm assuming that would be the only change in the package you'd upload?. ",
    "z-caterpillar": "I guess 433 is https port\uff0cIt has been binded\uff0cyou can stop https service;. ",
    "ppyoosuf": "this is my log info\nError description:\n   {could_not_start,rabbitmq_management,\n       {{could_not_start_listener,\n            [{port,443},\n             {ssl,true},\n             {ssl_opts,\n                 [{cacertfile,\"/home/rtm-aimpl-ca.crt\"},\n                  {certfile,\"/home/rtm-aimpl.crt\"},\n                  {keyfile,\"/home/rtm-aimpl.key\"}]}],\n            {shutdown,\n                {failed_to_start_child,ranch_acceptors_sup,\n                    {listen_error,rabbit_web_dispatch_sup_443,eacces}}}},\n        {gen_server,call,\n            [rabbit_web_dispatch_registry,\n             {add,rabbit_mgmt,\n                 [{port,443},\n                  {ssl,true},\n                  {ssl_opts,\n                      [{cacertfile,\"/home/rtm-aimpl-ca.crt\"},\n                       {certfile,\"/home/rtm-aimpl.crt\"},\n                       {keyfile,\"/home/rtm-aimpl.key\"}]}],\n                 #Fun,\n                 [{'_',[],\n                      [{[<<\"api\">>,<<\"overview\">>],\n                        [],rabbit_mgmt_wm_overview,[]},\n                       {[<<\"api\">>,<<\"cluster-name\">>],\n                        [],rabbit_mgmt_wm_cluster_name,[]},\n                       {[<<\"api\">>,<<\"nodes\">>],[],rabbit_mgmt_wm_nodes,[]},\n                       {[<<\"api\">>,<<\"nodes\">>,node],\n                        [],rabbit_mgmt_wm_node,[]},\n                       {[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>],\n                        [],rabbit_mgmt_wm_node_memory,\n                        [absolute]},\n                       {[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,\n                         <<\"relative\">>],\n                        [],rabbit_mgmt_wm_node_memory,\n                        [relative]},\n                       {[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>],\n                        [],rabbit_mgmt_wm_node_memory_ets,\n                        [absolute]},\n                       {[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,\n                         <<\"relative\">>],\n                        [],rabbit_mgmt_wm_node_memory_ets,\n                        [relative]},\n                       {[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,\n                         filter],\n                        [],rabbit_mgmt_wm_node_memory_ets,\n                        [absolute]},\n                       {[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,\n                         filter,<<\"relative\">>],\n                        [],rabbit_mgmt_wm_node_memory_ets,\n                        [relative]},\n                       {[<<\"api\">>,<<\"extensions\">>],\n                        [],rabbit_mgmt_wm_extensions,[]},\n                       {[<<\"api\">>,<<\"all-configuration\">>],\n                        [],rabbit_mgmt_wm_definitions,[]},\n                       {[<<\"api\">>,<<\"definitions\">>],\n                        [],rabbit_mgmt_wm_definitions,[]},\n                       {[<<\"api\">>,<<\"definitions\">>,vhost],\n                        [],rabbit_mgmt_wm_definitions,[]},\n                       {[<<\"api\">>,<<\"parameters\">>],\n                        [],rabbit_mgmt_wm_parameters,[]},\n                       {[<<\"api\">>,<<\"parameters\">>,component],\n                        [],rabbit_mgmt_wm_parameters,[]},\n                       {[<<\"api\">>,<<\"parameters\">>,component,vhost],\n                        [],rabbit_mgmt_wm_parameters,[]},\n                       {[<<\"api\">>,<<\"parameters\">>,component,vhost,name],\n                        [],rabbit_mgmt_wm_parameter,[]},\n                       {[<<\"api\">>,<<\"global-parameters\">>],\n                        [],rabbit_mgmt_wm_global_parameters,[]},\n                       {[<<\"api\">>,<<\"global-parameters\">>,name],\n                        [],rabbit_mgmt_wm_global_parameter,[]},\n                       {[<<\"api\">>,<<\"policies\">>],\n                        [],rabbit_mgmt_wm_policies,[]},\n                       {[<<\"api\">>,<<\"policies\">>,vhost],\n                        [],rabbit_mgmt_wm_policies,[]},\n                       {[<<\"api\">>,<<\"policies\">>,vhost,name],\n                        [],rabbit_mgmt_wm_policy,[]},\n                       {[<<\"api\">>,<<\"connections\">>],\n                        [],rabbit_mgmt_wm_connections,[]},\n                       {[<<\"api\">>,<<\"connections\">>,connection],\n                        [],rabbit_mgmt_wm_connection,[]},\n                       {[<<\"api\">>,<<\"connections\">>,connection,\n                         <<\"channels\">>],\n                        [],rabbit_mgmt_wm_connection_channels,[]},\n                       {[<<\"api\">>,<<\"channels\">>],\n                        [],rabbit_mgmt_wm_channels,[]},\n                       {[<<\"api\">>,<<\"channels\">>,channel],\n                        [],rabbit_mgmt_wm_channel,[]},\n                       {[<<\"api\">>,<<\"consumers\">>],\n                        [],rabbit_mgmt_wm_consumers,[]},\n                       {[<<\"api\">>,<<\"consumers\">>,vhost],\n                        [],rabbit_mgmt_wm_consumers,[]},\n                       {[<<\"api\">>,<<\"exchanges\">>],\n                        [],rabbit_mgmt_wm_exchanges,[]},\n                       {[<<\"api\">>,<<\"exchanges\">>,vhost],\n                        [],rabbit_mgmt_wm_exchanges,[]},\n                       {[<<\"api\">>,<<\"exchanges\">>,vhost,exchange],\n                        [],rabbit_mgmt_wm_exchange,[]},\n                       {[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,\n                         <<\"publish\">>],\n                        [],rabbit_mgmt_wm_exchange_publish,[]},\n                       {[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,\n                         <<\"bindings\">>,<<\"source\">>],\n                        [],rabbit_mgmt_wm_bindings,\n                        [exchange_source]},\n                       {[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,\n                         <<\"bindings\">>,<<\"destination\">>],\n                        [],rabbit_mgmt_wm_bindings,\n                        [exchange_destination]},\n                       {[<<\"api\">>,<<\"queues\">>],[],rabbit_mgmt_wm_queues,[]},\n                       {[<<\"api\">>,<<\"queues\">>,vhost],\n                        [],rabbit_mgmt_wm_queues,[]},\n                       {[<<\"api\">>,<<\"queues\">>,vhost,queue],\n                        [],rabbit_mgmt_wm_queue,[]},\n                       {[<<\"api\">>,<<\"queues\">>,vhost,destination,\n                         <<\"bindings\">>],\n                        [],rabbit_mgmt_wm_bindings,\n                        [queue]},\n                       {[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"contents\">>],\n                        [],rabbit_mgmt_wm_queue_purge,[]},\n                       {[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"get\">>],\n                        [],rabbit_mgmt_wm_queue_get,[]},\n                       {[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"actions\">>],\n                        [],rabbit_mgmt_wm_queue_actions,[]},\n                       {[<<\"api\">>,<<\"bindings\">>],\n                        [],rabbit_mgmt_wm_bindings,\n                        [all]},\n                       {[<<\"api\">>,<<\"bindings\">>,vhost],\n                        [],rabbit_mgmt_wm_bindings,\n                        [all]},\n                       {[<<\"api\">>,<<\"bindings\">>,vhost,<<\"e\">>,source,dtype,\n                         destination],\n                        [],rabbit_mgmt_wm_bindings,\n                        [source_destination]},\n                       {[<<\"api\">>,<<\"bindings\">>,vhost,<<\"e\">>,source,dtype,\n                         destination,props],\n                        [],rabbit_mgmt_wm_binding,[]},\n                       {[<<\"api\">>,<<\"vhosts\">>],[],rabbit_mgmt_wm_vhosts,[]},\n                       {[<<\"api\">>,<<\"vhosts\">>,vhost],\n                        [],rabbit_mgmt_wm_vhost,[]},\n                       {[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"permissions\">>],\n                        [],rabbit_mgmt_wm_permissions_vhost,[]},\n                       {[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"connections\">>],\n                        [],rabbit_mgmt_wm_connections_vhost,[]},\n                       {[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"channels\">>],\n                        [],rabbit_mgmt_wm_channels_vhost,[]},\n                       {[<<\"api\">>,<<\"users\">>],[],rabbit_mgmt_wm_users,[]},\n                       {[<<\"api\">>,<<\"users\">>,user],\n                        [],rabbit_mgmt_wm_user,[]},\n                       {[<<\"api\">>,<<\"users\">>,user,<<\"permissions\">>],\n                        [],rabbit_mgmt_wm_permissions_user,[]},\n                       {[<<\"api\">>,<<\"whoami\">>],[],rabbit_mgmt_wm_whoami,[]},\n                       {[<<\"api\">>,<<\"permissions\">>],\n                        [],rabbit_mgmt_wm_permissions,[]},\n                       {[<<\"api\">>,<<\"permissions\">>,vhost,user],\n                        [],rabbit_mgmt_wm_permission,[]},\n                       {[<<\"api\">>,<<\"aliveness-test\">>,vhost],\n                        [],rabbit_mgmt_wm_aliveness_test,[]},\n                       {[<<\"api\">>,<<\"healthchecks\">>,<<\"node\">>],\n                        [],rabbit_mgmt_wm_healthchecks,[]},\n                       {[<<\"api\">>,<<\"healthchecks\">>,<<\"node\">>,node],\n                        [],rabbit_mgmt_wm_healthchecks,[]},\n                       {[<<\"api\">>,<<\"reset\">>],[],rabbit_mgmt_wm_reset,[]},\n                       {[<<\"api\">>,<<\"reset\">>,node],\n                        [],rabbit_mgmt_wm_reset,[]},\n                       {[],[],cowboy_static,\n                        {file,\n                            \"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/index.html\"}},\n                       {[<<\"api\">>],\n                        [],cowboy_static,\n                        {file,\n                            \"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/api/index.html\"}},\n                       {[<<\"cli\">>],\n                        [],cowboy_static,\n                        {file,\n                            \"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/cli/index.html\"}},\n                       {[<<\"mgmt\">>],[],rabbit_mgmt_wm_redirect,\"/\"},\n                       {['...'],\n                        [],rabbit_mgmt_wm_static,\n                        [\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www\"]}]}],\n                 {[],\"RabbitMQ Management\"}},\n             infinity]}}}\nLog files (may contain more information):\n   /var/log/rabbitmq/rabbit@v189205.log\n   /var/log/rabbitmq/rabbit@v189205-sasl.log\n{\"init terminating in do_boot\",{could_not_start,rabbitmq_management,{{could_not_start_listener,[{port,443},{ssl,true},{ssl_opts,[{cacertfile,\"/home/rtm-aimpl-ca.crt\"},{certfile,\"/home/rtm-aimpl.crt\"},{keyfile,\"/home/rtm-aimpl.key\"}]}],{shutdown,{failed_to_start_child,ranch_acceptors_sup,{listen_error,rabbit_web_dispatch_sup_443,eacces}}}},{gen_server,call,[rabbit_web_dispatch_registry,{add,rabbit_mgmt,[{port,443},{ssl,true},{ssl_opts,[{cacertfile,\"/home/rtm-aimpl-ca.crt\"},{certfile,\"/home/rtm-aimpl.crt\"},{keyfile,\"/home/rtm-aimpl.key\"}]}],#Fun,[{'',[],[{[<<\"api\">>,<<\"overview\">>],[],rabbit_mgmt_wm_overview,[]},{[<<\"api\">>,<<\"cluster-name\">>],[],rabbit_mgmt_wm_cluster_name,[]},{[<<\"api\">>,<<\"nodes\">>],[],rabbit_mgmt_wm_nodes,[]},{[<<\"api\">>,<<\"nodes\">>,node],[],rabbit_mgmt_wm_node,[]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>],[],rabbit_mgmt_wm_node_memory,[absolute]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"relative\">>],[],rabbit_mgmt_wm_node_memory,[relative]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>],[],rabbit_mgmt_wm_node_memory_ets,[absolute]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,<<\"relative\">>],[],rabbit_mgmt_wm_node_memory_ets,[relative]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,filter],[],rabbit_mgmt_wm_node_memory_ets,[absolute]},{[<<\"api\">>,<<\"nodes\">>,node,<<\"memory\">>,<<\"ets\">>,filter,<<\"relative\">>],[],rabbit_mgmt_wm_node_memory_ets,[relative]},{[<<\"api\">>,<<\"extensions\">>],[],rabbit_mgmt_wm_extensions,[]},{[<<\"api\">>,<<\"all-configuration\">>],[],rabbit_mgmt_wm_definitions,[]},{[<<\"api\">>,<<\"definitions\">>],[],rabbit_mgmt_wm_definitions,[]},{[<<\"api\">>,<<\"definitions\">>,vhost],[],rabbit_mgmt_wm_definitions,[]},{[<<\"api\">>,<<\"parameters\">>],[],rabbit_mgmt_wm_parameters,[]},{[<<\"api\">>,<<\"parameters\">>,component],[],rabbit_mgmt_wm_parameters,[]},{[<<\"api\">>,<<\"parameters\">>,component,vhost],[],rabbit_mgmt_wm_parameters,[]},{[<<\"api\">>,<<\"parameters\">>,component,vhost,name],[],rabbit_mgmt_wm_parameter,[]},{[<<\"api\">>,<<\"global-parameters\">>],[],rabbit_mgmt_wm_global_parameters,[]},{[<<\"api\">>,<<\"global-parameters\">>,name],[],rabbit_mgmt_wm_global_parameter,[]},{[<<\"api\">>,<<\"policies\">>],[],rabbit_mgmt_wm_policies,[]},{[<<\"api\">>,<<\"policies\">>,vhost],[],rabbit_mgmt_wm_policies,[]},{[<<\"api\">>,<<\"policies\">>,vhost,name],[],rabbit_mgmt_wm_policy,[]},{[<<\"api\">>,<<\"connections\">>],[],rabbit_mgmt_wm_connections,[]},{[<<\"api\">>,<<\"connections\">>,connection],[],rabbit_mgmt_wm_connection,[]},{[<<\"api\">>,<<\"connections\">>,connection,<<\"channels\">>],[],rabbit_mgmt_wm_connection_channels,[]},{[<<\"api\">>,<<\"channels\">>],[],rabbit_mgmt_wm_channels,[]},{[<<\"api\">>,<<\"channels\">>,channel],[],rabbit_mgmt_wm_channel,[]},{[<<\"api\">>,<<\"consumers\">>],[],rabbit_mgmt_wm_consumers,[]},{[<<\"api\">>,<<\"consumers\">>,vhost],[],rabbit_mgmt_wm_consumers,[]},{[<<\"api\">>,<<\"exchanges\">>],[],rabbit_mgmt_wm_exchanges,[]},{[<<\"api\">>,<<\"exchanges\">>,vhost],[],rabbit_mgmt_wm_exchanges,[]},{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange],[],rabbit_mgmt_wm_exchange,[]},{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,<<\"publish\">>],[],rabbit_mgmt_wm_exchange_publish,[]},{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,<<\"bindings\">>,<<\"source\">>],[],rabbit_mgmt_wm_bindings,[exchange_source]},{[<<\"api\">>,<<\"exchanges\">>,vhost,exchange,<<\"bindings\">>,<<\"destination\">>],[],rabbit_mgmt_wm_bindings,[exchange_destination]},{[<<\"api\">>,<<\"queues\">>],[],rabbit_mgmt_wm_queues,[]},{[<<\"api\">>,<<\"queues\">>,vhost],[],rabbit_mgmt_wm_queues,[]},{[<<\"api\">>,<<\"queues\">>,vhost,queue],[],rabbit_mgmt_wm_queue,[]},{[<<\"api\">>,<<\"queues\">>,vhost,destination,<<\"bindings\">>],[],rabbit_mgmt_wm_bindings,[queue]},{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"contents\">>],[],rabbit_mgmt_wm_queue_purge,[]},{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"get\">>],[],rabbit_mgmt_wm_queue_get,[]},{[<<\"api\">>,<<\"queues\">>,vhost,queue,<<\"actions\">>],[],rabbit_mgmt_wm_queue_actions,[]},{[<<\"api\">>,<<\"bindings\">>],[],rabbit_mgmt_wm_bindings,[all]},{[<<\"api\">>,<<\"bindings\">>,vhost],[],rabbit_mgmt_wm_bindings,[all]},{[<<\"api\">>,<<\"bindings\">>,vhost,<<\"e\">>,source,dtype,destination],[],rabbit_mgmt_wm_bindings,[source_destination]},{[<<\"api\">>,<<\"bindings\">>,vhost,<<\"e\">>,source,dtype,destination,props],[],rabbit_mgmt_wm_binding,[]},{[<<\"api\">>,<<\"vhosts\">>],[],rabbit_mgmt_wm_vhosts,[]},{[<<\"api\">>,<<\"vhosts\">>,vhost],[],rabbit_mgmt_wm_vhost,[]},{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"permissions\">>],[],rabbit_mgmt_wm_permissions_vhost,[]},{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"connections\">>],[],rabbit_mgmt_wm_connections_vhost,[]},{[<<\"api\">>,<<\"vhosts\">>,vhost,<<\"channels\">>],[],rabbit_mgmt_wm_channels_vhost,[]},{[<<\"api\">>,<<\"users\">>],[],rabbit_mgmt_wm_users,[]},{[<<\"api\">>,<<\"users\">>,user],[],rabbit_mgmt_wm_user,[]},{[<<\"api\">>,<<\"users\">>,user,<<\"permissions\">>],[],rabbit_mgmt_wm_permissions_user,[]},{[<<\"api\">>,<<\"whoami\">>],[],rabbit_mgmt_wm_whoami,[]},{[<<\"api\">>,<<\"permissions\">>],[],rabbit_mgmt_wm_permissions,[]},{[<<\"api\">>,<<\"permissions\">>,vhost,user],[],rabbit_mgmt_wm_permission,[]},{[<<\"api\">>,<<\"aliveness-test\">>,vhost],[],rabbit_mgmt_wm_aliveness_test,[]},{[<<\"api\">>,<<\"healthchecks\">>,<<\"node\">>],[],rabbit_mgmt_wm_healthchecks,[]},{[<<\"api\">>,<<\"healthchecks\">>,<<\"node\">>,node],[],rabbit_mgmt_wm_healthchecks,[]},{[<<\"api\">>,<<\"reset\">>],[],rabbit_mgmt_wm_reset,[]},{[<<\"api\">>,<<\"reset\">>,node],[],rabbit_mgmt_wm_reset,[]},{[],[],cowboy_static,{file,\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/index.html\"}},{[<<\"api\">>],[],cowboy_static,{file,\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/api/index.html\"}},{[<<\"cli\">>],[],cowboy_static,{file,\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www/cli/index.html\"}},{[<<\"mgmt\">>],[],rabbit_mgmt_wm_redirect,\"/\"},{['...'],[],rabbit_mgmt_wm_static,[\"/var/lib/rabbitmq/mnesia/rabbit@v189205-plugins-expand/rabbitmq_management-3.6.9/priv/www\"]}]}],{[],\"RabbitMQ Management\"}},infinity]}}}}\ninit terminating in do_boot ({could_not_start,rabbitmq_management,{{could_not_start_listener,[{},{},{}],{shutdown,{}}},{gen_server,call,[rabbit_web_dispatch_registry,{},infinity]}}})\nCrash dump is being written to: erl_crash.dump...done\ni did not bind 443 port number to another process.i already verified. ",
    "etiennetremel": "I'm having the same issue with 3.7.1, 3.7.2 and 3.7.3-rc.1. The key\ntotal_memory_available_override_value is there since it was commented out from the entrypoint, see https://github.com/docker-library/rabbitmq/blob/master/3.7/docker-entrypoint.sh#L285-L288\nFrom what @michaelklishin mentioned, maybe we should keep these lines commented until 3.8.0 is out?\nConfiguration used:\n```\nRabbitMQ configuration\nRef: https://github.com/rabbitmq/rabbitmq-server/blob/master/docs/rabbitmq.conf.example\nClustering\ncluster_formation.peer_discovery_backend  = rabbit_peer_discovery_k8s\ncluster_formation.k8s.host = kubernetes.default.svc.cluster.local\ncluster_formation.k8s.address_type = ip\ncluster_formation.node_cleanup.interval = 10\ncluster_formation.node_cleanup.only_log_warning = false\ncluster_partition_handling = autoheal\nThe default \"guest\" user is only permitted to access the server\nvia a loopback interface (e.g. localhost)\nloopback_users.guest = false\nMemory-based Flow Control threshold\nvm_memory_high_watermark.absolute = 512MB\ntotal_memory_available_override_value = 1073741824\nlisteners.tcp.default = 5672\ndefault_pass = guest\ndefault_user = guest\ndefault_vhost = /\nhipe_compile = false\nmanagement.listener.port = 15672\nmanagement.listener.ssl = false\n```\nLogs:\n```\nBOOT FAILED\n===========\nConfig file generation failed 11:44:08.187 [error] You've tried to set total_memory_available_override_value, but there is no setting with that name.\n11:44:08.187 [error]   Did you mean one of these?\n11:44:08.305 [error]     memory_monitor_interval\n11:44:08.305 [error]     vm_memory_calculation_strategy\n11:44:08.305 [error]     vm_memory_high_watermark.absolute\n11:44:08.305 [error] Error generating configuration in phase transform_datatypes\n11:44:08.305 [error] Conf file attempted to set unknown variable: total_memory_available_override_value\nBOOT FAILED\nError description:\n    rabbit:start_it/1 line 458\n    rabbit:boot_error/2 line 858\n    rabbit_lager:log_locations/0 line 43\n    rabbit_lager:ensure_lager_configured/0 line 144\n    rabbit_lager:lager_configured/0 line 151\n    lager:list_all_sinks/0 line 317\n    lager_config:get/2 line 71\n    ets:lookup(lager_config, {'_global',handlers})\nexit:generate_config_file\nLog file(s) (may contain more information):\ninit terminating in do_boot (generate_config_file)\n{\"init terminating in do_boot\",generate_config_file}\n```. ",
    "mohitdd": "Hi All,\nwe have upgraded the rabbitmq to 3.7.3 in our existing application.\nOn running RMQ, I don't see any logs file generated.\n##  ##\n  ##  ##      RabbitMQ 3.7.3. Copyright (C) 2007-2018 Pivotal Software, Inc.\n  ##########  Licensed under the MPL.  See http://www.rabbitmq.com/\n  ######  ##\n  ##########  Logs: (none)\n          Starting broker...\n\ncompleted with 4 plugins.\nHelp Sought.. @michaelklishin could you please help here?. ",
    "shengis": "Thanks for the workarounds !\nTo be faire, I didn't tried to use the UI management. It's easier to use command line when using remote servers.. ",
    "Dean-Christian-Armada": "The question is why does it only occur when using Docker Swarm prdouction and not my local Docker. ",
    "MedusaLeee": "I kind of hit the wall at the same place.. ",
    "rolatsch": "Could you please tell me how to configure this in the classic config format?\nI have tried many things but i don't get it.\nThanks!. Thank you very much!. ",
    "adammichalik": "Thanks, I think you meant https://github.com/rabbitmq/rabbitmq-management/issues/511. ",
    "dimas": "One thing worth mentioning is that in my previous tests I used to see errors like\n2018-01-04 00:12:11.694 [error] <0.905.2530> SSL: {connection,{alert,2,20,{\"ssl_cipher.erl\",276},decryption_failed}}: ssl_connection.erl:861:Fatal error: unexpected message\n(RabbitMQ 3.6.12, Erlang 19.6.3)\nor\n2018-01-04 00:00:33.118 [info] <0.545.537> TLS server: In state {connection,\n                         {alert,2,20,\n                             {\"ssl_cipher.erl\",276},\n                             undefined,decryption_failed}} at ssl_connection.erl:848 generated SERVER ALERT: Fatal - Unexpected Message\n(RabbitMQ 3.6.14, Erlang 20.1.7)\nin rabbit logs. I do not get these in my test described in this ticket.\nMy assumption is that it is because I did not configure lager as it is configured on the live servers. If it is important, I can update rabbitmq.config for that.. That is just great you managed to find out what is going on. Thanks very much, @dumbbell !. By the way, if Erlang does not pass RabbitMQ some alert, does it also explains why RabbitMQ was not destroying that connection for the lack of heartbeats? (The connection was configured with 30 sec heartbeat but was listed by rabbitmqctl list_connections forever even after SSL error).\nCheers\n. ",
    "florianajir": "I have this error message when I try to connect my client to server with ssl; below the command I run (from the doc):\nopenssl s_client -connect rabbitmq.domain.com:5671 -ssl3\nLog:\nTLS server: In state hello at tls_handshake.erl:213 generated SERVER ALERT: Fatal - Protocol Version\nerlang v21.0\nRabbitMQ v3.7.7. ",
    "cachiama": "Thank you for your comments. We have repeated the experiment on our Linux live environment and posted the results on https://groups.google.com/forum/#!topic/rabbitmq-users/POPqrNLNZrs. Spoiler alert the drop is still present.. ",
    "liming-gd": "Met this problem again. @michaelklishin \n==> Downloading https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.7/rabbitmq-server-generic-unix-3.7.7.tar.xz\n/usr/bin/curl -q --show-error --user-agent Homebrew/1.7.5\\ \\(Macintosh\\;\\ Intel\\ Mac\\ OS\\ X\\ 10.13.5\\)\\ curl/7.54.0 --fail --location --remote-time --continue-at 0 --output /Users/liming-gd/Library/Caches/Homebrew/downloads/8a863689ea769030f83843348ac3841a9b5a8818e5fa6d35a6f1aca9b2f5e132--rabbitmq-server-generic-unix-3.7.7.tar.xz.incomplete https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.7/rabbitmq-server-generic-unix-3.7.7.tar.xz\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\ncurl: (22) The requested URL returned error: 403 Forbidden\nI can open the parent path https://dl.bintray.com/rabbitmq/all/rabbitmq-server/3.7.7/ in browser. \ud83d\ude02. ",
    "almorelle": "Thanks to @rsquelbut. ",
    "benmmurphy": "I updated the original issue but the erlang client libraries use gb_sets to handle a similar problem where the server can acknowledge multiple messages.. makes sense. kind of scary to change this because something that works better for the degenerate case might give worse performance for how most people are using it. . i had a look at collect_acks and the raw performance of the function implemented in gb_trees vs queue is about 5x as slow in the fifo use case (under R20 with take_any its about 3x) and about 3x as slow in the multiple use case. would probably have to see how long this function takes relative to the whole of processing an ack to see if it would be a problem or not.\n[https://gist.github.com/benmmurphy/66e2498ba27ca9a46fd29e89f43402b6] just beware that collect_acks gb_trees implementation is slightly broken as well.\n. i'm surprised using sorting + binary_search for a single ack would give better performance for the worst case scenario. so before for a single ack it had to traverse the whole list in order to remove the ack. so it had to do 'N' work. now, it has to sort the whole list (N log N work) and also remove an item from the list using  list:delete (worst case N work). in theory this should run slower not faster.. i think you might be right in this case the lifo performance of the original implementation of subtract_acks and the sort implementation of subtract_acks when benchmarked is very close. in my benchmark the lists:sort() version is slightly slower but i don't think it is distinguishable from noise on my machine and it might run faster on different versions of erlang/etc.\n1> c(subtract_acks_bench_original), subtract_acks_bench_original:bench_lifo().\n{4998943,{[],[]}}\n2> c(subtract_acks_bench_rmq), subtract_acks_bench_rmq:bench_lifo().\n{5047869,[]}\nthe problem is the lifo performance is still going to be so bad that it will be unusable. i run the fifo benchmarks 1000x in order to distinguish between the different results but these lifo benchmarks don't terminate in a reasonable amount of time if i run them 1000x times. to make the lifo performance usable you need about 100x performance increase. like a 2x or 3x performance increase is not going to be good enough and the only way you can get that is to use map/dict/gb* structure.\ni have some proof of concept code that uses map for subtract_acks and gb_tree for collect acks here: https://github.com/rabbitmq/rabbitmq-server/compare/master...benmmurphy:handle_degenerate_ack_ordering?expand=1\nthis was mostly to test if it would fully fix the lifo performance issue and if there wasn't any other hidden places that were slowing acks down. i can confirm it fixes the performance of the ruby bunny check. however, i've kind of approached fixing the code in the wrong way so there is probably lots of subtle breakages (or even completely broken implementation). writing these tests for the original implementation first was a much better way of approaching the problem. basically, i have little confidence in my code but it should give you an idea of the performance costs in the best fifo case and how much faster it can be in the worst lifo case.. that's interesting. i didn't test them separately and assumed both parts of the code would slow things down. \nmy new theory is the subtract_acks happens in the queue process so it blocks the queue which then becomes a problem for other people that are using the queue (ie: publishers) which is why it is showing up in the original bunny test. however, the collect_acks happens in the channel process so it blocks the channel and doesn't effect other consumers so doesn't show up in the original bunny test. i've modified the original bunny test so the consumer tries to publish after acking. without the collect_acks patch the delay is now visible in that part of the test.\nwithout the collect_acks patch:\nreceived_message: 100\n..\nreceived_message: 8000\nacknowledging_messages\npublish_time: 0.204051\nfinished_acknowledging_messages\ntrying to publish\n..\nconsumer_publish_time: 5.214303\npublish_time: 0.012243\n..\nwith the collect_acks patch:\nreceived_message: 100\n..\nreceived_message: 8000\nacknowledging_messages\nfinished_acknowledging_messages\ntrying to publish\npublish_time: 0.448687\nconsumer_publish_time: 0.073258\n```\n!/usr/bin/env ruby\nrequire 'bunny'\nQUEUE_NAME = \"test_lots_of_acks\"\nQUEUE_ARGS = {durable: true}\nACK_SIZE = 8000\nPUBLISH_INTERVAL = 0.1\ndef seed_queue\n  conn = Bunny.new\n  conn.start\n  begin\nch = conn.create_channel\nch.confirm_select\n\nq = ch.queue(QUEUE_NAME, QUEUE_ARGS)\nq.purge\n\nACK_SIZE.times do |i|\n  q.publish(\"test_message #{i}\", persistent: true)\nend\nch.wait_for_confirms\n\nensure\n    conn.stop\n  end\nend\ndef fifo_strategy(ch, tags)\n  tags.each do |tag|\n    ch.acknowledge(tag, false)\n  end\nend\ndef lifo_strategy(ch, tags)\n  fifo_strategy(ch, tags.reverse)\nend\ndef multiple_strategy(ch, tags)\n  last = tags[-1]\n  ch.acknowledge(last, true)\nend\ndef run_subscriber(strategy)\n  conn = Bunny.new\n  conn.start\n  ch = conn.create_channel\n  ch.confirm_select\n  ch.prefetch(ACK_SIZE)\n  q = ch.queue(QUEUE_NAME, QUEUE_ARGS)\ndelivery_tags = []\nq.subscribe(consumer_tag: \"test\", manual_ack: true) do |delivery_info, properties, payload|\n    if delivery_tags.nil?\n      next\n    end\ndelivery_tags << delivery_info.delivery_tag\nif delivery_tags.length % 100 == 0\n  puts \"received_message: #{delivery_tags.length}\"\nend\n\nif delivery_tags.length == ACK_SIZE\n  tags = delivery_tags\n  delivery_tags = nil\n  puts \"acknowledging_messages\"\n  strategy.call(ch, tags)\n  puts \"finished_acknowledging_messages\"\n\n  puts \"trying to publish\"\n  elapsed = time do\n    q.publish(\"timing message\", persistent: true)\n    ch.wait_for_confirms\n  end\n  puts \"consumer_publish_time: #{elapsed}\"\nend\n\nend\nend\ndef time\n  start = Time.now\n  yield\n  finish = Time.now\n  finish - start\nend\ndef run_publisher\nconn = Bunny.new(continuation_timeout: 30_000)\n  conn.start\n  begin\nch = conn.create_channel\nch.confirm_select\nq = ch.queue(QUEUE_NAME, QUEUE_ARGS)\n\nloop do\n  elapsed = time do\n    q.publish(\"timing message\", persistent: true)\n    ch.wait_for_confirms\n  end\n  puts \"publish_time: #{elapsed}\"\n  sleep_time = [0, PUBLISH_INTERVAL - elapsed].max\n  Kernel.sleep(sleep_time)\nend\n\nensure\n    conn.stop\n  end\nend\nif ARGV.length != 1\n  puts \"usage: #{$PROGRAM_NAME} lifo_strategy|fifo_strategy|multiple_strategy\"\n  exit(1)\nend\nseed_queue\nrun_subscriber(method(ARGV[0].to_sym))\nrun_publisher\n```\n. ",
    "jothan": "Does Erlang have a fast btree structure that would be appropriate for this ?. ",
    "jharting": "@michaelklishin that's awesome! Appreciate the quick turnaround.. ",
    "CiCCkany": "@michaelklishin It is in the rabbitmq.conf.example. ",
    "irfanjs": "Thanks lukebakken.\ni m using rabbitmq java library having version 1.5. \nthere is no any exception and log also does not have any info....\ni think once you have a look on code, you can easily understand where could be the issue. it might not take much time of yours ..\nplease let me know where i can paste the code ?\nregards,\n. ",
    "yanivg111": "Hi everyone,\nI know there are a lot of stuff about this issue but I was unable to find the correct answer.\nI'm following this guide trying to set up a pacemaker cluster for RabbitMQ (I know there is a newer way to set rabbitmq clustering but I've been asked to set this one):\nhttps://www.rabbitmq.com/pacemaker.html\nI've set up two ubuntu 16.04 machhines called rabbitmq1 & rabbitmq2 (rabbitmq version 3.6.15 Erlang 20.2).\nI was able to set DRBD and Corosync succfully with a reource of floating IP between them.\nBoth machines in the same VLAN and able to ping eachother by name (firewall is off).\nAfter all was set I added a this reource to pacemaker:\nconfigure primitive bunny ocf:rabbitmq:rabbitmq-server params mnesia_base=\"/media/drbd1\"\nHowever this resource showing in error when I run \"crm status\":\nbunny  (ocf::rabbitmq:rabbitmq-server):        FAILED (unmanaged)[ rabbitmq2 rabbitmq1 ]\nwhen I try to investigate it by command \"pcs resource debug-start bunny\" I see this output:\nstderr: INFO: Status of node rabbit@localhost Error: unable to connect to node rabbit@localhost: nodedown DIAGNOSTICS =========== attempted to contact: [rabbit@localhost] rabbit@localhost: erlang-solutions_1.0_all.deb connected to epmd (port 4369) on localhost erlang-solutions_1.0_all.deb epmd reports node 'rabbit' running on port 25672 erlang-solutions_1.0_all.deb TCP connection succeeded but Erlang distribution failed erlang-solutions_1.0_all.deb Hostname mismatch: node \"rabbit@rabbitmq1\" believes its host is different. Please ensure that hostnames resolve the same way locally and on \"rabbit@rabbitmq1\" current node details: - node name: 'rabbitmq-cli-65@rabbitmq1' - home dir: /var/lib/rabbitmq - cookie hash: O9bMZ3LMnkSPaakjdRYHuw==\n\nstderr: ERROR: Unexpected return from rabbitmqctl -n rabbit@localhost status: 69\n\nThen I run this command \"rabbitmqctl -n rabbit@localhost status\" and get this output:\nError: unable to connect to node rabbit@localhost: nodedown\nDIAGNOSTICS\nattempted to contact: [rabbit@localhost]\nrabbit@localhost:\n  * connected to epmd (port 4369) on localhost\n  * epmd reports node 'rabbit' running on port 25672\n  * TCP connection succeeded but Erlang distribution failed\n\nHostname mismatch: node \"rabbit@rabbitmq1\" believes its host is different. Please ensure that hostnames resolve the same way locally and on \"rabbit@rabbitmq1\"\n\ncurrent node details:\n- node name: 'rabbitmq-cli-30@rabbitmq1'\n- home dir: /var/lib/rabbitmq\n- cookie hash: O9bMZ3LMnkSPaakjdRYHuw==\nI tried copying this erlang cookie file to all usere home folder from this location /var/lib/rabbitmq/.erlang.cookie (both to the home folder of root user and to the home folder or rabbitmq user) but its not helping.\nwhat am I doing wrong?\n. ",
    "cwchien": "OK. Thank a lot :). ",
    "liusongjun": "The same problem, help me.... ",
    "SierraNL": "I would also like to see this implemented.\nLooking at the other peer discovery plugins, like the AWS one, I believe this could also be implemented for Azure Service Fabric, using the API:\nhttps://docs.microsoft.com/en-us/rest/api/servicefabric/sfclient-index\nYou can use containers in Service fabric, and give these containers a specific RabbitMQ service type, like the tags in the AWS plugin.\nSo you could make the plugin look for other RabbitMQ instances using the API to find instances of it's own service type.\nI've never tried Erlang, so I'm not comfortable writing the plugin, but I could help gather the required information or do some testing.. ",
    "jaredledvina": "Hey @michaelklishin,\nThanks so much for the quick reply! Would it be worth me PR'ing an update to https://github.com/rabbitmq/rabbitmq-website/blob/live/site/logging.md#logging-to-syslog to explicitly mark that configuration as not supported for 3.7? . ",
    "majormoses": "Should we not re-open this until we fixup the changelog and documentation about this? I don't think its a good stance to break working systems, not call our the changes, and close the issue before steps are taken to warn users.. Thanks for the clarification and quick turn around to updating the documentation.. ",
    "Bhaal22": "Yes and in the matrix: 3.5.x to 3.7.x is supported.. The cluster is one member node.\nDo you have CI pipeline on windows environment?\nMoreOver it happens only in a particular case\nWhen COMPUTERNAME environment variable != hostname\nif my hostname is \"rmq\" then environment variable COMPUTERNAME is \"RMQ\" (uppercase). could be an idea also to not fallback on default values for such a env variable.\nI think we can have the same issue on linux if the machine is renamed using uppercase in /etc/hostname\ninitially rmq and then RMQ\nI can test that on a linux container fast.. the issue I see with this is: \nWhen you start the migration with the different name RMQ -> rmq\nthen the file nodes_running_at_shutdown is updated like if it was a cluster with 2 members\nand then fails. So even if you rename your host afterwards for example, the process will fail until you remove manually the wrong entries in this file.\nYeah I understand. @michaelklishin in factissue already existed and you did a PR.\nWhich is closed but not merged as far as I see\nhttps://github.com/rabbitmq/rabbitmq-server/pull/637. I agree now we are in weird state.\nWith releases using uppercases and others no using it.\nThe PR is even not applicable. It would break existing deployment using lowercase\n. ",
    "ioradu": "Thanks. I'll post it there then.. ",
    "Tensho": "@michaelklishin Thank you for the reply \ud83d\ude47 . ",
    "karkaletsis": "Thank you very much for your response, and sorry for spamming :). ",
    "antoine-galataud": "Awesome work! \n@michaelklishin is backporting in 3.6 still in the roadmap?. ",
    "inikulshin": "Another suggestion, you'll probably reject because \"it's has been around for years\", is to add new environment variable for 3rd party plugins directories, e.g. RABBITMQ_3RD_PARTY_PLUGINS_DIRS.\nThen set RABBITMQ_PLUGINS_DIR = RABBITMQ_3RD_PARTY_PLUGINS_DIRS;RABBITMQ_PLUGINS_DIR for all internal usages.\nThis approach will solve the RabbitMQ with 3rd party plugins upgrade problem:\n\nIf 3rd party plugins are inside the RabbitMQ installation directory and RABBITMQ_PLUGINS_DIR is not set (default), RabbitMQ upgrade will fail to start service, because the 3rd party plugins were deleted and must be copied to the new RabbitMQ installation directory again.\nIf 3rd party plugins are outside the RabbitMQ installation directory (which is still not supported for Windows), RABBITMQ_PLUGINS_DIR must be updated to include new RabbitMQ installation directory before RabbitMQ installation, when the default RabbitMQ installation directory is still unknown and can be only guessed (Typically C:\\Program Files\\RabbitMQ..., but formally - unknown).\n. It is not only for Windows:\n\nSuppose RABBITMQ_PLUGINS_DIR supports multiple directories and we set it to\nRABBITMQ_PLUGINS_DIR=C:\\plugins;C:\\Program Files\\RabbitMQ Server\\rabbitmq_server-3.6.10\\plugins\nAnd now we want to write a script for RabbitMQ upgrade to version X.Y.Z, using default installation directory, which is typically known, but not guaranteed.\nPlease, advice the order of actions in a script, without service failure with \"nodedown\".\nP.S. Well, initial path C:\\Program Files\\RabbitMQ Server\\rabbitmq_server-3.6.10\\plugins is also \"guessed\", so the problem is not only in upgrade.\nP.P.S. Actually I can parse this path from Computer\\HKLM\\SOFTWARE\\Ericsson\\Erlang\\ErlSrv\\1.1\\RabbitMQ\\Args: -rabbit plugins_dir ...\nbut only after installation.. ",
    "liudasbk": "All queues are synchronized before restart (in test scenario they are empty) and in some cases they are correctly promoted. Even if they cannot be promoted, why they are not coming back when node returns to cluster? Actually they become stuck after node comes back to cluster not when it leaves.. There is one minute delay between restarts, but this sometimes fails even on the first restart (especially if VMs are tight on CPU resources). But if queues are empty and synchronized before restart, should each restart end up without any stuck queue? I get that \"ha-promote-on-shutdown\": \"always\" should resolve the issue, but why there are unsynced queues in the first place if there are no messages and enough time to sync.\nThe real life scenario is OS patching of RabbitMQ cluster, where each node is restarted and cluster ends up with stuck queues.. ",
    "eirinikos": "\nThis is a breaking change for tools that do not use Licensee, including RabbitMQ's own release pipeline. It is not uncommon for a separate license file to be required, whether it is by tools or guidelines of certain projects.\n\nThank you for pointing this out; I was unaware of this.\n\nSorry, I'm afraid we cannot accept that change. Changing a license is not a matter of just editing a file: we will have to get consensus from the contributors (keep in mind that the project is 11 years old, so there's a fair number of them) and Pivotal's legal department would have to approve this and it is extremely unlikely to happen quickly or at all (license changes were discussed before internally).\n\nThanks also for this explanation. I had not realized that I was effectively changing the license (in replacing version 1.1 with 2.0) and I understand that relicensing a project is not simply a matter of editing a file (it is also a matter of establishing consensus and understanding legal consequences). I had planned to close this PR (after receiving a ping from @dankohn), but you beat me to it. Had I known that my PR would cause a major disruption, I would not have submitted it.\n\nI did a couple of experiments and Licensee does not recognize MPL 1.1. Digging in the repo so far confirms that. That has to be addressed first.\n\nCorrect, Licensee does not recognize MPL 1.1. It remains to be seen whether the project maintainers will decide to expand the scope of the tool beyond the \"most popular licenses\" that are listed in the choosealicense.com repository.\n\nIn the meantime the README has been updated to be clearer about what MPL version is used.\n\nThis is great! \ud83d\udc4d \n. ",
    "josevalim": "Elixir v1.6.6 is out and fully supports Erlang/OTP 21. . ",
    "croensch": "Why do we need pinning in the installation routine? (https://www.rabbitmq.com/install-debian.html)\nCould we not set the min/max erlang version for the dependency to be stable? AFAIK apt also allows for installing \"unstable\" but i don't exactly know how it works.\napt show is\nPackage: rabbitmq-server\nVersion: 3.7.6-1\nDepends: init-system-helpers (>= 1.13~), erlang-nox (>= 1:19.3) | esl-erlang (>= 1:19.3), adduser, logrotate, socat`\nso like my six-step ansible script can install a stable server, or containers that get rebuild often... pinning a version there just looks overly complicated and locks that down to break later just my 2 cents\nOr am i wrong?. ",
    "postables": "Small update:\nIf I change {auth_mechanisms, ['EXTERNAL']} to {auth_mechanisms, ['EXTERNAL','PLAIN']} authentication works, however it doesn't appear to connect via TLS??. @michaelklishin  Thank you for the warning!  In production we are using up to date versions of RabbitMQ, I'm just doing this testing on my development machine quickly which is why it's using out-dated versions. What you mention about EXTERNAL is that why EXTERNAL+PLAIN works because RabbitMQ is doing the authentication via password and then using the TLS certs for encryption of traffic?\n@lukebakken  Ah gotcha, sorry for cluttering up your issues I'll post on that mailing list from now on. Haha no problem I know what it's like troubleshooting user issues with no info it SUCKS lol.\nThanks to you both :D. Gotcha. Thanks again. ",
    "Costriod": "the error log:\n\n[root@node1 rabbitmq]# rabbitmqctl stop_app\nStopping rabbit application on node rabbitmq@node1 ...\n21:50:53.772 [error]  System running to use fully qualified hostnames \n Hostname node1 is illegal \n21:50:53.791 [error]  System running to use fully qualified hostnames \n Hostname node1 is illegal \nError: unable to perform an operation on node 'rabbitmq@node1'. Please see diagnostics information and suggestions below.\nMost common reasons for this are:\n\nTarget node is unreachable (e.g. due to hostname resolution, TCP connection or firewall issues)\nCLI tool fails to authenticate with the server (e.g. due to CLI tool's Erlang cookie not matching that of the server)\nTarget node is not running\n\nIn addition to the diagnostics info below:\n\nSee the CLI, clustering and networking guides on http://rabbitmq.com/documentation.html to learn more\nConsult server logs on node rabbitmq@node1\n\nDIAGNOSTICS\nattempted to contact: [rabbitmq@node1]\nrabbitmq@node1:\n  * connected to epmd (port 4369) on node1\n  * epmd reports node 'rabbitmq' uses port 25672 for inter-node and CLI tool traffic \n  * TCP connection succeeded but Erlang distribution failed \n  * suggestion: check if the Erlang cookie identical for all server nodes and CLI tools\n  * suggestion: check if all server nodes and CLI tools use consistent hostnames when addressing each other\n  * suggestion: check if inter-node connections may be configured to use TLS. If so, all nodes and CLI tools must do that\n   * suggestion: see the CLI, clustering and networking guides on http://rabbitmq.com/documentation.html to learn more\nCurrent node details:\n * node name: 'rabbitmqcli62@node1.no-domain'\n * effective user's home directory: /var/lib/rabbitmq\n * Erlang cookie hash: tRgN9rAjHRNfqa1qWDBo2g==\n. the firewalld has been turned off . oh, thanks, I commented the USE_LONGNAME setting, then it works. XD. ",
    "82398485": "I have already solved problem. \nAt version 3.6.X , can not both output.\nBut version 3.7.X, is OK.\nThanks.\n . ",
    "Garagoth": "Nothing serious in rabbitmq.conf:\n```\nCluster formation settings\ncluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul\ncluster_formation.consul.host = 10.1.28.90\ncluster_formation.consul.deregister_after = 300\ncluster_formation.consul.svc_addr_auto = true\ncluster_formation.consul.svc_addr_use_nodename = true\nuse long RabbitMQ node names?\ncluster_formation.consul.use_longname = false\ncluster_formation.node_type = disc\ncluster_partition_handling = pause_minority\ncluster_partition_handling.pause_if_all_down.recover = ignore\nmirroring_sync_batch_size = 4096\ndisk_free_limit.relative = 1.4\nvm_memory_high_watermark.relative = 0.4\nvm_memory_high_watermark_paging_ratio = 0.6\nnet_ticktime = 30\nLogging\nlog.console = true\nlog.console.level = info\nlog.syslog = true\nlog.syslog.level = info\nlog.syslog.protocol = rfc5424\nlog.syslog.transport = udp\nlog.syslog.ip = 10.1.28.90\nlog.syslog.port = 514\nloopback_users.guest = false\ntotal_memory_available_override_value = 536870912\nlisteners.tcp.default = 5672\nhipe_compile = false\nmanagement.listener.port = 15672\nmanagement.listener.ssl = false\n```\nAnd yes, I saw those files and was suprised it is not working.\nWorks with this in advanced.config:\n```\n[\n %% Advanced configuration file - see https://www.rabbitmq.com/configure.html#advanced-config-file\n {syslog, [{protocol, {rfc5424, udp} },\n           {dest_port, 514},\n           {dest_host, \"10.1.28.90\" },\n           {app_name, \"rabbitmq\" },\n           {facility, daemon},\n           {multiline_mode, true}\n          ]\n }\n].\n```. Please note that docker images are running on different erlang. I did not try this with standalone erlang and rabbit.\nHard to mess installation there, only thing I added to docker image is consul plugin.. ",
    "blgm": "Hi @dumbbell.  Thank you for your feedback.  I have made the improvements you suggested.  Please let me know if there's anything else you'd like to see improved.. ",
    "xavierhardy": "Thanks for replying so fast, but no, we do not want them on the same host (that's why we need clustering in the first place), it's just that these nodes have the same FQDN because they are behind an LB that allows to select specific container instances using their TCP/UDP ports.\nImagine this kind of configuration:\ninstances: rabbit@4c01db0b339c , rabbit@d7886598dbe2, rabbit@d894ede837d2\n4c01db0b339c runs on host A, with the IP address 192.168.1.1 with an unknown port on the host which is exposed on TCP port 5555 on the load balancer (whose FQDN is ha.example.com).\nd7886598dbe2 on host B, 192.168.1.2, unknown host port, exposed on the same LB on TCP port 6666\nd894ede837d2 on host C, 192.168.1.3, unknown host port, exposed on the same LB on TCP port 7777\nOur only solution would be to use the container host IP addresses and to force the TCP port on the different hosts. This is risky for two reasons:\n- this host is used by other containers that select a random TCP port, our container might not start on that host if for instance, 5672 is not available\n- relying on IP addresses is a bad idea since the environment architecture assumes they might change at any moment, which is one of the reasons why people should rely on the FQDN of the LB\nI hope my explanation is clear enough, I understand that this feature might not be desirable in RabbitMQ. I think we'll have to postponed supporting HA in our environment for now. Thank you very much for your answer.. ",
    "micoq": "@michaelklishin I recompiled RabbitMQ from the latest sources and removed manually the calls to the file_handle_cache_stats:* functions and the behavior remain the same so the eprof results are probably not as relevant as I thought.\nFor your second comment, I know a single queue is backed by a single Erlang process and I already use several queues to distribute the load. I tried to understand why a single lazy queue (or with persistent messages) is twice as slow as a ram-only queue with transient messages (and with no apparent storage activity).. ",
    "mundus08": "To fix this i added \n[Service]\nPermissionsStartOnly=true\nExecStartPre=-/bin/mkdir /var/run/rabbitmq\nExecStartPre=-/bin/chown -R rabbitmq:rabbitmq  /var/run/rabbitmq\nto the systemd unit file. This works on Debian 9.. It was my fault - I use an ansible role which changes the PID file location.\nThanks for your great support!\n. ",
    "noxdafox": "Hello @lukebakken, you can find the stack trace and the steps to reproduce it in the issue I linked in the comment above.. I don't think that's the correct way to handle the problem.\nThe rabbit_backing_queue behaviour is currently not setting any restriction to the format of the data the info callback must return. Therefore, claiming that the data is \"unexpected or malformatted\" is incorrect.\nThis implies that any implementation of the behaviour is supposed to keep that into account. Nevertheless, the priority-queue implementation is enforcing a data format and by doing that it violates the API contract. Encouraging this violations severely hinders the scalability of the design. \nWe can in fact imagine what would happen if there would be more than one or two implementations of the  rabbit_backing_queue behaviour. A developer should check all the quirks of the other implementations to cope with his/her own.\nA better solution would indeed consist in defining a data format for the info callback which all other implementations could adhere to. This would also facilitate the folding/unfolding the priority-queue implementation is doing as it could rely on such format restrictions.\nI am open towards such design and willing to commit to it. But I'd rather not delve into the priority-queue logic to find a way around to fit my data into. At that point, the current workaround I implemented (not reporting any info if x-max-priority is set) is more than enough.. Uhm... there seems to be some issue with GitHub and this issue ticket as @michaelklishin comment seems to appear/disappear?\n@michaelklishin I explain what the issue is in both the commit message and in the \"further comments\". More information is also available at the linked issue. Can you please help me understanding what shall I clarify more?. According to the rabbit_backing_queue behaviour, the is_duplicate callback shall signal the queue process that the message about to be published is actually a duplicate. In such case, the queue process will not publish the message in the queue. \nNevertheless, the logic does not take this scenario into account when replying to the client. This becomes problematic if a client enables publish confirmation as the client is left hanging indefinitely. \nI did not take into account the mirrored queues implementation actually. I see how the raised concern is valid. \nI'll see if we can find a better way to tackle this. We don't want to send ack/nack back if the duplicate is the result of an internal duplication. Nevertheless, we don't want to let the client hang if a message is not enqueued because of some policy applied by the user.. I see, the behaviour documentation did not mention the callback was for internal use only.\nMay I suggest to change the callback interface or add a new one? This would still enable the Use Case without the need of affecting mirrored queues implementation.\nFor example, the is_duplicate callback could return a tuple {bool, symbol} where the symbol could instruct the process what to do. Something like {true, ignore} or {true, reject}.\nThe change would be minimal within the broker logic but I am not sure how flexible we can be when it comes to change behaviour APIs.\nEDIT: @kjnilsson was faster :). I will start by providing a RFC PR changing the is_duplicate behaviour and the related logic. \nWe can later discuss whether to introduce nack for duplicated messages and what are the implications.\n. Valid point. I will try some long running tests with HA and priority queues enabled.\nWe'll also need to change the management console message as now states: \"rejected Unable to publish message. Check queue limits.\"\nNow there are more cases where a message might not reach the queue than a mere queue overflow.. Got time to run some tests.\nSuccessfully ran several rounds with the following set up.\n- 10 queues\n- 10 priority levels per queue\n- 1 producer and 1 consumer per queue\n- Each producer publishing 10K messages per priority for a total of 100K messages per queue\n- Each consumer ensuring 100K messages are received correctly\nI did not encounter any issue. All queues were constantly empty and broker messages per second kept between 2.5K to 3K.\nI also run several tests with the above set up and the message deduplication plugin enabled to test the rejection flow. Did not observe any issue, performance close to the above mentioned. Messages were de-duplicated correctly. \nAny other scenario you think I should test?. ",
    "alues": "@michaelklishin that will nice feature for federate plugin to support sync msg from downstream to upstream?. Thank you for your time.\nDNS A/AAAA records has a large delay. cause to about several minutes unusable time.. ",
    "ctapmex": "sorry that from the beginning without logs. \ncollect_fail_node , logs_other_nodes . but without root privilege\nWe use a cluster of three servers, with 'ha-mode':all  and lazy. \nThis is dedicated servers. The same cluster configuration on virtual servers is still working without such errors.. 1. cluster of three nodes.  on dedicated bare-metal servers with SAS  HDD\n2. RMQ version 3.7.6\nErlang version 20.1.5\nEnabled plugins: [rabbitmq_auth_backend_ldap,rabbitmq_management,rabbitmq_shovel,rabbitmq_shovel_management].\n3. all nodes  - disk type\n4. enable  'ha-mode':all and lazy in policy for all vhost\n5. balancer works before the cluster, haproxy. \n6.  187 echanges , 527 queues.\n7. about 1800 connections, 4500 channels, 1625 consumers\n8. message rates on cluster: publish 8000/s , acknowledge 4000/s , confirm 2000/s, deliver 5000/s, redelivered 2000/s , \n9. message payloads  from 500 bytes to 50 Kb depending on the queue\n10. all queues durable, messages published as persistent on 80% cases\nscenario:\nThe cluster worked empty for a week without load. Then the load was applied to it from another cluster. And after about 3 hours an error occurred on msk-mbus-rmq51. ( https://github.com/rabbitmq/rabbitmq-server/issues/1696#issue-358052702) .  logs from msk-mbus-rmq51 and logs from others nodes.\nafter detecting problems and logging on to the cluster:\n- cluster worked\n- part of the queues was status 'stopped'\n- Not all queues were mirrored to all 3 nodes\nexample error from application \ncom.rabbitmq.client.ShutdownSignalException: connection error; protocol method: #method<connection.close>(reply-code=541, reply-text=INTERNAL_ERROR - access to vhost '/' refused for user 'epm_cnc': vhost '/' is down, class-id=10, method-id=40\n. @lukebakken thanks for the analysis\nMnesia overload - Is not this the result of a error badmatch?\nexclusive use - we use one exclusive queue and several consumers for reservation. If one falls, one of the others picks up.\nunknown delivery tag 185 - known error for us, the application after losing connection with rabbittmq not correctly restored the connection.. ",
    "stefanscheid": "Great, thanks. Works.\n:-DDD i mean, the upgrade doc contains a hard learning curve for those that are not born with Debian.. ",
    "tomkins": "@stefanscheid this seems to be an Ubuntu packaging bug: https://bugs.launchpad.net/ubuntu/+source/rabbitmq-server/+bug/1784757. ",
    "panjunDev": "More detailed msgs at here:\n{{shutdown,{server_initiated_close,404,<<\"NOT_FOUND - no binding .shadow.get.C2071886QnC34Jk9.G01D05017K00950 between exchange 'amq.topic' in vhost '/' and queue 'mqtt-subscription-C2071886QnC34Jk9&G01D05017K00950qos1' in vhost '/'\">>}},{gen_server,call,[<0.6950.12>,{call,{'queue.bind',0,<<\"mqtt-subscription-C2071886QnC34Jk9&G01D05017K00950qos1\">>,<<\"amq.topic\">>,<<\".shadow.get.C2071886QnC34Jk9.G01D05017K00950\">>,false,[]},none,<0.6938.12>},60000]}}\n2018-09-06 16:25:45.901 [error] <0.6938.12> CRASH REPORT Process <0.6938.12> with 0 neighbours exited with reason: {{shutdown,{server_initiated_close,404,<<\"NOT_FOUND - no binding .shadow.get.C2071886QnC34Jk9.G01D05017K00950 between exchange 'amq.topic' in vhost '/' and queue 'mqtt-subscription-C2071886QnC34Jk9&G01D05017K00950qos1' in vhost '/'\">>}},{gen_server,call,[<0.6950.12>,{call,{'queue.bind',0,<<\"mqtt-subscription-C2071886QnC34Jk9&G01D05017K00950qos1\">>,<<\"amq.topic\">>,<<\".shadow.get.C2071886QnC34Jk9.G01D05017K00950\">>,false,[]},none,<0.6938.12>},60000]}} in gen_server2:terminate/3 line 1166\n2018-09-06 16:25:45.901 [error] <0.6936.12> Supervisor {<0.6936.12>,rabbit_mqtt_connection_sup} had child rabbit_mqtt_reader started with rabbit_mqtt_reader:start_link(<0.6937.12>, {acceptor,{0,0,0,0,0,0,0,0},1883}, #Port<0.376896>) at <0.6938.12> exit with reason {{shutdown,{server_initiated_close,404,<<\"NOT_FOUND - no binding .shadow.get.C2071886QnC34Jk9.G01D05017K00950 between exchange 'amq.topic' in vhost '/' and queue 'mqtt-subscription-C2071886QnC34Jk9&G01D05017K00950qos1' in vhost '/'\">>}},{gen_server,call,[<0.6950.12>,{call,{'queue.bind',0,<<\"mqtt-subscription-C2071886QnC34Jk9&G01D05017K00950qos1\">>,<<\"amq.topic\">>,<<\".shadow.get.C2071886QnC34Jk9.G01D05017K00950\">>,false,[]},none,<0.6938.12>},60000]}} in context child_terminated\n2018-09-06 16:25:45.902 [error] <0.6936.12> Supervisor {<0.6936.12>,rabbit_mqtt_connection_sup} had child rabbit_mqtt_reader started with rabbit_mqtt_reader:start_link(<0.6937.12>, {acceptor,{0,0,0,0,0,0,0,0},1883}, #Port<0.376896>) at <0.69. ",
    "domhaas": "Thank you @lukebakken for the explanation. And thank's for the great work!\nFor anybody who has the same issue:\nEvery Server in my config has two domain addresses. One for internal (vpn) and one public. The hostname was set to the public domain (wanted that to).\nSo epmd is using the short name, so it is getting back the fqdn db1.testdomain.com, not the right one (db1.vpc.testdomain.com).\nAfter changing the hostnames of the servers to db1-3.vpc.testdomain.com it is working as expected. So this must be some kind of bug, expected as designed or I forgot something in my config.\nAnyhow, it's working now for me.. ",
    "mohag": "In RabbitMQ 3.7.9 it seems like the cipher order is reverse from what is configured in the new-style config file.\nI have:\nssl_options.ciphers.1   = ECDHE-ECDSA-AES256-GCM-SHA384\nssl_options.ciphers.2   = ECDHE-RSA-AES256-GCM-SHA384\nssl_options.ciphers.3   = ECDHE-ECDSA-AES256-SHA384\nssl_options.ciphers.4   = ECDHE-RSA-AES256-SHA384\nssl_options.ciphers.5   = DHE-RSA-AES256-GCM-SHA384\nssl_options.ciphers.6   = DHE-RSA-AES256-SHA256\nssl_options.ciphers.7   = ECDHE-ECDSA-AES128-GCM-SHA256\nssl_options.ciphers.8   = ECDHE-RSA-AES128-GCM-SHA256\nssl_options.ciphers.9   = ECDHE-ECDSA-AES128-SHA256\nssl_options.ciphers.10  = ECDHE-RSA-AES128-SHA256\nssl_options.ciphers.11  = DHE-RSA-AES128-GCM-SHA256\nssl_options.ciphers.12  = DHE-RSA-AES128-SHA256\nssl_options.ciphers.13  = ECDHE-ECDSA-AES256-SHA\nssl_options.ciphers.14  = ECDHE-RSA-AES256-SHA\nssl_options.ciphers.15  = DHE-RSA-AES256-SHA\nssl_options.ciphers.16  = ECDHE-ECDSA-AES128-SHA\nssl_options.ciphers.17  = ECDHE-RSA-AES128-SHA\nssl_options.ciphers.18  = DHE-RSA-AES128-SHA\nssl_options.ciphers.19  = ECDH-ECDSA-AES256-GCM-SHA384\nssl_options.ciphers.20  = ECDH-RSA-AES256-GCM-SHA384\nssl_options.ciphers.21  = ECDH-ECDSA-AES256-SHA384\nssl_options.ciphers.22  = ECDH-RSA-AES256-SHA384\nssl_options.ciphers.23  = ECDH-ECDSA-AES128-GCM-SHA256\nssl_options.ciphers.24  = ECDH-RSA-AES128-GCM-SHA256\nssl_options.ciphers.25  = ECDH-ECDSA-AES128-SHA256\nssl_options.ciphers.26  = ECDH-RSA-AES128-SHA256\nssl_options.ciphers.27  = ECDH-ECDSA-AES256-SHA\nssl_options.ciphers.28  = ECDH-RSA-AES256-SHA\nssl_options.ciphers.29  = ECDH-ECDSA-AES128-SHA\nssl_options.ciphers.30  = ECDH-RSA-AES128-SHA\nssl_options.ciphers.31  = AES256-GCM-SHA384\nssl_options.ciphers.32  = AES256-SHA256\nssl_options.ciphers.33  = AES128-GCM-SHA256\nssl_options.ciphers.34  = AES128-SHA256\nssl_options.ciphers.35  = AES256-SHA\nssl_options.ciphers.36  = AES128-SHA\nWhich results in:\n{ciphers,\n               [\"AES128-SHA\",\"AES256-SHA\",\"AES128-SHA256\",\"AES128-GCM-SHA256\",\n                \"AES256-SHA256\",\"AES256-GCM-SHA384\",\"ECDH-RSA-AES128-SHA\",\n                \"ECDH-ECDSA-AES128-SHA\",\"ECDH-RSA-AES256-SHA\",\n                \"ECDH-ECDSA-AES256-SHA\",\"ECDH-RSA-AES128-SHA256\",\n                \"ECDH-ECDSA-AES128-SHA256\",\"ECDH-RSA-AES128-GCM-SHA256\",\n                \"ECDH-ECDSA-AES128-GCM-SHA256\",\"ECDH-RSA-AES256-SHA384\",\n                \"ECDH-ECDSA-AES256-SHA384\",\"ECDH-RSA-AES256-GCM-SHA384\",\n                \"ECDH-ECDSA-AES256-GCM-SHA384\",\"DHE-RSA-AES128-SHA\",\n                \"ECDHE-RSA-AES128-SHA\",\"ECDHE-ECDSA-AES128-SHA\",\n                \"DHE-RSA-AES256-SHA\",\"ECDHE-RSA-AES256-SHA\",\n                \"ECDHE-ECDSA-AES256-SHA\",\"DHE-RSA-AES128-SHA256\",\n                \"DHE-RSA-AES128-GCM-SHA256\",\"ECDHE-RSA-AES128-SHA256\",\n                \"ECDHE-ECDSA-AES128-SHA256\",\"ECDHE-RSA-AES128-GCM-SHA256\",\n                \"ECDHE-ECDSA-AES128-GCM-SHA256\",\"DHE-RSA-AES256-SHA256\",\n                \"DHE-RSA-AES256-GCM-SHA384\",\"ECDHE-RSA-AES256-SHA384\",\n                \"ECDHE-ECDSA-AES256-SHA384\",\"ECDHE-RSA-AES256-GCM-SHA384\",\n                \"ECDHE-ECDSA-AES256-GCM-SHA384\"]}]},\n. @lukebakken It undermines the purpose of of these options:\n```\nssl_options.honor_cipher_order = true\nssl_options.honor_ecc_order    = true\n```\nWith those settings enabled, it results in the least secure supported ciphers being negotiated instead of the most secure ones. (I just have testssl.sh output, the server where I'm configuring in the new format does not have actual live traffic yet)\nHas server cipher order?     yes (OK)\n Negotiated protocol          TLSv1.2\n Negotiated cipher            AES128-SHA\n Cipher order\n    TLSv1:     AES128-SHA AES256-SHA DHE-RSA-AES128-SHA ECDHE-RSA-AES128-SHA DHE-RSA-AES256-SHA ECDHE-RSA-AES256-SHA\n    TLSv1.1:   AES128-SHA AES256-SHA DHE-RSA-AES128-SHA ECDHE-RSA-AES128-SHA DHE-RSA-AES256-SHA ECDHE-RSA-AES256-SHA\n    TLSv1.2:   AES128-SHA AES256-SHA AES128-SHA256 AES128-GCM-SHA256 AES256-SHA256 AES256-GCM-SHA384 DHE-RSA-AES128-SHA ECDHE-RSA-AES128-SHA DHE-RSA-AES256-SHA\n               ECDHE-RSA-AES256-SHA DHE-RSA-AES128-SHA256 DHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA256 ECDHE-RSA-AES128-GCM-SHA256 DHE-RSA-AES256-SHA256\n               DHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA384 ECDHE-RSA-AES256-GCM-SHA384\nIn this example, this results in a cipher without forward secrecy being used while ciphers with forward secrecy is intended to be higher priority. (In environments without AES-NI it might also result in an increase in CPU usage if less computationally intensive ciphers is preferred). Thanks. I currently swtiched of the server order setting, which should let the client get to an OK cipher, without getting the same result (ciphers meant for old clients being preferred) again if the order is changed in the future.. A workaround seems to be to delete /var/lib/rabbitmq/schema/rabbit.schema and restart RabbitMQ. The /usr/lib/rabbitmq/bin/rabbitmq-server script will copy the newer version in. (I'm not sure if there might be other dependencies though)\nThe package scripts might be one place to correct it, if there is a reason to copy it instead of using it from the installation location.\n(If changes are ever made that will break old configs, it might be necessary to keep the old schema and do something like providing a rabbitmq-update-conf-format (or more likely a rabbitmq-diagnostics subcommand) to let the user manually apply it?). I'm not sure why ${RABBITMQ_SCHEMA_DIR} doesn't just default to ${RABBITMQ_HOME}/priv/schema/? Is this to allow to user to load additional schemas?\n. ",
    "twillouer": "Hi, I do some test on my computer, with a debian stretch, rabbitmq 3.6.15  and rabbitmq 3.7.7 (with erlang 21.0.9), with no other node, only ha policy, and cpu is flying with 5k queues\n(for open questions n\u00b05). Hi,\nany updates on this issue ?. ",
    "kitsirota": "Have you tried using something like https://github.com/Ayanda-D/rabbitmq-queue-master-balancer to confirm your queue masters arent concentrated on any one node?\nIt would also be interesting to get the numbers for OTP 21.0 for comparison.  \n. @jaco-terbraak we had to set 10 minute minimum wait for checking if the node is healthy in 5 node clusters with HiPE enabled.  Typically HiPE adds about 5-8 mins to boot times for us.  The process will be running, and in the bosh release, the monit job will display running, but in the logs you should see a progress bar for HiPE compilation.\nOnce HiPE is compiled, it can take another 1-2 mins for the node to sync up and join the cluster (larger deployments can take up to 15mins to sync up on the last node).\nIn practice, any kind of maintenance on our clusters goes like this.  Bosh (PCF instance orchestration layer) takes 1 node down and waits 10 mins after the process is running before checking if its healthy and proceeding to the next node.\nIf youre monitoring a specific vhost you can also have your automation curl against \n<rmq-worker-ip>:15672/api/vhosts/<vhost-name> and grep for \"failed\" (I dont remember if its failed, or down).  To be sure you youre deployment is complete, you'll want to either wait for every vhost supervisor to be running or you could manually trigger a vhost supervisor restart on a node via the HTTP API.. I've been wondering about this also, this convention seems odd.  I may have missed something in the docs on this, please dont hesitate to correct me!\nIs there any reason that anything after @ wouldnt just inherit from the system's hostname?\neg rabbit@<hostname-derived-from-node>\nTo take it one step further, it seems it could be even more intuitive in case you have multiple rabbitmq instances co-located on a node (eg for dev/CI workloads) using a convention that looks something like this:\n<unique-process/cluster-identifier>@<hostname-derived-from-node>\nUltimately, it seems like anything after @ could be set to read-only and inherited from the system's current hostname.. ",
    "JochenABC": "We have reproduced this on a single node RabbitMQ 3.7.11 running on Erlang/OTP 21 [erts-10.2.3]  on Ubuntu 18.04. It only takes a few seconds to do:\nStep 1: Setup rabbitmq\nWe used these instructions to install on a fresh vm: https://computingforgeeks.com/how-to-install-latest-rabbitmq-server-on-ubuntu-18-04-lts/\nStep 2: Create 5.000 queues\nInstall python with pika lib:\nbash\napt install -y python-pip\npip install pika\nRun this python-script by running python scriptname\n```python\n!/usr/bin/env python\nimport pika\nrabbitmq_host = \"127.0.0.1\"\nrabbitmq_port = 5672\nrabbitmq_virtual_host = \"/\"\ncredentials = pika.PlainCredentials(\"admin\", \"admin\")\nconnection = pika.BlockingConnection(\n    pika.ConnectionParameters(rabbitmq_host, rabbitmq_port, rabbitmq_virtual_host, credentials)\n)\nchannel = connection.channel()\nfor i in range(0, 5000):\n    channel.queue_declare(queue=\"test{}\".format(i))\n```\nIt will create 5.000 queues. It assumes that there is a user named \"admin\" with password \"admin\".\nCreating the queues will take some cpu. However cpu usage goes down again after this.\nStep 3: Create ha-policy\nCreate ha policy by running:\nbash\nrabbitmqctl set_policy ha-all \".*\" '{\"ha-mode\":\"all\",\"ha-sync-mode\":\"manual\"}'\nCPU usage will go up and stay high - although we would expect it to go down since there are no other cluster members / no messages in the queues\nYou can now drop the policy again by running:\nbash\nrabbitmqctl clear_policy ha-all\nCPU usage will settle back to almost 0.\nObservations\nHere is rabbitmq_top output in high cpu state after enabling the ha policy:\n\nThe is rabbitmq_top output after creating the queues but NOT enabling the ha policy yet:\n\nVM graph when enabling the policy (red line):\n\nWe are affected by a high cpu issue with replication in a production cluster with about 2.5k queues and we believe that this issue could be related. Any help or advice on what to try or to run would be highly appreciated.\n(Also happy to provide an already set up test-vm if that makes reproduction easier)\nroot@jo-rabbit:/etc# rabbitmqctl status\nStatus of node rabbit@jo-rabbit ...\n[{pid,24412},\n {running_applications,\n     [{rabbitmq_top,\"RabbitMQ Top\",\"3.7.11\"},\n      {rabbitmq_management,\"RabbitMQ Management Console\",\"3.7.11\"},\n      {amqp_client,\"RabbitMQ AMQP Client\",\"3.7.11\"},\n      {rabbitmq_management_agent,\"RabbitMQ Management Agent\",\"3.7.11\"},\n      {rabbitmq_web_dispatch,\"RabbitMQ Web Dispatcher\",\"3.7.11\"},\n      {rabbit,\"RabbitMQ\",\"3.7.11\"},\n      {rabbit_common,\n          \"Modules shared by rabbitmq-server and rabbitmq-erlang-client\",\n          \"3.7.11\"},\n      {cowboy,\"Small, fast, modern HTTP server.\",\"2.6.1\"},\n      {ranch,\"Socket acceptor pool for TCP protocols.\",\"1.7.1\"},\n      {ssl,\"Erlang/OTP SSL application\",\"9.1.2\"},\n      {public_key,\"Public key infrastructure\",\"1.6.4\"},\n      {asn1,\"The Erlang ASN1 compiler version 5.0.8\",\"5.0.8\"},\n      {mnesia,\"MNESIA  CXC 138 12\",\"4.15.5\"},\n      {os_mon,\"CPO  CXC 138 46\",\"2.4.7\"},\n      {sysmon_handler,\"Rate-limiting system_monitor event handler\",\"1.1.0\"},\n      {cowlib,\"Support library for manipulating Web protocols.\",\"2.7.0\"},\n      {crypto,\"CRYPTO\",\"4.4\"},\n      {xmerl,\"XML parser\",\"1.3.19\"},\n      {inets,\"INETS  CXC 138 49\",\"7.0.5\"},\n      {recon,\"Diagnostic tools for production use\",\"2.3.6\"},\n      {jsx,\"a streaming, evented json parsing toolkit\",\"2.9.0\"},\n      {lager,\"Erlang logging framework\",\"3.6.5\"},\n      {goldrush,\"Erlang event stream processor\",\"0.1.9\"},\n      {compiler,\"ERTS  CXC 138 10\",\"7.3.1\"},\n      {syntax_tools,\"Syntax tools\",\"2.1.6\"},\n      {sasl,\"SASL  CXC 138 11\",\"3.3\"},\n      {stdlib,\"ERTS  CXC 138 10\",\"3.7\"},\n      {kernel,\"ERTS  CXC 138 10\",\"6.2\"}]},\n {os,{unix,linux}},\n {erlang_version,\n     \"Erlang/OTP 21 [erts-10.2.3] [source] [64-bit] [smp:2:2] [ds:2:2:10] [async-threads:64]\\n\"},\n {memory,\n     [{connection_readers,0},\n      {connection_writers,0},\n      {connection_channels,0},\n      {connection_other,24404},\n      {queue_procs,118434472},\n      {queue_slave_procs,0},\n      {plugins,110977900},\n      {other_proc,35293028},\n      {metrics,14281436},\n      {mgmt_db,31176656},\n      {mnesia,8983920},\n      {other_ets,8499904},\n      {binary,3249896},\n      {msg_index,28672},\n      {code,27492004},\n      {atom,1172689},\n      {other_system,14421723},\n      {allocated_unused,55224096},\n      {reserved_unallocated,18190336},\n      {strategy,rss},\n      {total,[{erlang,374036704},{rss,447451136},{allocated,429260800}]}]},\n {alarms,[]},\n {listeners,[{clustering,25672,\"::\"},{amqp,5672,\"::\"},{http,15672,\"::\"}]},\n {vm_memory_calculation_strategy,rss},\n {vm_memory_high_watermark,0.4},\n {vm_memory_limit,3266020966},\n {disk_free_limit,50000000},\n {disk_free,75307692032},\n {file_descriptors,\n     [{total_limit,32668},\n      {total_used,2},\n      {sockets_limit,29399},\n      {sockets_used,0}]},\n {processes,[{limit,1048576},{used,20386}]},\n {run_queue,1},\n {uptime,434},\n {kernel,{net_ticktime,60}}] . ",
    "bjoernHeneka": "Hey @michaelklishin I'm building with packer and AWS. So i have a plain ubuntu installation. ",
    "ar7z1": "@michaelklishin Thank you for your response and sorry for bother you!\nI've created this issue because I discovered very strange behavior (which is often a sign of repository compromise): the file Release.gpg was on Bintray about a month ago and suddenly has been removed after that.\nOk, I'll drop it into mailing list.. @michaelklishin @dumbbell Thank you for the great work!. @hairyhum Thank you! Ok, I'll fix it too. ;-). @michaelklishin, Should I add the changes in rabbitmq-plugins.bat and other bat scripts in this PR? Or create separate PRs?. @michaelklishin @hairyhum Thank you! Pushed! :-). @michaelklishin Thank you for review and merge :-). @michaelklishin Can you look at this PR? ;-). ",
    "vrsbrazil": "Hey guys!\nFirst of all, thank you for that update, it really improved the memory leak!\nI however have noticed that even though it definetly reduced the memory leak, it is still there somehow, the way I was able to reproduce it was by stopping consumption of the queue for some time (about 10 minutes) as it received payloads greater than 1MB at a rate of 2-5 per second.\nWhen you enable consumption to resume, even though the messages are consumed, the memory does not decrease. During consumption however this leak still persists, only at this version at a much lower rate, I would say the update improved memory leak at about 70%, but it is definetly still there.\nThank you.. OK, I thought this was the problem I am experiencing then. I am however getting the behaviour described above, the messages are being acknowledged and the list_queues shows no messages waiting, but the memory usage keeps enlarging, any thoughts on this? I am using the latest version.. ",
    "Docjones": "Oh, and i found another snipped in the crash.log:\n```\n2018-10-17 15:34:24 =SUPERVISOR REPORT====\n     Supervisor: {local,ldap_pool_sup}\n     Context:    child_terminated\n     Reason:     {{case_clause,{ok,{referral,[\"ldaps://ids.net)/(CN=SW_Entwicklung,OU=Gruppen,OU=Kleinostheim,DC=ids,DC=net)\"]}}},[{rabbit_auth_backend_ldap,object_exists,3,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,397}]},{rabbit_auth_backend_ldap,evaluate0,4,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,206}]},{rabbit_auth_backend_ldap,'-do_tag_queries/5-lc$^0/1-0-',6,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,708}]},{rabbit_auth_backend_ldap,do_tag_queries,5,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,706}]},{rabbit_auth_backend_ldap,call_ldap_fun,3,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,542}]},{rabbit_auth_backend_ldap,with_login,5,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,499}]},{rabbit_auth_backend_ldap,'-with_ldap/3-fun-14-',4,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,469}]},{rabbit_auth_backend_ldap,do_login,5,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,698}]}]}\n     Offender:   [{pid,<0.376.0>},{id,1},{mfargs,{worker_pool_worker,start_link,[ldap_pool]}},{restart_type,transient},{shutdown,4294967295},{child_type,worker}]\n2018-10-17 15:34:24 =ERROR REPORT====\nRanch listener rabbit_web_dispatch_sup_15672, connection process <0.709.0>, stream 1 had its request process <0.710.0> exit with reason {{{case_clause,{ok,{referral,[\"ldaps://ids.net)/(CN=SW_Entwicklung,OU=Gruppen,OU=Kleinostheim,DC=ids,DC=net)\"]}}},[{rabbit_auth_backend_ldap,object_exists,3,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,397}]},{rabbit_auth_backend_ldap,evaluate0,4,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,206}]},{rabbit_auth_backend_ldap,'-do_tag_queries/5-lc$^0/1-0-',6,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,708}]},{rabbit_auth_backend_ldap,do_tag_queries,5,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,706}]},{rabbit_auth_backend_ldap,call_ldap_fun,3,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,542}]},{rabbit_auth_backend_ldap,with_login,5,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,499}]},{rabbit_auth_backend_ldap,'-with_ldap/3-fun-14-',4,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,469}]},{rabbit_auth_backend_ldap,do_login,5,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,698}]}]},{gen_server2,call,[<0.376.0>,{submit,#Fun,<0.710.0>,reuse},infinity]}} and stacktrace [{gen_server2,call,3,[{file,\"src/gen_server2.erl\"},{line,329}]},{rabbit_auth_backend_ldap,user_login_authentication,2,[{file,\"src/rabbit_auth_backend_ldap.erl\"},{line,80}]},{rabbit_access_control,try_authenticate,3,[{file,\"src/rabbit_access_control.erl\"},{line,90}]},{rabbit_access_control,'-check_user_login/2-fun-4-',4,[{file,\"src/rabbit_access_control.erl\"},{line,75}]},{lists,foldl,3,[{file,\"lists.erl\"},{line,1263}]},{rabbit_mgmt_util,is_authorized,6,[{file,\"src/rabbit_mgmt_util.erl\"},{line,191}]},{cowboy_rest,call,3,[{file,\"src/cowboy_rest.erl\"},{line,1182}]},{cowboy_rest,is_authorized,2,[{file,\"src/cowboy_rest.erl\"},{line,346}]}]\n```. @lukebakken - Thanks a lot for the swift answer - just a question: does that mean, the config is basically ok? I will go and reqest my AD admin to have a look at that.. ",
    "lxc-vmaxx": "modify conf file. ",
    "zcgewu": "Java client connects queue error stack\n2018-09-30 15:50:43,126 WARN  Consumer raised exception, processing can restart if the connection factory supports it\norg.springframework.amqp.AmqpIOException: java.io.IOException\n at org.springframework.amqp.rabbit.connection.RabbitUtils.convertRabbitAccessException(RabbitUtils.java:112)\n at org.springframework.amqp.rabbit.connection.RabbitAccessor.convertRabbitAccessException(RabbitAccessor.java:106)\n at org.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:605)\n at org.springframework.amqp.rabbit.core.RabbitAdmin.initialize(RabbitAdmin.java:309)\n at org.springframework.amqp.rabbit.core.RabbitAdmin$10.onCreate(RabbitAdmin.java:243)\n at org.springframework.amqp.rabbit.connection.CompositeConnectionListener.onCreate(CompositeConnectionListener.java:31)\n at org.springframework.amqp.rabbit.connection.CachingConnectionFactory.createConnection(CachingConnectionFactory.java:230)\n at org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils$1.createConnection(ConnectionFactoryUtils.java:119)\n at org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.doGetTransactionalResourceHolder(ConnectionFactoryUtils.java:163)\n at org.springframework.amqp.rabbit.connection.ConnectionFactoryUtils.getTransactionalResourceHolder(ConnectionFactoryUtils.java:109)\n at org.springframework.amqp.rabbit.listener.BlockingQueueConsumer.start(BlockingQueueConsumer.java:199)\n at org.springframework.amqp.rabbit.listener.SimpleMessageListenerContainer$AsyncMessageProcessingConsumer.run(SimpleMessageListenerContainer.java:524)\n at java.lang.Thread.run(Thread.java:662)\nCaused by: java.io.IOException\n at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:106)\n at com.rabbitmq.client.impl.AMQChannel.wrap(AMQChannel.java:102)\n at com.rabbitmq.client.impl.AMQChannel.exnWrappingRpc(AMQChannel.java:124)\n at com.rabbitmq.client.impl.ChannelN.queueDeclare(ChannelN.java:733)\n at com.rabbitmq.client.impl.ChannelN.queueDeclare(ChannelN.java:61)\n at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n at java.lang.reflect.Method.invoke(Method.java:597)\n at org.springframework.amqp.rabbit.connection.CachingConnectionFactory$CachedChannelInvocationHandler.invoke(CachingConnectionFactory.java:348)\n at $Proxy150.queueDeclare(Unknown Source)\n at org.springframework.amqp.rabbit.core.RabbitAdmin.declareQueues(RabbitAdmin.java:342)\n at org.springframework.amqp.rabbit.core.RabbitAdmin.access$200(RabbitAdmin.java:46)\n at org.springframework.amqp.rabbit.core.RabbitAdmin$11.doInRabbit(RabbitAdmin.java:312)\n at org.springframework.amqp.rabbit.core.RabbitTemplate.execute(RabbitTemplate.java:600)\n ... 10 more\nCaused by: com.rabbitmq.client.ShutdownSignalException: channel error; reason: {#method<channel.close>(reply-code=404, reply-text=NOT_FOUND - failed to perform operation on queue 'order.union.returns.queue' in vhost '/' due to timeout, class-id=50, method-id=10), null, \"\"}\n at com.rabbitmq.utility.ValueOrException.getValue(ValueOrException.java:67)\n at com.rabbitmq.utility.BlockingValueOrException.uninterruptibleGetValue(BlockingValueOrException.java:33)\n at com.rabbitmq.client.impl.AMQChannel$BlockingRpcContinuation.getReply(AMQChannel.java:343)\n at com.rabbitmq.client.impl.AMQChannel.privateRpc(AMQChannel.java:216)\n at com.rabbitmq.client.impl.AMQChannel.exnWrappingRpc(AMQChannel.java:118)\n ... 22 more. \u7528\u8fd9\u4e2a\u8bd5\u8bd5\uff0c\u5f3a\u5236\u5220\u9664\u50f5\u6b7b\u961f\u5217\nrabbitmqctl eval '{ok, Q} = rabbit_amqqueue:lookup(rabbit_misc:r(<<\"vhost\u540d\">>, queue,<<\"\u961f\u5217\u540d\">>)), rabbit_amqqueue:delete_crashed(Q).'\n. ",
    "jaco-terbraak": "Thanks all for taking the time to look into this. I've posted a continuation thread here:\nhttps://groups.google.com/forum/#!topic/rabbitmq-users/XqHRTxfVVe0. ",
    "aashikam": "I still get this error. \n\n. ",
    "vvrpradeep": "Ok, will post my query there (in rabbimq-users).\nHowever just to answer your query, as mentioned in my query we are using the min-masters policy already and queues looks evenly distributed.  Distribution of the queue (master queues) per node as shown below.\nQueues/Memory from the /queues API:\nNode Name | Number of Queues | Sum of Memory (in Bytes)\nrabbit@centralinfravm1 | 180 | 43386992 (~43MB)\nrabbit@centralinfravm2 | 177 | 18253984 (~18 MB)\nrabbit@centralinfravm3 | 179 | 19671552 (~19 MB)\nGrand Total | 536 | 81312528 (~80MB). ",
    "Oduig": "I agree that it sounds like something outside RabbitMQ, otherwise we could expect more issues to be reported. At the same time it's quite some work to set up a runnable example, so it helps to know the effort will not be fruitless. Thanks! I'll put it up on GitHub and contact the user group.\nP.S. list_consumers returns a single consumer, just like your example. ",
    "songdony": "\nBut I upgraded RabbitMQ and Erlang.It's also missed.\nRabbitMQ version:3.7.2\nErlang version:10.1\n\n`The code:\n<?php      \n$queueName    = 'test_queue';  \n$exchangeName = 'test_exchange';  \n$routeKey     = 'test_route';  \n$message      = 'task--';     \n$connection = new AMQPConnection(array('host' => '127.0.0.1', 'port' => '5672', 'vhost' => '/', 'login' => '', 'password' => ''));\n$connection->connect() or die(\"Cannot connect to the broker!\\n\");\ntry {           \n        $channel  = new AMQPChannel($connection);    \n    $exchange = new AMQPExchange($channel);      \n    $exchange->setName($exchangeName);\n    $exchange->setType(AMQP_EX_TYPE_DIRECT);     \n    $exchange->setFlags(AMQP_DURABLE);          \n    $exchange->declare();\n\n    $queue = new AMQPQueue($channel);    \n    $queue->setName($queueName);\n    $queue->setFlags(AMQP_DURABLE);\n    $queue->declare();\n\n    $queue->bind($exchangeName, $routeKey);\n\n    for($i=0 ; $i<8;$i++){\n        $exchange->publish($message.$i,$routeKey);\n        var_dump(\"[x] Sent $message $i\");\n    }\n\n} catch (AMQPConnectionException $e) {\n        var_dump($e);\n        exit();\n}\n $connection->disconnect();\n?>`. ",
    "wjdavis5": "Ask your question here:\nhttps://groups.google.com/forum/#!forum/rabbitmq-users. ",
    "athinboy": "@michaelklishin \nnot delay delivery  . delivery on specific time ,    \u5b9a\u65f6\u6d88\u606f.\nsorry ,I can not access google .\nhttps://help.aliyun.com/document_detail/43349.html\nhttp://rocketmq.apache.org/docs/schedule-example/\n. ",
    "bharanidharan81": "@michaelklishin Thank you for quick response. ",
    "joaonuno": "\nThis is in no way coordinated with queue replica promotion and should not be. Queue replica promotion should in no way depend on client connections.\n\nI'm not implying that it should be or how it should behave. I'm just wondering if taking a node down when all its queues are replicated should cause such a noticeable downtime and proportional to the number of queues. Is it a bug or expected behavior?\n\nIn fact, because of properties such as exclusive and auto-delete, closing connections first makes more sense because some queues may be imminent for deletion\n\nThe queues in this test are not exclusive nor auto-delete.. ",
    "adobey": "Thanks!. ",
    "goetas": "Thanks!\nWill post the question in the ML. \n. As said in my initial message,  the issue is that stop_app does not work in my case.\nCommand results: \nroot@rabbit-master:/# rabbitmqctl stop_app\nStopping rabbit application on node rabbit@rabbit-master ...\nError: unable to perform an operation on node 'rabbit@rabbit-master'.\nLogs on the server:\nRabbitMQ hasn't finished starting yet. Waiting for startup to finish before stopping...\n. \nRunning stop runs forever.\nroot@rabbit-master:/# rabbitmqctl stop            \nStopping and halting node 'rabbit@rabbit-master'\n^C. ",
    "sluramod": "For those experiencing this problem: make sure you use path to erl executable, not symlink. In case of homebrew it will be in /usr/local/Cellar/erlang/<version>/lib/erlang/bin/erl otherwise you will get The application is not part of the firewall message at --unblock step:\n- /usr/libexec/ApplicationFirewall/socketfilterfw --add /usr/local/Cellar/erlang/<version>/lib/erlang/bin/erl\n- /usr/libexec/ApplicationFirewall/socketfilterfw --unblock /usr/local/Cellar/erlang/<version>/lib/erlang/bin/erl\n. ",
    "Niro23": "Update:\nRabbitMQ Version :\"3.6.9\"\nErlang_version: 19.0.4. ",
    "jsoref": "@michaelklishin : I can't figure out why travis doesn't like me. I'm happy with these two PRs. If you have an order of other repos you'd like me to visit, please feel free to suggest it. (I'm roughly a random walker otherwise.). Thanks. I've sent the erlang client as well. I don't think I'm going to do more today. I've stuck stars to remind me to look at mqtt&stomp. I expect I'll continue walking later... As noted above and in https://github.com/rabbitmq/rabbitmq-common/pull/302#discussion_r255809184 this change should probably be subject to approval by a lawyer). I have PRs to the upstreams for these changes, so while accepting this change now would make the code out of sync with the upstream, it shouldn't remain that way for particularly long. I'm vaguely concerned about these changes, since I really don't understand erlang, but afaict, they're just tokens and they aren't actually tied to anything else.. ",
    "thedrow": "Yes, it's expecting a longstr.\nThere's no reason why the username and password could also be a shortstr.. Yes, I'm working on it.\nI have to stress I don't know any Erlang at all.. You can close that commit I linked to.\nRun pip install tox tox-docker and then tox -e 3.7-integration-rabbitmq -- -k test_connect.. So there are no short strings in RabbitMQ?. Both don't implement shortstr inside tables.... I'm trying to improve the existing implementation.\nI'll take the errata in mind.\nI think we should introduce a symbol for shortstr somehow.\nShell I open an issue about it?. Idk, it saves some bytes on the wire :)\nWhat about integers? It seems like the clients you wrote don't use all the types. Why is that?. @michaelklishin I assumed as much :)\nI don't know how though.. ",
    "javiereguiluz": "typo? whene -> when\n. typo: messsages -> messages\n. grammar issue: q1 only get messages -> q1 only gets messages\n. get messages -> gets messages\n. typo: chaning -> changing\n. ",
    "mightydok": "Maybe tr -d '\\n' better for shell script.\n. ",
    "mattymo": "Why is this related to the patch to relocate the policy script?\n. %doc docs/examples/set_rabbitmq_policy.sh.example\nand update line 54 to match this\n. Maybe include that this file is distributed with rabbitmq-server package?\n. /usr/sbin/set_rabbitmq_policy.sh is a better path since most people will automate this via a package. But personally I would prefer to see this file as \"set_rabbitmq_policy\" with no suffix and without explicit path\n. ",
    "udf2457": "@michaelklishin bear in mind that systemd is not optional in 16.04.  ;-)\n. @michaelklishin and surely that's the very reason APT provides the ability for package suppliers to upload different packages for different releases.  The problem here is the rabbit team's blind insistence on forcing together some frankenstein \"one size fits all\" package.\n. "
}
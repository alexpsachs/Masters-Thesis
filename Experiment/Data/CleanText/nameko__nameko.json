{
    "tonybajan": ":+1:\n. :mushroom:  lgtm\n. If you call your file test_requirements.txt, it'll match our other projects and jenkins will install them.\n. lgtm, but we should get travis happy.\n. sorry, Jan & I discussed this but I forgot to put a note on here\n. lgtm and tests are passing for me locally.\n. makes lots of sense\n. lgtm :mushroom: \n. cool. Let's cut a release with this. And maybe #24? \n. :mushroom: \n. lgtm\n. if you want: :+1:\n. this is fine to merge\n. Yeah, my intention here is to make kill similar to stop, except discarding any results.\nstop works as follows:\n- Entry points are stopped, preventing any new requests from being handled.  As each is stopped, it unregisters from the queue consumer. When there are no more consumers, the consuming greenthread finishes.\n- We wait for all worker theads to complete. This can only expected to happen if there are other service process running.\n- Injections are stopped (including closing all sessions).\nDisregarding the timeout issue, kill works as follows on master:\n- kill all dependencies in an arbitrary order (including closing all sessions).\n- kill all worker threads\nAnd, as we've found, running worker threads continue to use their database sessions, starting new transactions, and so preventing the next test setup from completing.\nSwitching this order also does not work: killing the QueueConsumer's consuming thread prematurely prevents us from unregistering from rabbit properly.\nThis implementation of kill works as follows:\n- kill the QueueConsumer so we shut down the consuming greenthread and unregister from rabbit\n- kill all remaining worker threads, as we are not interested in their  results\n- kill all remaining dependencies, now that they are no longer being used.\nCurrently, killing the entrypoints does not result in the QueueConsumer stopping, as we don't unregister during kill. This would be easy to add, but we'd still need to wait for the QueueConsumer to stop before killing\nall other worker threads \u2013 and I don't see an obvious way of doing that.\n. congrats guys! :balloon: :tada: :balloon: \n. self.connection will stick around for the lifetime of the service and stay in the listening greenthread. I don't think there's any benefit to have it participate in the pool.  And this lets the worker greenpool and connection pool for responses be correctly the same size.\n. Don't need this for nameko\n. This might be clearer as _shutting_down or similar, unless there's other use cases.\n. typo here\n. this comment is no longer relevant\n. \"as-is\" doesn't seem right here, as it's been modified.\n. This isn't a uri as there's no scheme.  Needs to be renamed, or allow taking the http:// and strip it off :(\n. can we keep this file alphabetised, please\n. golang?\n. this is textbook publish-subscribe pattern, don't think you need the \"similar\"\n. namko run by itself doesn't seem to be valid \u2013 too many square brackets here\n. I'm not sure I'd use this as an example, as does_not_exist clearly does exist and deliberately returns a 404 \u2013 it's not a fallback handler. Perhaps a 204 or 403 instead?\n. maybe body over value (assuming this is the post body?)\n. I'd still follow PEP8 by making adding an Error suffix to the exceptions. There's also a confusion between NotImplementedError (the built-in exception) and NotImplemented (the built-in sentinel for comparators) which isn't a thing that can be raised.\n. I'm not sure \"interfaces\" is the clearest word here \u2013 it's often used to describe something implementation free / pure virtual, but this is where you get the concrete implementation of the dependency in the nameko lifecycle.\n. typography: hyphen should be an en dash\n. Should include the line Copyright 2015 Lifealike Ltd here. (see the appendix to the license)\n. Ideal place for a recipe for soup\n. \"manually\" doesn't mean much here \u2013 especially I was expecting a standalone rpc proxy to make the call and it's actually using entrypoint_hook. Maybe and trigger behaviour by \"firing\" an entrypoint using a helper: ?\n. for call being a namedtuple \u2013 makes so much more sense. ",
    "kollhof": "Dependency injection impl. and API is a proof of concept and to be considered work in progress.\nSee: https://onefinestay.atlassian.net/browse/OFS-397\n. I thought we'd be doing a pull request -> code review -> accept  -> merge dev cycle.\n. Hehe, you are forgiven ;-)\nSo, do we have jenkins building pull requests now?\nOn 12 March 2013 08:18, Fergus Doyle notifications@github.com wrote:\n\nentirely my fault. wanted to check that jenkins would build it correctly.\nIt won't happen again ;)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/lifealike/nameko/pull/6#issuecomment-14763301\n.\n\n\nJan-Klaas Kollhof\nPlatform Lead\nonefinestay\njan.kollhof@onefinestay.com\n5 St Johns Lane\nLondon EC1M 4BH\nhttp://onefinestay.com\n. :thumbsup: \n. looks good to me\n. would love to see a test, which reproduces the problem, other than that :thumbsup: \n. :thumbsup: \n. :thumbsup: \n. :thumbsup: \n. we are ignoring this branch\n. :thumbsup:  pending fix for build error\n. odd, locally it gets to 100%\n. missing tests but other than that :thumbsup: \n. oh and needs a minor version update\n. I wonder if the timeout should not just be for reading from the reply queue, but for the entire RPC call including the sending of the req. Other than that :thumbsup: \n. lgtm\n. As a temporary solution, this is OK\n. :thumbsup:  once travis is happy\n. :thumbsup: \n. :thumbsup:  pending travis\n. It all works, but I don't think we are quite there yet.\nOne problem is the naming and the other the way we define what extra worker ctx data keys a provider should provider values for. \n. :thumbsup:  pending the inheritance comment.\n. :thumbsup: \n. other than the comments :thumbsup: \n. :thumbsup: \n. :thumbsup: \n. :thumbsup: \n. :thumbsup: \n. :thumbsup: \n. :thumbsup:   besides the outstanding comment\n. I like @takluyver's suggestion.\nAnother option would be to use inspect.trace() inside the catch block.\n``` python\ndef foo():\n    return [a for a in None]\ntry:\n    foo()\nexcept TypeError:\n    print inspect.trace()\nwill have the current frame and foo's frame\ntry:\n   foo(1)\nexcept TypeError:\n   print inspect.trace()\nwill only have the current frame, thus it must be a type error\ndue to incorrect arguments\n```\nI think I like the getcallargs approach better than the stack introspection.\nBoth approaches are far better than using eval. \n. nice follow-up, so where is the pull request for the fix? ;-)\n. :-)\n. You kinda already moved into the territory of \"web framework\" by providing a http extension.\nI would argue even for a simple web-api it is important to separate the application logic and thus the component's URL space from hosting of the service, i.e the domain and base URL it is served from. (You would not specify the domain and port in your @http as you also should not specify the root). \nIt is very little overhead and the default would still work out of the box.\nWith respect to GET and POST handling, I think @http is and interesting proof of concept at the moment but it is starting off too high level. Nameko's events evolved from messaging. Messaging gave us low level tools and allowed us to figure out where we wanted to go with the next level of abstraction. \nI think a similar approach would be suitable for the http implementations, start with basic support for URL routing and req, resp, then simplify and abstract further. This would allow us to experiment with everything the werkzeug request and response objects give, but within the nameko framework and runtime. \nNow about using nameko as a web-framework. Lets say I have an easy way to deploy and manage nameko services locally and in a production environment. I have CI and CD figured out. I have also invested into logging, monitoring and debugging nameko applications.  And, I have a team who knows their way around nameko. Let's also assume I have heavily invested into building my own extensions.\nWhy would I want to introduce yet another framework with different deployment mechanisms, different ways to manage config, monitoring, logging, and different dev approaches?\nI may have to build new libraries to use the tools I have built for nameko, or at least refactor them to be able to use them outside of nameko etc. I then need to integrate the two systems e.g. via nameko.standalone or some other ways.\nI would rather spend my resources on making nameko.web work for me and have one consistent application framework to worry about. \nJust a thought.\n. How about accepting a config-file param?\nAs the number of extensions grows the list of args would grow making it a bit painful to list them as command line args. \nTo manage different sets of settings one is probably more likely to keep them in files, too.\n. This will move out of here!\n. I cannot find where I read this, and am not sure that this is the right approach to stop accepting messages without killing the channel.\nI will investigate.\n. consumer.cancel() should be the right operation\n. Everywhere else we have foo_bar, this one is fooBar?\n. flake8 does pyflakes. So, only need to call one.\n. fixed\n. yes\n. This is what you will be implementing.\n. moving it into a meta-class\n. yes, will discuss off-line\n. Here, will discuss off-line\n. But, but, I do!\n. -service-name instead of -service-type?\n. Why don't we default this to False if handler_type is BROADCAST otherwise leave it default True?\n. no, it does not wrap a functions\n. It is a base class, with a subclass requiring the service instance.\nIt happens that this method just does not make use of it.\nThe interface is very likely to change as we are defining the dependency injection and management.\n. None of the .call() should be supported by the proxy.\nIf these tests are here for coverage, then they can live here until the functionality has been deprecated and eventually removed.\n. This is making the Publisher instance state-full\nOne option would be to inject an actual publish() method into the service instance on call_setup() which keeps a ref to the worker context.\n. The context should be an instance of ServiceContext, which may or may not behave like a dict or even inherit from it.\nWe do have a defined interface and a minimum set of attributes.\n. oops\n. With the right logging config, one can get the file and method name.\nIt is better to use a logging message not containing anything the config can get to.\n. if we have e.g. timers taking up all workers and a consumer tries to spawn a next worker, what should the behavior be?\nWe can't block the the consumer thread as it would mean current workers could not ack their messages and we'd deadlock\n. we will need to be able to pass extra worker context data to spawn worker\n. service_class?\n. Should be a WorkerContext, similar to ServiceContext\n. needs an arg for context data set by the caller\n. srv_ctx instead of just ctx\n. args and kwargs should be 1st class attributes on the the context.\ndata should be an object defined by the initiating dependency-decorator impl.\n. this will move out of nameko altogether into a contrib package or similar\n. This pull req is purely for discussion/prototyping, it will never land like this in master and probably not even in the evolutionary branch. We are going to restructure the code before it gets merged. \n. The service class should not define it's name but rather the container associates a name with a service when starting it.\nThe Service() dependency injection may or may not be called with the name of the service it will proxy.\nIf no name is passed in, the name of the attribute it provides the injection for will be used as the service name.\nNot sure if we should support it that way. On one hand it is less to write on the other it seems a bit odd to infer the name like that.\n. probably pointless, the instance will be destroyed\n. A message can have a extra properties one of which could be a status code to indicated the payload is to be treated as a result object or an error vs encoding that info in the payload as the current code does.\n. yes\n. yes\n. yes\n. try:\n         provider = self.get_provider_for_method(routing_key)\n         srv_ctx = self._srv_ctx\n         provider.handle_message(srv_ctx, body, message)\nexcept Exception as exc:\n            self.handle_result(message, srv_ctx, None, exc)\n. registered in start, so why again?\n. should the exchange name come from config?\n. It is not really about processing the events but wether or not the queues and messages should persist in case of failure, which is the default behaviour. We don't care about reliable queues in this test, so disable it.\n. performance, rabbit may not write messages to disk, ... \nProbably should set that on all of the decorators in the test except on the ones we use for actually testing reliability.\n. Its a bit odd to have to explain the usage of params in some extra doc. If it is not clear from the test code, then that is probably an indication that the API is not clear. I think, despite a potential tiny little performance hit, it would be better to use the defaults instead of overwriting them and having people wonder why and then having to explain it in docs that have to be maintained.\n. old nova code moved into this module to keep all nova code together\n. this is not new code, just moved it here so all nova is in the same place\nno plan to change any of it as it is to be considered deprecated\n. yes, to nova\n. was needed to support subclassing RpcProvider to create a compatible NovaRpcProvider\n. because when calling rabbit management it would not create the vhost with leading / unless we encode the slash\n. can I blame that on a bad merge?\n. Awesome, no!? So far we have always been sending two messages even though one would be enough.\nThere is a good use case for multiple messages and that is for sending lists or streams. \nAnyways, old code moved here for keeping nova in one place, no intention of maintaining it much longer. \n. moved or not used anymore\n. so that the the qconsumer.gt and gt have a chance to die properly\n. moved or not used anymore\n. If it is a prefix, should it just be \"nameko\" or \"nameko.\"\nAlso, this should be on the messaging level as consumers and event handlers as well as publishers and dispatchers should support this functionality\n. If we have this data on the module we don't need it on the class, do we?\n. There is no need to assign it to a local var, no?\n. Kombu supports an AMQP URI like amqp://guest:guest@example.org:5672//foobar to use the vhost /foobar.\nBut, when using pyrabbit's API we have to encode /foobar as %2Ffoobar.\nWhat we could do is only encode it when using the pyrabbit's API and leave the value in the dict as /foobar, but this config is for pyrabbit.\n. Matt, we can't do it now, unless we keep a compat module in until we have migrated all clients to use the new pkg. \nI rather do that in a separate PR.\nFergus, yes. \n. they all don't have names because we dynamically create new classes in the fixture with new names\n. won't be missed\n. correct\n. we decided on the above\n. I don't think \"headers\" is the right term here. \nThey are headers in the messaging implementation but outside of that, would we still call them headers?\nIt could be a yes if we e.g. would create email or HTTP providers. Somehow though I feel like they are more related to the worker-context-data keys.\nI have yet to come up with a better term though.\n. I don't think it is the responsibility of the worker-context to handle this.\nAs only the provider has access to the raw data, only it knows how to turn it into context data. \nDifferent providers may have different mappings and ways to populate the data dictionary.\n. type(self).name?\n. Should this not happen at the top level of the module, so that the patching happens before any other imports?\n. We should document that nameko.messaging is for low-level messaging only and one should rather use events.\n. Why do we need a pyrabbit example?\n. Should use python's tempfile module to create a tempfile or at least a directory.\nThis should be done inside LogFile\n. Maybe call the file messaging_consumer ?\n. Maybe call the file messaging_dispatcher ?\nIt is more a server than a client, no?\n. maybe use a logger instead of print\n. Can we force that? What does it mean to forcibly stop a dep.?\n. Why?\n. hehe, should submit a patch to kombu ;-)\n. Why not put a __str__ on the RemoteError class and let it decide (like any other exc obj.) how it is stringified?\n. see stop() above, we need to kill the _worker_pool\n. _handle_worker_exited(...)\nit can be a consumer, but in this module it really is just a worker\n. replace consumer with worker\n. Publisher is already a HeaderEncoder.\n. add back the service name for debugging purposes\n. move to top\n. Wrap it in an api e.g. get_reply_event(correlation_id) which MethodProxy can wait on.\n. No warnings that we could not kill the dependencies in the time given?\n. you could make that even smaller, e.g. 0.0\n. I would call it just entrypoint or entry_point\n. we don't fail silently, this will cause the consumer to be shut down\n. It does not create any instances, just provides common functionality to register and unregister providers.\nThis class will very likely change with the future work planned for shared dependency management.\n. Are theses the issues due to the injection magic?\n. Do we really need both PROCESS_SHARED and process_shared?\n. maybe call it match_attrs?\nmissing docs for attrs\n. can't we get the Executor directly form concurrent.futures instead of the more internal looking _base module?\n. same as executor, why from concurrent.futures._base instead of concurrent.futures and why not just import Future directly?\n. This actually does not handle the thread exit but creates and returns a function which handles it.\nThis is confusing when reading the code.\nIf your intention is to keep a reference to the future consider using partial().\ne.g.\n```\nt.link(partial(self._handle_thread_exited, future))\ndef _handle_thread_exited(self, future, gt):\n     with ....\n``\n. I would call it ParallelProxy, as it proxies it's managed obj.\nIt would also be consistent with the rpc proxy\n. Could decorate do_submit with@functools.wraps(wrapped_attribute)` which might make things easier in a debugger or introspection or logging.  Wonder if we do/should that in other places in nameko as well. \n. should that not log before self._handle_container_kill()?\n. Why not just format(event_type) and let the instance stringify itself?\n. I don't believe that will work, as we want to overwrite the object \"known as\" SpawningProxy in the nameko.runners module. It would have already been imported at the time of patching, so changing it in utils would not work, I believe.\n. standard KeyError pattern \n``` python\ntry:\n  injection = injection[name]\nexcept KeyError:\n  injection = Mock()\n``\n. I would argue it should be the responsibility of WorkerContext to parse the data and apply a call_stack id as an attribute to self.\n. see comment above\n. At this point it is not guaranteed that all other providers have started. If this works fine.\nIs this just a testing utility? If so, maybe put it under/in the testing module\n. why not make it a property?\n. maybe just call it call_stack\n. would move **str** to the end of the class\n. why do we need this?\n. stop should wait till last has been unregistered\n. Jan Klaas Kollhof\n.8000looks odd as a fallback value, wouldn'tlocalhost:8000or0.0.0.0:8000` be a better default in terms of readability/understandability of the code?\n. I really like the idea of registered config keys, descriptions and defaults. I could then introspect a class and determine what configs I may need to run it and match it up with a preexisting config I already have.\nAs you said, command line args will end up getting silly, but I did not want to break compatibility. \nOne option could be to treat them as deprecated and print out an warning when used.\n. 3.10 is the lowest I could install with pip\n. ",
    "junkafarian": "like it\n. entirely my fault. wanted to check that jenkins would build it correctly. It won't happen again ;)\n. looks good pending those 2 comments\n. looks good, but the typo fix needs to be made in the function call too\n. this needed a code review\n. cool, no worries\n. COVVERRAAAAAAAAGE!!!!!!!!\n. better\n. cool :+1: \n. looking forward to using this stuff\n. :+1: pending travis\n. yup, those tests look much neater. happy with this pending travis\n. @mattbennett better?\n. looks good to me! :shipit: \n. looks good :+1: \n. Thanks @topiaruss we'll get this up on PyPI as part of the next version release\n. using alpha's, eh? stability ftw!\nI guess it's no worse than using a patched fork\n. boom! :boom: \n. jira markup? If we're planning on generating sphinx docs from docstrings, we should stick to rst\n. probably worth referencing https://onefinestay.atlassian.net/browse/OFS-397 in a comment here\n. I can't see this being used elsewhere?\n. considering we had to go digging for it, it's probably worth referencing what eventlet interface needs to be provided (and why) in the class docstring.\n. do we want to always reference the pool?  we could have this as a @property method\n. muscle memory. I've been using this convention in test cases for years, happy to change it though.\n. this should probably have an explicit assert at the end in addition to the expectation that it won't raise a timeout\n. this comment should either be actioned, removed or converted into an issue\n. Given the expansive docs for this, it's probably worth mentioning this restriction as well\n. probably want a @functools.wraps(fn) decorator here\n. Is this api a precursor to an implementation that uses service? Might be worth a comment to that effect if so.\n. stray print statement?\n. cool, I'd stick that in a comment\n. I'd make the import path of this more explicitly SQLAlchemy related. I think these providers should be as agnostic as possible\n. Yup, think the point still stands for the interim though\n. Are we expecting to have subclasses for each service specifying name as a class attribute? \nPassing name through to the constructor seems like it would be nice and explicit:\npython\nclass ServiceA(object):\n    name = 'service_a'\n    service_b = Service('service_b')\n. This feels very dependent on naming convention. I think this is an example where configuring the provider on a per project basis is reasonable. I'd suggest either:\n- accept a engine_factory arg to the constructor\n  OR\n- have a get_engine method to be overridden by a subclass\n. :heart: \n. is there a reason we're setting reliable_delivery to False for these handlers? We're expecting all of these events to be processed in the test\n. This could check the individual lengths of the handler events instead of the combined length. Removes the need for the events list\n. I suppose my question was why do we want to explicitly force this non-default behaviour\n. cool, makes sense. can you add that reasoning as a comment/docstring to the top of this file so that new test fixtures follow the same precedent\n. are we planning on having a non-nova implementation of the rpc proxy?\n. did it for readability but happy to change it\n. is there a reason self.containers isn't tidied up once they've been successfully terminated?\n. was thinking more from the api perspective: from nameko.runners import contextual_runner seemed more descriptive of the difference behaviour to ServiceRunner than from nameko.runners import service_runner. I think it's less obvious if you're outside this file.\n. was following the same format as the surrounding tests. can change to using sets if you prefer.\n. not really a fan of asserting logging as it implies a lot of knowledge of the implementation in the tests. eg. we might want to change the log level\nWill add it if you want though.\n. we have evals in here? :disappointed: \n. probably worth some usage docs for this.\n. maybe contextualised in some docs around language in general within nameko.\n. worth noting that this will break backwards compat with anything using this functionality\n. argh, sorry @kollhof :disappointed: \n. stray comma\n. maybe just link off to https://www.rabbitmq.com/download.html\n. can we please call this maths. It's one of my most hated americanisms\n. Also, if we encourage the convention, we might want to do a pass on these examples adding the _rpc suffix to proxy declarations\n. nice\n. surely \"ensure they are thread-safe\"\n. ",
    "davidszotten": "Name                      Stmts   Miss  Cover   Missing\nnameko/timer                 65      2    97%   69-70\n. loks good, pending travis\n. :+1: \n. :+1: \n. :+1: \n. are there any migration considerations? what happens if a service with this call stack code calls a service without it. and vice versa?\n. :+1: \n. :+1: \n. nice\n. :+1: \n. :+1: (pending travis... :)\n. :+1: \n. nice! a few small comments\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. what's our behaviour for rpc? (with regards to durability)\n. :+1: \n. :+1: \n. :+1: \n. :+1:  pending tests\n. the double assert in e.g. assert_stops_raising(assert_greenthread_dead) looks a little odd, but is fine\n. :+1: \n. :+1: \n. we normally don't bump versions until after a pr is signed off, in case something else gets in between\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. as discussed, consider not passing along exc.args for now since they may not be serializeable\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1:  pending rename\n. :+1: \n. :+1: \n. release notes?\n. :+1: \n. nice. changes?\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. just a note that we want to try to catch leftover rabbit connections at the end of tests explicitly\n. :+1: \n. :+1: \n. modulo https://github.com/eventlet/eventlet/issues/213 (e.g. with a local workaround), the test suite now passes on master with pypy 2.5.1\n. :+1: \n. :+1: \n. i don't believe it happened. i think it would still be interesting to do, though perhaps not high priority. i'm not sure we need to get to zero open issues, but maybe we could be categorising issues (e.g. this could be \"some-day\" or whatever)\nas for tactics, an initial step might be to add some travis targets that run with branch coverage and allow-failure (or without the limit) just to get a sense of how we are doing, or at least a local run to see how far off this we are atm\n. sure, though as per my note on the mailing list, while we work on getting you access, maybe you could have a thinkg and propose a set of tags and/or other processes to the mailing list\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: pending missing coverage\n. :+1: \n. :\n. :+1: \nrelease notes could include recommended alternative\n. :+1: \n. note to add a test for gc\n. :+1: \nawesome work!\n. included in 2.0\n. unsure if this is useful enough, so closing for now\n. https://pypi.python.org/pypi/sphinxcontrib-spelling/ might be worth a look, used by e.g. django\n. note that merging the spelling branch invalidated a bunch of comments\n. in 2.0 branch\n. :+1: \nthis can now be found at https://github.com/onefinestay/nameko-sqlalchemy\n. wrong base\n. i guess we could look for various shells. the django code linked to above looks for ipython and bpython.\n. rather than trying to keep up with a growing list of alternative repl's, maybe we can refactor things a little to make it easy for people to roll their own\n. :+1: \n. :+1: \n. :+1: \n. lol, no. just eventlet py3 support\n. i've been going back and forth on rabbitman. this (rabbit.py) is where i landed for now, but happy to discuss again\n. :+1: though we might want to hold of on landing this as part of 2.0\n. raised against wrong base\n. base_url:\nthis is moving into the territory of \"web framework\" which isn't really the sweet spot for nameko\n. request as first arg: I think the current default makes sense, again for the very basic use-case we have in mind. we could do a better job of promoting and giving examples of subclassing our providers to change their behaviour, and this might be a good example of that. \n. ah, nice catch\n. from what i can see, import_string doesn't help distinguishing missing modules from existing modules with bad imports\n. at least index.rst also needs updating. may want e.g. hello_service or some other foo_service as the name (as opposed to hello_world; c.f. hello world example)\n. :+1: \n. this looks really nice. i like the bash-style ${VAR_NAME<:DEFAULT_VALUE>}. @fabiocerqueira  would you like to raise a pull-request that implements this?\n. is it common for tools to support multiple config files like this? (none of the ones i can think of do)\n(i tend to handle this kind of sharing in my config management tooling). :+1: \n. :+1: \n. oh dear, thanks for spotting! we've been writing and re-writing the docs to make sure they're clear, and forgot the readme!\n. hey, nice catch. though i wonder if we should consider changing our defaults to use the default rabbit vhost (/) instead. \n. could possibly decouple \"new default\" from \"better error reporting\"\n. :+1: \nchanges/version bump?\n. :+1: \n. amazing work!\nneed to read the code, but a small initial comment from reading the summary: i'm not sure we should be encouraging the use of pickle (even as an example serializer/alternative to json)\n. hey, thanks for the contribution! will try to find time to review as soon as possible\n. cool. had a quick look at drying up the tests and noticed they fail if bpython or ipyhton are installed. added a fix on top (see #252)\n. thanks!\n. nice catch. actually, a whole heap of the examples have bugs\n(c.f. $ for x in docs/examples/*.py; do python $x; done)\nneed to experiment with timeout, since at least the standalone rpc example will just wait forever for a reply, but would be nice to add a test that checks that they at least don't raise exceptions\n. possibly, but only in the long term. gevent has different semantics to eventlet, so it's not a straight swap. at the moment, nameko is built assuming eventlet.\nwe have talked about abstracting out the differences and making nameko work with either eventlet or gevent, but that's only an idea at this point\n. thanks. will fold this into some ongoing work to add some tests for the examples. \n. :+1: \n. due to some accidental confusion, this feature actually landed in 2.1.2\n. where are you seeing this in 2.1.1 docs?\n. interesting. thanks for pointing this out. @mattbennett any idea what might have gone wrong with readthedocs (or how to trigger a re-build)?\n. :+1: \n. nameko is based on eventlet. not used grequests, but sounds very possible that they don't play nicely together. \npart of the point of using nameko is that you shouldn't need grequests, but rather use plain old requests, and get concurrency from having multiple workers running\n. nice, thanks\ndefinitely found a couple of bugs (including one in quantifiedcode :p)\n. please update changes\n. :+1: \n. i'm sure there was some reason for the way we structured this, but can't recall it, so happy to try this way around\n. :+1: \n. looks good. we probably want a beta run against the larger projects at ofs\n. this failed the spellchecker i'm afraid.  (installation.rst:41:plugin: near the bottom of https://travis-ci.org/onefinestay/nameko/jobs/72576685)\nplease add plugin to the dictionary at docs/spelling_wordlist.txt\n. great, thanks\n. awesome! thanks\n. Hi,\nthanks for the contribution!\nI might be missing something, but what does this add over just\n@http('GET', '/expected_custom_exception')\ndef expected_custom_exception(self, request):\n    return 400, json.dumps({\n        'error': 'INVALID_REQUEST',\n        'description': 'This is invalid request.',\n        'code': 400\n    })\nlike your example, the content type is text/plain but should probably be application/json, which can be achieved by returning a 3-tuple (status_code, headers dict, response body) instead of just (status code, response body)\n. ok, i see what you mean (though one might wonder why your service wants to notify sentry about caller errors. do your webapps log 404s or invalid forms to sentry?)\ngoing along with this though, dumping json into exceptions feels to me like mixing responsibilities. i think a better solution would be to open up for customising the way exceptions are turned into http responses (currently https://github.com/kooba/nameko/blob/custom-error-payload/nameko/web/handlers.py#L94). in fact, an early version of the web handler had a much more generic implementation (see e.g. https://github.com/kooba/nameko/blob/1a416524d52107fc93831bc42076bf5c14588da1/nameko/web/handlers.py#L69) we scaled it back until we could get a better feel for what we might want out of this type of feature. \nwe might not need something quite this generic (protocol, vs say just a method in charge of this that you can subclass HttpRequestHandler and override to customise)\n. is this not a duplicate of #262 ?\n. not released yet. there's an rc branch\n. nice. :+1: \n. can probably drop the redundant arg\n. it seems unusual to expose an endpoint to remotely shut down a service. is this definitely what you want when you say \"stop the service programatically\". Typically this is done e.g. via a process manager like supervisor, which stops (or kills) the service via posix signals. E.g. nameko run listens for SIGTERM and calls stop\n. seems like a reasonable approach\n. this probably warrants a line in the changelog since we are changing default behaviour when using our helpers to test apps\n. other than that i'm happy\n. :+1: \n. could we share the code that creates the config object based on the yaml file and or command line arguments to keep the two copies from diverging?\n. Hi. As discussed on #275 i think this is a good approach. a few comments inline\n. awesome! :+1:  sorry it's been a bit of a slug, but we got there in the end. please update CHANGES (for the currently pending version, and add yourself to contributors\n. n.rpc.greeting_service.set_delivery_options is problematic because we can't distinguish it from an rpc call to set_delivery_options\nis the main use case the standalone proxy or injected rpc proxies? i wonder if a better option is to make it easy to subclass the extension and set these options there\n. hm. elsewhere we work pretty hard to not infringe on \"user namespaces\", so would prefer to avoid. there might be workarounds (e.g off the top of my mind set_delivery_options(rpc_proxy, options), though with some thought we might be able to come up with something nicer)\ni didn't mean to just say \"subclass and override yourself\", but rather for us to see if we can tweak/reorg the classes and their interactions to make it fairly straightforward to create a subclass that changes rabbit options\n. Hi (and thanks)\nin nameko, each service \"owns\" its own events, so in that sense it doesn't quite make sense to talk about \"the same\" event from different services. However, if the apis (event data) are compatible, you can stack the entrypoint (any method can be exposed as multiple entyrpoints), e.g.\npython\n@event_handler(\"service_a\", \"event\")\n@event_handler(\"service_b\", \"event\")\ndef bla(self, payload):\n    pass\nwould that do?\n. sure. one of our design goals is to keep things composable enough that you can replace parts to suit your exact use-case\nnote that the decorator implementation you are overriding is designed to work for any entrypoint. since you have a concrete use-case, you can simplify it (e.g. cutting out the second half since an event-handler decorator always takes arguments)\n. we need to cut a new release before this shows up on e.g. readthedocs, but have updated the channel topic for now\n. this used to be the behaviour, but it was changed for 2.0 to wait first. @mattbennett do you recall the reason?\n. in the meantime, you could of course subclass the timer to change the behaviour:\npython\nclass MyEagerTimer(Timer):\n    def _run(self):\n        self.handle_timer_tick()\n        super(MyTimer, self)._run()\n. the problem is that in the current implementation, we measure how long self.handle_timer_tick(). but that function calls spawn_worker which is asynchronous, and returns as soon as the handler is spawned, and doesn't wait for it to finish.\nin the example above, you see 'tick' every second, even though the handler takes 5 seconds to run\n. to me it looks like the pr waits interval - elapsed_time, which is what the original code tried to do, and my preferred choice i think. is this your option 1?\n. Hi,\nThere are a few options for debugging. (i)pdb is one of them, and pdb.set_trace() inside a service method should work just like any other python process. What didn't work as expected here?\nThe backdoor is a way to get a shell inside the process of a running service, possibly one that's been running for a while, and could even be enabled in production. It looks like you have encountered a bug (it shouldn't crash when not finding netcat, but instead try other alternatives. Maybe you could raise a separate issue about the crash, and post some more info (os, python version) to help us debug that.\nWhen the backdoor works, you get a python shell prompt, from where you can interact with the objects inside the service process, e.g. the container, running workers, dependencies and other objects.\nI think the mailing list https://groups.google.com/forum/#!forum/nameko-dev is a better place for the general debugging discussion. Perhaps you could post a simplified example of what you are trying with pdb and how it's not behaving as expected?\nObviously you could also separately add logging statements (or even print) for debugging, but we should be able to do better. If by \"eager mode\" you mean turning rpc calls into function calls that you can step through, that's something I've thought about but not really spent any time one.\nHope this helps,\nDavid\n. Hi Yuri,\nI don't know anything about mprpc, but the nameko http entrypoint is broker-less so you should be able to get some ideas there.\nFrom a quick glance, mprpc is build on top of gevent, which might not play nicely with eventlet\n. @mattbennett it looks like mock's started moving again. how do we feel about bumping the minimum required version (would fix the issue here)\n. i'm having second thoughts on bumping mock, since nameko itself, and not just its test suite depends on mock. not sure i want to mandate mock version in order to use nameko\n. unfortunately not, since one of the issues is in nameko/testing/pytest.py which is part of the main codebase\none option (that we've considered previously) is to package the test helpers separately, though that has other drawbacks instead\n. i'd like to investigate the state of mock under the new maintainers before forcing an upgrade. don't mind if we leave this open or close in the mean time\n. brought up to date. didn't need to bump mock all the way, so left \"oldest\" where i could\n. Hi,\nEdit (tldr): Not out of the box, but the pieces are there for you to build one if you want. Happy to guide you through this, though the mailing list is probably a better forum. \nOriginal thoughts:\nThe WebSocketHubProvider is a dependency provider. Listening to connection events probably maps best to a nameko entrypoint. We don't have one out of the box, but it should be relatively straightforward to build one yourself. Like the WebSocketRpc entrypoint it would make use of the WebSocketServer, which would need to be subclassed to have add_websocket and remove_websocket also look for providers and invoke them (as it does in handle_websocket_request).\nAs I said, if you're interested in building this and want a hand, please post to the mailing list and I'd be happy to help.\nBest,\nD\n. Hi,\nNameko generally encourages extension by subclassing the Extensions and Entrypoints. In this case, perhaps something like\n``` python\nHttpRequestHandler lives in nameko.web.handlers\nclass CorsHttpRequestHandler(HttpRequestHandler):\n    def response_from_result(self, args, kwargs):\n        response = super(CorsHttpRequestHandler, self).response_from_result(*args, kwargs)\n        response.headers.add('Access-Control-Allow-Origin', '')  # or whatever\n        return response\ncors_http = CorsHttpRequestHandler.decorator\n```\n. re latest vs master: oh dear, that's been lying around for a while. will try to get a release out. thanks!\n. hi, yes sorry. given the required eventlet bump i think i want to at least mention that in the changelog and make the bump a bit bigger. apologies for the delay\n. hm. not sure what i think about this. will try to find some time to consider properly\nconcerned about backwards incompat and \"non-safe\" default, but also more generally about having semantics that make sense. e.g. if we have distinct use cases for the current and this new mode of operation, maybe we should allow both. or is this \"obviously\" the way one would like to use broadcast?\n. so have been thinking a bit about this, but not sure i can articulate my concerns very well\ni think there are a few independent issues (which keep getting muddled in my head). an attempt\n1: changing the default behaviour: i guess i'm not convinced that the new behaviour is \"better enough\" than the old one to change the default. esp. given that (#104) we might eventually end up going the other way on this. so maybe we it would be preferable to enable the new behaviour but leave the old default\n2: i'm not so keen on this api for supplying a differentiator. \na) it's an argument that only applies/means something for certain combinations of other arguments\nb) it feels a bit limiting, and would maybe be better exposed by the extension using a method to produce the queue name (which can be overridden by a subclass for your use-case, either in nameko or in your app/shared lib for your projects)\nc) defaulting to gethostname can lead to strange behaviour if there are multiple processes running with the same service on the same host. i guess when you call it deployments there's an implicit assumption that this isn't the case\n. as discussed, let's raise instead of just warning in the default implementation of broadcast_identifier if reliable_delivery is set to True. \nThis stops a naive user from shooting themselves in the foot by just enabling broadcast and leaving the rest as default, and also provides a loud warning to anyone upgrading about the backwards incompatible change (allowing them to explicitly set it back if they need)\n. :+1: modulo spelling\n. \ud83d\udc4d \nchangelog\n. Hi,\n1. I believe the reasons for a relatively low default were \n   a) the initial use-case (onefinestay) didn't have a huge number of concurrent activity, and \n   b) while the work was mostly io-bound, there were tasks which used a non-trivial amount of cpu, which \"starves\" all other greenthreads. This was mitigated by not having too many concurrent workers. Depending on your use-case, a much larger number of concurrent workers may work fine.\n2. Tested in the sense that it's been run in production environments that deal with sizeable amounts of data without any noticeable increase in long-term memory usage over time. During its development there have been memory leaks, but to my knowledge they have all been fixed. Of course no software is perfect but a memory leak would certainly be considered a bug.\n3. Nameko has been used in production at onefinestay for several years at this point, running the main backend systems.\n4. I generally run it with supervisord. If you launch your services with nameko run myservice, the runner installs a signalhandler for SIGTERM that requests a graceful shutdown (stops accepting requests, lets current requests finish and then exits)\nThe mailing list might be a better forum for continued discussion.\nBest,\nDavid\n. hey, thanks for the report. i remember thinking about this when pep 492 was accepted. i wonder if we want to go a step further and start issuing deprecation warnings for the old name, since we'll probably have to drop it when async becomes a keyword\nthat doesn't have to be done as part of this, but wanted to raise the issue while i remembered\nwhat i would like to see from this pr are\n- consensus on the name (since async will eventually have to go). i'm fine with call_async i think. @mattbennett any thoughts?\n- a test to codify the new name\n- the docs updated  (only docs/examples/async_rpc.py i think)\nthanks again!\n. i think only maintainers can trigger re-build. have done so. iirc, the log disappears when builds are re-reiggered, so posted in a gist if anyone wants to investigate\n. cool. please also add a note in the changelog (backwards compat so can just bump the minor version) and add yourself to the bottom of contributors (if you want). see e.g. https://github.com/onefinestay/nameko/commit/41013e8ab4683f4cfeb6108f4f1b7c8897b72e6e for an example\n. please see comment regarding spelling\n. \ud83d\udc4d \n. Thanks so much for the contribution!\n. circus is a supervisor like supervisord. iirc, it has a mode where the supervisor process owns the listening socket, and passes file descriptors for accepted connections to its children (i think supervisord can also do this)\nthe advantage is that the listening socket is kept open during app restarts\ni haven't looked into the mechanics of how this is implemented. i also don't know if this is the only way to run webapps through circus\n. yep. this is fine. (changelog entry?) \ud83d\udc4d \n. \ud83d\udc4d  pending ci\n. thanks for the pr! a few small comments\nalso, feel free to add yourself to CONTRIBUTORS.txt as part of this pr\n. \ud83d\udc4d \n. i don't know for certain, but i would guess that eventlet isn't compatible with cython, since it can't monkey patch the c code. you might be able to call cythonised code from a pure-python service, but nameko itself, and e.g. the rpc extensions are unlikely to work\n. hm. i thought we were... the deployment is done via travis. might be a bug.\nhttps://pypi.python.org/pypi/nameko/2.2.0 has both\nhttps://pypi.python.org/pypi/nameko/2.3.0 is wheel only\nnothing changed on the deployment side between those. will try to investigate, though if you have time, maybe you could have a look through the travis logs to see if anything stands out\n. non-tagged just means a regular pull-request build. only releases (tagged commits) get deployed.\nsuccess: https://s3.amazonaws.com/archive.travis-ci.org/jobs/83554040/log.txt\n500 error from the wheel upload: https://s3.amazonaws.com/archive.travis-ci.org/jobs/129506085/log.txt\nlooks like it might be a pypi issue:\nhttps://mail.python.org/pipermail/distutils-sig/2016-June/029083.html\nmight be able to do the same on travis, but hard to experiment in this area\nhttps://docs.travis-ci.com/user/deployment/pypi\n. for the pretty small part of that already fairly small library, you could probably just reimplement\n. i'm sorry if i wasn't very clear but my point definitely wasn't that we should provide our own general  @retry utility that we have to maintain. if a user of nameko needs that, they can just use retry from pypi. what did i mean was, since you are only using @retry with all the defaults (and in particular, no backoff, max_delay or allowed_exceptions), that we'd just implement a very simple decorator that takes no arguments and provides just the behaviour we need. given that we're decorating an internal function use by our pytest plugin, we probably don't even need wrapt (though i mind less depending on it)\n. \ud83d\udc4d \n. i believe the method argument is passed straight to werkzeug, so those probably just work\n. been thinking we should be saying something like \"nameko web is powered by werkzeug, the same wsgi library that underpins flask\"\ncan also change the heading from \"HTTP GET & POST\" to just \"HTTP\"\n. since we are just trying to reset the consumer, ignoring ioerrors as well seems fine. i'd like to understand when we get those instead of thesocket.error that we are already catching. if this turns out to be what we want, we can probably catch both together except (Foo, Bar) and amend the comment\n. cool, thanks for the investigation and the patch! \n(looks like you have a pyflakes error causing the tests to fail)\n. cool! if you fancy, you can amend this pr to add yourself to the list of contributors in the project root\n\ud83d\udc4d \n. optional parameters are supported, and the example above works for me\n```\n\n\n\nn.rpc.greeting_service.hello(12)\n{u'a': 12, u'b': None}\n```\n\n\n\nnote that the first parameter a in your example is required:\n```\n\n\n\nn.rpc.greeting_service.hello()\n\n\n\n...\n(as with regular python function calls, the self argument is included in the count)\nIncorrectSignature: hello() takes at least 2 arguments (1 given)\n``\n. i agree that the former is probably better. clear apis with messy or confusing implementations are better than the other way around\n. \ud83d\udc4d \n. nice idea! api-wise, did you consider just expanding the existingreplace_dependenciesto accept kwargs as well as args?\n. hm. that's a good point. though i guess we could keep the same behaviour with respect toargsand return values (so only returning the replacements corresponding to args, since the caller already owns the ones corresponding tokwargs`)\nit does make things a bit more complicated (and means we can't use kwargs to tweak behaviours in any way should we want to in the future; not sure if that's an issue)\ni think my preference is to expand the existing command rather than introducing a new, very similar one, but i'm open to other arguments against\n. given that we got this far with the args only api, i guess that's a pretty common case so i think it would be nice to keep it\n. i think there also an issue around the return value (because kwargs aren't ordered)\n. Hi,\nWe're going to need some more info to have any chance of helping you. To start with, could you specify your os+version, your python version, the steps you took to install nameko, the full command you are running above, and the full traceback. Also, for any terminal output, copy/pasting the text is easier to deal with than taking screenshots. \n. curious what your use-case for replacing these is\n. ok. do you also use a custom worker context? (does that definitely need to be configurable. ofs used that to transition to nameko, but have you come across use-cases beyond that?)\n. \ud83d\udc4d \n. i'd guess that your service is failing to connect to the rabbitmq broker. is rabbitmq running? is your service configured with the correct url to connect to it?\nwe should probably try to catch this and raise a more user-friendly error message\n. Hi,\nThis is not a problem with nameko, but a bug in the example GreetingService: \nhttps://github.com/onefinestay/nameko/blob/master/docs/examples/helloworld.py#L8 calls \"Hello, {}!\".format(name), which fails with non-ascii names. You can fix it by prefixing the string with u, (so u\"Hello, {}!\".format(name)) or by calling from __future__ import unicode_literals at the top of the helloworld.py\nLeaving this open, as we should get our examples fixed\n. i'm still on the fence about this. e.g. u prefix vs unicode_literals, or even just \"assuming\" py3\n. \ud83d\udc4d \n. Hi,\nThanks for the pull request!\nA few initial thoughts:\nSSL is a very generic key for something that only applies to the amqp extension. Maybe AMQP_SSL or AMQP_SSL_CONFIG or something along those lines?\n(unsure about this, but maybe) do we want to somehow group together the hostname (currently passed as an arg, but could be a kwarg instead) and ssl parameters (internally) askombu_connection_params, amqp_connection_params or similar to have them controlled from a single place?\n. is there a good way to end-to end test this (on e.g. travis)?\n. yes, nice new feature https://github.com/blog/2247-improving-collaboration-with-forks but off by default for prs that predate the feature\n. Hi,\nIs this from python or from some other language? For non-nameko python code, nameko ships a \"standalone rpc proxy\" which may be used to make rpc calls to nameko services\nFor non-python (or if re-implementing the protocol), the exclusive, guid named queue is, as you guessed, for replies. The caller is responsible for managing its own reply queue (nameko creates one per service on startup), and should pass the name of its reply queue in the reply_to header\n. If i recall correctly\nthe request and response bodies are json encoded data. for requests, (this is all a bit python centric for historical reasons) the body is a dict with two keys, args and kwargs meaning what they typically do in a python context (args is a list of unnamed params and kwargs is a dict of named params). i'm pretty sure nameko rpc is kwarg only which makes it easier to evolve apis)\nservices have queues bound on the nameko-rpc exchange, and should use the routing key <service name>.<method name>\nnameko uses a custom header call_id_stack to track the \"lineage\" of rpc calls (for debugging and other tracing). any nameko hosted service will generate a call_id when handling calls, (<service>.<method>.<uuid>) and push that onto the call ids for any subsequen calls it makes. you can leave this out entirely, though we found it pretty handy. similarly the built-in standalone proxy generates a call id to start this list\nthe correlation_id is used by the requesting service to multiplex replies on the same queue, to tie replies back to requests. a service should read this and include it with its reply, which it should post to the queue provided as reply_to.\nReplies are a dict with two keys, result and error. error is null for succesful calls and contains a dict for errors (result may be null for successfull calls that return nothing). error dict is as per https://github.com/onefinestay/nameko/blob/master/nameko/exceptions.py#L86\nnameko services ack their only after successfully replying for an \"at least once\" delivery model\nwill try to write this up somewhere at some point\n. kombu 4 is now out and we're not compatible out of the box, so we're pinning it below 4 for now\n. for production usage of the standalone proxy, i agree that a connection pool is required to get reasonable performance. perhaps to the extent that we should ship one in nameko. i think the library from clef linked above has a slightly nicer implementation, e.g. only instantiating proxies on demand.\nif we do provide one, it would be nice to have enough hooks to e.g. tie in to the django/flask request cycle\nthere are some further considerations, e.g. around call ids that i think the version of this i wrote for ofs (never got open sourced iirc)\n. i wasn't aware of queue ttl. that sounds like a good fit for reply queues (the rabbitmq docs for this even mention \"RPC-style reply queues\"). it's been around since 2010 so should be safe enough to switch to and should be backwards compatible. . in the past when we've talked about this, it was often required on a per-call basis. it's tricky because all parameters to rpc are considered rpc paramters\n. 1. no, i think the idea of timeouts is sound. though i'd say it's about ergonomics more than style\n2. we managed ok without timeouts at ofs (though the idea did come up from time to time, and we never got around to it). it's worth noting that the absence of timeouts does provide an extent of backpressure\na few other thoughts. there are two sides to what you are calling \"timeout\" here, client side (aborting the wait) and server side (some kind of ttl). do we definitely wanto to tie these together? (probaby yes?) we might need to think about how we handle the latter, e.g. (of the top of my head)\na) does it propagate to rpc calls made by the responding worker?\nb) does nameko set a timer and kill a worker handling a request with a ttl?\nyour example uses the standalone proxy, but i think the rpc extension is as, if not more important to consider\ni think i'm on the fence on configuring via context manager vs adding another call syntax with args and kwargs params\n. a few general comments: will try to look at code in more depth later:\ntype casting. i think that's fine to leave up to the user (extension author)\ni'm not a big fine of utils.py (or helper, misc etc). just becomes a grab bag of unrelated stuff. can this live in cli.config or something similar? (or even just in the one module that uses it)\n. i'm actually a little surprised we weren't already doing this. i've started requiring 100% coverage of tests even for projects that don't achieve this for the library code (yet), to avoid that kind of mistake (or duplicate test names etc)\n. \ud83d\udc4d \n. pull requests to the docs are most welcome\n. do you folks not use anything like this?\n. sounds good to me\n. nice catch. thanks\n. let's leave open until the fix lands\n. i'm hesitant to have such a blunt ignore. should at least check the errno i think. but would like to get a better understanding of what's going on. maybe this is actually a bug in one of the libraries we depend on. \n. can you reproduce reliably? what happens if you do the same thing to a flask and/or werkzeug hello world?\n. still investigating. i wonder if it's related to https://bugs.python.org/issue13322\n. nope. just an eventlet bug (codepath only used with py3). working on a patch. not sure what the best fix/workaround is going to be. there's no errno on the OSError but i don't think we want to blanket swallow them. might have to resort to comparing the msg :(\n. https://github.com/eventlet/eventlet/pull/353\nin combination with a slightly unhelpful error message from cpython\n. i think our assumption is that an uncaught exceptions could be anything, so the safest thing to do is suicide. \nin this particular case, it might be possible to somehow attribute this exception to the worker (hm. maybe not, since we might not have parsed enough of the request yet). if we can't, then this is an error on the extension level -> suicide\nand again, for this particular case, this particular exception (which is due to a bug) will now (once we catch it) no longer bubble out, so no reason to suicide\n. the eventlet release cadence is very jerky. and even if they release a version with this fix, are we going to set that as the minimum required version for nameko?\n. not yet. but given the bug, i think what we need is a socket with closed (or is_closed, don't recall now) False, but that raises an error with one of the error codes considered SOCKET_CLOSED https://github.com/davidszotten/eventlet/blob/_recv_loop_socket_closed/eventlet/greenio/base.py#L71\n. \ud83d\udc4d \n. i don't see the new job in this travis build\n. \ud83d\udc4d  (note that the coverage might be different for different python versions, but it's probably not worth worrying about until we get one of them sorted)\n. as a workaround you can use nameko shell --interface=plain to force the standard interpreter and avoid the bug\n. Hi,\nNot out of the box, but if you look in nameko/cli/run.py, it should be fairly straightforward to write a custom main that loads config from wherever and just passes that to run\n. started looking at kombu 4 compatibility. in praticular, it switches to (py-)amqp 2.0 (kombu 3 pins amqp below 2). 2.0 is pretty nice, e.g. has much better error messages for connection issues, and nicer basic return support.\nit might be a bit tricky to support both versions, at least without version and/or feature sniffing, which makes coverage tricky (or requires suppressing cov requirements) [should we consider running a single travis job so that we can combine coverage reports e.g. from different python versions]\ni guess the argument can be made that (unlike other requirements like 'requests'), nameko \"owns\" the kombu requirement (i.e. we wouldn't expect user code to depend directly on kombu), so we could decide to (bump our version and) just require kombu 4. not sure how i feel about that, esp. given how recently it came out. i don't really want to have to start maintaining multiple minor versions concurrently (i.e. nameko 3.x with kombu 4 and nameko 2.x with kombu 3)\nthoughts and other considerations welcome\n. agree that we are in no rush, though it's nice to be compatible with the latest versions of libraries, to be sure to get security updates and bug fixes (not sure what kombu's policy is, e.g. if 3.x will stay maintained at all)\n. for future reference i wonder if we want to hold off closing issues until we release a version with the fix. thanks\n. i guess once the toxiproxy lands we can add integration tests for the heartbeat. \ud83d\udc4d \nneeds changelog. Hi Steven,\nThank for the patch! Will try to find time to get back to you in the next few days. Apologies for the delay. \nBest,\nD. web_config and friends are pytest \"fixtures\", shared test helpers. nameko ships a pytest plugin nameko/testing/pytest.py where some of these are defined and can be used not only by the nameko test suite, but in test suites of apps and libraries that have nameko installed. Hi,\nTo get you started, you can just hook in your own config loader, and then hand off to existing nameko machinery\nhave a look at https://github.com/nameko/nameko/blob/master/nameko/cli/run.py\nThe command line entry point nameko run does the command line parsing and then hands off to run(services, config, backdoor_port=None) defined in that module. You can just write your own entry point that loads config from consul and then calls run\ni guess this is more 1 than 2, though no patching should be required. \nin general, our config story isn't as good as we'd like it to be, so we might consider bringing this in, but if not, it should be easy to turn the above into a nameko-consul package to join the eco system.. eventlet 0.20 did change some dns handling, e.g. https://github.com/eventlet/eventlet/blob/master/NEWS#L4\ncould you post some more details? is it possible to reproduce with eventlet only (without nameko). we don't yet support kombu >=4, but we do support eventlet 0.20, see e.g. tests passing on travis for https://github.com/nameko/nameko/pull/388. when/where did we conclude 1 (that kombu 4 requires eventlet 0.20)?\n(agree w.r.t. 2). if you have any further questions, feel free to post to the mailing list: https://groups.google.com/forum/#!forum/nameko-dev. this seems fine. at some point we might want to use a sorted data structure to make the order more predictable, but let's leave that for another day. may be worth a note that this is read only access to the config. don't have a coveralls account, but i think there's a tickbox to disable these comments (to reduce the notification spam) could you try to disable please?. thanks!. could you rebase this on top of master please?. Hi,\nIt depends a bit on what entrypoint you are using. For nameko (amqp) rpc, multiple instances of the same service will be load balanced by the rabbitmq broker.\nIf you use the http entrypoints, you need to put a load balancer in front of your services to distribute traffic.\nIn general it depends on the implementation of the entrypoint.\nFor future reference, please direct general questions to the mailing list instead of the issue tracker: https://groups.google.com/forum/#!forum/nameko-dev. i think there's a performance regression between v2.4.3 and v2.4.4. even with 10 workers, there's a gap between each chunk of 10 being handled. still looking. might this be related to the heartbeats added in that release?. Unless you have a re-write in progress, i think we need to look at ways to reverse this regression. Having the out-of-the-box defaults capped at 10 requests/second isn't great. well max_workers / safety_interval changed by 10x, going from what i might consider \"acceptable out of the box\" of 100/s to less so at 10/s. agreed. @mikegreen7892003 would you like to make a pr bumping it to max_workers + 1? (if not, i'm happy to but it was your idea). thanks for finding and the suggested fix!. hi. in case you've not been following the recent discussions you may be interested in https://github.com/nameko/nameko/pull/429#issuecomment-294789346 and the subsequent release of 2.5.4. Thanks! added some tests + fixes in #419 (on top of your commit here). please see @mattbennett's comment above (you touched the code so it's considered \"outdated\" by github) but it's still relevant. re: the failing tests: i guess the clue was that it was failing the build that uses our oldest supported dependency versions. in this case, the log behaviour of eventlet's wsgi server that you are making use of was introduced in 0.17.2 (pr)\nthe currently oldest supported version of eventlet is 0.16.1\nnot sure how that was chosen, but maybe we could consider bumping our minimum requirement. yes that sounds fine. it seems eventlet removes point releases from pypi but 0.17.4 is only another few months behind 0.17.2 so also fine. i've seen builds with 0.20.1 pass on travis. do you have an example of what doesn't work?. thank you\nas a library, we try to keep our requirements as open as possible, so that application developers can choose the versions that best work for them. the downside of this is, as you noted, that we are more susceptible to upstream dependencies causing issues. if this becomes a problem we can (and do) exclude or pin down certain versions (currently kombu is pinned below 4)\ni'm not yet sure of the scope of this particular issue, but if needed we can pin down eventlet. in the mean time i've raised https://github.com/eventlet/eventlet/issues/401\nthanks again for the report. Hi all,\na patch has been merged into eventlet and the author (@temoto) is asking people to help validate that it fixes the problem. if you have a chance, please reply to his comment. what makes you \"unconvinced that it [prefetch count bump] works as intended\"? \nwhen trying to reproduce the timeout currently on travis locally, are you only trying os x? or also linux (vm and/or docker)?. Hi,\nthanks for noticing the sentence being off. however, i don't think the maybe(/may be) should be there at all, but rather read \"... but uses a single reply queue ...\". looks like another bug in the docs. the problem is variable re-use cluster_proxy = cluster_proxy.start()\nthe stop should be called on the first object, but it's replaced by the result of start().\nthe best option is probably to use the context manager version (though we should fix the docs). argh, forgot to update travis. are there projects that manage travis/tox integration automatically?. iirc tox-travis isn't flexible enough\nstill on the fence about the approach in this pr. Hi Sergey, and thanks for the contribution!\nHowever, as Config also serves as simple example of a dependency provider, i think i would prefer to keep it as simple as possible.\nYou are of course able to create a subclass that better suits your needs as part of your own projects, or a standalone package\nBest,\nDavid. Hi,\ngood catch. would you like to try to write a fix? . fixed by #451. manually declaring errors to be unrelated (even if correct in this instance) is too error prone for my liking. have re-triggered the (sub) job that failed. unfortunately i think travis only lets maintainers do this. This is excellent! \nSome of the libraries (e.g. flask-nameko) aren't \"extensions\" in the nameko sense. do we want a separate section? or maybe just rename the page to something slightly more generic?. also, didn't realise you were still using cagoule! still working ok? given the size of that file, it might be worth re-writing history to remove all traces of it before landing this pr. since we already depend on werkzeug we could also investigate using the werkzeug reloader. no, i was hoping you would :p. is the issue that stopping and starting again ends up re-doing all the work anyway and so we wanted to make this explicit?. i guess my only worry would be if we make it seem like we're re-using machinery but are in fact re-setting everything up (with performance implications). fair. @bobh66 as discussed i'm happy to accept this. thanks again for the report and the pr!\ncould you please add a small note to the changelog, and (if you want) add yourself to the contributors list?. yep, that's our normal process. just mark it 'unreleased'. thanks. hey, thanks for the pr. i like the idea, though we may want to think about how we identify remote exceptions. while \"module path\" works pretty well for builtin exceptions, i'm not sure if we want to make the internal module structure of a service part of its api (see e.g. @remote_error('test.test_remote_error.RemoteError') from your test)\nmaybe <service name>.<exception type>? (though that might have other issues)\nthoughts? . just thinking about this a little more, we pass exception args along as well, don't we? maybe instead of decorating local classes, we could (/should?) just supply a list of exception types we want re-hydrated, and have the framework generate exception classes. they could then also have a common superclass (maybe some subclass of remoteerror) \nstill a half-formed thought but wanted to mention it\nwhat non-rpc req/response entrypoints did you have in mind @mattbennett ?. not had a chance to finish those yet, sorry. this was failing this morning when i checked. i guess you re-ran the job (so we still have some flaky tests :(. are pyflakes and pylint py2/3 independent?. it's not very well documented yet unfortunately, but did you know that the following is currently possible:\npython\n    @http('GET', '/multiple')\n    @http('POST', '/multiple')\n     def method(self, request):\n         return 200, request.method. wanna add it the the community-extensions list? (which is on master but not yet on \"stable\" which is default on readthedocs :(. unfortunately we don't officially support windows (none of the core devs use it so was hard to test on, and not sure we've checked that all dependencies support windows) though i would love to make that happen\nin your case, it looks like we're failing to connect to your rabbitmq server. you mention that's running linux while nameko is on windows. is it still accessible on localhost? (from the windows machine, as per your config). in addition to ping, are you able to connect to the rabbitmq port (5672) from the windows machine?. Hi,\nThanks for the contribution! However, i'm a bit confused, since that file includes from __future__ import unicode_literals, so that formatting string should already be unicode, not ascii.\nWhat version of nameko and python is this on? Could you post a minimal example to reproduce the error please?. Right, now i'm with you. Had misread decode vs encode.\nI think the best way to stay sane with regards to unicode is what e.g. ned batchelder calls the \"unicode sandwitch\". Basically, you want to decode your bytes as early as possible (and if required, encode back as late as possible)\nIn your example, since you know you are getting back bytes representing a utf8 encoded string,  i think you want to raise Exception(ba.dcode('utf-8')). nice idea! like it. ah, nice catch. for compatibility with old eventlet versions and py3, nameko pins pytest at 2.7, but only for the nameko test suite. it probably makes sense to test our \"latest\" with a newer pytest as well, esp since we only pin down in nameko's dev requirements. arg removed in #496 (can be restored for older pytest by installing pytest-catchlog). released as 2.8.2. does https://github.com/nameko/nameko/commit/6aa00da824c6e138a249c27cf50abdc47ff72dd9 only affect travis?. so that fix is now out, but we can't use it because the http is broken instead?. not sure i understand the reason for the tox change. extending should work, but you may have to be more specific with the runner, nameko run module:ServiceClass. for future reference, the mailinglist is probably a better place for questions like this. thanks. 5 minutes sounds like quite a long time to be disconnected. how did you choose it?\nregarding hanging rpc requests if the reply queue has been deleted, would this be possible to detect (e.g. on reconnection, check if reply queue still exists)?. Race may be wrong word, but connection might close between checking and acking\nSent from my phone\n\nOn 16 Feb 2018, at 11:23, Matt Yule-Bennett notifications@github.com wrote:\n@mattbennett commented on this pull request.\nIn test/conftest.py:\n\n@@ -161,6 +161,12 @@ def reset(self):\n     yield controller\n     controller.reset()\n\n\n\ndelete proxy\n\n\nallow some grace period to ensure we don't remove the proxy before\n\n\nother fixtures have torn down\n\nresource = 'http://{}/proxies/{}'.format(toxiproxy_server, proxy_name)\neventlet.spawn_after(10, requests.delete, resource)\nNo, a process won't wait for greenthreads to finish before exiting.\n\nThis cleanup really isn't required because we stop the whole toxiproxy server at the end of the test run. I added it while debugging a hanging toxiproxy (which turned out to be http://tanin.nanakorn.com/blogs/373) thinking that the problem might be the growing number of proxies that we add and never remove.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. > As implemented, we end up with a consumer, channel and connection for every queue from which we consume.\n\nany concern about the size of this for service which consumes a lot of different events? does rabbit cope ok?. sounds good. pre-release testing will also be good to stress-test the rest of the machinery since this is a pretty big re-write. the old stuff had issues but was at least battle-tested\nstill reading over the code and comments from that will get collated by github and get posted in a batch. what happened with this rc? are we ok to release 2.9.1?. error sounds fine for unexpected exceptions, but warning might be too high for expected ones, which (at least in my understanding) represent caller errors, eg malformed input. . might be easier to just namespace the logger. hi,\nit looks like you might have two service instances with the same name running, one with that method and one without it. \nfor future reference, the forum is usually a better place for questions like this.\n. that sounds like a strange (and undesirable) behaviour to me. is it a nameko design decision? what are the reasons for it?. Can you not just quote the value?. nameko always uses the mock library (for py2 compat, though i suppose it could be imported conditionally) so you have an instance of mock.Mock, not unittest.mock.Mock. is your FooBar definitely a dependency provider? only those get replaced by mocks by the worker factory. so don't know for sure but i wonder if we needed pytest-cov because testdir.runpytest used to launch a subprocess and wouldn't otherwise be seen by coverage. now it seems to happen in-process so coverage run works (and `pytest-cov gets confused because the main test suite already imports the plugin before coverage can start, c.f. http://pytest-cov.readthedocs.io/en/latest/plugins.html)\nalso for some reason py36 doesn't seem to like eventlet 0.20 so bumped oldest for that one to 0.21. guess we might cut a point release with the stuff currently pending on master and then at least bump the minor version for this change . sounds reasonable. i haven't done a thorough review of the pr (but happy if you have). we need a changelog entry and if you want you can add yourself  to contributors.txt. i did add a comment to this effect on the kombu upgrade pr but it was ignored. i figured i could just do it afterwards. i guess i'm +0. \nregarding implementation i guess it might now be worth factoring out . nice catch! +1. though this is an easy way to implement it, i wonder if this makes for a confusing api, especially for new users who may not understand the somewhat subtle difference \nmaybe we should have two different (eg thin wrappers) for a \"cluster\" vs \"service\" proxy. sounds reasonable.\nthough this made me think of something else: if we are considering changing the names, maybe we should also put the word Proxy on the table. not sure how we landed on that name in the first place, but maybe e.g Client or something else would make a better name. I don't have any strong views on actual names, though i think i prefer two separate ones. ServiceRpc and ClusterRpc seem fine. this seems fine to me\nnot sure i'd say a ServiceRpc is a ClusterRpc etc (this inherits atm) . It\u2019s defined as a subclass. sorry for being unclear. it's the fact that it's a subclass (=\"is a\") that i'm (mildly) poking at. Indeed. Hence only mildly prodding at this. nice!. Hi,\nThanks for the contribution!\nA small note: You write \"to run nameko you need [rabbitmq]\" which isn't quite right. You need rabbitmq to use the built-in rpc functionality that's used in the example, but it's not required by the core framework. I'm not sure what the bast way to phrase things but i think it's worth making this distinction clear\n(@mattbennett This is making me wonder if we should look for a different hello-world example. http has fewer dependencies but i think may be less good at making it clear how we're different from web frameworks like flask)\n. not sure why this is still marked as failing in gh, looks fine to me in travis. is this definitely a use-case we want to support?. can we have this alphabetically to help future merge conflicts please?\n. do we not want a try/finally around this?\n. would we be interested in both?\n. not (\n. \"should not be called directly\": rename to _run?\n. make this behaviour configurable (?)\n. maybe  coverage report --fail-under=100\n. possibly give the comments their own line to keep width down?\n. did you want to add ^?\n. ok i promise\nOn 23 October 2013 13:32, Matt Bennett notifications@github.com wrote:\n\nIn Makefile:\n\nfull-test: requirements test\n+\n+coverage_check:\n-   coverage report | grep \"TOTAL.*100%\" > /dev/null\n\nIf we're going to avoid this problem by running coverage externally\n(eventually) then let's merge this for now. Just don't make any files\ncalled TOTAL\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/onefinestay/nameko/pull/64/files#r7155340\n.\n\n\nDavid Szotten\nonefinestay\ndavid.szotten@onefinestay.com\n5, St John's Lane\nLondon EC1M 4BH\n. as discussed offline\njust return a DependencyDescriptor, possibly renamed ...Factory\nkeep @injection for registration only\n. still on the fence, but what would you think about this being __init__(self, dep_cls, *init_args, **init_kwargs):?\n(and this e.g. return DependencyFactory(HttpEntrypoint, bind_port) instead of DependencyFactory(HttpEntrypoint, (bind_port,))\n. probably worth changing name so as to not clash with http://www.python.org/dev/peps/pep-0249/#programmingerror\n. OrmSession? :p \n. since this is purely internal, consider a longer, but more descriptive name, (e.g. create_and_bind_instance or similar)\n. stray comments?\n. not a big fan of this name. don't think i like Publish ing Provider either though\n. maybe RabbitConsumer?\n. _queue_consumer maybe?\n. do we definitely want to fail silently here?\n. don't we need args and kwargs here to be futureproof?\n. see args/kwargs q above\n. +super\n. *towait\n. i'm not sure this is the right place for checks like this. i think we should assume that if a config is present it has correct type etc, and just do a \nmy_value = config.get(key, default) here\nit should be the responsibility of whatever we settle on for config management to enforce types etc\n. what's the reason for switching format? (and if we want to switch, should we switch everywhere we use uuids?)\n. can we have a docstring explaining how to use this please\n. this looks a lot like http://docs.python.org/2/library/itertools.html#itertools.count (except string coercion)\n. in what sense is this New?\n. do we know why? can we do anything about it?\n. This is quite a long timeout for a test. E.g. test_rpc uses timeouts of 1 second. Can we get away with something shorter?\n. if you want, you can make this a pytest fixture, which would let you avoid adding indentation\n. what is this for?\n. i'm not against nameko doing the extra checks, i think we should. just not sure it should happen in WorkerContextBase (or in general, at the point we use the settings)\n. ah, i see. well it's local to this function, so don't mind. (think i prefer \"custom\" to \"new\", but also happy to leave it if you want)\n. yep (not strictly required, a regular fixture could do it too, but much easier using yield_fixture)\nwould have the effect of your entire test running inside this context manager. (if it makes sense, you could even enable it fore entire (test) modules, or the whole suite)\n. i don't think we should even be doing type coercion here\n. pycharm..\n. extra line break on purpose?\n. dequeue is a little ambiguous here (and can also mean double-ended queue), could we rephrase please? (e.g. \"take messages of the queue\" or similar)\n. could we make the two Nones kwargs to make it clear what we are setting to None?\n. probably don't need this blurb\n. we're getting a bunch of these files now. do we want to move them to a requirements folder? or maybe merge docs and test into dev?\n. do we mention somewhere that these examples depend on rabbitmq? maybe a \"prerequisites\" section that includes a link to some rabbit getting-started docs?\n. with the ? (or an)\n. \"Advanced examples\" section or similar?\n. these don't mention their dependencies. may not work exactly the same here, but for taal i have\n```\ndocs/api/modules.rst: $(wildcard taal/*/.py)\n        sphinx-apidoc -f -o docs/api taal\nautodoc: docs/api/modules.rst\ndocs: autodoc\n        cd docs && make html\n```\nalso, is there something we can ask travis to run to check that the docs are up to date? e.g. readthedocs won't run custom commands (e.g. autodoc)\n. no, i meant \"wrapping then\" in a section titled \"Advanced\" or similar\n. are we expecting so many test utilities that we need to split unit/integration into separate modules?\n. maybe a comment here along the lines of \"dependencies are stubbed out by mocks by default. see [link to docs] for more info\"\n. i think this example service is so light [pure pass-through] that it may be confusing. could we have this service add some of its own value to make it clear what we are and aren't testing (our own add, but not math's add)\n. struggling to think of a good example.. maybe a simple unit conversion service that defers to a maths service for the maths?\ne.g.\n@rpc\ndef inches_to_cm(self, inches):\n    return self.math.multiply(inches, 2.54)\nor you may have a better one\n. the sqlalchemy docs tend to call this Base (and the name is optional)\n. what is TestProxy here?\n. should this be with patch_injection_provider?\n. probably want next(..., None) here and a subsequent check for None (or possibly an object sentinel). otherwise this will raise StopIteration if the attr isn't found, which is confusing (can have some interesting side effects)\n. i suspect this isn't possible, but it would be really neat if we didn't have to duplicate this here and for the test example. we might get away with this simply saying \"see examples/test/foo.py\"\n. in your examples you only ever seem to pass a service class. do we definitely need to be able to do this by name as well?\n. is it possible to host the same service twice in the same runner?\n. could possibly use yield_fixture instead\n. kill?\n. these are so much nicer!\n. to really test this, maybe we should give the second one \"a chance\" to response too, ie. let the timeout fire instead of breaking when we have a response\n. could we define this above, rather than below please?\n. what is going on here?\n. \"Barebones\" :p \n. though not superkeen on having essentially a copy of test/conftest.py\n. how are these tests intended to be run? e.g. is this definitely imported before conftest? (which imports pyrabbit)\npytest has a command line option to create a runtests.py, which we might edit to do the monkey patching?\n. don't need square brackets here :)\nor well, do we specify which python version we target?\n. are we explicitly promoting the pattern of returning (status, result) tuples? (as opposed to e.g. raising errors)\n. this is a really common pattern (injection-decorated method that just returns a factory wrapping a class)\ndo we have any examples of injections using a different pattern. would this be possible to abstract? \n. don't quite understand the TODO here\n. maybe explicitly call out that we are testing shop, and want real-ish instances of \"AcmeShopService, StockService, InvoiceService\" running to respond to our calls\n. good point re: docs\n. could there be a confusion between \"services\" and \"service names\" here?\n. docstring?\n. one*\n. docstring?\n. doc?\n. so the signature of replace_entrypoints is different to replace_injections? (list of names vs list of tuples)\ncould this be confusing? \nis using the same method with multiple entry point a common case?\ndo we want to consider letting replace_entrypoints accept a list of names as well as a shortcut (would disable all entrypoints for a given name)\n. in particular, maybe explain what gets mutated and how much needs to be re-done to \"reset\" (/get a new) container\n. do we need to be clearer about the behaviour around exception subclasses here? e.g. something like \"if ex_type is provided, fail unless (a subclass of) ex_type is thrown\"?\n. maybe \"trying\" instead of \"checking\" is more clear about what we do here (i.e. repeatedly calling the function)\n. which library needs these to be pre-escaped?\n. maybe \"has\" instead of \"had\"?\n. nice\n. do we not need this part of the comment anymore?\n. i think i prefer the previous phrasing of this. possibly with \"must\" instead of \"may\"\n. not sure if we can, but could we expand on what we mean by \"urgently\"?\n. not sure which one i prefer, but another option here would be \nself._active_threads.discard(gt)\nself._protected_threads.discard(gt)\n. as discussed\n. nice\n. maybe run_services?\n. is there no way to ask a container for its registered entry points?\n. as i mentioned, do we want to consider a name like \"limit_entrypoits_to\" or similar?\n. as discussed we may want to make sure all names are indeed entrypoints, to catch typos (can't think of a reason to be lenient here)\n. if we have this separation, would it be better to just collect into two separate buckets?\n. doesn't matter here since there is no performance worry, but otherwise it would probably be worth always calculating missing and then checking if its empty rather than doing both issubset and -\n. may be neater to loop over container.injections once and create a map {name: dependency} and then just look them up, rather than this double loop\n. if we get here, res can no longer be empty right? (since we're checking for missing)\n. how often does this happen in reality? (ie should this really be warn)? (or is it there so we can find out?)\n. do we want to explain why we need to copy it (and/or what we change)\n. like\n. let's try to get into the habbit of using autospec=True\n. do we want to swallow all exceptions here? (or is there a raise missing maybe? )\n. maybe move the 10 into a constant as well? DEFAULT_MAX_WORKERS or similar\n. maybe filter(is_entrypoint_provider, self.dependencies)?\n. as above (if we want it)\n. how come we need this in the package?\n. how come we're including docs and tests in the distribution?\n. is this a common pattern? (moving the metadata out of setup.py)\n. i don't think the package needs to include tests or docs. tests are for package devs, and users can read docs online\n. i take it this is the name for titles etc (distinct from service.name). do we need it at all?\n. what do we use this for?\n. can we have a more meaningful variable name please?\n. against, what's this for? (if you have a use-case, consider also implementing at least __ne__ (probably returning not (self == other)) for more consistent behaviour) \n. varname\n. the 2 looks a little magic. maybe consider passing it as a kwarg? and/or making it a constant\n. maybe write?\n. maybe move this \"index filename\" into a constant (since it's a slightly arbitrary (though perfectly good) name)\n. ah, i see we re-use \"Services\" later. maybe ignore the above then\n. did you consider using tripple-quoted strings for your multi-line templates?\n. out of these, think __version__ is the only one that needs to be exposed like this\n. having a command output this stuff seems a little gratuitous i think\n. given my feelings on the uses of this (we only really need the version number available programatically), could we go for the much simpler approach of just bundling a version.txt, containing the dotted number only, and just read()-ing that? that way no magic importing (or lengthy explanation) is needed\n. not often i say this, but this feels over-documented\n. if these classes are already sections, could this be called just render?\n. how come we need this base class (and corresponding meta)\nalso feels a little odd given that the different types have e.g. different __init__ signatures\n. how come this doesn't take a list of (term, definition) pairs?\ngiven just a string, in what sense is this specific to definition lists?\n. a bunch of the helpers below don't make use of self. could they be bare functions?\n. given that we're turning this feature into more of a library (c.f. bring-your-own-service-collector), i'm not sure there's much point in including a console script. \nmaybe just a function that takes a collector and an output dir, for the user to decide how to turn into a command\n(not so keen on passing function paths on the command line)\n. find this method name confusing. could you rename and/or add a docstring?\n. could we not create a container from this class and inspect the result? should have some of these helpers already\n. i wonder if we want to be a bit more careful here (e.g. abort unless some flag is passed), or at least warn in the docs that the output dir will be wiped.\n. well the community (links above + #python) seems pretty firmly in the include docs + tests camp. i stand corrected\n. what's the difference between graft/prune and include/exclude?\n. how come you went for this instead of namedtuple subclasses?\n. as i mentioned earlier\n\nwe normally don't bump versions until after a pr is signed off, in case something else gets in between\n\nnow something has\n. do we need a try/finally around this?\n. maybe write or something to indicate that this will actually write files. extract sounds (to me at least) like it might return something\n. i'm not sure if we need it (think that's handled by pytest actually, but was referring to \ntry:\n   yield\nfinally:\n   cleanup (delete created directories)\nnot sure what happens if you e.g. ctrl-c during the test execution\n. i meant namedtuple subclasses, i.e.\npython\nclass ServiceDescription(namedtuple('ServiceDescription', [\n    'name',\n    'module_path',\n    'class_name',\n    'sections',\n])):\n    def render(self, foo):\n        pass\nhas the advantage of nice pytest output if the == assertion fails and a free repr, but not completely convinced. think i like it better than the EquatableMixin hack though\n. because our \"owner\" should already be waiting on _died (via wait())?\n. is this now exc_info?\n. not completely sure on this, but \"kill the container\" sounds a bit like it's a third party (i.e. one of our attributes), when it is actually self here. not sure how to make it more clear, but wondering if we can do something\n\"commit suicide\" isn't a great phrase but something like that. maybe \"to be safe we call kill to kill our dependencies and exit (/die) with the exception\"\n. in what case would we multiple providers think they're the \"last\"?\n. maybe a comment about how this mimics Event.send_exception? (i.e. that raise is not enough)\n. how come we are changing this behaviour?\n. i'm not a big fan of these methods on mocks since they silently pass if you make a typo in the method name. in this case, could we use assert mock.call_count == 1 (can't recall exact attr name)\n. i think str(ex) is preferred over e.message\n. can we use the varnames from http://docs.python.org/2/library/inspect.html#inspect.getargspec\n. i guess we prefer .format()\n. looks like we don't want formatvalue\n. maybe a few functions with some variations on *args, **kwargs and defaults and some more exhauistive tests\n. could possibly do \narg_spec = inspect.getargspec(fn)\narg_spec.args[:1] = []  # remove self\nsignature = inspect.formatargspec(arg_spec)\n. how come we store this on the function (as opposed to the provider instance)?\n. to be safe, let's register with the module name as well as the class name\n. no sadface at all. check what we're evaling; actually really cool\nOn 27 March 2014 15:47, Fergus Doyle notifications@github.com wrote:\n\nIn nameko/dependencies.py:\n\n@@ -355,6 +359,15 @@ def registering_decorator(wrapper, fn, args, kwargs):\n                                       'DependencyFactory instances')\n         register_entrypoint(fn, factory)\n         wrapper.provider_cls = factory.dep_cls\n+\n-        regargs, varargs, varkwargs, defaults = inspect.getargspec(fn)\n-        regargs = regargs[1:]  # remove 'self'\n-        signature = inspect.formatargspec(\n-            regargs, varargs, varkwargs, defaults, formatvalue=lambda val: \"\")\n  +\n-        src = \"lambda %s: None\" % signature[1:-1]\n-        fn.shadow = eval(src)\n  +\n\nwe have evals in here? [image: :disappointed:]\n\nReply to this email directly or view it on GitHubhttps://github.com/onefinestay/nameko/pull/119/files#r11029073\n.\n\n\nDavid Szotten\nonefinestay\ndavid.szotten@onefinestay.com\n300 St John Street\nLondon, EC1V 4PA\n. how come we call this _de_serializable?\n. from the way this is used, i wonder if we just want to turn it into two methods? not sure the class adds much\n. don't love this name but can't really think of anything better\n. todo?\n. how come we're changing this?\n. do we want *data.get('args', ())?\n(probably over two lines though)\nargs = ...\nreg[key](*args)\n. when do we get exceptions?\n. do we still need this fixture?\n. move to .constants?\n. @injection?\nwhat about the other keys?\nmaybe separate injections re-using the same provider (with a non-exposed argument)\n. ```\n@injection\ndef language():\n    return dep_fac(ContextKeyProvider, key=LANGUATE_KEY)\n@injection\ndef user():\n    return dep_fac(ContextKeyProvider, key=...)\n``\n.json.dumpscan also raiseValueError` (for circular references)\n```\n\n\n\na = []\na.append(a)\na\n[[...]]\njson.dumps(a)\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/Users/david/.pythonz/pythons/CPython-2.7.3/lib/python2.7/json/init.py\", line 231, in dumps\n    return default_encoder.encode(obj)\n  File \"/Users/david/.pythonz/pythons/CPython-2.7.3/lib/python2.7/json/encoder.py\", line 201, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n  File \"/Users/david/.pythonz/pythons/CPython-2.7.3/lib/python2.7/json/encoder.py\", line 264, in iterencode\n    return _iterencode(o, 0)\nValueError: Circular reference detected\n``\n. maybe'other' + vhostin the unlikely event that someone is using the vhostother? :p \n. comment to explain the reason forno cover?\n. what is this?\n.returnnot needed\n. if serialisation fails we setresulttoNone, but should now probably also set a newexc_info`. need to take care if it's an existing exception we fail to serialise to try to keep as much info around as possible (can't wait for py3...)\n. i guess this could just be a list?\n\n\n\nshould probably add a fixture to empty it after each test to avoid keeping tracebacks around\n. or object() :p\n. calling modules logging is an easy way to get into trouble. how about log_helpers or similar?\n(point is that any siblings also need absolute_import)\n. if you're refactoring anyway, why not just pass the level down into _log_time instead of faffing with partial?\n. (/ define _log_time inline here and use a closure for the params)\n. do we need this at all now? (is errback a required kwarg?)\n. maybe a comment explaining that/why we need this\n. could we just fail the test from here? (not sure if that's actually nicer than runtimerror)\n. leftover?\n. stop vs kill here?\n. possibly is_valid_call\n. can just do getattr(worker_ctx.provider, 'expected_exceptions', ()):\n```\n\n\n\ntype(())\n\n```\n\n\n\nand don't need if expected_exceptions:\n```\n\n\n\nisinstance(object(), ())\nFalse\n``\n. ah yes. though i think i would prefer to make the default be()on the provider..\n. stray full stop after \"default\"?\n.expected_exception(last half + missing backticks)\n. probably want tolog.warnhere, no?\n. \"omitting\"\n. not sure if there's a better place thantest_brokerfor these..?\n. good call\n. we bubble out of this coroutine, stopping it\n. yes. will update to reflect the fact that we never actually use anything yielded from this coroutine\n. the test uses a standalone proxy, i want to test a regular proxy\n. as discussed, let's add some safety aroundself.provider.name(e.g.provider_name = getattr(self.provider, 'name', 'unknown provider')or similar, and use{!r}in in the return value to make sure we don't end up with unicode errors\n. no, returns a bound copy\n. good idea. think this also addresses your comment re: forever\n. we should use the latest pyrabbit which removes the need for this extra replacing\n. i agree that it looks nicer. my only concern is that it's more fragile. e.g.None` will raise attribute error, and while unlikely, if this happens e.g. whilst logging an exception, the original exception info is lost\n\n\n\nmaybe we could pass values through something like\ndef safe_str/str_for_repr/better_name(value):\n    try:\n        return value.encode('utf-8')\n    except Exception:\n        return repr(value)\n. we don't want isinstance here?\n. while we're sticking with py2 i think you can just write 'f\u00f6bar' here (skipping the u prefix)\n. no\n. the ExtensionSet class is a bit strange to me. in discover_extensions we combine providers and entrypoints, only to put them in a set with methods that loop over them to split them up again...?\n. don't feel like this interaction reads very well. will come back to this\n. what is this for?\n. see above. we may just want to keep two separate sets\n. CamelCase?\n. -> extensions.py?\n. i though this was going to happen via different bind implementations rather than params\n. not quite state i guess\n. possibly a few words about what the before/start split\n. if this is the only use case, we could make name and container params to clone\n. what is this trying to protect against?\n. could we have a more descriptive name here? (not sure what yet) \n(esp. given that we include self)\n. a little odd to use a string as a sentinel. (i.e. should this just be == do we want stronger guarantees)\n. goes away with subclass\n. and here\n. can wrapt no longer help?\n. what happens for\n@foobar()\ndef spam():\n    pass\n. let's have a chat about this. need to think about how we want \"sub\" extensions and containers to relate\n. we're making some pretty big changes. might just call it 2.0\n. class Service(object):?\n. do we need to distinguish them at all from dependencies?\n. todo\n. do we have anything that depends on this separation?\n. maybe an example of why one might want to use this\n. seems a little wasteful to serialise this. let's see if we can think of a nice way to get it back in parallel\n. did we have an intended use-case? i.e. some extension that wanted to use a dependency in another extension's teardown? think we ended up just using teardown tidy up from acquire_injection\n. not sure what convention we have wrt blank lines here\n. maybe __is_clone (if we need this at all)\n. let's put the note somewhere else\n. typo \"An Dep\"\n. maybe a comment mentioning that this is depth-first\n. this might be a good place to make sure extensions only have a weakref to the container. still investigating whether this is strictly needed, but it might be\nupdate: i think we need something. otherwise, the shared_extensions (weak key dict, so weak ref to containers but strong to extensions) will keep the extensions alive, which in turn keep the container alive, thus preventing gc\n. typo from sed\n. compat in a test?\n. possibly out of scope, but this looks like it's messing with internals in a way it probably shouldn't\n. clear between tests?\n. any reason not to just use ext.is_clone() here?\n. do we have some common errors that might cause this worth mentioning? (a la \"did you ...\"?)\n. i guess these are just chasing coverage, but a negative test might otherwise also be in order\n. guess we might want some small, but non-zero amount of time here (0.1?)\n. i guess we can drop __clone and have is_clone just look for container?\n. actually, we could make clone an internal concept and refer to them as bound ?\n. we keeping inject?\n. lost when landing bind-clone-alternative\n. normally assert not ext.is_bound / assert ext.is_bound\n. is there a reason this is a property (as opposed to a regular method)? (on the fence)\n. possibly \"cannot call bind on bound extensions\"? or even \"cannot bind a bound extension\" (since bind is the api)\n. bump?\n. how come this indirection?\n. should this be inspecting the class?\n. this is accessing container internals directly. would be good if that wasn't required for extension authors. could this not e.g. just call spawn_worker or similar?\n. \"bound extensions\" or \"a bound extension\"\n. possibly add another zero (0.01)?\n. used -twice- thrice\n. principles\n. maybe something like flask \n. clients? not clear what this means\n. possibly another sentence, or an example would make this clearer (possibly enough to link to \"dep injection\" below)\n. \"DependencyProviders\". i guess either in backticks or as two words?\n. maybe pseudocode \n. again, backtics or two words i think\n. not sure about the phrase \"support thread-safety\".\n. +1 for link to rabbit docs, but can also include basics on ox x/ubuntu (brew install / apt-get install)\n. don't know if pygments need a hand with guessing python-console here. if we're keeping the banner, maybe emphasize-lines (http://sphinx-doc.org/markup/code.html) could help \n. that non-nameko clients (possible \"e.g. a django or flask app\" or similar)\n. don't love this as an example\nalso, for this, where the caller example is tightly coupled to the service example, it might be better to keep them in the same file (e.g. here i think the response output has drifted out of sync with the method)\n. i wonder if we could re-write these as doctests and run them (with say a pytest hook that spins up the example services)\nthis should be  res.result(), not wait()\n. \"async\" -> asynchronous ?\n. we use both x/y and a/b for service names. does this matter?\n. nicer with trailing quotes on the same line?\n. do we definitely want to document/promote this? if so, do we want to consider it \"imitation\"?\n. probably need to explain that the default handler will serialize the return value as json (if we want to keep that as the default), and similarly, that post-data should be json (with content-type?)\n. with 3 headers, it's status, headers, payload (probably worth documenting; possibly along with the fact that this is built on top of werkzeug and also accepts a werkzeug response object for complete customisation)\n. though a 302 (and 404) is a slightly odd example. \n. probably want to label this as \"experimental\" for now\n. maybe MagicMock objects from the mock(link) library\n. mention that examples use pytest(link) which is what nameko's own test suite is using, but all testing utilities provided are test framework agnostic\n. probably worth a comment/link to http://www.voidspace.org.uk/python/mock/mock.html#mock.Mock.side_effect\n. if we're spinning out the sqla stuff we might need a new example here\n. probably worth making clear that we are using hooks to fire x, but the interaction between x and y (which we are testing) is just as if under normal operation\n. i think disabling a timer entrypoint would make a good example\n. not completely sure, but don't think we need to monkeypatch greenpipe anymore\n. not sure we want to blanket include everything here. is there much benefit over e.g. just browsing the code?\n. - getting in touch\n- contributing\n. this follows e.g. gunicorn http://docs.gunicorn.org/en/latest/run.html\n. why not?\n. probably \"Nameko\"? (a few more of those)\n. these are shell, not pycon\n. maybe drop makefile altogether?\n. injects positional args passed to tox\n. yep: http://tox.readthedocs.org/en/latest/config.html?highlight=envdir#confval-envdir\n. maybe put latest last for clarity?\n. and possibly add a blank line between groups\n. maybe something like \"Nameko is licensed under the Apache License, Version 2.0\". Please see LICENSE in the project root for details\n. \"stashed\" sounds a bit colloquial. \n. do we definitely want to allow both a single or a tuple of strings?\n. you can also get TypeError here if the argument doesn't match (e.g. trying to redact an integer thinking it's a dict)\n. () is an empty tuple\n. do we want some custom ConfigurationError? (e.g. to catch and display something nicer than a traceback in nameko run) or are we happy with a traceback in this case?\n. could we have some non-ascii chars in here for good measure?\n. don't think this is the right place to catch these (run may be used from other libraries)\nnameko/cli/main.py is probably the place (already catches and prints CommandErrors)\n. rather that doing this here (i.e. on all connection errors), maybe we could do a connection-check, e.g. in setup\nit doesn't look like rabbit is of much help (e.g. bad creds result in a pause when connecting, before a disconnect)\npika tries to track how far it gets during connection and gives heuristic error messages, but amqp doesn't help very much. best i could do was something like this\n```\nfrom amqp import Connection\nclass ConnectionTest(Connection):\n    def init(self, *a, k):\n        try:\n            super(ConnectionTest, self).init(*a, k)\n        except IOError as exc:\n            if not hasattr(self, '_wait_tune_ok'):\n                raise\n            elif self._wait_tune_ok:\n                print \"probably bad creds\"\n            else:\n                print \"probably bad vhost\"\n``\n. i guess these are technically backwards incompatible\n. single slash on purpose? (which i think makes vhostNone` meaning use amqp lib defaults)\nwe could use more defaults by having our default be just amqp://localhost\n. foo.get(key) or default is different to foo.get(key, default) if e.g. foo[key] == None  # or 0\n. i think we should just drop the foo.get(key) or default pattern, as the behaviour is somewhat unexpected (certain config values just being ignored)\ni did some digging for though the git history of this for the max_workers, and i can't see any direct reasons to override e.g. an explicit None or 0. It won't work of course, but neither will setting the value to foo. We may add some validation/sanity checking at some point later, but for now i think it's fine to \"allow\" those config values and just let any errors bubble\n. a note about how to do this \"verification\" (add to suite in ...)?\n. should broadcast make the default go the other way?\n. i would expect this file to live in the tests directory with our other tests\n. kill?\n. is this copied from the web tests?\n. in the other web tests we jump through hoops to make sure we pick a random (free) port\n. location? is this leftover from when this was a redirect?\n. is it confusing that the variable payload ends up containing the string \"payload\"?\n. are we not aiming for 100% coverage here?\n. is this now badly named?\n. typically is None\n. this is a strange example. possibly unhelpfully so\n. move to top?\n. i wonder if we still need these examples now that we have a cli\n. this is the listening socket, right? so calls in-flight can still finish and send responses?\n. might want to run this with at least py2 and py3 (c.f. top of this file)\n. probably wouldn't be a bad thing to always run (e.g. to make sure examples work for all versions we support)\n. don't think we need a separate option for this (think django has this for historical reasons. tried digging through the source and got as far as the svn->git conversion and lost track. from what i can tell, django originally had --plain to stop defaulting to ipython, and only later added --interface to support bpython\nhere i guess we could just add 'plain' as another choices in SHELLS?\n. not so keen on using globals like this\n. from what i can tell (using latest versions on pypi, might differ between versions), calling embed in bpython causes pythonstartup to be executed, but not in ipython. one option is to run it \"manually\" for all shells (like this), and then unset the environment variable to prevent it from running again. thoughts?\n. map doesn't return a list on python3\n```\n\n\n\nmap(None, [])\n\n``\n. we ok without specifying the rc file?\n. show-missing is handy for debugging failed builds on travis\n. as discussed, let's usesix.raise_fromlikenameko/amqp.py:21. are there some stray characters in this function name?\n. i guess we don't need credentials here?\n. do we need to worry about someone having this port open? we could use the trick fromtest/web/conftest.py` to ask the os for a free port, and then close it and use that?\n\n\n\nor maybe we could use one of the low, well known but unassigned ports, e.g. 4\n. maybe call it \"py.test\" or \"pytest\"\n. don't think you need this if you have pytest-cov installed. it adds a .pth file\n. nice\n. do we want coverage for the tests as well? (catches e.g. duplicate test names)\n. not sure which i prefer, but could use parallel = true in coveragerc instead\n. what triggers coverage combine?\n. do you also want to update tox.ini?\n. the reason we had the former was to get separate (and parallel) builds in travis\n. not sure i find this style more readable. how come the switch?\n. lib or library might be a better name than codebase\n. could we not just move commands out of the default section into a [testenv:lib]?\n. bump (got eaten by github when you changed codebase to lib)\n. i see. ok\n. 4.0 is now out\n. probably reverse this order\n. i like brackets for emphasis when assigning the outcome of a comparison:\nuse_random_vost = (vhost == \":random:\")\n. above you parse the url to explicitly compare the value for vhost to ':random:', but here your run the replace over the whole string. seems inconsistent. \n. \"exception formatting\" or possibly \"exception's formatting\". though maybe explicitly mention that this controls how exceptions are returned to a http caller\ni'm not sure \"safely\" is appropriate here. do we write about expected_exceptions elsewhere?\n. this looks double encoded. (might be related to a bug i saw elsewhere, when raising the exception. will comment there too)\n. it seems slightly strange to me to have the exception class (and by extension, the service code) care about things like http status codes. i would expect that decision to be made in response_from_exception\nHttpError, again seems to focus on mechanism rather than (as common with exception classes) what went wrong.\nmaybe something like InvalidAgruments (not a great name, but along those lines)\n. you are calling json.dumps both here and in response_from_exception. is this intentional?\n. i don't this (or indeed a bunch of other tests in this module) needs a rabbit_config\n. this is double-decoding and sounds to me like we're doing something wrong (as discussed above)\n. @mattbennett do you recall if we purposely didn't mention expected_exceptions because we weren't ready to commit to it as a public api?\n. such subclasses sound reasonable. and indeed, i would expect those classes to decide on status codes, not the service code raising the exception\nin your example, i would expect something closer to raise HttpError(\"This is (an?) invalid request\") or whatever (ideally some more detail, e.g. invalid argument foo), and have the class together with response_from_exception construct the payload\nbtw, how come the status code is also included in the payload?\n. agree that the name isn't great - we should consider alternatives\nalso, note that for for the rpc entrypoint, the concept of this set of exceptions was purely internal to the service, whereas for the http, we are making it part of the service api (since it affects the http status; something we don't have for rpc atm)\n. i would probably restructure the whole thing as something like\n``` python\nclass HttpError(Exception):\n    error_code = 'BAD_REQUEST'  # if we want to allow raising \"bare\" HttpErrors\n    status_code = 400\nclass InvalidArgumentsError(HttpError):\n    error_code = 'INVALID_ARGUMENTS'\ndef response_from_exception(self, exc):\n    if isinstance(exc, HttpError):\n        response = Response(\n            json.dumps({\n                'error': exc.error_code,\n                'message': unicode(exc),  # do we need to worry about this failing?\n            })  # or dumps failing?\n            status=exc.status_code,\n            mimetype='application/json'\n        )\n        return response\n    else:\n        return HttpRequestHandler.response_from_exception(self, exc)\ndef service_code():\n    raise InvalidArgumentsError(\"Argument foo is required.\")\n``\n. rather than adding another function here, maybe just bake this intohandle_timer_tick. However, we might want to think about the sleep time bug #303 \n. the usual way to get this behaviour is@pytest.mark.parametrize`\n. how come you are changing this? the old behaviour is explained in the comment just above and doesn't work with the replacement.\nbasically, if the service is stopped while the timer is sleeping, we don't want to wait until it's done sleeping before tearing down the timer entrypoint\n. at this point an eventlet Semaphore (remember to init with zero, not the default 1) might be more idiomatic\n. is that because we are missing a test that goes all the way around this loop?\n. could you remind me why we no longer need these?\n. ah yes\n. since you are bumping the version here, i think you should also add a changelog entry about the bumped minimum eventlet requirements (due to eventlet removing them from pypi)\n. have you switched linter? :p \n. how come this is a property?\n. can't we just raise this in the default implementation of broadcast_identifier?\n. docs: \"Exclusive queues may only be accessed by the current connection, and are deleted when that connection closes.\". that sounds incompatible with the example that uses hostname to share (re-use) broadcast queues. am i missing something here?\n. isn't the patch is un-applied at this point? though this whole fixture may no longer be used\n. think i prefer get_foo() to make it clear(er) to a \"caller\" that this is a function call. esp. since it might be doing non-trivial stuff\n. this comment (and the one for exclusive) don't add much info beyond the line of code they decorate. could we change it to explain why we want this behaviour (or drop, though maybe at least include the point about exclusive silently overwriting the auto_delete behaviour)\n. i guess we don't need to check handler_type in here\n. ah. though that's quite surprising behaviour i think, and unless we can figure out how to getmembers or similar without executing descriptors, i'd (independently of previous concerns) prefer making this a regular method\n. i think the strangeness is caused by a problem \"higher up\", namely trying to have a single class for both \"service pool\" and \"broadcast\" events (ignoring singleton for now)\nassuming any change to that is out of scope for now\nthe initial implementation had a kwarg that only made sense for certain values of handler_type. having a method is less bad, but we still have this method that only make sense if the class was initialised with a certain parameter value\nwe are extending EventHandler and added a protocol which means it calls broadcast_handler if it's in broadcast mode. i think calling that method \"out of turn\" is fine to be \"undefined behaviour\". returning None as you suggest is fine\nhowever, i think my (new) objection to the property decorator is that because we use getmembers, the method (or a new implementation in user code) gets called at unexpected times. so regardless of whether we can get this particular implementation of broadcast_handler to handle that, i'd prefer to stay away from descriptors until we can make our inspection not invoke them\n. \"don't use\" vs \"make sure is safe\" is a fair point. i'm still kind of hoping we can handle it so users don't have to, but haven't done much research and at a glance it wasn't obvious if/how it could be done. \n\"is safe\" might be slightly harder to explain but i feel less strongly about it now i guess\nre shared class with multiple modes i think that's a recipe for bad apis. we were already in that boat before this pr and i'm not suggesting we change that (as part of this at least). nor do i have any great suggestions for names (@service_event / @broadcast_event maybe?). concepts and apis should drive object design, not code reuse. anyway. all this for another time\n. as i said i'm less strongly against the descriptor now so if you much prefer it i'm happy to go back to it\n. unless _ override?\n. awkward line break\n. do you just run it (on save?) or also use for validation?\n. could we get away with a simpler example here? this increases what we describe as a \"simple yaml config file\" from 5 to 20 lines\n. can we not actually test this instead as matt suggests?\n. sure. the tests use py.test, are you familiar with that?\npy.test --help shows our custom options, including --amqp-uri=AMQP_URI   The AMQP-URI to connect to rabbit with. (admittedly not that easy to find if you don't know what you're looking for)\nyou can also let tox handle it all for you (pip install tox; tox). actually, i think tox calls a makefile, so you can probably write make test (or look in the Makefile for other targets to run subtests)\nto install test dependencies you can install nameko[dev], e.g. pip install -e .[dev] \nwill try to write some of this up and add to the docs\n. can we drop the no cover now?\n. afraid the docs use british english (\"favour\")\n. maybe add something like \"in preparation for async becoming a keyword\"\n. i thought we only needed to specify this for pytest-cov (it's already the default). do we definitely need this explicitly?\n. how come we are adding another ci tool?\n. how come these need to be weakkey?\n. probably want to call link before the worker_created hook in case the latter yields\n. passing self seems a bit ugly. also, if this is just for logging, do we want to be more explicit and call it out as that? an extension using multiple managed threads could then request them with different info, e.g. MyExt.method_foo. we could even fall back to fn.__name__ instead of \"unknown\"\n. maybe a sentence about when/for what one should (and shouldn't!) use this\n. this sounds like fairly surprising behaviour to me\n. i think the canonical way to iterate over dict while mutating is for key, val in list(dict.items()):\n. isn't the second argument optional?\n. in case gt gets scheduled and completes/dies before you get to call link\n. again, since .coveragerc is the default file location i think coverage looks for it even if we don't specify. seems to work for me locally (even via tox). Does it fail for you if you drop the explicit COVERAGE_OPTS?\n. Yea, ssh'ing into ci is nice, and afik travis doesn't offer it. travis is still fairly defacto standard so would prefer to stick to (just) that for now\nwas this just more convenient or were you not able to reproduce the issue (e.g. in a local vm)?\n. except you've just called copy which copies the data anyway.\n(dict.items() returns a view, which is linked to the original dict so that it updates dynamically)\n. given that they weren't before, and we use link pretty well, i think i'd prefer to avoid weakrefs\n. on a separate note, from a quick look it seems we only put the container in the worker_ctx to get to config and service_name. might be better just putting those in. something for another time\n. might change as discussed above, but if we're keeping it optional, could we not skip the warning if the value is None?\n. i don't understand that. are we not still mutating the dictionary inside the loop?\n. nice! if you use getattr(self, name) instead of calling self.__getattr__ you get the type check for free\n. if we switch to getattr above we can probably drop this test entirely\n. maybe \"is not a legal identifier in python\"\n. i recently realised that these are silent by default, and wondering whether we want to change that. until recently django used to use a subclass to un-hiden deprecation warnings https://github.com/django/django/blob/1.9/django/utils/log.py#L55 (was changed in https://code.djangoproject.com/ticket/25999 to aid their LTS process which involves third parties conditionally importing old stuff which would trigger these warnings). i don't think we have that problem, so we might want to consider doing this\n. my comment wasn't so much about this test, as about our desired behaviour in general (e.g. when using run). we recently started adding deprecation warnings but users might not be seeing them. also, doesn't need to be addressed in this pr, but wanted to highlight\n. i'm not a huge fan of this callback api. do you have other use-cases in mind than multiple calls?\n. was thinking a bit more about these. if the main use-case you have in mind is a better hook for something like the entrypoint waiter, we might want to keep this private for now, (and only add the teardown one). \n. no longer needed, right?\n. should stop passing self. pass string or nothing i guess\n. passing self, as above\n. self\n. fyi i think the fix is https://github.com/eventlet/eventlet/pull/257 . trying to push that along. (my journey's in https://github.com/eventlet/eventlet/pull/314)\n. i don't think the event dispatcher wants to raise here. there might not be anyone subscribed to the events yet. we use the confirm to get the server to \"acknowledge\" our message, but returned messages are also fine in this case\n. hm. i thought we were keeping this one (for use in the entrypoint waiter?\n. how come you are using the zip rather than git?\n. might also want fast_finish: true here\n. ah, i see\n. we are starting to add a whole bunch of deprecation warnings. we might want some kind of plan for how long we keep the old behaviours around, and maybe add a grep-able comment marker to tell ourselves when (which version) these can be tidied up\n. interesting. seems to have nice behaviour around missing target vs target having bad imports. could maybe use this in cli.run (separate pr)\n. i think that info is better kept in the code (e.g. as suggested above)\n. why not make an actual module?\n. no, i mean instead of makeconftest with a fake module, you could makepyfile with an actual module\n. this drops a few \"public\" attributes on the worker context. is that intentional? don't know if it still works, but e.g. nameko-sentry looks to make use of immediate_parent_call_id\n. probably overkill to go via a deque. e.g.\nstack = self._parent_call_id_stack[-parent_calls_tracked:] + [self.call_id]\n. how come we re-build this every time?\n. ok. don't feel that strongly. noted it because a) you only interact with the deque once (here) and b) you leave the attribute as a deque but convert it to a list every time it's read\n. oh, i meant as a comment but i guess this works\n. not sure having this as a fixture is worth it for the one place where it's used\n. maybe we should at least look for this key as well to make it possible for users to set the behaviour if they want? (e.g. params['cert_reqs'] = params.get('cert_reqs', CERT_REQUIRED)` or similar\nor is this never the case?\n. i know config is still up in the air, but eventually we might even move to something like a dict with all amqp config, e.g.\namqp:\n  url: amqp://localhost,\n  ssl: \n    ca_certs: ...\n    ...\n(or even)\namqp:\n  default:\n    url: amqp://localhost,\n    ssl: \n      ca_certs: ...\n      ...\n  other:\n    url: ...\n. how come we want multiline here?\n. i guess this doesn't need to be a group at all? could just be :?\n. since the question mark applies to the previous group, maybe keep it with that?\n. i think re.VERBOSE is more helpful for the reader than re.X\n. might use patch.dict here to avoid leaking side-effects\n. how come we are omitting these?\n. how come we need these?\n. is this not a test?\n. is this not protecting a race?\n. i like brackets around this kind of tuple unpacking, for clarity\n(foo,) = ...\n. pass it in? not sure i follow. also, as i mentioned in the commit message, don't like the api but it maintains backwards compat. \ncalled it socket since it's the return value from socket_creator\n. fyi i hit a few of these today (os x)\n. agree that the apis are a bit odd, probably from stemming our lack of usage experience. need to think more about the relationship between the 'app' and the 'socket' to decide who owns whom (or maybe the fixture should just return them both)\nbut this is all beyond the scope of this bugfix\n. if we prefer we could reduce this pr to just fixing the bug (without the test and the changing fixture)\n. would be good to figure out what's actually causing this (as opposed to faking it)\n. was thinking more along the lines of a hand crafted (partial) http/socket request \n. could we make events a queue and wait on it instead of sleeping?\n. ok\n. https://github.com/douglas/toxiproxy-python seems able to just download a pre-built version instead of installing globally on travis. seems worth a try to avoid needing sudo.\n. move to exceptions.py?\n. actually, on the fence\n. any reason not to make the constant ones actual properties? subclasses can still override with property-decorated methods if they want dynamic values\n. brief comment  what these mean?\n. possibly comment something like \"before calling acquire\", kombu Connection objects act as descriptions/configs/whatever for the properties to use when actually connecting and don't represent an \"actual connection\"\n. same q re properties\n. maybe we want to bundle these all up into a connection_params namedtuple or similar?\n. can we action this?\n. how come we're wrapping this in a function?\n. blank?\n. might be nicer as with patched[ANY].acquire() as producer:\n. this is the nameko tests suite, not the plugin. who would be overriding these fixtures? (can these not be constants? or if they need overriding, don't they need to be e.g. pytest-config or env vars?)\n. possibly factor out (used in a few places, right?)\n. not sure amqp_uri is guaranteed to have a port specified (could be using default)\n. maybe set_timeout?\n. maybe more common to keep the class in the module scope\n. why use the fixture that runs the test with this value if we're just going to skip?\n. i think this is the next chance the proxy has to run code. also, note that you caught the IOError, which we wouldn't expect a regular caller to do, right?\n. maybe check that with different values for confirm we get different producers\n. don't think this would work if we run pytest with --strict, which i thought we were (we aren't) and think we should\n. how come? do we need to fix this?\n. i didn't mean using the library (i saw you commented about it not being ready). i meant using their approach to travis setup\n. not sure we want this\n. i would be happy to keep ofs until we have an issue, and at that point make a nameko one\n. see also #381 \n. don't feel that strongly. python tends to take the \"we're all consenting adults\" approach as opposed to making things hard to change. not sure if this is something to worry about in practice. i guess regular attrs is a bit less code, but don't mind\n. ok\n. my understanding was that they have to be different since they have different connection params (fixed a bug in kombu a few years ago when it wasn't doing this properly causing issues in nameko)\n. unless it's faff, why not\n. ok\n. one benefit is that i get a good error message when i misspell parameterize \n. this seems a bit dangerous (what other binaries might end up in the working dir?). probably fine for travis, but better practice might be to put this binary into a subdirectory and just add that\n. thoughts?\n. actually, now that we understand this, and know this is (c)python complaining about a protocol violation by eventlet, i'm happy to just simulate that exact response\n. no cleanup needed here?\n. is this no longer an issue? how long is that timeout and would stopping have to wait for it without this?. is there any overhead to heartbeats? . are these just ignored with older (or non-forked) amqp?. ignored (as opposed to explodes) is great. this was taking longer than a second?\ndid we make stuff slower or is this travis having issues?. hm. looking at travis, builds historically take ~3 minutes. this pr is around 9. worth checking if this is travis having issues or this pr. did you move this intentionally? makes the diff harder to see. should we add these back? (make the failure case a timeout instead of a hung test)\ni guess they also helped us catch the performance issue. ah, forgot about pytest-timeout. though i can't see where we switch it on... sounds a bit long for an individual test but we can tweak later. listen sounds a little generic to me, maybe web-server-listen (that's a bit long, so open to other suggestions). while this may be a better key, i'm not sure it's better enough to justify breaking backwards compatibility. just wanted to check we are happy with the behaviour if multiple messages are sent while inside this cm. replace the (code) comment with this?. hm. every user of the free_port fixture will be given the same port. which probably isn't what we want. a warning is fine for now, but i think it would be better to choose either error or nothing. we normally avoid yoda conditions in python (here and below). it's not harmful, but it reads a bit odd and doesn't add anything, since python uses different operators for comparison and assignment, and e.g. if a = 2: is a syntax error. how come you're exploding this into separate settings?. don't love abbreviations, maybe rabbit-api-uri?. uri's are still a convenient way to specify driver (protocol), host, port and credentials in a single string. if we no longer allow the vhost to be specified i guess we can ignore it in the uri?\n(even if we change the default, do we not even want to let users override the vhost?) . how come these are now using rabbit_config?. i think the public api is str(config). how come these all need to test the broker?. ah yes. :(. hm. ok. (given how relative expensive it is to spin use the rabbit dep, maybe we can make those examples not need it). what's left?. sorry, read this the wrong way around. you are adding another transport which can use the TestTransport. what ensures (1)?. how come this is moving into a fixture?. we might want to think about \"hosting\" these tests in actual files (maybe outside the main test tree) so they don't have to live inside docstrings. could we use the same trick to speed up teardown of actual (non-test) services by detecting when all workers are done and deleting the reply queue?. ok. just shame about editor support (syntax etc). i think a second is a long time to wait for nothing at all but happy to leave for now. i think there is (or used to be) a tryfirst marker but probably be ok to leave as is for now. agreed. do we see this being re-used? (if not, maybe move to conftest?). if we're keeping this generic, maybe vhost -> resource or similar. how come we need this? (is it not enough to wait in wait below?). thought on using a contextmanager interface for get and discard as well?. how did you pick the size?. so do you think 3 is still a reasonable default for other use-cases? (or do we e.g. want to force the user to choose?). i'm still thinking about this but we might have a race here (e.g. if create is slow). may need e.g. a semaphore around the body of the while look in _create. this is a pretty generic name. the most common convention i know is to namespace by module (getLogger(__name__)). if you want further customisability you would typically add further namespacing below with another dot. what's the (True, False) tuple? could we give it a name to make that clearer?. should this use container.spawn_managed_thread?. maybe a more specific filename if this is just for this particular problem. not sure what you mean by \"as a reference\" but i guess i'd lean towards keeping. how come they are so slow? . identifier?. if the problem was that blocking in handle_message blocks the whole queue consumer, maybe it would be enough to check that that is no longer happening, rather than explicitly checking that heartbeats still work? . i think elsewhere we do this with a partial instead of a lambda though not sure about pros and cons. maybe one less function call / shorter call stack?. is this copied from somewhere? we don't seem to actually use e.g. max_delay. nameko/utils/retry.py maybe?. isn't inf the default max_attempts?. are routing keys always ascii?. actually, i guess provider could also be non-ascii. how come Exception can't be the default arg?. should this be for NotReady only?. well your format string needs to be unicode. ah yes, misread it above. and reasoning makes sense to me. maybe a note that a bunch of backwards compat shims are being dropped (as indicated by their warnings). how do you mean we're not using these? does flake8 pin exact versions of these?. do we also depend on requests?. ah, of course. i think i still like the idea of pinning everything in dev. since it's dev, there's no problem bumping these and i think we tend to try to keep them recent when we remember.\n(i thought requests bundled its own urllib3 but as above happy to bump). why is this required?. while we aren't exactly in a good place at the moment, i'd like to at least consider a more specific key here. we are currently often conflating rpc and amqp and have e.g. a top level key heardbeat which only applies to the rabbit broker, but unlike \"heartbeat\", ssl might confuse users of the web extension.\nwhilst fixing existing ones may require breaking backwards compat and is a job for later, i thought this worth bringing up before adding to this. i'm less worried about the name of the constant (though your suggestion is good) than the value of the key that needs to go into config.yml, currently just ssl. sounds good to me!. how come we need these lint exemptions?. nice way to document this stuff. how come you aren't just making the defaults the default parameters here? documentation? subclassing to override defaults? (are we adding too many ways to change settings?). should we consider adding a linter to enforce import orders?. wrong version?. how come this var is called propagate_ and not extra_? (propagate sounds like a bool to me). i think we tend to name vars like this publisher_cls. todo?. do you prefer this to call_args, call_kwargs = call?. i would be -1 on auto-rewriting. status quo is probably fine, though a checking is a fairly fast lint run with e.g isort. also has the bonus of optionally autosorting. something for another day. i had quick look in the source, and Call already has a bunch of interesting behaviours, e.g. it compares successfully to both a tuple (compared against args) and a dict (matched against kwargz) but nothing obvious to check membership in kwargs like you do\nhappy to keep this helper if you like it. i know this is just a toy but i'm still not sure i'm comfortable with example code suggesting stored plain text passwords . again?. maybe check timings but this is probably fastest (or second after flake8) so may be worth running before the slower pylint. not sure i understand the convention of blank lines in here. how about calling this show-config or similar to make it more clearly distinct from --config. old file removed but new file not added?. test_show_config?. show-config.... the original test actually disconnected. not sure about the fidelity of just mocking the expected side-effect. How hard/annoying is it to add a toxiproxi test? Not so keen on relying on mocks here, esp as we\u2019re looking at upgrading or even removing kombu. we changing the behaviour?. don't you have a race here? could we try and catch+ignore the exception?. does this mean that last one of these might keep the test run from finishing for 10 seconds?. does the broker reclaim immediately? (i.e. no grace period?). didn't mean for us to leave a grace period, but if the broker did (if so, we could try and just detect failures) but this is probably fine. ok. do we still need this then?. what's going on here?. this isn't public api right? time to drop this class entirely and just call the functions?. could maybe have serialization.setup return a namedtuple. maybe a comment to explain what's going on?. i'm less and less convinced of the benefits of these constants . not possible to share in a nice way?. possibly a bit too clever. neat. do you have any standalone rpc proxies in production? (for testing the pre-release of this). i wonder if we can/want to do something about the seemingly common case of wanting the dependency provider and the thing it provides have the same name . a bit surprised that the spellchecker (with all its false positives!) isn't catching these.... inline strings.... if i recall correctly, newer kombu has much better error messages. if so we can probably drop this whole module. was the copyright reassigned?. oki. this might benefit from a comment explaining how it works. ",
    "mattbennett": ":thumbsup: (apart from the grammar)\n. add_call_matching is legacy and deprecated, so closing this issue.\n. I'm happy to merge this and address the comments in my RPC implementation branch next sprint.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Very neat :)\n. :+1: \n. :+1: \n. Forcing travis to install test_requirements twice is ugly, but it does work.\npip seems to ignore the eventlet-patched version of coverage and install v3.7 to fulfil pytest-cov's requirements, even though it only needs >= 3.4 and the eventlet-patched version is 3.5pl1. Not sure if this is a bug or a \"feature\" of installing git+ packages.\n. I raised an issue with pip here: https://github.com/pypa/pip/issues/1249\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: love it\n. :+1: \n. Think we can consider this as done as possible in https://github.com/onefinestay/nameko/pull/187\n. As @davidszotten has said (offline), the most important thing before releasing this is to make sure we can find these logs again in sentry. Otherwise we might remove the RemoteError logging and not be able to locate the true source of errors.\nAs an aside, this change basically forces other users to employ some kind of log aggregator. I think that's fine, but we should add some notes about it to the docs.\n. Great stuff :+1: \n. :+1: \n. Mmm yeah, this is a bit icky. FakeContainer is the most egregious bit, which we might be able to hide. Maybe writing it with bare kombu will be cleaner though.\nI'm not sure we want to call it standalone_proxy either. Perhaps we can give it it's own module and call it singlethreaded.rpc_proxy?\n. :+1: \n. Combined with https://github.com/onefinestay/nameko/pull/114 I think we have cracked this.\n. I'm not sure about this. I understand that the current implementation is broken - naively timing out the 'kill' process will leave state lying around. On the other hand, this doesn't do much beyond stopping the container.\nIf the ability to forcefully kill a misbehaving dependency is overstepping our bounds (as killing the db injection before it's closed its sessions appears to be) then perhaps 'kill' should be as 'stop' is, except that we discard worker results and dependencies are encouraged to terminate as soon as possible.\nSo kill should be a faster but still blocking way to stop everything, and if someone writes a misbehaving dependency that hangs forever then tough. Or we add a force flag which comes with heavy warnings about the possibility of leftover state.\n. These commits introduce the notion of \"protected\" managed threads, of which the QueueConsumer's thread is one.\ncontainer.kill now kills things in the following order:\n1. non-protected managed threads (i.e. workers)\n2. dependencies, all at once as before\n3. protected managed threads\nSo we first throw away any current workers, then the dependencies, and finally kill anything left over. Killing the workers before the dependencies should stop any side effects of workers interacting with \"dead\" dependencies. The protected managed threads should all be handled by their owning dependencies, but we kill any left over just in case.\nAs an aside, the split between ManagedThreadContainer and ServiceContainer is now quite murky. I would prefer them to be one class again.\n. Opening this for review now, before it gets any longer.\nThere's a bug in the standalone rpc proxy that results in connections left open, causing failures elsewhere in the test suite.\n. :+1: \n. I'm not terribly keen on introducing a new utility to solve this problem.\nI think it would be better to add an abort_on_error (or similar) kwarg to  SpawningProxy. If given, it should kill off other threads and raise if any of the spawned threads raise an exception. pool.imap doesn't support this, but you could spawn the threads individually and use greenthread.link to process their results.\nIf all threads complete successfully, it might also be useful for SpawningProxy to return a set of their results.\n. Closing in favour of https://github.com/onefinestay/nameko/pull/97\n. Very nice.\nPlease add some tests for fail_fast_imap, since it's spun out into a standalone utility.\n. :+1: \n. :+1: \n. RPC queues are durable, and reply queues are not. As an RPC-exposed service you are a server, rather than a client, so it makes more sense for the queue to stick around while you restart.\nBut we could make RPC behave the same as events - that is, prefer clean over usable by default. There is less mess to clean up from RPC though - only one queue per service.\n. Having mused on this for a while I'm keen to implement this safe-by-default behaviour. Unfortunately  the RpcProvider > RpcConsumer > QueueConsumer delegation makes it non-trivial. I'd like to park this and re-evaluate with when we address https://github.com/onefinestay/nameko/issues/81.\n. This is still relevant and #81 didn't make it much easier!\nDirectly related to https://github.com/onefinestay/nameko/issues/294. The fact that Rpc entrypoints delegate to (possibly) shared RpcConsumers means we have to:\n- Instantiate the RpcConsumer inside the entrypoint constructor or\n- Explicitly update __params when new attributes are set post-instantiation so they're not lost using bind() (preferable) and\n- Add a custom sharing_key to RpcConsumer to make sure we don't reuse instances when they're supposed to have different configurations\n. This has been superseded by the work in #311. Reliable delivery is now enabled everywhere by default, which is consistent and a more sensible default. \n. Lovely :+1: \n. :+1:\nJust the name change and the E501.\n. Very neat :+1: \n. :+1: \n. @takluyver @kollhof \nThanks for your input! @davidszotten has written a followup blog post discussing these suggestions: http://tech.onefinestay.com/post/84435442712/eval-profiling\n. :+1: \n. Also adds forwards-compatibility for future releases that want to add more keys to serialized exceptions.\n. Yes, yes it is.\n. :+1: \n. :+1: \n. A note for @shauns: this invalidates some of the readme for sentry-nameko (WorkerContext no longer has an extra_for_logging property)\n. :+1: \n. :+1: \n. Tracking this as https://github.com/onefinestay/nameko/issues/248\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. It turns out there are many better ways to do this. Cleaning up this PR, finally.\n. Superb :+1: \n. :+1:\n. :+1: \n. Closing this in preference of removing wait_for_call altogether.\n. Branch coverage is at 99% \ud83d\ude2e \nI've added an allowed failure to travis in https://github.com/onefinestay/nameko/pull/372\n. :+1: \n. Closed in favour of https://github.com/onefinestay/nameko/pull/171\n. The issue here is that entrypoint_waiter requires the container to be idle before inserting itself as a dependency. It wouldn't be correct to pass the timeout value into wait_for_worker_idle, because the insert needs to happen before the waiter even starts counting.\nThis is a limitation of the way entrypoint_waiter is designed. If there's a long-running worker you can't use it at all. Furthermore if it's configured to wait on an entrypoint that fires but never completes, it will hang forever (doing wait_for_worker_idle again, before removing itself as a dependency).\nI think the correct solve for this is to redesign the entrypoint_waiter so it's no longer a dependency.\n. This is very nice.\n. :+1: superb\n. :+1: \n. And a note for the changelog\n. :+1:\n. :+1: \n. :+1: \n. :+1: Much more sensible.\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: Very nice indeed\n. :+1: \n. :+1: \n. This was useful for discussion but is not helpful as a PR, so closing.\n. The code in nameko/testing is providing test helpers for other users of the library. So if you build services in nameko you can use these helpers when writing tests for them. We should make sure they work as expected :)\n. :+1: \n. :+1: \n. Re-raised against 2.0 branch in https://github.com/onefinestay/nameko/pull/207\n. :+1: \n. :blush: Thanks guys!\n. :+1: \n. Fixed in https://github.com/onefinestay/nameko/pull/226\n. Much better :+1: \n. cool\n. :+1: \n. Merged with https://github.com/onefinestay/nameko/pull/252\n. :+1: \n. This is tremendous.\nWhere are you up to with https://github.com/davidszotten/rabbitman? nameko/testing/rabbit.py is nice enough but it would be better to delegate to a separate library I think.\n@noisyboiler asyncio support would be a big and fundamental change but you're welcome to give it a crack! A more reasonable goal might be to support regular threads as well as greenthreads, which probably adds value without costing too much (shouldn't cost a service developer anything). We could perhaps become fully concurrency model agnostic (i.e. including asyncio) but I think it will be hard, and perhaps not that useful.\n. :+1: \n. :+1: :+1:\n. Re-raised against master -- https://github.com/onefinestay/nameko/pull/253\n. :+1: \n. I like the base URL suggestion, although @davidszotten has suggested merging it with the existing WEB_SERVER_HOST and WEB_SERVER_PORT config keys to make a WEB_SERVER_URI setting.\nOn nameko not being a \"web framework\", the intention is to avoid competing (in the minds of the community) with much more established solutions like flask. We don't want to reinvent that wheel and we don't want people to think we're trying to either. But @kollhof raises a good point about having a consistent framework and making use of existing investment in nameko services; ultimately that's why we added HTTP support in the first place. And lots, if not the majority of people, use HTTP as the transport between microservices.\nIn the name of pragmatism, and the fact that @kollhof's is the first concrete use-case we have for HTTP, we should be open-minded about the \"web framework\" label. It's probably sufficient to say loudly at the outset that we don't include all the bells-and-whistles that you'd expect of a fully-featured web framework. in the box. It's quite possible that someone will write them anyway; the question is how much we should provide from the outset.\nI think for the \"built-in extensions\" we should strive to:\n- provide for the 80% case\n- with a lightweight implementation (no '000 line modules, rely on external libraries to do the heavy lifting)\n- not get in the way of extending to the full 100%\n- provide best-practice templates for others who want to create extensions\nFor HTTP the two difficult points are: identifying the 80% use-case, and the notion of best-practise for extensions. With AMQP messaging, we consciously don't give the service developer access to the message object, only the payload. Headers are available via a dependency. I think what we're achieving there is separation between the service code and the transport. And obviously Events and RPC are even higher abstractions.\nIt's a bit murkier with HTTP. The werkzeug Request object gives you access to form data, headers, cookies and file uploads. Requiring a dependency to access each of those seems a bit dogmatic, but the request object gives services lower-level access than we've made available elsewhere. I don't like the idea of service code looping over chunks of uploaded files, for example.\nNone of this precludes anyone writing their own extensions, of course. But back to the 80% case, it would be a bit silly if everyone basically built something like the above because it wasn't included out of the box.\nPerhaps I'm agonising about it too much and the built-in version should just be nice and useable. Either that or provide a low-level base and a sensible abstraction on top, much as we did with AMQP messaging and then Events.\nI like the parameterized POST URLs and hybrid handlers too.\n. Mostly implemented in https://github.com/onefinestay/nameko/pull/224 and https://github.com/onefinestay/nameko/pull/225.\nFor base URL support, feel free to raise another PR against the work in  https://github.com/onefinestay/nameko/pull/224.\nThanks @kollhof for the suggestions :)\n. This will be fixed (finally!) by #456. @rochacbruno, werkzeug.utils.import_string looks ideal\n. Ah, I had misremembered what the problem was here. Sadly, werkzeug.utils.import_string doesn't help. You still just get an ImportError.\n. Commandeering this issue to capture the discussion about registering configuration options.\nThanks to https://github.com/onefinestay/nameko/pull/255 we now have the ability to load config from a file, but there are a few other issues:\n- nameko run accepts a --broker option but none of the other configuration keys\n- most config is used by Extensions, which can be separate projects, but there's no way to inspect which config values are required or what their defaults are\n- importing FOO_CONFIG_KEY everywhere is ugly and you have to know where each Extension chooses to define them\nIf we registered configuration options and default values (via setuptools entrypoints?) we could:\na) access them from a known location (e.g. container.config)\nb) inspect the required keys and any default values for a given set of extensions\nc) automatically populate the argparse options for nameko run as @davidszotten suggests\n. @rochacbruno yep, I think env vars should be supported, but as part of a larger piece of work.\n. @rejoc I like this idea. We could bundle it into #520 (cc @iky) or make it as a standalone change first. Do you want to raise a PR?\n. :+1: pending @kollhof's suggestion\n. This is fine. I'd like to add something to the docs explaining that it's intentionally low-level and should probably be subclassed, but that can wait for another PR.\n. :+1: \n. :+1: awesome\n. There's no \"suspend\" per se, but the behaviour you describe is essentially just stopping (and presumably later restarting) a service.\n. Implemented in https://github.com/onefinestay/nameko/pull/242\n. @vtbassmatt thanks for the bug report! This is addressed in https://github.com/onefinestay/nameko/pull/237, and we tried to make the error message friendlier too - https://github.com/onefinestay/nameko/pull/240\n. Addressed by https://github.com/onefinestay/nameko/pull/237. Thanks!\n. Fixed in https://github.com/onefinestay/nameko/pull/237\n. Yes, yes we can https://github.com/onefinestay/nameko/pull/240\n. :+1: \n. Thanks! Glad you like it :)\nAbstracting the serialization is certainly something we'd like to do. Kombu supports pluggable serializers (see http://kombu.readthedocs.org/en/latest/userguide/serialization.html) but we lazily hardcoded to JSON for the serialization checks in the RPC implementation.\nRemoving the hardcoding should be straightforward and let you fall back to using kombu's serialization, which is a step in the right direction.\n. You can also register them programmatically - http://kombu.readthedocs.org/en/latest/reference/kombu.serialization.html#kombu.serialization.register\nQuite right about the config setting.\n. Done and landed! (finally) Thanks @ayoshi \n. :+1: nice\n. :+1: \n. :+1: \n. :+1: :+1: \n. This is a great question that we don't touch on in the docs (but should).\nShort answer: \nNameko does't dictate any particular tool. At onefinestay we log exceptions to sentry, timing stats to graphite and entrypoint invocation (service and method name, arguments, call stack, success/error status, result bytes, response time etc.) to elasticsearch/logstash. This is a really powerful setup for debugging across multiple services.\nEach of these loggers are service dependencies, and we intend to open source each of them as community extensions.\nLonger answer:\nOne of the nice things about nameko's dependency injection pattern is the integration with the worker lifecycle. DependencyProvider extensions are notified when workers are setup, torn down, and generate a result. A trivial logging dependency might look like:\n``` python\nclass LoggingDependency(DependencyProvider):\ndef __init__(self):\n    self.timestamps = WeakKeyDictionary()\n\ndef worker_setup(self, worker_ctx):\n\n    self.timestamps[worker_ctx] = datetime.datetime.now()\n\n    service_name = worker_ctx.service_name\n    method_name = worker_ctx.entrypoint.method_name\n\n    log.info(\"Worker %s.%s starting\", service_name, method_name)\n\ndef worker_result(self, worker_ctx, result=None, exc_info=None):\n\n    service_name = worker_ctx.service_name\n    method_name = worker_ctx.entrypoint.method_name\n\n    if exc_info is None:\n        status = \"completed\"\n    else:\n        status = \"errored\"\n\n    now = datetime.datetime.now()\n    worker_started = self.timestamps.pop(worker_ctx)\n    elapsed = (now - worker_started).seconds\n\n    log.info(\"Worker %s.%s %s after %ss\",\n             service_name, method_name, status, elapsed)\n\n```\nThen you can add it to your service in a nicely decoupled way:\n``` python\nclass Service(object):\n    name = \"myservice\"\nlogger = LoggingDependency()  # << just add this line\n\n@rpc\ndef method(self, arg):\n    ...\n\n``\n. This issue was moved to https://discourse.nameko.io/t/how-do-you-do-monitoring/319. Thanks @ayoshi for contributing! I think you also need to specify theacceptkwarg to theConsumer` constructor (on https://github.com/onefinestay/nameko/blob/master/nameko/messaging.py#L336)\nA test that demonstrated registering and using custom serializer would be great.\n. Hi @ayoshi,\nI confess I spent an hour or two on this over the weekend (before you raised this PR) and discovered that   by default kombu was not serializing to json if the data type was text or bytes. I agree with you that every message should be explicitly serialized, rather than letting kombu decide.\nI'll have a look at the error in kombu 3.0.1, but I'd probably prefer bumping the minimum version to working around the inconsistency in nameko.\n. The kombu commit that changes the behaviour is https://github.com/celery/kombu/commit/83cc99eca481ac35c23b52c8d293fe9f79dd8dc5, which landed in v3.0.11.\nI definitely think the correct thing to do is always specify a serializer, and avoid kombu's logic that sometimes doesn't bother encoding. \nThe failing tests all use the rabbit_manager fixture to publish a message, which does not set content_type or content_encoding. Hence the consumers in kombu < 3.0.11 refuse to accept them.\nMost of the tests using rabbit_manager.publish are dispatching events, so could and probably should use nameko.standalone.events.event_dispatcher instead, which was added after these tests were written.\nThe other places rabbit_manager.publish gets used should specify the content type and encoding using the properties kwarg:\npython\nmsg_properties = {\n    'content_type': 'application/json',\n    'content_encoding': 'utf-8',\n}\nrabbit_manager.publish(vhost, 'spam', '', 'shrub', properties=msg_properties)\nI'll put my other comments inline\n. Thanks @ayoshi, this is great. Really good to see the implicit behaviour made explicit, and I agree that json+pickle is a good combination of serializers for testing. \nI think parametrizing the mock_container is too broad though. Most of the tests where that fixture is used don't actually publish or receive messages, so we're running them twice without adding much value (or coverage). Some of these tests are fairly long in the tooth and not very accessible I'm afraid, so it's not obvious.\nI think I'd prefer to have parametrized tests around the individual components, e.g. make an RPC call and handle it (end to end) using both serialization formats; verify the standalone RPC proxy and event dispatcher honour the serialization format; publish a message and consume it using the queue_iterator again using both formats. You could optionally check the format on the wire by inspecting the rabbit queues, although that might be a bit of a faff. \nI'd also like to see a test that defines and registers a new trivial serializer and demonstrates that end to end. It's not strictly nameko's responsibility to test this but I'd like to verify that it can be done.\nI'll put my other small comments inline.\n. Sorry, these tests are in more of a mess than I realised. The reason that many of them don't honour the serializer is that the extensions are constructed manually by the tests, rather than via the container. They're in relevant places but written in a way that requires you to wire up the internals.\nTo answer your questions above:\n1. I don't think there are any existing tests that can be easily parametrized to use both serializers. test_messaging::test_publish_to_rabbit and test_messaging::test_consume_from_rabbit should test both serializers but should be rewritten rather than tweaked.\n2. I like the mock_container fixture. I think we should keep it and just drop the parametrization.\nThank you very much for your efforts on this. Don't want to step on your toes but I'd be happy to add the explicit serialization tests on top of this work. Or otherwise I could clean up the tests so the serializer parametrization worked in the way you'd expect.\n. @ayoshi thank you very much, looks great. I'll add the separate serialization tests on top.\n. @ayoshi apologies for the long delay on this. I took a long vacation and have been swamped since I got back.\nI've raised https://github.com/onefinestay/nameko/pull/264 which brings this up to date with master and adds high-level test coverage. Will ask one of the other collaborators to review my additions and then we'll get this merged. Thank you again for your contribution.\n. Thanks @Mytho. This has been folded into #250 \n. Thanks @junkafarian! This has been incorporated into #250 \n. For visibility, this is waiting on #266 to land so I can address the final comments\n. Merged as part of #280 \n. :+1: This is great. Thanks @Mytho for your contribution!\n. :+1: \n. :+1: \n. Can you add a small example to http://nameko.readthedocs.org/en/latest/cli.html#running-a-service?\n. Great stuff\n. @stchris Sorry for the confusion. As @davidszotten says, the feature landed in 2.1.2 by accident and consequently without release notes.\nWe were also serving the 'latest' docs by default on readthedocs, which is why --config was documented but not actually available when 2.1.1 was the latest stable release.\nAnd then weirdly readthedocs didn't rebuild 'latest' when 2.1.2 came out. Looks like a box ran out of memory (https://readthedocs.org/builds/nameko/2869567). I've just retriggered the failed build, so 'latest' should now be correct. I've also changed the default build to 'stable' so the docs should reflect the most recently released version.\nThanks for raising this.\n. Thank you\n. :+1: \n. QuantifiedCode are unfortunately shutting down, so I'm going to close this issue.. :+1: \n. Hi, apologies for the slow reply here.\nSQS support is certainly feasible. The way the project has evolved means we've probably made some assumptions that AMQP will be used, but that's not by design and shouldn't remain the case.\nNameko is organised around the notion of \"extensions\" and the AMQP features are implemented as such. My feeling is that SQS support should be implemented as a parallel set of extensions (so an entrypoint and proxy pair for RPC/synchronous calls and a dispatcher and handler pair for pub-sub/asynchronous messages). Writing these as subclasses of the AMQP ones might allow some amount of code reuse but the existing extensions weren't built with this in mind, so it may not be easy, at least for the time being.\nAn alternative approach would be to let kombu handle the differences. It is intended to be an abstraction on top of multiple transports and SQS is supported. There are certainly a few places where nameko assumes it's talking to RabbitMQ though, so we'd need to track those down. But as I said above, I think the nicest way would be a new set extensions that reused as much as possible of the AMQP ones (or a base class extracted out of them).\nThe good news is that SQS support is something that I'm pretty sure I'm going to need soon, so there's likely to be some progress in this direction in the next month or so. I'll keep this issue updated. What are your requirements? RPC, pub-sub, both or something else?\n. If I recall correctly we wanted to embrace tox but left the Makefile for convenience. This way we get both with less repetition, and you can dev locally without tox if you want.\nDropping show-missing and the rcfile was accidental, I'll add them back again. \n. Superseded by https://github.com/onefinestay/nameko/pull/266\n. Tests are failing on python3 due to the eventlet.monkey_patch() in the pytest plugin. Still investigating the root cause.\n. Agreed. I'm going to merge this into the \"fix-examples\" branch to address the final comments there, then cut an RC branch with everything that's marked as \"ready\"\n. Incorporated into #250 \n. Looks to me like your RabbitMQ broker isn't running.\n. Hi @morty, thanks for reporting. There is an open pull request that fixes this and a few of the other examples. See #250\nIt's currently blocked on a pytest-eventlet interaction bug, but a fix is in the works\n. :+1: \n. Thanks for the contribution! This is actually duplicate of #247 and has been incorporated into #250\n. Fixed in #283. Thanks @Mytho!\n. Sentry is a specific example. Any DependencyProvider can implement worker_result and it seems reasonable that an http entrypoint returning a 4xx/5xx status should follow the \"error\" path:\npython\ndef worker_result(worker_ctx, result=None, exc_info=None):\n    if exc_info is not None:\n        # process worker error\nI like the suggestion of an overridable method that transforms an exception into a response.\n. Superseded by #291\n. Nameko uses py.test for its own tests, and we encourage others to do so as well. It's also basically compatible with the other testing frameworks, so I don't see much value in adding non-pytest examples. You should be able to port the same concepts to another testing framework if you really need to.\n. @tyler46 yes please!. Not yet. I have done some high-level comparisons of nameko's HTTP entrypoint to other web frameworks (names withheld to prevent flamewars) and it is fast, presumably because it's so lightweight. I'll publish them when I'm confident I'm doing a fair test.\nBut profiling, no. I would love to see some though. I'm sure there's plenty of low-hanging fruit to be found and picked.\n. @noizex I suspect your benchmark involved creating a new ClusterRpcProxy for each call. Setting up the rabbit connections and reply queue is expensive.\nThis silly example of one service talking to another gives me ~110 calls per second locally:\n``` python\nimport itertools\nimport eventlet\nfrom nameko.rpc import RpcProxy, rpc\nfrom nameko.timer import timer\nclass Server(object):\n    name = \"server\"\n@rpc\ndef method(self):\n    return 42\n\nclass Client(object):\n    name = \"client\"\nserver_rpc = RpcProxy(\"server\")\n\n@timer(interval=1)\ndef start(self):\n    with eventlet.Timeout(1, exception=False):\n        for count in itertools.count():\n            self.server_rpc.method()\n    print(\"{} calls/s\".format(count))\n\n```\n\nThe RpcProxy keeps its reply queue alive between uses, so it's much faster. You will also get a performance boost if you turn off persistent messages as @astawiarski suggests, but that gain comes from processes inside the broker rather than nameko.\n. nameko run will start all the services defined in the module you pass it. You could run the \"server\" service and the \"client\" service in two different processes and the performance would be the same (potentially better if it's CPU-bound).\nAll I'm demonstrating is that the RpcProxy dependency provider reuses its connections, whereas the proxies in the standalone module (ClusterRpcProxy and ServiceRpcProxy) do not. There's no reason why they shouldn't reuse connections, but the current implementation does not, which makes them unsuitable for a performance test.\n. Nameko optimises for the first thing you mentioned, convenience of writing services. If you need to do thousands of requests/sec there are more appropriate tools.\nThat said, there is probably a ton of low-hanging fruit that can be picked to improve performance in Nameko, but nobody has really tried to do it yet. One area I've regularly seen performance bottlenecks is serialization -- Nameko does a lot of it and JSON serialization in Python is generally pretty slow. You could switch out the default serializer for something faster pretty easily.\nAlso note that since Nameko 2.7.0 it's been possible specify delivery options in your message publishers, for example to turn off persistence and publish confirmations. You will probably get decent mileage out of adjusting these options to your usecase. . This issue was moved to https://discourse.nameko.io/t/namekos-performance/318. It's definitely preferable to use an API rather than wrapping a command line tool. Also I strongly suspect that rabbitmqctl either directly calls the API itself, or they both invoke the same underlying code.\n. rabbitmqctl seems to be a wrapper around the erlang interpreter (https://github.com/rabbitmq/rabbitmq-server/blob/master/scripts/rabbitmqctl) so there's no benefit in going to what's \"underneath\" rabbitmqctl (same problems of the binary perhaps changing between platforms, not being on the path etc.)\nAs I said I suspect the fact that connections aren't immediately closed is just how things are implemented inside RabbitMQ, and doesn't have anything to do with the management API per se.\nI agree that it would be nicer to use and then clean the same vhost between tests, but it doesn't seem to be possible, at least reliably.\n. Fixed in #288\n. Wonderful, thank you!\n. Hi @javaguirre, thanks for raising this. It does relate partly to #275.\nGenerally we might want to think about a debug mode for the web handler that will return the stack trace when enabled (for example as flask and django do). At the very least we should make it easier to debug service errors with better default logging and/or docs on how to attach logging.\nWatch this space.\n. @noisyboiler is correct. The request parameter is superfluous in the empty_config fixture. It doesn't need to be there because it's not used, but also isn't affecting anything.\nYou will find that the following is correct:\npython\ndef test_foo(mock_container, empty_config):\n    assert mock_container.config == empty_config\n. Yeah we should\n. Your server script starts and then immediately stops the service https://github.com/jiamo/nameko_research/blob/master/test_service_server.py#L32\nYou don't need to create a ServiceRunner yourself. You can use the nameko command line to run your service:\nshell\n$ nameko run test_service_server --config nameko_server.conf\n('config is ', {'AMQP_URI_CONFIG_KEY': 'amqp://guest:guest@localhost', 'max_workers': 10, 'AMQP_URI': 'amqp://guest:guest@localhost', 'parent_calls_tracked': 10, 'rpc_exchange': 'nameko-rpc'})\nstarting services: helloworld\nConnected to amqp://guest:**@127.0.0.1:5672//\nAlso there is no need to implement the service client as you have. Just use the nameko command line:\n``` shell\nnameko shell --config nameko_server.conf\n\n\n\nn.rpc.helloworld.hello(name='John')\nu'Hello, John!'\n``\n. You need to both apply the [eventlet monkey patch](http://nameko.readthedocs.org/en/stable/key_concepts.html?highlight=eventlet#concurrency) and callwait()` on the runner to block until it exits.\n\n\n\n``` diff\nindex e764db8..8597563 100644\n--- a/test_service_server.py\n+++ b/test_service_server.py\n@@ -1,3 +1,6 @@\n+import eventlet\n+eventlet.monkey_patch()\n+\n from nameko.rpc import rpc\n from nameko.runners import ServiceRunner\n from nameko.testing.utils import get_container\n@@ -29,7 +32,7 @@ def main():\n     # start both services\n     runner.start()\n\nrunner.stop()\nrunner.wait()\n     # container = ServiceContainer(HelloWorldService, config=config)\n     # service_extensions = list(container.extensions)\n     # print (\"service_extensions is \", service_extensions)\n```\n\nYou should really consider using the code from nameko.cli.run even if you don't use the command line script, because the signal handing is non-trivial. For example, see https://github.com/onefinestay/nameko/blob/master/nameko/cli/run.py#L134\nI'm going to close this issue now. If you have more questions about how to use nameko please post them to the (new!) google group.\n. @laike9m start() does start the service, but you must wait() on the container or the runner to stop your process or thread from exiting.\nThe best way to run a service is nameko run on the command line. Or, if you want to embed the service in another application, wrap the run function in nameko.cli.run -- https://github.com/onefinestay/nameko/blob/master/nameko/cli/run.py#L117 (which is what the command line tool uses under the hood)\n. Sorry, I forgot about this thread.\nstop() waits for all running workers and managed threads to exit gracefully. You can't expose container.stop() in an entrypoint and expect to the container to stop when you call it -- this would create a deadlock between the container trying to stop and the worker or managed thread running the stop() method.\nIf you want to expose an entrypoint that is capable of stopping the container, you'd need to do it \"out of band\" (i.e. using a thread that wasn't created by the container)\nHere is a toy example that does that:\n``` python\nfrom nameko.rpc import rpc\nfrom nameko.extensions import DependencyProvider\nimport eventlet\nclass Stopper(DependencyProvider):\ndef stop(self):\n    # can't use container.spawn_managed_thread here either\n    eventlet.spawn_n(self.container.stop)\n\ndef get_dependency(self, worker_ctx):\n    return self\n\nclass Service(object):\n    name = \"stop\"\nstopper = Stopper()\n\n@rpc\ndef stop(self):\n    self.stopper.stop()\n    return \"stopping\"\n\n```\n(tmp-6d6e28416a4d3fa9)Matts-13-inch-Macbook-Pro:tmp mattbennett$ nameko run stop\nstarting services: stop\nConnected to amqp://guest:**@127.0.0.1:5672//\n```\n(tmp-6d6e28416a4d3fa9)Matts-13-inch-Macbook-Pro:~ mattbennett$ nameko shell\nNameko Python 2.7.10 (default, Jul  6 2015, 15:19:48)\n[GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)] shell on darwin\nBroker: amqp://guest:guest@localhost\n\n\n\nn.rpc.stop.stop()\n```\n. Yes indeed, the example above is not a demonstration of how you should achieve this. It is only meant to illustrate an \"out of band\" thread.\n\n\n\nGenerating a SIGTERM signal is a much better way to have the container to stop gracefully.\n. @davidszotten this is ready for another round when you have time\n. @davidszotten @noisyboiler this is ready, finally\n. This is great, thanks for the contribution!\nI'm happy for the AMQP_URI in the run-config.yaml file to change though, to avoid the need for a separate shell-config.yaml file. Please also rename it to just config.yaml.\n. @astawiarski Excellent, thanks. To round out this PR please add release notes to CHANGES and also add yourself to the bottom of the contributors file.\nFormat for unreleased changes is as follows:\n```\nVersion 2.2.1\n\nReleased PENDING\n\nRelease notes\nAs bullets\n\n...\n```\nWe follow semantic versioning and this is a backwards-compatible change so the version should be 2.2.1.\nRe: @davidszotten's comment. I think main.main should parse the yaml file and pass the result into run.main and shell.main. These functions can then add their required defaults if no config object was provided. I've not looked at the test fallout though and I'd say it's of marginal benefit.\n. :+1: Great stuff. Thank you @astawiarski!\n. Great catch.\nWe should probably draw attension to the C extension issue in http://nameko.readthedocs.org/en/stable/key_concepts.html#concurrency and reference it from the \"similar projects\" page.\nI'm happy to merge this as-is and create an issue to improve the docs re eventlet.\n. Completely agree that this should be possible. We are generally inconsistent about exposing delivery and other messaging options and to callers and there have been a number of issues related to it.\nI like Option B, because it allows us to keep the simple invocation in the normal case. If you really needed per-call options you could do it with a locking context-manager around the invocation.\nAre there other AMQP options/properties we should expose here as well?\nOn a related note we have AMQP publishing code scattered all over the codebase, and it's all pretty similar. The following all wrap a kombu publisher and pass slightly different options to it:\n- nameko.standalone.event_dispatcher\n- nameko.messaging.Publisher.get_dependency\n- nameko.rpc.Responder.send_response\n- nameko.rpc.MethodProxy._call\nIt feels like we should have a nameko.amqp module that consolidates all this repeated code.\n. Subclassing is nice but tricky to do in this case because MethodProxy is quite buried. You'd have to subclass ServiceProxy as well, and you'd have to do it for each permutation of options you wanted.\nSince this is RPC we can probably get away with using a property. Something called delivery_options is unlikely to be a remote method.\n. Fixed in #440. Thanks :)\nThis isn't as easy to achieve as it should be.\nYou could subclass the RpcConsumer with hardcoded options and pass them to the Queue when it's instantiated in RpcConsumer.setup. It's blunt. You'd need a subclass for every permutation of queue_arguments, but it would be easy to implement.\nA more elegant solution (e.g. where you could provide arguments to the decorator like @rpc(queue_arguments={...})) would require changes to the bind and extension sharing mechanisms that nameko uses internally. I would love to get to the point where it's possible though.\n. For posterity, getting this to work requires updating __params when you set properties on an extension after instantiation (otherwise they'll be lost when you bind), or moving the whole extension declaration to instantiation time. It also requires that these properties are taken into account when determining whether to reuse a shared extension. \n. @chassing feel free to tell me to mind my own business but I would recommend against having a custom subclass like this, for a number of reasons:\n- Entrypoints were designed to stack for exactly this use-case.\n- By overloading Entrypoint.decorator you've made your application brittle to changes in this \"internal\" nameko code. \n- (Subjective but) I don't consider multiple @event_handler decorators to be duplications. Each one is an explicit declaration, and having them separate will make for easier refactoring in the future. If you're listening to the same event from more than a couple of services you should consider whether your service architecture is correct.\n- Adding ~25 lines of complex decorator code is a lot less maintainable than \"duplicating\" these simple event handler declarations.\nJust my 2\u00a2. Can you write about what your specific use-case is?\n. Fixed in https://github.com/nameko/nameko/pull/440. :+1: \n. :+1: \n. This might not be the entire story, but I remember all the \"timer\" entrypoints firing simultaneously as soon as a service started, which didn't seem sensible.\nI think that both behaviours could be considered \"correct\" and it probably just needs documenting better. Or giving it an eager=True parameter.\n. @jessepollak \nYes, although I've made the test cleanup optional. I think my former-self was being overzealous there (sorry @Sparkycz!) It's a low priority fix.\n. I'm in favour of fixing this. I think the semantics should be either:\n- At most once per interval seconds or\n- Exactly interval seconds after the previous execution finishes\nLooks like @Sparkycz's PR does the second option.\n. @davidszotten Yep, waiting interval - elapsed_time is Option 1 and also my preference.\n@Sparkycz I will add a few comments to the PR. Please address them and then we'll merge this down. Thanks!\n. @daviskirk I think almost certainly not. A fix would be great!. Closing this issue to encourage discussion to move to the mailing list.\n@rochacbruno if you have more questions please ask them there, and if you're able please report the backdoor bug (here, as an issue). Thanks\n. There's been an issue open for a while - #276\n. The basic answer to this question is that it's possible to build a pair of nameko extensions to handle any kind of protocol. The built-in RPC implementation uses a (very simple) nameko-defined protocol on top of AMQP, but you could use any protocol on on transport.\nYou need an Entrypoint to handle incoming requests, and a client of some form to generate them. We don't really have docs on writing entrypoints, but you can study the nameko.web package for a simple example. It's built on werkzeug so what you find in that package is really the essence of an entrypoint.\nI'm going to close this issue but please write to the mailing list if you decide to move forwards on this.\n. @davidszotten, yep I'm fine with bumping the minimum required version. \n. I think you can just bump the version of mock required in [dev] requirements\n. How far do we have to move mock to fix this issue? I don't want to yak-shave our way into having to repackage the test helpers. I think the sensible options here are:\n1. Abandon this PR, and let the spec remain \"loose\"\n2. Upgrade our mock library to get the new feature\nUpgrading seems fine to me. Mock is quite a well behaved library, I don't think there will be many folks out there who are required to use an older version.\n. OK, that seems sensible. Let's leave this open for now\n. Mock 2.0.0 is stable and has been in use with our nameko services for some time, so we can bring this up to date and merge it now\n. @davidszotten, we missed the boat here.\nCan you bring up to date again? Then it's good to land, right?\n. \ud83d\udc4d \n. Superseded by https://github.com/onefinestay/nameko/pull/315\n. This change doesn't prevent the \"old\" behaviour so in a sense it does allow both modes of operation. To summarise:\nOld BROADCAST behaviour\n- reliable_delivery not possible (queues set to exclusive and auto-delete)\n- queue names appended with a uuid created on service start \nTherefore any messages sent to a previous instance of the service are not seen by new instances (e.g. after a restart)\nNew BROADCAST behaviour\n- reliable_delivery is possible, but not mandatory\n- you can supply a differentiator callable to provide the value appended to the queue name\n- default differentiator uses socket.gethostname so queues will be reused between service restarts\nIt is therefore possible to use the old behaviour by passing uuid.uuid4 (or similar) as the differentiator and setting reliable_delivery=False.\nI would argue that both modes of operation are valuable. Semantically, the old behaviour is \"broadcast to all service instance processes\", and the default new behaviour is \"broadcast to all service instance deployments\".\nIf we have to pick one the new behaviour is more useful, and we would want to keep the ability to provide a differentiator (for example if socket.gethostname didn't return something sane or useful) which basically you get the old mode for free anyway. Though we might choose not to document it. \nThis change is backwards incompatible in the sense that reliable_delivery in broadcast mode now defaults to True. Actually this brings it in line with the other modes, which also use reliable delivery by default. #104 has been open for some time to make us \"safe\" (i.e. non-reliable) by default, but I think this is a separate concern.\n. OK thanks. Let me try to address these concerns:\n1. There are two things that are backwards incompatible here:\n   - a) reliable_delivery for BROADCAST mode now defaults to True\n   - b) the \"differentiator\" changes from uuid.uuid4 to socket.gethostname (so we go from process broadcast to deployment broadcast).\nFor 1.a, we could keep the old behaviour by mapping a handler_type to a default value for reliable_delivery. But would then have to explain why it's like that, and I don't think there is a good reason, except for maintaining backwards compatibility. We would want to remove the anomaly eventually, so all we're doing is postponing the backwards-incompatible change. We include #104 in these changes too, which would compress the two into a single backwards-incompatible release.\n1.b can be solved by changing the default differentiator (or its equivalent) back to uuid.uuid4. We can definitely do this, but there's an argument that we're making things less useful for the sake of backwards-compatibility.\n2. I understand all of these concerns. I will have a crack at an implementation using a method that can be overridden. This would be more in line with the nameko way of doing things. Being able to change the entire queue name is an interesting idea, though the non-broadcast modes rely on well-defined queue names so that consumers connect to the appropriate queue.\nRegarding gethostname -- I wanted something that would be unique to the service instance but survive restarts, and for most people I figured hostname was a reasonable guess. It makes for nice readable queue names too ;) It won't always apply, but that's why you can specify it yourself.\nIt turns out that broadcast queues aren't exclusive, but they should be, which would also let us detect the situation where multiple service instances end up with the same broadcast queue name.\n. There's also hidden option number three: explicitly add PROCESS_BROADCAST and DEPLOYMENT_BROADCAST as separate modes. Although in the latter case you have to be able to provide a way to differentiate one deployment from another, since gethostname won't always be valid.\n. OK, this version is better I think. I've made the following changes:\n- Removed differentiator in favour of a broadcast_identity property, which can be overridden\n- Restored the default to a uuid that is created when the service starts\n- Added docs explaining this behaviour and some alternatives\nThe behavioural difference now is that reliable_delivery defaults to True in broadcast mode, and that we emit a warning in this case rather than raise an exception. Since we used to raise, there can't be any working code out there that will now behave differently.\nI also made the broadcast queues exclusive, although it turns out you can't easily detect the rejected connection so I didn't bother going further.\n. If I understand correctly there are two questions here:\nQ1. Is there any way to know how many threads are running?\nIt depends what you mean. The ServiceContainer._worker_pool is the pool of Greenthreads used to run workers (i.e. the threads used to execute service methods). You can call pool.running() to find out how many workers are currently active. I guess this is the metric you really want to know? If so, there are better ways to track the information, such as a DependencyProvider that increments a counter on worker_setup and decrements it again on worker_teardown.\nNameko uses threads for other things other than running workers too. Most entrypoint implementations  rely on a background thread (e.g. the Timer). If these threads are managed by nameko (which they should be, aren't guaranteed to be) you can find them in the ServiceContainer._active_threads and ServiceContainer._protected_threads attributes.\nQ2. Can @timer decorated methods know what interval argument they were called with?\nNot in the built-in implementation, but it's pretty easy to create a subclass that passes it in:\n``` python\nfrom nameko.timer import Timer\nclass AwareTimer(Timer):\n    def handle_timer_tick(self):\n        args = (self.interval,)\n        kwargs = {}\n        self.container.spawn_worker(self, args, kwargs)\naware_timer = AwareTimer.decorator\n```\npython\n@aware_timer(interval=10)\ndef testing(self,interval):\n  print interval  # works\nThe timer entrypoint is entirely declarative though, so why don't you do the following?\n``` python\ninterval = 10\nclass Service:\n    @timer(interval)\n    def testing(self):\n        print(interval)\n```\nThe user group is a better place for these kind of questions. If you have any more, please ask them there.\n. Seems like a fine name and approach to me. You'll need to add a test which covers the .async case (and ideally verifies the call to warnings) to get to 100% coverage.\n. I don't understand this request. Do you have an example?\n. \ud83d\udc4d \n. It's certainly possible. There are PHP libraries for RabbitMQ that you could use to reimplement the logic in https://github.com/onefinestay/nameko/blob/master/nameko/standalone/rpc.py.\nIf you have more questions the mailing list (https://groups.google.com/forum/#!forum/nameko-dev) is the most appropriate place to ask them.\n. This definitely should be a topic in the docs, which is why this issue is here.\nHave you discovered this forum post on the topic? You should be able to define and register a custom serializer that supports decimals.. @davidszotten this is ready for another round of review.\nApologies for all the commits. I have been back and forth a lot with various versions of pytest, eventlet, coverage and pytest-cov.\nThe root cause of all the failures above was upgrading to a recent pytest, which has issues with eventlet on python3. I've summarised these in a separate repo (https://github.com/mattbennett/eventlet-pytest-bug) but the short answer is that pytest > 2.7.3 is incompatible with eventlet > 0.17.4.\nAlong the way I also learned that (irrespective of python version) we can't use coverage on its own (i.e. without pytest-cov) because coverage from tests executed by the testdir subprocess aren\u2019t stored unless they return with 0 status code, and nor can we use pytest-cov > 2.1.0 because it doesn't honour the coveragerc file when used with the testdir pytest plugin. For posterity, I created and then reverted a couple of commits to demonstrate these failures.\nThere is at least one bug to raise against eventlet/pytest. It may also be considered a bug that coverage isn\u2019t collected in all cases when using testdir, and that the behaviour in pytest-cov changed since 2.1.0. I\u2019ll try to raise these too.\nIn the mean time I don\u2019t think there\u2019s anything blocking this PR from landing, if you\u2019re happy.\n. @davidszotten I think this can land now, if you're happy\n. It is correct that you must specify the source service as well as the event type. But even if that wasn't the case, the nameko event handler (and the pub-sub pattern in general) doesn't support returning results to the dispatcher.\nRPC is the way to achieve what you want here.\nIf you have more questions, the mailing list (https://groups.google.com/forum/#!forum/nameko-dev) is the best place to ask them.\n. I think Travis is smart enough not to reattempt the uploading on a subsequent build.  I've manually uploaded the missing files.\nWhy did you need them to be on PyPI (as opposed to just generating them from source)?\n. No sweat :)\n. Hey Jun,\nSeneca looks cool, and sympathetic to the way nameko is written -- keeping business logic separate from transports.\nI had a look over the AMQP transport and it would certainly be possible to make a Seneca-compatible set of nameko extensions. If you wanted to do this I would start with a nameko.rpc.RpcProxyequivalent that can make requests to a Seneca service, and then add an @rpc equivalent entrypoint that could handle requests from a Seneca client.\nThe seneca-amqp-transport docs only seem to talk about RPC. Does it also support pub-sub style messaging like nameko events? Or lower-level AMQP messaging with full control over queues, exchanges, bindings and routing keys?\nThe mailing list is the best place for these kinds of discussions. I will close this issue and we can pick up the conversation there if you like.\n. @davidszotten what do you think about introducing this library as a dependency?\nIt's only conditionally required if users also have pytest installed. I'm not sure if setuptools has the ability  to specify rules like that.\n. @davidszotten I have reimplemented the most useful bits of that library\n. No, I completely agree. I've removed it for a use-case specific version.\n. Indeed the other HTTP verbs do just work. I will make a note to update the docs!\n. The docs for this have now been updated thanks to @geoffjukes . @davidszotten @iky @timbu \nThis is ready for review now. The toxiproxy tests were useful for me to understand how the different failure modes were handled. I think they're useful to keep as functional demonstrations, but I could be persuaded otherwise.\n. Bump on this please chaps.\n. I'd like to bump the connection_params changes into a separate PR because it deserves its own consideration. Given that, this is ready for a final review.. Although it will (gently) clash with #383 so we may want to land one before the other.. @davidszotten, this is ready for another round.. I can't reproduce this locally.\nWhat library versions, platform, python and RabbitMQ versions are you using?\nAre you running the services with a config file?\nWhat is the state of the Rabbit queues when it hangs?\n. The missing feature is more likely to be consumer prefetch rather than publish confirms. Prior to RabbitMQ 3.3.0 the prefetch count (which nameko sets to the same value as max_workers) is shared across the entire connection. That includes the reply listener, so you ended up with a deadlock.\n. @davidszotten --\n@timbu and @iky and I have been discussing this PR, in particular your concerns about the similarity of the wait_for_call and entrypoint_waiter callback signatures.\nThey do similar things (both \"wait for a method call\") but the entrypoint_waiter is tailored to a special case and has to check much more than just that the service method was executed -- it waits for the method to be executed, the result to be handled and worker teardown to complete. We couldn't find a way to one a special case of the other.\nBut there is a way to make their callback signatures the same...\nThe argument against having the worker context in the entrypoint_waiter callback is that most test writers will not need it. Having said that is is useful for testing nameko internals, as I have done in https://github.com/mattbennett/nameko/pull/3/files#diff-c6900903a76fe66c078314cc5973ee04R655\nSo we could have a basic  documented callback argument with the same signature as the wait_for_call callback, and an another optional worker_callback (or similar) that gets the worker context instead of args and kwargs.\nI prefer having two separate signatures, but I will push something like the above for your consideration.\nAt the risk of adding another chef, do you have any thoughts on this @jessepollak? Is it confusing to have have both of these APIs?\nWould it be less confusing if we spun wait_for_call out into a standalone library, so the entrypoint_waiter was more obviously implemented on top of it?\n. OK great, I've reverted those changes.\nI think this is ready to land, if you're happy @davidszotten.\n. Can we not change the API to just accept named arguments? The named dependency would be replaced by the argument value, or a MockDependencyProvider if the value is None.\nWe could maintain backwards compatibility for people using positional arguments and raise a DeprecationWarning to notify of the changed API\n. Not sure I follow your argument.\nWith the changes in this PR, replace_dependencies has two slightly different APIs. One is basically an extension of the other. My suggestion is that, rather than maintaining and documenting both, we transition to the one that is most capable.\n. Hmm, yes that is different. I would be OK with a different return format for the deprecated API though -- i.e. return a list if you provide args only, and a dict otherwise.\nThis is in the spirit of there being \"one [obvious] way to do things\", with the other path deprecated.\n. Just making a note that we discussed this in person and decided that supporting both args and kwargs APIs was the best option. Will review in due course.\n. @timbu there are just a couple of outstanding comments on this\n. @timbu thanks! happy for me to fix the spelling and add a changelog note in this branch?\n. \ud83d\udc4d \n. @davidszotten this is ready for a round of review when you have a chance\n. Our use-case is for a custom ServiceContainer class.\nWe deploy our services to multiple regions and need slightly different behaviour in each, but don't want to maintain different branches or codebases. For example, it's only valid to write to the database in one region.\nThe custom container supports an \"entrypoint blacklist\", a configuration level way to disable certain entrypoints, so we can deploy the same codebase in all regions and tweak the behaviour via config.\n. We don't have a custom WorkerContext, but we can't get rid of it without another change:\nWorkerContext.context_keys specifies which bits of the context data get passed from call to call, and the only way to set it is with a subclass. It's an odd API though. I'm not sure why we need a distinction between data on the worker context and context_data, which is the subset that matches the keys.\n. @davidszotten as discussed in the offline meetup, I've removed WorkerContext.context_keys and the ability to (directly) customise the worker context class.\n. @davidszotten this is ready for another round\n. You\u2019re right, there is no standard way to do this in nameko. There are many different ways to do validation and I think it\u2019s preferable to leave the choice up to the application developer, rather than codifying a specific way into the framework.\nThat said, it\u2019s useful to talk about all the various options.\nAt Student.com we use marshmallow to validate rich objects (e.g. data passed to an \u201cupdate\u201d API). There\u2019s no hard-and-fast rule on when to use a schema \u2014 it\u2019s a function of the complexity of the object, how often it changes, and whether we trust/own the caller.\nWe also also use marshmallow schemas to describe the \u201cservice contract\u201d of our external APIs, which all happen to be RESTful. We don\u2019t use the schemas dynamically though, as you describe. They\u2019re more like formal documentation, which our customers can choose to bake into their clients if they want to.\nWe haven\u2019t implemented our validators as a decorator, but that would be a nice way to keep boilerplate validation separate from the service method. If the schemas were bound to the entrypoint they would be introspectable too, so you could discover the service contract by inspecting the service class. That would allow you to do things like automatically build documentation.\nWe use RPC inside our service cluster, between the services we own and control. We don\u2019t need anything more strict than the signature check here, but if we did we might opt for something like Thrift instead of RPC.\nI've not used type hints yet so can't really comment on how that might work. Can you still benefit from them if the hinted methods are not called directly?\nThe mailing list is a better place to continue this discussion, so I'm going to close this issue.\n. The stack trace you've posted shows a service that is starting, not running. Service start is actually the only code path that verify_amqp_uri is called from.\nIf the network glitches while the service is running, a few things may happen:\n- Consumers will notice, and try to reconnect with backoff\n- Publishers will notice immediately if publish-confirms are enabled, and sometime later if not. https://github.com/onefinestay/nameko/pull/337 adds support for this\n. I'm surprised verify_amqp_uri doesn't give a better error message in this case though\n. This is moot now verify_amqp_uri has been removed. Nameko uses eventlet for concurrency. Scrapy uses Twisted, and the two unfortunately cannot be used together. This is an inherent incompatibility between the two event loops.\n. Fixed by #350 \n. I am in favour of assuming py3. Seems fitting for an example to use the best version of the language.\nI will rework these.\n. @davidszotten happy for this to land?\n. It seems not: https://github.com/travis-ci/travis-ci/issues/6379\nWe seem to be fighting with Travis more and more. Time to switch to CircleCI?\n. It's actually pretty easy to run travis builds against a dockerized rabbitmq (e.g. https://github.com/nameko/nameko-amqp-retry/blob/master/.travis.yml), so end-to-end tests are quite possible here. It's a one-liner to start a rabbit broker and pass the certs in.\n@hedin are you interested in picking this PR back up again?\n. @jakedahn apologies for the delay. I think it just needs an end-to-end test on CI, which probably means switching over to a dockerized rabbit rather the Travis service.\nI will take a look in the next couple of days and confirm exactly what's needed.. I'm happy for this to land when the two remaining items on the todo list in the description are done.. These tests will fail against the built-in Travis RabbitMQ broker because it's not configured for SSL.\nI have spun up a dockerized RabbitMQ with SSL enabled using the following steps:\n\nGenerate ca, server and client certs following https://www.rabbitmq.com/ssl.html\nGather certs and keys from the server and ca directories into a directory (ssl)\nRun dockerized rabbit identifying the appropriate certs/keys:\n\ndocker run -it --rm -v \"$PWD\"/ssl:/ssl -e RABBITMQ_SSL_CACERTFILE=/ssl/cacert.pem -e RABBITMQ_SSL_CERTFILE=/ssl/servercert.pem -e RABBITMQ_SSL_KEYFILE=/ssl/serverkey.pem  --hostname rabbit --name rabbit -p 15672:15672 -p 5672:5672 -p 15671:15671 -p 5671:5671 -e RABBITMQ_DEFAULT_USER=guest -e RABBITMQ_DEFAULT_PASS=guest rabbitmq:3-management\nUnfortunately, in the official RabbitMQ docker image, enabling SSL disables the normal ports (https://github.com/docker-library/rabbitmq/issues/79) so it can't be used here as a trivial drop-in replacement for the built-in broker.\nI guess the next step would be to create our own Dockerfile to run a containerised RabbitMQ that runs with SSL and non-SSL ports enabled simultaneously.. @hedin I have created a dockerized rabbitmq for testing -- https://hub.docker.com/r/nameko/nameko-rabbitmq/. The image includes self-signed certs and keys for the server, client and CA under /mnt/certs.\nYou should be able to use this to write an end-to-end test on CI.\n. Sorry, I didn't reply to @hedin above. I would like to see an end-to-end test over SSL. Just a simple service talking AMQP to an SSL enabled rabbit.\nIt should be simple to add now we have a docker container that will accept SSL and non-SSL connections.. @hedin new images with updated certs are available now. Valid for 10 years this time ;). This PR actually enables SSL for all the AMQP extensions, not just RPC. So for completeness we should probably have the following tests:\ntest_rpc.py:\n\u2022 test_rpc_over_ssl: handling an rpc request in an ssl-connected service\n\u2022 test_rpc_proxy_over_ssl: using an rpc proxy over ssl\nstandalone/test_rpc.py:\n \u2022 test_rpc_proxy_over_ssl: same as above but using standalone proxy\ntest_events.py:\n \u2022 test_event_handler_over_ssl: handling an event in an ssl-connected service\n \u2022 test_event_dispatcher_over_ssl: dispatching an event in an ssl-connected service\nstandalone/test_events.py:\n \u2022 test_event_dispatcher_over_ssl: same as above but using standalone dispatcher\ntest_messaging.py:\n \u2022 test_consume_over_ssl: handling a message in an ssl-connected service\n \u2022 test_publish_over_ssl: publishing a message from an ssl-connected service\nIn each case there are two channels of communication, client->broker and broker->service, and it's possible for one or both legs to be SSL'd. I think it's sufficient to test just the appropriate leg in each test.\nIn the interest of shipping this change soon, I'd be happy to add these on top of your commits, if that's OK with you?\n. Commits on top of this branch are in #524. Thanks again @hedin for raising this PR.. @hedin @roblofthouse @gianchub @jakedahn @pimiela @bweaver-cxp\nThis functionality is now (finally) available in a pre-release build on PyPI: 2.9.1rc0. I don't have the ability to test it extensively, so could one or more of you please try and report back if everything seems good and stable?\nThanks @hedin for the original PR.. Thanks for the feedback @bweaver-cxp. Thanks for raising this! Sorry it's taken me a while to review.\nCoverage looks good, although it would be nice to add a test demonstrating the use of a custom wsgi app and/or server.\nUnfortunately there is no \"HTTP\" section of the docs to really describe these changes in. Putting a note in the docstring of the WebServer class is probably sufficient for now, and a reference in the changelog.\nThen add yourself to the contributors file if you like, and it'll be good to go.\nThank you for addressing the other comments so quickly! \n. Beautiful PR :) Just a couple more comments\n. @jessepollak this is great.\nYou have a couple of linter errors and then it's good to go. Thanks!\n. @jessepollak I forgot to mention a changelog entry, sorry. If you can add that then it'll be ready for release. This is backwards compatible so the pending release will be version 2.4.1.\n. Timezones have not been our friend here. Sorry for all the back and forth!\nThere was a typo in your changelog, so #342 leapfrogged and now there's a conflict. I tried to push a fix to your branch (which seems to be a new feature github have enabled) but don't have permission.\n. \ud83d\udc4d  Great stuff. Thank you @jessepollak!\n. @izmailoff, on which platform did you capture this tcpdump trace?\nI'm surprised by the frame that couldn't be dissected (Unknown command/control class 78). I've seen this before too, but am not able to reproduce it myself.. @izmailoff if you can, that would be great. Presumably you tested it with nameko 2.4.2 and the most recent versions of all the downstream dependencies at the time? Was it is a local rabbit broker?\n. I figured this out. The dissector is confused by consumers that are already running when the trace starts. In a longer example, you will see Unknown command/control class messages early in the trace followed by Unknown frame type messages for every basic-deliver to those consumers.\nIf you start the trace before the service(s) everything looks normal.. This is awesome, thanks @izmailoff. Looks good. Only thing to note is that we're actually in the process of removing the exclusivity of reply queues. More detail here: https://github.com/nameko/nameko/issues/359. This issue was moved to https://discourse.nameko.io/t/missing-detailed-docs-on-rpc-implementation-integration-with-other-frameworks/314. Fixed by #376\n. Nameko doesn't pin amqp to a specific version so it should float to whatever is set by kombu, which ought to be compatible. This seems like a packaging problem with kombu's RC.\n. There is some overlap here with https://github.com/clef/flask-nameko. Connection pooling is clearly a use-case we need to support. The performance improvement is very significant!\nWe intentionally made the standalone package independent of eventlet to avoid clashes with whatever concurrency pattern the client is using. \nThis is a nice PR but it might be better as a separate library, as flask-nameko is, unless there's a non-eventlet based pool implementation we can use here. I think that would be the best solution, if possible.\n@davidszotten thoughts?\n. This could be enhanced to instantiate proxies only on demand, no? With that change (and using non-eventlet primitives) this would be an enhancement of nameko's out-of-the-box offering. It would be even better if it pooled connections inside the existing API, rather than adding a new one.\nA new django-nameko package would be cool too, especially if there are Django specific additions to make. That's probably the path of least resistance would be a nice outcome.\n. Great stuff, thanks. Looking forward to the results!\n. @jessepollak there is some overlap here with our recent mailing list thread.\nI think the ideal API here has the following qualities:\n- Is backwards compatible with the existing proxy (i.e. no change in the API, or having to interact with the pool)\n- Connections are pooled and re-used rather than creating a new one for each use\n- The pool supports a minimum number of connections (to be created at startup) and a maximum number (and blocks or raises when exhausted), and others are created on demand.\n- Stale connections are detected and recovered transparently\n. @jessepollak if it can be backwards compatible I think we should consider it an enhancement to the existing ClusterRpcProxy.\nIf you decide to experiment with a different AMQP library as per the mailing list, that would be best as a PoC.\n. This happens when you try to bind a new consumer to an exclusive queue.\nNameko uses exclusive, auto-delete queues in the RPC ReplyListener. Exclusive means a queue can have exactly one consumer. Auto-delete means the queue is removed when its last consumer disconnects. The queue should therefore be removed when the consumer disconnects.\nRabbit takes some time to reap the old queues though. It looks like the PollingQueueConsumer is reconnecting and redeclaring its queue before the old one has had a chance to be removed.\nAre you using the stock standalone RPC proxy here, or some modified version of it?\nThere may be a bug here. Reconnecting to an exclusive queue seems to be asking for trouble. We want reply queues to be private to their consumers, but we also want to tolerate network glitches without losing replies.\nI think the behaviour in the non-standalone case is different because we're using kombu's ConsumerMixin, which probably re-establishes the connection without replacing the consumer, meaning it doesn't violate the exclusivity rule.\n. This is interesting. It's not what I expected.\nI wonder whether kombu is connecting with a new consumer, or simply that the old queue has been reaped by the time the second connection is made.\nDo you have this in a reproducible test case?\n. I've been thinking about this some more and I no longer believe that reply queues should be exclusive. If we dropped this constraint we'd be able to support temporarily disconnecting clients and avoid these error cases after a rabbit failure.\nRelated, if we also dropped auto-delete in preference of RabbitMQ's queue expiry policy, we'd be able to guarantee that clients reconnecting within a grace period won't lose replies (at the moment any reconnection will raise \"disconnected waiting for reply\" errors for all outstanding requests, since we assume the worst case scenario of the reply queue and message being auto-deleted). @kuznero The reply queue name could still be guaranteed-unique and generated by the client (i.e. uuid-based). The difference would be that the client could tolerate a temporary outage and reconnect to that same queue using a different connection.. We experienced this issue today -- exclusive auto-delete queues that should have been automatically removed were not, and could not be manually be removed either. I guess there had been a network partition previously.\nIn our case the service stayed up, but the QueueConsumer for an RpcProxy DependencyProvider kept trying to reconnect. The expected behaviour is for the proxy to redeclare its reply queue, which it can only do once the previous one has been removed. Since the exclusive queue was never reaped by the broker, this \"recoverable\" error become unrecoverable and the service got stuck.\nI don't know if the problem is tied to particular versions of RabbitMQ, but we're seeing it on 3.6.9.\nI think this is more justification for dropping the exclusivity of the reply queue.\n. I intend to pick this up over the Christmas break or shortly thereafter. This has been merged with #504 and is available in the 2.9.0-rc0 pre-release which is on PyPI now.. My thoughts:\nI\u2019m fine with approach (2). I agree that a nice \u201cdefault\u201d and a slightly more involved alternative makes for a good API.\nI also favour supporting defaults on the proxy, in a way that they can be changed with a subclass or at instantiation time. Which is more-or-less what this PR adds.\nAdding this to the standalone RPC proxy, you get an API similar to the final one in #292:\npython\nwith ServiceRpcProxy('service', rabbit_config, timeout=30) as rpc_proxy:\n    ...\nThese two approaches can obviously be combined to create a proxy with a default but also allow options to be set on a per-call basis:\n``` python\nservice case\nclass Service(object):\n     name = \"service\"\n other_rpc = RpcProxy(\"other\", timeout=30)\n\n @rpc\n def method(self):\n     self.other_rpc.remote_method.call(args=args, timeout=15)\n\nstandalone case\nwith ServiceRpcProxy('service', rabbit_config, timeout=30) as proxy:\n    proxy.remote_method(...)\n    proxy.remote_method.call(..., timeout=15)\n```\n@jessepollak I would certainly be open to you expanding this PR to cover these cases. Stylistically, I think it would be nice if the timeout was added as a property that could be overridden by a subclass if the user wanted to. I have used this pattern here.\nOn the \"two sides\" of a timeout:\nI think the client abort and the TTL should be tied together, but only in a limited way.\nA client using a timeout should set that same timeout as a TTL on its request. If the message expires before being consumed, we'll have avoided some unnecessary work. But I think that trying to kill workers that are already running would cause more problems than it solves. The library has no knowledge of what the worker is doing, so it's not possible to stop them safely.\nSimilarly for propagating the TTL to RPC calls made by that worker -- it's not necessarily true that child RPC calls (or events, or any other kind of side-effect) should be aborted if the client stops caring about the result.\nIf aborting workers was really required, it could be implemented with a custom entrypoint that extracted the incoming TTL and allowed the worker to alter its behaviour accordingly. In other words I think this is application-level logic rather than something to be implemented by the framework.\n. Timeouts are still viable and desired. I think both \"client-side\" and TTL-based timeouts would be valuable, and I like the interface that was proposed:\npython\nn.rpc.service.method.call(args=(), kwargs={'foo': 'bar'}, timeout=10, expire=10)\nThere's no way to tell whether a service is available I'm afraid, other than trying to call it.. \ud83d\udc4d  Thanks @jessepollak!\n. Thanks @kooba and @fabiocerqueira!\n. @davidszotten can you give these changes a once-over and make sure they seem fine?\n. I was surprised too -- I thought we already required it.\n. You need to use a config file with the key WEB_SERVER_ADDRESS:\n``` yaml\nconfig.yaml\nAMQP_URI: amqp://guest:guest@localhost\nWEB_SERVER_ADDRESS: 0.0.0.0:8888\n```\nshell\nnameko run --config config.yaml your_service_module\n. There are two config variables related to the RabbitMQ broker. One specifying the AMQP connection, and other specifying the address of the Management interface.\n``` yml\nconfig.yaml\nAMQP_URI: amqp://guest:guest@localhost:5672/vhost\nRABBIT_CTL_URI: http://guest:guest@localhost:15672\n```\nDoes that answer your question?\n. We don't.\nI can't even figure out the API here, so I think it best to drop this code. If something like this feature is useful it could easily (and better) be developed as a standalone package.\n. I think you've found a bug in the WebServer extension.\nThe WebServer spawns a couple of threads with container.spawn_managed_thread. One of them is raising this OSError and, since the server is an Extension and not tied to a particular worker, the exception bubbles up to the container which considers it a fatal error.\nI believe the OSError is raised when a client disconnects very early in the request. The WebServer should probably handle this as an acceptable error and not let it bubble to the container.\nI will try to reproduce and fix it.\n. That is correct. You can reproduce it by refreshing a browser very quickly, or aborting a curl call shortly after executing it. \n. That is a good question. I don't know what other circumstances can result in an OSError here.\nThe thread is exclusively used to handle an incoming request though, so it's probably fine to ignore any OSError and assume that the request will be retried, rather than letting the exception bubble and take down the whole container. @davidszotten do you agree?\n. The problem is with the eventlet WSGI server. You can demonstrate it using Flask with the following, which uses eventlet's own server wrapper for the app:\n``` python\nimport eventlet\neventlet.monkey_patch()\nfrom eventlet import wsgi\nfrom flask import Flask\napp = Flask(name)\n@app.route('/answer')\ndef hello_world():\n    return '42'\nwsgi.server(eventlet.listen(('', 5000)), app)\n```\nThe following will trigger the error and you'll see the traceback in the console:\n``` python\nimport eventlet\neventlet.monkey_patch()\nimport requests\ntry:\n    while True:\n        gt = eventlet.spawn(requests.get, \"http://localhost:5000/answer\")\n        eventlet.sleep(.005)\n        gt.kill()\nexcept KeyboardInterrupt:\n    pass\n```\nThe exception doesn't take down the server process though. The thread running Server.process_request isn't linked at all, so any exception thrown is ignored. See https://github.com/eventlet/eventlet/blob/master/eventlet/wsgi.py#L885\nInterestingly enough, the OSError is only raised on Python 3. I wonder whether that is intentional or amounts to a low-impact bug in eventlet.\n. Awesome. What do you want to do while we're waiting for that fix to land?\nEventlet's own server blanket swallows everything, which may be why this bug went unnoticed. I'm OK string-matching the error for the time being, although I wonder what the best behaviour is here -- should the nameko process die if the recv thread throws an unhandled exception? That is consistent with our behaviour everywhere else.\n. OK great. I've reflected these change in #370. There's one outstanding comment there about faking vs triggering the failure.\n. Just to be clear, I'm happy to land these changes as they are \ud83d\udc4d \n. For a regression test that we don't expect to live very long, I'm not sure it's that helpful\n. Good point. We wouldn't want to do that until the release has proved itself to be stable.\nDo you have something in mind for the replacement test?\n. I forgot to add the new env to the global list\n. This is a pretty regularly requested feature. It's been discussed in https://github.com/nameko/nameko/issues/292 previously.\n337 improves the situation slightly (by standardising the AMQP implementations to some extend) and I have some work in progress on top of that which exposes more message and delivery options. So watch this space.. There is a PR for this now: #456 . Added in #456. Also fixes #355\n. It's also fairly easy to implement this outside of nameko.\nAt Student.com we use a combination of consul, confd and docker. When the docker containers start, confd pulls the latest config and generates the config.yaml file for the service.\n. This issue was moved to https://discourse.nameko.io/t/reading-remote-configuration/315. Nameko 2.4.3 has now been released, which pins kombu back down to a compatible version.\nI'll leave this issue open until we fix the compatibility issue. Thanks for reporting!\n. I agree that we should avoid supporting multiple versions of kombu concurrently, and with the argument that nameko \"owns\" kombu as a dependency. I would be fine upgrading to support kombu 4.0 only after it's shown itself to be reasonably stable.\nHowever, I don't think we really need kombu at all. We only use the AMQP component, and we've had discussions in the past about how kombu restricts us in some ways.\nI wonder how much work it'd be to change the AMQP components to use py-amqp (2.0) or another library directly instead of kombu, at least compared to the work required to upgrade to 4.x.\n. Also I guess there's no great urgency here. If nameko \"owns\" the kombu dependency it's fine for us to keep it pinned down to 3.x for the foreseeable future.\n. I would still like to ditch kombu in favour of a pure AMQP library, and am getting closer to that point in a couple of branches on my fork.\nIn the mean time though, I think we should look at upgrading to kombu 4 and dropping support for older versions. If anyone here has the time and inclination to experiment with that I'd be receptive to a pull request.\n@bweaver-cxp does the change in #524 help you?. Oh interesting. I see this same error in the branch in #524, but hadn't noticed the negative side-effects. I guess there's a bug in Eventlet's green SSL implementation, but it may also be in pyamqp. Kombu 4 jumps to pyamqp 2.0, either fixing or avoiding this problem.\nIf we can identify and work around this bug perhaps the 4.0 upgrade isn't required.\n524 assumes you want to do certificate verification of the client, which I will relax. For now, the AMQP_SSL params need to be:\nAMQP_SSL:\n    ca_certs: <certificate-authority-cert-location>\n    certfile: <client-cert-location>\n    keyfile: <client-key-location>\n. Actually the branch at #524 doesn't require you to provide client certificates. You can specify AMQP_SSL as just:\nAMQP_SSL: True\nThe \"No SSL wrapper\" error can be reproduced by running the following a few times, even without eventlet:\n```\nfrom kombu import Connection\nconn = Connection(\"amqp://guest:guest@localhost:5671/\", ssl=True)\nconn.connect()\nassert conn.connected\nconn.release()\n```\nSo the bug is plainly in pyamqp. I looked for an issue there but haven't found one.. This traceback is fixed in pyamqp 2.0.3 by this commit: https://github.com/celery/py-amqp/commit/7db1b2d4a5cfc546ab941697ae7035ff34645a5a#diff-e3b09014ce12269dff5eece1f9f7c082\nI think that __del__ is sometimes called by the garbage collector after the connection has already been unwrapped. If I comment out the conn.release() I can no longer reproduce the error.\nUnfortunately I think this means it's unlikely to be the cause of whatever is leaving your connections open. I have just backported the fix into #524 so you can try and see?. The incompatibility is with pyamqp 2.x. They are not major, but fixing them required feature sniffing to keep supporting the older version of the library so we didn't.\n2.x has been out for long enough now though that I would be happy to upgrade and drop support for older versions.\nAre you using both Celery and Nameko in the same process? If not, can't you run them from different environments?. Fixed in #564. Closing in favour of #381 \n. The test suite passed for me locally (OS X, Python 3.4.3) but the failures here do seem to be real. I will investigate.. On reflection I think the best resolution for now is to restore the consume override which processes on_iteration before breaking the loop. It's unfortunate that it requires an overridde, but when the QueueConsumer is stopping, switching the order is not a material change.\nI have changed the safety_interval timeout back to the kombu default though, because I think it has the potential to introduce strange networking behaviour.\nI'd like to land this PR as it is so we get a) \"default\" timeout parameters and b) consumer heartbeats. We can assess whether there's a better way to use the ConsumerMixin (or replace it altogether) separately later.. It turns out that xdist requires eventlet 0.20.1 on py3. I don't want to force such a recent release as our oldest supported version just to get faster tests, parking this for now.. The AMQP entrypoints (@rpc, @event_handler and @consume) all guarantee \"at least once\" delivery. This is the case because they acknowledge the message only after it's been processed -- i.e. after the service method exits.\nIf the connection to the broker is lost (if the service crashes, for example, or any other reason) any outstanding \"un-ack'd\" messages go back to being \"ready\", meaning they can be processed by another consumer. This is where \"at least once\" comes from -- the message will be picked up by a second worker, but there's no way to know whether it was successfully processed by the first worker or not.\nMessages are acknowledged when the worker completes, irrespective of whether the worker returned a normal response or raised an exception. The @event_handler and @consume entrypoints have an additional requeue_on_error flag; if this is set, messages resulting in worker exceptions are requeued. This is sometimes useful but can be dangerous if you don't combine it with checks to make sure messages don't end up in an infinite loop.\nRabbitMQ unfortunately does not support the \"max deliveries\" message property, but you can implement something like it yourself.\nThis kind of question is better suited to the mailing list. If you have followup questions please ask them there, and I'll close this issue.. We should probably add this config DependencyProvider to the library, rather than having it reimplemented everywhere. I'll leave this issue open until we do that. . Thanks to @vmikki, this config DependencyProvider is now included in the library (https://github.com/nameko/nameko/pull/395). At Student.com we consul, docker and confd to push configuration loading \"upstream\". \nWe run a container per service, and the container is built with confd and the required templates baked in. The CMD in our Dockerfile looks like:\nCMD confd -onetime -backend consul ...; \\\n    . /appenv/bin/activate; \\\n    nameko run --config config.yaml app.service --backdoor 3000\nconfd fetches the latest configuration from consul and injects them into the template for config.yaml, updating the file if required.\nWhen new config is available we just restart the service container.\n. There are a couple of threads of conversation happening here now. Let me try to summarise:\n\n\nKombu 4.0 requires eventlet 0.2.0, but nameko pins kombu to 3.0.37, so this is irrelevant. #378 is not related.\n\n\nEventlet 0.2.0 bundles dnspython and seems to use it to monkeypatch the socket library's DNS functions. Unfortunately dnspython doesn't have exactly the same behaviour -- in particular, it won't resolve a fully qualified domain name without a . at the end if there's a search domain specified. From what I understand about DNS that's the correct behaviour, but the fact that socket.getaddrinfo behaves differently suggests there's bug is in eventlet's assumption that they're equivalent.. I took it as read from @rgardam's message above. It's not correct -- there's no dependency on eventlet in kombu.. I have raised this as a bug with eventlet https://github.com/eventlet/eventlet/issues/363\n\n\nIn the mean time, the workarounds are:\n\npin eventlet down to <0.2.0 in your application\nresolve using a strictly fully-qualified domain name (i.e. put a . at the end)\n\nFor the moment I think it's too restrictive to pin eventlet down in nameko itself.\n. \ud83d\udc4d  @temoto. With the fix it works as expected.. @alexander-mayr you will always see eventlet in the stack trace for a Nameko service (eventlet runs all the threads)\nYour error is not related to DNS. It looks like Nameko cannot connect to your RabbitMQ broker.. Oh sorry, I missed those 'No address found' errors in the middle there.\nThat is a little weird. I don't understand why you'd see both ConnectionRefusedError and No address found in the same stacktrace (Did it resolve an IP address and fail to connect to it, or fail to resolve the name? They seem mutually exclusive).\nNot ruling this out being the same issue. If you can debug further that would be helpful.\n. You have to register it as a serializer with kombu. See https://github.com/nameko/nameko/blob/master/test/test_serialization.py#L176-L209 for an example.. This issue was moved to https://discourse.nameko.io/t/how-to-add-a-custom-serializer/317. Awesome, thanks. +1 for making the order more predictable too.. Good shout. I'll add this in the RC branch. Hi @juliotrigo,\nThis turns out to have some overlap with https://github.com/nameko/nameko/pull/400, which exposes the keyword arguments to publisher.publish to the Publisher DependencyProvider constructor.\nIf it's OK with you I'd like to cherry pick some of these changes (particularly the backwards compatibility shim and docs) and put it on top of #400.. I made a couple of commits against #400 in a separate branch -- https://github.com/mattbennett/nameko/pull/7. Would you mind reviewing it?\nI would like to wait for #400 to be reviewed before merging https://github.com/mattbennett/nameko/pull/7 though, since it's a related but slightly different concern.. This is now supported by the declare kwarg that you can pass to the Publisher constructor or Publisher.publish.\nOne difference between this implementation and the existing one is that the declare kwarg declares queues lazily on publish. I think this is sufficient -- there's no point in a queue existing if before it's either published to or consumed from (the consumer also declares queues). If the queue declaration is needed at startup publisher for some reason, you could subclass the Publisher and customise the setup() method.. Having started on the \"next step\" mentioned in the comment above, it might make sense to hold off the review here, other than a glance to get the feel of it.. Closing in favour of #456. I'm not convinced that coveralls adds much. I may try codecov.io instead eventually but will close this for now.. I guess you figured this out already, but connections are pooled by kombu.\nThe API to the standalone dispatcher isn't wonderful, particularly having to pass in the \"nameko config\" object -- because actually all it needs is the AMQP URI and the serializer to use. I have a WIP branch that cleans up the AMQP publishers to some extent, so hopefully this will change soon.. Can you please post the output of pip freeze?. I think the problem is that you have librabbitmq installed, which doesn't support basic_return.\nIf librabbitmq is installed, kombu will use it in preference to py-amqp for any URIs beginning with amqp://. Nameko isn't compatible with librabbitmq. https://github.com/nameko/nameko/pull/361 updated most URIs to use the more explicit pyamqp scheme, but I guess we missed some usages in the CLI.\nYou can verify that librabbitmq is causing the problem by adding --broker pyamqp://<rabbitmq-uri>to your nameko run and nameko shell commands.. You're welcome. #404 updates all the defaults to avoid this happening again. . Superseded by #412. Thanks for raising this.\nOne of the requirements for the \"standalone\" features is that they should be usable without having to run the eventlet event loop -- so they can be used by clients with other concurrency models. Unfortunately this implementation needs eventlet.\nWhat does this give you over https://github.com/clef/flask-nameko? I guess there's a guarantee of exactly one connection, rather than using the pool?. @scarchik, sorry to leave this PR hanging.\nI understand the advantage of using this over flask-nameko now. As I mentioned, the \"standalone\" package is for things that will work with a vanilla interpreter, so I don't want to add the MultiQueueConsumer there.\nDo you want to raise a smaller PR that just allows the reply_listener_cls to be specified, so you can use this in your own application without also having to subclass StandaloneProxyBase?. Great, thank you. Your branch is failing the linter check at the moment with lines >79 chars.\nI think it could be useful as a PyPI package, for people who're using eventlet-based clients to call nameko services.\n. Your requests.get call is being made at class definition time. Service classes in Nameko used to define a service, and there's quite a lot of magic that happens between a service being defined and it actually running.\nWhat are you trying to achieve here? When did you expect your request to be made? . Yes, the docs are sparse. Sorry about that. I plan to give them some more attention soon. In the mean time I will try to explain here.\nPretty much every time you want your service to talk to an external entity it should use a dependency provider, because it makes testing easier. In this case, you also need to be able to do something outside of a worker.\nRemember that a worker only exists for the duration of a method call -- that is, one will be created when you call the RPC method, and it'll be destroyed again afterwards -- and the \"dependency\" that it sees is whatever the dependency provider returned from its get_dependency method.\nThe dependency provider lives as long as the service is running. You can write one that creates a background thread that fetches your data periodically, so it's available whenever the RPC method fires.\nHere's a quick example:\n```python\nimport time\nimport requests\nfrom nameko.extensions import DependencyProvider\nfrom nameko.rpc import rpc\nclass Collector(DependencyProvider):\nlatest_data = None\n\ndef run(self):\n    # fetch latest data every 60 seconds\n    while True:\n        self.latest_data = requests.get(\"https://example.com/data\")\n        time.sleep(60)\n\ndef start(self):\n    # ask the container to run a thread for you\n    self.container.spawn_managed_thread(self.run)\n\ndef get_dependency(self, worker_ctx):\n    # return the data\n    return self.latest_data\n\nclass Service:\n    name = \"service\"\ndata = Collector()\n\n@rpc\ndef test(self):\n    # self.data is now a reference to the collector instance's \"latest_data\" attr\n    return get_current_mean(self.data)\n\n```\nA better dependency provider may use the \"wrapper\" pattern (example here)  and return something with a get_current_mean method rather than the raw data (or indeed precalculates the mean), but hopefully you see what I mean.. Glad you're enjoying Nameko :)\nThe linting errors are an unfortunate side-effect of dependency injection.\nIn an ideal world you'd be able to tell pylint that the attribute is actually an instance of a different type, but I don't know how to do that.\nYou can ignore specific lines though -- just add # pylint: disable=no-member or # pylint: disable=E1101. You can also disable the rule globally by running run pylint with the --disable=no-member flag, or via a pylintrc file.\n. This issue was moved to https://discourse.nameko.io/t/does-nameko-support-load-balancing/313. You're doing something wrong, but I can't tell what from what you've posted.\nHow are you running the service? And how are you calling it?\n. @mikegreen7892003 @Ryannnnnnn thanks for the debugging!\nPrefetch count should definitely be configurable. It was a mistake to tie it to max_workers because they're really distinct concepts -- for example, you may want to limit AMQP consumption without blocking other types of entrypoints (e.g. http).\nUnfortunately it's not trivial to make prefetch_count configurable in the current implementation. You can do it with an override of the QueueConsumer, but you have to also override the RpcConsumer and Rpc entrypoint too:\n` python\nclass QueueConsumer(NamekoQueueConsumer):\n    \"\"\" Subclass of :class::`nameko.messaging.QueueConsumer` so we can\n        customise theprefetch_count`` value.\n    \"\"\"\n# override the property on nameko's QueueConsumer so that we can\n# write to the attribute in setup()\nprefetch_count = None\n\ndef __init__(self, prefetch_count):\n    self.custom_prefetch_count = prefetch_count\n    super().__init__()\n\ndef setup(self):\n    super().setup()\n    self.prefetch_count = self.custom_prefetch_count\n\n@property\ndef sharing_key(self):\n    return \"{}-{}\".format(type(self), self.custom_prefetch_count)\n\nclass RpcConsumer(NamekoRpcConsumer):\n      queue_consumer = QueueConsumer(prefetch_count=...)\nclass Rpc(NamekoRpc):\n     rpc_consumer = RpcConsumer()\n```\nI'd suggest not setting prefetch count to self.container.max_workers * 2 unless max_workers is exactly one. Otherwise you risk consuming messages from the queue and having them block waiting for a worker to become available.\nI've not made any attempt to fix the current implementation because I'd like to refactor away from the QueueConsumer altogether. There are a number of problems with the design, which you can read about here: https://groups.google.com/forum/#!topic/nameko-dev/WZ-xRYd2-vw\nFor your specific case of restricting to a single thread, I'd set max_workers (and prefetch_count) to something >1 and put a lock/semaphore around the work you want to serialise.\n. @davidszotten it's not exactly a regression in 2.4.4 -- the change there just makes the problem more obvious. This behaviour has been present forever, and it's intrinsic in the way the kombu consumer works.\nExample using Nameko 2.4.0:\n``` python\nservice.py\nfrom nameko.rpc import rpc\nimport logging\nclass Consumer(object):\n    name = 'test'\n    @rpc\n    def work(self):\n        logging.info('hello')\n        return 42\n```\n``` yaml\nconfig.yaml\nAMQP_URI: \"pyamqp://guest:guest@localhost:5672/\"\nmax_workers: 1\nLOGGING:\n    version: 1\n    disable_existing_loggers: false\n    formatters:\n        simple:\n            format: \"%(asctime)s: %(message)s\"\n    handlers:\n        console:\n            class: logging.StreamHandler\n            formatter: simple\n    root:\n        level: INFO\n        handlers: [console]\n```\n(tmp-1ce58f0405f7905b)Matts-13-inch-Macbook-Pro:tmp mattbennett$ nameko run service --config config.yaml\n2017-03-16 11:07:07,832: starting services: test\n2017-03-16 11:07:07,858: Connected to amqp://guest:**@127.0.0.1:5672//\n2017-03-16 11:07:07,871: hello\n2017-03-16 11:07:07,978: hello\n2017-03-16 11:07:08,081: hello\n2017-03-16 11:07:08,183: hello\n2017-03-16 11:07:08,288: hello\n2017-03-16 11:07:08,392: hello\n2017-03-16 11:07:08,497: hello\n2017-03-16 11:07:08,601: hello\n2017-03-16 11:07:08,713: hello\n2017-03-16 11:07:08,816: hello\n2017-03-16 11:07:08,920: hello\n2017-03-16 11:07:09,025: hello\n2017-03-16 11:07:09,129: hello\nI'll do a similar test with the kombu consumer on its own, but I think @mikegreen7892003 is right -- it's limited to max_workers / safety_interval messages per second.. @davidszotten I agree that we need to do something in the short term.\nI like @mikegreen7892003's suggestion of setting prefetch_count = max_workers + 1. It breaks the deadlock sufficiently but leaves at most one message blocked on the worker pool.\nThe same example I ran earlier with that implemented (max_workers = 1, prefetch_count = 2):\n2017-03-16 13:24:24,224: starting services: test\n2017-03-16 13:24:24,246: Connected to amqp://guest:**@127.0.0.1:5672//\n2017-03-16 13:24:24,269: hello\n2017-03-16 13:24:24,289: hello\n2017-03-16 13:24:24,292: hello\n2017-03-16 13:24:24,294: hello\n2017-03-16 13:24:24,296: hello\n2017-03-16 13:24:24,298: hello\n2017-03-16 13:24:24,301: hello\n2017-03-16 13:24:24,303: hello\n2017-03-16 13:24:24,305: hello\n2017-03-16 13:24:24,308: hello\nAnd with 1000 requests and default values (max_workers = 10, prefetch_count = 11):\n2017-03-16 13:35:37,076: starting services: test\n2017-03-16 13:35:37,095: Connected to amqp://guest:**@127.0.0.1:5672//\n2017-03-16 13:35:37,107: hello\n<snip>\n2017-03-16 13:35:39,379: hello\n(~500 reqs/sec.)\n. This has been discussed before but nothing has landed in the main repo. Check this thread on the mailing list -- https://groups.google.com/forum/#!searchin/nameko-dev/reload%7Csort:relevance/nameko-dev/aNAs2cHBJME/62QS1G3cAQAJ\nIt would be a welcome addition!. Sorry to leave this without a reply for so long.\n0.17.2 is two years old, and was actually only released 3 months after 0.16.1. So I'm happy to drop support for 0.16. @davidszotten do you agree? If so, yes please change the requirement in this PR @Trex-Boolat  (it's actually in tox.ini and only needs changing for python 2.7)\n. I'll cut a new release as soon as possible.. I've not seen this before, but we're not running on Python 3.6. Actually I'm pretty sure nameko's test suite does not pass against 3.6.. I think the snippet you have here is a perfectly good solution, and it's better for it to live in your application (tailored to your particular use-case) than to become part of the framework.. Nameko is now compatible with more modern versions of eventlet in which this bug has been fixed. FYI I plan to release this a 2.6.0 once the hotfixes in 2.5.4 are out, so the 2.5 series ends in a stable place.. Thanks @zhouxiaoxiang for the contribution!\nWe added dict-access for services because they can contain characters that can't be used in python identifiers (e.g. \"my-service\"). That isn't case for method names because they also need to exist as methods on the service class.\nWhy do you find this approach more convenient?. Hi Conor,\nSorry to hear about this! We've also struggled with hung services occasionally at Student.com, but the culprit always seemed to be RPC proxies waiting for replies (usually due to a downstream service that was overloaded). That blinded us to this underlying problem, which has existed in some form for a long time but is made much worse by the 2.5.3 release. I've tried to capture the various failure cases in the tests in the PR.\nWhich version of Nameko are your services running?\nAs I said above, I'm coming to the conclusion that max_workers is an unhelpful feature, at least for entrypoints where you can otherwise constrict the amount of concurrency (e.g. the AMQP entrypoints, although the implementation there is far from perfect too). It's probably sensible to set it to a very high number for now, unless you have a particular reason to throttle a service.\nThe fix is coming soon, and it's embarrassingly simple \ud83d\ude33 . Being on 2.5.3 explains why you're suddenly experiencing them more often.\nSorry, my advice to jack up the worker count was a little cavalier. You'll separately need to set the prefetch_count to something sensible, otherwise (as you pointed out) the AMQP entrypoints will try to consume that many messages in one go.\nIt is a bit of a faff but you can override the prefetch count by subclassing the QueueConsumer, and the chain of Entrypoints / Extensions that use it:\n``` python\nlimited.py\nfrom nameko.messaging import QueueConsumer as NamekoQueueConsumer\nfrom nameko.rpc import Rpc as NamekoRpc\nfrom nameko.rpc import RpcConsumer as NamekoRpcConsumer\nclass QueueConsumer(NamekoQueueConsumer):\n    prefetch_count = 10\nclass RpcConsumer(NamekoRpcConsumer):\n    queue_consumer = QueueConsumer()\nclass Rpc(NamekoRpc):\n    rpc_consumer = RpcConsumer()\nrpc = Rpc.decorator\nclass Service:\n    name = \"your-service\"\n@rpc\ndef method(self):\n    pass\n\n```\nYou'd need to do the same for any consume and event_handler entrypoints too.\nRun this with the following config\n``` yaml\nconfig.yaml\nmax_workers: 1000\nAMQP_URI: amqp://guest:guest@localhost:5672/\n```\nAnd you'll see the RPC consumer is limited to 10 but has a pool size of 1000:\n$ nameko run limited --config config.yaml --backdoor 3000\nstarting services: your-service\nbackdoor server listening on 127.0.0.1:3000\nConnected to amqp://guest:**@127.0.0.1:5672//\n```\n$ nameko backdoor 3000\nPython 2.7.10 (default, Jul  6 2015, 15:19:48)\n[GCC 4.2.1 Compatible Apple LLVM 6.1.0 (clang-602.0.53)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(InteractiveConsole)\n\n\n\nct = list(runner.containers)[0]\nct._worker_pool.size\n1000\n```\n. Fixed by https://github.com/nameko/nameko/pull/431 and released in 2.5.4. Unfortunately this change undoes the performance workaround added in #418.\n\n\n\nThe test is flakey. It passes reliably for me locally, but fails reliably here on Travis. It seems to be a problem of timing based on network traffic to and from rabbit. You can sometimes end up waiting for a timeout on the socket before being able to ack, and it seems once it happens everything falls into lock-step. \nNow the QueueConsumer delegates handling the message to a new thread, we end up in this situation much more frequently in this particular test (on linux at least).\nI guess the fix in #418 is also unreliable, but we didn't happen to hit a failure condition in the (very limited) test. In reality, services that are susceptible probably fall back into prefetch_count / safety_interval requests/second eventually, even with this fix. Most real services probably aren't susceptible because they're handling traffic for more than one entrypoint, which will break the \"timeout\" lock-step again.\nThere's not much good news here. We know there are a couple of deep bugs with the current implementation, which have been around for a long time but are made worse by recent releases:\n\n\nServices handling lots of tiny requests can end up with throughput limited to prefetch_count / safety_interval requests per second. Nameko 2.4.4 increased the safety interval to a second, so makes this much more obvious. Nameko 2.5.3 contains a quick workaround, which I suspect doesn't work in all scenarios.\n\n\nServices ending up with more unack'd messages than their worker pool will block their QueueConsumer, which can result in deadlock and/or disconnections. Prior to Nameko 2.5.3 this was only possible with the unack'd messages spread across multiple AMQP queues. Since 2.5.3 it will happen whenever the number messages on a queue exceed the worker pool size, because the prefetch_count is always more than max_workers.\n\n\nI think the changes in this PR need applying as soon as possible. We should perhaps also revert the prefetch count change from 2.5.3, since I'm unconvinced that it works as intended.\nThe performance problem still exists. To mitigate, we could change the safety_internal back to 0.1 again, but this doesn't actually solve the problem. It also negates the reason I changed it in the first place, which was to (possibly) fix the connection flakiness we were seeing.\nI think my preference in the short term is to reduce the safety internal again in this PR, and revert 2.5.3's change. And then we need to look more urgently at replacing the QueueConsumer.\n. When 2.5.3 was released I thought the performance restriction came from the fact that the queue consumer has having to block for the worker pool, and that the extra prefetch would allow it to ack before the worker was complete.\nIt turns out that this is not what's happening at all, which is obvious now the queueconsumer thread spawns a thread to wait on the pool. The performance becomes slow when the network traffic to rabbit is such that the queueconsumer has to wait for a socket timeout before being able to ack previous messages. The relative size of the worker pool is unrelated, so my conclusion is that bumping the prefetch worked by accident. I will try to prove this.\nI can reproduce the test_prefetch_throughput failure locally on linux, and the difference between it and OS X is just that in the failing case we end up timing out on the socket on every iteration. I said \"once it happens everything falls into lock-step\" because I've also observed a test running without timeouts but then falling into them, at which point the performance drops to prefetch_count / safety_interval reqs/s.\nI've also seen this test pass on Travis, but only once or twice, which further suggests that it's an issue of timing.. The prefetch_count = max_workers + 1 fix works because it guarantees that when a worker finishes, there's another thread waiting to run (blocked on the worker pool). So you never end up with all the activity happening at once, then having to wait for a timeout on the socket.\nThat means the fix does always work, but only if you're prepared to block the queue consumer thread on the worker pool.\n. Tests seem to be failing on a hanging toxiproxy server (https://github.com/Shopify/toxiproxy/issues/159).\nI think lowering the safety_interval has changed the timing sufficiently to now reliably hit the hanging scenario. I can reproduce it locally (on linux only) and increasing the safety_interval makes it more stable.. Having said in #428 that this was a \"simple\" fix, it's turning out to be quite difficult to actually land.\nI'm not happy with these changes. I'm convinced that main change here is correct (not blocking the queueconsumer on the worker pool) but the fallout is bad because we re-expose the performance problem stemming from acking messages inside the queue consumer loop.\nTrying to fix the performance problem I stumbled across this stack overflow question (which, ironically enough, I'd previously commented on and then forgotten about).\nIt turns out that you can safely ack messages from any thread, and doing so completely removes the performance problem. I have a branch in progress that does this: https://github.com/mattbennett/nameko/compare/master...mattbennett:simplify-queue-consumer?expand=1. Unfortunately there is test-fallout to fix here too, because it has a substantial impact on timing and thread execution order, and our tests are too delicate.\n . The \"branch in progress\" mentioned above is now ready for review in #430.\nI've implemented the salient changes from this PR on top of it in #431, so closing this in preference.. The best I've found is tox-travis.\nI actually have a branch where I was playing with more python versions -- https://github.com/nameko/nameko/compare/master...mattbennett:more-pythons\nIt's more complex to dynamically build the matrix, but it does avoid having to list out every possible combination as a separate TOX_ENV entry.. Actually now that I think about it I'm remembering things...\nThe TOX_ENV approach doesn't work on Travis if you want to test on anything other than python 2.7, 3.3 or 3.4. Those are the only Pythons available without specifying them in the python section of the Travis config file.\nOnce you have multiple entries in the python section, Travis will run every provided environment on each one, so you get enormous duplication if you try to use the environment to specify the Python version.\n. Is this preferable to using tox-travis?. Nameko makes a conscious choice to use dependency injection rather than global or thread-local variables. Dependency injection is chosen because it makes testing easier, and many of the bundled test helpers rely on this pattern.\nThe \"nameko way\" of achieving your use-case would be to define a DependencyProvider that performed the logging. For an example, see this mailing list thread.\nIt's also viable to expose different parts of the worker_ctx directly to workers if you need to (as in contextdata.py), but it should always be done via a DependencyProvider.. I understand the desire now. In this case, I would implement a DependencyProvider that pulled the current call_id out and stashed it in a thread-local variable during worker_setup. Your logging filter could then just read the call id from there, and there'd be no need for internal changes to Nameko.\nAlso, note we use a \"correlation_id\" as an internal part of the RPC protocol. You might want to change your naming convention to reflect that what you're capturing is really the Nameko call_id.. Using a custom WorkerContext class has been deprecated since Nameko 2.4.0. I don't really want to bring it back.\nThe use of a WorkerContext object in the standalone RPC proxy is strange anyway. It only exists to satisfy the \"fake\" ContainerFactory that is used there, which in turn only exists to help re-use code from the non-standalone RPC implementation.\nYou pass the context_data into the proxy when you instantiate it. Unfortunately there's no way to specify it per-call. You could implement a context manager that does it though:\n``` python\ndef request_context(proxy):\n    # modify the context data\n    proxy._worker_ctx.context_data['request_id'] =  request.request_id\n    yield\n    # undo the modification\n    del proxy._worker_ctx.context_data['request_id']\nrpc_proxy = ClusterRpcProxy(config)\nwith request_context(rpc_proxy):\n    rpc_proxy.foo.bar()  # use proxy\n```\nIf the context manager is too messy, you could subclass the ClusterRpcProxy to always insert the request context.\nYou may also want to look at https://github.com/clef/flask-nameko, which might be nicer and/or easier to modify to your requirements.. I'm not a flask expert, but isn't the request object a thread-local? In that case, even a globally shared proxy object would be able to access the request object for a given execution. In that case, I would expect the request_context above to work.\nThe issue I have with overriding the WorkerContext in this case is that I don't like having to specify a worker context to a standalone RPC proxy at all. It is only used to extract the headers for the outbound message, and there is a better way to do that. The current design is a lazy way to reuse code in the MethodProxy.\nWe deprecated custom WorkerContext classes because you can also specify custom ServiceContainer classes, which should be enough for most use-cases, since the service container instantiates the worker context in most cases anyway. The exception is the RPC proxy, which is a mis-design.\nI think it's fine for you to continue to solve the problem for \"you only\" for the time being. A better general design for the RPC proxy would allow you to easily set context data at proxy call time, not just definition time.. I'm pretty sure this is backwards compatible. Custom entrypoints may need updating to support the API if they don't accept **kwargs and/or call super().__init__().\nGood call on the docs. Will add a page \ud83d\udc4d. @davidszotten docs added.\nA little more than I anticipated, but I had to add a long overdue thing about writing entrypoints before I could talk about the arguments they accept.\n. @davidszotten this is also ready for another review when you have time.. There are a couple of flakey tests in master. I restarted this job and it passed this time.\nIt would be nice if there was a unit test verifying the banner. Do you want to try adding one? If not, I'm happy to add one on top of this work. This is tricky because pytest is both a test requirement (therefore pinned) and a library requirement which we allow to float. For these dependencies we don't actually test  oldest, pinned and latest versions like we do for all the other library requirements. Perhaps we should, since people are obviously using modern pytest with their nameko applications.\nUnfortunately Nameko's test suite doesn't support pytest>2.7.3 because of some coverage collection issues. Even if it did though, we'd either need to merge the coverage runs or exclude the lines that don't apply to specific versions.\nI think the pragmatic fix here is to try to use getfixturevalue, fall back to getfuncargvalue, and add a pragma: no cover to the fallback path.. Thanks @geoffjukes for the contribution! Glad you're enjoying Nameko and managed to solve your problem in #448.\nIn general I prefer not to have two ways of doing things. We added dict-access for services because they can contain characters that can't be used in python identifiers (e.g. \"my-service\"). That isn't case for method names because they also need to exist as methods on the service class.\nSince Python already provides the getattr helper I'd rather not add this extra method to the Nameko codebase.\n. Thank you @ornoone!. It is certainly possible to implement a set of extensions that use ZeroMQ as the transport. I imagine RPC and Events would translate quite well.\nThe basic requirements for implementing an entrypoint are:\n\nInherit from nameko.extensions.Entrypoint\nListen to some socket, queue or other source of messages\nCall container.spawn_worker when an appropriate message is received\n\nA nice and simple example of an custom entrypoint is https://github.com/iky/nameko-slack. For a more complex example, check https://github.com/Overseas-Student-Living/nameko-bayeux-client\nImplementing a dependency provider (e.g. a ZeroMQ-backed RPC proxy) is even easier:\n\nInherit from nameko.extensions.DependencyProvider\nImplement the get_dependency() method to return an appropriate interface for the service to use the downstream dependency.\n\nA simple example of a custom dependency provider is https://github.com/onefinestay/nameko-sqlalchemy.\n. This issue was moved to https://discourse.nameko.io/t/zeromq-support/303. There is one for Node: https://github.com/and3rson/node-nameko-client. This issue was moved to https://discourse.nameko.io/t/is-there-other-languages-nameko-rpc-clients/316. @davidszotten I think I've addressed all the comments here. History also rewritten to remove the cagoule file.\nI've implemented the TODO on top of this but left it out of this branch. PR of the diff here for easier review: https://github.com/mattbennett/nameko/pull/8.. This is a very welcome feature! Thank you for raising a PR.\nI have a few thoughts --\nThere's a lot of code here for watching and tracking file changes. Is is possible to outsource some of that to a library? Watchdog comes to mind. @davidszotten has used it as a wrapper before, but I want this feature to become part of Nameko out of the box.\nI'm not convinced by the value of a second config file for logging. I think it can just as easily go in the normal config file and then won't need any special treatment.\nFinally, I think we use an --auto-reload flag instead of / as well as the config option to specify auto-reload would be helpful. That seems to be the \"normal\" way of enabling auto-reload in similar libraries, and makes it very easy to do in simple cases that might not have a config file yet.. My resistance to adding the logging config file option is only that there's already a way to configure logging via the normal config file.\nWhat's the motivation for using a logging config file rather than the same information in a yaml file that can also contain the rest of the service configuration? . OK, let's abandon the werkzeug reloader. \nOne more possibility is https://github.com/Pylons/hupper. It looks like it's process based so eventlet compatibility should be fine (which is probably where watchdog fell down).\nEvery framework out there seems to have implemented their own reloader in the past. Hupper is itself a standardisation of other approaches. If we can leverage someone else's solution rather than rolling our own I think it'd be worth it.\nFor the logging config file -- the motivation is clear now, thanks. It's probably worth splitting that change out into a separate PR though, so each change can be discussed and landed independently.. Oh interesting, thanks. I had assumed that kombu serialized them using whatever encoder is configured, but I guess not. A PR would be great!\nActually the warning about None values is kinda unhelpful because it tends to generate a lot of logs. Something that warned once would probably be good (using the warnings module) or otherwise just dropping them silently.. Hi Geoff,\nThere's nothing out of the box that provides the kind of \"task distribution\" that Celery does I'm afraid, but there are plenty of lower-level tools to implement something like it.\nTo make sure I understand the request here, you want to kick off some potentially long-running request, and then later from a different worker check whether it's completed yet and/or fetch the result?\nTo implement this you'd need somewhere to store the shared state and a way of serializing the task results in and out of it. The storage would contain some kind of empty or status value for each \"in-flight\" request, and the results for (recently) completed tasks. If the storage was shared you could have the task executors write their status/results to it, otherwise that could happen when the result was returned to the \"master\" task.. This issue was moved to https://discourse.nameko.io/t/call-async-result/309. Gevent is not a drop-in replacement for eventlet unfortunately, and we make heavy use of some eventlet features that have no equivalent, such as the Event object. An abstracted concurrency interface is a nice idea in theory, but it would take some work to implement.\nUnfortunately I don't think there's a lot that can be done to stitch two libraries that are bound to different event loops together. For a quick win I think your best bet is to put your gevent dependency in a separate process and give it a network interface that your nameko service can treat as the dependency.. This issue was moved to https://discourse.nameko.io/t/support-for-gevent/305. Fixed by https://github.com/nameko/nameko/pull/462. Thanks!. @bobh66 thanks for the PR!\nThis looks good to me, although I can't remember why we protect against a stopped PollingQueueConsumer. @davidszotten do you recall?. @davidszotten perhaps that was the reasoning. Perhaps the consumer setup logic was different back then too, and restarting was not an option?\nI don't think it matters really, unless you can think of a scenario where this won't work? The test @bobh66 has added suggests it does!. You have to leave and reenter the context manager though, which usually implies some kind of work.. @remote_error('<service_name>', '<exc_type>') seems straightforward and nicely namespaced to me.\nThis decorator could also be used for non-RPC request-response type entrypoints so I'm not sure the rpc package is the right place for it. Perhaps a top-level serialization package? The safe_for_serialization helper that currently lives in nameko.exceptions also needs a new home. . @davidszotten there isn't anything specific that already exists, but it's easy to imagine a request-response type entrypoint that might want to map remote errors to specific exceptions in the client.\nWe do pass exception args, so I think this approach assumes the signatures are compatible (which should probably be tested/documented here, if we don't change the implementation). Having a common superclass and (presumably) guaranteeing that the remote error re-hydrates successfully sounds nice.. Hi Geoff,\nThanks \ud83d\ude00 \nYour code is valid, but perhaps isn't the best way of achieving your goal. Events are intended to be dispatched from an originating service, and handled in any \"interested\" subscribing services. The standalone dispatcher is a helper for \"faking\" that behaviour when needed (e.g. in tests) or to originate events from non-nameko services.\nThe code above will send a \"ping\" event claiming to originate from a \"monitor\" service. It is valid, but breaks the (implicit and undocumented!) intention that events are sourced at real services.\nAssuming the aim is to get all services to do something when the \"status\" command is issued in slack, I would do it by dropping down to the lower-level \"messaging\" extensions that events are built on top of.\nNameko events are little more than a particular configuration of underlying elements of AMQP (the exchanges, queues, bindings, routing keys and consumers). For your use-case you probably want to configure these elements slightly differently.\nI guess you want a single exchange that all services bind a queue to, and a consumer in each service instance. This is basically what you get with broadcast events, but doing it yourself you can control the exchange name, routing keys and queue names.\nSomething like this:\n``` python\nslackbot\nfrom nameko.messaging import Publisher\nfrom kombu.messaging import Exchange\nmonitoring_exchange = Exchange(name=\"monitoring\")\nclass Slack:\n    name = 'slackbot'\n    config = Config()\npublish = Publisher(exchange=monitoring_exchange)\n\n@rtm.handle_message('status')\ndef on_status(self, event, message):\n    self.publish(routing_key=\"ping\", event['channel'])\n\n```\n``` python\nservice_x.py\nfrom nameko.messaging import consume\nfrom kombu.messaging import Exchange, Queue\nmonitoring_exchange = Exchange(name=\"monitoring\")\nping_queue = Queue(name=\"\", routing_key=\"ping\", exchange=\"monitoring\")\nclass Service:\n    name = \"x\"\n@consume(queue=ping_queue)\ndef handle_ping(self, payload):\n    pass\n\n. This issue was moved to https://discourse.nameko.io/t/standalone-event-dispatcher/310. This will create conflicts in the open PRs, although running\nAUTO_FIX_IMPORTS=1 make imports \n```\nshould get something back to a state that git can auto-merge.\nIf you're happy with https://github.com/nameko/nameko/pull/440 and https://github.com/nameko/nameko/pull/465 it might be best to land those first though?\n. Hi Geoff,\nAsking questions is good :) Although we do have a mailing list that is more appropriate for these kind of things than a Github issue.\nI will try to answer these questions here though.\n\nIs it considered bad practice to call a method defined in an RPC service, directly?\nHow 'normal' is a class that defines an RPC task?\n\nOne of Nameko's principles is that entrypoint decorators do not mutate the underlying methods, so it's not bad practice at all. When one service method calls another, there's literally no difference between the target method being entrypoint-decorated or not.\nIn many ways a service that defines an RPC task is just a normal class with one of the methods decorated with @rpc. In some ways it's not though ;) One of the biggest is that you don't control construction, so  in your example you'd have no opportunity to pass filepath parameter to FileProcess.\nAnother is that workers are stateless. Every worker gets a new instance of the service class, so you can't write something to self in one worker and expect it to be visible to another worker.  This is best illustrated with an example (of what not to do):\n``` python\nthis won't work\nclass Service:\n    name = 'serv'\nstate = \"empty\"\n\n@rpc\ndef intro(self, name):\n     self.state = name\n\n@rpc\ndef greet(self):\n    return \"Hello {}\".format(self.state)\n\n\n$ nameko shell\n\n\n\nn.rpc.serv.intro(\"geoff\")\nn.rpc.serv.greet()\n.... \"Hello empty\"\n```\n\n\n\nIf you need to store state between workers, you should use a DependencyProvider.\nI hope that helps!. One other thing to note is that inheritance works just fine. If you have some common logic it's a good idea to use Python's powerful inheritance system to include it in your service classes.. This issue was moved to https://discourse.nameko.io/t/sharing-methods/308. Unfortunately these are now failing because Travis has updated their default linux image, which changes the RabbitMQ version.\nThe changes in https://github.com/rabbitmq/rabbitmq-management/issues/236 (released in 3.6.7) seem to have added a lag to the API so a bunch more tests fail. We could add retries and timeouts, but it will slow down our test suite. I think there are better ways to write the tests that are failing.\nIn the mean time I will try to get Travis to run with the old images, and we can cherry-pick that change into the current PRs.\n. @davidszotten this is ready for another round when you have a chance.. Caught red-handed :-s\nThere is a second simultaneous read error being thrown in the websocket test_client_closing_connection on py35-oldest-lib, which must have been there for a while. I haven't had chance to investigate further, and would like to get this tranche of flakey fixes out first (and the fix for #359, which is sitting on top of it). I assume so, although not really sure. We could easily run both 2x and 3x in parallel.. Looks like it does make a difference (https://stackoverflow.com/questions/23052637/specify-which-python-version-pylint-should-evaluate-for) so I've added another step to the static build stage. This issue was moved to https://discourse.nameko.io/t/about-wsgiapp-with-cors/311. Kombu (the library we use to send and receive AMQP messages) only supports heartbeats for consumer connections. Publishers use a pool of connections managed by Kombu, and at least within Nameko there's no mechanism to regularly send the heartbeats down these connections (unlike the consumers, which have a thread each).\nYou can probably improve your memory consumption by limiting the size of the pools.\nBy adding the heartbeat parameter to get_producer you're probably configuring the underlying connection with a heartbeat, but never actually sending the beat from the client side. The server will close the connection after two heats are missed, which is probably why you get your memory back. It's very inefficient though, since every connection lives for a maximum of 2 * heartbeat seconds.\nUltimately I'd love to support heartbeats for publisher connections, but we'd need to replace kombu with a better AMQP library to do so.. @epetrovich the ultimate solution is to replace kombu, which actually in progress now. No promises on when it will land though.\nIn the mean time, you shouldn't see that many dead connections assuming you're using the kombu pools (which nameko does most of the time). What is \"a lot\" and what problems occur as a result?. @alexeysofin seems like a reasonable idea. A bit of a hack perhaps, but it will probably work.\nIf you have a subclass of nameko.amqp.publish.Publisher you can make the AMQP extensions use it with another relatively light subclass, specifying the publisher_cls attribute:\nhttps://github.com/nameko/nameko/blob/b33a36197cc3d819360317f213f10afd0901b589/nameko/messaging.py#L74-L78\nRe: progress on removing kombu. Thus far I've concentrated on simplifying the AMQP extensions. The most recent branch is https://github.com/mattbennett/nameko/tree/remove-queue-consumer.\n@epetrovich I see. I think @alexeysofin's suggestion will help the broker prune the connections sooner than it would without a heartbeat.. This issue was moved to https://discourse.nameko.io/t/why-heartbeat-param-not-in-amqp-get-producer/306. Hey Shaun,\nIt's very unlikely this would work. We used Kombu because it seemed like the best AMQP library at the time, rather than to be transport agnostic. We rely on certain bits of AMQP and even make use of certain RabbitMQ extensions to the spec.\nOne of my longstanding desires is actually to remove kombu for one of the newer AMQP libs, so we can support things like publisher heartbeats.. Nameko is an extensible framework. There's no reason you couldn't create a set of extensions that implemented Nameko RPC or Events over Redis. The built-in versions are AMQP implementations though, and it would not be worth trying to make them work over a different transport, even with kombu in the middle. . This issue was moved to https://discourse.nameko.io/t/question-nameko-with-alternative-kombu-transports-redis/288. This isn't something I've experienced or heard about before.\nThe health of the caller shouldn't have any impact on the callee. The message ack happens between the consumer in the callee service and the RabbitMQ broker. Even if the RPC reply queue has been removed (which it will sometime after the shell's proxy disconnects), the reply message would still be published, just discarded by the exchange. Unless you have requested mandatory delivery of RPC replies, this would not raise an error.\nIf you can reproduce with a script I would be interested in seeing it. The behaviour you're describing is consistent with the consumer losing connection with the RabbitMQ broker before ack'ing the RPC request message. I bet that actually is what's happening, but it's not directly related to the shell being killed.. There is a problem with our build environment on Travis at the moment, which is why the tests are failing so spectacularly. I will hopefully be able to get that fixed in the next few days . The build fails because the machine images were upgraded. It's now using a different version of rabbitmq, whose management api is less responsive, which causes some bogus test failures.. @bobh66 the test fixes for the new Travis infrastructure are now in master. Do you want to bring this branch up to date so CI can pass?. Nice investigation.\nYou're right that there's nothing that can be done at the Nameko level, but it does deserve calling out in the docs.\nThis is a limitation of cooperatively yielding co-routines -- if they don't yield, they starve any other threads. This is particularly a problem in Nameko if you're using the AMQP extensions, because the consumer thread needs to send a heartbeat periodically. If the consumer misses two heartbeats the broker assumes the connection is dead and kills it from the other end, and since Nameko's delivery promise is \"at least once\", the unack'd message will be requeued.\nThis is the intended behaviour. The workaround you've identified is valid, as would be explicitly yielding somewhere in your prime calculation.\nEventlet will automatically yield for you if your thread blocks for I/O. If your workload is entirely CPU-based it's good practice to yield periodically to avoid starving other threads and to allow more balanced scheduling. If it's not possible to yield (e.g. if the work happens inside a c-extension) then delegating to a normal thread via the tpool is the way to do it. . Ignoring the disparaging comments about Eventlet, your problem is probably that you're using librabbitmq rather than pyamqp. The former is a C-library which means it is not automatically monkeypatched into the Eventlet scheduler. If you have it installed kombu will prefer it unless you specify pyamqp:// in your AMQP URI.\nEventlet and the other greenthreading libraries work just fine and can outperform native Python threads if you work within their constraints.\nRe: making the ServiceRunner dynamic, I disagree. It's simple enough to restart the process with a different configuration when the seasons change.. I can't reproduce this. Working debian:stretch container example here: https://gist.github.com/mattbennett/53c0fb4bf8260b7e086ad0f817cdd005\nThe ServiceRunner mostly exists as a convenience and a dev tool. For production deployments I'd highly recommend using a runner per service, so they get an independent process and you can make use of multiple cores.. The code you've added to the gist isn't valid for several reasons. The reason that threads aren't working for you is that you've re-implemented the runner but omitted the eventlet monkey patch.\nFrom the docs:\n\nNameko is built on top of the eventlet library, which provides concurrency via \u201cgreenthreads\u201d. The concurrency model is co-routines with implicit yielding.\nImplicit yielding relies on monkey patching the standard library, to trigger a yield when a thread waits on I/O. If you host services with nameko run on the command line, Nameko will apply the monkey patch for you.\n\nSince you've not using nameko run, you need to apply the monkeypatch yourself.\n. It's here. To understand what it's doing you should read eventlet docs, specifically the bit on patching.\nThe Nameko service running machinery is spread across three locations:\n\nnameko.cli.run is the main entry point script. It finds and imports service classes, attaches signals, starts the ServiceRunner etc.\nnameko.runners.ServiceRunner is a simple wrapper around multiple ServiceContainers\nnameko.containers.ServiceContainer actually hosts the service class, manages entrypoint and dependency provider lifecycles, and spawns workers.\n\nThere is configuration-level support for using a custom ServiceContainer (the SERVICE_CONTAINER_CLS context key). If you want a custom runner or wrapper script, you'll need to implement it yourself. The eventlet monkeypatch should be applied as soon as possible, because any imports that happen beforehand will not be the patched versions.. DependencyProvider.setup or DependencyProvider.start is the place to do this. You can subclass the nameko-sqlalchemy DependencyProvider to implement or override these methods:\n``` python\nfrom nameko_sqlalchemy import DatabaseSession\nclass MemoryLoadingDatabaseSession(DatabaseSession):\n    def start(self):\n        session_cls = sessionmaker(bind=self.engine)\n        session = session_cls()\n        ...  # load data into memory\n        session.close()\n```. @xqliang I think we need to know exactly what your desired outcome is.\nFrom what I understand above, you want to extract some data from a relational database and insert it into Redis, and you want to do it only once and when the service starts. Is that correct?\nIf so, there are a couple of options. You could create a new DependencyProvider that talks to both the relational database and redis, and put the load/insert code in its start() method. Or you could use the existing nameko-sqlalchemy and nameko-redis DependencyProviders and do the load/insert in a service method. There is an entrypoint in nameko.testing.services which fires exactly once, when the service starts.. Nameko doesn't have any kind of built-in hook for this. I have wondered in the past whether a \"hook registration\" system might be useful, to allow arbitrary code to be executed at different service states (e.g. before/after service code loaded, dependencies started etc.) but there isn't anything like this at the moment.\nIf you really want to do this you have the option of using a custom ServiceContainer class, which could start dependencies, then call your hook, then start entrypoints. Writing a custom DependencyProvider would probably be easier though.. This issue was moved to https://discourse.nameko.io/t/how-to-do-some-setup-before-starting-any-entrypoint/301. Yep, it is exactly eventlet/eventlet#420. The signature of process_request has changed in Eventlet master. I'm not sure what the best way to work around it is yet.\nIt's interesting that nameko run raises the error but the test suite hangs. It is probably worth some more investigation to figure out why that is. The pytest-timeout plugin should abort any test that takes longer than 30 seconds, so it's surprising that it hangs completely.. Thanks @notpeter. I will release a hotfix. Nameko will refuse to start if it can't connect to the AMQP broker, in the same way that a webserver won't start if it can't bind to the port you've given it. It's not really a crash.\nNameko is a framework. You can write services that use the bundled extensions that talk AMQP, HTTP, both or neither. It doesn't make sense for Nameko to start one type of extensions before the other... because it doesn't know what extensions are in use or what the requirements are.\nThe best way to solve your problem is probably to run the service behind something like supervisord, and restart the process if it fails to come up for some reason.. Closing this as unrelated to Nameko. Hope you fixed your issue @eugeneRover . Sorry for the back and forth. All comments addressed now, and I've checked the test actually pass ;). Thanks. Will add changelog entry when I cut a release. Most people put a load-balancer in front of the RabbitMQ nodes. Another approach is to pass a semicolon delimited string with the addresses of each node as the AMQP_URI, e.g.\nyaml\nAMQP_URI: pyamqp://guest:guest@rabbitmq1:5672/;pyamqp://guest:guest@rabbitmq2:5672/\nThis passes through to Kombu, which will round-robin between URIs when it can't connect.\nHA strategies in general have been covered on the mailing list: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!searchin/nameko-dev/HA%7Csort:date/nameko-dev/DpR8LeNUod0/uOethL81AgAJ\n. This issue was moved to https://discourse.nameko.io/t/connecting-multiple-rabbitmq-host-from-nameko/293. Thanks @geoffjukes for this PR!\nCan you please also add a test for this? e.g. add a new method here\nhttps://github.com/nameko/nameko/blob/a0c61ad82650a04b2aa24264974a189fff6bf911/test/web/test_http_handler.py#L11-L23\nand verify that it can be called successfully with multiple methods.\nThis change probably deserves a line in the docs somewhere too.. You can't do this with the built-in RPC implementation because of the way the message routing is configured. All service instances consume from a single queue and are therefore indistinguishable.\nThe routing is as follows:\n```\nMESSAGE          ROUTING KEY        EXCHANGE     BINDING                  QUEUE                             CONSUMERS\n                               nameko-rpc                                                            +----------------+\n                                 +---+                   +-----------------------------+      +------>service instance|\n\n+-+                                |X X|                   |                             |      |      +----------------+\n  |X| +----------------------------> | X +-------------------> {service-name}              +------+\n  +-+  {service-name}.{method-name}  |X X|  {service-name}.* |                             |      |      +----------------+\n                                     +---+                   +-----------------------------+      +------>service instance|\n                                                                                                         +----------------+\n```\nIn words:\n Messages are published to the nameko-rpc exchange...\n With a routing key describing the target service and method name\n One queue per service is bound to the (topic) RPC exchange with a wildcard binding matching the service name and any method name\n Therefore all request messages for a target service will be deposited into the queue for that service\n* Finally all instances of the target service consume from the service queue. Messages are consumed in a round-robin.\nHopefully that illustrates why targeting service instances isn't possible.\nYou could get your desired behaviour by using a different routing topology. Something like this:\n```\nMESSAGE          ROUTING KEY              EXCHANGE     BINDING                  QUEUE                             CONSUMERS\n                                     nameko-rpc\n                                       +---+                        +-----------------------------+\n\n+-+                                      |X X|                        |                             |        +----------------+\n  |X| +----------------------------------> | X +------------------------> {service-name}-foo          +-------->service instance|  Tags: foo\n  +-+  {service-name}.{method-name}.{tag}  |X X|  {service-name}..foo  |                             |        +----------------+\n                                           +---+-+                      +-----------------------------+-+\n                                                 +-+                                                    +-+\n                                                   +-+                  +-----------------------------+   +-+\n                                                     +-+                |                             |     +-->----------------+\n                                                       +----------------> {service-name}-bar          +-------->service instance|  Tags: foo, bar\n                                                  {service-name}..bar  |                             |        +----------------+\n                                                                        +-----------------------------+\n```\nThis could be implemented as a custom extension that extended the built-in ones.. This issue was moved to https://discourse.nameko.io/t/how-to-specify-a-custom-service-instance-when-using-clusterrpcproxy/302. 6aa00da will apply if you run your tests locally using tox as well. . Yes :(\nI've had some discussion about the breaking change in https://github.com/eventlet/eventlet/issues/420 but honestly got a bit lost with it.. The tox change is required because we were no longer actually testing against eventlet master. See https://travis-ci.org/nameko/nameko/builds/328005814?utm_source=github_status&utm_medium=notification\nBefore the change tox was executing the following commands:\npip install --upgrade https://github.com/eventlet/eventlet/archive/master.zip (from mastereventlet match)\npip install --editable .[dev] (from lib match)\nmake test_lib (from lib match)\npip install --editable .[dev] after pip install ...master.zip was pulling eventlet back down to the pinned version. You just need to start multiple processes.\n```\n$ nameko run module:Service &\n[1] 96201\nstarting services: service\nConnected to amqp://guest:**@127.0.0.1:5672//\n$ nameko run module:Service &\n[2] 96215\nstarting services: service\nConnected to amqp://guest:**@127.0.0.1:5672//\n$ ps aux | grep nameko\nmattbennett      96235   0.0  0.0  2424600    368 s004  R+    7:39am   0:00.00 grep nameko\nmattbennett      96215   0.0  0.2  2439900  28724 s004  S     7:39am   0:00.60 /Users/mattbennett/.virtualenvs/nameko/bin/python3.5 /Users/mattbennett/.virtualenvs/nameko/bin/nameko run module:Service\nmattbennett      96201   0.0  0.2  2439900  28576 s004  S     7:39am   0:00.90 /Users/mattbennett/.virtualenvs/nameko/bin/python3.5 /Users/mattbennett/.virtualenvs/nameko/bin/nameko run module:Service\n```. This issue was moved to https://discourse.nameko.io/t/build-multiple-nodes-of-one-service/284. You can actually do this statically. I think the piece of the puzzle you're missing is how entrypoints register themselves. It's here:\nhttps://github.com/nameko/nameko/blob/fb7560e08eaf14fbe1684abf6939efbd7aa9fbca/nameko/extensions.py#L249-L256\nThe ENTRYPOINT_EXTENSIONS_ATTR constant is just nameko_entrypoints.\nHere's an example of static inspection:\n``` python\nimport inspect\nfrom nameko.rpc import rpc\nclass Service:\n    \"\"\" A service \"\"\"\nname = 'service'\n\n@rpc\ndef method(self):\n    \"\"\" A method \"\"\"\n    return True\n\ndef is_decorated(obj):\n    return inspect.isfunction(obj) and getattr(obj, 'nameko_entrypoints', None)\nentrypoint_methods = inspect.getmembers(Service, is_decorated)\nprint(Service.doc)\nfor name, method in entrypoint_methods:\n    print(method.doc)\n```\nIt would be easy to wrap this up into a callable entrypoint as well, of course.. This issue was moved to https://discourse.nameko.io/t/custom-entrypoints/300. 5 minutes is fairly arbitrary. Long enough to cover extended downtime, but not so long that the broker is full of dead queues.\nRe: hanging requests; yes, you can tell whether a queue already existed on declaration, so we could totally do this. I will add.\n. @davidszotten update on this branch...\nI've added a couple of tests for the desired behaviour of raising if we detect we've been disconnected long enough to lose a reply. Unfortunately the implementation is not possible for a couple of reasons:\n\n\nIn the standalone proxy we can't even test the behaviour because the current implementation reconnects immediately on disconnection, without retry and only twice, which doesn't take long enough for the broker to reap the expired queue.\n\n\nIn the service proxy, the queue declaration (with its return value that states whether the already queue existed or not) is inside the consumer creation, which delegated to the queue consumer. So the reply listener doesn't have an appropriate hook to know whether its outstanding replies will be answered or not.\n\n\nFortunately, I have a couple of branches already in the works that fix these problems:\n\nhttps://github.com/nameko/nameko/compare/master...mattbennett:rpc-refactor?expand=1\n\nIn this branch, the standalone proxy has been refactored to use the normal ConsumerMixin pattern. This means it's possible to attempt to reconnect for long enough that the reply queue expires.\n\nhttps://github.com/nameko/nameko/compare/master...mattbennett:remove-queue-consumer?expand=1\n\nIn this branch, the QueueConsumer is completely removed in favour of making each AMQP consumer (RpcConsumer, ReplyListener, EventHandler, Consumer) implement ConsumerMixin themselves. The refactor isn't finished yet, but it would give the ReplyListener the hook it needs to determine whether the queue existed on reconnection.\nThere would be a lot to review and the remove-queue-consumer refactor isn't ready yet.\nWe can reduce the risk of hanging RPC requests by increasing the expiry timeout to something quite large (e.g. 30 minutes) so that we can release these fixes without waiting for the subsequent refactors to land.\n. @davidszotten The PRs listed above address the first problem in my previous comment. They are ready to be reviewed. In those branches the standalone RPC proxy will raise if its reply queue has been expired with pending replies while the proxy was disconnected. The service proxy is still unable to detect it until the queue consumer is refactored out (still a work in progress)\nIn the interest of reducing the amount of unmerged changes I'm going to merge this PR now. If you're OK with it I think we should also release this even before the other branches are ready -- rationale being that this fixes a known problem that can take down services, which is more bad than the risk of some stuck RPC requests.. @geoffjukes actually that test is failing because the new method added in the example shadows the existing one (they're both called get_method)\nFYI test_examples does install the current codebase version (pip install -e .[dev]) but it is reported as 2.8.3 because that's the version in setup.py at the moment.\nIt occurred to me just now that you can actually achieve your desired behaviour just by stacking the decorators:\npython\n@http('GET', '/resource')\n@http('POST', '/resource')\ndef resource(self, request):\n    pass\nI think that's actually more readable, and it doesn't need any new code. What do you think?. Ha, we've all done it! Sometimes you can't see the wood for the trees.\nI see what you mean about being ugly. Your do_stuff method is interesting though...\nYou pass the request.method in, so does that mean you split it back out on the other side? e.g.\n``` python\n@http('PUT,GET,PATCH,DELETE', '//deliver')\ndef deliver(self, request, book_id=False):\n    return do_stuff(book_id, request.method)\ndef do_stuff(book_id, request.method):\n    if request.method == \"GET\":\n        # do GET stuff\n    elif request.method == \"POST\":\n        # do POST stuff\n```\nIf so, why not do:\n``` python\ndeliver_resource = '//deliver'  # for easier refactoring\n@http('GET', deliver_resource)\ndef get_delivery(self, request, book_id=False):  # or a more descriptive name\n   # do GET stuff\n@http('POST', deliver_resource)\ndef submit_delivery(self, request, book_id=False):\n   # do POST stuff\n```\n?\n. OK cool, I see. Well this is a tiny amount of logic that is obviously helpful in some situations. I will merge and release it in the next cut.\nThank you for the contribution and the discussion! . This is way too broad a question to give any meaningful recommendations. If you have a specific question or design in mind I may be able to help. You may also find that the mailing list is more responsive.. @fobiols this is failing because of https://github.com/timothycrosley/isort/issues/676\nPlease add a closing newline to test/test_call_id_stack.py. Thank you @fobiols!. @TimGraf can I ask what the motivation for changing the exchange name was?\nI was actually considering dropping that feature because it didn't feel particularly useful.. The main reason is that RPC is simple. It can be invoked just like a \"normal\" method and is very flexible in terms of what the server can offer. Unlike REST it doesn't force you to write your APIs in a particular way.\nThere's a good (tongue-in-cheek) article about it here: https://medium.freecodecamp.org/rest-is-the-new-soap-97ff6c09896d . \"RPC\" just means Remote Procedure Call. It doesn't imply an implementation or even a protocol. You'll only be able to talk to your \"existing RPC server\" using whatever protocol it uses.\nYou could build an extension in Nameko that implemented whatever protocol that is so your Nameko service could speak it, but you can't just use the built-in AMQP client unless your remote server is listening for Nameko AMQP RPC.. This issue was moved to https://discourse.nameko.io/t/why-did-nameko-use-rpc-for-communication-between-service/304. Thanks @ornoone, this seems like a useful addition. \nThe only thing I'm slightly cautious of is forcing users to accept another library dependency. If we can make the use of the regexlibrary optional I would be in favour of accepting this feature.\nDo you want to make a PR with your suggested implementation?. Implemented in #515. Fixed in #512 . @noisyboiler I'm happy to release as soon as it's been through review. Some critical eyes would be helpful, thanks. I would also like @davidszotten's approval before landing it.. @ornoone I don't know what that's suddenly started failing, but adding setuptools to docs/spelling_wordlist.txt will fix it.. The main reason is that Nameko started out as a compatibility library for Openstack's Nova RPC protocol.\nNameko has evolved into a framework now though, and you can write your own extensions. It would not be much code to add a set of extensions that did JSON-RPC if that's what you need.\nThe \"built-in\" extensions like AMQP-RPC are included in the main library because they're battle-tested and most historical Nameko installations use them.. This issue was moved to https://discourse.nameko.io/t/why-nameko-did-not-use-something-like-json-rpc-for-serialization/312. You may be able to figure out what the greenthreads are doing by printing their stack traces. If you backdoor into the process, you can access the currently running managed and worker threads via the container.\nLooking at their current stack may give you an idea what they're waiting for.\nIt looks like your queue consumer thread is waiting to receive data and not getting any, which makes sense if you're already holding max unacked messages. So I would start by trying to figure out what the worker threads are doing, and why they've not finished and acked their messages.\nThe \"cannot switch to a different thread\" message is suspicious. Whatever caused it may have broken the eventlet scheduler.\n. I'm confused. Why did you choose to consume all the disk space to reproduce the issue? Couldn't this be causing a completely different problem?\nFrom what I understand of tpool, it's intended to wrap non-green functions that would otherwise block the event loop. If the thing you're wrapping yields, why are you using the tpool?. Ha, no worries. Yep, let's close this issue.\nI guess the root cause is either the out of disk space error or the yield inside the tpool. You might want to check over at github.com/eventlet/eventlet whether a yield inside a tpool is expected to cause problems. Seems a bit fragile to me.. Not out of the box, but it's trivial to write a service that can introspect itself:\n``` python\nfrom collections import defaultdict\nfrom nameko.extensions import DependencyProvider\nfrom nameko.rpc import rpc\nclass EntrypointList(DependencyProvider):\ndef get_dependency(self, worker_ctx):\n    # all Extensions have access to self.container\n    return list(self.container.entrypoints)\n\nclass Service:\n    name = \"discovery\"\nentrypoint_list = EntrypointList()\n\n@rpc\ndef introspect(self):\n    entrypoints = defaultdict(list)\n    for ep in self.entrypoint_list:\n        entrypoints[ep.method_name].append(type(ep).__name__)\n    return entrypoints\n\n shell\n$ nameko shell\nNameko Python 3.5.3 (default, Jun 30 2017, 18:28:54)\n[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] shell on darwin\nBroker: pyamqp://guest:guest@localhost\n\n\n\nn.rpc.discovery.introspect()\n{'introspect': ['Rpc']}\n```. This issue was moved to https://discourse.nameko.io/t/automatic-service-discovery/283. Nice. Want to raise a PR to add these to the  community extensions page?. Thanks @raybotha . @iky can you give a quick update on the state of this branch?. Exciting to see some commits landing in this branch @iky!\n\n\n\nStatic analysis is failing on incorrectly sorted imports. You can fix them by running FIX=1 make imports (or running isort on save or as a pre-commit hook, which is what I now do). > nameko.config_update and nameko.config_setup helper functions and context managers added. \n There is a question, should these helpers then be moved to testing module?\nI don't think they do any harm in the main namespace. There may well be non-testing usecases for them, so perhaps they should stay where they are.\n. What do you think about apply_config (or set_config) and update_config instead of config_setup and config_update? Verb-first feels a little more natural to me.. There is probably a better workaround here.\nI think the bug against kombu is valid, but the change added in https://github.com/nameko/nameko/pull/481 (which is the trigger for the hanging tests) has other problems too:\nThe kombu pool is shared at the process level and the vhost pipeline destroy runs in a background thread, not synchronously at the end of tests. So when tearing down from a previous test we may be closing publisher connections created by a currently running test. It is a little bit surprising that this hasn't caused other problems.\nI think we need a different solution for the problem that #481 tries to address, as well as this workaround.. The alternative discussed in #481 was just to increase the number of file descriptors on the rabbit instance, which now seems much more reasonable than trying to accommodate a particular test suite's requirements with a change to the underlying library. I will revert ecf6c58.. @iky you're right, connection pools are per vhost. Not sure how I missed that. We use a different vhost per test, so the failure here is only a result of the kombu bug. I will revert the revert \ud83d\ude2c. > 1- Can the event.handler/pub-sub be used as a job worker?\nNot sure what you mean by \"job worker\" here. Actually services do pull messages from the broker, even event handlers, but perhaps what's not obvious is that they pull up to a certain amount at a time (the \"prefetch count\"). By default in Nameko this value is inherited from the max_workers config value, which is 10.\nYou can make your services look more like workers that \"pull\" work when the existing work is done by reducing the value of max_workers.\nHave I understood the question correctly?\n\n2- If one event.handler event-let is taking 1 hr to complete? is every other event waiting to be delivered? i.e. none of other event.handler will run or does event-let primitively pause the green-thread and give resources to other green-threads? I think resource hogger event.handler will block the whole system. any way around it?\n\nEventlet will yield any thread that blocks for I/O, so unless your \" resource hogger\" thread is unyielding then other threads will still run. If you have a workload that is unyielding, you should delegate it to a tpool to avoid blocking the event loop.\n\n3- Can you explain the message data flow of inbound and outbound calls of rpc i.e. which objects of nameko are doing what when the message is received.\n\nSee this thread on the mailing list. This issue was moved to https://discourse.nameko.io/t/nameko-service-event-handler-as-a-pull-job-worker/296. I've changed the pytest options here to be called --amqp-ssl-options, which better reflects what you can actually with them. This is now ready for review @davidszotten . The way to do this is to use different config files for each service.\nChanging the config key or value inside the service would take quite a lot of hacking because it's not read in a single place. You can do it, but you'd have to subclass every AMQP extension that you're using. . This issue was moved to https://discourse.nameko.io/t/how-to-config-different-amqp-uris-for-different-services/298. If you use nameko run on the command line you can specify dictConfig style logging in your configuration file.\nSee http://nameko.readthedocs.io/en/stable/cli.html?highlight=logging#running-a-service. This issue was moved to https://discourse.nameko.io/t/logging-setup/290. Please read the key concepts page of the documentation. That should answer most of your questions.\nSpecifically for the questions you've asked here:\n\nwhen is a service created, is its init method called?\n\nNo. The service class is instantiated every time an entrypoint fires, and that instance is discarded again when the entrypoint returns.\n\nwhen are dependencies started, injected and their init methods called\n\nDependencyProviders and all other Extensions are started when the service container is started. Dependencies (the result of DependencyProvider.get_dependency) are injected into the instance of the service that is created when an entrypoint fires.. This issue was moved to https://discourse.nameko.io/t/lifecycle-of-service-and-dependancies/297. Sorry for the slow response to this @rizplate. The answer is that you have to create extensions to do this.\nNameko service classes are instantiated for every worker and discarded afterwards. That means if you save some state to self.whatever in one method, it won't be there when the next worker executes. You should use dependency providers to manage state between workers.. This issue was moved to https://discourse.nameko.io/t/support-for-init-method-on-service/285. Yep, nameko run expects a module path, not a file path. You can invoke from a different direct by specifying the dotted module path to your service (assuming its finable from your python path), e.g.\nnameko run mypackage.myservice\n. This is the same problem as https://github.com/nameko/nameko/issues/425. \nWhat versions of Nameko and Eventlet are you using?. A bug was introduced in Eventlet 0.21.0 that causes this problem on certain operating systems. It is fixed in Eventlet 0.22.0, but unfortunately Nameko is not compatible with Eventlet 0.22.0 yet.\nPlease pin your eventlet down to 0.20.0. We don't do this in setup.py because this bug only affects Python 2.. The answer to this question depends on how you're running your services. Nameko intentionally doesn't concern itself with this.\nI consider it best practice to at least run each service in its own process, e.g.\nshell\nnameko run service_a\nnameko run service_b\nshell\n$ ps | grep nameko\n42649 ttys004    0:01.02 ~/.virtualenvs/nameko/bin/nameko run service_a\n42984 ttys005    0:00.72 ~/.virtualenvs/nameko/bin/nameko run service_b\n43253 ttys006    0:00.00 grep nameko\nThen you can stop and restart service processes independently.\nOther recommended patterns would using a process manager (e.g. supervisord) or containers. See https://github.com/nameko/nameko-examples for a nice example of how to package nameko services as docker containers.. This issue was moved to https://discourse.nameko.io/t/how-to-manage-lots-of-services-in-nameko/286. Included in #542. Included in #542. I have had good success diagnosing leaks with pympler via the backdoor. As far as I'm aware it can see everything.\nThe problem is probably in nameko-sqlalchemy. Which of the two DependencyProviders are you using? The newer one is more efficient if you're handing a lot of concurrent requests.\n. In the two cases I've diagnosed, the causes were:\n\nA circular reference causing data in the worker context not to be garbage collected\nLots of backed up requests accepted by the @http entrypoint but blocked on the worker pool (not strictly a leak) \n\nIn both cases I could see the objects in pympler though.. @bludau-peter my first investigation would be to find out in what situations a process will consume memory and the objects not be visible in pympler. At a guess it's something to do with c-extensions, in your case either in pandas or numpy. The fact that memory grows after only one worker execution suggests it has nothing to do with Nameko or the request-response cycle.. This issue was moved to https://discourse.nameko.io/t/identifying-a-memory-leak/291. Nameko HTTP is built on Werkzeug, which is the WSGI library that underpins Flask. url_for is one of the conveniences that Flask adds, so it's not available out of the box.\nIt's not that hard to implement though. Nameko entrypoints attribute the service methods that they decorate, so you can inspect the service class to find them. This snippet should get you started:\n``` python\nfrom nameko.extensions import ENTRYPOINT_EXTENSIONS_ATTR\nfrom nameko.web.handlers import http, HttpRequestHandler\nfrom nameko.rpc import rpc\nclass Service:\n    name = \"serv\"\n@http('GET', '/url-one/<token>')\ndef handle_url_one(self, request, token):\n    pass\n\n@rpc\ndef get_url_one(self, token):\n\n    entrypoints = getattr(self.handle_url_one, ENTRYPOINT_EXTENSIONS_ATTR)\n    routes = [\n        entrypoint.url for entrypoint in entrypoints\n        if isinstance(entrypoint, HttpRequestHandler)\n    ]\n    return routes\n\n```\nReturns ['/url-one/<token>']. The logic you need to substitute the arguments into the string must exist in werkzeug somewhere. . Nice!. This issue was moved to https://discourse.nameko.io/t/a-way-to-build-a-url-route/299. It's because you've quoted it.\nyaml\nWEB_SERVER_ADDRESS: 0.0.0.0:${PORT:8000}\nWill work as you expect.\nIf you need to quote the value for some reason, you can use the explicit matcher.\nyaml\nWEB_SERVER_ADDRESS: !env_var \"0.0.0.0:${PORT:8000}\"\nAlso does what you'd expect.\n. This issue was moved to https://discourse.nameko.io/t/config-yml-file-for-web-server-address/295. @davidszotten this is ready for a review. I'm sorry it's so massive :/. I got carried away and did some more refactoring here, so the interim #537 is not useful anymore. The RPC proxy interface that was added in that branch has been changed slightly.\nI will add a summary all the changes between this branch to master in a comment here soon. They are quite significant so I would like to propose that after some rounds of review we release an alpha build for people to experiment with.. As promised, a list of all the changes incorporated in this PR:\n\n\nAdds a new helper at nameko.amqp.consume.Consumer. Similar to the helper at nameko.amqp.publish.Publisher, we use it any time we want to consume messages from AMQP. It uses the kombu ConsumerMixin but kombu is mostly behind the abstraction now.\n\n\nRemoves the QueueConsumer. All entrypoints, extensions and utilities that consume from AMQP now use the nameko.amqp.consume.Consumer helper to do so.\n\n\nAdds checks for replies lost in expired RPC reply queues that could not be implemented in #504.\n\n\nSimplifies the RPC implementation, replacing ServiceProxy and MethodProxy with single object (simply Proxy, could do with a better name) that is less coupled to the DependencyProvider implementation of the RPC proxy. This means the standalone RPC proxy implementation can be much simpler.\n\n\nDrops the graceful stopping of AMQP consumers on container kill (surplus to requirements since we stopped sharing a vhost between tests). The tests are now much faster and the fast_teardown fixture hack makes no difference, so has also been removed.\n\n\nReplaces fragile uses of the RabbitMQ Management API (which got very slow in 3.6.6) and inspects queues with an AMQP-based inspection.\n\n\nAllows Extensions that consume AMQP messages to be explicitly configured with a prefetch count, distinct from the worker count. There is a new configuration option to set this globally, and a value for prefetch_count can be passed directly when instantiating the extensions (with the exception of the @rpc entrypoint)\n\n\nDrops various deprecations that were slated for removal in 2.9.0.\n\n\nI will put comments in the diff to explain the changes in each module.. The rabbit docs talk about optimising for hundreds of thousands of connections, so it's certainly capable. Whether it works with the default settings will depend on the host and the size and number of services connecting to it.\nAt Student.com we have over 200 event handlers across our service cluster so we'll be able to do some testing with a pre-release build. . @kooba Thanks for the review. I will fix all the typos.\nYes, I think this should land in a v3.x.x release. It is compatible in terms of the external API, but I think the internal changes (e.g. many more rabbit connections) warrant a version bump.\nI would also like to have upgraded kombu as part of the final 3.x release, but we can possibly make an alpha 3.x release to test beforehand.. @davidszotten @kooba comments addressed.\nIf you're both happy I'd like to create a 3.x branch that we can do some testing with.. Merged into v3.0.0-rc branch with #578. All the AMQP entrypoints do at-least-once-delivery actually. The ack is not sent until after the service method has returned, and if the connection is lost in the mean time the unack'd message is reclaimed by the broker and returned to the queue for redelivery.\nOne caveat to this is if you are using the \"broadcast\" handler type (as opposed to the default \"service pool\"). In this mode you have to tell Nameko how to uniquely identify a service instance if you care about persisting events between restarts. See the docs on the broadcast_identifier.\nUnfortunately the only way to set the broadcast_identifier at the moment is to subclass the EventHandler. There's no way to pass it into the constructor.. There is no default, but message TTL can be set by passing the expiration keyword to the EventDispatcher constructor or when dispatching an individual event.. This issue was moved to https://discourse.nameko.io/t/at-least-once-events-delivery/294. Looks like you've already figured this out but I will answer here for completeness.\nThe Nameko @rpc entrypoint uses a custom protocol over AMQP, so it's unrelated to gRPC. However, Nameko is a framework and it is possible to implement any protocol as set of extensions. gRPC is great as a protocol/transport but the development experience is not pleasant. I think the combination of Nameko and gRPC would be really fantastic.\ngRPC is essentially protobufs over HTTP2, so it should be possible to implement this in Nameko. I will close this issue and we can use #516 to continue the discussion.. There hasn't been any feedback except https://github.com/nameko/nameko/pull/351#issuecomment-397102500.\nI think we're OK to release.. I think the original rationale for logging these as INFO was that a worker exception (expected or not) is different from an unhandled exception in an extension, and we wanted the latter to be more important.\nWith the benefit of hindsight though, it would be better to elevate these worker exceptions to ERROR if unexpected and possibly WARNING if expected. We should probably do whatever webservers do when they serve 4xx and 5xx errors.\nAdditionally, when extensions do throw unexpected exceptions, it kills the service container. We could  elevate the log messages in ServiceContainer.kill to FATAL.\n. The \"not found\" exception case exists in REST APIs too, and would be logged as a 404 WARNING.\nI think we should follow the webserver convention and let users provide a filter in their logging configuration if their usecase results in too much verbosity.. I think this is great. @davidszotten?. This issue was moved to https://discourse.nameko.io/t/one-of-our-rpc-call-fails-and-success-in-a-sequence/289. We don't have any kind of formal definition of the \"public\" APIs, but it should be safe.\nThe fundamental signature of some services to run, and the config to use when doing so, is not going to change. In the future it might grow some more keyword arguments like backdoor_port, but they'd be backwards compatible.. Yes, probably. I'm not sure utils is the right place, but certainly these two things are more about running services than the CLI. Perhaps a new nameko.run would be a good module to house them.. Sorry for the late reply here @gpkc. You're right that things are messy. These facilities have independently evolved and never been refactored into a neater place.\nI would welcome a PR that incorporated run_services into nameko.cli.run.run() and perhaps also moved the whole lot to a better named module like nameko.run. \nThere is also the ServiceRunner class which is really just a convenience wrapper around multiple ServiceContainers. I'm not overly keen on it. I don't think it's really necessary and it could certainly be better named (ServiceContainerCollection perhaps, although that's a bit long)\n. This issue was moved to https://discourse.nameko.io/t/importing-nameko-cli-run-to-execute-run/292. Please see #533 for an explanation of this.. This is what is sounds like -- your process ran out of file descriptors. The maximum number of file descriptors per process is controlled by the operating system.\nRunning 1000 services in a single process doesn't seem like a good idea. You'll get better performance splitting your workload across multiple processes because a Python process cannot use more than one CPU core. In fact, my recommendation is to use a process per Nameko service for most workloads.\n. Nameko is telling you that it can't connect to RabbitMQ at localhost:562, presumably because it's not running on that port.\nWhat are you expecting to happen here?. OK, I see what you're getting at. You're trying to configure High Availability for RabbitMQ but without using a load-balancer.\nWhen you provide an AMQP_URI containing two addresses, Nameko will connect to each address in a round-robin fashion, which is an alternative to using a load-balancer if you target the individual nodes of a cluster.\nThe problem you're running into is that Nameko will only round-robin when it reconnects. Because the first node is down when the service starts, it does not retry. This is a disadvantage of the round-robin approach.\nIf you make the second URI the one that's broken you'll find that Nameko connects and the service runs until it loses connection with the first node; at this point it will try to connect to the second one, and retry until it succeeds.\n. No, it's kombu. We pass the connection string straight through.\nSee http://docs.celeryproject.org/projects/kombu/en/latest/reference/kombu.connection.html#connection\n\nParameters: | \u00a0URL\u00a0(str,\u00a0Sequence) \u2013 Broker URL, or a list of URLs.\n\n. This issue was moved to https://discourse.nameko.io/t/trying-to-run-two-instances-of-rabbitmq-for-nameko/307. This is likely to be an issue with python 3.7 and eventlet. Which versions of eventlet and requests were you using?\nCan you reproduce by making a requests.post inside an eventlet greenthread without Nameko involved?. Nice, thanks. Looks good to me. Static analysis just failing with line length.. It will work if the value of the environment variable is quoted.\nThe shell strips quotation marks when declaring env vars (FOO=\"foo\" is equivalent to  FOO=foo), but if you have control of the variable you can double-quote it to ensure Nameko sees it as a string: FOO=\"'foo'\"\nThis PR may still be useful for cases where you don't have control of the underlying variable though.. There is perhaps a bug here too. I would expect\n```\nFLOAT=0.10\nBAR: !env_var \"${FLOAT}\"  # with surrounding quotes\nto yield\nBAR: \"0.10\"\n``\n... but you get0.1` instead. @santiycr This is great as it is, thank you very much.\nI considered this possible bug a little further. This works:\n```\nFLOAT=0.10\nBAR: !env_var \"'${FLOAT}'\"  # with double surrounding quotes\nYields:\nBAR: \"0.10\"\n```\nIt works for the same reason as the double-quote env var hack: the outer quotes perform a grouping operation so that you can pass values with spaces in, hence are stripped. I don't this constitutes a bug. It has been an interesting detour but I much prefer the explicit approach you've added here.\nWhile experimenting I added some tests for double-quoted values which I have pushed to your branch. \nThis is good to go, thank you very much!\n. That error suggests that the mock is not being applied by the worker_factory. I can't reproduce though. This works:\n``` python\ntest_service.py\nfrom nameko.events import event_handler\nfrom nameko.extensions import DependencyProvider\nfrom nameko.testing.services import worker_factory\nfrom mock import Mock\nclass FooBar(DependencyProvider):\n    def get_dependency(self, worker_ctx):\n        return \"foobar\"\nclass FirstService(object):\nname = 'first_service'\n\ndependency = FooBar()\n\n@event_handler('second_service', 'complete')\ndef foo(self, payload):\n    self.dependency.bar(payload)\n\nclass TestFirstService(object):\ndef test_foo(self):\n    service = worker_factory(FirstService)\n    assert isinstance(service, FirstService)           # passes\n    assert isinstance(service.dependency, Mock)        # also passes\n\n shell\n$ py.test test_service.py --pdb\n============================= test session starts ==============================\nplatform darwin -- Python 3.5.3 -- py-1.4.34 -- pytest-2.7.3\nrootdir: /private/tmp, inifile:\nplugins: timeout, stepwise, cov, cagoule, cache, nameko\ncollected 1 items\ntest_service.py .\n```\n. This issue was moved to https://discourse.nameko.io/t/worker-factory-is-not-mocking-dependencies-as-expected/287. Seems sensible to me.\nWe're failing here cause this version of pytest doesn't pick up the coverage generated by testdir.runpytest. I can't remember why I avoided using coverage run but perhaps that's an easy fix.. Q1: It means that exactly one worker executes at a time. If your worker is consuming from a queue (e.g. an event hander or RPC queue) then I think yes, it's working as you expect as a \"regular queue\"\nQ2: Not out of the box, no. You may find the nameko-tracer project useful for tracking request/responses though. It would not be hard to build a service-registration extension to track the number of services and their entrypoints either. \nQ3: For the AMQP extensions, it is at-least-once and it's not currently configurable. Messages are ack'd at the end of the service method, and if the connection is lost after delivery but before acknowledgement, RabbitMQ will reclaim and redeliver the message. You could change this behaviour with custom extensions that acknowledged the message before executing the service method.\nIf you have more questions about retries and timeouts, please ask them on the mailing list. This issue was moved to https://discourse.nameko.io/t/max-workers-1-explanation/282. I don't use PyCharm but I don't see why it would be any different from pdb -- set a breakpoint, and step.\nWhat have you tried and how is it different from what you expected to happen?. Heartbeats aren't supported on publisher connections because there's no easy way (in kombu) to periodically send the heartbeat. Unlike the consumer, there is no thread or loop around which we're constantly iterating, thus no opportunity to send the heartbeat.\nThis is one of the many reasons why my long-standing ambition is to replace kombu with a different AMQP library. The AMQP extensions have been refactored significantly recently (especially in https://github.com/nameko/nameko/pull/542) which makes this reality a bit more likely, but it'd still be quite a lot of work.. Nice! TCP keepalives would be a much easier solution.\nNameko pins kombu<4 because of the jump to pyamqp 2.x, which is not backwards compatible with the 1.x series. The incompatibility is small though, and I think we're ready make the leap and drop support for the older versions now.\nFrom memory, you can no longer declare on a Connection and must use the Channel instead, and there are more explicit exceptions (which I think make detecting undeliverable messages easier). TCP keepalives supported with the changes in #564. That's interesting. Given all those reported problems, the safest course of action is probably to set the transport_options as you have. Then at least we won't be silencing exceptions that people may be relying on.\nThe best solution would be to make transport_options configurable somehow, in the same way that publish_confirm are. This is best saved for a later PR though.\n. These lines can be removed now, we never reach the original place where UndeliverableMessage was raised:\nhttps://github.com/nameko/nameko/blob/c7e8f43a9f2c31d1d16dc76efd464f6e47aa9ee3/nameko/amqp/publish.py#L217-L223\nAlso it looks like the mock_connection fixture no longer has any uses. You can either remove it or #pragma: no cover it.. So you did, sorry. I completely missed that.. You're missing an eventlet.monkey_patch. See the note at the bottom of this section of the docs. nameko run applies this for you.\nIt happens to work in the timer-only case because that particular extension doesn't do any I/O, so doesn't need the eventlet patch to be applied. . A Kafka entrypoint would be great a addition to the ecosystem.\nThe nature of Nameko as an extensible framework means there are many different possibilities for extensions, and my preference is to keep them as separate libraries. The \"built-in\" extensions are included because of Nameko's history, and in the future I think it's more likely that we'll spin those out (e.g. into nameko-amqp and nameko-http) than bring other extensions in.\nI would certainly bless a suitably well-built library though, for example by adding it to the list of community extensions.\nYou may want to look at this recent post on discourse forum if you haven't seen it already.. Awesome! I will leave this issue open to signpost the development effort.\nLooking forward to seeing it progress :). A Nameko event is just an AMQP message with a particular payload, so a standalone event handler is just something that consumes those messages. You should be able to use any AMQP library.\nIf I understand your question correctly, you could also use your Nameko service to communicate with your tornado services using a websocket connection.  You can use the websocket-client library for that, and there's an example of using it in the nameko codebase.\n. For future reference, the forum is a better place for support questions like these. I will copy this issue there now and we can continue discussion on the forum if needs be.. This issue was moved to https://discourse.nameko.io/t/how-can-i-connect-event-callback-to-tornado/329. Your message to the mailing list got stuck in moderation, so I've answered it there rather than on this duplicate.\n. Sorry for the slow reply here. I'm not very familiar with the websocket implementation, but I think at least the \"client quit\" case has come up before. Have you seen this post on the mailing list and the linked issue?\nI would welcome a PR with either better docs, an improved implementation or \"websocket rpc\" implementation as discussed in those posts.. Something has broken here with the examples tests. I don't think it's related to this PR but I will leave this open until the issue is fixed.. Thanks for this @bright-pan. The tests are broken at the moment for some reason, not related to this change. I will fix them up and merge this as soon as I get a chance.. Continuing the discussion from https://github.com/nameko/nameko/issues/223 here.\nThis seems like a small change that would bring about some benefit. It is a little esoteric, but the cost of supporting it is very low.\nThe CI pipeline seems to be broken somehow, but once that's fixed I'm happy for this to land. @davidszotten, thoughts?. This seems quite a niche use-case to me. It's not outrageous to send the RPC reply to the exchange from which the request came, but I'm sure there will be implications of making this change. There may already be some failing tests, and if not perhaps we're missing some coverage.\nIs the situation you're describing in the summary a hypothesis, or reality after this change? If it's reality then some tests demonstrating as much would be great ;)\n. Alternate exchanges are a nice feature, and this use-case of a shared set of RPC services is pretty neat! \nI also agree with you that replying to the exchange specified in the message is nicer than to some arbitrary application-configured one. Since RPC requests and replies all currently flow through the same exchange, making this change should be backwards compatible. Is the fallback to the configuration-specified exchange actually necessary?\nI would love to see an end-to-end test for this use-case still, if nothing else than because tests are (currently) the best documentation we have. It is possible to declare an AE with the client, which should make doable during a test-run.\n. > To implement the alternate exchange through policies (the preferred solution for RMQ) , I think I need to use the python rabbitmq_admin library https://pypi.org/project/rabbitmq-admin/. What do you think about that?\nThis library wasn't around when Nameko first needed to manipulate the RabbitMQ REST API, so we implemented our own wrapper in nameko.testing.rabbit. We could replace this with a dependency on rabbitmq_admin. But since (I think) RabbitMQ 3.6.7 the management API is harder to use -- the HTTP request returns before the changes are applied, which makes for flakey tests.\nEven though AMQP declaration of AEs is \"discouraged\", if it works then I think it's a better approach for the specific use-case of Nameko's test suite. And for this specific use-case, the exchange will never previously exist, because each test is run in its own vhost.. Thanks @daviskirk!. @vlcinsky 3.0.0rc branched from master before these changes landed. It will be merged at some point soon. \nv3 should also be released soon.\n. > TL;DR - no guarantee that your task will complete if your service crashes. If you gently shut it down, I think the default behavior is to wait for the current active tasks to finish before shutting down, so you'll get a result from the call above, but if your service crashes you should expect the tasks running on it to crash as well\nThis is correct. If your service crashes, the unack'd request message will be reclaimed by the broker when it detects that the consumer is gone. It's then available to be picked up by another consumer, and the RPC client will happily wait until it receives a response message from somewhere. There are currently no limits to the number of times that a message may be requeued.\nThe messaging guarantees are \"at least once\" delivery -- your request message will be processed at least once, maybe more than once. You should aim to make your RPC entrypoints idempotent because of this.\nIf you gracefully shut down a service, it will wait until it's finished processing all the current workers, and the request messages will be ack'd in that case.\nThere's no difference in behaviour between sync and async RPC except that the client doesn't wait for the result message.. That's a good point. Introducing this as a new feature would make it easier to communicate too.\nWe use the thin wrapper pattern in the standalone package. In the v3 branch I renamed the standalone ClusterRpcProxy to simply RpcProxy, which is unfortunate if we want to alias the ServiceRpcProxy DP to RpcProxy for backwards compatibility here.\nI think the best thing to do is roll back that change so we end up with a ServiceRpcProxy and ClusterRpcProxy in both the standalone and normal packages. We should add an RpcProxy alias to ServiceRpcProxy in the normal package to maintain backwards compatibility, and perhaps switch the new standalone RpcProxy alias to point to the ServiceRpcProxy instead, so it matches.\n. Ha! Yes, Proxy has always been awkward. What do you think about the following?\n\nRpcProxy / ServiceRpcProxy => ServiceRpc or ServiceRpcClient or ServiceClient \nClusterRpcProxy => ClusterRpc or ClusterRpcClient or ClusterClient\n\nOr possibly one object called RpcClient with the API as currently implemented, service name optional. Maybe less confusing if we're replacing the existing API with a brand new one?\nI think my preference is ServiceRpc and ClusterRpc.\n. I have ended up using ServiceRpc and ClusterRpc for the DependencyProviders, and ServiceRpcClient and ClusterRpcClient for the standalone clients. Mostly because we have several test files that import from both modules.. @davidszotten are you happy with the changes here now?. > not sure i'd say a ServiceRpc is a ClusterRpc etc\nHave I said that somewhere?\n. Sorry, still confused. ServiceRpc is a pre-targetted ClusterRpc, which happens to be implemented as a subclass. Have I said something to the contrary somewhere?. Oh I'm with you. I guess it would be nicer to use composition rather than inheritance, but I don't think this is the first use of inheritance in Nameko that isn't strictly a \"is-a\" type relationship.\nI will merge this and cut another pre-release for the 3x branch.. @davidszotten are you good with this?. Thanks @shirishc! The old list has actually been completely imported into Discourse, so I think you can drop the old link.. It seems like this is caused by the eventlet monkey patch. Not easy to fix I'm afraid.\nThere's a related stack overflow post here: https://stackoverflow.com/questions/38448305/can-not-remote-debug-if-there-is-eventlet-monkey-patch-in-code. @daimon99 that code applies the monkey patch that enables eventlet's cooperative yielding.\nPycharm would stop crashing, but your Nameko service wouldn't run anymore ;)\nGoing to close this issue because there's nothing we can do in Nameko to mitigate it.. This seems like a bug in eventlet's greendns module. Which version of eventlet do you have installed?. There have been a bunch of changes to greendns in the last handful of releases. I would try rolling back through the versions and see if there is one which fixes it. If you can demonstrate that it's an eventlet regression I'm sure the folks over at https://github.com/eventlet/eventlet would appreciate a bug report.. @tyler46 thanks for the PR.\nThe difficulty here is that this specific example requires RabbitMQ, because it's using the built-in AMQP RPC features. I think it'd be sufficient to just say that.\nOn the project page we feature an RPC example and an HTTP example, but it may be distracting to have both here. The RPC one has the advantage of also being able to show the shell usage underneath.\n. I assume you don't mean OS-level process, but rather an RPC method call?\nNameko RPC isn't a drop-in replacement for Celery's tasks. You can't abort a method once it's started.\nYou could implement something like Celery's tasks in Nameko without too much trouble, but that's not what the RPC extensions provide.\nAlso note that the forum is a better place to ask questions like this.. Travis doesn't restart the pipeline if a failing job is retried and then passes. You have to manually retrigger all the subsequent pipeline stages (even those marked \"allow failure\") before the status of the whole build is updated.\nThis looks good though, thanks! . We pipe input to the shell quite frequently, as a way of essentially writing small scripts that use the n helper. It is helpful to bubble exceptions up to the process level in this case.\nThe alternative is to write a standalone script, but you have to reimplement all the config parsing and helper setup.\n. This is happening because the local http module is shadowing the http package in the standard library. \nImportError: No module named 'http.client'; 'http' is not a package\nThe \"another exception occurred\" message is a bit misleading. That's an exception thrown while werkzeug is discovering that urllib2 is not available, and is correct behaviour.\nI'll change our example so it suggests using a different module name. Thanks for reporting!. Gah, yes it is! Sorry @timbu, I completely forgot about that PR.\nInteresting that this was introduced for the non-RPC extensions before the big refactor that landed in 3.x.. I guess we should land #573 and then bring master into the v3 branch.. > @timbu @mattbennett It's not just Publisher of messaging module, EventDispatcher of events module needs to be changed too, see 5c416ce\nAh, quite right. And that means #573 is missing the equivalent fix for the event dispatcher.\nI agree, let's make the fixes here, then #573 can land, then we'll merge master into the v3rc branch.\n. Hello @tkang007. Thanks for the kind words.\nYou ought to be able to use whatever protocols you like for connecting the front and backend. Either HTTP or AMQP would be fine. The AMQP PRC protocol is very simple so would not be too tricky to implement, assuming C# has a decent AMQP library.\nIf you're feeling experimental you may like to try out https://github.com/mattbennett/nameko-grpc and use the C# GRPC client.. If your Config is the built-in DependencyProvider, then it must be used inside a service method (as per the linked docs). Outside a service config = Config() will give you  theDependencyProvider instance and not the dependency that it injects into the service.\nYou might want to take a look at the prerelease of Nameko 3 (pip install --pre nameko) because it adds a global helper that gives you access to the config. In Nameko 3 you can do simply:\n``` python\nfrom nameko import config\nconfig.get('whatever')\n```\nThe pull request which added this functionality is https://github.com/nameko/nameko/pull/520\n. There must be some incompatibility in the pattern used in the nameko CLI and the regex library installed by Anaconda.\nWhich version of the regex module did you have installed?. It's tested against 2018.2.21, so that version at least should work. Would be interesting to see which subsequent release breaks it, and whether there are any notes in the changelog. I would hope that there is a workaround.\nI think it's preferable for us to support later versions even if that means dropping support for the older ones. Clearly we're failing to do so at the moment.. This is almost certainly a bug in eventlet.green.subprocess. Eventlet monkeypatches the standard library subprocess module to invoke the green version instead.\nYou can demonstrate that it's an eventlet bug by reproducing without Nameko, but with the monkey patch applied (by calling eventlet.monkey_patch() early in your script).\nUnfortunately there is no workaround possible in Nameko; we need the monkeypatch to be applied.. You can do this with Nameko 3.\nIn Nameko 3, config is a global helper, so you can do almost exactly what you have in that snippet:\n``` python\nfrom nameko import config\nclass MyService:\n    name = \"my_service\"\n@http('GET', config.get('endpoint'))\ndef some_op(self, request):\n     print('some_of called')\n\n```\nNameko 3 is currently available as a pre-release. pip install --pre nameko to install it.. Branch is failing because it also needs to handle pyyaml upgrade. This is being addressed in other current PRs so waiting for them to land rather than fixing it here.. Also, CI is failing here because of a recent release of pyyaml. This is being fixed in other branches so merging master after they land should fix the failures here.. I like the idea of catching this at class declaration time.\nNot sure what the rest of this comment means. Is it still relevant?\n. typo\n. Errant comma here\n. These TODOs need completing. What's the difference between the Standard example and the Singleton example?\n. Jan <3 commas\n. typo\n. Couple of typos here. I think this comment would be better in the module docstring\n. Still TODO?\n. Do we want the serialisation to happen here, or in the caller?\n. You don't need all these commas!\n. teardown_function in test_messaging.py used to do this. Was it incorrect?\n. Can't we do this with pytest fixtures rather than funcargs?\n. Let's remove this if it's old syntax\n. Did you put these back? I thought we moved them into the worker context.\n. Should we set the attr back to the AttributeDependency instance?\nsetattr(service, injection_name, self)\nOr is that pointless because the service instance is about to be destroyed?\n. I like this :)\n. What?\n. Presumably itermessages blocks until something is received, never raising a StopIteration?\n. It doesn't need to be, but I slightly prefer this pattern to creating it in the __init__\n. Not required because the dependency stop will wait for jobs to complete?\n. This depends. Is there a usecase for a provider that may want to interact with the result of a call, even if it didn't initiate it? Like a logger perhaps, or some kind of performance monitor.\nThe result / exception isn't kept in the worker context anymore, so we can't process the information in call_teardown either. Maybe we need another method?\n. This should probably be a DecoratorDependency\n. typo\n. Yikes. Is this new, or just how nova does things?\n. Should we move this into the contrib directory? Maybe even the nova stuff should go in there.\n. Why are we passing service_name in now?\n. Can we have a docstring here please. Not entirely sure what it's for\n. .last and not .next?\n. Do we consider this a feature, or a necessity to support a consumer that's backwards compatible with nova?\n. This is a merge error. It doesn't need to be in start and on_container_started\n. I know we talked about this earlier, but patched_attr_dependency is a better name...\n. Like it\n. Why is this required?\n. ?\n. Nova lets you publish results in multiple messages?\n. BrokenQueueConsumer(amqp_uri=None, prefetch_count=3) would be more obvious\n. Is this invocation order correct? Why the need to reset the event?\n. Why do we need to sleep here?\n. Inline comments would be helpful here...\n```\npatch connection to raise an error\ntry to start the queue consumer\nwait for the queue consumer to begin starting and then immediately stop it\n```\nI'm guessing here because without docs it's not clear!\n. None of these valid anymore?\n. Again, kwargs here would make the test easier to read\n. Feels suspiciously like we're exploiting a bug here. Can't we use a vhost name that doesn't start with a special character?\n. nameko/nova.py -> nameko/contrib/nova/rpc.py\nnameko/proxy.py -> nameko/contrib/nova/proxy.py\nGo on then...\n. Typos, corrected below\n```\nAllows the user to serve a number of services concurrently.\nThe caller can register a number of service classes with a name and\nthen use the start method to serve them and the stop and kill methods\nto stop them. The wait method will block until all services have stopped.\nExample:\n``\n. I thought we decided to keep this?\n. Does resetting the event immediately after firing it definitely have no impact on listeners? There's no possibility that they'll miss it?\n. Aneventet.sleep()isn't guaranteed to allow this to happen though, is it? Can't this thread get rescheduled before the QueueConsumer has stopped fully?\n. Yeah this is tricky. In the worker context they're not prefixed with anything either. Anameko.lang` \"header\" in a message becomes just a \"lang\" key in the worker context data. The prefix was to make debugging clearer \"over the wire\", which doesn't make much sense outside of a messaging implementation.\nA better catchall term is perhaps \"meta\" because it doesn't imply messaging. But changing the name doesn't solve the prefix problem...\n. This logic originally lived in the providers, but there wasn't a clean way to specify your own implementation. With RPC for example you have to define your own Consumer and Responder. In contrast it's easy to specify a custom WorkerContext. I agree that the above feels a bit clunky though.\n. Yep. I've added a comment to each of the examples explaining why.\n. MessagingPublisher?\nI've added a note about the intended usage of messaging to the readme\n. You're right, we don't. I've nixed it.\n. pylint: E211 whitespace before '('\n. Because the exception already gets printed by _log.error when you provide exc_info, and for RemoteError we include the remote trackback in the stringified version.\nWithout the above change we see the remote trackback twice, one either side of the local traceback.\n. So it is!\n. Kind of. It's enforced by the ServiceContainer, which \"dies\" KILL_TIMEOUT seconds after asking the dependencies to stop. We're leaving the response to a dead container up to the application (no sys.exit in nameko code) so it's more correct to say that it may be forcibly stopped. I'll update the docstring accordingly.\n. You can't kill a whole pool, so we're now tracking active worker greenthreads\n. Nope, --fail-under requires coverage>=3.6 :(\n. If we're going to avoid this problem by running coverage externally (eventually) then let's merge this for now. Just don't make any files called TOTAL\n. No, it's explaining the different ways the decorator can be used, i.e.\n@foobar\ndef foo():\n    pass\nor\n@foobar(arg, kwarg=\"kwarg\"):\ndef foo():\n    pass\n. I changed this to bring it in line with ConsumeProvider, but none of the other DecoratorProvider subclasses have a ...Provider name.\nPublisher is fine, but Consumer seems somehow too generic. It's worth bearing in mind that these two are lower-level than the rpc and events providers.\n. stopped\n. Why not? Any error here would be unrecoverable, so it seems reasonable to just forcefully kill them.\n. s/semy/semi\n. ourselves?\n. s/uexpected/unexpected\n. safe? sure?\n. This is already assigned in the init, no?\n. Can we have a comment explaining that this will wait forever? Or that the only way out is via an exception.\n. This should be in a timeout, and really with an inline comment explaining that it won't be fired unless do_wait gets killed\n. s/excepetion/exception\n. That there are no extra workers is not being verified\n. Yeah. I've reverted and apply them to the classes now though.\n. Since ParallelProvider is an InjectionProvider, you can use self.container here. See DependencyFactory.create_and_bind_instance\n. There shouldn't be any mentions to Mocks outside the tests or testing helpers. It'd be better to test with something that can be wrapped, or add the required attributes to the Mock. Maybe a WrappableMock class?\n. thread_provider is just a ServiceContainer instance right? Probably better to call it .container until we're thread-agnostic.\n. Docstring:\nHandle the completion of a thread spawned for a future.\n. Docstring:\n`\nSet a result or exception onfuture``\nCalled when the future's function completes, inside the spawned thread.\n``\n. Won't this set an exception for every successful future?\n. By virtue of greenthreads I don't think this is required.\n. Let's call thisgtfor consistency\n. This should benamerather thanitem.\n. Should we add asetattrthat raises an exception with this message?\n. What about using a lambda function that proxies the call onto the mock?\n. The \"you\" here is ambiguous. I think you can say \"Set the future running immediately. It's not possible for it to have been cancelled\".\n. This can't be used as a context manager, only theParallelProxyManagercan. Can we move ParallelProxyManager.**call** into this class so it can be? (And remove the Manager)\n. No need to save it toself.if we won't reference it again\n. OK cool. Let's move it to a utility with a docstring explaining that it unobtrusively fails if the argument can't be wrapped.\n. Ah yes, of course. Maybe mention that situation in the comment?\n. Actually that won't work. But you could put **call** as a nested function inside acquire_injection. If we keep the class, can we give it a better name? MaybeParallelProxyFactory`?\n. I'm happy with the ServiceContainer / ManagedThreadContainer split. But can we group the under-methods and the public methods together?\nElsewhere we (mostly) follow in the pattern:\ndunder\npublic\n_under\n. This name confuses me. What is this testing?\n. Maybe make this a specific exception, so you can't accidentally catch something else going wrong.\n. Expand cm in the function name here\n. publisH\n. Not any more\n. its\n. One disadvantage of this approach is that it removes the structure of the uuid4. Base64 uses - and _ so you can get strings with different length \"sections\". I find that and the expanded character set make them more difficult to read at a glance. Just my two cents...\n. Does this print nicely?\n. This is fine but we should invest some time in a test helper that lets us poke an entrypoint / spawn a worker directly.\n. Nice!\n. This is enormous. We need to truncate because the number of calls in a chain is unbounded, which means we need to rely on some other tool to explore the entire chain. The reason for tracking parent calls is just to enable that other tool to recreate the whole pattern.\nTechnically I think you only need to track one parent to do this, but ~10 would make manually debugging easier.\n. On reflection I think I'm against this. It's odd to use this form inconsistently, and as noted below the call stack does not need to be very long.\n. Why is this not a docstring? Also n is self.parent_calls_tracked\n. What is transfer here?\nI wonder whether we should rename:\n- data_keys -> context_keys\n- data_for_transfer -> context_data (and make it a property)\n- context_from_transfer -> get_context_data\n. wait_for_end?\n. How about this?\n```\n    class ProxyContainer(object):\n        \"\"\" Implements a minimum interface of the containers.ServiceContainer\n        to be used by the subclasses and rpc imports in this module.\n        \"\"\"\n        service_name = container_service_name\n    def __init__(self, config):\n        self.config = config\n\n```\nIt avoids using the word \"Fake\" at least...\n. Yep, I like both of those ideas\n. It's mentioned in readme.rst, at the top level, which I think is sufficient.\nGood idea to link to the RabbitMQ tutorials.\n. You mean rename \"Messaging Example\" to \"Advanced Example\"?\n. Yep. The context manager is just used to return the producer and connection to their respective pools when it closes. So you can nest as deeply as the pool (default size is 1000 I think).\nHow do you mean \"play nicely\" with the rpc proxy?\n. There's no problem here. I verified locally but I don't think it makes much sense to keep it as a test.\n. What is this used for? Might be neater as a property. \n. This should be a docstring:\n```\nContextual data to pass with each call originating from the active worker.\nComprises items from self.data where the key is included in context_keys,\nas well as the call stack.\n```\n. This call is so simple I'd be tempted to put the logic inline:\nself.call_id_stack = self.parent_call_stack[-self.parent_calls_tracked:]\nself.call_id_stack.append(self.call_id)\n. What's the use-case for this? Maybe make it a property on WorkerContext like context_keys is.\n. There's a triple \"t\" here\n. I meant to rename e actually.\nwait_for_service_start and wait_for_responses would be good names.\n. This is a strange API. When we discussed this I imagined something like:\n```\nwith service_runner(config, ServiceX, ServiceY, ...):\n    # do stuff; services all started\nexited; services now stopped\n```\nWhen would we use runner.stop() or runner.kill()?\n. There are several examples of this failing on Travis:\nhttps://travis-ci.org/onefinestay/nameko/builds/15574391\nhttps://travis-ci.org/onefinestay/nameko/builds/15285366\nhttps://travis-ci.org/onefinestay/nameko/builds/14608400\nAnd plenty more.\nI can't reproduce it locally, but I suspect it's just that the response_greenthread isn't being set up in time, so the RPC message gets discarded by the exchange. Hopeful fix here: https://github.com/onefinestay/nameko/pull/95\n. Not really. I did it to encourage use of the correct utility for the different types of tests.\nBut given our conversation about entrypoint_hook not being a true integration utility anyway I might merge them and let the examples inform best practice.\n. Good point. This is perfect.\n. This is an old example that isn't really relevant anymore.\nI think patch_injection_provider itself should be deprecated, because what it attempts to provide (at least in that old example) is now done better by the instance_factory utility\n. Actually you can do this with the literalinclude directive. I thought it might be useful to have a readable example inline, but they are exactly the same. I'll include the example instead.\n. Probably not. I'll remove it until we definitely do need it.\n. In practice it's not possible because services are keyed on service name, which is determined from the class. If you add the same class multiple times it has no effect.\nI'll add a test for the service runner to demonstrate as much.\n. Yep, good idea.\n. No, I think we want to stop gracefully. Otherwise we risk leaving state behind for the next test.\n. Good catch. I think it's better to verify the messaging setup though (with pyrabbit) rather than wait for an arbitrary timeout.\n. OK I see. Perhaps make it a property since it has no arguments. Then it can be overridden with a class attribute in simple cases.\n. I tried to move the logic to a function as we discussed offline, but couldn't make it any clearer.\nI've expanded the inline comment instead. It's duplicated but I think it's clearer to have it inline.\n. Yeah I don't know what to do about this. We could keep this conftest.py and make all the other examples in the \"testing\" directory also runnable with pytest, rather than as standalone files. It doesn't reduce the duplication but might be more coherent. \n. That's a very good point. I added these lines before decided that it was better to run the example with pytest.\nI reckon putting them in conftest.py would be sufficient. runtests.py collects all the sources and encodes them into base64, which we don't need.\n. No, we don't. Our setup.py says we target 2.7 and until we decide otherwise and add a tox config let's stick to modern syntax.\n. No. I'm not sure why that pattern emerged. It should be pretty simple to replace it with something more normal.\n. Yes, quite a few - rpc_proxy, messaging publisher, parallel_provider. Anything that takes arguments.\nWe could add a utility that generates an injection in this simple case e.g.\nshopping_basket = make_injection(ShoppingBasket)\nOr you could add a switch to the existing injection so it can be used in the same way:\nshopping_basket = injection(ShoppingBasket)\nIt would save some boilerplate but at the expense of adding a secondary path to achieve the same thing.\n. Sorry, this is more of a mental note that got left behind.\nThis runner_factory is slightly different from the one in test/conftest.py because it takes the rabbit_config from the fixture signature, rather than expecting it to be passed in to make_factory.\n. An argument for keeping things as they are is it gives you an opportunity to write both developer and user (i.e. service developer) docs. Developer docs go in the provider class definition. User docs go in the @injection decorated method.\nThe impression of boilerplate is amplified when you're seeing a full test like this, and I really like the separation of documentation. The service developer shouldn't see or care how the injection is implemented.\n. The QueueConsumer is a specific entrypoint dependency, that may or may not be in use.\nI think you want to kill all entrypoints here, which will include any event handlers, timers etc.\nself.dependencies.entrypoints.all.kill()\n. Good spot. This was actually lifted from platform, so we should make the change there too...\n. I think kombu. This was taken from the main conftest.py. I remember asking about it previously and there was a good reason for it (although I now can't remember what it was)\n. I've changed it to refer to instances of exception_type, which does strictly include instances of any subtype.\n. If we wanted to check whether the collector currently has any providers, we can check the size of the set. Here we need to know whether any providers were ever registered, because then we should wait for the event.\nSo I think \"had\" is clearer. We could give it a less clunky name if you can think of one though.\n. On re-reading, so do I. I'll put it back.\n. Seemed a bit pointless considering the pending merge of these two classes. It's even more necessary now we're stopping the entrypoints before killing the workers.\n. This adds an extra second to the tests and doesn't exercise fail_fast_imap. It's sufficient to say that pool.imap will block, rather than demonstrate it.\nBetter to make good_call fire an event after some timeout, and demonstrate that it never gets there because failing_call triggers the fail fast. Then you can get rid of the blocking_call as well.\n. I think we should just call it service_runner. There are already too many things called \"context\" and it's obvious that it's a context manager.\n. If events was a set you wouldn't have to sort it later\n. Oh yes, I like that.\n. Must have been @kollhof being lazy ;)\n. No, but I think it would be a reasonable thing to have. There is also a cleaner way of doing this.\n. Not really, because there are three classifications: injection, entrypoint and plain dependency, which are the nested ones. \n. We need to return the replacements in the same order as the names, so you have to iterate over injections. But the second loop can collapse because you can't have two injections with the same name.\n. Neater and faster!\n. This function is only testing whether the value is None, so the name and log message are a bit disingenuous. It's so specific I don't think it warrants a separate function.\n. Also verify the logging?\n. A bit neater? It keeps the logging separate.\nif None in worker_ctx.context_data.values():\n    _log.warn(\"Attempted to serialise a header ('{}') with the value `None`. \"\n              \"The header will be dropped from the payload.\".format(key))\nheaders = {self._get_header_name(key): value\n           for key, value in worker_ctx.context_data.items()\n           if value is not None}\n. We do this in test_container.py, but there we're really using it to track the code path.\nI think you're right - it'll add a burden here without much value. We're exercising the code, which is the most important bit.\n. No, the aim here is just to get the start() method to exit. We're already catching the exception by wait()ing on the greenthread.\n. It's mostly there so that we can find out. It is completely recoverable now that we've fixed the bug, so perhaps downgrading to info is appropriate, but we'd then be less likely to notice dodgy connections. I think it's appropriate for the framework to warn; we can always filter the logs if our environment turns out to be very unreliable.\nBy virtue of the previous bug we could grep the syslogs to find out how often it happens in practice. I'd rather not though!\n. Good idea. Will also raise a separate pull request, because we do this all over the place.\n. The reason is in there as an inline comment. I've promoted it to the docstring because it's clearer there.\n. By a quirk of autospec it doesn't work though :(\nSee the note at the very bottom of http://www.voidspace.org.uk/python/mock/helpers.html#autospeccing\n. This seems like a powerful reason not to use a separate metadata module. What advantage does it bring?\n. Need a full stop on the previous sentence for this comment to read correctly\n. This comment doesn't explicitly say that with publisher confirms enabled, a channel.wait() is guaranteed a response, which I think is worth mentioning.\n. I had assumed the same as @davidszotten here. The most common interaction with PyPI is just to pip or easy_install the packages, which throws away everything else. But PyPI is not just the backend for a package manager (arguably barely that) and we can expect devs to look there for the whole project.\n. The implementation here could be less obtuse:\n``` python\ndef is_rpc_service_method(method):\n    if inspect.ismethod(method) and hasattr(method, 'nameko_entrypoints'):\n        entrypoint_dep_classes = {\n            factory.dep_cls for factory in method.nameko_entrypoints\n        }\n        if any(issubclass(dep_cls, RpcProvider)\n            for dep_cls in entrypoint_dep_classes):\n                return True\n    return False\ndef get_service_rpc_method_names(service_cls):\n    return [name for (name, method)\n            in inspect.getmembers(service_cls, is_rpc_service_method)]\n``\n. My objection to theeqandnemethods, voiced offline but not here, is that you're adding code to the implementation which is only used by the tests. Even if the test helper that you have to write is ugly, it's better than polluting the implementation.\n. Nothing necessarily has to be waiting, but it's notstop()s responsibility to raise. We only wait on the event becausestop()should block until the dependency has actually stopped (or completed its kill, in this case).\n. This fixes a bug inQueueConsumer.kill- it jettisons all the providers that were registered with it, but they still try to unregister in their respectivekillimplementations.\n. This is related to the change todiscard` / unregistering a non-registered provider becoming a no-op. If the provider wasn't in the set it might already be empty, and so we'd try to send the event again.\nI reverted the implementation to remove but now exit early if the provider isn't in the set, which is more obvious.\n. I agree. I've changed this occurrence and would be happy with a moratorium on these methods. They get used quite a lot throughout the tests, but it's no harder to write\nassert mock.call_args_list == [call()]\n. Very nice\n. You don't actually have a provider instance at this point, just the factory.\n. All exceptions are serializable by RemoteErrorWrapper. Only some of them can be deserialized back into anything other than a RemoteError.\n. Quite possibly. Maybe this will clear up the serializable / deserializable confusion above?\n. Ah, yes. Since this test has so many parameterised options, it seems a bit silly to spin up and tear down the service for every one. Unfortunately you can't just change the scope here, because the other fixtures make assumptions about a clean rabbit. Will remove the TODO and create a ticket.\n. As far as I can tell, we don't need to be mutating the body, and by doing so it means we can't later inspect it for the msgid to send the error with.\n. I'm in the same boat. I think it's preferable (more descriptive) than just deserializable though\n. When vhost doesn't exit. Seems a bit silly to write a test for it.\n. Raised by :meth:`Container.spawn_worker` if it has started a ``kill`` sequence.\n. providers\n. Slight edit:\nWe need this because eventlet may yield during the execution of\n:meth:`Container.kill`, giving entrypoints a chance to fire before\nthey themselves have been killed.\n. assert waiter.wait() == \"result_a\" perhaps? \n. I don't understand this test. Why is this the case?\n. nameko's pylint is quite strict, and can't figure out dependency injection. E1101 is \"variable has no member\".\nE1123 isn't actually required here (I copied this from nameko.rpc, which does more magic)\n. I've added tests instead. They're a little vacuous but they do unit-cover our implementation.\n. This was actually patching the publisher in the standalone rpc proxy, so a bogus test.\n. Nope!\n. I don't think this is possible. As discussed you see a PASS followed by ERROR\n. Actually to wait for entrypoints to complete, right?\n. Reminder to add a test to show normal usage here.\n. Not quite; expected_exceptions is None if the provider supports it but there's no value provided.\nI changed the getattr default to None  though for consistency. \n. Add a new exception MalformedRequest that rpc providers can raise if they detect an invalid message. Raise this exception in the default RpcProvider if args or kwargs keys are missing from the message.\n. Call this test_rpc_invalid_message?\n. Maybe test_handle_message_raise_malformed_request\n. And handle_message_raise_other_exception\n. Good idea\n. What happens here if the iterator receives a message with a correlation id that it's not expecting? It continues to wait for itermessages to yield another one, right?\nWe have tests to verify that unknown correlation ids are discarded, but not that the proxy continues to wait. Which should probably be considered a hole in our coverage...\n. I wonder whether this should be an underscore method.\n. This test was duplicate\n. This is a bit icky but as discussed hopefully going away soon anyway...\n. Perhaps add a little note here that we only get here in the exceptional (i.e. disconnect) case. Most of the time when we're ready to consume there won't be any pending reply events.\n. This last bit isn't quite clear. In what situation are we leaving the \"current\" proxy unusable? And how?\n. So here we're yielding the connection error and waiting to be invoked again with a new correlation_id, at which time we loop back to the top of the while True and wait again on itermessages?\n. Yeah, probably. None of them really test the broker. Almost all are RPC tests, but not everything. They could be split up into their respective areas (rpc providers, proxies, standalone proxy + some other places) or we could rename this module to test_disconnects.\nDo you know of any project that uses labels or tags to manage this stuff? Would be nice to be unshackled from this kind of one-dimensional categorization.\n. Why implement this retry method rather than call method twice inside the test?\n. Oh of course, got it.\n. (since reply queues auto delete).\n. The causality here is a bit confused. How about the following?\n```\ntimeout is implemented using socket timeout, meaning the connection\nis lost when it fires, causing the reply queue to be deleted\n``\n.Queue.bind` doesn't mutate?\n. How about doing this with two different methods (or one that accepts a sleep argument), rather than the dance to create a queue with no consumers?\n. Not exactly forever ;)\nDo we need to test this? Or rather, can we? All this demonstrates is that the call doesn't complete immediately if there are no consumers.\n. This comment is out of place now. Below also.\n. I think this is slightly better than what we discussed offline. Firstly, I don't think it's unreasonable for us to expect that service and method names are strings. Secondly, you get much nicer output if you actually use a unicode character. For example with a service name of \"f\u00f6bar\":\nUsing !r in the format string:\n``` python\n\n\n\nstr(dep)\n    \"\"\nprint dep\n    \n```\n\n\n\nExplicitly encoding the strings:\n``` python\n\n\n\nstr(dep)\n    \"\"\nprint dep\n    \n``\n. typo\n.ClusterProxynow\n. Is_setuppreferable to aniniton the base class and asupercall?\n. multi -> cluster\n. double indent here\n. typo here\n. It is a bit weird. If we make the superclass responsible for calling the container lifecycle methods instead then we can definitely refer to the entrypoints and injections/dependencies explicitly.\n. Nope. This was as straightforward as I could make it with the container knowing about all extension instances. I can't believe I didn't think about using the superclass!\n. This is an experiment / talking point. The idea is to pass a limited interface of some sort into the extensions, rather than giving them unfettered access to the whole container.\n. Yep, was just waiting for an initial pass on the review first.\n. I'd forgotten what we agreed. Using aSharedExtensionsuperclass is nicer.\n. Yeah that might be nice\n. It's not the entire state of the extension no, only the initialisation parameters. Maybe__params`?\n. Good idea\n. It's possible we'll lose these. It is mostly me asserting that my implementation worked correctly, but you're not supposed to make clones of clones. It might work but it's not correct.\n\n\n\nAn alternative would be to delete the clone method during the clone process. That would ensure safety without the extra code, but might be more difficult to understand for any developer coming across it.\n. Sorry, yes. The docstring here is only a token effort!\n. Wrapt only supports decorators with optional kwargs, not both args and kwargs. I guess the comparison we're doing below is considered icky - it means you can never write a decorator that genuinely accepts a function as its first arg e.g.\n``` python\ndef foo():\n    pass\n@bar(foo)\ndef baz():\n    pass\n```\nI think it's fine not to support this though.\n. This works fine. I will add some test coverage\n. I've done away with the ExtensionSet and we now deal with the three categories explicitly. I'm not super keen on subextensions as a name but that is what they are. \n. I don't think it's possible to abstract away the differences, at least currently. \"Dependencies\" can be injected into service code; they are the top level. Whereas \"extensions\" is the catch-all for everything managed by the container, including entrypoints.\nWe danced around having entrypoints and dependencies manage their own subextensions so the container didn't need to know about them. With more intelligent stop behaviour that may be be possible, but it currently isn't. If it does become possible then it'll be pretty easy to transition to it from here, and the ugly \"subextensions\" variable will go away.\n. Yep. I will at least make it a weakref for now\n. Good call\n. Yeah, there's potentially quite a lot of I/O going on in acquire_injection, depending on how lazy the injections are.\n. It's not safe to use another dependency during teardown (there's no guarantee of order; teardown all happens in parallel) so I don't think there is a use case. The worker is about to be discarded so I don't think it's really required.\nNot sure I follow your last sentence though.\n. No. You're most likely to hit this in tests where you're manually creating extensions.\n. Good call\n. Whoops, probably a good idea!\n. I think these tests precede is_clone. Will update them\n. collect.stop doesn't yield, so this is fine. The ProviderCollector is slated for deprecation anyway...\n. We don't have one but I wouldn't be opposed to it.\n. We won't when we start stashing the container.\n. I think so\n. Yep, is_clone will become is_bound. Cloning is an implementation detail.\n. I've reverted to using an inject method on Dependency for now, which works but seems a bit backwards to me.\nI find the SpawningSet a bit clunky really. I'd rather have a spawning_imap helper or similar that just accepts a callable. Would be more explicit.\n. No, sub-extensions can be added at init time\n. What makes you say that?\n. I think for this PR, yes\n. Did we land on unicode instead of repr?\n. actually true now ;)\n. I don't think we need this\n. :)\n. duplicate\n. funky indent\n. You could define event_dispatcher here and do this import the other way around.\n. Why is this one different?\n. yeah let's drop the whole thing. we can add it back eventually with more prose around the various sections\n. Hmm. Probably best to drop it from the docs for now, at least this page.\n. Yep, I'm going to add a whole page on serialization and will link to it from here.\n. Ah, wishful thinking. I'd like to change the discovery mechanism so this would be valid. Will make it right for now though\n. Better example than 302 for a status that includes a header in the server response?\n. Nice idea. I'll just fix the code for now though\n. Yep. Actually recommended by PEP8\n. I figured it was better to help distinguish between the arbitrary choices. a/b work together, as do x/y\n. I thought the values were unpacked, but it looks like I was wrong.\n@davidszotten was this the original implementation?\n. Happy to use an alternative example if you can suggest one.\n. :+1: for typography critique\n. I take your point about \"firing\", but that's why it's in quotes. It's tricky because we're actually not making RPC calls or dispatching events, instead triggering the action that would happen as if we had.\n. I want to avoid future because we don't match the concurrent.futures API\n. Thanks @marbru. I'll add this when we finalise the http behaviour.\n. Yeah this is a bungled explanation of the mistake we made with nameko-chat. I think I'll drop the whole paragraph.\n. Yeah, I will probably split this into different sections so it isn't so imposing.\n. Yeah there's a whole missing section on deployment or possibly a \"service development guide\" describing some of the patterns we use in platform (that aren't enforced but we think are sensible) \n. Yep this is confusing. It's about to get moved out into a separate codebase as a \"community extension\". Hopefully that will also help clarify the whole extension/entrypoint/dependency confusion.\n. Good point, will add\n. You can't specify multiple classes? e.g. nameko run foo:Foo bar:Bar\n. No longer true\n. By default it will find classes that look like services (anything with nameko entrypoints), ...\n. Could do with some usage examples in here.\n. Why is this now services?\n. defaulting to JsonProtocol?\n. We don't log these kinds of errors in other entrypoints. error seems a bit severe\n. handle_message feels like leakage from the AMQP entrypoints. You could check the signature and spawn the worker inline.\n. Lost my previous comment on this. We don't log this type of error in the other entrypoints. error seems severe.\n. - see HttpRequestHandler.handle_request\n. Without shallow mode there's a risk of code accessing form data too early\nand deadlocking because the browser isn't reading from our socket.\n(Better phrasing but I'm not sure I understand the situation)\n. Some assertion about the contents of the error?\n. What is the expected behaviour here?\n. Also here\n. How is this http \"only\"?\n. I've added a section to the \"key concepts\" section introducing the container. I'll also tidy up these examples so we don't use so many different utilities.\nThe difference BTW is that get_container extracts an existing container out of a ServiceRunner (which is just a wrapper around multiple container) and ServiceContainer actually constructs a container. container_factory is a test fixture that also constructs a new container.\n. They're not strictly for integration testing, just useful in that context. Putting them under the integration testing heading suggests that you shouldn't use them elsewhere, which isn't the case.\n. what does posargs do?\n. envtmpdir is presumably a tox provided thing?\n. Perhaps not. I was matching the interface of expected_exceptions.  A common usecase would be something like:\npython\nclass Service(object):\n    @rpc(sensitive_variables=(\"foo\",), expected_exceptions=FooError)\n    def method(self, foo, bar):\n        pass\nThe parentheses seem slightly superfluous given we don't require them for expected_exceptions.\n. Good catch\n. I find this more obvious -- easier to distinguish it amongst all the other parenthesis. Happy to change if you'd prefer though.\n. Quite right, thanks\n. Eventlet will bind to 0.0.0.0 if you pass an empty string for the address, so 0.0.0.0:8000 would be explicit.\n``` python\n\n\n\nsock = eventlet.listen(('', 8000))\nsock.getsockname()\n... ('0.0.0.0', 8000)\n```\n. Re https://github.com/onefinestay/nameko/issues/236, I think this is as good as we can do. Kombu doesn't give you anything more specific than \"socket closed\"\n. I like this idea. I'll back it out for a separate PR though\n. Is it possible to do this with a subclass, to avoid lifting this code? e.g.\n\n\n\n``` python\nclass TransportTester():\n    Connection = ConnectionTester\ndef verify_amqp_uri(uri):\n    conn = Connection(uri)\n    transport = TransportTester(conn.transport.client)\n    transport.establish_connection()\n``\n. I think this is leftover from the kombu lift. Probably shouldn't get a free pass if not AMQP\n. Think I'd prefer dropping theKOMBUfrom these variables.\n. Nicer to use theget` method with a default:\npython\nserializer = nameko_config.get(SERIALIZER_CONFIG_KEY, DEFAULT_SERIALIZER)\nNot sure why we do config.get(MAX_WORKERS_CONFIG_KEY) or DEFAULT_MAX_WORKERS) further up. Maybe you can fix that too...\n. Now might be the time to promote these fixtures out of conftest.py so we can import rather than reimplement them here.\n. Yeah that would be nicer\n. If we put it in the tests directory it'll be executed on every test run, as opposed to only the examples testenv. You also have to manually import the modules because the docs directory is outside the normal package structure. Not opposed to it though.\n. Yeah, this was lifted from conftest.py which still uses stop (which we should also fix). c.f. my note about hoisting the implementations so we can import them here.\n. I didn't want to duplicate all that machinery, but we probably shouldn't hardcode the port like this.\n. 201 returns a location header. It's the URI of the created resource.\n. Asking the question is probably enough to answer it ;) I'll change it.\n. It is the listening socket, and the greenthread doing the accepting is already dead.\nUnrelated to this change though, I don't think the webserver is a good citizen with regard to letting in-flight calls finish. It doesn't track the greenthreads spawned for each request, so stop may exit while they're still running, in which case the container will kill them ungracefully. I'll raise as a separate issue.\n. Fair enough, will drop.\n. That was my original aim, but I don't think we'll get there without contorting the examples. Also coverage doesn't count the code executed via execfile.\n. They're present in the docs (http://nameko.readthedocs.org/en/latest/key_concepts.html#running-services) so I think they should be tested here.\n. Actually this is fine. The WebServer is a subextension so it's stopped after all the workers have gracefully completed. By the time this method is called there won't be any in-flight calls.\n. Should probably use config.get(SERIALIZER_CONFIG_KEY, DEFAULT_SERIALIZER) as elsewhere\n. This should be the lowest version of pyyaml we're prepared to work with.\n. Do we want to keep --broker as an argument? What about adding --web-server-address?\nObviously requiring every possible config value to be specified as a command line option is ridiculous, but maybe allowing a subset would be helpful. And then which subset?\nIs there a world where the runner allows any possible config to be specified on the command line, and it's up to the user to use a config file when the invocation gets silly? It would require us to start registering config keys, but that's something I'd like to do anyway -- each Extension could specify the config it requires and default values; the ServiceContainer could do the same (for MAX_WORKERS and PARENT_CALLS_TRACKED).\n. Let's keep --broker for backwards compatibility. @davidszotten already raised the notion of registering config keys in https://github.com/onefinestay/nameko/issues/223. We can track these ideas there.\n. This does more than set up the queue now. It also creates/replaces the consumer\n. What happens to the first consumer in the event of a timeout, temporary error or KeyboardInterrupt? Can we end up multiple consumers? \n. Might be worth a note here for posterity, explaining that some platforms set self.consumer to None on disconnection and some don't -- so it's not possible to reasonably cover this.\nAlternatively, we could write a test that runs conditionally on the platform.\n. Quite right. I didn't spot this in the review on #244. There's no realistic situation where setup() isn't called.\n. We need to catch everything here, to make sure we avoid the nasty suicide and requeue problem.\nIf serializer is a bogus value for example, kombu will probably throw something other than nice serialization error, and we want to catch it regardless.\n. Thanks, will do\n. Doesn't appear to work.\nI see the .pth, and it contains the following (with the conditional in an exec so it's on one line):\n``` python\nimport os, sys;\nif 'COV_CORE_SOURCE' in os.environ:\n    try:\n        from pytest_cov.embed import init\n        init()\n    except ImportError:\n        sys.stderr.write(\"Failed to setup coverage.\"\n                         \"Sources: {[COV_CORE_SOURCE]!r}\"\n                         \"Config: {[COV_CORE_CONFIG]!r}\"\n                         \"Exception: {!r}\".format(os.environ, exc)))\n```\nThose environment variables are set. It seems like pytest-cov is attempting to handle collection of coverage from subprocesses itself, but maybe not working. If we can fix that maybe this entire fixture can go away.\n. Rather than using a uuid? Then you'd have to figure out what the source file was called in order to move it.\n. pytest-cov I assume\n. Yes, that's why the travis build is broken!\nAlternatively we could land https://github.com/onefinestay/nameko/pull/265\n. I think we should if possible. Will need a bit of work to get it up to 100% though. Separate PR I think\n. It's tricky to have this file as test/test_examples.py and also check coverage of the docs/examples directory. I've settled on giving it a separate entry in the Makefile and adding it as one of the steps under make test.\n. This fix landed in coverage 4.0b3, so we can aim for 100% coverage now.\n. It got ugly making the examples run with multiple python versions. You have to make a testenv that exactly matches the expanded environments so that they don't inherit the default. Like this:\n```\n[testenv:py27-examples]\npip install --editable .[dev,examples]\nmake test_examples\n[testenv:py34-examples]\npip install --editable .[dev,examples]\nmake test_examples\n...\n```\nIt's not terrible with only two repetitions, but would get worse with more python versions or varieties of commands. FWIW I find the new version easier to understand too.\n. Ah, I wondered what happened to this! I saw the email but couldn't find it...\n[testenv:lib] won't be matched by anything. The name has to exactly match one of the expanded  values (e.g. py27-latest-lib). \n. Thanks. I added this to #250 and cherry-picked it here. If you're happy with that branch can you give it a :+1: ?\n. @davidszotten I think that's probably the case. I intended for it to be a public API on all entrypoints, but we only had experience of using it in RPC. This PR obviously solves some of that.\nThe docs need a bit of a revamp. The \"built-in extensions\" content is pretty light. I always intended  to fill in the detail on the underlying implementations, and I think we also need somewhere to document these features that are common to many implementations (expected_exceptions, sensitive_variables, the whole concept of communication between extensions etc.)\n. This is a bit self-referential without somewhere describing why an exception might be \"expected\" at all (the name is a bit of an oxymoron). As discussed below I think we should have a separate section of the docs for this, and reference it from here. I am happy to add that as a separate PR.\n. The else is unnecessary here.\n. It seems unconventional to pass a dict into Exception.__init__. I think\npython\nsuper(HttpError, self).__init__(\"{}: {}\".format(error, message))\nwould be more normal. @davidszotten do you have an opinion on this?\n. Did you mean to put this comment on a different line? pools.reset just closes any connections that are held open in the kombu connection pools.\n. I think you've misunderstood how try/finally flow control works. I'm not handling the RuntimeError here.\nI want to raise the exception if any connections are left over. This exception will bubble up and stop the execution of the test. The finally is there because, whether I throw the exception or not, I should always delete any vhost the test created. In the case where the user provided a vhost (i.e. it's not a random one) we assume they don't want it destroyed.\nRegarding your other points:\n- Yep, agree that the error should identify the connection. I will add.\n- Checking conn['state'] == \"open\" isn't sufficient because there are other states that should also be considered \"lying around\"\n- rabbit_manager.get_connections gets connections on all vhosts. get_rabbit_connections filters that response. \n- On RuntimeError vs something else... was here before this PR but I'm happy to raise something more specific if we can think of something. Maybe CleanupError?\n. This is how you use the testdir fixture. `runpytest runs pytest in a subprocess and it will discover any test files you've written into the \"testdir\".\nresult.ret is the return value of the subprocess, so \"0\" means my test passed. \n. I think they're both as vague as each other really. We already had this debate in another PR and settled on lib, I just missed this reference.\n. Sometimes I do. If the next section is a reasonably big block or particularly distinct I'll usually give it some breathing space.\n. You shouldn't need to do this. These are from \"i.e.\", right? Travis correctly considers it a word.\nAre you developing locally on OS X? If so, the master branch of pyenchant has more up-to-date dictionaries. It hasn't been released though :(\n. Please change\n\"Unfortunately, many C-extensions\" => \"Note that many C-extensions\"\n\"Among them\" => \"Among them are\"\nAnd then add\n\"Modules with C-extensions that are not \"green\" may be wrapped in an eventlet thread-pool <http://eventlet.net/doc/threading.html>.\"\n. That is curious because we already have \"i.e.\" in the master branch. Maybe something changed on Travis...\n. Shows how out of date these dictionaries are!\n. I think we should take this opportunity to improve the docs, including explicitly documenting the behaviour you've just fixed.\n`` python\n\"\"\"\nTimer entrypoint. Fires everyinterval` seconds or as soon as the previous\nworker completes if that took longer.\nThe default behaviour is to wait interval seconds before firing for the\nfirst time. If you want the entrypoint to fire as soon as the service starts,\npass eager=True.\n\"\"\"\n``\n. Maybe call thisworker_complete?\n. You don't actually need to passresultandexc_infoin here. It would be sufficient to sendTrue.\n. Not a big deal but this check can go higher in the loop. There's no need to reset the event or work out the newsleep_timeif you're going to exit the loop.\n. @davidszotten, is this valid/recommended pytest usage?\n. The tests for this module have always been a mess. Rather than a broadtest_provider` test I'd prefer to break down each method and test its code paths individually, and then have a single \"end-to-end\" test verifying all the pieces work together.\nTake a look at https://github.com/mattbennett/nameko-sentry/blob/master/test_nameko_sentry.py to see tests that I've written in this way on a similar extension. Testing the _run method separately makes things much simpler.\n. @davidszotten is correct. The thread will probably spend most of its time asleep, and we have to be able to wake it when stop is called. We should have a test for this too, something like:\n``` python\ndef test_stop_while_sleeping(container_factory):\nclass Service(object):\n    name = \"service\"\n\n    @timer(interval=5)\n    def tick(self):\n        pass\n\ncontainer = container_factory(Service, {})\ncontainer.start()\n\n# raise a Timeout if the container fails to stop within 1 second\nwith eventlet.Timeout(1):\n    container.stop()\n\n``\n. Thereset_rabbit_vhosthelper doesn't exist anymore. We either create and later delete a random vhost, or use whichever was specified and don't modify it.\n. Quite right. I will do that\n. Hehe, yep. I now useisort. It's awesome!\n. Yes, we can. That'd be much better\n. I find myself using apropertywhen the alternative method would beget_foo()with no arguments. \n. Correct on all counts. I'm surprisedpatched.reset_mock` doesn't raise actually...\nI will remove this fixture\n. Ah. It turns out that exclusive implies auto_delete even if you request otherwise (and nothing complains if you supply both). I will change it so we only use exclusive when reliable delivery is off.\n. I don't think attribute access implies trivial. property gets used all the time to hide lazy creation of expensive objects, or provide a helpful interface on top of a moderately complex operation (e.g. https://github.com/onefinestay/nameko/blob/master/nameko/web/server.py#L64)\nobj.foo is just a little bit cleaner than obj.get_foo()\n. Ah yes. I will try to improve it\n. Actually you do, because self.broadcast_identifier can be inspected at any time regardless of the handler type (and is, in fact, during bind)\n. I don't think it's strange to check both conditions are met before raising the error. Otherwise you're relying on it only being called in specific circumstances.\nIt doesn't matter if it's a method or a property. This would be weird:\n```\n\n\n\nevent_handler = EventHandler(\"svc\", \"evt\")\nevent_hander.get_broadcast_handler()\nTraceback:\n...\nEventHandlerConfigurationError: You are using the default broadcast identifier\nwhich is not compatible with reliable delivery...\n```\n\n\n\nIt is less likely that you'd intentionally call get_broadcast_handler than you would getmembers, but I'm not sure that matters.\n. Alternatively we could add\npython\nif self.handler_type is not BROADCAST:\n    return None\nbefore the check. I think I prefer that. It's reasonable that broadcast_identifier is undefined outside of broadcast mode.\n. I see your point about sharing a class for \"service pool\" and \"broadcast\" events. It's that way because they're (even now) very similar in implementation. It is only the queue name, and now the exclusive value, that changes.\nI would advocate a separate class if it helped from an API perspective, but I don't think it does. Broadcast makes sense to me as an extension of the default \"service pool\" behaviour; they're both \"event handlers\"; if we made them separate implementations we'd need to give them distinct names, and I can't think of good ones.\nIf a single implementation with multiple modes is appropriate then I don't think it's \"bad\" in any sense to have an attribute, property or method that doesn't always apply. As you say, it's fine to have undefined behaviour when calling \"out of turn\".\nI have converted broadcast_handler to be method rather than a property, for consideration, but I don't think it's really an improvement. It feels less idiomatic without benefit to me.\nWith regard to getmembers and descriptors... Is it really better to say \"don't use descriptors\" than it is to say \"make sure descriptors are safe to be invoked at any time\"? I think latter is more pythonic.\n. As far as I can see, the only control you get over getmembers is the predicate argument, which evaluates each item. That would invoke the descriptors if you're inspecting an instance of a class.\nFor the bind case specifically we can avoid the problem by inspecting the class rather than the instance, but we also call getmembers in iter_extensions and that isn't a simple change. I would really like to remove iter_extensions and daisy-chain calls to setup, start, stop etc. by making extensions responsible for looking after their subextensions, but that is a bigger issue for another time. I guess the summary is that's not an insurmountable problem.\nIn the mean time, \"is safe\" is nebulous but there is at least a concrete description: the descriptor on an extension will be called during container creation (after the extension's __init__ and before setup) and may be called at any point afterwards.\nAs for healthy APIs... I completely agree that they should be driven by concepts not code reuse. I guess I like the handler_type part of the existing API - it makes it clear that the value changes how the handler behaves, as opposed to broadcast_event, which might suggest that the event type is different. That's a discussion for another time though.\nWhat would you like me to do about this PR? I'm still fairly keen on using a descriptor with a caveat in the docs and an intention to improve it later.\n. unless you override. I will fix, thanks.\n. Oh that is awkward. No idea how that got there!\n. You'll also need to use the --rabbit-ctl-uri flag, which specifies the management interface of the rabbit broker (you'll have to enable to management plugin too, if it's not already)\n. I have trimmed it down some. It assumes you know about about how dictConfig works but I think that was probably the case beforehand anyway.\n. We also need it for the coverage merge command to work (which is what pytest-cov does internally).\n. Circle has an amazing SSH feature that lets you connect to a box after a failed run. I added this so that I could live-debug what turned out to the the eventlet-pytest-py3 compatibility issue (on my local box it just hangs).\nCircle doesn't parallelise builds using tox though, so the test run is slower. I'm happy to remove this again, but I will keep it in a gist for the next time!\n. The WorkerContext has a reference to the ServiceContainer, and these dicts are on the container. I think we're disciplined enough about the worker lifecycle to make these non-weak references, but I'm not sure.\nI'm don't know what best practice is here -- should we make them weakrefs just in case, or is that sloppy?\n. Happy to do this, but why does that matter?\n. Great suggestions\n. Will do\n. Hmm, yes. I think we can scratch this.\nI wanted to apply some kind of retry policy for replies, so that (for example) an intermittent connection error would be overcome. That is clearly the responsibility of the entrypoint though.\n. Is it? In Py3 dict.items() will give you back a dict_items object, so if you copy() you're also iterating over a dict_items (which is a generator, I think) rather than reading them all into a list first.\n. Optional but raises a DeprecationWarning\n. Ah yes, of course. Thanks!\n. Just more convenient that a local VM. I'll remove the circle.yml file\n. Yep, works fine. I will remove this\n. For posterity: https://gist.github.com/mattbennett/aa7cf713c97e689c56257991231471e5\n. OK, will remove. I completely agree on removing container from the WorkerContext. Bound extensions all have access to it anyway, we don't need to pass the reference around everywhere.\n. I guess copy is just as bad as list. Happily, removing the WeakKeyDictionaries makes this problem go away.\n. Yep, this has solved itself :)\n. Struggling to come up with something that's helpful here. It's a hook for you to do what you want with... I don't think there's anything specific you really shouldn't do\n. Hmm, maybe this works by accident. We kill the greenthreads here, but _handle_x_thread_exited isn't called until we yield. You would think that the log call would/could yield though.\nI will add the list back for safety. \n. The \"default\" behaviour which django enables there also only shows the warning once, so subsequent trips through the same code path don't trigger it. To fix the currently failing tests I need to set it to \"always\" mode.\nI have added it as a top-level fixture (rather than in pytest_configure) so that it can be disabled by overriding if needs be.\n. Sorry, I should have put a note on this PR. It's a work on progress.\nI think I'm going to scrap it for an even-better-entrypoint-waiter PR...\n. That is the main use-case and I'm happy to make that change. I'll wait 'til you see my proposed implementation for the entrypoint waiter though, since there might be other things to discuss.\n. I have backed them out because _worker_teardown is as good as worker_destroyed for my (only) use-case\n. Correct. I will fix all of these\n. Amazing. I'm enjoying watching those tickets progress\n. It only raises when the mandatory flag is set on the message. It would be unusual to set mandatory delivery for an event dispatcher, but less so for a normal publisher. And reasonable to raise in any case if it is set.\n. it turns out that worker_teardown is just as effective (the patched method is run before the hook)\n. Apparently it's much faster, presumably because it doesn't have to clone the whole repo. See http://stackoverflow.com/a/24811490/128749\n. Nice!\n. Definitely agree with this. Something like two major releases or 6 months, whichever comes last?\nI'll create document somewhere to describe which deprecation warnings we have and when they were introduced, so we know when they're eligible for removal.\n. as opposed to a dynamically generated one? seems nicer to have it all contained within this test, rather than having a file floating around somewhere else.\n. Can we mark this as \"do not skip\" on Travis only?\n. Ha, yes that is a better option!\n. Context data can be modified through the course of a worker executing\n. That's quite a lot less readable. Also conceptually, the size-limited stack is exactly a deque.\n. Ah, thank you. It was intentional to drop most of these, but I hadn't noticed that immediate_parent_call_id is read by nameko-sentry (which is in use in several places).\nAfter adding it back, only self.parent_call_stack and self.unique_id have been removed, leaving (I think) a public API.\n. I would rewrite these two paragraphs as:\n\nDependencies named in *dependencies will be replaced with a :class:MockDependencyProvider, which injects a MagicMock instead of the dependency.\nAlternatively, you may use keyword arguments to name a dependency and provide the replacement value that the MockDependencyProvider should inject.\n. I think we can keep the original heading as Usage and just give this an introductory sentence:\nProviding a specific replacement by keyword:\n. This isn't the most compelling example because it doesn't achieve anything different from the other API. How about the following?\n\npython\nstub_maths_rpc = RpcProxy('stub-maths')\nYou'd need to drop the assertion about the mock too. Maybe just state what you would do, rather than demonstrate it...\n. It's odd to pass the amqp_uri and the whole config object (which contains the AMQP URI data).\nI would prefer if the arguments here were amqp_uri and ssl_params (or just ssl)\n. It is possible to test against a non-local rabbit actually, though not with the Makefile. You could make this slightly more correct by switching the order of the two sentences:\n`` rst\nThere is a Makefile with convenience commands for running the tests. To run them locally you must have RabbitMQ :ref:installed ` and running, then call::\nmake test\n\n```\n. I've been trying to follow this advice on argument alignment. Nameko's style is a bit sporadic, but I would prefer:\npython\ndef get_wsgi_server(\n    self, sock, wsgi_app, protocol=HttpOnlyProtocol, debug=False\n):\n    ...\n. I like this idea, but still think that verify_amqp_uri should either take the whole config object, or the uri and ssl params separately.\n. This is perfect. Demonstrates how to use the feature as well as testing that it works \ud83d\udcaf \n. Is there a meaningful example we could demonstrate here, rather than using a mock? Something like applying a fixer, without too adding too much code.\n. Strictly speaking, methods are overridden rather than subclassed\n. This change should be considered optional to for this PR. I'd prefer to land these fixes rather than force them to sit behind stylistic cleanup of old tests.\n. What's the second match here? Can't we have the regex discard it? \n. I think I would prefer these defined as variables, although in this module rather the constants.py. That allows you to give them readable names, e.g.\npython\nenv_var_matcher = re.compile(r'\\$\\{([a-zA-Z][^}:\\s]+)(:([^}]+))?\\}')\nquoted_env_var_matcher = re.compile(r'.*\\$\\{.*\\}.*'))\n. Also worth following raymondh's advice here https://twitter.com/raymondh/status/766654913526890496?lang=en-gb\n. I think you should mention the !env_var resolver here for quoted values.\n. in the :ref:`concurrency` section\nor\nin ref:`concurrency`\n. ```\nmatches the literal : character zero or one times\n```\nwould be a clearer way to say this\n. I would say\n`\nYou can use bash style syntax:${ENV_VAR}``\nOptionally you can also provide default values: ${ENV_VAR:default_value}\n.\nTo run your service and set environment variables for it to use:\n``\n. If you need to quote the values in your YAML file, the explicit!env_varresolver is required:\n. This feature was somehow missed from the 2.4.1 release\n. Actually there is no good reason. I will remove these\n. The entrypoints need to be defined for various reasons (e.g. here the class would not be considered a service without one) but they're never actually executed.\n. It is, but a pointless one. It should have been deleted rather than just updated when we removed theEvent` superclass. \n. I will add these\n. They assumed that the events would not be received immediately and we'd need to wait until they were processed.\nAs it happens, rabbit_manager.publish appears to always deliver the message before returning, so they're not required. I'll add resilience if this isn't reliably the case, but I'd rather not unless I have to.\n. If we're going to attach the ws_app to the socket object, perhaps make_virtual_socket should pass it in at construction time?\nAlso, I found it confusing that this variable is called socket, assuming it to be the underlying socket object. It is really a websocket proxy (although \"proxy\" is also a heavily used word with other meanings).\n. make_virtual_socket instantiates the socket object and the ws_app object, and returns both. I was suggesting to make the app an explicit member of the Socket class, rather than attaching it here.\nI also think socket_creator is also a confusing name, but let's not muddy the waters of this PR.\n. I'm happy to delegate all these changes to another PR. I would prefer to land this with the test though.\n. It's difficult to demonstrate that something doesn't crash when the crash cause is non-deterministic.\nI have something that seems to work though. Do you think the following is preferable?\n``` python\nimport eventlet\nimport pytest\nimport requests\nfrom eventlet.wsgi import Server\nfrom nameko.testing.waiting import wait_for_call\nfrom nameko.web.handlers import http\n@pytest.fixture\ndef container(container_factory):\nclass Service(object):\n    name = \"service\"\n\n    @http('GET', '/answer')\n    def answer(self, request):\n        return \"42\"\n\ncontainer = container_factory(Service, {})\ncontainer.start()\n\nreturn container\n\ndef test_foo(container):\nrun = [True]\n\ndef wait_for_os_error(args, kwargs, res, exc_info):\n    if exc_info and exc_info[0] == OSError:\n        del run[:]\n        return True\n\nwith eventlet.Timeout(1):\n    with wait_for_call(Server, 'process_request', callback=wait_for_os_error):\n        while run:\n            gt = eventlet.spawn(requests.get, \"http://localhost:8000/answer\")\n            eventlet.sleep(0.01)\n            gt.kill()\n\n# still works\nassert requests.get(\"http://localhost:8000/answer\").text == \"42\"\n\n```\n. You might need to show me...\n. https://github.com/onefinestay/nameko/pull/371 makes these more robust\n. It's a bad test, because we need to verify that exactly two events are received -- not one or three. The only way to do that is give them a fair chance to propagate through the system. For absolute certainty I should also be checking that all the queues are empty.\nFor this little irritation I'm not sure it's worth all the extra effort to refactor.\n. That library would also avoid me having to control the server using requests. Unfortunately it does not seem to be ready...\n. @property makes them read-only on the instance, so you can only override them with a subclass. I think that's a helpful restriction, but I could be persuaded otherwise.\n. Yep, I will add these\n. Yes. It seems this would be helpful (https://github.com/onefinestay/nameko/issues/374)\nI'll make this enhancement as part of this PR\n. I thought we discussed this and you preferred to use one of our accounts rather than a shared one?\n. Should probably drop this. Email isn't a required value\n. Ahh OK! I'll see what I can do...\n. Missing a trailing underscore here\n. Actually not sure. I've unrolled it so it's more inline with the other producers\n. Good points. I've turned them into constants in the module.\n. Yep, used in a few places. I'll turn it into a fixture in the plugin\n. I'm not convinced that's better. The Controller object here is very specific to the toxiproxy fixture.\n. Much better!\n. Yeah I'm not convinced this is good.\nI was trying to demonstrate that these cases aren't recoverable unless you have publish confirms enabled. In contrast the other cases behave the same way regardless of confirms.\nI'll change it so we don't try to run them when without confirms enabled, and put the context into a docstring.\n. Strict mode actually works fine if you register enable_retry as a mark in an ini file.\nIs there a better way to control the behaviour of a fixture like this? Also do you want me to enable strict mode as part of this PR?\n. This passes:\n``` python\n    def test_different_producers(self, rabbit_config):\n        amqp_uri = rabbit_config['AMQP_URI']\n        producer_ids = []\n    # get a producer\n    with get_producer(amqp_uri, True) as confirmed_producer:\n        producer_ids.append(id(confirmed_producer))\n        assert len(set(producer_ids)) == 1\n\n    # get a producer with the same parameters\n    with get_producer(amqp_uri, True) as confirmed_producer:\n        producer_ids.append(id(confirmed_producer))\n        assert len(set(producer_ids)) == 1\n\n    # get a producer with different parameters\n    with get_producer(amqp_uri, False) as unconfirmed_producer:\n        producer_ids.append(id(unconfirmed_producer))\n        assert len(set(producer_ids)) == 2\n\n```\nBut I'm not sure it's a valid test. Publish confirms are a parameter of the connection -- isn't it internal to kombu whether you get the same or different producer instances back in this case?\n. Eventlet throws Second simultaneous read errors. I don't think we need to fix it.\nThe ServiceRpcProxy is synchronous and blocking so it won't interfere with whatever concurrency framework a client app uses. It assumes that it's being used either in synchronous code or safe concurrent code, whereas here we're sharing a single instance between multiple threads.\n. It turns out this test is quite broken. I introduced the bad connection for the publisher and the consumer. It's the consumer which throws the RpcConnectionError.\nI've updated the test to limit the toxic to the publisher and the standalone proxy now behaves identically to the dependency provider version.\nI'll add another to demonstrate the behaviour of the consumer under broken network conditions. It's not strictly related to publish confirms but I think it's an interesting test and may answer the question in the TODO here.\n. Ah yes that rings bells. I'll add this\n. Cool. Will enable strict mode.\n. :) I nearly raised a pull request to enable both US and UK spellings...\n. It seems not. You have to re-establish the consumer (and its exclusive reply queue) before sending any request, otherwise the reply queue may not exist when the response is published. The MethodProxy is used by both the standalone and dependency versions of the RPC proxy, so you can't use check the consumer connection before sending a request.\n. I don't think so. The managed thread will exit without error and the server will continue waiting for another accept()\n. OK, I'll make a subdirectory here\n. If this works with our oldest supported kombu then I think we're fine. It's ignored for non-pyamqp transports and has been supported there since 0.9.1.. There is some extra network traffic but no performance implication as far as I'm aware. Services will probably get faster because broken connections are discovered earlier.. The default timeout is 0.1 seconds so I don't think there's anything to worry about there. If timeout is never set as per the comment it has no effect anyway.\n@kollhof can you remember why you did this?. Hard to say. I added a timer context manager around this locally and container.wait only ever took ~0.005 seconds.. But I also can't get it to fail locally.... Travis is variable. Re-running master now is taking 4-5 mins: https://travis-ci.org/nameko/nameko/builds/177243811\nThere is some small impact here on tear-down times, but I think it is just 0.1 seconds. It becomes apparent in here because we do it a lot.\nDo we really care that services take slightly longer to stop? I could retrofit the call to conn.heartbeat_check() into our lifted method, but I'd rather remove the override. We wouldn't justify adding the override for a small improvement in test-run time.. It turns out that safety_interval was also changed when we lifted this code. Kombu sets it to 1, which explains the more-than-expected slowdown.\nWe've added a second to QueueConsumer.stop, which is used in about 120 tests. This is unfortunate but I don't think it justifies keeping this lifted code. I'm also concerned about the 10x difference in timeout interval -- I'm uncomfortable being so far away from value set/expected by kombu for no apparent reason.. Sorry, not intentional. Have moved it back again. I don't think we should put them back. We use pytest-timeout to catch hanging tests now, which is what these were originally used for.\nTracking the average test execution time would be a much better way to spot performance issues. It probably wouldn't be too difficult to send stats to a free tier on something like https://plot.ly.. Ha, I thought it defaulted to a 30 second timeout if installed. But apparently not. I'll add it. Quite right. It needs to be a factory fixture (or just a util function). Hmm. As it's implemented here you can send multiple undeliverable messages and be notified about only one of them, and then only after you exit the contextmanager.\nIt might be better to push this check down in to the different implementations.. I've done this and added explicit tests about how to use the mandatory flag. It's nicer.. Strictly speaking, these are built-in Dependency Providers. It's not a great name :(. values at run time. This is quite an \"internal\" looking test. For such a simple dependency provider it would be better just to demonstrate that it works.\nYou could simply duplicate the test you have above to verify your example. That is \"real-world\" usage, therefore less brittle, and would show an example service and how to use it next to each other.. I didn't know this had a name!. It turns out that producer.publish accepts a declare kwarg, that will do this for you. It might be easier to test within nameko, but you could drop these declarations and allow kombu to do it. Up to you.\n. It would be nice to continue to accept the queue argument for backwards compatibility.\nIf you pass exchange=None and queue=<some bound queue> to the current implementation, it'll declare the queue and publish to the exchange the queue is bound to. So to maintain compatibility you'd need to:\n\nAccept a queue argument\nIf the queue is bound and no exchange is provided, set self.exchange to the queue's exchange\nAdd the queue to queues_to_declare\nIdeally raise a warning noting that the queue arg is deprecated. I'm not too sure about these groupings. It may be simpler to just put each one as a class attribute (and without the property decorator). \ud83d\udc4d  much better. Because it's not possible to specify an AMQP_URI without a vhost. Even amqp://localhost implies the default vhost.. OK cool. Using a URI but ignoring the vhost sounds good to me.\n\nLetting users specify their own really isn't helpful, because so many tests assume they're being run in a clean vhost.. The service that it starts has an @rpc entrypoint.\nThe old empty_config wasn't really empty -- it had an AMQP_URI with an empty value, which was translated into amqp://localhost. The test failed if you ran it without having a broker running locally and on the default port.. Hmm. The docs are ambiguous. There is an example of using it that way, but it's not explicit.\nI think .strpath shows the intent more clearly.. The n global creates a ReplyListener that connects to rabbit. These tests also used to assume there was a broker running at amqp://localhost. What do you mean?\nI guess you could call it with librabbitmq://, but that would break in all kinds of other ways because it doesn't work with eventlet. What was the original purpose of this check?\n. So that it can have the tracker in scope, which is a fixture.\nI originally changed it because I thought the reset_mock was responsible for the dodgy test, and only later discovered the duplicate event dispatch.\nThe \"tracker\" mock is the pattern we've used elsewhere to track calls like this though, so I left it.. I think I prefer them like this. Everything is in the same place, so you can't accidentally let things get out of sync or used for more than one purpose.. I think it's because of the autouse and the fact that the \"reorder fixtures\" are all in the same plugin module. I don't think it's an official behaviour that's guaranteed to work in future versions but it does for now.\nIt might be possible to guarantee it with some deeper pytest magic (I'm following https://github.com/pytest-dev/pytest/issues/1216), but I think this is OK for now given that we pin pytest.. I guess so, although I'm not sure that speeding up teardown of real services adds much value. You'd save at-most one second per container, which is only really significant during a test run that starts and stops hundreds of containers in sequence.. The docs for tryfirst refer to marking a \"hook implementation\". I tried applying it to fixtures and couldn't make it have any affect.. My preferred solution for that is to consume messages with something other than kombu, which is getting closer to being possible.. Yeah that is slightly annoying, but I think I still prefer it this way. . Ah, nice idea. I would like to use it in nameko-sqlalchemy to speed up creation of database schemas.. size=3? From trial and error in the vhost usage -- 3 resulted in the best speedup. It's a balance between having something available when you need it and the \"waste\" of destroying what's left unused at the end of the run.. Yep, thanks. It's for the case where the create loop is blocked waiting to put an item into the ready queue (i.e. waiting for it to be less than maxsize).\nunused = self.ready.get() unblocks the create thread but doesn't yield, so we exit the while loop here before giving it a chance to run. Once unblocked, the create thread deposits its item into the ready queue, and if we've already exited the loop, that final item isn't destroyed.\nI've expanded the inline comment to hopefully explain. I prefer having a default rather than forcing the user to select something. I think 3 is fine.. I think you're right. I'll rework it.. Maybe add a note that this is a regression test for #417?. The name doesn't make anything clearer to me. But then again, neither does digging through the pyyaml source (http://pyyaml.org/browser/pyyaml/trunk/lib3/yaml/resolver.py#L140)\n@Trex-Boolat can you explain what that third argument does?. This makes sense to me now, thanks \ud83d\udc4d  . Great to have these extra tests. Yes, it should. Thanks. I wondered whether to keep these tests in the repo. They were helpful for verifying the fix, and are a good reference, but they're quite slow even when they pass (~45 seconds).\nI could pull them out into a gist and reference them here and in the issue?. By \"as a reference\" I mean it would be nice to see all the different failures that this fix addresses, alongside the fix.\nThey're slow because several of them only fail after heartbeats are missed, and it can't realistically beat faster than once every few seconds. . Good idea.. Good spot, thanks. Does it matter? The identifier is only used to help identify thread when they're killed: https://github.com/nameko/nameko/blob/master/nameko/containers.py#L470. Yes that's better, will move it.. Yeah, I lifted it from our application codebase. Actually it's previously been part of the Nameko codebase too, but we reverted it in favour of a less generic solution (which has since been rolled back).\nI think if it's going to live as a helper function it's useful to keep the max_delay argument.. No reason! I'll change it :). Yes it should. Thanks. No, default is 3.\nI think it would be dangerous to have infinite max attempts and for_exceptions=Exception as the defaults -- too easy to end up retrying forever.. Actually no, we want to catch exceptions from requests here too, e.g. if the server isn't accepting connections yet.. I forgot that we pinned exact versions of dependencies used by tests. Will revert. requests is an install requirement. It is used in tests too - should we pin a version if you install [dev] requirements?. Until very recently (2.16.0) requests did vendor urllib3, although here we are using it directly anyway.\nI will pin requests here in [dev] so everything is consistent, and bump all the versions to something recent.. Strictly I think sqlalchemy-filters is a JSON-serializer for sqlalchemy query operations. It's not tied to HTTP or REST APIs specifially. The once entrypoint is a bit special in that is passes **kwargs directly to the entrypoint. If you don't pop them off you get unexpected argument errors.. They're \"unused imports\" otherwise. I'm only pulling them into this package for a backwards compatibility import of from nameko.amqp import verify_amqp_uri. Several of them have truthy defaults, so it's hard to compare whether a new value has been provided. This is a little more verbose but consistent for every param. I also like that the default values sit next to the docs for each option.\nA slightly less verbose option could be the following:\n``` python\ndef init(self, amqp_uri, **kwargs):\n    self.amqp_uri = amqp_uri\nself.use_confirms = kwargs.pop('use_confirms', self.use_confirms)\n...\n\n```\nSlight disadvantage here is that the sphinx docs would only show **kwargs.\nRe: adding too many ways to change settings. Maybe. This does allow subclassing to override defaults as well as at instantiation (and use-time). Subclassing used to be the only way to change these things previously though, and we encourage subclassing for other reasons.\n. Perhaps, although it's going to be another annoying step that frequently gets forgotten and fails the build. Out of order imports are annoying though, and so are casual rearrangements inside other commits.\nCould we automatically re-write them on a commit hook? Then the order will always be consistent and any re-arrangement would be made automatically and in its own commit.\n. Yep, thanks. Shows how long this PR has been in the works!. I think I was trying to communicate that these are headers that propagate from one call to the next. I agree it sounds like a bool.\nI guess it's good enough to call them extra_headers.. Wasn't sure whether to add this as part of this PR. It's much easier to do now, and it would bring it in line with the standalone RPC proxy.. I think it's slightly easier to interpret using the named attribute rather than remembering that it unpacks to args, kwargs. If you're only interested in one or the other you don't have to do args, _ = call or whatever.\nI'm not wedded to it if you'd rather not introduce a new helper though.\nReally I just want Mock's call to be a namedtuple out of the box.. @tonybajan is that an endorsement for call being a namedtuple or for the helper?. Fair. I will update it.. Whoops!. It's about 3x faster than flake8 actually. Have rearranged it. I prefer to keep these developer's choice ignores out of here, and in a global gitignore instead. Otherwise we'll end up with an ever growing list.\nThe .codeintel and sublime ones snuck in at some point!. It is a bit weird. Was following https://blog.travis-ci.com/2017-05-11-introducing-build-stages .\nRe-reading that suggests that they're optional though, so I will put them in.. Is there a significance to max_attempts=9? (seems rather a lot). Nice one! How to do this has come up on the forum a few times before... hopefully not so much anymore.. That is a much better name!. Whoops!. The original test isn't particularly representative of real behaviour -- closing the connection from the broker API actually sends a consumer-cancel rather than yanking the connection.\nWe could put toxiproxy in here, but it seems like overkill if we trust that kombu will throw ConnectionErrors when something goes awry with the connection.. That's a good point. It's probably not too hard. Will try to add it. Good call on adding this -- the behaviour on a real disconnection is different, because kombu keeps the state of the connection and we use it to determine whether the proxy has been previously stopped.\nI've updated the test, although the whole thing will be made moot by the fix for https://github.com/nameko/nameko/issues/359. No, it's the test.\nWhen we used the management API to close the connection, it closed the existing connection but didn't actually prevent the consumer reconnecting here\nhttps://github.com/nameko/nameko/blob/fb7560e08eaf14fbe1684abf6939efbd7aa9fbca/nameko/standalone/rpc.py#L168-L170\nNow, toxiproxy remains disabled until after the consumer fails to reconnect and bubbles out the socket.error, so then the following triggers on next use:\nhttps://github.com/nameko/nameko/blob/fb7560e08eaf14fbe1684abf6939efbd7aa9fbca/nameko/standalone/rpc.py#L57-L65\nThere is a (slightly ugly) hook we can use the re-enable the connection between the first and second attempts, so I will add a test that does that.. Race between what?\nTry/catch is problematic because the exception raised is RecoverableConnectionError, which \"connection already closed\" just as a string. It's worth noting that the connection is not actually recoverable from here (we used to retry on these errors, and even after a new connection is established the ack is still attempted through the dead channel) and even if it was, the broker will already have reclaimed the message.\nThe logic here is \"if the channel is closed, the message will have been reclaimed, so do nothing\".. No, a process won't wait for greenthreads to finish before exiting.\nThis cleanup really isn't required because we stop the whole toxiproxy server at the end of the test run. I added it while debugging a hanging toxiproxy (which turned out to be http://tanin.nanakorn.com/blogs/373) thinking that the problem might be the growing number of proxies that we add and never remove.. > Race may be wrong word, but connection might close between checking and acking\n(GitHub email didn't reply to my comment)\nYes I am with you. I would like to keep the conditional to show intent, but will add a try/catch to make sure we don't blow up if we lose the race.. As soon as it detects the connection is closed, yes. I don't think a grace period would make any difference.. I guess not. Will remove. entrypoint_hook triggers the entrypoint without actually using whatever transport it's connected to, so this isn't using AMQP.\nYou need to generate a real AMQP message, for example with the standalone RPC proxy:\npython\nwith ServiceRpcProxy(\"service\", rabbit_config) as proxy:\n    assert proxy.echo(\"foo\") == \"foo\" \nThe proxy can connect using SSL or not. In fact, it might be nice to test both.. Let's deprecate the --broker flag arg with this change. You'll be able to use --define AMQP_URI=whatever instead. I think we need to check whether the specified serializer and accept values have been registered as serializers. . Reinforcing the comment above: we should check earlier than here that there is a registered serializer for this content_type.. Might be worth splitting this into multiple tests. If you put them all into a class you can do the setup in fixtures to avoid duplication. Same here. I think separate tests for these two cases would be more readable (and more targeted if one fails). You might even be able to parametrize them. This could actually be from --define now. I'm not sure this message is helpful enough to keep.. This is very neat!. Shouldn't the expected value here be ${INDICE}_val? Note the extra }. s/recurtion/recursion. Is this desirable behaviour though?\nActually I would expect this case to evaluate as\nFOO: b_val. Comparing this case to the one above in test_environment_vars_recursive_in_config:\n```\ntest_environment_vars_recursive_in_config:\nrecursive env requiring data\n(\n    \"\"\"\n    FOO: ${FOO:val_${INDICE}}\n    \"\"\",\n    {\"INDICE\": 'b'},\n    {\"FOO\": 'val_b'},  # << expected value \"val_b\"\n),\n```\n```\ntest_unhandled_recursion:\nrecursive env requiring data\n(\n     \"\"\"\n     FOO: ${FOO:${INDICE}_val}\n     \"\"\",\n     {\"INDICE\": 'b'},\n     {\"FOO\": '${INDICE_val}'},   # << expected value should be \"b_val\"\n)\n```\nWhat's the difference?. Ahh, I understand now. Sorry for the confusion. This is the new Consumer utility (obviously). Removal of deprecated options now we're passed the 2.9.0 milestone. Split this into functions so they can be used without inheriting from HeaderEncoder. Removal of the QueueConsumer. This workaround was present in the QueueConsumer and must now be implemented in every entrypoint that consumes AMQP messages. We now allow overrides of these \"consumer options\" by passing them to the constructor of this entrypoint, and fall back to the config-specified or default value if not set.. This whole \"waiting for providers\" dance was required to prevent an RPC entrypoint stopping before the RpcConsumer and QueueConsumer that it depended on; if that happened we'd consume a message and then throw MethodNotFound exceptions.\nWe have a test for this (test_rpc_consumer_sharing) and it still passes with the new implementation.. Same as https://github.com/nameko/nameko/pull/542/files#r188260399. Same as https://github.com/nameko/nameko/pull/542/files#r188260194. Same as https://github.com/nameko/nameko/pull/542/files#r188260399. This implements the check for lost replies due to an expired RPC reply queue. Workaround until https://github.com/nameko/nameko/wiki/Roadmap#refactor-sub-extension-management is done. This object is the replacement for the ServiceProxy and MethodProxy. Standalone RPC proxy completely refactored to use the new nameko.rpc.Proxy object and a ReplyListener that manages its own consumer and looks a lot like the service-based version at nameko.rpc.ReplyListener. Fixture provides no speedup now that we don't wait to cancel consumers on kill.. Removed a couple of unused helpers. Tests for the new nameko.amqp.consume.Consumer helper. unittest.mock.patch and mock.patch have different APIs (only one returns the patched object) so have reverted to using just one. It is possible now :). ConsumeEvent doesn't exist anymore. The diff is messy here. TestStandaloneProxyReplyListenerDisconnections is completely new and adds the standard disconnection tests now that the standalone reply listener follows the standard \"consumer\" pattern. . Moved unchanged from test_queue_consumer.py. TestHeartbeats and TestHeartbeatFailure are new tests verifying that the heartbeat works correctly on entrypoints/extensions that use the new nameko.amqp.consume.Consumer helper. All of these tests are defunct or otherwise covered by other tests in the suite. The changes in this module are either small tweaks to accommodate the new implementation (e.g. expect more connections now) or cleanup where we stop using the rabbit_manager to inspect queues (slow and flakey) in favour of the new queue_info mechanism that determines queue info using AMQP.. New test for unchanged functionality. New test for unchanged functionality. New test for unchanged functionality. New test for unchanged functionality. All these tests are now defunct or have been moved elsewhere. Better test for unchanged functionality. Helper no longer exists. Fixture no longer exists. I'll add an inline comment that clarifies this. We use the shell fairly often, but that's all. Ideally we want someone who uses https://github.com/jessepollak/flask-nameko to test the pre-release.. It is slightly annoying. Open to suggestions!. I got tired of typing AUTO_FIX_IMPORTS whenever I wanted make imports to actually fix the problem rather than just complaining. Didn't intend to commit it, will revert.. Good idea. I will drop the class. That would be nice. I tried not to make any changes outside the scope of the AMQP refactor (serialization.setup landed in #535) but now we're in principle happy with these changes I'll allow myself some out-of-scope improvements too :). They are cumbersome. What's the alternative?. Fair enough. I'll make it more explicit. Yep, you can do n.rpc[\"service-name\"][\"method-name\"](). Spellchecker unfortunately only runs on the .rst files, not docstrings. Might be because we don't include the apidoc. I'll see if I can fix it.. My spelling always seems to end up as \"transatlantic\" \ud83d\ude00 Will fix.. Actually spelling_lang = 'en_GB' in the sphinx config. Explains why these weren't flagged by the spell checker already.. Similarly here, UnknownService is a Nameko exception that's part of the RPC API. I think we need to keep it rather than raise the generic exception coming from kombu.. Do we need this still? I think the hasattr check was to protect the attribute access below that has now been removed.. Review comment got lost somehow:\nI think we need to keep UndeliverableMessage when publisher.publish can't deliver a mandatory message.. Can you not catch and inspect the ChannelError from https://github.com/nameko/nameko/blob/master/nameko/amqp/publish.py#L179?\nSomething like\npython\ntry:\n    publisher.publish(\n       ...\n    )\nexcept ChannelError as exc:\n    if str(exc) == \"NO_ROUTE\":\n        raise UndeliverableMessage()\n    raise. This would be slightly neater here:\npython\nif transport_options is None:\n    transport_options = DEFAULT_TRANSPORT_OPTIONS.copy()\ntransport_options['confirm_publish'] = confirms\nNo need for deepcopy if transport_options doesn't contain any nested mutable objects.\nI wonder if we should deprecate passing confirms as its own keyword argument and expect it to be passed as part of transport_options.. This makes transport_options configurable at the service level, rather than the extension level. That might be what we want, but some transport options (e.g. confirms) may want to be different across extensions in the same service.\nFor the DependencyProviders (Publisher, EventDispatcher and RpcProxy) you could allow transport_options to be passed to the constructor (in the same way that use_confirms can be) so you could override the default for that particular extension. The corresponding entrypoints don't have the same mechanism for accepting parameters in their constructors, so we'll have to use the service-level options there.. use_confirms can be passed into https://github.com/nameko/nameko/blob/master/nameko/messaging.py#L73  as part of **options, and then extracted out of them when that dict is passed to the constructor of https://github.com/nameko/nameko/blob/master/nameko/amqp/publish.py#L35. Actually transport_options could already be provided to the DependencyProvider constructor and would also end up in **options.\nThe test at https://github.com/nameko/nameko/blob/0bffbf7b3c2a26ab664075a48d5fc3e136205413/test/test_messaging.py#L1189-L1213 demonstrates this for use_confirms. It'd be good to have an equivalent for transport_options.\nAt the moment, the line https://github.com/nameko/nameko/pull/564/files#diff-309aaafd3b61875d021b62c5b3926f91R178 will cause a duplicate keyword argument exception if you did pass transport_options to the Publisher DependencyProvider constructor. I think if you just removed that line we'd get the desired behaviour.\nDoes that make sense?\n. Rogue print statement here. Can we call this expected_headers and define it after the call to publish?. You should also accept **kwargs here too, and pass them to super().__init__(). Then it will be possible to mark expected exceptions on a timer:\npython\n    @timer(expected_exceptions=Foo)\n    def method(self):\n         pass. I think it would be more explicit to define the service inline here. Like\npython\nclass Service:\n    name = \"service\"\n    @timer(interval, eager)\n    def tick(self):\n        return tracker()\nI don't think this is functionally different to what you have here, but it defines everything inline.\nAlso note that I've used tracker here, which is just another pytest fixture that returns a mock. We use this pattern elsewhere in the test suite for tracking calls (e.g. here). Again, should be functionally identical, but is more consistent with the existing tests.. eventlet.sleep(timeout) would be better here, since the variable is already defined.. I think it's sufficient to test only one of these modes. Testing \"in container\" seems better because it requires less knowledge of the internals.. That is the general philosophy, yes. Although in this case it's not intentional -- the tracker was added for one test and then the pattern spread to others, and now we used in lots of places. It's probably due for a refactor into conftest.. It wouldn't be expected for the container to die even if the error wasn't expected, so this line is misleading. You can probably just drop it.\n. This looks like a duplicate test case. As I understand it, the copyright is shared by all contributors. It's also implied and does not need to be stated anywhere in order to be in effect, but including a statement somewhere is recommended for the avoidance of doubt. \nSo this doesn't change anything material but is a better reflection of reality.\nSee https://softwarefreedom.org/resources/2012/ManagingCopyrightInformation.html\n. I updated it to be less obtuse. This is a pre-cursor to just turning on black ;). Should this be an absolute import?. serialization.setup() now only happens if there was no serializer specified in the publisher_options. I think this means we're skipping things like importing modules in that case.. Why did this change?. There is already a fixture called amqp_uri that pulls the URI out of rabbit_config. Perhaps you could remove it and use the name here? rabbit_uri is a bit vague because there is also the URI for the management interface.. amqp_ssl_uri?. Do you actually need rabbit_config here? I think the same for a few of the tests in this file. This spam/egg/ham business drives me a bit crazy!\nI know there is a precedent for it elsewhere in the codebase, but it'd be nicer to have something easier to read here.. Probably don't need rabbit_config if you're using the mock_container. Same for lots of tests in this module. This ssl=None isn't necessary. Was it added for readability?. Ah yes, OK \ud83d\udc4d . Yep, this is good reasoning. You've convinced me \ud83d\ude09 . You might be able to no cover the whole module in one go.\nI think it'd also be worth putting a little note explaining why it's excluded from coverage.. Yep, you're right. Explicit is better than implicit :) Thanks. It's a little confusing that this fixture is defined but not used. I think you can drop this and just demonstrate the decorator usage.. Might be clearer to call this data. I was briefly confused and thought the implementation below was writing to the global config var.. I think it would be easier to understand these tests if they used the testdir.runpytest approach like the other tests in this module. Is there a reason not to do that here?. Would be slightly terser to use an itertools counter here:\npython\nstart = time.time()\nfor count in itertools.count(start=1):\n    yield max(start + count * self.interval - time.time(), 0)\nWith this, eager also approximates start=0, which may or may not be a nicer way to implement it. Can't quite decide.. Can we add a little more here? Something like \"timer now waits for the spawned entrypoint to complete before firing again, as documented\"\nAs @bobh66 suggested on the forum earlier today we can add a waits flag in the near future (defaulting to True) so that people can opt back in to the old (technically buggy) behaviour if desired. . Is this here because we pin it? I don't think astroid is used directly. Oh yes, I see david's commit (https://github.com/nameko/nameko/pull/596/commits/1a72c70d10ae9c81eaa6590e9c8a1140d797ca85). ",
    "noisyboiler": "lgtm\n. Test looks good to me - but it just looks like you're testing a test helper, which is not expected in test_service. Does this relate to framework code? Do we need to test test helpers? Apologies if i've missed the point here!\n. is this using the new async library?\n. @mattbennett @davidszotten \nI'd like to kick off an asyncio branch. This is mostly for selfish reasons such as exploring this new library and figuring out how Nameko ticks. It may get no further than a proof of concept but it sounds like tremendous fun to me.\n. Hey @mattbennett. Why can't you use something like rabbitmqctl delete_vhost test at the end of a test run?\n. by \"something like\" i meant whatever underneath this command line call. what i'm curious about is why we can't create a test vhost and then kill it at the end of the run - seems much tidier than creating random ones.\n. the request argument is part of the pytest framework and is an object you can use to introspect the \"requesting\" test function and/or add finalisers/teardown to them.\n. I'm wondering whether you have RabbitMQ running yet?\n. no worries, our fault - we'll fix that.\n. Hi @mattbennett \nWe'd love to see this land as the issue is a blocker for us getting nameko into production.\nWhat are your expectations for getting this into a new release?\nthanks\nwould you like us to help review it?\n. much appreciated chaps. \ud83d\udc4d . when setup is called this value is ignored, and indeed your docstring states it will always be overriden. So why maintain the value on the instance at all? is there a case when setup is not called and we determine the correct serializer from this value?\n. as map always returns a list, do you need the extra call of list?\n. can we now except Kombu's custom exceptions instead?\n. this is a weird pattern... it's not clear if any of your instructions raise exceptions or if it's just your own RuntimeError that you're handling.\nAnything that doesn't raise an expected Exception should be taken out of the try block.\nMaybe you could remove the vhosts explicitly based on the count rather than use the finally clause from the Exception that you raise yourself? And don't you have to remove the vhost if it's a random one or not?\nAlso, would be useful to know which of the connections were left open in your exception msg.\nis there an option to check for conn['state'] == \"open\" because this is ambiguous if there are other states.\ndoes the rabbit_manager object not have an API itself to return connections, as it has one for delete? Else could you move your helper function logic to the rabbit_manager?\nuse_random_vhost appears to be handling a problem rather than solving it.\nWhy a RuntimeError rather than something more specific?\n. sorry, that was a lot of questions!\n. it would feel better if this action raised something like a ResetError rather than raising a RuntimeError based on your count.\n. why do you leave blank lines in your test cases? you don't in the core modules.\n. i'm unfamiliar with this; what is the .ret telling you? is a negative assertion strong enough to tell us that this test is doing something useful?\n. is this essentially test_self? lib is more open to interpretation than codebase.\n. agreed! :)\n. ",
    "shauns": ":+1:\n. Migration will be okay -- the lack of an existing stack on a call just implies this is a 'fresh' call. Equally, if you call something that doesn't have this supported yet, nothing will happen either. We just use the existing dictionary on the worker context\n. Yes, am aware of the importance on the integration side.\n. Agree with Matt on moving this into its own module. I wonder how much of it needs to be public outside of standalone_proxy.\n. Hopeful fix at #95 \n. :ship: \n. :+1: \n. That sounds fine, and wait() won't block then if one container is killed, but I'm not sure how we'd handle the 'graceful stop' part of this in that case. I suppose it'd have to be done by passing in a fallback method to call on other items in the container after the first failing call. I'll put together something and see if it looks better.\n. :+1: \n. :+1: \n. :+1: on Travis\n. travis\n. :+1: \n. Okay, well that's good to know before I went too far down the rabbit hole (pun intended). We can -- I didn't notice they'd exposed it at module level\n. will change, same reason as above :+1: \n. Good spot -- thanks!\n. Thanks for the tip -- that's neater than the factory function\n. Sure, wfm.\n. Ah, nice idea. I think there's other places this can be done in nameko. I'll have a look.\n. I did try that, giving name and doc to the Mock, the issue was attaching module -- I couldn't get that to work.\nUsing @wraps isn't essential, all you're getting out of it is a slightly nicer presentation of the wrapped function, so this could be dropped.\n. It can be ManagedThreadContainer as well, and is in the tests. Anything that fulfills the requirements of being able to provide threads and be stop/kill/waitable. Will change the name to .container.\n. No, the thread that the future is running on always returns a result, so this exit handler won't have the exception thrown. This case is for setting an exception on the future when the container is killed: when this happens, the container kills the thread the future is attached to, so we're just logging that killing-thread exception.\n. That'll work for our tests, but my only concern is that if this is used in platform, its more ceremony needed for testing I suppose. The current way gives you a nicer wrapping, if it can, but asks nothing of the user. Wrapping adds nothing logically.\nIf we're less forgiving, then any consumers of this API are also going to have to go through a layer of abstraction (however they choose to do it) if they want to use a mock, or if they want to use something else that doesn't play nicely with @wraps.\nI haven't written anything that consumes nameko apart from the tests for this PR, so I leave it up to you as to what best fits with the rest of the library...\n. Let me check this one -- I did add include locking for a reason but the internals have changed a bit since then.\n. True, it's a holdover from the old checking-when-exit discussion. Will remove.\n. I've changed this to match your description :+1:\n. That's added -\n. Maybe nameko.utils.SpawningProxy? It's imported from there in runners\n. separate\n. Could we move this class inside the proxy CM method, and therefore use the correct value for service_name?\n. Worth documenting what this is for I think.\n. Good catch, will change. Its doing event work, so I don't think 1s is going to be long enough though.\n. I haven't been able to get it to reproduce -- have seen it fail precisely once. I wanted to get the TODO to flag it as I saw it during testing. It is separate to the rest call stack work.\n. It's passing on a general basis too of course.\n. Presuming side_effect supports an open-ended iterable to get its varying results, I'll change this.\n. Just reducing the string length in logging -- service name, plus method, plus a long str(uuid) is quite verbose, so taking the chance to crop it down. With a decent stack length the extra 60% makes a difference. As for using it everywhere -- well it is shorter, but not much value in changing other uses for the sake of it.\n. This is mainly to support mock configs and testing. Given we're aiming for ease-of-use for the service developer, I've made this more forgiving. If we make it less forgiving, then its a case of updating nameko tests (which is fine), but also any service developers will need to provide some 'real' config. I'm open to either approach.\n. Sure, will add.\n. CustomWorkerContext?\n. Would that be with @pytest.yield_fixture?\n. I feel like putting the responsibility at the consuming end is right (given plain dictionary configs).\nCurrently, the container publicises the config object it was given directly. So the pattern here is consistent with everywhere else (e.g. timer interval, SQLAlchemy URI, AMQP URI, number of pool workers -- that's something that needs type coercion). \nIf you don't clean at point of use, this puts a responsibility on the container to know about the configuration any consumers need, which doesn't fit well if you then develop some custom entry-point or injection of your own. \n. good catch - dropping that.\n. I think the current name is appropriate as its waiting for go-ahead inside the entrypoint, once all services are started.\n. transfer is any communication, RPC or event dispatch.\n. Yep, but its long if the call stack is.\n. Will change. The number has changed across the various reviews from 1, to unbounded, and now 200. The main thing is its a default that can be overridden.\n. Can I open another dispatcher up when I'm inside this context, hitting another service? Can we also check if it plays nicely with the RPC proxy?\n. As we're improving our docs - maybe :py:class:\\containers.ServiceContainer`?\n. I think a docstring on this would be useful\n. If this is going to be the 'introduction' for the docs for this section, an example would be good I think (or look at it when the examples docs are next looked at)\n. Much better name!\n. That's great, just wanted to check the semantics of theblocking` parameters on connection and producer.\nre: RPC interaction - Just nesting the standalone rpc proxy in standalone event dispatcher or vice versa -- checking they don't tread on each-others toes (nothing stands out that implies that would be the case)\n. Really nice!\n. It's something you can choose to override -- we do so in the OFS platform as we know that we have https://github.com/onefinestay/sentry-nameko available. In that case we ensure that the tags etc. that we'd like to log are included.\n. The work is all being done in the constructor (as with the other fields) as the context's state doesn't change over time. Can move if you prefer.\nIf you implement extra_for_logging in a sub-class of worker context, this is a useful field to have. For instance in OFS platform we use it to link together calls that have the same immediate parent i.e. all the calls a given entrypoint makes.\n. got you -\n. I think the original is clearer in how its expected to be called -- also its a stricter test\n. Fair enough, that was the only thing from me --\n. e.g. https://github.com/onefinestay/nameko/pull/115/files#diff-97c20b52d471ac86ed257f22c2849e38R30\n. comparisons in tests\n. I think it's better to have them (see http://docs.python.org/2/distutils/sourcedist.html#specifying-the-files-to-distribute , tests are intended to be included). Regardless though, it seems a mistake to be including the development requirements if we're not including tests.\nhttps://github.com/saltstack/salt/blob/develop/MANIFEST.in\nhttps://github.com/django/django/blob/master/MANIFEST.in\n. It's in the repo, its used when linting nameko. Same motivation as other comment.\n. No, it's the service name. I can drop it and get it from the class if you prefer. They get a service name by virtue of going through register_service\n. Not RPC only entrypoints and requires a config. This way is better\n. I've removed it, but having different constructor signatures would be normal for concrete classes surely?\n. I could have sworn I'd reverted this down to just match master. Will resolve.\n. I'm not sure which bit?\n. graft/prune are most like recursive-include/exclude, but minus the pattern-matching support\n. :+1:\n. I wanted to keep the methods on them so I reckoned this to be the least ugly way of achieving that -- bare functions less nice in this case I think as there's a degree of polymorphism there with the different variety of sections\n. Post-yield stuff works as it should apparently: http://pytest.org/latest/yieldfixture.html\n. @davidszotten I get you now -- good idea. Changing it.\n@mattbennett Keeping it in the main code, two reasons 1) helper will be pretty ugly and brittle as its nested equality we're doing, 2) I see no real downside, seeing as though this is meant to be library code, to providing useful classes. It's equality methods, I think we can manage the mental overhead...\n. Maybe a GreenPile instead? \npile = GreenPile()\nfor ...:\n    pile.spawn(....)\nresults = set(pile)\n. Not really sure what you mean by 'client' here.\n. I'd put this way nearer the front, maybe even on the front page -- or at least the pip install\n. \"built-in\" and \"extension\" seem mutually exclusive. I know extension is the internal name but maybe we can come up with something clearer given this is for fresh readers\n. confusing as to whether you're talking about the service you just made. worth clarifying this is mocking the injected proxy to a different service\n. -1 on \"firing\". For beginners to the framework I think we should talk more specifically about making RPC calls/dispatching events/etc.\n. This example is huge and blocks the other sections below.\n. Spelling\n. Too late maybe but module.Class is more common. https://docs.djangoproject.com/en/1.7/ref/settings/\n. This isn't enough for me to know how to use Nameko in production. Which might be fine! But its definitely missing.\n. plural\n. 201\n. Future is pretty standard terminology for what you get out of async, think it should be referenced.\n. Think this is a prime spot for n as a docstring\n. release notes/changelog\n. Bit opaque at this point in the docs. Worth mentioning what an entrypoint normally is i.e. something responding to an RPC call or handling an event.\n. ",
    "takluyver": "I just read the blog post about this. Did you consider using inspect.getcallargs() for this? It looks like it should be able to achieve the same thing without having to generate the 'shadow'.\n. Nice, it's interesting to see the performance of the different options.\n. ",
    "jessepollak": "Did this ever happen? If not, is there still interest in switching to branch coverage? If the answer is yes, we can re-tag this and start putting together a plan to make it happen.\n. @davidszotten agreed re: 0 issues and categorization. unfortunately, I need write access before I can make labels :(\n. Since progress here seems to have halted, I re-opened these changes as a new PR with the comments address in #361. Can we close this and merge that in? Thanks!\n. Wanted to follow up on this PR with two questions:\n1. @mattbennett and @davidszotten if the comments in this PR were addressed, would this change be pulled into master and shipped? If so, what priority level is the additional functionality here on a scale of 1-10?\n2. @Sparkycz do you have any interest in finishing out the work to get this shipped? If not, would you mind letting us know so we can either close the PR or delegate the work to another contributor?\nThanks!\n. We are running into this issue a fair amount both in the services we run and with clef/flask-nameko - is there an ETA for a next release that will include this fix?\n. @mattbennett I don't think I have enough context on this PR to have a strong opinion! sorry!\n. Interesting example! We've actually been discussing similar WSGI-extensibility in the mailing list here as well!\n. @mattbennett comments resolved!\n. @mattbennett tests added and docstrings + contributors file updated. anything else I can do?\n. @mattbennett awesome - updated! good call with the example-driven test :)\n. @mattbennett linter errors fixed and commits rebased into two cleaner messages! let me know if you think there's anything else I should do here.\n. done!\nOn Tue, Sep 13, 2016 at 11:00 AM Matt Bennett notifications@github.com\nwrote:\n\nI forgot to mention a changelog entry, sorry. If you can add that then\nit'll be ready for release. This is backwards compatible so the pending\nrelease will be version 2.4.1.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/onefinestay/nameko/pull/352#issuecomment-246768192,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABDA4QKBpDEsAU_tNEPR_mGNq7gcNz0xks5qpuSbgaJpZM4JtusS\n.\n\njesse pollak\njesse@pollak.io\n\u2192 register to vote: iOS http://bit.ly/2aigjW5 or web\nhttp://bit.ly/2a0Rufc\n. \ud83d\udc4f  thanks @mattbennett!\n. @and3rson nice work! I wrote the flask-nameko library, which is super similar. @davidszotten and @mattbennett - since there's so much overlap between flask-nameko and django-nameko, it seems like we should definitely try to consolidate the logic / call style into something standard across the two of them (at least at the underlying implementation level).\n\nI'd much prefer to have a connection pooled proxy merged into nameko than maintain an outside project - what are the major blockers / constraints to make that happen? @mattbennett I know you mentioned you'd want it to fall within the existing APIs, can you flesh that idea out a little bit more?\nThanks!\n. @mattbennett just to clarify, do you think this should replace the ClusterRpcProxy or should it be a new class?\n. Thanks for this in-depth explanation. I'd generally piece all this together, but it's good to have it laid out by someone who knows.\nFor the above traceback, we're using the stock standalone RPC (in the shell). That said, we're also seeing this issue in the non-standalone case. Here's a traceback for that:\nSep 20 18:03:22 IOError: Socket closed\nSep 20 18:03:22 Traceback (most recent call last):\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/mixins.py\", line 177, in run\nSep 20 18:03:22     for _ in self.consume(limit=None):  # pragma: no cover\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/nameko/messaging.py\", line 404, in consume\nSep 20 18:03:22     conn.drain_events(timeout=safety_interval)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/connection.py\", line 275, in drain_events\nSep 20 18:03:22     return self.transport.drain_events(self.connection, **kwargs)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/transport/pyamqp.py\", line 95, in drain_events\nSep 20 18:03:22     return connection.drain_events(**kwargs)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/amqp/connection.py\", line 303, in drain_events\nSep 20 18:03:22     chanmap, None, timeout=timeout,\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/amqp/connection.py\", line 366, in _wait_multiple\nSep 20 18:03:22     channel, method_sig, args, content = read_timeout(timeout)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/amqp/connection.py\", line 337, in read_timeout\nSep 20 18:03:22     return self.method_reader.read_method()\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/amqp/method_framing.py\", line 189, in read_method\nSep 20 18:03:22     raise m\nSep 20 18:03:22 Connection to broker lost, trying to re-establish connection...\nSep 20 18:03:22 Connected to amqp://account-service:**@aws-us-east-1-portal.12.dblayer.com:15353/staging-rpc-rabbitmq\nSep 20 18:03:22 Connection to broker lost, trying to re-establish connection...\nSep 20 18:03:22 Traceback (most recent call last):\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/mixins.py\", line 177, in run\nSep 20 18:03:22     for _ in self.consume(limit=None):  # pragma: no cover\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/nameko/messaging.py\", line 398, in consume\nSep 20 18:03:22     with self.consumer_context(**kwargs) as (conn, channel, consumers):\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/mixins.py\", line 186, in consumer_context\nSep 20 18:03:22     return self.gen.next()\nSep 20 18:03:22   File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\nSep 20 18:03:22     with self.Consumer() as (connection, channel, consumers):\nSep 20 18:03:22   File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\nSep 20 18:03:22     return self.gen.next()\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/mixins.py\", line 235, in Consumer\nSep 20 18:03:22     with self._consume_from(*self.get_consumers(cls, channel)) as c:\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/nameko/messaging.py\", line 349, in get_consumers\nSep 20 18:03:22     accept=self.accept)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/messaging.py\", line 364, in __init__\nSep 20 18:03:22     self.revive(self.channel)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/messaging.py\", line 376, in revive\nSep 20 18:03:22     self.declare()\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/messaging.py\", line 386, in declare\nSep 20 18:03:22 ResourceLocked: Queue.declare: (405) RESOURCE_LOCKED - cannot obtain exclusive access to locked queue 'rpc.reply-account_service-6e82e969-bd52-4fea-996d-a2950479fb7c' in vhost 'staging-rpc-rabbitmq'\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/entity.py\", line 522, in declare\nSep 20 18:03:22     self.queue_declare(nowait, passive=False)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/kombu/entity.py\", line 548, in queue_declare\nSep 20 18:03:22     nowait=nowait)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/amqp/channel.py\", line 1259, in queue_declare\nSep 20 18:03:22     (50, 11),  # Channel.queue_declare_ok\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/amqp/abstract_channel.py\", line 69, in wait\nSep 20 18:03:22     return self.dispatch_method(method_sig, args, content)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/amqp/abstract_channel.py\", line 87, in dispatch_method\nSep 20 18:03:22     return amqp_method(self, args)\nSep 20 18:03:22   File \"/venv/local/lib/python2.7/site-packages/amqp/channel.py\", line 243, in _close\nSep 20 18:03:22     reply_code, reply_text, (class_id, method_id), ChannelError,\nSep 20 18:03:22     queue.declare()\nSep 20 18:03:22 Connected to amqp://account-service:**@aws-us-east-1-portal.12.dblayer.com:15353/staging-rpc-rabbitmq\nThat said, it looks like in the non-standalone case, the error is logged, but also retried and the connection can be re-established.\n. In this PR, I've add a default timeout on the RpcTimeout initialization and then the ability to override that default (or specify one if none was supplied as a default) in the method call through the _timeout parameter.\nLooking through the discussion here, it seems like there's a strong preference towards not entering the \"user space\" of the base function call of a method. The two possible solutions that come to mind are:\n1. Using a context processor similar to the one described here that would allow one to modify the timeout on a per call basis.\n2. Add an additional way to call an rpc call that exposes more functionality, while leaving the plain calling method with the defaults around. This would look something like:\npython\nn.rpc.greeting_service.hello(name='Matt')\nn.rpc.greeting_service.hello.send(args=(), kwargs={'name': 'Matt'}, timeout=2)\nCelery does the same thing with delay (which uses defaults and mimics a standard function call by passing through all of the args and kwargs) and apply_async (which you have to specify kwargs and args, but allows much more customization.\nPersonally, I lean towards implementing an interface like (2) where the \"default\" way of calling a method is the simplest / easiest to read, but uses system default, while a slightly more involved way of calling the method allows full customization of how the call is made.\nThree questions for you @davidszotten:\n1. Is there any opposition to adding a timeout that doesn't stem from the stylistic concerns you've described here? If so, what is that opposition?\n2. How do you handle timeouts / service disappearances in situations where you currently don't have this timeout-like functionality?\n3. Would you be open to me modifying this PR to implement the pattern loosely described in (2) above? If so, do you have any guiding thoughts / stylistic preferences about the implementation?\nThanks for your thoughts / time! Also, @mattbennett would love your thoughts on this.\n. @mattbennett fixed!\n. ",
    "dAnjou": "I'd strongly vote for bpython.\n. Really nice answer, thank you very much!\n. ",
    "iho": "Hi! \nipython is pretty good, but how about  this https://github.com/jonathanslenders/ptpython ?\n. ",
    "etataurov": "Yes, this will be great,\nbut...\nfor python 2 we need a lot of code, because we need to aquire import lock, call find_module for each submodule in 'module1.module2.module3'\nfor python 3 imp is deprecated, and the best choice is to use importlib's find_spec and then load_module, but importlib module has changed between 3.3 and 3.4 (new methods was added, some became deprecated).\nSo if implement this then there will be deprecated code or a lot of code to handle different import libraries for different python version.\nIMHO current solution is hackish, but works and doesn't require a lot of code. so it's more robust\n. ",
    "rochacbruno": "what about werkzeug.utils.import_string?\n. @mattbennett what about reading environment variables?\nexport NAMEKO_AMQP_URI=\"amqp://guest:guest@localhost\"\nthen in nameko.cli.main\nimport os\nconfig = {k.partition('NAMEKO_')[-1], v for k, v in os.environ if k.startswith('NAMEKO_')}\n. ",
    "fabiocerqueira": "Maybe this example could be a good way to use environment variables on YAML config file: \nhttp://stackoverflow.com/questions/26712003/pyyaml-parsing-of-the-environment-variable-in-the-yaml-configuration-file\n. nameko_cli\n``` python\n!/usr/bin/env python\n-- coding: utf-8 --\nimport yaml\nimport os\nimport re\nfrom nameko.cli.main import main as nameko_cli_main\nENVVAR_PATTERN = re.compile(r'\\${([a-zA-Z][^}:\\s]+)(:([^}]+))?}')\ndef replace_envvar(match):\n    env_var, _, default = match.groups()\n    if default == 'null':\n        default = None\n    return os.environ.get(env_var, default)\ndef envvar_constructor(loader, node):\n    value = loader.construct_scalar(node)\n    return ENVVAR_PATTERN.sub(replace_envvar, value)\ndef setup_yaml_parser():\n    yaml.add_implicit_resolver (\"!envvar\", ENVVAR_PATTERN)\n    yaml.add_constructor('!envvar', envvar_constructor)\ndef main():\n    setup_yaml_parser()\n    nameko_cli_main()\nif name == 'main':\n    main()\n```\nconfig.yaml\nyaml\nAMQP_URI: ${AMQP_URI:amqp://guest:guest@localhost}:${AMQP_PORT:5672}\nEMAIL_PASSWORD: ${EMAIL_PASSWORD}\nSENTRY:\n    DSN: ${SENTRY_DSN:null}\nPattern must be ${VAR_NAME<:DEFAULT_VALUE>} allow null.\n<:DEFAULT_VALUE> is optional\n./nameko_cli run <module>[:<ServiceClass>] --config=config.yaml\n. @kooba go ahead :D I think this would be great. It will be nice if supports different types, int values, float values, boolean values. Thank you.\n. ",
    "kooba": "@davidszotten  @fabiocerqueira I'll be happy to pick this up if that's ok.\n. Primary reason for raising vs just returning would be ability to capture and alert on these outside of main code with something like nameko-sentry \nI agree application/json should be returned in this instance. \n. @davidszotten - Changed to use safe_for_serialization\n. @davidszotten Thank you! This was a great experience. CHANGES and CONTRIBUTORS.txt updated.\n. I had it directly on main module initially but somehow decided to move it to utils. Will move it back there.\n. Moved implementation back to main module and removed unnecessary flags. \n. Comments addressed @mattbennett \n. Comments addressed @davidszotten \n. @mattbennett copy changes addressed\n. Dependency provider is the easiest way to get config values at the moment. It's quite trivial to get it going:\nclass Config(DependencyProvider):\n    def get_dependency(self, worker_ctx):\n        return self.container.config.copy()\nand then in your service class:\nclass MyService(object):\n    name = \"my_service\"\n    config = Config()\nBasic support for environment variable substitution in config files is there:\nhttp://nameko.readthedocs.io/en/stable/cli.html#environment-variable-substitution\nCheers.\np.s. please use Nameko mailing list for these types of questions:\nhttps://groups.google.com/forum/#!forum/nameko-dev\n. Hi @manfred-chebli & @rgardam have you docker \"linked\" your broker container to your service container? Or do you just rely on Docker embedded DNS server by simply using the name (broker in your case) of the container?\nOnce we've upgraded to master version of Eventlet (before they cut 0.20 release, we needed it to use a fix to HTTPS body parsing https://github.com/eventlet/eventlet/blob/master/NEWS#L8) we noticed that Docker embedded DNS name resolution does not work any more. \nSince eventlet vendored dnspython I've opened issue directly with dnspython here https://github.com/rthalley/dnspython/issues/219 but they said this is expected behaviour. There is a gist to reproduce the issue. \nThe workaround we've employed for now is to trick dnspython into thinking that our container names are fully qualified domains by adding period . to the url. In your case it would be amqp://user:pass@broker.:5672\nI think by using dnspython for dns resolution eventlet broke some functionality. \n. Separated extensions from other contributions. Added new words to spelling wordlist.\nPlease let me know what you think. . @lagelalegal have you looked at this extension? https://github.com/harel/nameko-cors as an example of cors support?. I'm bit worried about adding anything without tests to that list \ud83d\ude04 ... I might open PR against nameko-cors with some tests first \ud83d\ude09 . I think you want to call dependency provider before entrypoint fires, maybe this will help:\n```python\nimport wrapt\nimport pytest\nimport inspect\nimport json\nfrom nameko.dependency_providers import DependencyProvider\nfrom nameko.web.handlers import http\nclass DataLoader:\ndef __init__(self, client):\n    self.client = client\n\ndef load_data(self, wrapped, foo=None):\n    @wrapt.decorator\n    def wrapper(wrapped, instance, args, kwargs):\n        self.client(\"loading data from external source {}\".format(foo))\n        return wrapped(*args, **kwargs)\n    return wrapper(wrapped)\n\nclass MyDependency(DependencyProvider):\ndef start(self):\n    \"\"\"Any setup for data store client session\"\"\"\n    self.client = lambda message: print(message)\n\n@staticmethod\ndef load_data(foo=None):\n    def find_loader(instance):\n        \"\"\"Find instance of our DependencyProvider,\n             what is returned from get_dependency\n        \"\"\"\n        data_loader = inspect.getmembers(\n            instance, lambda obj: isinstance(obj, DataLoader))\n        return data_loader[0][1]\n\n    @wrapt.decorator\n    def wrapper(wrapped, instance, args, kwargs):\n        decorated = find_loader(instance).load_data(wrapped, foo=foo)\n        return decorated(*args, **kwargs)\n    return wrapper\n\ndef get_dependency(self, worker_ctx):\n    return DataLoader(self.client)\n\nclass MyService:\nname = 'my_service'\n\ndata_dependency = MyDependency()\n\n@http('GET', '/foo')\n@data_dependency.load_data(foo='bar')\ndef my_http_endpoint(self, request):\n    return json.dumps({'ok': True})\n\n@pytest.fixture\ndef web_session(container_factory, config, web_session):\n    container = container_factory(MyService, config)\n    container.start()\n    return web_session\ndef test_will_load_data_from_before(web_session):\n    response = web_session.get('/foo')\n    assert response.status_code == 200\n```\nExample above creates dependency provider that exposes @data_dependency.load_data decorator you can use on every entrypoint.\nPlease checkout Community Extension page for example of Redis dependency providers.. All heavy initialization should happen in def start(self): method. It will be run only once during the start of your service. . RPC over RabbitMQ, as implemented in Nameko, gives you service discovery for free where you can have multiple services handling RPC requests effortlessly. Additionally Nameko comes with HTTP endpoint (based on werkzeug) which lets you build any REST APIs you'd like. . Thank you @ornoone looks great. Just a few more documentation fixes and I think it will be good to go. . @mattbennett @davidszotten this looks good to me. This change in combination with global config from https://github.com/nameko/nameko/pull/520 will give users quite a flexibility.. @worldmind gRPC is very interesting indeed. We are very early in process of exploring feasibility of adding client/server gRPC extensions to Nameko. gRPC relies on HTTP2 and maybe with the help of h2 library this would be possible. \nIf you look at current official gRPC python support, workflow that is accomplished via statically generated stub (via C core lib) leaves lots to desire. Maybe Nameko implementation could provide simpler, more pythonic approach to creating gRPC services. But again it is very early and the scope is unknown at the moment. \n. @gpkc nothing to report yet.. Would drop of QueueConsumer be considered a breaking change and this land in v3.x.x? . Hi @MMmaomao \nhttps://discourse.nameko.io/ is a great place for any questions about Nameko.\nTo answer your question, Nameko (just like Flask) is using werkzeug for its web request processing needs. \nrequest arg you get in your http entrypoint will have files attribute on it which \"basically behaves like a standard file object you know from Python, with the difference that it also has a save() function that can store the file on the filesystem.\". Set of Kafka entrypoints and dependencies would be awesome! Would it be best to start them as separate library and battle test them in your production environment? I have an idea for the name nameko-kafka, looks like it's available on pypi.org. \ud83d\udc4d . Just tried hello world example from https://docs.nameko.io/en/stable/#nameko and had no issue. I'm on macOs though. Use of regex is optional and that dependency is not installed with Nameko by default and re should be used. \nUnfortunately none of the developers working on Nameko use Windows OS so there might be some incompatibility. . Changed wording slightly. This is first mention of expected_exceptions in the docs. \n. My bad, this was a mistake in double encoding in the example. \n. This is fixed now. Doing it in response_from_exception only.\n. Changed all HTTP tests in this module to user web_config and web_session\n. Once response_from_exception is overridden, client's only option to raise different http errors would be to carry that information in exception. To be more explicit other helper exceptions that inherit from HttpError could be created by client like InvalidAgruments (422), EntityNotFound (404) or BadRequest (400) etc... \n. I simplified example of raising custom http exception. Adding status code in payload is a common practice (twilio, eventbrite, uber) but I removed it from the example to avoid any confusion.\nPlease review.\n. Removed mention of expected_exceptions. This will be covered by #293\n. Incorporated changes this suggestion. \n. Removed.\n. You're right, seems redundant here, we are only processing one line at a time. \n. Looks like you used tab instead of 4 spaces as this file expects. . This seems to be a new sentence. Maybe we should start it with capital letter and end with period? Looks like 2 sentences above yours missing period as well. . Not 100% sure how this works, but was this supposed to be extra: regex==2018.2.21?. this feature requires regex package. many levels of default values. this config accepts AMQP_URI as an environment variable, if provided RABBITMQ_* nested variables will not be used.. the parser for environment variables will pair all brackets.. amqqp/amqp. amqqp/amqp. attibute/attribute. Actually not sure if the sentence is correct. Is dict access really another way to specify target service and method name?. Maybe you taught your spellchecked to accept attibutes by now ;) . locsl/local. Not a typo but I think Nameko settled on American English spelling? behaviour/behavior?. times for out / times out for. Might want to add context_data to :Parameters: doc above.. ",
    "rejoc": "It would be interesting to allow multiple config files.\nIn many applications, you have part of the configuration that is common to many/all services (like AMQP_URI, exchange, serializer, database url...) and some services need vry specific paramters.\nAllowing --config to be a list (nargs='+') and updating config with these files would do the job \nsomething like this:\nconfig = {}\nfor conf_file in args.config):\n    with open(conf_file) as fle:\n            config.update(yaml.load(fle)). the config management of other tools allow you to \"include\" config files in config files. But there is no such thing with yaml (or I did not find it). It would have been my prefered choice.\nPersonnaly, I don't like the idea of having an external tool to build the config file from a \"meta\" configuration file. . Yes it's a reality ;-)\nIt's not very easy to buid automatic tests as you need to setup a policy to describe the alternate exchange of one exchange.\nThen, in a pure amqp philosophy, I think it's better to answer a RPC request to the connection_id AND the exchange given by the request (or to respond directly to the queue that sent the request without using an exchange). \nThe exchange defined in the configuration file is to be used to send requests.. . I also think that defaulting the exchange to the config definition is not necessary. But I was not sure 100%. I'll take it off.\nI have to check the definition of the alternate exchange within the exchange definition. This is not the solution I choosed for my configuration because it is not the \"prefered\" method (I did the AE definition policy from the RMQ web interface..). \nMy other concern about this method is about : what happens if the exchange already exists and you add or remove an alternate exchange. Does it accept the change or return an error ?\nTo implement the alternate exchange through policies (the preferred solution for RMQ) , I think I need to use the python rabbitmq_admin library https://pypi.org/project/rabbitmq-admin/. What do you think about that ?\nWhatever the method to create/bind this AE, I could add a \"rpc_alternate_exchange\" parameter in the config file and use it to automaticaly create the AE.\n. > Even though AMQP declaration of AEs is \"discouraged\", if it works then I think it's a better approach for the specific use-case of Nameko's test suite. And for this specific use-case, the exchange will never previously exist, because each test is run in its own vhost.\nI agree with you for tests but, actually, I had the idea to implement a new rpc_alternate_exchange parameter that could be use to directly configure the AE without having to go to the RMQ manager for real applications (not tests). Here we have to deal with potentialy existing exchange. \nBut yes, first, I will setup a test.  . ",
    "baverman": "\njust stopping (and presumably later restarting) a service.\n\nI am a dumb-dumb, missed a https://nameko.readthedocs.org/en/latest/cli.html#running-a-service section. I thought that nameko run starts all services it can find. Thank you.\n. ",
    "vtbassmatt": "Great, thanks!\n. Seems like that would work too. I'm new to rabbitmq so don't know what the community most expects.\nSent from Outlook for iPhone\nOn Fri, Apr 10, 2015 at 3:27 PM -0700, \"David Szotten\" notifications@github.com wrote:\nhey, nice catch. though i wonder if we should consider changing our defaults to use the default rabbit vhost (/) instead. \n\u2014\nReply to this email directly or view it on GitHub.\n. ",
    "ayoshi": "Agree, that definitely is a step in the right direction - though kombu's API for registering custom serializers is not pretty - it requires a module installed with a registered entry point, but at least it's doable.\nThere still would need to be a configuration setting in nameko' - since serializers are registered at a consumer level, which nameko  abstracts over.\n. #244 \n. @mattbennett \nHi.\nI've done a second iteration, and the problem is much hairier than it looks.\nBasically, kombu's behavior when you don't pass serializer is different from when you simply pass json as a serializer to a publisher and accept to a consumer.\nIn case no values are passed, it doesn't care about message type, and accepts them silently, defaulting to 'text' if string is passed and 'data' if it sees binary data. In such cases it will NOT serialize them at all (This is what happens in the current Nameko version). If you pass accept however, It refuses to accept any other types and encodings. \nThis breaks some of the tests which rely on passing unserialized data (test_serialization.py, test_rpc_responder.py) but those are easy to fix, I did it in one of the commits, creating parametrized fixtures to test both json and pickle.\nThere is a bigger problem, however:\nIn unit tests in many places the Container is mocked with empty_config fixture. Since the serializer/accept values will be set to None in this case, kombu will refuse to decode the message, the moment accept list is empty.\nDuring normal lifecycle this doesn't happen in, since I use Container. init to set up the serializer correctly for the container.\nThis behavior breaks many tests for py27-oldest tox environment (kombu-3.0.1)\nIt seems the behavior changes somewhere after kombu-3.0.1 (I didn't check when exactly) - if None is passed, kombu will revert to default behavior - so all the other versions pass all the unit tests.\nIt might be that I missed something else, I'm not familiar with all the details of behavior yet.\nOne option would be to refactor empty_config fixture to use DEFAULT_KOMBU_SERIALIZER.\nThat would fix the tests, and probably is the correct way to do things - this will make all messages during the test serialized, and more importantly will set accept properly for test Consumers\nThe other would be to add None/None Content types to the default accept list, but IMHO this will just hide improper behavior - I think every message in the system probably should be serialized.\nI'm submitting what I already have into this pull request for you to review - let me know how you would like it worked around.\nOtherwise - all the test work in other versions ( I tested by setting DEFAULT_KOMBU_SERIALIZER ) to pickle\n. After I can fix this py27 - I'll refactor the commit to be more manageable - and I can also add parametrization over container_factory fixture to run all the tests both with json and pickle.\n. Phew!\nI think I'm done. Summary on the change - since that's a lot of commits:\n1. Messaging / Events / RPC are now aware of accept/serialize.\n   The settings are on the container level, but can be overridden by directly accessing the values after setup\n   None is always replaced by default serializer, so we don't send or accept unserialized messages. \n   Again, it can be overriden post-setup.\n2. Config settings : SERIALIZER_CONFIG_KEY/ DEFAULT_SERIALIZER ( removed kombu reference )\n3. ALL versions including py27-oldest work properly, actually that specific version failed, because it turned out that test suite relied on all sorts of implicit behaviors, which became apparent after serialization support had been introduced, so it should stay :)\n4. Most of the changes required test overhauls, that is what has been done:\n   - All tests using rabbit_manager to publish now explicitly send content_type in places where we use kombu.consumers to verify\n   - Mocking of the container has been factored out and parametrized as a pytest fixture. That means that all tests using containers will test both json and pickle as serializers.\n   - Most of the messages that we sent raw are now sent as json strings '\"msg\"' instead of 'msg'. That is a minor change, so kombu consumers can deal with them, since we only accept serialized messages now.\n   -  test that specifically relied on JSON have been generalized and parametrized with both json/pickle\nBecause of parametrized mock_container all relevant features are tested both with json and pickle, so serialization is properly covered by test 100% \nOver to you.\n. @davidszotten \nI used pickle to support serializer testing, because it's already in the standard library and is supported by kombu. I could use yaml, but I think pickle+json a better bet for testing, since some unittests relied on json losing types during serialization, and yaml behaves the same way. I wanted to see how something like pickle would perform.\nAs for documentation - yes, I agree - it would be better to use yaml in examples. \nThough, for the record - I'm not really afraid of people using pickle - for many systems security concerns apply on a different level - and when you're working on a distributed system and your types are not yet stable, pickle is much more convenient, you can work quickly without fighting with serialization formats.\nAlso, to test yaml - it's a small change here:\npython\n@pytest.fixture(params=['json', 'pickle', 'yaml'])\ndef mock_container(request, empty_config):\n    container = Mock(spec=ServiceContainer)\n    container.config = empty_config\n    container.config[SERIALIZER_CONFIG_KEY] = request.param\n    container.serializer = container.config[SERIALIZER_CONFIG_KEY]\n    container.accept = [request.param]\n    return container\nI didn't make it, because it would increase testing time a bit - 20 more tests, but we can add yaml too if you'd like.\n. @mattbennett \nWith mock_container factored out - we can simply use another one (something like mock_container_with_serializer)  to single out tests that do publish/receive message and parametrize that specific fixture - how does that sound? If that's an option - let me know which tests you want with mock container.\nOtherwise - I wanted to make sure all the tests that really send/recieve messages work with both serializers - it's only 20 tests that are affected, and all seem to be in relevant places ( test_events, test_messaging, test_rpc ) - nobody else uses mock containers, though I'm not sure about specific tests -\nIt seemed like a most bang for the buck.  I'm not familiar with test codebase enough to create a new array of tests specifically to test serializers - and it looks like time investment would be too prohibitive for me.\nThe difference in run time is 69s vs 73s on my laptop.\nI'll see if I can add simple custom serializer test.\n. @mattbennett \n1. Ok, many tests with mock container actually publish raw data, so don't need to be with parametrized mock container. Could you let me know if there are tests which don't use mock containers need to be tested with both serializers?\n2. Though I still think that having mock container as a fixture is a good idea, do you want to remove it from all tests, and apply only in tests that test publishing?\n. @mattbennett \nI'll drop parametrization from mock_container ( the rest of the test fixes are OK, I think ). I also want to reformat the commit to be smaller ( per file ) - there were too many changes during experimentation,\nfactor out an unserializable fixture ( it's used twice ), will fix a small mistake in parameters and will pass it over to you - If you could add a couple of explicit tests that would be great - It'll take me too much time to go through the tests at this stage of familiarity with nameko's codebase.\nAfter that I'll see if I can help cleaning up the tests a bit in general - but unfortunateIy I can't give a hard promise - I'm currently overloaded a bit, maybe piece by piece reorganisation one commit at a time.\n. @mattbennett \nOk, I redid the commits so they would be more manageable, removing all the intermediate changes. I also removed parametrization from container mock, and cleaned up small things. \nPassing it over to you, to add separate serialization tests. Let me know if anything is lacking.\n. Yes, they are all different with regard to None.\nI'm not sure yet whether to allow None at all,  or where to allow it. I'll\nhave a new variant, which seems to be a bit cleaner, I will post it after\nfixing the tests\nOn 27 Apr 2015 20:17, \"David Szotten\" notifications@github.com wrote:\n\nIn nameko/standalone/events.py\nhttps://github.com/onefinestay/nameko/pull/244#discussion_r29168017:\n\n@@ -31,6 +34,11 @@ def event_dispatcher(nameko_config, **kwargs):\n     def dispatch(service_name, event_type, event_data):\n         conn = Connection(nameko_config[AMQP_URI_CONFIG_KEY])\n-        if KOMBU_SERIALIZER_CONFIG_KEY in nameko_config.keys():\n-            serializer = nameko_config[KOMBU_SERIALIZER_CONFIG_KEY]\n-        else:\n-            serializer = DEFAULT_KOMBU_SERIALIZER\n\nfoo.get(key) or default is different to foo.get(key, default) if e.g. foo[key]\n== None # or 0\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/onefinestay/nameko/pull/244/files#r29168017.\n. \n",
    "tzengerink": "Thanks for your constructive feedback.\nI have:\n- removed the --plain argument and made plain an available shell.\n- added ShellRunner responsible for starting the interactive shells, so the globals are gone.\n- added the manual startup script to be run before each of the interactive shells. \n. ",
    "rouge8": "The upcoming gevent 1.1 release supports PyPy, is that an option?\n. ",
    "astawiarski": "edited:\nRecent versions of PyPy support both eventlet and gevent. \nNameko is passing whole test suite under PyPy3 2.4.0 (Python 3.2.5 compatible), but fails with plenty of exceptions under PyPy 4.0.0 (Python 2.7.10 compatible). I haven't identified why is it so.\nHowever, code coverage check has to be disabled when testing for either PyPy. Coverage plugin uses CTracer extension that's written in C and was made to work with greenlets (eventlet), but PyPy can not load it. So it's trying to use PyTracer instead - which does not support eventlet.\nWe had Nameko working under PyPy 2.4.0 for quite a while in testing environment and there haven't been any issues. I think it would be worth to extend tox/travis config to include PyPy at least as an optional build step.\nOn another note, our initial performance tests didn't show any improvements for Nameko itself when running under PyPy. But the more CPU-bound you have in your services, the more likely you're going to benefit from running it with PyPy.\n. We had similar issues and decided to turn off the persistancy of messages - they need to be done immediately or not at all was our approach. I started working on a nicely configurable approach in nameko, but didn't have time to finish. For the moment, we are overriding settings for the whole project.\nYou can see some comments and work started in https://github.com/onefinestay/nameko/issues/292\nIn summary, the following on its own gave 10x increase in message rates. \nfrom kombu import Exchange\nExchange.delivery_mode = 1\n. @mattbennett : I made \"shell\" and \"run\" more consistent and both are now using config.yaml\n@davidszotten : Good idea, I will improve it on separate branch since it may involve more changes (unless you prefer a larger pull here?). Initially I wanted to just add parsing to main.main() , but that won't sit well with current tests that are calling parse_args() directly. So I'll most likely try with argparse.Action . \n. I had a look at unifying that config parser and there is more to it indeed. Tests for shell and run would need to change. I just updated contributors and changelog. \n. Added a few more paragraphs. There is probably much more to it, but I hope it will be enough for developers to understand potential issues and not crawl through code with debugger in hand :)\n. I've got another proof of concept ready where all the delivery options are provided when instantiating the service proxy. I think I found all the places where those options could be applied and the only real trouble is ensuring that delivery options of responses follow those of requests. \nIn addition to that I extended @rpc decorator to allow providing delivery options for responses specifically. The need for us was to allow defining compression on message for some methods only - the ones that return large lists of entries. So it made sense that the service decides whether to apply compression or not.\nI will push both later today (will still be WIP). Unit tests are pain though, cause requests cannot be sniffed as easily as response messages (nameko tests suite only supports sniffing responses). \nIn regard to other AMQP options, there are more of course, but those 3 I mentioned cover the majority of use cases and they are the ones fully supported (virtually or natively) by other kombu transports (should you want to support them later). \n. So my new proposal is as follows. When creating ServiceRpcProxy or calling on dependency provider to get such proxy, developer can provide delivery_options attribute in order to override kombu's defaults whenever interacting with a target service.\nIf, for some reason, different set of attributes is required for specific calls, developer can just create another proxy with different configuration.\nIdeally, whatever parameters are being applied to the request, should also be applied to the response message.\nIn some cases, we'd like to be able to override those options for all responses from a particular method. For example one that returns large chunks of data (compression). \nSample usage:\n``` python\nlarge_msg_opt = {'compression': 'gzip', 'delivery_mode': 'transient'}\nfast_msg_opt = {'delivery_mode': 'transient', 'timeout': 5}\nclass Service(object):\n    name = \"service\"\n@rpc\ndef echo(self, arg):\n    return arg\n\n@rpc(delivery_options=large_msg_opt)\ndef echo_compressed(self, arg):\n    return arg\n\nwith ServiceRpcProxy('service', rabbit_config, delivery_options=fast_msg_opt) as rpc_proxy:\n    rpc_proxy.echo(test_data)\n    rpc_proxy.echo_compressed(test_data)\n    rpc_proxy.echo.async(test_data)\n    rpc_proxy.echo_compressed.async(test_data)\n```\nInitial development, far from finished, but you may get some idea what I'm trying to achieve:\nhttps://github.com/onefinestay/nameko/compare/master...astawiarski:custom-delivery\n. Yes, it was from \"i.e\", but it was failing on travis and on my local (ubuntu/mint). I can revert those.\n. ",
    "stchris": "Quick question about this: the 2.1.1 docs advertise the --config option as being available, but it doesn't seem to work:\nnameko: error: unrecognized arguments: --config\nThis is supposed to be in the 2.1.1 release, right?\n. The confusing thing is that the latest version docs are labeled 2.1.1, this is where I read about the --config option. According to http://nameko.readthedocs.org/en/latest/release_notes.html there is no 2.1.2.\n. ",
    "zmrow": "Any word on this?  :) Interested as well. \n. I didn't see anything at the end of the current build's logs...  However I did find this line at the end of build 1895.12:\nSkipping a deployment with the pypi provider because this is not a tagged commit  (Line 475)\nI'm not a travis user, so it's more than possible I missed something.\n. Oh interesting!  Hmm.  Anything I could help out with?\n. The 2.2 tarball got me unblocked, but I'd like to be on the latest - would another deployment possibly fix this?\n. Thanks so much!  My company has an import tool that is opinionated about where and how it gets its packages.  Really appreciate it.\n. ",
    "chris-hailstorm": "This is a great idea -- probably needs to be SQS + SNS, however.\nSQS doesn't do pub-sub -- there is no exchange or routing concept.\nThe pub-sub pattern for AWS combines SNS topics and SQS queues.. Awesome, thank you\n. ",
    "omsobliga": "Thanks :)\n. ",
    "morty": "Hi @mattbennett, I don't think that PR fixes the issue I'm seeing. It just switches to using the print statement.\n. @mattbennett OK, now I see events.py is changed too.\n. ",
    "radekj": "Should be fine now, sorry.\n. ",
    "davidweterings": "Exception now uses six.raise_from!\n. Yikes, newbie mistakes! Both are fixed now.\n. The port is now also changed to 4.\n. ",
    "jiamo": "Just about the doc. I only find code in test using py.test framework to test service. Can we have some examples don't using py.test. In real project we may don't use py.test helper as client code to call service.\n. Thanks. Some times we need run without  nameko command.\nAfter using nameko run test_service_server --config nameko_server.conf\nmy client py can work now.\nBut after comment the  runner.stop(),  python test_service_server.py still can' t work.\n. ",
    "omidraha": "It's obviously, but please add something like this to the example of index page:\n\nTo run above code, insure that you have RabbitMQ running, for example:\n$ docker run  -p5672:5672 rabbitmq:latest. \n",
    "tyler46": "@mattbennett I can handle this addition on the docs if it's ok.. @mattbennett great, I will prepare a PR for review.. Hi @davidszotten,\nI thought by writing \"nameko has built its RPC and other features over AMQP\", I was explaining why we  need [rabbimtq]. Of course I can rephrase it to be more accurate. Do you think something like the following would be better:\n\nBefore running nameko service, please be sure that you have a [rabbitmq] instance running, since nameko built-in rpc functionality relies on AQMP that is being implemented by [rabbitmq].\n\nI agree with you that using a hello-world example using http entrypoint would be less good that current one. \n. Hi @mattbennett,\nOK I will update PR accordingly.\nThank you. thanks @iky !. ",
    "iky": "Fixed by https://github.com/nameko/nameko/pull/591\nThanks @tyler46!. Hi David, thanks for the quick reply.\nYou are right, addressing errors by service name instead of module path is better. RP calls are addressed by service and method names, the remote_error should address exceptions the same way. Maybe the decorator should live in (be imported from) rpc module as well?\nThe path quite works in our current setup where exceptions always live in the same place of the service package (myservice.exceptions.NotFound), but that does not need to be always the case and mainly it brings the contract to the internal module structure of the service as you mentioned. Using service name and exception type name is much better.\n```python\nfrom nameko.rpc import remote_error\n@remote_error('service-a', 'NotFound')\nclass ItemNotFound(LookupError):\n    pass\n. Added the possibility to define config entries from console (overrides possible config file entries):\n$ nameko run service \\\n--define AMQP_URI=pyamqp://someuser:*@somehost/ \\\n--define max_workers=1000 \\\n--define SLACK=\"{'TOKEN': '***'}\"\n```. ## Remaining Changes\nIn order to fully move to the global config, some additional things need to be solved, such as backward compatibility so the change will not brake 3rd-party extensions. Also pytest fixtures should keep existing config related features.\nBackward compatible Config on Container\nBuilt-in extensions should be changed to use nameko.config. 3rd party extensions should then catch up and migrate too:\n``` python\nfrom nameko import config\nclass Database(DependencyProvider):\n    def setup(self):\n        db_uris = config[DB_URIS_KEY]\n        # ...\n```\nFor making the changes backward compatible, the container config if accessed should raise a deprecation warning saying that the nameko.config should be used instead, but still returning the expected config.\nThe Container.config property after raising the deprecation warning should return nameko.config keeping the 3rd-party extensions still working:\npython\nclass Database(DependencyProvider):\n    def setup(self):\n        db_uris = self.container.config[DB_URIS_KEY]  # <- should still work\n        # ...\nOnly issuing a warning:\n.../nameko/containers.py:173: DeprecationWarning: Use ``nameko.config`` instead.\n  warnings.warn(\"Use ``nameko.config`` instead.\", DeprecationWarning)\nConfig setup helpers\nMainly for tests, but also for custom runners requiring in-program config setup, it would be good to have a helper function for setting up config in a context...\n``` python\nfrom nameko import config_setup\nwith config_setup(my_config):\n    ServiceContainer(Service)\n```\n...rolling back config changes on the context exit. The context manager allows for sync use of multiple different configs per program which I think has practical use only in tests.\nThe config_setup should be available as a simple function too - useful for bootstraps and scripts. The CLI commands of Nameko maybe should use it too when setting up config from file and from cli options.\nContainer and Runner\nOther places documented as part of Nameko API and using config are service and container runners.\npython\nServiceContainer(Service, config={})\nServiceRunner(config={})\nI would propose removing the config passing completely as a breaking change:\npython\nServiceContainer(Service)\nServiceRunner()\nIf a specific config should be used, it can be achieved by using config_setup context manager described above.\nStandalone\nSame approach as with the other runners, but reusing the existing context manager in case of standalone RPC proxy. By making the config optional argument of ClusterRpcProxy, one can still do\n``` python\nfrom nameko.standalone.rpc import ClusterRpcProxy\nconfig = {\n    'AMQP_URI': AMQP_URI  # e.g. \"pyamqp://guest:guest@localhost\"\n}\nwith ClusterRpcProxy(config) as cluster_rpc:\n    cluster_rpc.service_x.remote_method(\"hell\u00f8\")  # \"hell\u00f8-x-y\"\n``\nIf a config is passed, the proxy context is also wrapped inconfig_setup`, otherwise nameko global config is used as it is.\nIn case of the (so far undocumented) standalone event publisher, the dispatcher can be also made context manager with optional config:\n``` python\nfrom nameko.standalone.events import event_dispatcher\nconfig = {\n    'AMQP_URI': AMQP_URI  # e.g. \"pyamqp://guest:guest@localhost\"\n}\nwith event_dispatcher(config) as dispatcher:\n    dispatcher.dispatch('some_service', 'some_event', {'some': 'payload'})\nor by using `config_setup` as context manager (probably only if two clusters are involved, or some other complex case): python\nfrom nameko import config_setup\nfrom nameko.standalone.events import event_dispatcher\nwith config_setup(config_cluster_foo):\n    dispatcher = event_dispatcher()\n    dispatcher.dispatch('some_service', 'some_event', {'some': 'payload'})\nwith config_setup(config_cluster_bar):\n    dispatcher = event_dispatcher()\n    dispatcher.dispatch('some_other_service', 'some_event', {'some': 'payload'})\nor simply as as a function (in a script or when bootstrapping something more complex than a simple script): python\nfrom nameko import config_setup\nfrom nameko.standalone.events import event_dispatcher\nconfig = {\n    'AMQP_URI': AMQP_URI  # e.g. \"pyamqp://guest:guest@localhost\"\n}\nconfig_setup(config):\ndispatcher = event_dispatcher()\ndispatcher.dispatch('some_service', 'some_event', {'some': 'payload'})\n```\nMaybe all three should work. The last two should definitely work.\nTesting\nConfig Fixtures\nExisting rabbit_config, web_config and maybe empty_config will be kept but made all pytest.yield_fixture generators using the config_setup helper to \"reset\" the config after each use. Rabbit config already is a generator - it's yield will only get and extra wrap with config_setup.\nUsage of existing config fixtures remain the same except they will not be passed anymore to container or runner factories:\npython\ndef test_service_x_y_integration(runner_factory, rabbit_config):\n    runner = runner_factory(ServiceX, ServiceY)\n    runner.start()\n    # ...\nNot sure what is the most correct pytest way if a fixture is not used inside the test function, maybe the use_fixture decorator is then better:\npython\n@pytest.mark.usefixtures('rabbit_config')\ndef test_service_x_y_integration(runner_factory):\n    runner = runner_factory(ServiceX, ServiceY)\n    runner.start()\n    # ...\nUsers can define test config for their services reusing existing nameko config fixtures::\n``` python\nfrom nameko import config_setup\n@pytest.fixture(autouse=True)\ndef config(web_config, rabbit_config):\n    config = {\n        # some testing config, defined in place or loaded from file ...\n    }\n    config.update(web_config)\n    config.update(rabbit_config)\n    with config_update(config):\n        yield\n```\nFactory fixtures\nBoth container_factory and runner_factory fixtures take config as required argument. Passing the config can stay making it only optional which if passed would apply coinfig_setup context manager. Other option would be to remove it as a breaking change similarly as with ServiceContainer and ServiceRunner.\npython\n@pytest.mark.usefixtures('rabbit_config')\ndef test_event_interface(container_factory):\n    container = container_factory(ServiceB)\n    container.start()\n    # ...\nDeprecating Config Dependency Provider\nGlobal config would make the Config DD obsolete and deprecated. The get_dependency should raise a deprecation warning.. @mattbennett I should get back to it very very soon!. This is now ready to be reviewed.\nBesides adding nameko.config and --define, this PR includes also the following changes:\n\nContainer.config is now deprecated in favour of nameko.config and no built-in Nameko component uses it any more. It still returns, so existing community extensions would work. A deprecation warning is raised if the container config is accessed.\nNo Nameko component gets config passed now - nameko.config  is used instead. Therefore there are braking changes - non of the following takes config as an argument anymore:\nServiceContainer and ServiceRunner.\nStandalone ClusterRpcClient, ServiceRpcClient and event_dispatcher.\nTesting fixtures container_factory and runner_factory.\nnameko.config_update and nameko.config_setup helper functions and context managers added. These are used only in tests (config update example, another update example, config setup example). There is a question, should these helpers then be moved to testing module?\nThere is a partial change to the use of empty_config, rabbit_config and web_config which now do NOT return the config dictionary any more, instead they use nameko.config_update to update nameko.config just for the context of the tests these fixture are used for - this is also a braking change. Existing tests changed accordingly.\nBuilt-in Config dependency provider deprecated.\n--broker CLI option is deprecated in favour of --define and --config. Still works (for now) but raises a deprecation warning.\nAll messaging dependency providers (publisher, RPC publishers and events dispatchers) now take uri as an argument setting up rabbit URI (just for the dependency, defaults to  AMQP_URI config setting). Here's ServiceRpc example.. > What do you think about apply_config (or set_config) and update_config instead of config_setup and config_update? Verb-first feels a little more natural to me.\n\nYep, agree, will change it .... > > What do you think about apply_config (or set_config) and update_config instead of config_setup and config_update? Verb-first feels a little more natural to me.\n\nYep, agree, will change it ...\n\nDone. It looks like the destroy part of #481 shuts down connections only for particular vhosts. When some teardowns are still running beside a running test, the shooting down should not interfere with connections used in the running test as the closings of each teardown is done for different vhost connections. Unless the connections are also shared across vhosts, which seems unlikely as they have to be per vhost authorised. Or am I missing something else?. Thanks for the review @mattbennett!\nI added multi-serialization support also to standalone RPC proxy and same serializer setup to standalone event publisher too. For that I extracted the serialization setup to its own module.. Suggestions are welcome. Maybe entrypoint expected_exceptions can be also taken into account here and if the exception raised inside a worker is one of the expected ones, it then can be logged as WARNING or maybe even INFO. But the \"unexpected\" ones I think should be logged as ERRORs or at least as WARNINGs.. Thanks for the review.)\nExpected worker exceptions are logged as INFO, unexpected as ERROR. Container exceptions as CRITICAL.\n@davidszotten I agree that WARNING for expected exceptions might be quite high. It really depends - bad request kind of exceptions logged as WARNING might suggest to tell clients to fix the way they call the service. On the other hand \"not found\" sort of exceptions are used regularly by clients to check if an object exists and these are better logged as INFO as there might be many of them.\n\nWe should probably do whatever webservers do when they serve 4xx and 5xx errors.\n\nTornado, Django and Flask all log 5xx as ERROR and 4xx as WARNING. We're pretty close .). I changed the expected exception level from INFO to WARNING to follow the common practice. This results to the following logging levels of nameko.container logger:\n CRITICAL for container errors\n ERROR for unhandled worker errors\n WARNING for unhandled but expected worker errors\n INFO and DEBUG for debugging purposes\nThe main benefit is to \"highlight\" entrypoint failures in the logging stream and doing that mainly for Nameko users (meaning people writing and running services) whilst keeping the logger well usable also for Nameko developers (people writing and maintaining extensions).. Ah, did not see that. @timbu @mattbennett It's not just Publisher of messaging module, EventDispatcher of events module needs to be changed too, see https://github.com/nameko/nameko/pull/600/commits/5c416cee1dcfd791839e2a7bedd2b8ee557a1aca. I've updated the remaining places. The fixes are tiny, maybe we can merge this to 3.0.0 before #573 lands in master and then dealing with merging the two together?. Sure thing, parameterised.. Sure thing, parameterised.. True, it should be checked and it should raise ConfigurationError with a clear message rather than KeyErrors later on and on various places.. True, it should be checked and it should raise ConfigurationError with a clear message rather than KeyErrors later on and on various places.\nA check added.. having each url example defined in its test is much more readable than all coming from one fixture :+1: . how \"PEM lib\" in the message explains a missing ssl option? .). A little bit confused from the var naming - ssl above, ssl_params here and ssl_options below in tests - aren't these three the same thing? .). nice .). :+1: deprecated - raising a warning, but still setting the broker uri.. True, I removed it from the banner.. This is a good point, if you for example set this publisher option to a serialiser which has options defined in config (encoder, decoder), these options may not be loaded in.. This was to retry on any error, including connection error which is caught and wrapped into\ngeneric exception by the rabbit management client. We can get connection refused if sqs requests mock leaks to the vhost deleting green thread.\nSee https://github.com/iky/nameko/commit/f7f502fd5cc18a7a6f74a57981333ce621532c23.. I believe there is rabbit URI shown in the banner of the interactive interpreter so the shell command needs it.. Maybe we can change it to shown it in the banner only if the URI is provided in the config?. True, I'll find something where the changes would be more obvious.. Yep, it is there on purpose  sets up SSL service, but NONSSL client.\nThis test uses rabbit_ssl_config fixture so the SSL is loaded from config where it is present. With the args I'm telling the client don't use the SSL setup from config. The service is run with SSL config, but the client is set NOT to use any SSL, similarly the client has the SSL rabbit url (which it would also load from config) is overriden with non SSL uri. . > Maybe we can change it to shown it in the banner only if the URI is provided in the config?\nIt's not just the banner, rpc and event_dispatcher are automatically loaded in the shell (nameko helper).. fixed.). done.). > There is already a fixture called amqp_uri that pulls the URI out of rabbit_config. \nI left the amqp_uri for backward compatibility, there may be code outside using it.\n\nrabbit_uri is a bit vague because there is also the URI for the management interface\n\nCan be rabbit_management_uri if needed .)\nI understand the consistency worries. Made me think again. :thinking: \nWith the name used, it was hard to say which is better or  more consistent. There was rabbit_config, rabbit_ssl_config and rabbit_manager. I added rabbit_uri, rabbit_ssl_uri and rabbit_ssl_options. It felt somehow inconsistent to use the protocol name for URIs and leave the broker name for the rest. Also the web module does not use the protocol name (http) for server uri and port. On the other hand AMQP_URI is used as config key and inside amqp extensions. Also amqp extensions may in theory work with different AMQP brokers, not just with rabbit.\n:thinking: . When reasoning about it again, we've got:\n\nrabbit_manager - fits well as it wraps management API of RabbitMQ\nrabbit_config - combines rabbit_uri and config (used to use rabbit_manager via vhost_pipeline with implementation tying it to RabbitMQ as specific broker)\nrabbit_uri - now uses rabbit_manager via vhost_pipeline making it again RabbitMQ specific\nrabbit_ssl_config, rabbit_ssl_uri and rabbit_ssl_options - same for SSL rabbit setup\n\nThe main argument for using rabbit in rabbit_uri is the fact that is sets and tears down messaging setup with tooling specific to RabbitMQ message broker. Rabbit is the chosen broker (at least for testing).. done.). mock_container does not configure messaging, it's independent to it, so rabbit_config or memmory_rabbit_config need to be used when testing messaging extensions.\nWell, this particular test is actually working without rabbit_config fixture as it has the messaging part mocked by mock_roducer ;) - I removed it. Scanned the other tests in the module, looks like they still need the config.. good catch, thanks! .). :+1: . :+1: . ",
    "noizex": "I tried profiling non-async RPC call with Nameko over vanilla rabbitmq installation, and performance is pretty bad or I'm doing something totally wrong. Using ClusterRpcProxy and calling just a method that returns some number (and does nothing else) I'm getting to 20 calls per second. I'm not printing to screen or doing anything that could slow it down, so no idea why an empty RPC call takes 50ms on average (AMQP is local, as is nameko service). What number of calls are you getting with just RPC over AMPQ (not HTTP-enabled). \nI tried running 10, 20, 30 workers and it doesn't make a difference (it made a difference with 1 worker, and it went down to a crawl). I tried running 2 nameko services on 2 different machines, still no difference. Any idea what I'm doing wrong?\n. Thanks Artur & Matt, I did as suggested and managed to get some decent amount of processed calls (~600 on a better machine than my laptop). Indeed, ClusterRpcProxy was the problem, as it made a new connection to RabbitMQ every time I called RPC method, and slowed whole thing down significantly. \nI haven't tried playing with persistent messages but I will try this too once the amount of processed calls becomes bottleneck. For now it looks promising and we will give it a go in our upcoming microservice-oriented project. \n. Hmm, one thing Matt I don't understand in your example. Does your method requires these 2 services to be defined in the same Python module? What I used was some external script that wasn't service itself, so I used the nameko.standalone namespace. I guess then I need to provide AMQP config and use ClusterRpcProxy?\n. I will have to check it on my work laptop, so this has to wait till Monday. I'm running nameko on OSX, installed from brew, so should be quite fresh version. AMQP is served by external server, and from what I remember the queue of these async calls stays filled. So for example if I did 100 async calls, and my nameko workers processed 5 of them and hung, there will be 95 messages in queue waiting. What's more, if I kill nameko process (which doesn't exit immediately on CTRL+C but waits for workes to \"finish job\" even though they shouldn't be doing anything) and restart, it will pick up next 5 jobs in line and hang on them. \nI will give more feedback on Monday - versions, state of AMQP between calls and so on.\n. Ok, so once again, it's OSX 10.11.5, I attached my pip freeze to the comment, RabbitMQ version 3.6.1. Services are indeed using config file, if I haven't zipped it last time there it is:\nAMQP_URI: 'amqp://...'\nWEB_SERVER_ADDRESS: '0.0.0.0:8000'\nrpc_exchange: 'nameko-rpc'\nmax_workers: 2\nparent_calls_tracked: 10\npip_freeze.txt\nI attach screenshots of queues:\n1 - just after starting nameko fresh\n2 - when I execute \"test.py\" - it's important to note that I never consume responses from async call, just quit without waiting for result\n3 - after I stop nameko, have to do it using 2 x CTRL+C, and it dumps this when it quits:\n^C\nstopping <ServiceContainer [service_a] at 0x10b009850>\nstopping <ServiceContainer [service_b] at 0x10b0c0750>\nstopped <ServiceContainer [service_b] at 0x10b0c0750> in 0.016s\n^C\nkilling <ServiceContainer [service_a] at 0x10b009850>\nalready stopped <ServiceContainer [service_b] at 0x10b0c0750>\nkilling 2 active thread(s)\nkilling active thread for <Rpc [service_a.run] at 0x10b0097d0>\nran handler for <WorkerContext [service_a.run] at 0x10b0eca90> in 52.747s\nran worker <WorkerContext [service_a.run] at 0x10b0eca90> in 52.750s\n<ServiceContainer [service_a] at 0x10b009850> thread killed by container\nkilling active thread for <Rpc [service_a.run] at 0x10b0097d0>\nran handler for <WorkerContext [service_a.run] at 0x10b0ec750> in 52.748s\nran worker <WorkerContext [service_a.run] at 0x10b0ec750> in 52.750s\n<ServiceContainer [service_a] at 0x10b009850> thread killed by container\n4 - after starting nameko for the second time, it looks like 2 previously executed calls to Service A have returned to the queue and we're again exuecuting 2 out of 9, not 7. So this hangs even before acknowledging that initial call was successful (makes sense, as it doesn't execute any code after line 18: self.test_service.test(cnt)\n\n\n\n\n. Tracked it down a bit more, it hangs on RpcReply(), \"Waiting for RPC reply event\" after executing method in Service B. Unfortunately I have no idea why would it wait there, as that method ends immediately and just returns simple number. I guess something is wrong with my queues maybe? Does nameko require some specific rabbitmq config, or would work on basic one?\n. Allright, that RabbitMQ was my local version, I'm using server now that has 3.2.4, seems pretty old, maybe it doesn't have this confirm_publish extension and that's the reason. I will check on my local RabbitMQ and see if I can reproduce it.\n. And there it is, RabbitMQ from Ubuntu is 3.2.4 and doesn't work, my local version is 3.6.1 and works fine, looks like some missing feature then. Problem self-solved I guess..\n. ",
    "yunfan": "i just happened to this issues too, after a quickstart of nameko, i found its convenient for writing services\nbut the benchmakrs made my heart breaks, i only got about 300 pqs on my company's server(controled by me, yes, i am a devops),  i remember the old time we use our own handcrafted rpc solution, which is about 3k to 6k qps, this nameko performance really made me cant try microservice, or at least cant try microservice on it . @mattbennett thanks for the tips . for other tools , do you have any recomends?. ",
    "timbu": "Indeed it is. Thought I'd checked the latest release, but obviously not... :)\n. Aha! Or might already be fixed in v2.2 :)\n. thanks @davidszotten :) - I wasn't quite sure which was the best way to go... I can definitely see an advantage in having only one method (as both are essentially doing the same thing). But the downside is we will make the docstring quite complex by accepting both args and kwargs  - e.g. the return values are needed in the args case, but are irrelevant (unusable really, as they are an ordered list based on unordered kwargs) in the kwargs case .\nAm happy to change if you think kwargs is the way to go.\n. cool - will do \ud83d\udc4d \n. The old connections (and vhosts) were all removed at the end of the test run - so normally you wouldn't notice. However, on services with many integration tests, rabbitmq would run out of file-descriptors. This could be worked around by increasing the number allowed, but would require additional config when building. \nActually we really only noticed it when we added a new publisher to the service (as this increased the number of connections created for every integration test). @mattbennett Is this the same problem from here? https://github.com/nameko/nameko/pull/573/files. LGTM \ud83d\udc4d . Note this is an API code change (i don't think reraise is ever set to True so have removed the code) - but worth a review...\n. str won't work on RemoteError here - does this matter?\n. I've gone with a Stub class - let me know what you think :)\n. not really -other than to give it around 10 secs :). \ud83d\udc4d  done. ",
    "javaguirre": "I think It might be related with https://github.com/onefinestay/nameko/pull/275.\n. ",
    "laike9m": "Hi matt, it seems neither invoking start() on container or runner will start service unless I call wait(). Is this expected?\n. Thank you. I'm using a runner because in this way I'm able to stop the service programmatically. Actually there's another issue: I have to call kill() to stop it while stop() doesn't work.\n. Thx matt. It would save people a lot of time if this is written in doc.\n. So a client disconnects early in the request and server cannot read data, therefore OSError occurs, is that the reason?\n. I see you ignored OSError when processing request in above commits, but what if other OSError happens?\n. I'll try to find a way to reproduce when I have time.\n. ",
    "chassing": "hi,\nyes that's also a solution. I will use my own EventHandler to avoid code duplications.\nthx,\nchris\n. ",
    "Sparkycz": "Thanks for the fast response.. I wonder the reason because when i will want the first wait i can check first round and break it by own way\n. There's no rush. I solved it by other way. Thank you ;-)\nI just think it's strange..\n. I added parameter eager how Matt wrote. What do you about this change? :)\nThere is still \"sleep first\" mode by default..\n. it's not final solution yet..\n. wait a few days pls. i'm currently on vacation..\n. I'm afraid i don't understand you.. \nYou want to wait on finish of last round and then new one will start, don't you?\n. the both PRs are wrong.. i'm sending the first one now and then i will try to solve the second one. I'm sorry I havn't had a free time for it..\n. Currently it probably does what we want. If the service method works longer than interval Timer waits for the completion and then fires the next one.\n. Ok, I'll update it. thanks\n. of course, thanks\n. It can be..\n. Yes, but it's needed for calculating of test coverage in Travis's tests.. (When i move this lines higher, under the self.event.reset(), the lines 69-74 are not used in tests)\n. Uff.. Yes, i understand\nI replaced the eventlet.Timeout() by my solution because self.should_stop.wait() wasn't called correctly under \"with Timeout(..):\".\nI will think of an other solution for solve the problem.\nthx\n. ",
    "daviskirk": "Is this still being worked on?  If it's just a matter of finishing up the tests I can give it a try. @mattbennett see #579 . I'd be interested in this for the simple reason that gevents community seems to be a lot bigger than eventlets and I know a few people how are hesitant on nameko for this reason.  I also seem to run into alot more weird bugs and cryptic error messages using eventlet than gevent, but this is a purely subjective non-scientific observation.  I'd be happy to pitch in if there's an initial POC somewhere.. Thanks for the answer, I think you're right that it must be the consumer losing connection.  I'm still trying to figure out why it loses the connection but can apparently still consume the message a second time.\nSo the timeline is something like:\n\nRPC consumer receives the message and starts processing\nFinishes the processing and tries to ack (or at least reaches the handle_result method)\nAck fails because of a disconnect\nStart from the top again\n\nI perhaps should note that I've set max_workers to 1.. The problem seems to be the same as here: https://groups.google.com/forum/#!topic/nameko-dev/A2M9YQgkM5s. I don't think I can continue on #302 (or at least I don't know how), so I pulled the branch, kept those commits, rebased on master (as there have been a few changes since then) and then added the rest.  Hope that way of doing it is ok. . Added this while I was at it.  If this isn't added and there's an exception in the method that uses the timer endpoint there's an exception in the service container caused by a missing expected_exceptions endpoint.  The test_timer_error test below tests this.. did this.  A question for future code, so I do this correctly next time:  Is the general philosophy in the tests to keep each test as independent as possible and compromising a bit on \"DRY\"?  I noticed that the tracker is also defined in every test module instead of in conftest.py where it could be used everywhere.. yes!. removed the \"timer_only\" mode. Sure does... sorry about that. ",
    "jijingg": "thanks very much  , i'm new on nameko, the document not mentioned that at first page .\n. ",
    "Brad19940809": "Till now, still waiting for fix.. ",
    "buriy": "Hi @mattbennett @davidszotten ,\nwhat you've provided is a server-side part example, which is trivial to produce,\nbut I haven't seen any client-side example -- for HTTP you used curl, and for any RPC you always use AMQP.\nDo I need just to use my chosen library (mprpc) client implementation , but replace server-side @rpc with my own server decorator?\nAm I right that I also need to implement all in rpc package by myself then?\nSo, how to make it reusable for another RPC protocol? Do you have any API docs but this https://nameko.readthedocs.io/en/stable/built_in_extensions.html#rpc ?\nOr is it free from any API obligations?\n. ",
    "ghost": "Thank you kindly, I will do just that!\n. Cheers.\n. Hey Matt,\nYou must be reading my mind as I have been struggling with these deadlocks today. Last night one of our servers died which created more load on our other nodes, which then starting raising deadlock errors like crazy. \nOur solution to the deadlocks has always been to just deploy more nodes. I've never fully understood the right max_worker count for our deployment. Our gateway (web socket service) is set to 1,000 max_workers and all other services are set to 100. These were just guesses, and the reason we set the web socket service to 1000 is that it seemed the most susceptible to deadlocks. \nAnyway, a fix to this issues would be greatly appreciated by our team!\nThanks,\nConor\n. Hi Matt,\nSimple fixes are the best fixes \ud83d\udc4d \nWe are on 2.5.3, we updated as soon as it hit hoping the max_workers +1 would solve our issues.\nSo if we set the max_workers to something \"very high\", does that mean that the Nameko service will prefetch that many tasks from rabbit? Meaning if I have 1000 tasks sitting in my queue and my max_workers is set to 1000 will the first nameko host grab all 1000?\nCheers,\nConor. Thank you Matt, this is a great help.. ",
    "tino": "Ah, that sounds like a sensible approach. Thanks!\n. For people wondering why this doesn't work (as I did): it won't work on 2.2.0, but you have to get the latest version from master...\nEdit: and for future reference, this is a full development implementation, allowing everything:\n``` python\nclass CorsHttpRequestHandler(HttpRequestHandler):\n    def handle_request(self, request):\n        self.request = request\n        return super(CorsHttpRequestHandler, self).handle_request(request)\ndef response_from_result(self, *args, **kwargs):\n    response = super(CorsHttpRequestHandler, self).response_from_result(*args, **kwargs)\n    response.headers.add(\"Access-Control-Allow-Headers\",\n                         self.request.headers.get(\"Access-Control-Request-Headers\"))\n    response.headers.add(\"Access-Control-Allow-Credentials\", \"true\")\n    response.headers.add(\"Access-Control-Allow-Methods\", \"*\")\n    response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n    return response\n\ncors_http = CorsHttpRequestHandler.decorator\nusage:\nclass MyService(object):\n    name = \"my_service\"\n@cors_http('GET', '/ping/')\ndef ping(self, request):\n    return \"pong\"\n\n```\n. ",
    "harel": "If anyone gets here from Google, as I did, I used this code snippet as a base of a slightly larger component that auto registers OPTIONS endpoints and manages the various headers via decorator:\nhttps://github.com/harel/nameko-cors. Quick note - the above repository (https://github.com/harel/nameko-cors) has been updated to resolve a bug with OPTIONS requests going through the entire request flow instead of returning an empty body.. ",
    "lagelalegal": "@harel please you look the issue #471 about the nameko-cors.. Thank you very much, It's my fault.\nHandle the CORS on browser, It's not good idea.\nSo I want to write it on config _yaml file. . @kooba good, thank you very much.. Do you want add nameko-cors into community-extensions? I think I must remind the nameko-cors author at first. . ",
    "floqqi": "Could this be merged? \nI'm feeling dirty using -e git+https://github.com/onefinestay/nameko.git#egg=nameko in my requirements. :)\n. ",
    "genbits": "Thank you so much for the reply.\nI will use the mailing list from now on for any questions.\nCheers\n. yes, it happened while running (a network hickup?)\nhow would you handle such cases?\n. ",
    "andriykohut": "https://github.com/onefinestay/nameko/pull/318\n. Oh, good point! I'm on it.\n. Hmm, one of the jobs failed, https://travis-ci.org/onefinestay/nameko/jobs/127208999#L381 but this doesn't seem related, not sure how I can trigger that job again\n. I've added the test, mind taking a look?\n. Looks like rebuild required for this job: https://travis-ci.org/onefinestay/nameko/jobs/127563677#L365\n. \ud83d\ude80 \n. Nice! Thanks for helping me out with this\n. @davidszotten Is there way to run tests locally? I can't find any explanation in the docs. It clearly expected that rabbitmq should be running on localhost, not sure how to pass it different amqp uri\n. Hmm, I already tried that (py.test --amqp-uri amqp://192.168.99.100:5672), seems like it's hardcoded somewhere (or default value is used). I've ended up just sed-ing every localhost to my docker-machine ip, will try to figure out what's going on when I'll get a chance\n. Done!\n. Ah, ok, I assumed both spelling will work.\n. ",
    "MrLokans": "I also think this is the moment that needs to be clarified in the documentation. I'm currently working on a project where some financial data should be stored and passed across multiple services, so we use python's decimal.Decimal class to represent it and we can't find a way to pass it to RPC call due to JSON serializer usage.. ",
    "gianchub": "Thank you so much David,\nall comments addressed, ready for another pass.\nfab\n. Hello, \na friend of mine had the same issue. His boss (non extremely tech person) was discarding nameko thinking that it isn't capable of handling requests other than GET and POST. So my friend did some digging, discovered Werkzeug is used internally, and therefore concluded nameko handles the other methods as well.\nIt would probably be very good to add a note in the documentation to point this out, so that people aren't discouraged by this.\nCheers,\nfab\n. Hello all, we're very interested in this functionality to land in nameko. What is the current state of things for this PR? Is anyone on it? Would the author be happy to accept contributions so that this can finally land?. Thanks @mattbennett :) Please do let us know if we can help getting this in quickly.. Ah yes, that's much better!\n. Removed.\n. ",
    "kodonnell": "Ah, right. I'm certainly no expert (and couldn't find an exact answer) but you look to be right. Also, to clarify, I can run cythonised code from a pure-python service (for my example). If this isn't (?) a reliable option, I wonder if there are other possibilities?\nI'll leave this issue open (I'm happy for someone else to close it though), as I suspect mine might (?) be a common enough use case: I've got a few frequently used services that are bottlenecks (in this case, data parsing), and need to run as fast as possible, hence the cythonising.\n. ",
    "jmatsushita": "Thanks Matt. Did so here https://groups.google.com/forum/#!topic/nameko-dev/gl0sqGnxO1U \nAlso maybe @rjrodger @geek will want to join in.\n. ",
    "aheaume": "transport.py in py-ampqlib will raise IOError when it can't read from or write to the socket. I guess this is cannot be considered a \"socket error\" since the socket never actually failed, it's just closed.\nIn fact, according to the python 2 socket docs socket.error has been a child of IOError since 2.6. In the python 3 socket docs it is a subclass of OSError since 3.3 (of which IOError is an alias, I believe).\n. For some reason I couldn't get the Travis log until now. I'll fix the line length and squash my commits.\n. Done, thanks @davidszotten \n. ",
    "grepsr": "You are right, closing this now. I had forgotten to restart, nameko, duh! My bad!\n. ",
    "kgrvamsi": "@davidszotten  closing this issue because when i run via sudo command the command runs with no issues.Thanks for quick response\n. ",
    "zsrinivas": "closing in favour of #352 \n. ",
    "roblofthouse": "We're very much interested in getting this into Nameko as we require secure service to service communication.. ",
    "hedin": "Ok, I'll reup it in a couple of days. Yep! I had to use my local non-docker rabbitmq server for testing.. @mattbennett thanks for this container it works perfectly. Should I write more tests than just testing SSL connection?\nIf so, could you please tell me what tests you want to see. I'm just starting my way in testing.. Hi, @mattbennett . While I'm working on updating my fork and adding tests, could you please update certificates in docker image. They are expired:\nbash\nopenssl s_client -connect localhost:5671 -cert clientcert.pem -key clientkey.pem -CAfile cacert.pem -tls1_2\n...\nVerify return code: 10 (certificate has expired)\n. Hi, @mattbennett . I've added simple rpc over ssl test.. Ok, I found more places to pass ssl_params and it works now.\n\nThe proxy can connect using SSL or not. In fact, it might be nice to test both.\n\nShould I update the test to use both ssl and non-ssl connection at the same time?. Sure!. Something like AMQP_SSL_CONFIG_KEY?. Oh I get it. Will AMQP_SSL_CONFIG_KEY = 'AMQP_SSL' be ok?. Thanks for comment! My code doesn't work now on real messaging. Going to find why. Hi. Since all valuable options are PEM-files, missing any of them will raise PEM loading error: SSLError: [SSL] PEM lib. ",
    "jakedahn": "I'm also interested in the functionality provided by this PR -- I'm willing and happy to pitch in to help get these changes merged into master.\n@hedin @mattbennett @davidszotten What are the remaining items that need to be figured out before this pr is mergeable?. ",
    "bweaver-cxp": "Sorry for the delay; the project we're doing in nameko has been put on hold for several months, so it took a while to carve out some time to run that system again. My testing wasn't extensive either, but this build (2.9.1rc0) worked nicely under the testing I did do.. Can we get an update on this?\nWe're trying to use amqps, which is not supported in kombu <4.0.0, which we've sort of gotten around by using amqp with ?ssl=true at the end. It mostly works, but it spits out a bunch of errors and it's causing issues in our automated deployments (connections aren't being reliably closed when our containers are replaced, causing our RabbitMQ to slowly run out of memory). We've forked nameko and upgraded the dependency to use kombu >=4.0.0 and switched our uris to use amqps and everything seems to be working great (but of course the nameko tests don't pass).\nWe need SSL at every level due to HIPAA requirements. We might just run a parallel nameko repository and periodically merge official changes into it, with the only difference being the kombu version. I don't love that solution, but this is a pretty significant issue for us.. It doesn't seem to, but I might be using it wrong. What should I have under AMQP_SSL in my config?\nThe error we fixed by upgrading to kombu 4.0.0 is this:\nException ignored in: <bound method _AbstractTransport.__del__ of <amqp.transport.SSLTransport object at 0x7f91ecd06940>>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/amqp-1.4.9-py3.6.egg/amqp/transport.py\", line 116, in __del__\n    self.close()\n  File \"/usr/local/lib/python3.6/site-packages/amqp-1.4.9-py3.6.egg/amqp/transport.py\", line 141, in close\n    self._shutdown_transport()\n  File \"/usr/local/lib/python3.6/site-packages/amqp-1.4.9-py3.6.egg/amqp/transport.py\", line 217, in _shutdown_transport\n    self.sock = unwrap()\n  File \"/usr/local/lib/python3.6/site-packages/eventlet-0.21.0-py3.6.egg/eventlet/green/ssl.py\", line 265, in unwrap\n    super(GreenSSLSocket, self).unwrap))\n  File \"/usr/local/lib/python3.6/site-packages/eventlet-0.21.0-py3.6.egg/eventlet/green/ssl.py\", line 109, in _call_trampolining\n    return func(*a, **kw)\n  File \"/usr/local/lib/python3.6/ssl.py\", line 1055, in unwrap\n    raise ValueError(\"No SSL wrapper around \" + str(self))\nValueError: No SSL wrapper around <eventlet.green.ssl.GreenSSLSocket fd=6, family=AddressFamily.AF_INET, type=2049, proto=6, laddr=('172.21.0.8', 50800)>. We don't know for sure that that particular error is causing our unwanted behavior, we just know that when we don't use SSL (or when we used the most recent Kombu) we don't see our issue, and that correlates with not getting that error. And our problem is only in a very specific environment; it doesn't happen locally or in manual starting and stopping of (Docker) containers on AWS, just in automated deployments on AWS. It's hard to pin down exactly what/where/why the problem is.\nI tried both of those configuration options, not realizing that this branch wasn't meant to fix the \"No SSL wrapper\" error.. #524 does fix the SSL wrapper error and our connections are still not closing. So I have no idea what the issue or fix is, except that SOME commit between py-amqp v1.4.9 and v2.2.2 must do the trick. I might try a deployment at each version to narrow down the search.\nIt's nice to have all those logs cleaned up though. Makes it easier to see what's happening.. ",
    "izmailoff": "Hi @davidszotten , thanks for the reply. My \"client\" is written in Python and uses rpc proxy to call the \"server\" which has a method annotated with nameko rpc. This works well as they both use nameko. Now I want to replace Python \"server\" with something else (Scala) which has no support for nameko style rpc. So I want to implement something similar to rpc annotation on the server side. Sorry I was not very clear about that. I'm trying to figure out how to reply back to the \"client\"/rpc caller side.\nHere is what I see being sent to the server (Scala):\n```\nbytes:\n{\"kwargs\": {}, \"args\": [\"encoded string\"]}\nconsumerTag:\namq.ctag-qpCtJA_afw_fvbXlT_yi7w\nenvelope:\nEnvelope(deliveryTag=1, redeliver=false, exchange=nameko-rpc, routingKey=cv_parsing_service.parse)\nproperties:\ncontentHeader(content-type=application/json, content-encoding=utf-8, headers={\nnameko.call_id_stack=[standalone_rpc_proxy.call.f70038cf-0f85-49fc-8f9f-e86c6f87ea69]\n}, \ndelivery-mode=2, priority=0, correlation-id=d7e9065b-d700-4593-b7f4-5c7251c3d375, reply-to=706a0408-5166-4c72-a108-4d4c3bba9ed1, expiration=null, message-id=null, \ntimestamp=null, type=null, user-id=null, app-id=null, cluster-id=null)\n```\nI also noticed that this queue was created:\nrpc.reply-standalone_rpc_proxy-706a0408-5166-4c72-a108-4d4c3bba9ed1\nI guess if I figure out how to construct this queue name above from the received headers/props and reply back with message (need to set any other props?) I should be done. Are there any other considerations? How do you serialize exceptions?\nThanks a lot\n. Thanks a lot @davidszotten . This is very useful and clear. I'll try to implement it quickly and will update here.\n. @davidszotten I've got it working, thanks for your help. Here is Wireshark AMQP trace for the part where nameko subscriber/server is already connected and nameko publisher/client is calling the RPC method. This is based on hello world example from README\n. ```\nNo.     Time           Source                Destination           Protocol Length Info\n      8 1.663536620    127.0.0.1             127.0.0.1             AMQP     117    Basic.Publish x=nameko-rpc rk=greeting_service.hello \nFrame 8: 117 bytes on wire (936 bits), 117 bytes captured (936 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44088 (44088), Dst Port: 5672 (5672), Seq: 1, Ack: 1, Len: 49\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 1\n    Length: 41\n    Class: Basic (60)\n    Method: Publish (40)\n    Arguments\n        [Publish-Number: 1]\n        Ticket: 0\n        Exchange: nameko-rpc\n        Routing-Key: greeting_service.hello\n        .... ...1 = Mandatory: True\n        .... ..0. = Immediate: False\nNo.     Time           Source                Destination           Protocol Length Info\n      9 1.663607823    127.0.0.1             127.0.0.1             AMQP     286    Content-Header type=application/json \nFrame 9: 286 bytes on wire (2288 bits), 286 bytes captured (2288 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44088 (44088), Dst Port: 5672 (5672), Seq: 50, Ack: 1, Len: 218\nAdvanced Message Queueing Protocol\n    Type: Content header (2)\n    Channel: 1\n    Length: 210\n    Class ID: Basic (60)\n    Weight: 0\n    Body size: 40\n    Property flags: 0xfe00\n    Properties\n        Content-Type: application/json\n        Content-Encoding: utf-8\n        Headers\n            nameko.call_id_stack (array)\n                [0] (string): standalone_rpc_proxy.call.789dbe44-9e63-4405-ba06-4f180b9c4f6c\n        Delivery-Mode: 2\n        Priority: 0\n        Correlation-Id: ec23013d-1aeb-4a27-8b62-cce6d5b0943b\n        Reply-To: 53879dd9-6fd4-46a3-bf1e-4eaa7609b3ea\nNo.     Time           Source                Destination           Protocol Length Info\n     10 1.663625618    127.0.0.1             127.0.0.1             AMQP     116    Content-Body \nFrame 10: 116 bytes on wire (928 bits), 116 bytes captured (928 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44088 (44088), Dst Port: 5672 (5672), Seq: 268, Ack: 1, Len: 48\nAdvanced Message Queueing Protocol\n    Type: Content body (3)\n    Channel: 1\n    Length: 40\n    Payload: 7b2261726773223a205b5d2c20226b7761726773223a207b...\nNo.     Time           Source                Destination           Protocol Length Info\n     12 1.664114170    127.0.0.1             127.0.0.1             AMQP     395    \nFrame 12: 395 bytes on wire (3160 bits), 395 bytes captured (3160 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 5672 (5672), Dst Port: 44104 (44104), Seq: 1, Ack: 1, Len: 327\nAdvanced Message Queueing Protocol\n    00.. .... = Format: 0\n    .... 0001 = Position: ---e (1)\n    Type: Control (0)\n    Length: 256\n    Track: Control (0)\n    Channel: 13568\n    Class: Unknown (78)\n    [Expert Info (Error/Protocol): Unknown command/control class 78]\n        [Unknown command/control class 78]\n        [Severity level: Error]\n        [Group: Protocol]\nNo.     Time           Source                Destination           Protocol Length Info\n     14 1.669652288    127.0.0.1             127.0.0.1             AMQP     89     Basic.Ack \nFrame 14: 89 bytes on wire (712 bits), 89 bytes captured (712 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 5672 (5672), Dst Port: 44088 (44088), Seq: 1, Ack: 316, Len: 21\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 1\n    Length: 13\n    Class: Basic (60)\n    Method: Ack (80)\n    Arguments\n        Delivery-Tag: 2\n        .... ...0 = Multiple: False\nNo.     Time           Source                Destination           Protocol Length Info\n     19 1.671130325    127.0.0.1             127.0.0.1             AMQP     76     Protocol-Header 1-0-9\nFrame 19: 76 bytes on wire (608 bits), 76 bytes captured (608 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44106 (44106), Dst Port: 5672 (5672), Seq: 1, Ack: 1, Len: 8\nAdvanced Message Queueing Protocol\n    Protocol: AMQP\n    Protocol ID Major: 1\n    Protocol ID Minor: 1\n    Version Major: 0\n    Version Minor: 9\nNo.     Time           Source                Destination           Protocol Length Info\n     23 1.694175637    127.0.0.1             127.0.0.1             AMQP     556    Connection.Start \nFrame 23: 556 bytes on wire (4448 bits), 556 bytes captured (4448 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 5672 (5672), Dst Port: 44106 (44106), Seq: 1, Ack: 9, Len: 488\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 0\n    Length: 480\n    Class: Connection (10)\n    Method: Start (10)\n    Arguments\n        Version-Major: 0\n        Version-Minor: 9\n        Server-Properties\n            capabilities (field table)\n                publisher_confirms (boolean): true\n                exchange_exchange_bindings (boolean): true\n                basic.nack (boolean): true\n                consumer_cancel_notify (boolean): true\n                connection.blocked (boolean): true\n                consumer_priorities (boolean): true\n                authentication_failure_close (boolean): true\n                per_consumer_qos (boolean): true\n                direct_reply_to (boolean): true\n            cluster_name (string): rabbit@gazella\n            copyright (string): Copyright (C) 2007-2016 Pivotal Software, Inc.\n            information (string): Licensed under the MPL.  See http://www.rabbitmq.com/\n            platform (string): Erlang/OTP\n            product (string): RabbitMQ\n            version (string): 3.6.3\n        Mechanisms: 414d51504c41494e20504c41494e\n        Locales: 656e5f5553\nNo.     Time           Source                Destination           Protocol Length Info\n     25 1.694549493    127.0.0.1             127.0.0.1             AMQP     248    Connection.Start-Ok \nFrame 25: 248 bytes on wire (1984 bits), 248 bytes captured (1984 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44106 (44106), Dst Port: 5672 (5672), Seq: 9, Ack: 489, Len: 180\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 0\n    Length: 172\n    Class: Connection (10)\n    Method: Start-Ok (11)\n    Arguments\n        Client-Properties\n            product (string): py-amqp\n            product_version (string): 1.4.9\n            capabilities (field table)\n                connection.blocked (boolean): true\n                consumer_cancel_notify (boolean): true\n        Mechanism: AMQPLAIN\n        Response: 054c4f47494e530000000567756573740850415353574f52...\n        Locale: en_US\nNo.     Time           Source                Destination           Protocol Length Info\n     26 1.694693554    127.0.0.1             127.0.0.1             AMQP     88     Connection.Tune \nFrame 26: 88 bytes on wire (704 bits), 88 bytes captured (704 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 5672 (5672), Dst Port: 44106 (44106), Seq: 489, Ack: 189, Len: 20\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 0\n    Length: 12\n    Class: Connection (10)\n    Method: Tune (30)\n    Arguments\n        Channel-Max: 0\n        Frame-Max: 131072\n        Heartbeat: 60\nNo.     Time           Source                Destination           Protocol Length Info\n     27 1.694813859    127.0.0.1             127.0.0.1             AMQP     88     Connection.Tune-Ok \nFrame 27: 88 bytes on wire (704 bits), 88 bytes captured (704 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44106 (44106), Dst Port: 5672 (5672), Seq: 189, Ack: 509, Len: 20\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 0\n    Length: 12\n    Class: Connection (10)\n    Method: Tune-Ok (31)\n    Arguments\n        Channel-Max: 65535\n        Frame-Max: 131072\n        Heartbeat: 0\nNo.     Time           Source                Destination           Protocol Length Info\n     28 1.694856112    127.0.0.1             127.0.0.1             AMQP     84     Connection.Open vhost=/ \nFrame 28: 84 bytes on wire (672 bits), 84 bytes captured (672 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44106 (44106), Dst Port: 5672 (5672), Seq: 209, Ack: 509, Len: 16\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 0\n    Length: 8\n    Class: Connection (10)\n    Method: Open (40)\n    Arguments\n        Virtual-Host: /\n        Capabilities: \n        .... ...0 = Insist: False\nNo.     Time           Source                Destination           Protocol Length Info\n     30 1.694942196    127.0.0.1             127.0.0.1             AMQP     81     Connection.Open-Ok \nFrame 30: 81 bytes on wire (648 bits), 81 bytes captured (648 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 5672 (5672), Dst Port: 44106 (44106), Seq: 509, Ack: 225, Len: 13\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 0\n    Length: 5\n    Class: Connection (10)\n    Method: Open-Ok (41)\n    Arguments\n        Known-Hosts: \nNo.     Time           Source                Destination           Protocol Length Info\n     31 1.695115508    127.0.0.1             127.0.0.1             AMQP     81     Channel.Open \nFrame 31: 81 bytes on wire (648 bits), 81 bytes captured (648 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44106 (44106), Dst Port: 5672 (5672), Seq: 225, Ack: 522, Len: 13\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 1\n    Length: 5\n    Class: Channel (20)\n    Method: Open (10)\n    Arguments\n        Out-Of-Band: \nNo.     Time           Source                Destination           Protocol Length Info\n     32 1.695472629    127.0.0.1             127.0.0.1             AMQP     84     Channel.Open-Ok \nFrame 32: 84 bytes on wire (672 bits), 84 bytes captured (672 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 5672 (5672), Dst Port: 44106 (44106), Seq: 522, Ack: 238, Len: 16\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 1\n    Length: 8\n    Class: Channel (20)\n    Method: Open-Ok (11)\n    Arguments\n        Channel-Id: \nNo.     Time           Source                Destination           Protocol Length Info\n     33 1.695706890    127.0.0.1             127.0.0.1             AMQP     131    Basic.Publish x=nameko-rpc rk=53879dd9-6fd4-46a3-bf1e-4eaa7609b3ea \nFrame 33: 131 bytes on wire (1048 bits), 131 bytes captured (1048 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44106 (44106), Dst Port: 5672 (5672), Seq: 238, Ack: 538, Len: 63\nAdvanced Message Queueing Protocol\n    Type: Method (1)\n    Channel: 1\n    Length: 55\n    Class: Basic (60)\n    Method: Publish (40)\n    Arguments\n        [Publish-Number: 1]\n        Ticket: 0\n        Exchange: nameko-rpc\n        Routing-Key: 53879dd9-6fd4-46a3-bf1e-4eaa7609b3ea\n        .... ...0 = Mandatory: False\n        .... ..0. = Immediate: False\nNo.     Time           Source                Destination           Protocol Length Info\n     34 1.695740864    127.0.0.1             127.0.0.1             AMQP     156    Content-Header type=application/json \nFrame 34: 156 bytes on wire (1248 bits), 156 bytes captured (1248 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44106 (44106), Dst Port: 5672 (5672), Seq: 301, Ack: 538, Len: 88\nAdvanced Message Queueing Protocol\n    Type: Content header (2)\n    Channel: 1\n    Length: 80\n    Class ID: Basic (60)\n    Weight: 0\n    Body size: 41\n    Property flags: 0xfc00\n    Properties\n        Content-Type: application/json\n        Content-Encoding: utf-8\n        Headers\n        Delivery-Mode: 2\n        Priority: 0\n        Correlation-Id: ec23013d-1aeb-4a27-8b62-cce6d5b0943b\nNo.     Time           Source                Destination           Protocol Length Info\n     36 1.695769214    127.0.0.1             127.0.0.1             AMQP     117    Content-Body \nFrame 36: 117 bytes on wire (936 bits), 117 bytes captured (936 bits) on interface 0\nLinux cooked capture\nInternet Protocol Version 4, Src: 127.0.0.1, Dst: 127.0.0.1\nTransmission Control Protocol, Src Port: 44106 (44106), Dst Port: 5672 (5672), Seq: 389, Ack: 538, Len: 49\nAdvanced Message Queueing Protocol\n    Type: Content body (3)\n    Channel: 1\n    Length: 41\n    Payload: 7b22726573756c74223a202248656c6c6f2c204d61747421...\n```\n. I'll try to put some code example for whoever might need the same thing. Thanks.\n. @mattbennett I used latest Wireshark with latest Fedora at that time (few months old now). If you need the dump files I might still have them.. I wrote a short article on RPC implementation in Nameko. It's not exactly detailed as much as technical docs would be but explains enough to know how it works: http://izmailoff.github.io/architecture/rpc-revived/. Hopefully it will be useful for someone. Pls let know your feedback and if I missed anything if you read it. Cheers.. thanks @mattbennett for the note on #359 , I'll update the post soon. ",
    "and3rson": "JFYI: I've made an integration lib for Node.JS (although I'm not a huge Node fan), just for PoC.\nYou can check it out here: https://github.com/and3rson/node-nameko-client\n. Hey @mattbennett, thanks for the response!\nDependency on eventlet is not necessary, it can simply be replaced by built-in Queue implementation which can be used \"as is\" or monkey-patched by gevent or whatever the client is using. I just used eventlet-powered Queue because it seemed like nameko is using it as well.\nThanks for clarification, please check out the upcoming commit.\n. I'm thinking of probably moving this out into some new django-nameko package. Some Django support would be pretty convenient, I think.\n. @mattbennett Right. Currently it blocks if the pool is drained. We can make it spawn more proxies in this case.\nI'm currently packaging it within django-nameko package in order to do some good testing in real projects, will be in touch.\n. In case you're interested, here's the plugin for Django: https://github.com/and3rson/django-nameko\n. @jessepollak @mattbennett Thanks for your feedback, guys!\nI'm willing to cooperate on this, so let me know if I can make some help.\n. ",
    "albertmenglongli": "@and3rson , your django-nameko is cool as a client to call rpc, any idea  how to provide as rpc service in django using nameko, thanks in advance.. ",
    "lewisoaten": "Did this endeavour die off? A pool would be pretty useful!. ",
    "ArtikUA": "Great pool request!\nNameko should have pools anyway: it's very costful to create and close new connection for any request\nOtherwise, everyone should create similar realization on their own project. ",
    "kuznero": "Have exactly same error for exclusive queues stayed there after recovered from network partition. Clients have re-connected generating new exclusive queues. Old ones stays for some reason (they were not declared as durable). And when I try removing it from management UI, I get exact same error:\n405 RESOURCE_LOCKED - cannot obtain exclusive access to locked\nqueue 'position-api-154787f3-cdf3-4e1c-a06c-3c2eb97fa3f8' in vhost '/'\nP.S. To get out of network partition I restarted all nodes one by one start with those having least amount of active connections.. @mattbennett but as a client side developer I can tell that it is sometimes easier to generate temporary reply queue names and not re-use.. ",
    "rolandobloom": "Any updates on this issue?. ",
    "dhensen": "Hello @mattbennett I hope you are well. Have you got any time to look at this?. ",
    "manfred-chebli": "I'm digging up the subject here.\nIs implementing a timeout still a viable option? Since it's been a while since the last commit, is there any way to detect a service is down before calling it via RPC?. Hello !\nWe had this problem too for the last two weeks. When nameko uses the config.yml with an hostname to connect to the broker, the resolution is not handled properly, and the service cannot connect to the broker. \nMy config.yml looks like this:\nAMQP_URI: 'amqp://user:pass@broker:5672'\nThe stack is like this:\nTraceback (most recent call last):\nFile \"/usr/local/bin/nameko\", line 9, in <module>\n  load_entry_point('nameko==2.3.2', 'console_scripts', 'nameko')()\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/cli/main.py\", line 26, in main\n     args.main(args)\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/cli/run.py\", line 185, in main\n     run(services, config, backdoor_port=args.backdoor_port)\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/cli/run.py\", line 132, in run\n     service_runner.start()\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/runners.py\", line 67, in start\n     SpawningProxy(self.containers).start()\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/utils.py\", line 180, in spawning_method\n     return list(pool.imap(call, self._items))\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenpool.py\", line 238, in next\n     val = self.waiters.get().wait()\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 175, in wait\n     return self._exit_event.wait()\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/event.py\", line 121, in wait\n     return hubs.get_hub().switch()\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 294, in switch\n     return self.greenlet.switch()\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 214, in main\n     result = function(*args, **kwargs)\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/utils.py\", line 175, in call\n     return getattr(item, name)(*args, **kwargs)\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/containers.py\", line 198, in start\n     self.extensions.all.setup()\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/utils.py\", line 180, in spawning_method\n     return list(pool.imap(call, self._items))\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenpool.py\", line 238, in next\n     val = self.waiters.get().wait()\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 175, in wait\n     return self._exit_event.wait()\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/event.py\", line 121, in wait\n     return hubs.get_hub().switch()\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py\", line 294, in switch\n     return self.greenlet.switch()\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py\", line 214, in main\n     result = function(*args, **kwargs)\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/utils.py\", line 175, in call\n     return getattr(item, name)(*args, **kwargs)\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/messaging.py\", line 191, in setup\n     verify_amqp_uri(self.amqp_uri)\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/amqp.py\", line 48, in verify_amqp_uri\n     with transport.establish_connection():\n   File \"/usr/local/lib/python2.7/dist-packages/kombu/transport/pyamqp.py\", line 116, in establish_connection\n     conn = self.Connection(**opts)\n   File \"/usr/local/lib/python2.7/dist-packages/nameko/amqp.py\", line 25, in __init__\n     super(ConnectionTester, self).__init__(*args, **kwargs)\n   File \"/usr/local/lib/python2.7/dist-packages/amqp/connection.py\", line 165, in __init__\n     self.transport = self.Transport(host, connect_timeout, ssl)\n   File \"/usr/local/lib/python2.7/dist-packages/amqp/connection.py\", line 186, in Transport\n     return create_transport(host, connect_timeout, ssl)\n   File \"/usr/local/lib/python2.7/dist-packages/amqp/transport.py\", line 299, in create_transport\n     return TCPTransport(host, connect_timeout)\n   File \"/usr/local/lib/python2.7/dist-packages/amqp/transport.py\", line 75, in __init__\n     socket.SOCK_STREAM, SOL_TCP):\n   File \"/usr/local/lib/python2.7/dist-packages/eventlet/support/greendns.py\", line 464, in getaddrinfo\n     raise socket.gaierror(socket.EAI_NONAME, 'No address found')\n socket.gaierror: [Errno -2] No address found\n 2016/12/20 14:10:59 Command exited with error: exit status 1\nservice exited with code 1\nIt seems the last version of kombu (> 0.4.x) requires the last version of eventlet (0.20.0), which \nTo workaround, we forced kombu at version 3.0.37, and eventlet at version 0.19.0 in our requirements.. ",
    "frisellcpl": "Follow up question on this topic. The port used by the connection to rabbitmq (not the port used by rabbitmq itself). Can that somehow be specified?\n. Sorry for a late respons. Yes that did answer my question.\nThank you!\n. ",
    "gitricko": "+1\n. Thanks matt ! It works... should re-read doc carefully !. Thanks for the feedback.. yup... saw this today !. ",
    "xqliang": "OK, thank you.\n. But how to load data from database to another dependency, such as Redis?. Thank you for your answer, but there is still a small problem, I hope only doing the heavy initialization once at program startup time, because the heavy initialization will slow down the first batch of concurrent requests.. > From what I understand above, you want to extract some data from a relational database and insert it into Redis, and you want to do it only once and when the service starts. Is that correct?\nYou are right.\n\nYou could create a new DependencyProvider that talks to both the relational database and redis.\n\nI can create a new DependencyProvider, but then I can't reuse existing nameko-sqlalchemy and nameko-redis.\nLook at the following example code, I already have a session and cache instance, and I want reuse them to do the heavy initialization once before running any entrypoint.\n~~~python\nfrom nameko.rpc import rpc\nfrom nameko_sqlalchemy import Session\nfrom nameko_redis import Redis\nfrom .models import Model, DeclarativeBase\nclass Service(object):\n    name = 'service'\n    session = Session(DeclarativeBase)\n    cache = Redis('cache')\ndef init(self):\n        data = self.session.query(Model).get(id=1)  # heavy operater\n        self.cache.set('heavy_initialization_data', data)\n        return data\n\n# def run_once_before_entrypoint_and_after_dependency(self): \n#     self.init()\n\n@rpc\ndef hello(self):\n    data = self.cache.get('heavy_initialization_data')\n    if data is None:\n        data = self.init()\n    return data\n\n~~~\n\nThere is an entrypoint in nameko.testing.services which fires exactly once, when the service starts.\n\nThis is great, but we can not make sure the running once entrypoint start before others entrypoint (such as the hello in above example code), so it still may slow down the first batch of concurrent requests?\n. @mattbennett OK, thank you.. Missing return. timeout = kwargs.pop('_timeout', self.timeout) will make sense.. ",
    "levchik": "I don't know if my problem deserves separate issue, but I have a problem now, which seems to be related to this topic. I have latest celery (4.1.0) in project which depends on kombu 4.1.0. Since I have also nameko, I can't use pytest (and probably code that invokes calls to the nameko microservice, haven't tested yet), it just fails with:\n```python\nTraceback (most recent call last):\n  File \"/usr/lib/python3.6/site-packages/pluggy/init.py\", line 397, in load_setuptools_entrypoints\n    plugin = ep.load()\n  File \"/usr/lib/python3.6/site-packages/pkg_resources/init.py\", line 2404, in load\n    self.require(args, *kwargs)\n  File \"/usr/lib/python3.6/site-packages/pkg_resources/init.py\", line 2427, in require\n    items = working_set.resolve(reqs, env, installer, extras=self.extras)\n  File \"/usr/lib/python3.6/site-packages/pkg_resources/init.py\", line 875, in resolve\n    raise VersionConflict(dist, req).with_context(dependent_req)\npkg_resources.VersionConflict: (kombu 4.1.0 (/usr/lib/python3.6/site-packages), Requirement.parse('kombu<4,>=3.0.1'))\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\n  File \"/usr/bin/pytest\", line 11, in \n    sys.exit(main())\n  File \"/usr/lib/python3.6/site-packages/_pytest/config.py\", line 50, in main\n    config = _prepareconfig(args, plugins)\n  File \"/usr/lib/python3.6/site-packages/_pytest/config.py\", line 160, in _prepareconfig\n    pluginmanager=pluginmanager, args=args)\n  File \"/usr/lib/python3.6/site-packages/pluggy/init.py\", line 617, in call\n    return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)\n  File \"/usr/lib/python3.6/site-packages/pluggy/init.py\", line 222, in _hookexec\n    return self._inner_hookexec(hook, methods, kwargs)\n  File \"/usr/lib/python3.6/site-packages/pluggy/init.py\", line 216, in \n    firstresult=hook.spec_opts.get('firstresult'),\n  File \"/usr/lib/python3.6/site-packages/pluggy/callers.py\", line 196, in _multicall\n    gen.send(outcome)\n  File \"/usr/lib/python3.6/site-packages/_pytest/helpconfig.py\", line 68, in pytest_cmdline_parse\n    config = outcome.get_result()\n  File \"/usr/lib/python3.6/site-packages/pluggy/callers.py\", line 76, in get_result\n    raise ex[1].with_traceback(ex[2])\n  File \"/usr/lib/python3.6/site-packages/pluggy/callers.py\", line 180, in _multicall\n    res = hook_impl.function(*args)\n  File \"/usr/lib/python3.6/site-packages/_pytest/config.py\", line 943, in pytest_cmdline_parse\n    self.parse(args)\n  File \"/usr/lib/python3.6/site-packages/_pytest/config.py\", line 1108, in parse\n    self._preparse(args, addopts=addopts)\n  File \"/usr/lib/python3.6/site-packages/_pytest/config.py\", line 1071, in _preparse\n    self.pluginmanager.load_setuptools_entrypoints('pytest11')\n  File \"/usr/lib/python3.6/site-packages/pluggy/init.py\", line 402, in load_setuptools_entrypoints\n    \"Plugin %r could not be loaded: %s!\" % (ep.name, e))\npluggy.PluginValidationError: Plugin 'pytest_nameko' could not be loaded: (kombu 4.1.0 (/usr/lib/python3.6/site-packages), Requirement.parse('kombu<4,>=3.0.1'))!\n```\nWhen lowering down kombu to 3.0.17 i get error from celery (something about imports and other errors as well).\nAm I missing something obvious or I just can't use nameko and celery in one project due to kombu versions mismatch (can you suggest something in this case)?. To the continuation of my previous post, actual code running nameko instruction doing just fine. Successfully sent an event to my microservice from django. Pytest's still an issue though.. ",
    "asyncee": "Can someone clarify is there are still major incompatibility with kombu>4.0? We are using celery, which depends on kombu 4.x. This issue hardly influence the architectural choices, because it can not be used concurrently with celery.\nThanks.. We are currently in a situation where monolithic application is using celery and in a process of migration to soa/microservices architecture. We had two requirements:\n\nuse flask-nameko or\nshare a codebase in one environment \n\nWe can not split environments due to current architecture. Anyway, this approach introduces unnecessary complexity to support and deploy.\nFinally, i wrote a service (adapter) that receives json via HTTP and proparages it as event via rabbitmq, but i personally do not like such approach, because it is fragile and has it's own drawbacks (no free service discovery, for example). \nI think it would be great to freeze development of 2.x version and start working on new 3.x that will be backwards incompatible. It can even drop support for python2, if it is still supported. :). Perfect, thank you! I can use werkzeug's Rule to instantiate a url rule, and build url map (werkzeugs Map) which can resolve urls with parameters.\nThe documentation is here: http://werkzeug.pocoo.org/docs/0.14/routing/#werkzeug.routing.MapAdapter.build\nThanks!. I have created basic implementation. Check it out: https://gist.github.com/asyncee/afff12559e364b0665c89b0e53341c28. I did not expected such behaviour, thank you for explanation, that is just great! One question: are there any TTL for events by default?\nI'll close an issue, because you answered my main question.\nThanks!. Thanks!. ",
    "asteven": "Have long  moved on and implemented this in my own package.. Thanks for the pointers.\nI'm wondering if the whole configuration could/should use the same or similar extension machinery as the rest.\ne.g.\n```sh\nnameko run --config /path/to/file.yaml\n   - use YamlConfigProvider\n      - load the config file from disk\n      - optionally monitor the file for changes and update or restart or whatever \nnameko run --config consul://127.0.0.1:8500/v1/kv/myservice/config\n   - use ConsulConfigProvider\n      - load config from the consul kv store\n      - optionally could monitor kv store for changes\n```\nBrowsing quickly through the code it seems it would not be that difficult to allow a ServiceContainer in concert with a ServiceRunner do something like this.\nAnyway, just thinking out loud here.\nFor now I'll go with wrapping nameko to run under containerpilot.\nThen do all the heavy lifting there and just restart the nameko service on changes.\nIn the meantime I have found your mailing list. Would send any future questions there instead of opening a ticket.. Maybe --listen-http ? Don't care to much how this is called.. I don't break backcompat. I just changed the default config key to the new value.\nIn the code that interprets this I check for the old key.. ",
    "jgericke": "Thanks @kooba will limit any further queries to the mailing list :). ",
    "rgardam": "Hey, yes, I just posted a bit more.\n. Ok, that makes sense.\ni can see that Kombu is pinned to not use <4, but I guess this doesn't stop eventlet from using the latest. Do we want to reference this here? \nhttps://github.com/nameko/nameko/issues/378\nand just close this one? \nSeems to be related.. @kooba It seems like inside of newer versions of docker-compose the link: option doesn't do what it used to do. \nPreviously docker would create /etc/hosts entries, but this is not the case and it now only uses the internal dns.\nI will try your trick. \n. What I was saying is that in the nameko setup.py is only set to use greater than 0.16.1 which will install the latest. \ninstall_requires=[\n        \"eventlet>=0.16.1\",\n        \"kombu>=3.0.1,<4\",\n        \"mock>=1.2\",\n        \"path.py>=6.2\",\n        \"pyyaml>=3.10\",\n        \"requests>=1.2.0\",\n        \"six>=1.9.0\",\n        \"werkzeug>=0.9\",\n    ],\nThis is currently broken in docker-compose since the change they made to not add hosts file entries.\n. Thanks for the quick resolution on this! :)\n. ",
    "temoto": "Hello.\nI've noticed responsibility shifting tendency, so it's not Eventlet fault either. It's OS getaddrinfo that is bugged! You can always blame libc. :-)\nresolv.conf docs suggest that names with strictly less than options:ndots must be resolved using search domains. ndots defaults to 1, that affects unqualified names like broker.\nDnspython was always providing kinda proper behavior (it forces ndots:1 regardless of config) but Eventlet <0.20 used system (blocking) getaddrinfo by default, unless special environment variable set. Now its resolving is always green.\nI don't see a problem for Eventlet to support broken OS getaddrinfo, so make your pins >=0.20.1.. Please try this version with fix\npip install -U https://github.com/eventlet/eventlet/archive/f266be30f5c3ff1889e9ac3f0bad698a49d40e99.zip\nyou can always revert back to your favorite with pip install -r requirements.txt.. The fix is included in Eventlet v0.20.1 version that is already available on PyPI.\nIf you pinned requirements to 0.19 or installed zip from long url above, please update to eventlet==0.20.1 now.\nFly safe.. Please use this version for testing\npip install https://github.com/eventlet/eventlet/archive/b756447bab51046dfc6f1e0e299cc997ab343701.zip. ",
    "alexander-mayr": "I am having the same issue with eventlet 0.24.1 (see below). I'm not sure if it's the same issue but according to the stack trace this should be somehow related to eventlet again.\n```\namqp (2.4.0)\ncertifi (2018.11.29)\nchardet (3.0.4)\ndnspython (1.16.0)\neventlet (0.24.1)\nGDAL (2.1.3)\ngreenlet (0.4.15)\nidna (2.8)\nimportlib-metadata (0.8)\nkombu (4.2.2.post1)\nmock (2.0.0)\nmonotonic (1.5)\nnameko (2.11.0)\nnumpy (1.11.0)\npath.py (11.5.0)\npbr (5.1.1)\npip (8.1.1)\npycurl (7.43.0)\npygobject (3.20.0)\npython-apt (1.1.0b1+ubuntu0.16.4.2)\nPyYAML (3.13)\nrequests (2.21.0)\nsetuptools (20.7.0)\nsix (1.12.0)\nunattended-upgrades (0.1)\nurllib3 (1.24.1)\nvine (1.2.0)\nWerkzeug (0.14.1)\nwheel (0.29.0)\nwrapt (1.11.1)\nzipp (0.3.3)\n```\n\ntest_service_1  | Traceback (most recent call last):\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 137, in _connect\ntest_service_1  |     host, port, family, socket.SOCK_STREAM, SOL_TCP)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/support/greendns.py\", line 537, in getaddrinfo\ntest_service_1  |     raise socket.gaierror(socket.EAI_NONAME, 'No address found')\ntest_service_1  | socket.gaierror: [Errno -2] No address found\ntest_service_1  | \ntest_service_1  | During handling of the above exception, another exception occurred:\ntest_service_1  | \ntest_service_1  | Traceback (most recent call last):\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/hubs/poll.py\", line 109, in wait\ntest_service_1  |     listener.cb(fileno)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenthread.py\", line 219, in main\ntest_service_1  |     result = function(args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/utils/init.py\", line 181, in call\ntest_service_1  |     return getattr(item, name)(*args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/messaging.py\", line 226, in setup\ntest_service_1  |     verify_amqp_uri(self.amqp_uri, ssl=ssl)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/amqp/utils.py\", line 41, in verify_amqp_uri\ntest_service_1  |     with transport.establish_connection():\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/kombu/transport/pyamqp.py\", line 130, in establish_connection\ntest_service_1  |     conn.connect()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/connection.py\", line 307, in connect\ntest_service_1  |     self.transport.connect()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 81, in connect\ntest_service_1  |     self._connect(self.host, self.port, self.connect_timeout)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 148, in _connect\ntest_service_1  |     \"failed to resolve broker hostname\"))\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 161, in _connect\ntest_service_1  |     self.sock.connect(sa)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenio/base.py\", line 265, in connect\ntest_service_1  |     socket_checkerr(fd)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenio/base.py\", line 51, in socket_checkerr\ntest_service_1  |     raise socket.error(err, errno.errorcode[err])\ntest_service_1  | ConnectionRefusedError: [Errno 111] ECONNREFUSED\ntest_service_1  | Removing descriptor: 4\ntest_service_1  | Traceback (most recent call last):\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 137, in _connect\ntest_service_1  |     host, port, family, socket.SOCK_STREAM, SOL_TCP)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/support/greendns.py\", line 537, in getaddrinfo\ntest_service_1  |     raise socket.gaierror(socket.EAI_NONAME, 'No address found')\ntest_service_1  | socket.gaierror: [Errno -2] No address found\ntest_service_1  | \ntest_service_1  | During handling of the above exception, another exception occurred:\ntest_service_1  | \ntest_service_1  | Traceback (most recent call last):\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/hubs/hub.py\", line 460, in fire_timers\ntest_service_1  |     timer()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/hubs/timer.py\", line 59, in call\ntest_service_1  |     cb(args, kw)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/event.py\", line 177, in _do_send\ntest_service_1  |     waiter.throw(exc)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenthread.py\", line 219, in main\ntest_service_1  |     result = function(args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/utils/init.py\", line 181, in call\ntest_service_1  |     return getattr(item, name)(args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/containers.py\", line 188, in start\ntest_service_1  |     self.extensions.all.setup()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/utils/init.py\", line 186, in spawning_method\ntest_service_1  |     return list(pool.imap(call, self._items))\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenpool.py\", line 244, in next\ntest_service_1  |     val = self.waiters.get().wait()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenthread.py\", line 180, in wait\ntest_service_1  |     return self._exit_event.wait()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/event.py\", line 125, in wait\ntest_service_1  |     result = hub.switch()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/hubs/hub.py\", line 297, in switch\ntest_service_1  |     return self.greenlet.switch()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenthread.py\", line 219, in main\ntest_service_1  |     result = function(*args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/utils/init.py\", line 181, in call\ntest_service_1  |     return getattr(item, name)(args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/messaging.py\", line 226, in setup\ntest_service_1  |     verify_amqp_uri(self.amqp_uri, ssl=ssl)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/amqp/utils.py\", line 41, in verify_amqp_uri\ntest_service_1  |     with transport.establish_connection():\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/kombu/transport/pyamqp.py\", line 130, in establish_connection\ntest_service_1  |     conn.connect()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/connection.py\", line 307, in connect\ntest_service_1  |     self.transport.connect()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 81, in connect\ntest_service_1  |     self._connect(self.host, self.port, self.connect_timeout)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 148, in _connect\ntest_service_1  |     \"failed to resolve broker hostname\"))\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 161, in _connect\ntest_service_1  |     self.sock.connect(sa)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenio/base.py\", line 265, in connect\ntest_service_1  |     socket_checkerr(fd)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenio/base.py\", line 51, in socket_checkerr\ntest_service_1  |     raise socket.error(err, errno.errorcode[err])\ntest_service_1  | ConnectionRefusedError: [Errno 111] ECONNREFUSED\ntest_service_1  | Traceback (most recent call last):\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 137, in _connect\ntest_service_1  |     host, port, family, socket.SOCK_STREAM, SOL_TCP)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/support/greendns.py\", line 537, in getaddrinfo\ntest_service_1  |     raise socket.gaierror(socket.EAI_NONAME, 'No address found')\ntest_service_1  | socket.gaierror: [Errno -2] No address found\ntest_service_1  | \ntest_service_1  | During handling of the above exception, another exception occurred:\ntest_service_1  | \ntest_service_1  | Traceback (most recent call last):\ntest_service_1  |   File \"/usr/local/bin/nameko\", line 11, in \ntest_service_1  |     sys.exit(main())\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/cli/main.py\", line 107, in main\ntest_service_1  |     args.main(args)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/cli/commands.py\", line 110, in main\ntest_service_1  |     main(args)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/cli/run.py\", line 184, in main\ntest_service_1  |     run(services, config, backdoor_port=args.backdoor_port)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/cli/run.py\", line 131, in run\ntest_service_1  |     service_runner.start()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/runners.py\", line 66, in start\ntest_service_1  |     SpawningProxy(self.containers).start()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/utils/init.py\", line 186, in spawning_method\ntest_service_1  |     return list(pool.imap(call, self._items))\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenpool.py\", line 244, in next\ntest_service_1  |     val = self.waiters.get().wait()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenthread.py\", line 180, in wait\ntest_service_1  |     return self._exit_event.wait()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/event.py\", line 125, in wait\ntest_service_1  |     result = hub.switch()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/hubs/hub.py\", line 297, in switch\ntest_service_1  |     return self.greenlet.switch()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenthread.py\", line 219, in main\ntest_service_1  |     result = function(*args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/utils/init.py\", line 181, in call\ntest_service_1  |     return getattr(item, name)(args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/containers.py\", line 188, in start\ntest_service_1  |     self.extensions.all.setup()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/utils/init.py\", line 186, in spawning_method\ntest_service_1  |     return list(pool.imap(call, self._items))\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenpool.py\", line 244, in next\ntest_service_1  |     val = self.waiters.get().wait()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenthread.py\", line 180, in wait\ntest_service_1  |     return self._exit_event.wait()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/event.py\", line 125, in wait\ntest_service_1  |     result = hub.switch()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/hubs/hub.py\", line 297, in switch\ntest_service_1  |     return self.greenlet.switch()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenthread.py\", line 219, in main\ntest_service_1  |     result = function(*args, kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/utils/init.py\", line 181, in call\ntest_service_1  |     return getattr(item, name)(args, **kwargs)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/messaging.py\", line 226, in setup\ntest_service_1  |     verify_amqp_uri(self.amqp_uri, ssl=ssl)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/nameko/amqp/utils.py\", line 41, in verify_amqp_uri\ntest_service_1  |     with transport.establish_connection():\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/kombu/transport/pyamqp.py\", line 130, in establish_connection\ntest_service_1  |     conn.connect()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/connection.py\", line 307, in connect\ntest_service_1  |     self.transport.connect()\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 81, in connect\ntest_service_1  |     self._connect(self.host, self.port, self.connect_timeout)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 148, in _connect\ntest_service_1  |     \"failed to resolve broker hostname\"))\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/amqp/transport.py\", line 161, in _connect\ntest_service_1  |     self.sock.connect(sa)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenio/base.py\", line 265, in connect\ntest_service_1  |     socket_checkerr(fd)\ntest_service_1  |   File \"/usr/local/lib/python3.5/dist-packages/eventlet/greenio/base.py\", line 51, in socket_checkerr\ntest_service_1  |     raise socket.error(err, errno.errorcode[err])\ntest_service_1  | ConnectionRefusedError: [Errno 111] ECONNREFUSED\n. @mattbennett \n\nthank you. The stackstrace itself is rather weird as it brings \"connection refused\" and dns errors. I am out of the office for today but I see if I can get some more details tomorrow.. @mattbennett \nThis was a pebcak. Problem was solved by adding a sleep script to the container entrypoint to properly wait for the broker to startup. Sorry for the confusion.. ",
    "vmikki": "Thank for the review! I have addressed your comments and modified the example so it more looks like a \"real-life usage\". I have also added myself to the contributors.. I have fixed the broken builds and now my PR is ready to be reviewed.\nI had to use subprocesses in order to reliably test the new feature and the code covered by those tests is marked as not covered in coverage. The possible solution for that would have been a bit hacky and unreliable so I just added no cover instead, I hope it's fine.. I came from PHP, it's good to know that it should be avoided in python, thanks!. I have added from __future__ import absolute_import and now it works in Python 2.7.. Is there a way to add no cover to the whole module? I think excluding the whole file can only be done by specifying the omit parameter for coverage but in that case it will not be visible in the module that it is omitted from coverage. I agree that it should be explained why no cover is there but perhaps it would be better to be explicit and keep the current no covers with some comment next to them.  . ",
    "jdsolucoes": "My bad, resolved. The url was not the same: /role != /role/ \nSorry =/ . ",
    "juliotrigo": "Hi @mattbennett \nSorry I am a bit late in my reply, of course you can cherry pick those changes. I was also preparing the changes for the publish declare as you suggested.\nI've had a look at #400 and how keyword arguments have been added to the Publisher interface, including the declare parameter. How is this going to affect this current PR? Is it going to be replaced by #400?. I have reviewed https://github.com/mattbennett/nameko/pull/7 and left a few comments.. I have added the queue argument back with the changes that you suggested to maintain compatibility. I should have included those ones in the first place with my request.\nWe're now also working with a copy of the original queues_to_declare inside the Publisher to ensure the provided sequence is a list (so that we can add elements to it later) and to avoid side effects outside this class when adding queue to it.\nThe code now also raises a warning when queue is provided. Having a look at other examples in the codebase, I have assumed these changes could be included in version 2.5.2 and also that this warning would be removed in version 2.7.0. Please let me know if you are happy with this.\n. Thanks for the suggestion. As far as I see it, the big change here would be that we will ensure queues and exchanges are correctly bound before publishing a message rather than just once when the dependency is set up, so Kombu will have to declare those entities each time.\nI am happy to change it and let Kombu do it for us when publishing, if you think those extra declarations are acceptable.. ",
    "coveralls": "\nChanges Unknown when pulling e17752684d65376be568036ebdfe4e402bc39b69 on mattbennett:coveralls into  on nameko:master.\n. \n\nChanges Unknown when pulling e17752684d65376be568036ebdfe4e402bc39b69 on mattbennett:coveralls into  on nameko:master.\n. \n\nChanges Unknown when pulling e17752684d65376be568036ebdfe4e402bc39b69 on mattbennett:coveralls into  on nameko:master.\n. \n\nChanges Unknown when pulling e17752684d65376be568036ebdfe4e402bc39b69 on mattbennett:coveralls into  on nameko:master.\n. \n\nChanges Unknown when pulling e17752684d65376be568036ebdfe4e402bc39b69 on mattbennett:coveralls into  on nameko:master.\n. \n\nChanges Unknown when pulling e17752684d65376be568036ebdfe4e402bc39b69 on mattbennett:coveralls into  on nameko:master.\n. ",
    "minhhg": "Hi,\nHere is the output of pip freeze\n--------------------------------------------------------------------\nalabaster==0.7.9\namqp==1.4.9\nanaconda-client==1.5.5\nanyjson==0.3.3\nawscli==1.11.13\nBabel==2.3.4\nbackports-abc==0.4\nbackports.shutil-get-terminal-size==1.0.0\nbackports.ssl-match-hostname==3.4.0.2\nbeautifulsoup4==4.5.1\nbilliard==3.3.0.23\nbokeh==0.12.3\nboto==2.43.0\nbotocore==1.4.70\nBottleneck==1.1.0\nbrewer2mpl==1.4.1\ncashew==0.2.7\ncelery==3.1.23\ncffi==1.9.1\nchardet==2.3.0\nchest==0.2.3\nclick==6.6\ncloudpickle==0.2.1\nclyent==1.2.2\ncolorama==0.3.7\ncolorlover==0.2.1\nconfigparser==3.5.0\nconstantly==15.1.0\ncoverage==4.2\ncryptography==1.6\ncufflinks==0.8.2\ncycler==0.10.0\nCython==0.25.1\ndask==0.12.0\ndecorator==4.0.10\ndexy==1.0.15\ndexy-viewer==0.0.5\ndill==0.2.5\ndocutils==0.12\nenum-compat==0.0.2\nenum34==1.1.6\net-xmlfile==1.0.1\neventlet==0.20.1\nfaulthandler==2.4\nflake8==2.5.1\nFlask==0.11.1\nflower==0.9.1\nfuncsigs==1.0.2\nfunctools32==3.2.3.post2\nfuture==0.16.0\nfutures==3.0.5\ngitdb2==2.0.0\nGitPython==2.0.9\ngreenlet==0.4.11\nh5py==2.6.0\nHeapDict==1.0.0\nidna==2.1\nimagesize==0.7.1\nincremental==16.10.1\ninflection==0.3.1\nipaddress==1.0.17\nipdb==0.10.1\niptools==0.6.1\nipykernel==4.5.1\nipython==5.1.0\nipython-genutils==0.1.0\nipywidgets==5.2.2\nitsdangerous==0.24\njdcal==1.3\nJinja2==2.8\njmespath==0.9.0\njsonschema==2.5.1\njupyter==1.0.0\njupyter-client==4.4.0\njupyter-console==5.0.0\njupyter-core==4.2.0\nkombu==3.0.35\nlibrabbitmq==1.6.1\nlinecache2==1.0.0\nlocket==0.2.0\nlog4mongo==1.5.0\nlogutils==0.3.3\nlpsolve55==5.5.2.0\nlxml==3.6.4\nMarkdown==2.6.7\nMarkupSafe==0.23\nmatplotlib==1.5.3\nmccabe==0.3.1\nMDP==3.5\nmistune==0.7.3\nmock==2.0.0\nmultiprocess==0.70.4\nnameko==2.5.1\nnb-anacondacloud==1.2.0\nnb-conda==2.0.0\nnb-conda-kernels==2.0.0\nnbconvert==4.2.0\nnbformat==4.1.0\nnbpresent==3.0.2\nnetCDF4==1.2.4\nnetworkx==1.11\nnose==1.3.7\nnosexcover==1.0.10\nnotebook==4.2.3\nnumexpr==2.6.1\nnumpy==1.11.2\nopenopt==0.5625\nopenpyxl==2.4.0\npandas==0.19.1\nparamiko==2.0.2\npartd==0.3.6\npath.py==10.1\npathlib2==2.1.0\npathos==0.2.0\npatsy==0.4.1\npbr==1.10.0\npep8==1.7.0\npexpect==4.0.1\npickleshare==0.7.4\nplotly==1.12.9\nply==3.9\npox==0.2.2\nppft==1.6.4.6\nprompt-toolkit==1.0.9\nptyprocess==0.5.1\npudb==2016.2\npy==1.4.31\npyasn1==0.1.9\npycairo==1.10.0\npycparser==2.17\npyflakes==1.3.0\nPygments==2.1.3\npykalman==0.9.5\npymongo==2.8\nPyMySQL==0.7.9\npyparsing==2.1.4\npytest==3.0.4\npytest-faulthandler==1.3.1\npython-dateutil==2.6.0\npython-modargs==1.7\npytz==2016.7\nPyYAML==3.12\npyzmq==16.0.2\nqtconsole==4.2.1\nRandom123==0.0.0\nrequests==2.11.1\nrope==0.9.4\nrsa==3.4.2\ns3transfer==0.1.9\nscikit-learn==0.18.1\nscikits.timeseries==0.91.3\nscipy==0.18.1\nseaborn==0.7.1\nsetproctitle==1.1.10\nsimplegeneric==0.8.1\nsingledispatch==3.4.0.3\nsix==1.10.0\nsmmap2==2.0.0\nsnakefood==1.4\nsnowballstemmer==1.2.1\nSphinx==1.4.8\nSQLAlchemy==1.1.4\nstatsmodels==0.6.1\ntables==3.3.0\nterminado==0.6\nTheano==0.8.2\ntoolz==0.8.0\ntornado==4.4.2\ntraceback2==1.4.0\ntraitlets==4.3.1\nTwisted==16.5.0\nunittest2==1.1.0\nurwid==1.3.1\nwcwidth==0.1.7\nweb.py==0.38\nWerkzeug==0.11.11\nwidgetsnbextension==1.2.6\nxarray==0.8.2\nxlrd==1.0.0\nXlsxWriter==0.9.3\nxlwt==1.1.2\nzope.interface==4.3.2. Yes, that is the case. Many thanks for your help. M. ",
    "scarchik": "I wanted to use flask-nameko, but after testing I found out that it not what I need. It create too many connections, not much better than use ClusterRpcProxy standalone. \nIn system with large number of users make a connection for each call is wasteful. \nWith current change I can use one ClusterRpcProxy which have 1 connect to RabbitMQ to send messages and many lazy connections (they created by kombu) for waiting replies. Advantages best be seen on long polling queries.. Ok, I already change this request as you say, it be useful.\nThank you for your comments.\nPS: Does it make sense to create pypi package (like: nameko_proxy) with this code or it not useful for other?. Thanks for your answer.\nUnfortunately request_context is not working for me, because I use ClusterRpcProxy as global variable with multi threading.\nIn my project I need use API Gateway in front nameko microservices. For this task I should ensure that on Gateway I have less is better connections to RabbitMQ. In high load systems create new connections for each call is wasteful. In this case flask-nameko is not resolve connections issue.\nAs I see it for \"standalone app & nameko\" we don't need any pool of ClusterRpcProxy's but need have asynchronous Queue Consumer as nameko services use itself. I already write it and now testing: https://github.com/fraglab/nameko-proxy. Please look.\nI can save overriding WorkerContext for me only, but it is better to have fair opportunity to do this.\n. ",
    "greole": "I'd like to have a service that connects at start up to some external site to do some data fetching every once in a while.  Then some data processing inside the data handler takes place and the result would be accessible via the rpc call. \n```\nimport requests\nclass Service:\ndata_handler = Connector()\n\n@rpc\ndef test(self):\n    return data_handler.get_current_mean()\n\n```\nI though the request would be made on startup of  the service nameko run   test. Should this be implemented via dependency injection? I could not figure it out from the docs.. It runs now. Thx for your help. The recursion depth exceeded error was related to python3.7 and it runs flawlessly with python 3.4.. ",
    "the01": "That is not entirely true. You could write your own start script.\nLets call it mynameko.py:\n``` python\nfrom nameko.cli.main import main\nfrom kombu.serialization import register\ndef encode(value):\n    # Encode your objects to string however you want\n    return value_as_String\ndef decode(value):\n    # Decode your objects from strings however you want\n    return value_as_object\nregister(\n        \"new_serializer_name\",\n        encode, decode,\n        \"application/new-serializer-name\", \"utf-8\"\n)\nmain()\n``\nAnd instead of starting the shell withnameko shellyou would run it withpython mynameko.py shell. It is a drop in replacement fornameko, except now you can use yournew_serializer_name` - serializer. ",
    "jazzburger": "Hi, thanks for looking into it. I tried what you suggested, but still got this error: Can't decode message body: ContentDisallowed('Refusing to deserialize untrusted content of type json (application/json)',) (type:u'application/json' encoding:u'utf-8' raw:'u\\'{\"args\": [\"world\"], \"kwargs\": {}}\\''') when calling n.rpc.service_a.hello(\"world\")\nstart scipt: nameko_start.py\n```python\nimport json\nfrom kombu.serialization import register\nfrom nameko.cli.main import main\ndef encode(value):\n    upper_value = json.dumps(value).upper()\n    print \"encode: upper_value => %s\" %upper_value\n    return upper_value\ndef decode(value):\n    lower_value = json.loads(value.lower())\n    print \"decoded: lower_value => %s\" %lower_value\n    return lower_value\nregister(\"upperjson\", encode, decode, \"application/x-upper-json\", \"utf-8\")\nmain()\n``custom_serializer.py`:\n```python\nimport json\nfrom kombu.serialization import register\nfrom nameko.rpc import rpc\ndef encode(value):\n    upper_value = json.dumps(value).upper()\n    print \"encode: upper_value => %s\" %upper_value\n    return upper_value\ndef decode(value):\n    lower_value = json.loads(value.lower())\n    print \"decoded: lower_value => %s\" %lower_value\n    return lower_value\nregister(\"upperjson\", encode, decode, \"application/x-upper-json\", \"utf-8\")\nclass ServiceA(object):\n    name = \"service_a\"\n@rpc\ndef hello(self, name):\n    return \"Hello, {}!\".format(name)\n\n``config.yaml`:\npython\nserializer: 'upperjson'\nAMQP_URI: 'pyamqp://guest:guest@localhost:5672'\nrpc_exchange: 'nameko-rpc'\nmax_workers: 10\nparent_calls_tracked: 10\nRan:$ nameko run --config ./config.yaml custom_serializer and $ python nameko_start.py shell\n. ",
    "abulte": "Ok thanks. Can't wait for this ideal world to come true ;-). ",
    "erhuabushuo": "Thanks a lot. ",
    "parachvte": "I think how I call the service is irrelevant, since I push hundreds of async RPC calls into queue all at once and just to see how service works.\nThe way I run the service is almost the same as nameko run (a.k.a. nameko.cli.run.run()) except that I call it manually (to rewrite the default max_workers). I posted the full demo here.\n. I think it's irrelevant to nameko, but about your terminal encodings. You can check if sys.stdout.encoding is UTF-8.. ",
    "mikegreen7892003": "I work with @Ryannnnnnn .\nThere is a dead lock between msg.ack and drain_events while container.max_workers equals or large than prefetch_count.\ne.g.\nWe set max_workers = 1, so QueueConsumer.prefetch_count = 1  and consumer.qos(prefetch_count=self.prefetch_count). \nRabbitMQ with the prefetch_count=1 will not delivery new message while the old one is not acked.\nmsg.ack() in QueueConsumer._process_pending_message_acks is called by QueueConsumer.on_iteration for each drain_events loop iteration.\nSo drain_events could not get new message util on_iteration called or time out. It will be time out after safety_interval seconds.\nhttps://github.com/nameko/nameko/blob/master/nameko/messaging.py\nPython\n    @property\n    def prefetch_count(self):\n        return self.container.max_workers\nPython\n    def get_consumers(self, consumer_cls, channel):\n           ******\n            consumer.qos(prefetch_count=self.prefetch_count)\n           ******\nPython\n    def on_iteration(self):\n        ******\n        self._process_pending_message_acks()\n        ******\nPython\n    def on_iteration(self):\n        ******\n        self._process_pending_message_acks()\n        ******\nPython\n    def _process_pending_message_acks(self):\n         ******\n                msg.ack()\n        ******\nhttps://github.com/celery/kombu/blob/master/kombu/mixins.py\nPython\n    def consume(self, limit=None, timeout=None, safety_interval=1, **kwargs):\n        elapsed = 0\n        with self.consumer_context(**kwargs) as (conn, channel, consumers):\n            for i in limit and range(limit) or count():\n                if self.should_stop:\n                    break\n                self.on_iteration()  # ack will be called here\n                try:\n                    conn.drain_events(timeout=safety_interval)\n                except socket.timeout:\n                    conn.heartbeat_check()\n                    elapsed += safety_interval\n                    if timeout and elapsed >= timeout:\n                        raise\n                except socket.error:\n                    if not self.should_stop:\n                        raise\n                else:\n                    yield\n                    elapsed = 0\n        debug('consume exiting'). I suggest that\nPython\n    @property\n    def prefetch_count(self):\n        return self.container.max_workers * 2\nor prefetch_count could be configuable. @davidszotten  I found that too.  safety_interval was changed between v2.4.3 and v2.4.4 .\nCurrent implementation could process about max_workers / safety_interval messages per second.. @mattbennett  @davidszotten \nThanks for your review.\nmax_workers / safety_interval limitation is because the messages are acked by nameko every drain_events.\nTo fix the problem quickly, we could simply set prefetch_count larger than max_workers, e.g. max_workers + 1 could be a reasonable option.\nOur fixture is following,\n```Python\nfrom nameko.messaging import QueueConsumer\n@property\ndef prefetch_count(self):\n    return self.container.max_workers + 1\nQueueConsumer.prefetch_count = prefetch_count\n```\nI agree with @mattbennett  and think the problem is intrinsic. Unless message is acked once it's processed.\ne.g.\nI modify the code of @mattbennett with prefetch_count = 1 and GreenPool(size=2). The code will process one message per second.\npublisher.py\n```Python\ncoding=utf-8\nimport eventlet\neventlet.monkey_patch()\nimport logging\nfrom tornado.options import parse_command_line\nparse_command_line()\nfrom eventlet.greenpool import GreenPile\nfrom kombu.pools import producers\nfrom kombu import Exchange, Queue\nexchange = Exchange('exchange', type='direct')\nqueue = Queue('queue', exchange, routing_key='queue')\nif name == 'main':\n    from kombu import Connection\nconnection = Connection('amqp://guest:guest@localhost')\n\ndef publish(index):\n    with producers[connection].acquire(block=True) as producer:\n        producer.publish(index, routing_key='queue', serializer='json')\n\npile = GreenPile()\nfor index in xrange(100):\n    pile.spawn(publish, index)\nlist(pile)\n\n```\nconsumer.py\n```Python\ncoding=utf-8\nimport eventlet\neventlet.monkey_patch()\nimport logging\nfrom tornado.options import parse_command_line\nparse_command_line()\nimport time\nimport threading\nfrom eventlet.greenpool import GreenPool\nfrom kombu.mixins import ConsumerMixin\nfrom kombu.log import get_logger\nfrom kombu import Exchange, Queue\nfrom kombu import Connection\nexchange = Exchange('exchange', type='direct')\nqueue = Queue('queue', exchange, routing_key='queue')\npool = GreenPool(size=2)\nclass Worker(ConsumerMixin):\n    def init(self):\n        self._message_to_ack = []\ndef on_iteration(self):\n    message_list = self._message_to_ack\n    self._message_to_ack = []\n    for message in message_list:\n        message.ack()\n\n@property\ndef connection(self):\n    return Connection('amqp://guest:guest@localhost', heartbeat=60.0)\n\ndef handle_message(self, body, message):\n    logging.info(\"CONSUME %s in %s\", body, threading.current_thread().get_name())\n    gt = pool.spawn(self.process_task, body, message)\n\ndef get_consumers(self, Consumer, channel):\n    consumer = Consumer(queues=[queue],\n                        accept=['json'],\n                        callbacks=[self.handle_message])\n    consumer.qos(prefetch_count=1)\n    return [consumer]\n\ndef process_task(self, body, message):\n    logging.info(\"ACK %s in %s\", body, threading.current_thread().get_name())\n    self._message_to_ack.append(message)\n\nif name == 'main':\n    try:\n        worker = Worker()\n        pool.spawn(worker.run).wait()\n    except KeyboardInterrupt:\n        print('bye bye')\n```. @davidszotten  Thanks for your advise.\nI make a PR and explain why.. ",
    "Trex-Boolat": "for eventlet's 0.17.2 - i must update setup.py inside my pull request? or you make all work in master?. checks done :) https://travis-ci.org/nameko/nameko/builds/218489677\n. When i see release with PR's 421+422?. Ok. Changed.. Changed... \n. This (True, False) magic - only way to say pyyaml: try resolve node by content and do not return string in all cases. Via hinter from yaml.add_implicit_resolver('!env_var', IMPLICIT_ENV_VAR_MATCHER) any yaml node with ${...} routed to _env_var_constructor() and avoid pyyaml resolver. So when i use yaml\na: True\nb: ${env_b}\nwithout this fix - always boolean a and string b, even if env_b == \"True\". ",
    "ornoone": "I just wrote some snippet to achieve this via promise python package: \n```python\nimport eventlet\nfrom promise.promise import Promise\ndef make_promise(result_async):\n    def promise_caller(resolve, reject):\n        def promise_waiter():\n            resolve(result_async.result())  # will wait for rcp reply\n        eventlet.spawn(promise_waiter)\n    return Promise(promise_caller)\nwith ClusterRpcProxy(config) as cluster_rpc:\n    set_handler_for_events()\n    hello_res = cluster_rpc.service_x.remote_method.call_async(\"hello\")\n    promise = make_promise(hello_res)\n    @promise.then\n    def after(result):\n        teardown_handler()\n```. thanks for the fast reply :smile: .\nit's true and I understand why you want to keep it outside the framework, but it may be a good idea to integrate some snippet like this to the doc, showing how nameko can be used along with differents patterns.. this PR can solve the build error on #443. can you merge it ? . no problem. I will work on it this morning. I tried to add some tests, but I was unable to run some other random tests with success in my computer. \nIf you can write it, I will be eager to see what is does look like and I will try to make it run localy. \n. thanks for the tests. . Ok, I was expecting something like that. \nwe will work on the pool size instead\nthanks to you for this very helpful response. I hope I will be of some use for this project one day :D . after some search, I have found a regex which work and solve both cases: \nregex\n\\$\\{(?:([^{}:\\s]+)(?::(.*(?R)?.*))?)*(?<!\\\\)\\}\ncheck it at regex101.com\nit seem to solve the problem of the \u00abfirst }\u00bb occurrence by a \u00ablast }\u00bb occurrence. but it support a \\ escape char for \u00ab}\u00bb.\nit will solve the problem of nested env_var support if the function \ntodo for implementation: \n- use regex module (from pypi) instead of builtin re, since re don't support recursive regex (?R)\n- replace escaped \\} after parsing to set it back to }\n- maybe change env_var_constructor to support nested resolver ?\nregex explained : \nregex\n\\$\\{  # match opening bracket \n(?:   # non capturing group A for repeat for all env_var token\n(      # capturing group for var_name\n[^{}:\\s]+  # all allowed char for var name (minus space brackets and comma\n)      # end of capture var_name\n(?:   # start non capturing group B optional \n:      # var_name => default value separator\n(     # default value capture group\n.*   # any char before a sub pattern\n(?R)?   # recursive env_var  if exists\n.*      # any char can follow a env_var recursive patern\n)      # close  capturing of default value\n)?    # close group B which is optional (can skip default value)\n)*     # match as many times as required the patern (group A)\n(?<!\\\\)\\}    # non escaped closing bracket. hi @mattbennett , I will work on it and will take care of your dependency concern. . @mattbennett travis seem to complains about spelling on files I did not changes... \ncontent of spelling/output.txt: \ninstallation.rst:21: (setuptools) \ntesting.rst:81: (setuptools)\nwhat can I do to fix it on my branch ? . I finally passed all test. to fix coverage 100%, i added some # pragma: no cover to code specific to the presence or not of the package regex. . @kooba thanks for your review. sorry for all this typo, english is not my mother language as you can see... thanks for your work.\n. the ENV_VAR_MATCHER regex behavior is to match until the first }. this split the given config into: \n${FOO:${INDICE}+_val}\nwith FOO having default value of ${INDICE concatenated with raw string _val}. I have added test for both old and new behaviors. the behaviors described is for the old behavior @pytest.mark.skipif(has_regex_module, => don't run if we are in the env with regex, so this run only without regex, and the old behavior is expected.\n=> the difference is in the @pytest.mark.skipif(==>no<==t has_regex_module,. ",
    "mat10tng": ":+1:  , I also have this problem. If you manual install eventlet==v.20.0 then it started to working again.. Yes, of course, log file is included. I had multiple service run with docker and every service are hit with the same kind of errors when rebuild without specify the eventlet version.\nlog\nlog.txt\nrequirement\nnameko==2.5.2\ndockerfile\nFROM python:2.7\nENV PYTHONUNBUFFERED 1\nRUN mkdir /module\nCOPY compose/requirements/ /module/compose/requirements/\nRUN pip install -r /module/compose/requirements/dev --cache-dir /pip-cache\nEXPOSE 5000\nrun\nnameko run --config config.yml service --backdoor 3000\n. ",
    "mohamedrez": "Thanks @davidszotten there is only one thing that I want to add, It was very difficult for us to debug because the error was thrown on:\nfrom django_dynamic_fixtures import G\nthe log was showing:\nE   RuntimeError: no suitable implementation for this system\nwe started reviewing our requirement.txt file and nothing was changed, then we realized that the setup of nameko was specifying a none fixed release dependency to eventlet.. ",
    "stevebasher": "Specifying eventlet==0.20.0 within my requirements.txt fixed this issue for me.. ",
    "zhouxiaoxiang": "I have written a project framework for my project, which can discover and register all services for my app. \n\u3000\u3000Now code looks like:\ndef call_service(service, method, **kwargs):\n    method = getattr(service, method)\n    ......\n    return method(kwargs)\n\u3000\u3000But, I prefer so:\ndef call_service(service, method, **kwargs):\n    return service[method](kwargs)\n\u3000\u3000~~There's more than one way to do it.~~.   The latest pytest  causes conflict with nameko, because:\n\ncommit 8eafbd05ca2d980b36541fbc9d547e52b6016a9a      \nAuthor: Thomas Hisch t.hisch@gmail.com \nDate:   Wed Sep 13 11:25:17 2017 +0200                 \nMerge the pytest-catchlog plugin\n\n:040000 040000 8405b71041e3f4eee167bafb11f7407c1483a49f > > >db75ea6b6af2ec5bdcdf0887daff3a5e19285e20 M     pytest\n\n+    add_option_ini(\n  +        parser,\n  +        '--log-level',\n  +        dest='log_level', default=None,\n  +        help='logging level used by the logging module'\n  +    )\nI have to pin pytest at older version temporarily if I want to use nameko and pytest together.. @davidszotten \nThis PR permits mix-in parent class as a normal class  and will not be added into runner.. ",
    "2XL": "ok,\nanother Question:\nfollowing the docs\nhelloworld.py\n```python\nclass TargetServiceName:\n    name = \"target_service_name\"\n@rpc\ndef method(self, name='method'):\n    msg = \"Hello, {}!\".format(name)\n    print(msg)\n    return msg\n\n```\nnameko_examples/client_rpc_helper.py\npython\ncluster_proxy = ClusterRpcProxy(config)\ncluster_proxy = cluster_proxy.start()\ncluster_proxy.target_service_name.method()\ncluster_proxy.stop()\ntxt\nTraceback (most recent call last):\n  File \"nameko_examples/client_rpc_helper.py\", line 33, in <module>\n    cluster_proxy.stop()\nTypeError: 'ServiceProxy' object is not callable\nwhat does this mean? \nWhy am I getting this error? \n. ok solved\n```python\n\nCLUSTER_PROXY\n\nwith ClusterRpcProxy(config) as cluster_proxy_raw:\ncluster_proxy_raw.greeting_service.hello()\ncluster_proxy = ClusterRpcProxy(config)\ncluster_proxy_connector = cluster_proxy.start()\ncluster_proxy_connector.target_service_name.method()\ncluster_proxy.stop() \n```\nThanks David!, appreciate your support.\n. ",
    "defat": "Implementing this in a DependencyProvider instead of global variable seems extremely reasonable for me.\nHowever there is only one way to append correlation_id or call_id to every log record including logs from other libraries - to implement a filter object (not formatter as I mentioned earlier, sorry for this). Implementation should be smth like:\npython\nclass CorrelationIdFilter(logging.Filter):\n    def filter(self, record):\n        if global_worker_ctx:\n            record.correlation_id = global_worker_ctx.call_id_ctack[0]\n        return True\nWith logging cfg smth like:\nyaml\nLOGGING:\n  ...\n  formatters:\n    ....\n      format: '(asctime)(msecs)(levelname)(process)(module)(lineno)(args)(msg)(correlation_id)'\n      class: pythonjsonlogger.jsonlogger.JsonFormatter\n  filters:\n    request_id_filter:\n      (): my_module.CorrelationIdFilter\n...\nSuch approach gives ability to tag and retrieve all log records made for particular request even made by other libraries.\nLooks like implementing DependencyProvider storing needed information from worker_ctx in a global variable that will be used in log filter is the right way.. Yeah, I've used correlation_id just for example.\nThanks for so fast feedback and recommendations!\nPull request should be closed.. ",
    "szhjia": "Sorry, my mistake, it is ok with requests-future.. ",
    "frexvahi": "The CI failure is due to a TimeoutError in an unrelated component, can it be merged anyway?. I found a workaround - run the non-yielding function with eventlet.tpool\n```\nimport datetime\nimport itertools\nimport eventlet.tpool\nfrom nameko.rpc import rpc\nclass PrimeService:\n    name = \"prime_service\"\n@rpc\ndef prime(self, n):\n    return eventlet.tpool.execute(self._prime, n)\n\ndef _prime(self, n):\n    primes = []\n    for i in itertools.count(2):\n        for p in primes:\n            if (i % p) == 0:\n                break\n        else:\n            primes.append(i)\n        if len(primes) >= n:\n            break\n    p = primes[-1]\n    return p\n\n```. Possibly this is related to https://github.com/eventlet/eventlet/issues/420. An additional problem is that, when testing this with pytest and container_factory, the test will hang rather than failing.. ",
    "geoffjukes": "Solved! I read thru the source and noted the dictionary-like access for the service. So I combined that with a getattr, and it works. \ndef call(service, method, **kwargs):\n\u2028    with ClusterRpcProxy(config) as cluster_rpc:\n\u2028        return getattr(cluster_rpc[service], method)(**kwargs)\nAdding the dict-like access for the method names would simplify my code, and I'll see about submitting a pull request. \nAwesome tool. Thanks for releasing it!\nGeoff\n. Fair enough :). Hi Matt,\nThanks for the response. Your understanding is correct, and your solution makes absolute sense.\nOne frustration with Celery was related to task deduplication. I have a Couchbase cluster handy \ud83d\ude04, and so I could use a distributed lock (like Sherlock) to prevent duplicate tasks, then a result store using a predictable key format. Storing results (as you say) from the \"master\" task. \nI like it. Thanks for the help!. What an excellent answer, thanks Matt!. Wanted to say thank @frexvahi for your investigation and fix here. This helped be figure out an issue I was having,which I thought was network related - but was actually a (very) long running subprocess call blocking.\nIn my case, I was able to switch to the greened subprocess on one side of the equation - now I need to solve the other side (where I execute multiple requests via a concurrent.futures ThreadPool). You're welcome! Nameko is definitely my favorite import right now :) . I have no idea how to update Pull requests, so I'll just resubmit this. My apologies.. @mattbennett OK, so making additional changes to an open pull request is just magical - I've never done it before, so I was expecting it to be a bit more involved/complicated.. @mattbennett The build fails on the test_examples. Pretty sure this is because the updated example needs the updated code, and the test_examples installs nameko 2.8.3 - so no update.\nI can remove the example to make the tests pass, if that is preferred.. Oh my word, I cannot believe I missed that. Thanks for not mocking me!\nI currently stack the decorators, but it gets pretty ugly. With more complex URLs, it makes changes more involved. It's your choice of course, but for me, I think this would be a very useful change.. Example:\n@http('PUT', '/<string(length=4):book_id>/deliver')\n@http('GET', '/<string(length=4):book_id>/deliver')\n@http('PATCH', '/<string(length=4):book_id>/deliver')\n@http('DELETE', '/<string(length=4):book_id>/deliver')\n    def deliver(self, request, book_id=False):\n        return do_stuff(book_id, request.method)\nvs\n@http('PUT,GET,PATCH,DELETE', '/<string(length=4):book_id>/deliver')\n    def deliver(self, request, book_id=False):\n        return do_stuff(book_id, request.method). Yes, I do check the method later. The example was over simplified. More often I find my endpoints share 99% of the same code, changing only the return, for example a GET returns the computed data, but a PUT submits it to a secondary process. These methods can be rather long. . Thanks, and no worries!. Temporarily patched GreenSocket like this:\n```\n\nMonkey patch Eventlet GreenThread to add context manager\nThis is not required in eventlet 0.22 and up\nfrom eventlet.greenio.base import GreenSocket\ndef add_enter(self):\n    return self\ndef add_exit(self, *args):\n    self.close()\nGreenSocket.enter = add_enter\nGreenSocket.exit = add_exit\n\n```\n. ",
    "Blackbelly": "@Ryannnnnnn  Thanks. The terminal in Pycharm IDE does not support unicode. I run the service on OS X terminal and it works. \u8c22\u8c22\u57fa\u4f6c!. ",
    "bobh66": "Thanks Matt.  It's definitely not a drop-in replacement, but it looks like most of the eventlet functionality has corresponding functions in gevent.  gevent.event.AsyncResult is pretty close to eventlet.Event, (it uses set() instead of send()).  WSGIServer and WebSocket appear to be the biggest differences.\ngunicorn has support for both eventlet and gevent in their AsyncWorker, and that might be a good model to use.  I'll put together a PR when I get a chance and see what you think.\n. I started working on it over the summer and then got distracted by other things.  My first pass was to try to isolate the existing eventlet references into one place and wrap them inside an interface.  I'll see if I can find the code and if it's in reasonable shape I'll push a PR so it can be discussed.  I think the two main issues are (1) how to determine whether to use eventlet (obviously would need to be the default) or gevent, and (2) the testing framework is highly eventlet-focused and a lot of work would be needed to test the gevent implementation.  Probably more work than just implementing the change.\nI'll see if I can find the code and if it's in reasonable shape I'll push a PR so it can be discussed further.. Created PR https://github.com/nameko/nameko/pull/462 for this issue.. @davidszotten will do - thanks!\n. @davidszotten - should I add an entry to the CHANGESfile with no associated version, and assume the version will be added when it is released?  Thanks. Thanks Matt, will do.  I'll remove the codeintel and sublime too?\nAny idea why the build fails?\n. Updated with .gitignore changes.  Any other comments?. Thanks Matt!. Thanks Matt!  I appreciate the quick turnaround.  I realized that this also lets me send events from my gevent-based code, which will be handy.. @s-maj Thanks for all of the effort!  The original investigation was triggered by RabbitMQ holding open sockets that had been closed (maybe improperly) by the client, so when a publisher re-used the socket (because it was available on the client side) it hung for 15+ minutes since the server thought the socket was already open.  I haven't (yet) been able to capture a trace of the client socket closure that leaves the server socket open, but I'll test with 2.11 when it's available and see if the TCP keepalives prevent the hung connections.\nThanks again!. TCP keepalive on the server doesn't seem to help, even with the default 2 hour timeout I have found connections that have been stuck open for weeks, according to the connected_at time reported by rabbitmqctl list_connections.\nSince the client side is originating the TCP connection, I wasn't sure if the server-side TCP keepalive was invoked.  Client-side TCP keepalive might not help if the socket is being torn down improperly without completing the AMQP disconnect procedure that RabbitMQ is looking for.\nI added AMQP heartbeats to the Publisher and ClusterRPCProxy connections and that seemed to help, but it also causes the connections to be torn down quickly since it is not responding to the heartbeat from the server, so it prevents reusing connections from the kombu pool.. ",
    "bright-pan": "Support for gevent!!! . ",
    "raybotha": "@bobh66 Did you ever get to pushing a PR?\nGevent compatibility would be great, especially when you need to use any kind of grpc service. gRPC has some support for gevent now but not eventlet, and it's close to impossible to get something like Google Cloud's client libraries working with eventlet/nameko as @mattbennett helped me learn a few weeks ago (https://groups.google.com/forum/#!topic/nameko-dev/z5ndticLNKI).. Cool, done: https://github.com/nameko/nameko/pull/523. Versions:\nnameko==2.8.4\nnameko-sqlalchemy==1.1.0. I have this in services that don't use mongo, pandas or numpy but a lot of my services consistently build up memory use until they get restarted. Rough figures, at around 1 request per second taking ~50ms it can get up to 3GB after two or three days.\nI'm getting this in some services that don't use sqlalchemy, here's a pip freeze for one such service that's acting as an API gateway and not doing much of anything except calling other services:\namqp==1.4.9\nanyjson==0.3.3\nattrs==17.4.0\ncertifi==2018.1.18\nchardet==3.0.4\nddtrace==0.11.1\nenum-compat==0.0.2\neventlet==0.21.0\ngreenlet==0.4.13\nidna==2.6\nkombu==3.0.37\nmock==2.0.0\nmsgpack-python==0.5.6\nnameko==2.8.5\npath.py==11.0.1\npbr==4.0.2\nphonenumbers==8.9.3\npycountry==18.2.23\nPyYAML==3.12\nrequests==2.18.4\nsimplejson==3.13.2\nsix==1.11.0\ntoastedmarshmallow==0.2.6\nurllib3==1.22\nWerkzeug==0.14.1\nwrapt==1.10.11. ",
    "epetrovich": "How to solve the dead TCP connections problem then? \nRabbitmq documentation says to use heartbeats for all connections. We see a lot of dead connections when producers connect/disconnect.\nhttps://www.rabbitmq.com/heartbeats.html\n\nHeartbeats can be disabled by setting the timeout interval to 0. This is not a recommended practice.\n\nWe see timeout=0 on the producer connections with no option to change this.. @mattbennett \nOur producers are running on AWS spots for batch scheduled jobs. \nWe can launch and shut down about 200-500 spot instances  per day. \nOne working day adds about 2-3k dead connections from producers. We are working on graceful shutdown of rabbitmq producers also.  \nWe do not want to hit the connection limits on the rabbitmq cluster obviously.\nI believe that hertbeat option should be somewhere here\nhttps://github.com/nameko/nameko/blob/master/nameko/amqp/publish.py#L29\nKombu.Connection allows to set it\n\nclass kombu.Connection(hostname=u'localhost', userid=None, password=None, virtual_host=None, port=None, insist=False, ssl=False, transport=None, connect_timeout=5, transport_options=None, login_method=None, uri_prefix=None, heartbeat=0, failover_strategy=u'round-robin', alternates=None, **kwargs)[source]\n. \n",
    "alexeysofin": "@mattbennett Is there some branch to see the progress? By the way what do you think about allowing to override get_producer here https://github.com/nameko/nameko/blob/master/nameko/amqp/publish.py#L175, which will allow to set some heartbeat to producer connection and thus will help rabbitmq close idle producers connections which are not closed gracefully (as far as i understand if pool sees a broken connection it will release it and create a new one)?. @mattbennett we found our problem regarding too many dead connections. We just did not shutdown supervisor gracefully when terminating spot instances. But still it would be a nice feature to have producer heartbeat separate from consumer heartbeat (which might be None by default keeping backward compatibility). I personally do not like the workaround with inheriting from PublisherCore because I will have to copy-paste the whole PublisherCore.publish method, because there is no way to override get_producer inside it. Producer heartbeat although has no application (until there is an implementation supporting producer heartbeats) except for RabbitMQ will have a chance to clean up dead connections after a timeout, so maybe adding a support to supply connection kwargs though Producer class is not bad. What do you think?. @mattbennett I managed to reproduce the issue, but the problem seems to have bad side-effects. What I did was dispatching a lot of events, and then dd from /dev/zero to a file until there is no space left on device (we are running nameko under supervisor). So when I am printing the tracebacks of all greenlets, \nimport gc\nimport traceback\nfrom greenlet import greenlet\nfor ob in gc.get_objects():\n    if not isinstance(ob, greenlet):\n        continue\n    if not ob:\n        continue\n    print(''.join(traceback.format_stack(ob.gr_frame)))\nthey are all stuck on self.greenlet.switch()\nHere is an example:\nFile \"/home/dev1-irm/.venv/lib/python3.5/site-packages/eventlet/greenthread.py\", line 218, in main\n    result = function(*args, **kwargs)\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/nameko/containers.py\", line 393, in _run_worker\n    result = method(*worker_ctx.args, **worker_ctx.kwargs)\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/ingest/base/retry.py\", line 72, in wrapper\n    return wrapped(*args, **kwargs)\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/ingest/base/retry.py\", line 72, in wrapper\n    return wrapped(*args, **kwargs)\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/ingest/release_manager/main/service.py\", line 198, in handle_delivery_arrived\n    delivery_handler.parse, release_uri)\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/eventlet/tpool.py\", line 117, in execute\n    rv = e.wait()\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/eventlet/event.py\", line 121, in wait\n    return hubs.get_hub().switch()\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/eventlet/hubs/hub.py\", line 295, in switch\n    return self.greenlet.switch()\nAnother interesting thing I noticed is that inside a tpool there is a context switch which is caused by datetime.strptime function \nd = datetime.strptime(duration, frm)\n  File \"/usr/lib/python3.5/_strptime.py\", line 510, in _strptime_datetime\n    tt, fraction = _strptime(data_string, format)\n  File \"/usr/lib/python3.5/_strptime.py\", line 313, in _strptime\n    with _cache_lock:\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/eventlet/semaphore.py\", line 129, in __enter__\n    self.acquire()\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/eventlet/semaphore.py\", line 115, in acquire\n    hubs.get_hub().switch()\n  File \"/home/dev1-irm/.venv/lib/python3.5/site-packages/eventlet/hubs/hub.py\", line 295, in switch\n    return self.greenlet.switch()\nBut I can not understand what is actually causing a hub to stop processing greenlets. At this time there is no \"cannot switch to a different thread\" log message.\nMy question is if context switch inside a tpool must always trigger that error? If yes, why sometimes greenlets do not block?\nAnd another question is how do I find out the actual cause why greenlets are stuck?. @mattbennett sorry for a little chaotic reply :) I chose to consume all disk space because about at the  time we had hanging consumers we had \"no space left on device\" errors and I thought that this might be causing the problem. \nGegarding tpool, I actually wrapped python code that seemed to have no yields but when I inspected greenlets with the help of backdoor, I noticed that strptime function uses some semaphore which is patched by eventlet and this is causing a yield. \nI agree that full disk and hanging consumers are actually separate problems and the real cause was the implicit yield from strptime. So I am going to remove tpool (and use it more wisely next time) from that part of code and give it a try.\nShall i close this issue for now and maybe reopen if the problem occurs again?. ",
    "jacohend": "I need to second this...I've found rabbitmq to be very unreliable at scale, especially when paired with modern cluster orchestration (swarm, kubernetes). Would much prefer a redis broker. . ",
    "akotulu": "Restarting the whole universe to add a new planet seems logical.\nInstalled packages in Docker debian:stretch container\napt-get install -y python3 python3-pip erlang rabbitmq-server\npip3 install pyodbc apscheduler nameko\nConfig:\n```\nAMQP_URI: 'pyamqp://guest:guest@localhost'\nWEB_SERVER_ADDRESS: '0.0.0.0:8000'\nrpc_exchange: 'nameko-rpc'\nmax_workers: 10\nparent_calls_tracked: 10\nLOGGING:\n  version: 1\n  handlers:\n    console:\n      class: logging.StreamHandler\n      formatter: simple\n    file:\n      class: logging.handlers.TimedRotatingFileHandler\n      level: DEBUG\n      formatter: simple\n      when: W0\n      backupCount: 4\n      filename: /var/log/reports.log\nloggers:\n    nameko:\n      level: DEBUG\n      handlers: [console]\n    amqp:\n      level: DEBUG\n      handlers: [console]\nformatters:\n    simple:\n      format: '%(asctime)s > %(thread)d - %(levelname)s - %(message)s'\n```\nWell you should have a look around in Eventlet code, I sed what it does.. Provided example project as a comment in your example I wanted to do, but was forced to use another approach.. Well if you would be so kind and point me where this so called 'monkey patching' magic happens in the runner? Your runner starts first service and waits when it receives rpc call and repeats for next service, till all of them are 'loaded.'. ",
    "notpeter": "I just crashed into this because I didn't have a transitive requirement pinning for eventlet. Naturally an hour ago eventlet 0.22.0 was released which broke tests for my http services.\nIs it worth pushing a hotfix for nameko which explicitly boxes eventlet in setup.py so that instead of the current eventlet>=0.16.1 it becomes eventlet>=0.16.1,<0.21.0?. ",
    "peterclearmetal": "Thanks @mattbennett for releasing nameko 2.8.3 as a hotfix. Much appreciated.. For future reference and googlability the error I got running pytest with Nameko <= 2.8.1 installed is:\nargparse.ArgumentError: argument --log-level: conflicting option string(s): --log-level\nAnd the related pytest issue is eisensheng/pytest-catchlog#36.. ",
    "SaikrishnaViridis": "@davidszotten Thanks for responding to my question. Sorry its my bad I did not mention my configuration details properly earlier, Now i have updated my question details.\nYes I'm able ping linux server from Windows ( i.e nameko box to rabbitmq box ). ",
    "eugeneRover": "I use Python 2.7.9 and nameko 2.8.0.\nHere is reproducing code (reproduce.py) :\n```\n!/usr/bin/python\nIf I remove below line, UnicodeDecodeError goes away\nfrom future import absolute_import, unicode_literals\nimport logging \nlogging.basicConfig()\ntry:\n    # I've got following utf-8 encoded string in exception from Postgres driver.\n    # Actionally, it is cyrillic characters \n    ba = bytearray([208,158,208,168,208,152,208,145,208,154,208,144])\n    raise Exception(ba)\nexcept Exception as ex:\n    logging.getLogger().error ('error %s', ex, exc_info=True)\n```. ",
    "kiorq": "Moving this to mailing list... sorrry.. ",
    "askorski-fc": "I have the exact same issue:\nTraceback (most recent call last):\n  File \"/home/dummyuser/envs/dummyenv/bin/pytest\", line 11, in <module>\n    sys.exit(main())\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/config.py\", line 50, in main\n    config = _prepareconfig(args, plugins)\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/config.py\", line 160, in _prepareconfig\n    pluginmanager=pluginmanager, args=args)\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/pluggy/__init__.py\", line 617, in __call__\n    return self._hookexec(self, self._nonwrappers + self._wrappers, kwargs)\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/pluggy/__init__.py\", line 222, in _hookexec\n    return self._inner_hookexec(hook, methods, kwargs)\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/pluggy/__init__.py\", line 216, in <lambda>\n    firstresult=hook.spec_opts.get('firstresult'),\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/pluggy/callers.py\", line 196, in _multicall\n    gen.send(outcome)\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/helpconfig.py\", line 68, in pytest_cmdline_parse\n    config = outcome.get_result()\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/pluggy/callers.py\", line 77, in get_result\n    _reraise(*ex)  # noqa\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/pluggy/callers.py\", line 180, in _multicall\n    res = hook_impl.function(*args)\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/config.py\", line 943, in pytest_cmdline_parse\n    self.parse(args)\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/config.py\", line 1108, in parse\n    self._preparse(args, addopts=addopts)\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/config.py\", line 1073, in _preparse\n    self.known_args_namespace = ns = self._parser.parse_known_args(args, namespace=self.option.copy())\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/config.py\", line 557, in parse_known_args\n    return self.parse_known_and_unknown_args(args, namespace=namespace)[0]\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/config.py\", line 563, in parse_known_and_unknown_args\n    optparser = self._getparser()\n  File \"/home/dummyuser/envs/dummyenv/local/lib/python2.7/site-packages/_pytest/config.py\", line 542, in _getparser\n    arggroup.add_argument(*n, **a)\n  File \"/usr/lib/python2.7/argparse.py\", line 1308, in add_argument\n    return self._add_action(action)\n  File \"/usr/lib/python2.7/argparse.py\", line 1509, in _add_action\n    action = super(_ArgumentGroup, self)._add_action(action)\n  File \"/usr/lib/python2.7/argparse.py\", line 1322, in _add_action\n    self._check_conflict(action)\n  File \"/usr/lib/python2.7/argparse.py\", line 1460, in _check_conflict\n    conflict_handler(action, confl_optionals)\n  File \"/usr/lib/python2.7/argparse.py\", line 1467, in _handle_conflict_error\n    raise ArgumentError(action, message % conflict_string)\nargparse.ArgumentError: argument --log-level: conflicting option string(s): --log-level\n\nPython 2.7.12\npytest 3.3.1\nnameko 2.8.1\n\nDowngrading pytest to a version lower than 3.3 indeed works.. ",
    "sammy200592": "Hi Matt,\nI tried the method you suggested:\nAMQP_URI: amqp://guest:guest@localhost:562/;amqp://guest:guest@localhost:5672/\nThis works fine when my first port address is set to the correct port i.e 5672 but it gives me an error when the first port is wrong and the second port is right just like in the case above.\nI am trying this for the Nameko service and getting the following error:\nOSError: [Errno 61] ECONNREFUSED . i want that if the first port is not present then nameko should connect with the second port provided so that the service does not go down.. Awesome Matt.\nThis is what I also understood by my research. \nThanks a lot for clarifying the doubts though.. ",
    "jackytu256": "Thanks matt!!!. ",
    "WoLfulus": "Awesome! \nBut how do I manually register an entrypoint? Just setting the nameko_entrypoints makes nameko detect it? As I was reading the code, I do need to wrap/extend the service with several base classes right?\nI tried extending the service class with another class (that has the @rpc documentation method) and it doesn't work because nameko tries to register both services. \nStill haven't tried but I'm thinking about creating a decorator for the service class itself. I don't know if there's a better/proper way to do this\nAlso, thanks for the really quick response <3. Nice! I'll give it a try!. ",
    "TimGraf": "We found our issue.. We have a clustered RabbitMQ in our development environment hosted in AWS.  We originally had our RabbitMQ cluster behind an ELB which caused problems with RabbitMQ as it turns out.   \nWhen we developed locally using Docker containers we didn\u2019t see any issues.  As part of investigating our issues we had hoped to point our local services to our development environment RabbitMQ cluster and isolate our local services\u2019 RPC calls by using a different exchange name.  \nWe are using the Django REST Framework for our REST API as gateway services to our internal Nameko RPC/PubSub services.\nOur issue turned out to be the configuration on the DRF side which was missing the exchange name setting in the config so it was defaulting to \u201cnameko-rpc\u201d.   \nIn any event we resolved the ELB and clustered RabbitMQ issue as well as adding the exchange name to the config on the DRF side so we probably will not need to change the exchange name again.  . @mattbennett sorry, I forgot to mention you in my reply above.  . ",
    "codebase02": "Does nameko only support RPC over *MQ, not raw RPC?  If I have an existing RPC server, and I want to use nameko just for the RPC client, is that possible?\nthanks.. ",
    "worldmind": "gRPC sounds interesting too. ",
    "gpkc": "@kooba How is support for gRPC going?. Do you think it would be appropriate to move some of the functions found under the cli module, such as run or setup_backdoor, to the utils package for instance? Since they are not really related to the CLI itself, but instead being used by it.. I think maybe nameko.run could be easily confused with the nameko.runner module. And I just noticed there is a context manager named run_services inside the nameko.runner module. Is there a reason this is not used on nameko.cli.run.run()? Maybe nameko.cli.run.run() could be adapted to use nameko.runner.run_services and also be incorporated in that module as a more complete solution (or even turnkey), as it integrates with eventlet while nameko.runner.run_services alone don't.. I'll try to make those changes and create a PR.. ",
    "authentik8": "Hi Matt,\nThanks for the speedy reply - that's much more elegant than what I'd bodged together in the meantime - thanks!. ",
    "rizplate": "@mattbennett Thanks, your answers and resources are very helpful. I will review them more.. @mattbennett Thanks for the information.. ",
    "pimiela": "Looks good to me :+1:. ",
    "alleriaken": "Thanks @mattbennett alot\nI found a way to do that is write my own run command for nameko instead of using nameko run so that I can load my setting file and use it freely. ",
    "MMmaomao": "I got it\non windows, you must cd to the service py folder.\nthen name run service_1. it''s fine now. Thanks.. thanks. Hi, @kooba \nThank you very much. ",
    "TaceyWong": "Thanks for reply !\n\n\nnameko-2.8.5\n\n\neventlet-0.21.0\n\n\ngreenlet-0.4.13\n\n\n(on ubuntu 17.10). modify setup.py:install_requires=[\"eventlet==0.20.0\"].It works !\nThanks,it's time to turn to Py3 :smile: . ",
    "bludau-peter": "Hi all, \nI have a similar issue where the RAM usage is about 2GB after 1 long running but finished http request (no other requests made to the service since start) and same results as @raybotha using pympler.  My dependencies are 'pymongo',  'nameko==2.9.0', 'pandas', 'numpy'. Does anyone have any idea why this may happen?. ",
    "t1m0t": "I've read the answer in issue #516. Team is thinking about impletementing gRPC but it is in a very early process.. ",
    "huyidao625": "Thanks!. ",
    "santiycr": "nameko/cli/main.py:97:80: E501 line too long (80 > 79 characters) \ud83e\udd26\u200d\u2642\ufe0f . @davidszotten that's what I was expecting, but the way things are implemented, neither single. or double quotes work. oh, that's a nice hack, to double quote the var at the env export side. I do agree there's a bit of a bug on how the env_var parser behaves, but I thought I shouldn't change a behavior as heavily used as the default var parsing in configs as the ramifications for current users could be big. Maybe something to fix for a backwards incompatible release?. @mattbennett anything else needed to approve this? Let me know happy to help out further. Oh, I like your temp solution. My current one while I waited for a release was a lot uglier:\nBAR: !env_var \"IGNORE_${FLOAT}\"\nThanks for sharing a better approach in the meantime!. ",
    "muzzi92": "Hi David, thanks for the quick reply.\nSo I removed the unittest mock import first, which resulted in an 'undefined error for Mock'. I then imported Mock from mock and I get the same error as I did in the first place, which reads something like:\nassert False\n + where False = isinstance(<foobar.FooBar object at 1234>, Mock)\n+ where <foobar.FooBar object at 1234> = <first_service.FirstService object at 5678>.mailer\n. This was really helpful thank you guys, it was a silly mistake on my behalf! I had imported DependencyProvider but forgotten to assign it to the class.\nAll green now.\nThanks David,\nThank you Matt!. ",
    "qqqays": "thanks, I haven't used pdb.\nI will try pdb. I resolve it\nimport requests\n.....\nres = requests.get(url='http://localhost:15672/api/queues', auth=('guest', 'guest'))\nfor gaining the queues of rabbitmq information. ",
    "s-maj": "Instead of using \"tricky\" AMQP heartbeats there is an easier solution: TCP keepalives. This feature was initially introduced in py-amqp 2.0 and tuned-up in py-amqp 2.1.4 (it is not supported by librabbitmq but so the AMQP heartbeats). Sadly, this is blocked by kombu<4 dependency.. Ok, this would be extremely useful for deployments with clustered  RabbitMQ. In case of abrupt stop (hardware failure, network failure, etc) one of nodes in RabbitMQ cluster, producer is currently unable to detect this and reconnect to the healthy node. This also should unblock folks from https://github.com/nameko/nameko/issues/378.\nI did a quick PoC according to your suggestions (https://github.com/nameko/nameko/pull/564) but some tests are failing. Guidance appreciated :). Ok, cool! This should help as long you have direct communication (no NAT, not load balancer). You could also configure aggressive TCP keepalive on the RabbitMQ server(s) - default value timeout value is over 2h.. I've fixed basic stuff but it looks like writing to the the closed connections is somewhat broken. See tests:\n test/test_messaging.py:852 TestPublisherDisconnections.test_down[True]\n test/test_messaging.py:852 TestPublisherDisconnections.test_down[False]\n test/test_messaging.py:870 TestPublisherDisconnections.test_timeout[True]\n test/test_messaging.py:870 TestPublisherDisconnections.test_timeout[False]\nIt seems that exceptions is never raised and hanging thread is just killed after timeout. \nRelated to https://github.com/celery/kombu/pull/769.. I've added transport.options since some tests were timeouting without specic error (similar behaviour to https://github.com/celery/celery/issues/4296 and other related tickets), where the initial problem with tests was related to dropping  (by accident) confirm_publish from transport.options in get_producer. \nIf I understand this correctly, connections with kombu later then 4.0.2 will hang indefinitely without transport options.  . I've changed this slightly to keep constants immutable and provide reasonable defaults for low level methods. Let me if this is ok (tests yet to be fixed).. Ok, with newer Kombu this exception looks like this: https://gist.github.com/s-maj/3cf85e49ae1aae69f1054243aa6a2c67. I'm not sure how I can preserve UnknownService here.. Hmm, I can't see it any references to use_confirms in https://github.com/nameko/nameko/blob/master/nameko/messaging.py#L73 except https://github.com/nameko/nameko/blob/master/nameko/messaging.py#L132. \nAll use_confirms usage seems to be in https://github.com/nameko/nameko/blob/master/nameko/amqp/publish.py#L35 and  I already added transport options https://github.com/nameko/nameko/pull/564/commits/413006b183fdef0b0098771ee5a68428ee2285a6#diff-da23ae33a4d5c536cd0bfb9ca895717aR120.. ",
    "cblegare": "@kooba @mattbennett \nAgreed!\nI'll try to make something working quickly so I can legitimately take that name on pypi.\nHere is the repository: https://gitlab.com/cblegare/nameko-kafka.  Whether or not you close this issue, I'll keep you posted.\nEDIT changed the project name for a better fit with existing Nameko-related libraries. Hello again!\nSince we talked about a \n\nsuitably well-built library \n\nI feel the need to expose here what I intent to do with this.\nI have something that may not be broken yet: a simple consumer dependency provider.  I did not bother make it Python2-compatible, but tests are set to run for 3.4 -> 3.7 for now and their code coverage reports should combine if necessary (there is no conditional code yet regarding python version).\nMost of boring boilerplate is done.\n\n[x] Tests, coverage reports, linting, basic static analysis\n[x] Hosted docs (no content yet), including PDF version and coverage report (see it here! https://cblegare.gitlab.io/nameko-kafka/)\n\nHere is a roadmap and/or planned lifecycle\n\nThe first 0.1 release should provide an entrypoint that looks like a SERVICE_POOL event handler, including end-to-end tests using kafka as a gitlab-ci service\nI would like to make kafka a quasi drop-in replacement for AMQP for 1.0 but that seems to be a lot of work.  This would imply the removal of the above-mentionned entrypoint.\nCommits to master after the first release will all be deployed to PyPI as pre-releases. Releases will be tagged.\nDon't expect 1.0 in 2018. Swarms of bytes should hit nameko-kafka in our production envs before 1.0 though.\n\nSince I am not very knowledgeable of Nameko (yet) I would appreciate reviews but hey, we're all pretty busy.\nAnd if my understanding of Nameko is right, that code in that discourse post above might be prone to bugs, since kafka.KafkaConsumer is not thread safe and shared across threads.\nWill post back on first release. Hi everyone,\nSad news, our teams decided to go without Kafka and I can't spare enough my nights and weekends on this feature anymore.\nGiven the fact that people at Kombu are working on the Kafka support (see celery/kombu#301), implementing this feature should be easier in a near future.. ",
    "wyattsuen": "Thanks very much for the reply and this very beatiful framework. I just use nameko in a week, and switch my most work architechture to it. I know I have to solve this problem with kombu. As I see the kombu document for these days, I find it difficult for me to understand the process without the rabbitmq experience. Can you help me to provide the example how to get event_handler outside nameko using kombu.\n```\nclass ServiceA:\n    \"\"\" Event dispatching service. \"\"\"\n    name = \"service_a\"\ndispatch = EventDispatcher()\n\n@rpc\ndef dispatching_method(self, payload):\n    self.dispatch(\"event_type\", payload)\n\n```\nHow to accept this event with SERVICE_POOL  and BROADCAST method in kombu without nameko. I think the example will be very helpful to add to the nameko document. Or it is best to provide a standalone event_handler just like the standalone rpc service.\nAlthough I have not met the problem, but I think it useful to send the event outside nameko, and accept the event in nameko is also welcome.  Guessing tornado accept a message from user, and I want to send this message to many potential services, maybe broadcast the event is useful.\n```\nclass ServiceB:\n    \"\"\" Event listening service. \"\"\"\n    name = \"service_b\"\n@event_handler(\"service_a\", \"event_type\")\ndef handle_event(self, payload):\n    print(\"service b received:\", payload)\n\n```\nHow to send the event with SERVICE_POOL  and BROADCAST method in kombu without nameko. The examples are very important for us without rabbitmq experiences. So kindly and patiently providing the examples is welcome. Thanks a lot.. ",
    "vlcinsky": "@mattbennett  Why is this accepted PR not seen in 3.0.0rc branch? eager is not present there.. ",
    "derekjanni": "Here's a good test:\nHere's our helloworld.py greeting service, slightly modified so it takes between 1-2 minutes to complete\n```python\nfrom nameko.rpc import rpc\nimport numpy as np\nimport time\nclass GreetingService:\n    name = \"greeting_service\"\n@rpc\ndef hello(self):\n    start = time.time()\n    time.sleep(np.random.randint(60,120))\n    duration = time.time() - start\n    return duration\n\nWe run the service with:\nnameko run helloworld\nAnd in a separate shell, lets invoke a call:python\nfrom nameko.standalone.rpc import ClusterRpcProxy\nimport time\nconfig = {\n    'AMQP_URI': 'amqp://guest:guest@localhost:5672'  # out of the box Rabbit config. Change as you see fit\n}\nwith ClusterRpcProxy(config) as cluster_rpc:\n    print(cluster_rpc.greeting_service.hello())\n``\nOnce that call is invoked, go to your [MQ management console](http://localhost:15672/#/queues/%2F/rpc-greeting_service) and look at it for a sec. Notice that there's a message that gets placed in the queue, but it will not disappear until your function completes while the service is running. Wait 30s after firing the rpc call above and then shut down your greeting service that was running with Ctrl+Z. You'll notice after leaving your calling application for 5 minutes that there's no response, even though the message should have been picked up and should have been processed. But since the service died, the consumer ack never happened - the task never reported back to MQ that it was complete and thus, it never really did complete from the client's perspective. I'm interested to try this with something more concrete like a file write or a db operation, but I don't have time for that right now. Might also be interesting to try withcall_async` and see what happens.\nTL;DR - no guarantee that your task will complete if your service crashes. If you gently shut it down, I think the default behavior is to wait for the current active tasks to finish before shutting down, so you'll get a result from the call above, but if your service crashes you should expect the tasks running on it to crash as well. ",
    "shirishc": "\nThanks @shirishc! The old list has actually been completely imported into Discourse, so I think you can drop the old link.\n\nDone and submitted !!!. ",
    "daimon99": "how about comment this code in testing/pytest.py?\ndef pytest_load_initial_conftests():\n    # make sure we monkey_patch before local conftests\n    # import eventlet\n    # eventlet.monkey_patch()\n    pass. ",
    "lorenzoferrante": "I have version 0.24.1 both on Windows and Ubuntu on WSL.. Ok perfect. I\u2019ll try what you suggested. Thank you. . ",
    "amnonkhen": "a workaround for this issue is to force nameko to parse the environment variable regular expression using re package rather than regex by editing main.py. Thanks. \nI uninstalled regex and it did the trick.\n\nOn 19 Feb 2019, at 22:10, Jakub Borys notifications@github.com wrote:\nJust tried hello world example from https://docs.nameko.io/en/stable/#nameko and had no issue. I'm on macOs though. Use of regex is optional and that dependency is not installed with Nameko by default and re should be used.\nUnfortunately none of the developers working on Nameko use Windows OS so there might be some incompatibility.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. The stacktrace confirms that the compilation of the regexp pattern is what causes the RuntimeError to be throw.\nI tried it with regex 2018 and latest 2019. Both failed.. \n",
    "nuthanmunaiah": "Thank you for your response. I will reproduce this issue using eventlet.green.subprocess and open an GitHub Issue in the eventlet project.. ",
    "marbru": "And/or maybe add an extra code block with a curl command exemplifying the request-response.\n. I think it would help if you could extend this example with another block of code showing a standalone rpc proxy calling the rpc method, and then you can add comments pinpointing \"this is a dependency\", \"this is an entrypoint\", \"here we are instatiating a worker\". I think it helps to see each of the concepts \"in action\".\n. It maybe appropriate to mention at some point in this document something like \"Nameko includes a bunch of built-in entrypoints and dependencies\" (and then a link to the built-in extensions) \"or you can make your own\" (and the the link to writing dependencies). \nThis is because I found a bit confusing that the section describing the built-in stuff is calls it \"extensions\", when that term isn't mentioned anywhere in this terminology doc. Including the links from here would help make the connection IMO.\n. I'm surprised this OrmSession is not listed on the built-in extensions section.\n. You can also provide a list of entrypoint names, should you want to keep several, is this right? Might be worth mentioning/showing an example in this docstring.\n. I found it a bit confusing that in every example in the integration test section, we are obtaining the container in a different manner (get_container here, container_factory on restrict_entrypoints, ServiceContainer on replace_injections). \nAlso, as a matter of fact, nowhere else in the docs is mentioned what a container is, which adds to the confusion.\n. Don't really get why these helpers are in a separate section, when they also only apply to integration testing.\n. Should this not be 'handle_event' instead of 'hello'? Or else I don't get it.\n. ",
    "signalpillar": "I've tried this key and the one that is mentioned in the RabbitMQ documentation and used by kombu (expiration) and it doesn't work.\nExpected that when the expired message reaches the head of the queue it shouldn't be delivered but it is.. Ok, I've got it working. Instead of setting it to the headers (app specific) we need to set the property on the message when we publish it.\n```\nclass _TimeoutAwareMethodProxy(nameko.rpc.MethodProxy):\n    \"\"\"Timeout aware RPC method proxy.\"\"\"\ndef __init__(\n        self, *, timeout_secs: float, **kwargs,\n) -> None:\n    super().__init__(**kwargs)\n    self.publisher.expiration = timeout_secs\n\n```. "
}
{
    "shykes": "Thanks!\n. Thanks!\n. ",
    "jpetazzo": "There is no support for sticky session at this point. When a WebSocket connection has been established, it remains on the same application instance (since it is a TCP connection).\nHowever, if you want further WebSocket connections from the same client to be handled by the same application instance, then you need sticky sessions, indeed.\nIt would be possible to use a mechanism similar to the one of HAProxy. I.e., when a given cookie is present, it would be used to map the client to a given back-end, and when the cookie is not present, a back-end is selected, and the cookie is set. However, this is currently not on our road map.\n. If #10 (bind to a specific IP address) is implemented, it would also allow to handle multiple SSL certificates\u2014by running multiple copies of Hipache.\n. Marking as duplicate of #20 (sorry, I should have marked #20 as a duplicate of your request, but I hadn't seen yours first!)\n. Not planned for now; but tagging as enhancement since this would be indeed an interesting feature.\n. Dropping privileges would be nice, but in the short term, setcap is totally acceptable.\n. This is a good idea. It would be an interesting enhancement, and it is easy to implement.\nWe have no plan to work on that in the short term (since our production Hipache cluster is on EC2, and EC2 supports only 1 IP address per machine), but this would be a low hanging fruit if someone wants to contribute some code.\n. This is not implemented, but if node-http-proxy supports such middlewares, it is probably easy to add.\n. Thanks for reporting this. The attached pull request fixes the issue.\n. This seems to be a problem with your local nodejs or npm installation.\nOn our production servers (running Ubuntu), we use the following PPA to install the nodejs and npm packages:\nhttps://launchpad.net/~chris-lea/+archive/node.js/\nI recommend trying again with that, and re-opening the issue if it still happens!\nThank you.\n. This is very nice! If I understand correctly, it talks with the Redis server, but doesn't interact with Hipache directly, right?\n. Unfortunately, this is probably an issue related to the cluster module.\nHere are a few suggestions:\n- try to run on a different port;\n- make sure that nothing else is listening on port 80 (check with netstat -ntlp);\n- run hipache with strace (e.g. strace -o log -f hipache -c config_test.json) and put the log in a gist.\nCan you try that and let us know the results?\nThank you!\n. Thanks a lot for your contribution! However, we went through the easy route, e.g. just changing the configuration format instead. It lets the redis library pick the default host and port automatically if they are not specified in the configuration file. Thanks a lot anyway!\n. Hipache allows to dispatch requests according to the HTTP Host: header.\nExample:\n- your Tomcat servers are tomcat1.example.com:8080 and tomcat2.example.com:8080,\n- your Node.js servers are node1.example.com:3000 and node2.example.com:3000,\n- you run Hipache on a front-end, hipache.example.com.\nYou want to add two DNS entries, e.g. tomcat.example.com and node.example.com, pointing (as CNAME) to hipache.example.com.\nThen, using the configuration instructions, push those entries to Redis:\nredis-cli rpush frontend:tomcat.example.com tomcat\nredis-cli rpush frontend:tomcat.example.com http://tomcat1.example.com:8080\nredis-cli rpush frontend:tomcat.example.com http://tomcat2.example.com:8080\nredis-cli rpush frontend.node.example.com tomcat\nredis-cli rpush frontend:node.example.com http://node1.example.com:3000\nredis-cli rpush frontend:node.example.com http://node2.example.com:3000\nNote: as explained in #20, you cannot map e.g. www.example.com/tomcat and www.example.com/node to different places. Sorry about that!\n. Hi,\nThis would be an interesting improvement.\nHowever, it is harder than it seems.\nCurrently, when a request comes in, we look at the Host: header, and do a simple GET in Redis (well, actually, LRANGE and SMEMBERS queries).\nIf we want to route using URIs, things get more complicated: what do we want to lookup?\nIf a request for www.example.com/path/subpath/foo.js comes in, we would have to lookup (in that order):\n- www.example.com/path/subpath/foo.js\n- www.example.com/path/subpath\n- www.example.com/path\n- www.example.com\n  ... As well as wildcard entries.\n  Alternatively, we could add a totally different configuration format, e.g. each virtual host could store a hash, mapping paths to back-end servers.\nFor those reasons, we don't have plans to implement that in a near future, but I'm keeping this ticket around for reference if anyone asks for that feature later!\n. Apache has multiple levels of look-ups:\n- the Host: header is used to send to a specific <VirtualHost> section of the configuration;\n- if mod_rewrite is enabled, it can mangle the request in all possible ways, depending on URI, Host: header, and actually any other HTTP header;\n- contexts like <Directory>, <Location>, etc. are used;\n- last but not least, there are some special directives like Alias and custom handlers, that can alter the request.\nWhen proxying requests, there are at least two schemes:\n- mod_rewrite's RewriteRule with a [P] flag;\n- mod_proxy with its special directives like ProxyPass.\nThe \"easy trick\" is split in two parts:\n- first, add an extra layer of analysis, to check the URI (instead of only the Host: header);\n- then, load the complete configuration in RAM, so that lookups like prefix lookups are efficient.\nFor Hipache, it means two completely new code paths; that's why it's not an easy hack.\n. Hi @aionescu,\nWhen developing Hipache, we had (at least) three goals:\n- dynamic reconfiguration (i.e. add/remove virtual hosts and back-end servers without restarting or reloading the whole configuration),\n- configuration (and reconfiguration) through a well-defined interface (hence the choice of Redis),\n- support for WebSocket.\nIf you don't want WebSocket nor dynamic reconfiguration, you can use almost any other web server (including but not limited to Apache and Nginx). They have been around for muuuuch longer than Hipache, and run under virtually any kind of environment.\nIf you are looking for a WebSocket proxy, but don't need the dynamic reconfiguration, you could have a look at recent versions of Nginx (see http://nginx.com/news/nginx-websockets.html). That will work very well, and it should be supported on your platform, even if it is quite exotic.\nLast but not least, if you really want Hipache for some reason, it means that you need a fairly recent version of Node.js as well. I'w willing to make a bold assumption: if your platform runs a recent version of Node.js, it probably also runs Redis!\nNow, if you really-really-really want to run Hipache without Redis, you need to change lib/cache.js, and more specifically getBackendFromHostHeadeder (https://github.com/dotcloud/hipache/blob/440e33b2a31f56102d252909d7c92ad1a937a41a/lib/cache.js#L119).\nI'm marking this ticket as closed, unless you have further questions, of course.\nBest regards,\n. At some point during the design of Hipache, we considered adding an extra layer of indirection.\nAn HTTP request would have required the following look-ups:\n- HTTP Host Header \u2192 virtual host symbolic name (e.g. www.cooldomain.io \u2192 cooldomainweb)\n- virtual host symbolic name \u2192 set of backends (e.g. cooldomainweb \u2192 http://1.2.3.4:80/, http://1.2.3.5:80)\n- backend \u2192 state (e.g. http://1.2.3.4:80 \u2192 DOWN)\nBut we were worried about the extra latency: since each step requires the result of the previous step, they need to be performed sequentially. So we went for the current scheme, which can retrieve all the needed information in one pass:\n- HTTP Host Header \u2192 set of backends\n- HTTP Host Header \u2192 set of backend states\nHowever, since the latency with a local Redis is so low, it might very well be possible to implement CNAMEs as you suggest, without adverse effect.\nWe don't have plans to implement that right now, but if you want to implement it (and switch the behavior with a config flag, to remain backward compatible), it's something that we would consider merging.\n/cc @samalba \n. Weird! Can you try to capture request and response headers in both cases\n(fast and slow)?\nThe solution very probably lies within here.\n. What about using the Redis notification system?\nLook in cache.js, there is already a mechanism to PUB a message on a Redis channel when Hipache detects a dead backed. Would that work for you?\n. Yes!\nHipache will accept SSL connections if it is provided with SSL key and\ncertificate; and it will always query the backend over plain HTTP.\nSo one could say that it will perform SSL off-loading out of the box!\nOn Tue, Sep 17, 2013 at 4:56 AM, Ben Booth notifications@github.com wrote:\n\nIs it possible to perform SSL off-loading with Hipache?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/42\n.\n\n\n@jpetazzo https://twitter.com/jpetazzo\nLatest blog post: http://blog.docker.io/2013/09/docker-joyent-openvpn-bliss/\n. Hi,\nThanks for submitting this patch. Since it is optional and doesn't change the default behavior of Hipache, I don't see a reason to not merge it (but @samalba will have the last word, since he's the maintainer of Hipache).\nHowever, I think I would change a couple of things.\n1. Requiring os in cache.js is, IMHO, a dirty hack. It should be imported by the lookup function (using a closure to avoid re-importing it each time it's called).\n2. Overriding just the wildcard lookup seems a little weird.\nTo address the latter, I would suggest one of the following solutions.\n1. If hostKeyTemplate is specified, disable both default lookups to use the template-ized lookup instead.\n2. Always perform the default lookups, and add the template-ized lookup as a 3rd one (if it is defined).\n3. Since the wildcard subdomain lookup is already pretty custom, it could be transformed into a hostKeyTemplate.\nThe latter could be interesting because it would allow to easily switch between different wildcard behaviors without editing the code. I.e. currently, a.very.long.vhost.with.multiple.subdomains.com maps to .subdomains.com; some people want it to map to .very.long.vhost.with.multiple.subdomains.com.\nIf you like the last approach and want to implement it, maybe wait for @samalba's feedback first, though.\n. Hi Andre,\nI'll let @samalba get back to you; be patient however, since I know that\nhe's currently on vacation :-)\n. This is currently outside of the scope of Hipache.\nWe try to keep Hipache simple and lean, to make sure that it's easier to audit, debug, and for performance.\nOne solution would be to implement something in the foobar.com so that it redirects to foobar.com all requests sent with a different Host header.\nAnother solution would be to  send www.foobar.com to a different backend, which would perform an unconditional redirect. This special backend could be just a few lines of JS or Python or your_favorite_language, and it could run alongside Hipache.\nWe use a variant of that solution for healthchecks: Hipache itself doesn't have healthchecks (e.g. an URL that you can ping and it will reply \"OK\"), so we configure a custom domain in Hipache configuration that sends everything to localhost:12345; and on this address, we have a simple HTTP server that just replies 200 OK to everything.\n. We like to pronounce it \"hip-AH-chee\", so like \"apache\" I guess.\nBecause the project tagline is \"Hipache is Apache for Hipsters!\" :-)\n. Warning: if you use a non-local Redis, you will have some extra latency.\nLocal Redis has almost no latency (it's not measurable compared to the total request time), but using a remote Redis will add a couple of roundtrips.\nIf you are on a fast 10G LAN, the latency should be less than 1ms so you should be fine.\nIf you are on some public cloud on different AZ, you could see more than 10ms penalty at each request, tough.\n. That seems to be OK. I wonder if it would be easy to add relevant tests to the test suite.\n. Just create two frontends. If IP1 is 1.1.1.1 and IP2 is 2.2.2.2, that would be:\nredis-cli rpush frontend:www.server.com www1\nredis-cli rpush frontend:www.dotcloud.com http://1.1.1.1:80\nredis-cli rpush frontend:server.com www2\nredis-cli rpush frontend:www.dotcloud.com http://2.2.2.2:80\n. The lookup logic is at https://github.com/dotcloud/hipache/blob/master/lib/cache.js#L149.\nAs you can see, frontend:* won't be looked up, unless the HTTP Host: header is unqualified.\nHowever, you could force this lookup to happen by changing line 151, and removing + this.getDomainName(hostKey).\nBut what would be the point of this? Having a fallback for unmatched domains?\nRegarding your other question, backends are supposed to be plain HTTP. I don't know if HTTPS would work; @samalba might know!\n. Hi Miles,\nHTTPS is currently only supported on the frontend. This is the primary use-case for Hipache, as a load balancer: it is intended to be running on the same network as the backends, and the backends are exposing plain HTTP since the communication goes over an internal network.\nAnother way to look at it is to consider Hipache as a \"SSL unroller\".\nNow, regarding your question \u2014 yes, you can probably use Hipache for that. However, it looks like it might be simpler to use something else. What are you trying to achieve? Load balance HTTP connections across multiple outbound links? Do you need the proxy just to relay the requests, or also for its caching features?\n. Hi, do you still have this issue?\nYou might want to make sure that you have the latest version of the hipache image (i.e. by running docker pull stackbrew/hipache first).\nLet us know, then we'll reproduce to be sure!\nThank you.\n. There is no support for sticky session at this point. When a WebSocket connection has been established, it remains on the same application instance (since it is a TCP connection).\nHowever, if you want further WebSocket connections from the same client to be handled by the same application instance, then you need sticky sessions, indeed.\nIt would be possible to use a mechanism similar to the one of HAProxy. I.e., when a given cookie is present, it would be used to map the client to a given back-end, and when the cookie is not present, a back-end is selected, and the cookie is set. However, this is currently not on our road map.\n. If #10 (bind to a specific IP address) is implemented, it would also allow to handle multiple SSL certificates\u2014by running multiple copies of Hipache.\n. Marking as duplicate of #20 (sorry, I should have marked #20 as a duplicate of your request, but I hadn't seen yours first!)\n. Not planned for now; but tagging as enhancement since this would be indeed an interesting feature.\n. Dropping privileges would be nice, but in the short term, setcap is totally acceptable.\n. This is a good idea. It would be an interesting enhancement, and it is easy to implement.\nWe have no plan to work on that in the short term (since our production Hipache cluster is on EC2, and EC2 supports only 1 IP address per machine), but this would be a low hanging fruit if someone wants to contribute some code.\n. This is not implemented, but if node-http-proxy supports such middlewares, it is probably easy to add.\n. Thanks for reporting this. The attached pull request fixes the issue.\n. This seems to be a problem with your local nodejs or npm installation.\nOn our production servers (running Ubuntu), we use the following PPA to install the nodejs and npm packages:\nhttps://launchpad.net/~chris-lea/+archive/node.js/\nI recommend trying again with that, and re-opening the issue if it still happens!\nThank you.\n. This is very nice! If I understand correctly, it talks with the Redis server, but doesn't interact with Hipache directly, right?\n. Unfortunately, this is probably an issue related to the cluster module.\nHere are a few suggestions:\n- try to run on a different port;\n- make sure that nothing else is listening on port 80 (check with netstat -ntlp);\n- run hipache with strace (e.g. strace -o log -f hipache -c config_test.json) and put the log in a gist.\nCan you try that and let us know the results?\nThank you!\n. Thanks a lot for your contribution! However, we went through the easy route, e.g. just changing the configuration format instead. It lets the redis library pick the default host and port automatically if they are not specified in the configuration file. Thanks a lot anyway!\n. Hipache allows to dispatch requests according to the HTTP Host: header.\nExample:\n- your Tomcat servers are tomcat1.example.com:8080 and tomcat2.example.com:8080,\n- your Node.js servers are node1.example.com:3000 and node2.example.com:3000,\n- you run Hipache on a front-end, hipache.example.com.\nYou want to add two DNS entries, e.g. tomcat.example.com and node.example.com, pointing (as CNAME) to hipache.example.com.\nThen, using the configuration instructions, push those entries to Redis:\nredis-cli rpush frontend:tomcat.example.com tomcat\nredis-cli rpush frontend:tomcat.example.com http://tomcat1.example.com:8080\nredis-cli rpush frontend:tomcat.example.com http://tomcat2.example.com:8080\nredis-cli rpush frontend.node.example.com tomcat\nredis-cli rpush frontend:node.example.com http://node1.example.com:3000\nredis-cli rpush frontend:node.example.com http://node2.example.com:3000\nNote: as explained in #20, you cannot map e.g. www.example.com/tomcat and www.example.com/node to different places. Sorry about that!\n. Hi,\nThis would be an interesting improvement.\nHowever, it is harder than it seems.\nCurrently, when a request comes in, we look at the Host: header, and do a simple GET in Redis (well, actually, LRANGE and SMEMBERS queries).\nIf we want to route using URIs, things get more complicated: what do we want to lookup?\nIf a request for www.example.com/path/subpath/foo.js comes in, we would have to lookup (in that order):\n- www.example.com/path/subpath/foo.js\n- www.example.com/path/subpath\n- www.example.com/path\n- www.example.com\n  ... As well as wildcard entries.\n  Alternatively, we could add a totally different configuration format, e.g. each virtual host could store a hash, mapping paths to back-end servers.\nFor those reasons, we don't have plans to implement that in a near future, but I'm keeping this ticket around for reference if anyone asks for that feature later!\n. Apache has multiple levels of look-ups:\n- the Host: header is used to send to a specific <VirtualHost> section of the configuration;\n- if mod_rewrite is enabled, it can mangle the request in all possible ways, depending on URI, Host: header, and actually any other HTTP header;\n- contexts like <Directory>, <Location>, etc. are used;\n- last but not least, there are some special directives like Alias and custom handlers, that can alter the request.\nWhen proxying requests, there are at least two schemes:\n- mod_rewrite's RewriteRule with a [P] flag;\n- mod_proxy with its special directives like ProxyPass.\nThe \"easy trick\" is split in two parts:\n- first, add an extra layer of analysis, to check the URI (instead of only the Host: header);\n- then, load the complete configuration in RAM, so that lookups like prefix lookups are efficient.\nFor Hipache, it means two completely new code paths; that's why it's not an easy hack.\n. Hi @aionescu,\nWhen developing Hipache, we had (at least) three goals:\n- dynamic reconfiguration (i.e. add/remove virtual hosts and back-end servers without restarting or reloading the whole configuration),\n- configuration (and reconfiguration) through a well-defined interface (hence the choice of Redis),\n- support for WebSocket.\nIf you don't want WebSocket nor dynamic reconfiguration, you can use almost any other web server (including but not limited to Apache and Nginx). They have been around for muuuuch longer than Hipache, and run under virtually any kind of environment.\nIf you are looking for a WebSocket proxy, but don't need the dynamic reconfiguration, you could have a look at recent versions of Nginx (see http://nginx.com/news/nginx-websockets.html). That will work very well, and it should be supported on your platform, even if it is quite exotic.\nLast but not least, if you really want Hipache for some reason, it means that you need a fairly recent version of Node.js as well. I'w willing to make a bold assumption: if your platform runs a recent version of Node.js, it probably also runs Redis!\nNow, if you really-really-really want to run Hipache without Redis, you need to change lib/cache.js, and more specifically getBackendFromHostHeadeder (https://github.com/dotcloud/hipache/blob/440e33b2a31f56102d252909d7c92ad1a937a41a/lib/cache.js#L119).\nI'm marking this ticket as closed, unless you have further questions, of course.\nBest regards,\n. At some point during the design of Hipache, we considered adding an extra layer of indirection.\nAn HTTP request would have required the following look-ups:\n- HTTP Host Header \u2192 virtual host symbolic name (e.g. www.cooldomain.io \u2192 cooldomainweb)\n- virtual host symbolic name \u2192 set of backends (e.g. cooldomainweb \u2192 http://1.2.3.4:80/, http://1.2.3.5:80)\n- backend \u2192 state (e.g. http://1.2.3.4:80 \u2192 DOWN)\nBut we were worried about the extra latency: since each step requires the result of the previous step, they need to be performed sequentially. So we went for the current scheme, which can retrieve all the needed information in one pass:\n- HTTP Host Header \u2192 set of backends\n- HTTP Host Header \u2192 set of backend states\nHowever, since the latency with a local Redis is so low, it might very well be possible to implement CNAMEs as you suggest, without adverse effect.\nWe don't have plans to implement that right now, but if you want to implement it (and switch the behavior with a config flag, to remain backward compatible), it's something that we would consider merging.\n/cc @samalba \n. Weird! Can you try to capture request and response headers in both cases\n(fast and slow)?\nThe solution very probably lies within here.\n. What about using the Redis notification system?\nLook in cache.js, there is already a mechanism to PUB a message on a Redis channel when Hipache detects a dead backed. Would that work for you?\n. Yes!\nHipache will accept SSL connections if it is provided with SSL key and\ncertificate; and it will always query the backend over plain HTTP.\nSo one could say that it will perform SSL off-loading out of the box!\nOn Tue, Sep 17, 2013 at 4:56 AM, Ben Booth notifications@github.com wrote:\n\nIs it possible to perform SSL off-loading with Hipache?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/42\n.\n\n\n@jpetazzo https://twitter.com/jpetazzo\nLatest blog post: http://blog.docker.io/2013/09/docker-joyent-openvpn-bliss/\n. Hi,\nThanks for submitting this patch. Since it is optional and doesn't change the default behavior of Hipache, I don't see a reason to not merge it (but @samalba will have the last word, since he's the maintainer of Hipache).\nHowever, I think I would change a couple of things.\n1. Requiring os in cache.js is, IMHO, a dirty hack. It should be imported by the lookup function (using a closure to avoid re-importing it each time it's called).\n2. Overriding just the wildcard lookup seems a little weird.\nTo address the latter, I would suggest one of the following solutions.\n1. If hostKeyTemplate is specified, disable both default lookups to use the template-ized lookup instead.\n2. Always perform the default lookups, and add the template-ized lookup as a 3rd one (if it is defined).\n3. Since the wildcard subdomain lookup is already pretty custom, it could be transformed into a hostKeyTemplate.\nThe latter could be interesting because it would allow to easily switch between different wildcard behaviors without editing the code. I.e. currently, a.very.long.vhost.with.multiple.subdomains.com maps to .subdomains.com; some people want it to map to .very.long.vhost.with.multiple.subdomains.com.\nIf you like the last approach and want to implement it, maybe wait for @samalba's feedback first, though.\n. Hi Andre,\nI'll let @samalba get back to you; be patient however, since I know that\nhe's currently on vacation :-)\n. This is currently outside of the scope of Hipache.\nWe try to keep Hipache simple and lean, to make sure that it's easier to audit, debug, and for performance.\nOne solution would be to implement something in the foobar.com so that it redirects to foobar.com all requests sent with a different Host header.\nAnother solution would be to  send www.foobar.com to a different backend, which would perform an unconditional redirect. This special backend could be just a few lines of JS or Python or your_favorite_language, and it could run alongside Hipache.\nWe use a variant of that solution for healthchecks: Hipache itself doesn't have healthchecks (e.g. an URL that you can ping and it will reply \"OK\"), so we configure a custom domain in Hipache configuration that sends everything to localhost:12345; and on this address, we have a simple HTTP server that just replies 200 OK to everything.\n. We like to pronounce it \"hip-AH-chee\", so like \"apache\" I guess.\nBecause the project tagline is \"Hipache is Apache for Hipsters!\" :-)\n. Warning: if you use a non-local Redis, you will have some extra latency.\nLocal Redis has almost no latency (it's not measurable compared to the total request time), but using a remote Redis will add a couple of roundtrips.\nIf you are on a fast 10G LAN, the latency should be less than 1ms so you should be fine.\nIf you are on some public cloud on different AZ, you could see more than 10ms penalty at each request, tough.\n. That seems to be OK. I wonder if it would be easy to add relevant tests to the test suite.\n. Just create two frontends. If IP1 is 1.1.1.1 and IP2 is 2.2.2.2, that would be:\nredis-cli rpush frontend:www.server.com www1\nredis-cli rpush frontend:www.dotcloud.com http://1.1.1.1:80\nredis-cli rpush frontend:server.com www2\nredis-cli rpush frontend:www.dotcloud.com http://2.2.2.2:80\n. The lookup logic is at https://github.com/dotcloud/hipache/blob/master/lib/cache.js#L149.\nAs you can see, frontend:* won't be looked up, unless the HTTP Host: header is unqualified.\nHowever, you could force this lookup to happen by changing line 151, and removing + this.getDomainName(hostKey).\nBut what would be the point of this? Having a fallback for unmatched domains?\nRegarding your other question, backends are supposed to be plain HTTP. I don't know if HTTPS would work; @samalba might know!\n. Hi Miles,\nHTTPS is currently only supported on the frontend. This is the primary use-case for Hipache, as a load balancer: it is intended to be running on the same network as the backends, and the backends are exposing plain HTTP since the communication goes over an internal network.\nAnother way to look at it is to consider Hipache as a \"SSL unroller\".\nNow, regarding your question \u2014 yes, you can probably use Hipache for that. However, it looks like it might be simpler to use something else. What are you trying to achieve? Load balance HTTP connections across multiple outbound links? Do you need the proxy just to relay the requests, or also for its caching features?\n. Hi, do you still have this issue?\nYou might want to make sure that you have the latest version of the hipache image (i.e. by running docker pull stackbrew/hipache first).\nLet us know, then we'll reproduce to be sure!\nThank you.\n. ",
    "vivekdurai": "+1 for sticky sessions/haproxy-style balancing cookie. \n. +1 for sticky sessions/haproxy-style balancing cookie. \n. ",
    "kernys": "+1 I REALLY WANT IT !\n. +1 I REALLY WANT IT !\n. ",
    "eav": "+1 MUST HAVE\n. +1 MUST HAVE\n. ",
    "aaronjudd": "+1\nHas anyone tried https://github.com/Treeptik/Hipache-sticky-session ?  (getting ready to test myself)\nDoes it work, can the code be merged in? \n. @ibash per virtual host would be logical I think. I'll check out your branch this week if it's ready for testing.\n. +1\nHas anyone tried https://github.com/Treeptik/Hipache-sticky-session ?  (getting ready to test myself)\nDoes it work, can the code be merged in? \n. @ibash per virtual host would be logical I think. I'll check out your branch this week if it's ready for testing.\n. ",
    "ibash": "For those above, what kind of api would you want for sticky sessions?\nWould you want to configure it in config.json, or configure it on a virtual host basis?\n. If anyone is interested I started working on sticky session support at https://github.com/ibash/hipache\nLooking for feedback on the api.\n. @aaronjudd Currently it will route requests according to the cookie, however there are a few other things I want to put in before I consider it feature complete.\nThat list is:\n- Don't send Set-Cookie header on every request, only when needed\n- Route sticky sessions to new backend when a backend is down\n- Support maxidle and maxlife like haproxy\n- Allow configuring the cookie name\nAs a bonus I want to be able to configure it to route existing sessions to backends, but not new traffic. The main motivation for this being able to do blue/green deployment without breaking user sessions.\n. For those above, what kind of api would you want for sticky sessions?\nWould you want to configure it in config.json, or configure it on a virtual host basis?\n. If anyone is interested I started working on sticky session support at https://github.com/ibash/hipache\nLooking for feedback on the api.\n. @aaronjudd Currently it will route requests according to the cookie, however there are a few other things I want to put in before I consider it feature complete.\nThat list is:\n- Don't send Set-Cookie header on every request, only when needed\n- Route sticky sessions to new backend when a backend is down\n- Support maxidle and maxlife like haproxy\n- Allow configuring the cookie name\nAs a bonus I want to be able to configure it to route existing sessions to backends, but not new traffic. The main motivation for this being able to do blue/green deployment without breaking user sessions.\n. ",
    "DennisBecker": "+1\nThere should be an explanation how you can setup hipache with SSL for multiple IPs for incoming requests and then calling HTTP backends. If I'm right, you have to push in Redis\n$ redis-cli rpush frontend:https://example.com http://127.0.0.1\n. +1\nThere should be an explanation how you can setup hipache with SSL for multiple IPs for incoming requests and then calling HTTP backends. If I'm right, you have to push in Redis\n$ redis-cli rpush frontend:https://example.com http://127.0.0.1\n. ",
    "samalba": "I agree with you, it's currently not supported but it's important and useful. I'll add that to the TODO page.\n. It's only possible to root by domain name right now, not by URL. But I agree it would be a nice feature.\n. Hi Matt,\nthanks for the patch, I'll review it asap.\nBest,\n- Sam\n. Hi Matt,\nthis changeset looks good, let me do some tests first since it adds another lookup for each request.\n. Hi Matt,\nOk, I'll see to add this to the next release.\nThanks for the changeset.\n. Actually can you create another pull request from the last revision with only this changeset: https://github.com/mattparlane/hipache/commit/68564881a6878bbd3befa51d9596a29119d752f1\nThe one for the x-forwarded-* headers has been included already in another commit.\n. Merged: https://github.com/dotcloud/hipache/pull/15\n. Hi,\ncould you try using the test config? Looks like there is an error when it tries to bind on localhost.\nTry: hipache -c config_test.json\n(specify the correct location for the config_test.json file)\n. Could you rebase?\n. Sorry, I skipped @sstelfox comment above, closing.\n. Could you test the hipache master branch with node 0.10? I am working on a\nnew release of hipache based on node 0.10 that will address this issue.\nOn Monday, February 24, 2014, jolos notifications@github.com wrote:\n\n+1, using 0.8.26 does the trick.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/24#issuecomment-35890816\n.\n\n\n@sam_alba\n. +1\n. Thanks Victor!\n. Last time I checked, nodejs http lib was unable to do 1.0 requests. 1.1 only is still fine with me :-)\n. Hipache is able to use different config (whether by passing --config or the SETTINGS_FLAVOR env var). Since supervisord.conf could be changed to switch the config, I'd rather update all config file, not just config_dev.json.\n. This is interesting, however I would make this configurable with a username in the conf rather than hardcoding \"nobody\"\n. Cannot be merged with the hardcoded username. Closing for now, feel free to reopen with user configurable.\n. The only reason is basically that Hipache is shipped in opensource in the exact same version as we run it on prod. But I totally it makes more sense to have a generic page for everyone else :-)\n. Backends timeout can be due to slow backends (timeout can be increased in the config). As for failover optimization, I invite you to take a look to: https://github.com/samalba/hipache-hchecker\n. We can switch back to node-proxy 0.10.3, however why this \"/opt/hipcache\" thing in the Dockerfile?\n. Ok, closing the PR then\n. Makes, sense. We'll release a new one this week.\n. Yes\nWe have a production setup with several instances of Hipache. The only difference is that each hipache node runs a separate Redis. A shared Redis should work just fine.\n. Sorry for the delay, trying to catching up on Hipache. This looks good to me, this setup is really cool.\n. No, traffic to backends using https does not work but pretty trivial to make it work: https://github.com/dotcloud/hipache/blob/master/lib/worker.js#L364\n. I did my best to abstract Redis in lib/cache.js. So it should be pretty trivial to make it a plugin, basically another cache_something.js that implements the same API.\nWe would accept PRs, definitely\n. This PR looks really good, thanks!\nTests are not passing though, any idea why?\nhipache@0.2.5 test /home/travis/build/dotcloud/hipache\nistanbul test _mocha --report html -- test/*.js --reporter spec\nsh: 1: istanbul: not found\nnpm ERR! Test failed.  See above for more details.\nnpm ERR! not ok code 0\nI guess you introduced a new dep?\n. :+1: \n. LGTM\n. This looks good to me.\nThe only thing to be really careful with enhancing this part of the code is the Redis lookups. Right now, we make sure we limit number of lookups (since the redis roundtrip is done for every single request, even though the LRU limits this a little bit...), and each lookup has a low complexity (except for dead lookup which is using smembers... but the set is small anyway).\nIn other terms, I don't think this PR will impact performance, so it's fine.\n. This is a big one... :+1: \n. still don't get the name of the branch though ;-)\n. LGTM.\nShould we release 0.3.0 before merging this one? @dmp42 what do you think?\nThe changelog is going to be interesting :-)\n. LGTM\n. I think it's caused by the way MemoryWatcher handles the overflow:\n1. Monitor memory, if the process mem reaches 100M:\n2. Stop accepting new connections on the worker\n3. Wait 30 for clean shutdown\n4. If process is still running, kill it\nIn this case, the registry is slow to read the answer (slower than the client to push), then the overflow goes into RAM. Since the payload is huged, it takes more than 30 sec to fill and flush the buffer (see step 3). The process is then killed (in step 4), which causes the \"connection reset by peer\" on the client side.\nI think the MemoryWatcher is fine the way it currently works. I am thinking more on a fix in the buffering. In this scenario, hipache should slow down the read from the client side to avoid filling the buffer (the process memory) like crazy...\n. @shin- could you run the same test with hipache/master? It uses node 0.10 with node-http-proxy 1.0\n. Yes I think this should go in 0.3 (added to the milestone).\nI think the idea is to disable the proxy buffering (I think that's the cause for having the memory watcher going crazy every now and then...)\n. According to http-proxy changelog, versions >= 1.0.x use stream v2 which is supposed to nicely handle pause/resume events to remove buffering completely. In other words, the issue is supposed to disappear with 1.0...\n. @dmp42 I'll do the same on my side, I'll update the issue if I get anything interesting.\n. That sucks... Do you think something like this would be useful? https://hacks.mozilla.org/2012/11/tracking-down-memory-leaks-in-node-js-a-node-js-holiday-season/\nI am gonna try...\n. I tried on hipache/master (node 0.10.26, http-proxy 1.0.3). Interestingly, the worker never goes beyond 311MB for a PUT with a payload of 3GB. So I think the leak does not explode the memory... \nA workaround could be to tweak the MemoryMonitor for now...\nTesting with hipache stable (0.2.4) and node 0.10.26:\nThe worker does not go beyond 260MB\nI am tempted to release a 0.2.5 with just the monitor tweaked and have a dependency on node 0.10... It'll give some room to debug this cleanly for 0.3...\nThe idea would be:\nIf the mem reaches > 100MB:\n1. stop accepting new connections on the worker\n2. finish the request whatever it takes\n3. recycle the worker\nWe can also adopt this policy on master or even make the memory monitor configurable...\n. Just so you know, I am working on an improved memoryMonitor that will allow an unlimited gracefulWait (for the current connection to finish) based on configuration.\nAiming for a PR tomorrow.\n. So, I gathered some data as well. The new graceful shutdown on memoryMonitor can DDoS the proxy easily...\nSince we wait gracefully for active requests to finish, if all of the workers are in this state, you can't serve other requests... I verified that this state can quickly happen with prod traffic.\nAfter reading previous comments, I think I will work on another stable fix for the memoryMonitor that I would backport again on 0.2:\n1. Enable --expose-gc by default (I guess it's possible in the bin script shebang)\n2. Bump the memlimit to 150MB or 200MB (thought?)\n3. Call the global.gc(); in the tick only when we spot a memory overload (and call the exit only if the memory is still over the limit after the gc call).\n4. Re-enable a hard limit to kill the worker (after 5 min?).\n5. Backport #106 in 0.2 as well\nThis would be 0.2.9, what do you think?\n. Agreed. On the other side, I think it's ok to slightly block the worker for few milliseconds instead of killing it and break the active requests as we used to do :-)\nBut you're right, I would not call the gc at every memory.tick(). I'll submit a PR soon.\n. Ok, I'll ping you when the branch 0.2 is updated.\n. Agreed\n. LGTM\n. Also, if we get this merged, I'd like to backport this in 0.2.4 and release a hotfix 0.2.5. So we can spend more time stabilizing the master for 0.3. @dmp42 what do you think?\n. @dmp42 yes of course, it's a workaround. But it's good to go since the leak goes quickly to ~300MB and stays at the amount (log(x)). Which means we won't crash the machine on prod.\nBut I agree we should still investigate the leak. It got worse after 0.2.4...\n. Not a clue :/\n. Updated, ready for review again.\n. Good question, why not. We should provide a feature complete replacement if we go for this. I think it manages websocket through an external library.\n. This needs a rebase, but as soon as it's done, LGTM!\nGood to go\n. 0.2.6 released!\n. LGTM\n. Ok\n. 0.2.7 released. @notnmeyer could you retry and confirm this fixes the issue?\n. My bad.. let me know when you're ready to republish\n. Released 0.2.8 (the npm unpublish command does not work..).\n@notnmeyer Feel free to close the issue as soon you confirm the fix.\n. Not ready to be merged until I find this shebang issue...\n. Finally found a approach that works for both Mac and Linux with any symlinks in between...\nRead for review!\n. Previous comments addressed.\n@dmp42 I don't think there is a need for a \"$0\" ${1+\"$@\"}\nI ran couple tests: with the bin script symlinked from a location with white spaces in the path. I also tested with whitespaces in the args (config path for instance). All my tests passed now.\n. @dmp42 hack committed, ready to merge :-)\n. Cool, thanks. Sorry for the time taken there. I did not get your hack the first time :-)\nYep, backporting to master.\n. npm http PUT https://registry.npmjs.org/hipache\nnpm http 201 https://registry.npmjs.org/hipache\n+ hipache@0.2.9\n:+1: \n. LGTM\nOn Friday, April 11, 2014, Mangled Deutz notifications@github.com wrote:\n\n@samalba https://github.com/samalba I borked the rebase in #103https://github.com/dotcloud/hipache/pull/103somehow.\nThis is a new branch with the same hand-picked commits, fresh on top of\nmaster.\nSorry for the noise...\nYou can merge this Pull Request by running\ngit pull https://github.com/dotcloud/hipache leaks-redux-proper\nOr view, comment on, or merge it at:\nhttps://github.com/dotcloud/hipache/pull/112\nCommit Summary\n- Make jslint fail the build\n- 1.0.3 is broken (broken websockets, broken python tests) - see #102\n  #24\n- Cleaned-up functional tests + now testing memory limit\n- Fix redis client uncaught exception on disconnect to crash the build\n- And stop logging like crazes\n- Damn you travis, you should FAIL on the upload!\n- Stop logging again\n- Tweaking payload size vs. timeout\nFile Changes\n- M gulpfile.jshttps://github.com/dotcloud/hipache/pull/112/files#diff-0(14)\n- M package.jsonhttps://github.com/dotcloud/hipache/pull/112/files#diff-1(2)\n- M test/fixtures/commander.jshttps://github.com/dotcloud/hipache/pull/112/files#diff-2(143)\n- M test/fixtures/servers/generic.jshttps://github.com/dotcloud/hipache/pull/112/files#diff-3(2)\n- M test/functional/http.jshttps://github.com/dotcloud/hipache/pull/112/files#diff-4(489)\nPatch Links:\n- https://github.com/dotcloud/hipache/pull/112.patch\n- https://github.com/dotcloud/hipache/pull/112.diff\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/pull/112\n.\n\n\nSam Alba\nDocker, inc\n. Agreed on the list.\n. I am fine to release it asap :-)\nOn Wed, Apr 16, 2014 at 2:16 AM, Mangled Deutz notifications@github.comwrote:\n\n@samalba https://github.com/samalba ok, let me know then if you want\nmore things to get into the release, and/or when you want to release it ;)\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/115#issuecomment-40578734\n.\n\n\nSam Alba\nDocker, inc\n. LGTM\n. LGTM\n. LGTM\n. Hmm, changing the config format is a big one. I would add some code to detect an old config and raise an error when starting. Otherwise people will be confused (since most of the config directives have default values)...\n. Oh, LGTM then :-)\nOn Mon, Apr 21, 2014 at 8:48 AM, Mangled Deutz notifications@github.comwrote:\n\nThe legacy class is here to support the previous syntax transparently\n(map to the new format magically and output a warning).\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/pull/128#issuecomment-40945707\n.\n\n\nSam Alba\nDocker, inc\n. Could you give me your npm username? I'll give you the right on the package.\n. done\nOn Thu, Apr 24, 2014 at 8:43 AM, Mangled Deutz notifications@github.comwrote:\n\n@samalba https://github.com/samalba Here: https://www.npmjs.org/~dmp\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/133#issuecomment-41295698\n.\n\n\nSam Alba\nDocker, inc\n. LGTM\n. I agree with you, it's currently not supported but it's important and useful. I'll add that to the TODO page.\n. It's only possible to root by domain name right now, not by URL. But I agree it would be a nice feature.\n. Hi Matt,\nthanks for the patch, I'll review it asap.\nBest,\n- Sam\n. Hi Matt,\nthis changeset looks good, let me do some tests first since it adds another lookup for each request.\n. Hi Matt,\nOk, I'll see to add this to the next release.\nThanks for the changeset.\n. Actually can you create another pull request from the last revision with only this changeset: https://github.com/mattparlane/hipache/commit/68564881a6878bbd3befa51d9596a29119d752f1\nThe one for the x-forwarded-* headers has been included already in another commit.\n. Merged: https://github.com/dotcloud/hipache/pull/15\n. Hi,\ncould you try using the test config? Looks like there is an error when it tries to bind on localhost.\nTry: hipache -c config_test.json\n(specify the correct location for the config_test.json file)\n. Could you rebase?\n. Sorry, I skipped @sstelfox comment above, closing.\n. Could you test the hipache master branch with node 0.10? I am working on a\nnew release of hipache based on node 0.10 that will address this issue.\nOn Monday, February 24, 2014, jolos notifications@github.com wrote:\n\n+1, using 0.8.26 does the trick.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/24#issuecomment-35890816\n.\n\n\n@sam_alba\n. +1\n. Thanks Victor!\n. Last time I checked, nodejs http lib was unable to do 1.0 requests. 1.1 only is still fine with me :-)\n. Hipache is able to use different config (whether by passing --config or the SETTINGS_FLAVOR env var). Since supervisord.conf could be changed to switch the config, I'd rather update all config file, not just config_dev.json.\n. This is interesting, however I would make this configurable with a username in the conf rather than hardcoding \"nobody\"\n. Cannot be merged with the hardcoded username. Closing for now, feel free to reopen with user configurable.\n. The only reason is basically that Hipache is shipped in opensource in the exact same version as we run it on prod. But I totally it makes more sense to have a generic page for everyone else :-)\n. Backends timeout can be due to slow backends (timeout can be increased in the config). As for failover optimization, I invite you to take a look to: https://github.com/samalba/hipache-hchecker\n. We can switch back to node-proxy 0.10.3, however why this \"/opt/hipcache\" thing in the Dockerfile?\n. Ok, closing the PR then\n. Makes, sense. We'll release a new one this week.\n. Yes\nWe have a production setup with several instances of Hipache. The only difference is that each hipache node runs a separate Redis. A shared Redis should work just fine.\n. Sorry for the delay, trying to catching up on Hipache. This looks good to me, this setup is really cool.\n. No, traffic to backends using https does not work but pretty trivial to make it work: https://github.com/dotcloud/hipache/blob/master/lib/worker.js#L364\n. I did my best to abstract Redis in lib/cache.js. So it should be pretty trivial to make it a plugin, basically another cache_something.js that implements the same API.\nWe would accept PRs, definitely\n. This PR looks really good, thanks!\nTests are not passing though, any idea why?\nhipache@0.2.5 test /home/travis/build/dotcloud/hipache\nistanbul test _mocha --report html -- test/*.js --reporter spec\nsh: 1: istanbul: not found\nnpm ERR! Test failed.  See above for more details.\nnpm ERR! not ok code 0\nI guess you introduced a new dep?\n. :+1: \n. LGTM\n. This looks good to me.\nThe only thing to be really careful with enhancing this part of the code is the Redis lookups. Right now, we make sure we limit number of lookups (since the redis roundtrip is done for every single request, even though the LRU limits this a little bit...), and each lookup has a low complexity (except for dead lookup which is using smembers... but the set is small anyway).\nIn other terms, I don't think this PR will impact performance, so it's fine.\n. This is a big one... :+1: \n. still don't get the name of the branch though ;-)\n. LGTM.\nShould we release 0.3.0 before merging this one? @dmp42 what do you think?\nThe changelog is going to be interesting :-)\n. LGTM\n. I think it's caused by the way MemoryWatcher handles the overflow:\n1. Monitor memory, if the process mem reaches 100M:\n2. Stop accepting new connections on the worker\n3. Wait 30 for clean shutdown\n4. If process is still running, kill it\nIn this case, the registry is slow to read the answer (slower than the client to push), then the overflow goes into RAM. Since the payload is huged, it takes more than 30 sec to fill and flush the buffer (see step 3). The process is then killed (in step 4), which causes the \"connection reset by peer\" on the client side.\nI think the MemoryWatcher is fine the way it currently works. I am thinking more on a fix in the buffering. In this scenario, hipache should slow down the read from the client side to avoid filling the buffer (the process memory) like crazy...\n. @shin- could you run the same test with hipache/master? It uses node 0.10 with node-http-proxy 1.0\n. Yes I think this should go in 0.3 (added to the milestone).\nI think the idea is to disable the proxy buffering (I think that's the cause for having the memory watcher going crazy every now and then...)\n. According to http-proxy changelog, versions >= 1.0.x use stream v2 which is supposed to nicely handle pause/resume events to remove buffering completely. In other words, the issue is supposed to disappear with 1.0...\n. @dmp42 I'll do the same on my side, I'll update the issue if I get anything interesting.\n. That sucks... Do you think something like this would be useful? https://hacks.mozilla.org/2012/11/tracking-down-memory-leaks-in-node-js-a-node-js-holiday-season/\nI am gonna try...\n. I tried on hipache/master (node 0.10.26, http-proxy 1.0.3). Interestingly, the worker never goes beyond 311MB for a PUT with a payload of 3GB. So I think the leak does not explode the memory... \nA workaround could be to tweak the MemoryMonitor for now...\nTesting with hipache stable (0.2.4) and node 0.10.26:\nThe worker does not go beyond 260MB\nI am tempted to release a 0.2.5 with just the monitor tweaked and have a dependency on node 0.10... It'll give some room to debug this cleanly for 0.3...\nThe idea would be:\nIf the mem reaches > 100MB:\n1. stop accepting new connections on the worker\n2. finish the request whatever it takes\n3. recycle the worker\nWe can also adopt this policy on master or even make the memory monitor configurable...\n. Just so you know, I am working on an improved memoryMonitor that will allow an unlimited gracefulWait (for the current connection to finish) based on configuration.\nAiming for a PR tomorrow.\n. So, I gathered some data as well. The new graceful shutdown on memoryMonitor can DDoS the proxy easily...\nSince we wait gracefully for active requests to finish, if all of the workers are in this state, you can't serve other requests... I verified that this state can quickly happen with prod traffic.\nAfter reading previous comments, I think I will work on another stable fix for the memoryMonitor that I would backport again on 0.2:\n1. Enable --expose-gc by default (I guess it's possible in the bin script shebang)\n2. Bump the memlimit to 150MB or 200MB (thought?)\n3. Call the global.gc(); in the tick only when we spot a memory overload (and call the exit only if the memory is still over the limit after the gc call).\n4. Re-enable a hard limit to kill the worker (after 5 min?).\n5. Backport #106 in 0.2 as well\nThis would be 0.2.9, what do you think?\n. Agreed. On the other side, I think it's ok to slightly block the worker for few milliseconds instead of killing it and break the active requests as we used to do :-)\nBut you're right, I would not call the gc at every memory.tick(). I'll submit a PR soon.\n. Ok, I'll ping you when the branch 0.2 is updated.\n. Agreed\n. LGTM\n. Also, if we get this merged, I'd like to backport this in 0.2.4 and release a hotfix 0.2.5. So we can spend more time stabilizing the master for 0.3. @dmp42 what do you think?\n. @dmp42 yes of course, it's a workaround. But it's good to go since the leak goes quickly to ~300MB and stays at the amount (log(x)). Which means we won't crash the machine on prod.\nBut I agree we should still investigate the leak. It got worse after 0.2.4...\n. Not a clue :/\n. Updated, ready for review again.\n. Good question, why not. We should provide a feature complete replacement if we go for this. I think it manages websocket through an external library.\n. This needs a rebase, but as soon as it's done, LGTM!\nGood to go\n. 0.2.6 released!\n. LGTM\n. Ok\n. 0.2.7 released. @notnmeyer could you retry and confirm this fixes the issue?\n. My bad.. let me know when you're ready to republish\n. Released 0.2.8 (the npm unpublish command does not work..).\n@notnmeyer Feel free to close the issue as soon you confirm the fix.\n. Not ready to be merged until I find this shebang issue...\n. Finally found a approach that works for both Mac and Linux with any symlinks in between...\nRead for review!\n. Previous comments addressed.\n@dmp42 I don't think there is a need for a \"$0\" ${1+\"$@\"}\nI ran couple tests: with the bin script symlinked from a location with white spaces in the path. I also tested with whitespaces in the args (config path for instance). All my tests passed now.\n. @dmp42 hack committed, ready to merge :-)\n. Cool, thanks. Sorry for the time taken there. I did not get your hack the first time :-)\nYep, backporting to master.\n. npm http PUT https://registry.npmjs.org/hipache\nnpm http 201 https://registry.npmjs.org/hipache\n+ hipache@0.2.9\n:+1: \n. LGTM\nOn Friday, April 11, 2014, Mangled Deutz notifications@github.com wrote:\n\n@samalba https://github.com/samalba I borked the rebase in #103https://github.com/dotcloud/hipache/pull/103somehow.\nThis is a new branch with the same hand-picked commits, fresh on top of\nmaster.\nSorry for the noise...\nYou can merge this Pull Request by running\ngit pull https://github.com/dotcloud/hipache leaks-redux-proper\nOr view, comment on, or merge it at:\nhttps://github.com/dotcloud/hipache/pull/112\nCommit Summary\n- Make jslint fail the build\n- 1.0.3 is broken (broken websockets, broken python tests) - see #102\n  #24\n- Cleaned-up functional tests + now testing memory limit\n- Fix redis client uncaught exception on disconnect to crash the build\n- And stop logging like crazes\n- Damn you travis, you should FAIL on the upload!\n- Stop logging again\n- Tweaking payload size vs. timeout\nFile Changes\n- M gulpfile.jshttps://github.com/dotcloud/hipache/pull/112/files#diff-0(14)\n- M package.jsonhttps://github.com/dotcloud/hipache/pull/112/files#diff-1(2)\n- M test/fixtures/commander.jshttps://github.com/dotcloud/hipache/pull/112/files#diff-2(143)\n- M test/fixtures/servers/generic.jshttps://github.com/dotcloud/hipache/pull/112/files#diff-3(2)\n- M test/functional/http.jshttps://github.com/dotcloud/hipache/pull/112/files#diff-4(489)\nPatch Links:\n- https://github.com/dotcloud/hipache/pull/112.patch\n- https://github.com/dotcloud/hipache/pull/112.diff\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/pull/112\n.\n\n\nSam Alba\nDocker, inc\n. Agreed on the list.\n. I am fine to release it asap :-)\nOn Wed, Apr 16, 2014 at 2:16 AM, Mangled Deutz notifications@github.comwrote:\n\n@samalba https://github.com/samalba ok, let me know then if you want\nmore things to get into the release, and/or when you want to release it ;)\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/115#issuecomment-40578734\n.\n\n\nSam Alba\nDocker, inc\n. LGTM\n. LGTM\n. LGTM\n. Hmm, changing the config format is a big one. I would add some code to detect an old config and raise an error when starting. Otherwise people will be confused (since most of the config directives have default values)...\n. Oh, LGTM then :-)\nOn Mon, Apr 21, 2014 at 8:48 AM, Mangled Deutz notifications@github.comwrote:\n\nThe legacy class is here to support the previous syntax transparently\n(map to the new format magically and output a warning).\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/pull/128#issuecomment-40945707\n.\n\n\nSam Alba\nDocker, inc\n. Could you give me your npm username? I'll give you the right on the package.\n. done\nOn Thu, Apr 24, 2014 at 8:43 AM, Mangled Deutz notifications@github.comwrote:\n\n@samalba https://github.com/samalba Here: https://www.npmjs.org/~dmp\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/133#issuecomment-41295698\n.\n\n\nSam Alba\nDocker, inc\n. LGTM\n. ",
    "DazWorrall": "Multiple SSL support on the same IP using SNI would be great as well, Apache Traffic Server does this.\n. Multiple SSL support on the same IP using SNI would be great as well, Apache Traffic Server does this.\n. ",
    "miracle2k": "+1 for SNI support. This is a big limitation using hipache in a private PaaS type of setup.\n. I would have assumed sniCallback would be able to query the certs when they are needed during the request, no pubsub needed.\n. +1 for SNI support. This is a big limitation using hipache in a private PaaS type of setup.\n. I would have assumed sniCallback would be able to query the certs when they are needed during the request, no pubsub needed.\n. ",
    "niclashoyer": "There was a PR #70, but for some reason it was closed. Maybe that is a good starting point? It adds SNI with on the fly configuration through redis.\n. @fastner why was this PR closed? I'd love to see SNI support in hipache.\n. My use case is very simple. Almost in every scenario I use hipache as the service that is reachable on port 80 to forward traffic to whatever runs on the server. To switch these hosts to TLS, I would simply use SNI and could route traffic without changing services. (edit: for this use case it would be great to support certificate storage in redis).\nThe other option here would be to deploy a nginx in front of hipache that does SNI and forwards traffic to hipache. But this seems redundant und more complex to integrate with other software (now I need to handle nginx and hipache configuration).\nIs there any way to get the https changes upstream? Or just the asynchronous SNI callback support upstream? If not, why not?\n. My main use case would be to use hipache as proxy for vhost hosting, e.g. docker containers going up/down managed by a deployment system. For continuous deployment it would be excellent to just add the certificate to redis and tell hipache to use it.\nIf node 0.11+ supports async SNI, would it be feasible to let the SNI part depend on node 0.11+? I'm totally happy with that, since I'm using the hipache docker container, and it doesn't really matter which node version runs inside of it.\n. There was a PR #70, but for some reason it was closed. Maybe that is a good starting point? It adds SNI with on the fly configuration through redis.\n. @fastner why was this PR closed? I'd love to see SNI support in hipache.\n. My use case is very simple. Almost in every scenario I use hipache as the service that is reachable on port 80 to forward traffic to whatever runs on the server. To switch these hosts to TLS, I would simply use SNI and could route traffic without changing services. (edit: for this use case it would be great to support certificate storage in redis).\nThe other option here would be to deploy a nginx in front of hipache that does SNI and forwards traffic to hipache. But this seems redundant und more complex to integrate with other software (now I need to handle nginx and hipache configuration).\nIs there any way to get the https changes upstream? Or just the asynchronous SNI callback support upstream? If not, why not?\n. My main use case would be to use hipache as proxy for vhost hosting, e.g. docker containers going up/down managed by a deployment system. For continuous deployment it would be excellent to just add the certificate to redis and tell hipache to use it.\nIf node 0.11+ supports async SNI, would it be feasible to let the SNI part depend on node 0.11+? I'm totally happy with that, since I'm using the hipache docker container, and it doesn't really matter which node version runs inside of it.\n. ",
    "dmp42": "10 is implemented by #128, along with the ability to have different certificates per-ip. I'm also allowing passphrases for keys, and additional CA.\nSNI will follow soon (but that's a different horse).\nHere's the suggested (new) syntax:\nSimple syntax:\nhttp: {\n // if not specified, will use 80\n   port: 1080,\n// If not specified, will bind to 127.0.0.1\n   bind: \"192.168.1.100\"\n}\nMultiple ips binding, with default and non-default port:\nhttp: {\n // if not specified, will use 80\n   port: 1080,\n// If not specified, will bind to 127.0.0.1\n   bind: [\n    // Will bind to general port 1080\n    \"192.168.1.100\",\n    // Override port for this interface\n     {\n        address: \"192.168.1.5\",\n        port: 1082\n     }\n   ]\n}\nFor https, one needs to specify a certificate and key as well, either globally (that will get inherited -same as with port) or per-ip, possibly overriding the global.\nhttps: {\n // if not specified, will use 443\n   port: 1443,\n// Default certificate / key\n   cert: \"path\",\n   key: \"path\",\n   passphrase: \"optional key passphrase\",\n// If not specified, will NOT bind\n   bind: [\n    // Will bind to general port 1443, with general key / cert\n    \"192.168.1.100\",\n    // Override port and key / cert for this interface, along with additional CA certs\n     {\n        address: \"192.168.1.5\",\n        port: 1444,\n        cert: \"other\",\n        key: \"other\",\n        ca: [\"some\", \"other\", \"certs\"]\n     }\n   ]\n}\nAny comments welcome - as this is not yet ready to be merged.\n. Experimental SNI support is testable. Your opinions/tests welcome @ #130 \n. I'm closing this as the O.P. (@tracker1) description is the ability to bind to independent ips with different certificates (which is now live on master).\nSNI is different, and follows on its own ticket / PR (#129 and #130 ).\n. Is this still something wanted in the app?\nOr can we instead suggest that people start the app with a non-root user / non privileged port and instead use setcap / authbind / iptables?\n. @samalba @jpetazzo \nI would love to get your input on this (see my previous comment). Also, from the OS integration/distribution future POV? (not at ease with that part)\n. @jpetazzo Thanks! I'll get priv dropping in the 0.4 timeframe then.\n. Here is the proposed behavior implemented in the PR:\n- if running as root:\n  - if no user key specified in the configuration, scream and drop dead\n  - if one really wants to keep it root, one needs to have a user: \"root\" (or user: 0) in the config file\n  - if there is a user key specified to something else than root, the workers will drop privileges as soon as they bound\n  - if there is a group specified, drop group privileges to its value\n  - if there is no group specified, we will try to drop group privileges as well, to a group named after the user key (that's nginx behavior IIRC)\n- if not running as root, do nothing...\nNote:\n- the master keeps running as root - which may be considered a bad thing but is obviously necessary to pop new workers with the appropriate rights\n- support both names (will block during resolution) and ids\n. It's on master (-> 0.4) (#143).\n. It's on master (-> 0.4) (#143).\n. @emblica This looks very good! I added a section in the documentation to list third-party softwares of interest, and added airfield in it, so that you get more exposure and feedback for it.\n. @siminm were you using tsuru package?\n. Confirmed.\nLooking into it.\n. @samalba web-sockets heads-up: https://github.com/nodejitsu/node-http-proxy/issues/612 - we should likely freeze to http-proxy-1.0.2 until they sort things out\n. Hi @hansent \nI tested with Docker version 0.8.1, build a1598d1 and it's WFM. As @vieux pointed out, we use ADD, and it's working as expected now.\nI'm closing this - if you feel the problem is not addressed, please reopen.\nAnd thanks for your time reporting this!\n. @dgryski Thanks for your report!\n@samalba do you plan working on that, or would you consider http 1.1 being mandatory for use with Hipache (in which case I'll update the documentation)?\n. Hi @ubermuda,\nFirst, thank you for your work on this.\nAs you expressed the desire for some code review: I wouldn't use relative/absolute paths in the config file, but only module names. npm installed modules in the hipache folder itself (or globally installed) should be accessible by their name without having to mind about path resolution. Now, if you would really like to resolve module paths relatively to the config file, that should be easy (use nodejs path.resolve just after hipache loads the config file).\nApart from that, your code looks ok to me.\nNow, about the opportunity of this:\n- in most cases, people interested in middlewares seem to not really need hipache, but rather just http-proxy, which does that very well - makes me question if there is a point for us in supporting full-fledged middlewares\n- although, some kind of middleware might be interesting to add to hipache - I just:\n  - want a clear and clean architecture to create these middleware and share them\n  - want a safeguarded API that prevents middleware from killing the performance\n  - not sure yet if declarative (eg: config) is better here than API (make Hipache more usable as a library)\nIn any case, we are currently refactoring quite a lot of code, and are considering removing http-proxy as a dependency - so, middleware thinking (and possible implementation) will have to wait a bit (likely Hipache 0.4). If you are still interested at that time, I would be happy to include your ideas / code by then. About this PR specifically, as (unfortunately) the code is bit-rotten, I'll close it.\nThanks again for your contribution here.\nBest.\n. Hi @andrevdm \nIf I understand your use case correctly, you are looking for a middleware able to alter the lookup mechanism, right?\nWe are currently refactoring a lot of code right now, with a focus on memory footprint and performance, so, unfortunately, your PR bit-rot. Also, @samalba argument still stand... hence I am going to close this.\nNow, if the time is not yet there for implementation, ideas are definitely welcome on #107 !\nThanks again for your interest\n. @hanwoody which version of hipache are you running? Can you share your hipache configuration (including vhost definitions in redis?)? Also, did you try to curl from the machine/container running hipache?\nThanks.\n. @jamescarr \n\nI noticed is that if a backend is dead but still active in the backend list we get a few timed out requests\n\nDo you mean: \"a few timed-out requests on the dead backend\", or on another, alive backend?\n@drefined from your log file it seems that retryOnError is working as expected (trying first with backend 1, then trying again with backend 0, finally dropping it as the retry limit is reached - am I missing something?\nThanks!\n. @drefined @jamescarr \nWe now have tests for failing backends: https://github.com/dotcloud/hipache/blob/master/test/functional/http.js#L246 that demonstrate a failing backend is hit once, then valid backends are used.\nThis makes me think that whatever the problem for you was, it was fixed on master, which has changed significantly over the last few months.\nAs we are about to release 0.3 from master very soon now, I'm going to close this ticket.\nIf you feel I shouldn't have, that the symptoms are still here, or that 0.3 doesn't address your problems, please reopen it!\nThanks a lot for your time reporting this.\nBest.\n. Hi @jamescarr \nThanks for your interest and contribution!\nUnfortunately your PR bit-rot (we changed quite a lot of code recently), so, if you would really like this to get in, I guess you need to start over (drivers have been introduced so redis is no longer the only one, etc) - and which is why I'm closing this PR for now.\nBest.\n. Several new versions got generated since then, on the 0.2 branch - and 0.3 should be out the door soon hopefully!\nClosing this.\n. #77 was a duplicate of this ticket, and that was fixed with this commit: https://github.com/dotcloud/hipache/commit/2f93072e8326203b67293161cb1692625573fe2a\n. That should be fixed with the upcoming Hipache version (which requires nodejs 0.10 and has been updated to use redis 0.10).\nYou can check against the master if you can use it - otherwise the new version will be there soon fortunately!\nThanks!\n. I'm closing this then - if something is still wrong, please say so!\nThanks for the report.\n. Hi Adrianbro,\nWhat long-polling technique do you have in mind? BOSH? What server component backend do you plan to use?\nThanks.\n. I'm working on a pluggable datastore architecture right now. So, am looking into supporting etcd and/or memcached additionally to mainline redis.\nWould you guys recommend a specifc etcd node client over another (as there is at least three different: https://www.npmjs.org/search?q=etcd)?\nThanks!\n. Being unable to return asynchronously is a problem indeed... (that means all certificates need to be read beforehand)\nNow:\nhttps://github.com/joyent/node/commit/048e0e77e0c341407ecea364cbe26c8f77be48b8\n... if I read this correctly, the signature of the method changed, and next-node will allow asynchronous SNICallback.\nAlso, maybe it's worth investigating reading from the socket directly to get the servername indication without resorting to node TLS lib? Sounds hackish, but that really would be better to be able to read the key at request time.\nWhat do you think?\n. Hi @morpheu \nUnfortunately, this bitrot. Also, @jpetazzo 's remarks would need to be addressed before this could be merged. Finally, I'm wondering if node-syslog is the right choice here (what about syslog instead? - https://github.com/cloudhead/node-syslog). What about rethinking hipache logging entirely? What would we like to support if we were to (apart from file and syslog)?\nStill, I think this is interesting, and would certainly welcome such a PR during the 0.4 development timeframe - so, while I'm going to close this specific PR for now, I would definitely review a new one that would address the issues here. \nThanks for your contribution anyhow.\nBest.\n. Hi Nicolas,\nUnfortunately, it's very hard to provide any useful advice without detailed technical information (measures, logs, etc).\n- what do you call \"very bad performance\"? Based on what metric?\n- what makes you think your problem is with hipache (and not, say, with your JEE WebApp)?\n- which platform/version do you use?\n- what kind of hosting is that?\n- what's your configuration?\nUnless you can pinpoint the exact problem so that it can be reproduced simply, you might have more luck having someone look into your setup instead.\nSorry not to be more helpful...\n. Hi @zepouet \nDid you manage to get more informations about your problem so we can help?\nBest\n. Hi,\nDoesn't using a different database (which is possible right now using redisDatabase in your config file) cover your case? (eg: avoid conflict with other data)\nThanks!\n. :-)\nIs that a matter of personal preference, then?\nOr are there strong arguments for not using redis databases to segregate data?\nThanks again for your input on this! \n. Fixed by #89 \n. Rats - pushed too much on this branch :/\n. My bad. Let me fix it.\n. Hi.\nYou then need to rename the existing jshint.json into .jshintrc. (and modify package.json as well)\n. Thanks for your efforts on this!\nEnhancing wildcard matching indeed is long due.\nNow, I would really prefer something that really fix the root of the problem (eg: properly lookup .every.level.domain.com, .level.domain.com, *.domain.com when requesting foo.every.level.domain.com), with no additional configuration flags - sure, that would require more work and is tricky to do right (without too much performance penalty).\n@samalba what do you think?\n. @ksikka @bjyoungblood (and others interested): what's the real life, max-depth domain you guys would use?\nfoo.bar.domain.com?\nfoo.bar.baz.domain.com?\nLonger?\n. LGTM\nThanks a lot for your work!\n. Fixed by #89\n. Fixed by #89\n. Yeah, the build broke - this is because of some bizarre overlap between multiple spawned redis instances on travis - will fix that.\n. About the branch name: I'll explain with a drink :-)\nAbout Travis: I want a divorce!\nAbout the python tests: something is time sensitive - I get about one out of ten tests runs failing here - apparently more on Travis. I suspect some race between Redis TTL and tests runs.\nDo you mind if I migrate the python tests entirely into mocha? (when the mocha tests will be solid enough)\n. FYI, I hooked in Code Climate. I kind of like that stuff - granted, it's not much more than jshint in bootstrap clothes, but I dig the colors and big icons :-).\nHere: https://codeclimate.com/github/dotcloud/hipache/compare/la-jolla - it's telling us:\n- that the overall project quality got up 0.4 points on that branch\n- that I missed a radix parameter on a parseInt\n- that we should really get to cleanup worker.js :-) (https://codeclimate.com/github/dotcloud/hipache/code)\n. We are now also testing functional with mocha. Most python tests have been duped in there (save test_parseerror).\nWe will be able to tests https and web sockets very soon!\n. Let's merge then release 0.3 :-)\nI'm out for two/three days, so, early monday (from 9AM PST on) would be good for me for a release so I can react fast if something is broken.\n. In case you don't have one already, here is a helper script that takes care of versioning / tagging / npm publishing: https://gist.github.com/dmp42/9932454\n. Still have some intermittently failing python tests. I'll look into it.\n. Hi @jkingyens \nWhat I refer to in this \"ticket\" is a different issue -\nbut in your case if Hipache is crashing, this needs to be looked into.\nInterestingly, the ip you list here (169.254.169.254) is used by amazon to distribute metadata.\nCan you copy the entire output before the crash (inc. possible stacktrace)? Also, node and npm version, hipache version, and hipache configuration file?\nThanks a lot!\n. @jkingyens Glad to hear you got it fixed! Was it related to Hipache configuration? Or to the Google Compute Engine configuration?\n. Nice!\nThanks a lot for these!\n. > I will try to improve upon this at some point!\nI'm not sure yet what form these CLI will look like, and I won't work on them immediately, so please do!\nand definitely add any idea / wish / remark in here, they are welcome.\nBest.\n- Olivier\n. Just so that things are clear: this includes the TLS overhead - eg: this is the number of bytes written on the socket, not the http payload size.\n. Hello @cpuguy83 \nThanks for your report.\nI will need more details in order to diagnose this.\nI assume your session token is a cookie?\nCan you show your hipache config, and dump your redis backend information?\nAlso, it would really help if you would be able to capture http requests:\n- between client and your server app (successful)\n- between client and hipache (unsuccessful)\n- and ideally between hipache and your server app (unsuccessful)\nThanks a lot!\n. Just in case: are you using Hipache in front of two instances of your app (and they don't share session data?)\n. Ok - there is only one backend, right? Can you list it as is in redis?\nThen let's look into the http requests.\n. So, that's just the frontend and identifier, right? Can you list the entire output of redis-cli lrange frontend:emp.envuplay.com 0 -1?\nThanks!\n. Ok  - for the http requests, can you try getting them? (you can use firebug to dump the requests + answers on the client side)\n. Thanks! That's the successful connection I assume (straight to nginx)?\nCan you play the same thing with hipache in front, reproducing the problem? Sorry for this rather tedious process, but there really is no other way...\n. Ok. Are you able to trace what's happening between the hipache server and your backend?\nSomething in the line of: sudo tcpdump -lnni eth0 -w dump.txt -s 65535 host emp-front.emp.prod.envu.io and port 8080\n(adjust eth0 to the appropriate network interface)\n. Great. I'll look into it asap. Though, it looks like the transaction is forwarded correctly back and forth by hipache...\nMore later.\n. @cpuguy83 can you try running hipache on http (instead of https) and see what it does?\n@samalba from the dump here: https://www.dropbox.com/s/fz3svcgcraqe5ae/dump.txt - any idea why x-forwarded-proto and x-forwarded-protocol are different (eg: https vs. http)? (being hosted on dotcloud, I assume there is an hipache in front of his hipache? but why would there be mixed-up http/https here?)\n. Good!\nHappy it helped.\n. @shin- also, can you tcpdump the request so we had it to the tests?\n. @shin-  gist blog too big\nhttps://gist.githubusercontent.com/anonymous/9933561/raw/8d79cce80a164216f1acb3f029d3aece85694e57/gistfile1.txt\nCan you use dropbox or spideroak (or whatever else), and use these options:\ntcpdump -lnni eth0 -w bugdump -s 65535 host HOSTNAME and port 80 (<- adjust eth0, HOSTNAME and port)\n. @samalba maybe we should get to the bottom of this before releasing 0.3 - what do you think?\n. Ok - I pushed the milestone date to next week.\nI'll start looking into it tomorrow (I really want a test for that as a starter) - ping me if you have any update/idea meanwhile.\n@shin- thanks for the dump\n. It certainly is partly our fault.\nWith Hipache I hit the memory wall after only 50MB uploaded (!).\nIf replacing worker.run by this:\n```\nvar http = require('http'),\n    httpProxy = require('http-proxy');\nvar proxy = httpProxy.createProxyServer({});\nvar server = require('http').createServer(function(req, res) {\n        console.warn('proxying');\n        proxy.web(req, res, { target: 'http://localhost:1000', xfwd: false });\n});\nserver.listen(1080);\nvar tick = function () {\n        var currentMemory = process.memoryUsage().rss;\n        console.warn(Math.round(currentMemory / (1024 * 1024)));\n};\nsetInterval(tick, 500);\n```\n... I can get to about 1.3GB upload without hitting the roof.\nTrue, it's still leaking, but way more slowly.\nI went deep into http-proxy, it's likely not their fault - more likely a core node issue (http request and/or stream and/or ee) :/\nIMHO: we can start by making the situation better, by plugging our leaks (but that likely requires ripping worker apart pretty hard) - but then if we want to fix the root of that crap, I'm afraid we will have to hatch nodejs core library...\n@samalba what's your call? rewrite worker now and reduce our leaks?\n. I tried it quickly - it seems quite limited...Nodejs instrumentation sucks (or I suck at finding the proper tools...)\n. I had to pinch myself and quadruple check, but incredibly, read-installed consumes 30 to 40 MB of memory per-worker (that's doubling the memory a cold start worker consumes otherwise). read-installed is not responsible for the leak though - it's just plain f silly as the only use of it is to get hipache version itself.\nI'll PR the removal of it independently.\n. Heads-up!\nTo get there I rewrote http-proxy (the good parts) and ripped-up worker to bare bones - with pretty much the same results (save PR #106).\nThe bottom line is: try running hipache with node --expose-gc --always-compact, then modify memoryMonitor so that you call gc() inside tick. This will force garbage collection before sampling memory.\nThe result is that we do leak (actually, we do, but node does as well) - just, it is very slow: after a 1GB upload, I barely hit 70MB (cold-starting at 60MB).\nAlso, we never get over ~300MB because node does trigger GC auto-magically in that case (and release). This is not a logarithmic leak.\nThe solution to this as a whole depends on how we see things:\n- plan A: \"don't know, don't care\" - accept this is a GC language: then don't bother monitoring (small amounts of) memory (just prevent it from going over, say, 500MB), and let node do its thing\n- plan B: \"old-school tight arse\" - recommend that people launch node with --expose-gc --nouse-idle-notification, and call GC explicitly ourselves - and monitor memory like we do now, including chasing actual leaks \nWe can certainly find a middle-ground: if automatic garbage-collection is disabled from the command-line (like we could recommend), then memoryMonitor gets into tight-arse mode and takes charge, trigger gc on its own, and mercilessly kill worker above 100MB - otherwise, we just warn the user and up the memory monitor limit to 500MB.\nSecond post here is interesting: https://groups.google.com/forum/#!topic/nodejs/BO6JdYi4n2k\nAnd more background over there:\n- http://blog.caustik.com/2012/04/08/scaling-node-js-to-100k-concurrent-connections/\n- http://blog.caustik.com/2012/04/11/escape-the-1-4gb-v8-heap-limit-in-node-js/\n.  Agreed on all (yes, it does work in the shebang).\nWe kind of opened the pandora box here anyhow, by fiddling with the way node handles memory...\nRight now, I can't think of anything better... Though, I'm worried about the perf impact of explicit stop-the-world gc calls - how long will these calls take under prod stress? (and why on earth do we need to do that in the first place?)\nI'll continue digging further into node http library.\n. About nodejs iddle garbage collection (that got removed in 0.11...): https://github.com/joyent/node/commit/d607d85\nAbout nodejs gc generally: http://www.tuicool.com/articles/zUnMby - mind the advice:\n--expose-gc\nThis option will allow you to call the gc() function within your javascript and trigger a\nmanual garbage collection. This is an expensive garbage collection and will stop your world. \nUse with caution, or better yet, don\u2019t use.\nUpdate: I'm not sayinggc() is not the way to go - just that we need to tread carefully.\n. Anything is better than what we had :-)\nI'm a bit concerned with people stating that gc can take up to 3 to 4 seconds though - either way, we will tweak that up eventually, so, let's go with that!\n. With heartbleed's sh storm, I won't be able to check before late tomorrow, so, don't wait for me if you need it in very quickly.\n. Should we close this?\nTrue, we haven't been deep enough (yet) into node's lair to understand why it's not freeing immediately, but at least the symptoms are fixed thanks to @samalba 's patches.\n. Hi @ruudud \nThanks a lot for this!\nWe will definitely look into this - but that will have to wait a bit.\nAs we will soon release a new version with some quite big changes, we need to focus on bugs redux for now.\nWill get back to this soon, hopefully.\nThanks again.\n. @samalba would you look into that one? If it's ok for you, I would rather have it merged before I refactor worker as it will be harder later on.\n. @ruudud Let's get this in then ;)\n. Apart from my nitpicking, this looks good.\nAdmittedly, we lack documentation on the tooling I commited, but you should:\n- run gulp hint once in a while to see if the linter is happy\n- or alternatively run gulp hack:hipache in a console alongside your editor to have it run live while your work (will run both the linter and unit tests)\nand I should really move to fix the build on travis now :/\n. @samalba I tested against my own hipache droplet - this does prevent the worker from getting killed, but obviously doesn't fix the leak (memory consumption is still skyrocketing).\nAgreed for the rest: lets merge, issue 0.2.5, and gut worker.js for 0.3.\n. @samalba I finally managed to have a test for that in the test suite (PR ASAP): PUTting 100MB from /dev/urandom which does (fortunately) show that your workaround works ok.\nThat will make it way easier to hunt down the leaks.\nBtw: any idea why the memory consumption is logarithmic?\n. Short answer is \"they don't\" - but I'm reluctant to go into possibly breaking changes with websockets without any ws tests. Depending on the outcome of our memory hogging problem, I may push this to 0.4 instead.\n. Note to self: current version 1.1.1 brings nothing over what we use (1.0.2) \n. Unfortunately the bug is in node core, and not in http-proxy - their \"patch\" brushes the underlying node problem under the rug, but breaks subtly in a number of ways (that they don't test).\nI'll look into this and see if I can find something satisfactory.\n. @cywjackson we can discuss this at #141 - thanks a lot for your report\n. The travis build failing is expected (now) - as soon as we merge with the new tolerant memorymonitor, the failing test should pass. \n. \\o/ travis is green at last!!! \\o/\n. I porked it, it seems... :/\nProper PR in #112\n. Sounds good to me! Let's rock it! :+1: \n. http://www.joyent.com/blog/walmart-node-js-memory-leak\nhttps://github.com/davepacheco/node-stackvis\nhttp://stackoverflow.com/questions/19595184/nodejs-application-with-memory-leak-where-is-it\n. And the failing test on travis is resolved by downgrading http-proxy to 1.0.2 (see PR #103)\n. Thanks a lot.\nWill look into this ASAP.\n. Indeed #53 broke it (should have used git+https I think, which is now committed on the branch).\n@samalba can we release 0.2.7 ASAP? Also, maybe we should rename that branch to 0.2.\n. @samalba it seems you overwrote my commit while renaming the branch: https://github.com/dotcloud/hipache/commits/0.2\nFixing it.\n. Fixed.\nInstall tested npm install git://github.com/dotcloud/hipache.git#0.2.\nGood to go ;)\n. npm install hipache now WFM\n. It is env that sucks and doesn't split the argument (in some setups - it's working ok on OSX) .\nMaybe we can fallback on sh and exec (if feasible)?\n. There:\n```\n!/bin/sh\n':' //; exec node --expose-gc --always-compact -- \"$0\" ${1+\"$@\"}\n```\nThe tricky part is to get the javascript runtime and sh to parse that the way we want.\nThe question remains, though: do we care about using sh vs. env?\n. @shin- are you talking about the warning you list below? (node) warning: possible EventEmitter memory leak detected. 11 listeners added. Use emitter.setMaxListeners() to increase limit.\nIf so, it's an indication that we might be leaking (many listeners getting stacked on an EventEmitter), not that node has a problem.\n. > @dmp42 I don't think there is a need for a \"$0\" ${1+\"$@\"}\nAgreed.\n. Let's get this in :+1: \n. I'm doing a couple of tests and then we can npm publish - I'll ping you in a minute.\n. @samalba meanwhile, can you backport the changes to master? \n. I wasn't clear enough either - should have gist-ed in the first place!\nI'll get better at that ;)\n. Looks ok for npm publish!\n. \\o/\n. Hi @stefanfoulis \nHipache marks a backend as dead when it returns an error code (>= 500/501) save the case it returns a 503 along with a retry-after header (which would indicate maintenance).\nhttps://github.com/dotcloud/hipache/blob/master/lib/worker.js#L294\nWhat happens when a backend is marked as dead is, if passiveCheck is enabled, it adds en entry into redis indicating that this specific backend is dead:\nhttps://github.com/dotcloud/hipache/blob/master/lib/drivers/redis.js#L120\nThat redis entry will expire after some time (the deadBackendTTL config setting).\nTo answer your question specifically: under no circumstance a dead:<domain-name> entry can be created by Hipache. It is always per-backend.\nSo:\n- are you using hipache-hchecker alongside hipache?\n- can you copy your current hipache version and config file?\n- can you dump your redis config for that domain?\n- are you positive the entry was exactly dead:<name> without anything else?\nThanks!\n. > Why is there a global dead: entry for the whole domain? Wouldn't it make more sense to have dead: entries for individual backends?\nThat was a performance choice to reduce the number of round-trips to redis.\nWhat we have:\n- lookup backends list and dead backends list for the domain in one trip\n- pick a valid backend\nThat's 1 redis trip per http request.\nWhat we would have with your suggestion:\n- lookup backend list\n- pick a backend\n- check if the backend is alive\n- rinse and repeat steps 2 and 3 until we have a valid backend\nThat's (2 + K / N) redis trips per http request, where K is the number of dead backends and N the total number of backends for that domain.\nWether or not this is significant performance-wise is up for debate - but that was the main argument at the time.\nAs for your problem, unfortunately, I'll need more than that to diagnose it - specifically, the content of \"dead:www.example-domain.com\".\nIf this is not a production system, may I ask you that you add an extra (bogus) backend to your domain, and do a dozen of queries on it, so that the bogus backend gets hit and create an entry in redis that you will be able to list?\nAs for entirely disabling hipache handling of dead backends, I don't think there is a simple way to do that right now (with the Redis driver at least) - I'll consider it for future releases though.\nAlthough, you can achieve it by:\n- writing a very simple script that writes a \"hchecker_ping\" entry in redis every 20 seconds (containing the timestamp) - that will actually prevent Hipache from populating dead entries\n- maybe use hipache-checker in dry-run mode (not sure about that - @samalba will have to confirm)\n. About deadBackendon500 it will actually prevent backends returning 500 to be considered as dead - but that's just for 500, not 50x...\n. With your actual config (deadBackendTTL: 0.5) the \"dead\" backend entries will stay just for half a second - this might be too short a time window for you to get one.\nEither way, there are tests on master now for failing backends (https://github.com/dotcloud/hipache/blob/master/test/functional/http.js#L287) - though they are a bit messy right now.\nI'm confident that hipache on master is working as it should - now, you are certainly running it from npm, so if you would like to investigate a bit more on this before we close this ticket, just to be sure it's working ok, that would be cool.\nbtw, I just found out an alternative hchecker implementation written for node (here: https://github.com/runnable/hipcheck)\n. Hi @tjmehta \nThanks for this!\nHappy to see interest in Hipache, and an alternative health checker - I will definitely take a closer look at it later next week!\nAbout this PR:\n- @samalba 's Hipache Health Checker is not written in python, but in go - can you change that in your PR?\n- nevermind the failing travis build for now - not your fault\n. @tjmehta you guys at runnable are using Hipache?\n. Nice! :-)\n. I know you from Musicbrainz, don't I? (my nick is dmppanda there)\nI'll take a look at this tomorrow - thanks for the report!\n. @warpr what version of hipache are you using? stock from npm?\n. @warpr one thing is sure - \"address\": [\"0.0.0.0\", \"::1\"] is the syntax for master (upcoming 0.3) - and that changed. The syntax for your (npm) version, is visible on the 0.2 branch: https://github.com/dotcloud/hipache/tree/0.2\nHaving said that, have a look here: https://github.com/dotcloud/hipache/pull/100\n... and you might try master indeed :+1: \nTell me if that helps.\nBest.\n. @warpr let me know if anything new here - I let you close this if master is working for you then. \n. LGTM - feels good to see travis happy again as well! \n. @samalba ok, let me know then if you want more things to get into the release, and/or when you want to release it ;)\n. Ok! package.json reads 0.3.0 already - I think all is left to do is git tag & npm publish :-) (I don't think I have the rights to do it? sharing rights on npm used to be a total pita - not sure it got any better...)\n. It's live.\n. @thaJeztah +1\n. The linked PR does some validation, and paves the way for more.\n. @willdurand agreed in the grand scheme of things :)\nNow, there apparently exists one already here: https://github.com/tsuru/tsuru-deb/tree/master/node-hipache-deb\nand since people are using it and are having issues linked to it (#16) I would love to connect with @morpheu (or someone else maintaining this package)\n. Mmmm... it still failed these past days. Let's keep it open for now.\n. http://googleonlinesecurity.blogspot.fr/2013/11/a-roster-of-tls-cipher-suites-weaknesses.html\nhttp://www.bolet.org/TestSSLServer/\nhttps://www.ssllabs.com/\n. https://github.com/joyent/node/issues/2727\n. \\o/\n. Tests fixed.\nMisses documentation work, but I need #124 to get in first.\n. The legacy class is here to support the previous syntax transparently (map to the new format magically and output a warning).\n. Nice :-)\nI'd like to wait for 0.3 to be released (hint, hint :-)) before merging it anyhow\n. #143 has the merged / cherry picked version of this. \n. @niclashoyer Thanks for the feedback.\nMy main question is roughly: is the use case a setup with a massive number of certificates that gets added / removed often (just like vhost / backends), or is it more of a limited number (dozen) certificates that could be loaded / handled on start.\nFirst scenario means we really need asynchronous SNI - second scenario we can live with synchronous SNI (as upstream).\nI can even implement both - I would like to get a feeling about which scenario is likely / not likely.\nAbout upstreaming my changes, that wouldn't make much sense, because node 0.11 (and upcoming 0.12) completely rewrote http(s) (which was admittedly crappy) and does indeed support asynch SNI - and I doubt they would accept such a (complex / API breaking) change on the current stable / legacy (0.10) branch.\n. Well, the problem is node 0.11 is not exactly reliable (it does crash a lot on our current test suite).\nLet me sleep over this a few days to come up with a nice solution.\nThanks a lot for the feedback!\n. I'll go with a two tier system, where activating dynamic SNI loads https2 while the basic (on boot) support only loads vanilla https.\nThanks for your feedback guys.\n. Hi @Siedrix \nLet me get back to you about this later this week - definitely good to see interest!\n. @hpg4815 I'm not sure what you ask for, but what you describe really looks to me like SNI (the capacity to handle multiple SSL vhosts with different certificates on the same ip) - and yes, that ticket is about adding support for that :-)\nCurrently Hipache doesn't have said capacity (this branch is about adding that feature though).\nPlease note that no other node project has this capacity either (which is why this is a complicated subject), for the reason nodejs doesn't allow asynchronous SNI certificate retrieval (making the certificate looking blocking, which is unacceptable).\nAbout SSO:\n- there is no such feature in Hipache - if authentication there is, it's the responsibility of the backends to implement it (using SSO or otherwise)\n- if you would want hipache to perform some kind of authentication for all backends, then it's a matter of adding a middleware - requires hacking code, but probably not that complicated\n- nodejs doesn't have per-se a notion of SSO (and after-all SSO is just a generic term, not a specific technology, right?)\nAbout SiteMinder / Ping Federate I don't know what they are or what they do :-)\nHope that helps!\n. > This would also allow me to add cert and key files for each hostname/ip binding but would just come with the requirement of restarting Hipache afterwards?\nYes.\n\nwhen the term vhost is used in regards to redis, does this imply one would be able to add all configurations in Redis similar to that of the Apache NameBasedVirtualHost?\n\nYes and no. There is very little you can configure for a specific \"vhost\" with Hipache. Pretty much, all you do is point an \"entry\" domain name to a set of backends, and you don't have more control than that.\nThat being said, hostname are preserved.\nHope that helps!\nBest\n. @stefanfoulis lack of time and motivation, sorry about that... \nI'll look into this again early January as time permits.\n. @stefanfoulis thanks for reaching out and for your proposal. Since I'm a Docker employee, I'll decline your offer - though I would happily review a PR fully implementing this, or finish my own PR if time/motivation permits.\n. @fanatic : great!!! Love it!\nLet me have a look at it and get back to you soon (we need to get 0.3 released first, but this will definitely make it for 0.4).\nAbout skipping the tests (on environments where we don't have / want a zookeeper service), have a look at how it's done for etcd:\n- https://github.com/dotcloud/hipache/blob/master/.travis.yml#L6\n- https://github.com/dotcloud/hipache/blob/master/test/unit/driver-etcd.js#L7\nThanks!\n. Also: can you run gulp hint (or jshint) and make jshint happy on your files?\n. @fanatic I guess zookeeper can be launched (by our fixtures script) specifying command line arguments and/or a custom config file. That config file would then point the zookeeper dataDir to a specific folder (say: /tmp/hipache-test) that we may be able to rm once the tests are finished - at least it wouldn't mess-up with other things on that zookeeper.\nWhat do you think?\n. I have the hope that zkServer is just a (very? simple?) wrapper that ultimately does something in the line of: java org.apache.zookeeper pathtoconfig\nGiven this is for the purpose of running tests available to developers only, I don't quite care if our launcher is not \"very\" portable.\nSo, if you can find a hack, or even commit a copy of zkServer.sh in our fixtures, I'd be ok with that.\nAnd yes, commit the tests' config file as well.\n. Ok, I only have some minor nitpicks on this (see inline), mainly:\n- would love to have a functioning fixture script to start / stop zookeepers servers instances on demand with custom config\n- would love to have travis install zookeeper and run the tests\nApart from that, I'm a bit worried about performance right now - domains.with.lots.of.subdomains will generate lots of queries to ZK. Any idea how fast it is at that?\n. Hi @fanatic \nNo problem for the delay - I was and am busy as well.\nI like what you did quite a lot.\nCan you rebase it on top of master?\n. LGTM\n. Would be happy to merge this once rebased / reviewed again (cc @willdurand)\n. Just adding that we forgot to tag 0.2.9 though we released it on npm :(\nWe really need a safer process.\n. @samalba dmp (https://www.npmjs.org/~dmp)\n. Interesting.\nDo I read you well? This is node http request module which does the _read, not node-http-proxy, right? (the http-proxy module really does little, and I'm thinking about dropping it entirely anyhow)\nAbout how to do deal with this... really need to start with a mocha test that reproduces the scenario.\nThen either dive into node http request or try with an intermediate stream (pipe the request into it and see if we can \"keep\" a copy of the first buffer until we are sure it's not failing).\nNow, this raises another question: what if a working backend fails in the middle of a post? (thinking about the registry, and very huge PUT)\nRight now, I guess we are retrying that as well, and that will fail for the same reason - and there, it's not reasonable to keep the whole payload, is it?\nPut otherwise, I'm tempted to say that we simply shouldn't retry a request that has a payload, as a rule of thumb. Then, we might try to make an exception for the ECONNREFUSED case.\nWhat do you think?\nI'm scheduling this for 0.4.\n. It might very well be http server as well that is responsible for this.\nWhen I see this: https://github.com/joyent/node/blob/v0.10.26/lib/http.js#L127 .........\nI wouldn't blame the streams model (and buffering) too early...\n. Scratch that: only the tests are doing multiple arguments rpush. They need to be fixed though.\n. Hi there - it should be fixed. I published an update to npm.\nThis is unrelated to the use of Ubuntu 14:04, although it's (also) telling you that for 0.3 we changed the syntax for redis driver - you might want to review the new doc and update your config.\nTell me if that helps.\nThanks for catching this!\n. @cywjackson\nFor some reason I can't reproduce the leakage here.\nDo you have a simple test case that we could possibly add to mocha?\nThanks! \n. Thanks a lot for the detailed feedback! It does help.\nhttp-proxy 1.0.3 \"fix\" is not acceptable - as it also kills client-side keep-alive (and websockets).\nI need to do a few tests, and hopefully we can come up with a fix.\n. And I'll be overworked in upcoming weeks, so, don't expect a lightning fast fix :-)\n. Thanks!\n. Thanks\n. Thanks!\n. Thanks!\n. Agreed. Anyone willing to PR it?\n. Happy you figured it out ;)\nClosing this, then.\n. Thanks!\n. Do you mean, is Hipache able to route imap or pop protocols? If so, no, it's an HTTP load-balancer and routing layer.\n. There are no such plans ATM - but throwing in ideas can't hurt.\n. I like the eson approach to allow overriding any config settings via the env.\nDoes go well with docker run way IMO.\n. @qbraksa @alibby @raelmax if you still see this problem, please say so and I will reopen.\nThanks!\n. Thanks!\n. Thanks. Merged.\n. X-FORWARDED-FOR is informative only. Also, given a chain of proxies, there is no way to validate the ip end-to-end.\nWhat problem are you trying to solve?\n. Ok. I guess it boils down to: how do we differentiate between legit proxies setting the X-FORWARDED-FOR header from rogue clients spoofing it?\nEither way, if you want to do something, that should be here: https://github.com/hipache/hipache/blob/master/lib/worker.js#L151\n. Would you be interesting in submitting a patch for this to fly?\n. You should rebase on master - there is no plan to drop websockets (http-proxy broke it though, among other issues with it, hence the desire to rewrite it) - but then, little dev happen here these days for lack of time.\nAs for developing on it, you do need to understand (a bit) the nodejs environment - but then I wouldn't expect you would need more than understanding what basic npm command do.\n. Running\n. Probably needs a rebase.\n. Looks good! Thanks a lot!\n. I have mixed feelings...\nBut then if people see this as a valid use-case, ok. I'll review.\n. I would say, go for it - I'll merge a solid/slick version of this :).\n. @willdurand keep up the good work.\n. Minor nitpick - like it otherwise :)\n. Sorry for the delay @willdurand and thanks for pushing this.\nAre the tests still failing with redis? (eg: is this a memcache only problem)\nThanks again!\n. Ok... Let's merge this then, at least until I have time enough to drill to the reason and re-enable the tests.\nThanks again!\n. I think the build is ok. A couple of notes inline.\n. I haven't read your blog post - but node 0.10 indeed was terrible as far as TLS was concerned...\n. I like it - as long as node 0.11 is sufficiently tested, I would be happy to merged this in place.\n. Mmmm... this is my hand writing :-) -> https://github.com/hipache/hipache/blob/c32e3d2138590cbdb70a67e696c5f8252f7143dc/.travis.yml#L3\nNow, it was quite some time ago, so it's worth trying again.\nLet's start with that: is it stable against the test suite?\n. Yep\n. LGTM - thanks!\n. Thanks a lot!\nLet travis build and merged this.\n. Merged :)\n. That's probably worth bringing up to upstream.\n. I can't make promises... my schedule is unfortunately very busy these days :(\n... but I will try!\n. @thaJeztah @willdurand a good first step would be a simplified way (extracted from the tests) to trigger the segfault.\n. @willdurand did you try IO yet?\n. Yeah.\n. @willdurand keep up the great work.\nI'll catch-up reviewing the PRs.\n. Agreed.\n. I guess we can still name it 0.4 - I'm going to move remaining issues to the next release though.\n. IIRC starting with 0.3 we did follow semver, and used PATCH for what it is.\nSo, I'd be happy releasing 0.4 soon with the bunch of new features that are in (then 0.5, or maybe indeed 1.0 if we have enough meat).\n. Agreed again :).\n. I know :) (and bonne ann\u00e9e \u00e0 toi aussi!)\nI'll need a couple more days though, sorry about that.\n. LGTM\n. You are now overriding the travis install step - which does npm install IIRC.\n. Actually, I'm seriously considering moving to circleci.\n. @willdurand told you I was growing useless here :-) - am too slow and too old for this!\n. If it was failing properly when there is a linting error, yes.\n. Actually, it should... https://github.com/hipache/hipache/blame/master/gulpfile.js#L26\n. Well, not passing style checks should break the build IMHO. Maybe I'm a bit fascist on this but well :)\n. You would bypass these special cases like that: https://github.com/hipache/hipache/blob/a3ab4a8f149c7a0b418c1dabf64149632454eba2/test/functional/http.js#L252\nFor the camelcase one, /*jshint camelcase:false*/ would do (valid for a block) - so, yeah, let's restore failing the build on style issues, and fix these issues :)\n. ... and our style is defined :-)\nhttps://github.com/hipache/hipache/blob/master/.jshintrc\n. It's in!\n. \u2764\n. Rad!\n. Thanks @fanatic for all the good work in writing this - and thanks @willdurand for the final push.\n. Etc tests never ran on travis. Should figure out a way to install and configure etcd.\nAbout memcache, I would like to investigate but we can live with them not being on for now.\n. \\o/\n. One minor nitpick about secureOptions, otherwise LGTM!\n. LGTM\n. Ok.\n. Ok.\n. Mmm, I see the discussion over there.\nI'm reluctant to that kind of thing in the core - would love to see rather a clean middleware extension mechanism...\n. Disable them all if you don't have any installed.\neg:  NO_MEMCACHED=true NO_ETCD=true NO_ZOOKEEPER=true npm test\n. etcd and memcached should be fixed by the PR I submitted.\nI'm looking into zk now.\n. Merged, thanks!\n. Restarted. You should be able to trigger a rebuild by just git push -f it.\n. LGTM otherwise! Nice to see interest in these.\n. What about a CONTRIBUTING.md document where these (and other dev infos) would be put, instead of into the general README doc?\n. +1 on @willdurand suggestion\n. Merged then :)\n. See my comment over there.\nThis points to a bug in either the redis driver or the test itself - either way that should be fixed, not removed.\nBottom-line: this should fail connecting to redis and be caught silently, like it does for the other tests.\n(and, ah, mocks are evil :))\n. Ok, let's get to the bottom of this - since I can't reproduce.\nCan you please copy the output of these:\nnode --version\nredis-server --version\ncd your_hipache_checkout; npm list | grep redis\nps aux | grep redis\n. Bottom-line: they don't rely on a running redis server. They should run without (and they do for me).\nI'll try on a random ubuntu box from scratch and see what goes.\n. Fixed\n. Given this: https://github.com/hipache/hipache/blob/master/test/unit/factory.js#L31\nit was meant so that errors are caught.\nThe way the test is written is possibly faulty (maybe the object has been collected when the driver emits?) - but it should be fixed, not removed.\n. Instead, can you try to insert here:\nhttps://github.com/hipache/hipache/blob/master/lib/drivers/redis.js#L26\nthe error handler declaration from below?\neg, move this below at line 26:\nclient.on('error', function (err) {\n            // Re-emit unspecified error as is\n            this.emit(this.ERROR, new DriverError(DriverError.UNSPECIFIED, err));\n        }.bind(this));\n. Fixed\n. Fixed bogus memcached path handling as well.\n. @willdurand PTAL\nSpecifically https://github.com/hipache/hipache/commit/7d3e0044d06edbe9adf6937bebeeff38d8e4fecd\n. @willdurand @msabramo additionally should finally fix #198 as well.\nBottom-line: you were right - redis is erroring out twice, in the case where:\n- redis server is not running\n- an explicit database is specified (first erroring on createClient then on select, which are done sequentially)\n. It's merged.\nZookeeper still needs to be tamed.\n. Love it! (just one comment about the zookeeper modification).\n. @willdurand @msabramo care to review #203? :)\n. Ok. A better way would be to let functional tests be run against whatever backend specified, but let's start with that.\n. Duplicates part of #203:\nhttps://github.com/hipache/hipache/pull/203/files#diff-a8b480116e174a3be08a289d738e6ba7R37\n. Fixed by #203\n. Yeah, java stuff... timeouts are not a proper solution - need to figure out of to proper quit this.\n. - it seems random - sometimes the java process is gone, sometimes it's still there.\n. IIRC it did break the tests - but maybe it's worth another try.\n. @msabramo @willdurand https://github.com/hipache/hipache/issues/107\nWould love to see something like this happen. People (including you :-)) want extensibility / additional features - and I want the \"core\" to stay simple.\n. Being compatible with the ecosystem is a plus - but not at the cost of either performance or complexity.\nMaybe it's possible to implement some very lightweight mechanism of our own?\nEither way, looking into what connect does is definitely a good idea.\n. @msabramo sorry for the delay - can this wait a bit more? I'll get back to Hipache as soon as possible (including a long due release to make @willdurand happy :-))\n. Thanks!\n. Thanks!\n. I've been holding of the release for too long, agreed.\nYou should be pretty happy with the master currently indeed.\n. @Starefossen I'm not sure I understand what you mean by \"the test suite require the dbs to live on the host\".\nAlso, you can selectively disable any backend you don't want to run tests against using env vars (eg: NO_ETCD etc)\n. Then the db servers can / should live in the container, not in the host.\nYou should be able to write a simple Dockerfile with all the required servers bundled. For example, here is what we do for Travis: https://github.com/hipache/hipache/blob/master/.travis.yml\nAlternatively, indeed you should disable most alternative db tests (save redis probably).\n. 0.12 is untested so far.\nI would recommend you stick with 0.10 until 0.12 received proper testing.\nThanks for this, though, it will help!\n. Yes, this is quite likely a nodejs behavior.\nNot sure what can be done here (beside rewriting part of node core http code...).\n. Ok, a couple of extra points here:\n- last I checked node-0.12 was segfaulting on some of our tests - unfortunately, the upgrade is not going to be painless\n- getting rid of http-proxy is something I'd love to do - but I really lack time to work on this\n- I understand it's painful for you - though, if your c++ client is expecting CamelCase headers, maybe you can hack hipache/http-proxy to re-normalize the headers that way? otherwise, yeah, ditch that c++ client and rewrite it.......\n. How was this installed?\n. Thanks for the work on this guys.\nSorry for I sucked so much at releasing lately.\nWill take a look ASAP.\n. > Wait a second has this become deprecated because the maintainers cant be bothered\nLack of time is the main issue. Interests shifting, as well.\n\nor is there a technical reason for it? The project has 16 pull requests and over 2000 stars why just give up on the project I don't understand.\n\nThe reason above ^ - in that context, not acknowledging that, while there are great alternatives, would not be very responsible.. #10 is implemented by #128, along with the ability to have different certificates per-ip. I'm also allowing passphrases for keys, and additional CA.\nSNI will follow soon (but that's a different horse).\nHere's the suggested (new) syntax:\nSimple syntax:\nhttp: {\n // if not specified, will use 80\n   port: 1080,\n// If not specified, will bind to 127.0.0.1\n   bind: \"192.168.1.100\"\n}\nMultiple ips binding, with default and non-default port:\nhttp: {\n // if not specified, will use 80\n   port: 1080,\n// If not specified, will bind to 127.0.0.1\n   bind: [\n    // Will bind to general port 1080\n    \"192.168.1.100\",\n    // Override port for this interface\n     {\n        address: \"192.168.1.5\",\n        port: 1082\n     }\n   ]\n}\nFor https, one needs to specify a certificate and key as well, either globally (that will get inherited -same as with port) or per-ip, possibly overriding the global.\nhttps: {\n // if not specified, will use 443\n   port: 1443,\n// Default certificate / key\n   cert: \"path\",\n   key: \"path\",\n   passphrase: \"optional key passphrase\",\n// If not specified, will NOT bind\n   bind: [\n    // Will bind to general port 1443, with general key / cert\n    \"192.168.1.100\",\n    // Override port and key / cert for this interface, along with additional CA certs\n     {\n        address: \"192.168.1.5\",\n        port: 1444,\n        cert: \"other\",\n        key: \"other\",\n        ca: [\"some\", \"other\", \"certs\"]\n     }\n   ]\n}\nAny comments welcome - as this is not yet ready to be merged.\n. Experimental SNI support is testable. Your opinions/tests welcome @ #130 \n. I'm closing this as the O.P. (@tracker1) description is the ability to bind to independent ips with different certificates (which is now live on master).\nSNI is different, and follows on its own ticket / PR (#129 and #130 ).\n. Is this still something wanted in the app?\nOr can we instead suggest that people start the app with a non-root user / non privileged port and instead use setcap / authbind / iptables?\n. @samalba @jpetazzo \nI would love to get your input on this (see my previous comment). Also, from the OS integration/distribution future POV? (not at ease with that part)\n. @jpetazzo Thanks! I'll get priv dropping in the 0.4 timeframe then.\n. Here is the proposed behavior implemented in the PR:\n- if running as root:\n  - if no user key specified in the configuration, scream and drop dead\n  - if one really wants to keep it root, one needs to have a user: \"root\" (or user: 0) in the config file\n  - if there is a user key specified to something else than root, the workers will drop privileges as soon as they bound\n  - if there is a group specified, drop group privileges to its value\n  - if there is no group specified, we will try to drop group privileges as well, to a group named after the user key (that's nginx behavior IIRC)\n- if not running as root, do nothing...\nNote:\n- the master keeps running as root - which may be considered a bad thing but is obviously necessary to pop new workers with the appropriate rights\n- support both names (will block during resolution) and ids\n. It's on master (-> 0.4) (#143).\n. It's on master (-> 0.4) (#143).\n. @emblica This looks very good! I added a section in the documentation to list third-party softwares of interest, and added airfield in it, so that you get more exposure and feedback for it.\n. @siminm were you using tsuru package?\n. Confirmed.\nLooking into it.\n. @samalba web-sockets heads-up: https://github.com/nodejitsu/node-http-proxy/issues/612 - we should likely freeze to http-proxy-1.0.2 until they sort things out\n. Hi @hansent \nI tested with Docker version 0.8.1, build a1598d1 and it's WFM. As @vieux pointed out, we use ADD, and it's working as expected now.\nI'm closing this - if you feel the problem is not addressed, please reopen.\nAnd thanks for your time reporting this!\n. @dgryski Thanks for your report!\n@samalba do you plan working on that, or would you consider http 1.1 being mandatory for use with Hipache (in which case I'll update the documentation)?\n. Hi @ubermuda,\nFirst, thank you for your work on this.\nAs you expressed the desire for some code review: I wouldn't use relative/absolute paths in the config file, but only module names. npm installed modules in the hipache folder itself (or globally installed) should be accessible by their name without having to mind about path resolution. Now, if you would really like to resolve module paths relatively to the config file, that should be easy (use nodejs path.resolve just after hipache loads the config file).\nApart from that, your code looks ok to me.\nNow, about the opportunity of this:\n- in most cases, people interested in middlewares seem to not really need hipache, but rather just http-proxy, which does that very well - makes me question if there is a point for us in supporting full-fledged middlewares\n- although, some kind of middleware might be interesting to add to hipache - I just:\n  - want a clear and clean architecture to create these middleware and share them\n  - want a safeguarded API that prevents middleware from killing the performance\n  - not sure yet if declarative (eg: config) is better here than API (make Hipache more usable as a library)\nIn any case, we are currently refactoring quite a lot of code, and are considering removing http-proxy as a dependency - so, middleware thinking (and possible implementation) will have to wait a bit (likely Hipache 0.4). If you are still interested at that time, I would be happy to include your ideas / code by then. About this PR specifically, as (unfortunately) the code is bit-rotten, I'll close it.\nThanks again for your contribution here.\nBest.\n. Hi @andrevdm \nIf I understand your use case correctly, you are looking for a middleware able to alter the lookup mechanism, right?\nWe are currently refactoring a lot of code right now, with a focus on memory footprint and performance, so, unfortunately, your PR bit-rot. Also, @samalba argument still stand... hence I am going to close this.\nNow, if the time is not yet there for implementation, ideas are definitely welcome on #107 !\nThanks again for your interest\n. @hanwoody which version of hipache are you running? Can you share your hipache configuration (including vhost definitions in redis?)? Also, did you try to curl from the machine/container running hipache?\nThanks.\n. @jamescarr \n\nI noticed is that if a backend is dead but still active in the backend list we get a few timed out requests\n\nDo you mean: \"a few timed-out requests on the dead backend\", or on another, alive backend?\n@drefined from your log file it seems that retryOnError is working as expected (trying first with backend 1, then trying again with backend 0, finally dropping it as the retry limit is reached - am I missing something?\nThanks!\n. @drefined @jamescarr \nWe now have tests for failing backends: https://github.com/dotcloud/hipache/blob/master/test/functional/http.js#L246 that demonstrate a failing backend is hit once, then valid backends are used.\nThis makes me think that whatever the problem for you was, it was fixed on master, which has changed significantly over the last few months.\nAs we are about to release 0.3 from master very soon now, I'm going to close this ticket.\nIf you feel I shouldn't have, that the symptoms are still here, or that 0.3 doesn't address your problems, please reopen it!\nThanks a lot for your time reporting this.\nBest.\n. Hi @jamescarr \nThanks for your interest and contribution!\nUnfortunately your PR bit-rot (we changed quite a lot of code recently), so, if you would really like this to get in, I guess you need to start over (drivers have been introduced so redis is no longer the only one, etc) - and which is why I'm closing this PR for now.\nBest.\n. Several new versions got generated since then, on the 0.2 branch - and 0.3 should be out the door soon hopefully!\nClosing this.\n. #77 was a duplicate of this ticket, and that was fixed with this commit: https://github.com/dotcloud/hipache/commit/2f93072e8326203b67293161cb1692625573fe2a\n. That should be fixed with the upcoming Hipache version (which requires nodejs 0.10 and has been updated to use redis 0.10).\nYou can check against the master if you can use it - otherwise the new version will be there soon fortunately!\nThanks!\n. I'm closing this then - if something is still wrong, please say so!\nThanks for the report.\n. Hi Adrianbro,\nWhat long-polling technique do you have in mind? BOSH? What server component backend do you plan to use?\nThanks.\n. I'm working on a pluggable datastore architecture right now. So, am looking into supporting etcd and/or memcached additionally to mainline redis.\nWould you guys recommend a specifc etcd node client over another (as there is at least three different: https://www.npmjs.org/search?q=etcd)?\nThanks!\n. Being unable to return asynchronously is a problem indeed... (that means all certificates need to be read beforehand)\nNow:\nhttps://github.com/joyent/node/commit/048e0e77e0c341407ecea364cbe26c8f77be48b8\n... if I read this correctly, the signature of the method changed, and next-node will allow asynchronous SNICallback.\nAlso, maybe it's worth investigating reading from the socket directly to get the servername indication without resorting to node TLS lib? Sounds hackish, but that really would be better to be able to read the key at request time.\nWhat do you think?\n. Hi @morpheu \nUnfortunately, this bitrot. Also, @jpetazzo 's remarks would need to be addressed before this could be merged. Finally, I'm wondering if node-syslog is the right choice here (what about syslog instead? - https://github.com/cloudhead/node-syslog). What about rethinking hipache logging entirely? What would we like to support if we were to (apart from file and syslog)?\nStill, I think this is interesting, and would certainly welcome such a PR during the 0.4 development timeframe - so, while I'm going to close this specific PR for now, I would definitely review a new one that would address the issues here. \nThanks for your contribution anyhow.\nBest.\n. Hi Nicolas,\nUnfortunately, it's very hard to provide any useful advice without detailed technical information (measures, logs, etc).\n- what do you call \"very bad performance\"? Based on what metric?\n- what makes you think your problem is with hipache (and not, say, with your JEE WebApp)?\n- which platform/version do you use?\n- what kind of hosting is that?\n- what's your configuration?\nUnless you can pinpoint the exact problem so that it can be reproduced simply, you might have more luck having someone look into your setup instead.\nSorry not to be more helpful...\n. Hi @zepouet \nDid you manage to get more informations about your problem so we can help?\nBest\n. Hi,\nDoesn't using a different database (which is possible right now using redisDatabase in your config file) cover your case? (eg: avoid conflict with other data)\nThanks!\n. :-)\nIs that a matter of personal preference, then?\nOr are there strong arguments for not using redis databases to segregate data?\nThanks again for your input on this! \n. Fixed by #89 \n. Rats - pushed too much on this branch :/\n. My bad. Let me fix it.\n. Hi.\nYou then need to rename the existing jshint.json into .jshintrc. (and modify package.json as well)\n. Thanks for your efforts on this!\nEnhancing wildcard matching indeed is long due.\nNow, I would really prefer something that really fix the root of the problem (eg: properly lookup .every.level.domain.com, .level.domain.com, *.domain.com when requesting foo.every.level.domain.com), with no additional configuration flags - sure, that would require more work and is tricky to do right (without too much performance penalty).\n@samalba what do you think?\n. @ksikka @bjyoungblood (and others interested): what's the real life, max-depth domain you guys would use?\nfoo.bar.domain.com?\nfoo.bar.baz.domain.com?\nLonger?\n. LGTM\nThanks a lot for your work!\n. Fixed by #89\n. Fixed by #89\n. Yeah, the build broke - this is because of some bizarre overlap between multiple spawned redis instances on travis - will fix that.\n. About the branch name: I'll explain with a drink :-)\nAbout Travis: I want a divorce!\nAbout the python tests: something is time sensitive - I get about one out of ten tests runs failing here - apparently more on Travis. I suspect some race between Redis TTL and tests runs.\nDo you mind if I migrate the python tests entirely into mocha? (when the mocha tests will be solid enough)\n. FYI, I hooked in Code Climate. I kind of like that stuff - granted, it's not much more than jshint in bootstrap clothes, but I dig the colors and big icons :-).\nHere: https://codeclimate.com/github/dotcloud/hipache/compare/la-jolla - it's telling us:\n- that the overall project quality got up 0.4 points on that branch\n- that I missed a radix parameter on a parseInt\n- that we should really get to cleanup worker.js :-) (https://codeclimate.com/github/dotcloud/hipache/code)\n. We are now also testing functional with mocha. Most python tests have been duped in there (save test_parseerror).\nWe will be able to tests https and web sockets very soon!\n. Let's merge then release 0.3 :-)\nI'm out for two/three days, so, early monday (from 9AM PST on) would be good for me for a release so I can react fast if something is broken.\n. In case you don't have one already, here is a helper script that takes care of versioning / tagging / npm publishing: https://gist.github.com/dmp42/9932454\n. Still have some intermittently failing python tests. I'll look into it.\n. Hi @jkingyens \nWhat I refer to in this \"ticket\" is a different issue -\nbut in your case if Hipache is crashing, this needs to be looked into.\nInterestingly, the ip you list here (169.254.169.254) is used by amazon to distribute metadata.\nCan you copy the entire output before the crash (inc. possible stacktrace)? Also, node and npm version, hipache version, and hipache configuration file?\nThanks a lot!\n. @jkingyens Glad to hear you got it fixed! Was it related to Hipache configuration? Or to the Google Compute Engine configuration?\n. Nice!\nThanks a lot for these!\n. > I will try to improve upon this at some point!\nI'm not sure yet what form these CLI will look like, and I won't work on them immediately, so please do!\nand definitely add any idea / wish / remark in here, they are welcome.\nBest.\n- Olivier\n. Just so that things are clear: this includes the TLS overhead - eg: this is the number of bytes written on the socket, not the http payload size.\n. Hello @cpuguy83 \nThanks for your report.\nI will need more details in order to diagnose this.\nI assume your session token is a cookie?\nCan you show your hipache config, and dump your redis backend information?\nAlso, it would really help if you would be able to capture http requests:\n- between client and your server app (successful)\n- between client and hipache (unsuccessful)\n- and ideally between hipache and your server app (unsuccessful)\nThanks a lot!\n. Just in case: are you using Hipache in front of two instances of your app (and they don't share session data?)\n. Ok - there is only one backend, right? Can you list it as is in redis?\nThen let's look into the http requests.\n. So, that's just the frontend and identifier, right? Can you list the entire output of redis-cli lrange frontend:emp.envuplay.com 0 -1?\nThanks!\n. Ok  - for the http requests, can you try getting them? (you can use firebug to dump the requests + answers on the client side)\n. Thanks! That's the successful connection I assume (straight to nginx)?\nCan you play the same thing with hipache in front, reproducing the problem? Sorry for this rather tedious process, but there really is no other way...\n. Ok. Are you able to trace what's happening between the hipache server and your backend?\nSomething in the line of: sudo tcpdump -lnni eth0 -w dump.txt -s 65535 host emp-front.emp.prod.envu.io and port 8080\n(adjust eth0 to the appropriate network interface)\n. Great. I'll look into it asap. Though, it looks like the transaction is forwarded correctly back and forth by hipache...\nMore later.\n. @cpuguy83 can you try running hipache on http (instead of https) and see what it does?\n@samalba from the dump here: https://www.dropbox.com/s/fz3svcgcraqe5ae/dump.txt - any idea why x-forwarded-proto and x-forwarded-protocol are different (eg: https vs. http)? (being hosted on dotcloud, I assume there is an hipache in front of his hipache? but why would there be mixed-up http/https here?)\n. Good!\nHappy it helped.\n. @shin- also, can you tcpdump the request so we had it to the tests?\n. @shin-  gist blog too big\nhttps://gist.githubusercontent.com/anonymous/9933561/raw/8d79cce80a164216f1acb3f029d3aece85694e57/gistfile1.txt\nCan you use dropbox or spideroak (or whatever else), and use these options:\ntcpdump -lnni eth0 -w bugdump -s 65535 host HOSTNAME and port 80 (<- adjust eth0, HOSTNAME and port)\n. @samalba maybe we should get to the bottom of this before releasing 0.3 - what do you think?\n. Ok - I pushed the milestone date to next week.\nI'll start looking into it tomorrow (I really want a test for that as a starter) - ping me if you have any update/idea meanwhile.\n@shin- thanks for the dump\n. It certainly is partly our fault.\nWith Hipache I hit the memory wall after only 50MB uploaded (!).\nIf replacing worker.run by this:\n```\nvar http = require('http'),\n    httpProxy = require('http-proxy');\nvar proxy = httpProxy.createProxyServer({});\nvar server = require('http').createServer(function(req, res) {\n        console.warn('proxying');\n        proxy.web(req, res, { target: 'http://localhost:1000', xfwd: false });\n});\nserver.listen(1080);\nvar tick = function () {\n        var currentMemory = process.memoryUsage().rss;\n        console.warn(Math.round(currentMemory / (1024 * 1024)));\n};\nsetInterval(tick, 500);\n```\n... I can get to about 1.3GB upload without hitting the roof.\nTrue, it's still leaking, but way more slowly.\nI went deep into http-proxy, it's likely not their fault - more likely a core node issue (http request and/or stream and/or ee) :/\nIMHO: we can start by making the situation better, by plugging our leaks (but that likely requires ripping worker apart pretty hard) - but then if we want to fix the root of that crap, I'm afraid we will have to hatch nodejs core library...\n@samalba what's your call? rewrite worker now and reduce our leaks?\n. I tried it quickly - it seems quite limited...Nodejs instrumentation sucks (or I suck at finding the proper tools...)\n. I had to pinch myself and quadruple check, but incredibly, read-installed consumes 30 to 40 MB of memory per-worker (that's doubling the memory a cold start worker consumes otherwise). read-installed is not responsible for the leak though - it's just plain f silly as the only use of it is to get hipache version itself.\nI'll PR the removal of it independently.\n. Heads-up!\nTo get there I rewrote http-proxy (the good parts) and ripped-up worker to bare bones - with pretty much the same results (save PR #106).\nThe bottom line is: try running hipache with node --expose-gc --always-compact, then modify memoryMonitor so that you call gc() inside tick. This will force garbage collection before sampling memory.\nThe result is that we do leak (actually, we do, but node does as well) - just, it is very slow: after a 1GB upload, I barely hit 70MB (cold-starting at 60MB).\nAlso, we never get over ~300MB because node does trigger GC auto-magically in that case (and release). This is not a logarithmic leak.\nThe solution to this as a whole depends on how we see things:\n- plan A: \"don't know, don't care\" - accept this is a GC language: then don't bother monitoring (small amounts of) memory (just prevent it from going over, say, 500MB), and let node do its thing\n- plan B: \"old-school tight arse\" - recommend that people launch node with --expose-gc --nouse-idle-notification, and call GC explicitly ourselves - and monitor memory like we do now, including chasing actual leaks \nWe can certainly find a middle-ground: if automatic garbage-collection is disabled from the command-line (like we could recommend), then memoryMonitor gets into tight-arse mode and takes charge, trigger gc on its own, and mercilessly kill worker above 100MB - otherwise, we just warn the user and up the memory monitor limit to 500MB.\nSecond post here is interesting: https://groups.google.com/forum/#!topic/nodejs/BO6JdYi4n2k\nAnd more background over there:\n- http://blog.caustik.com/2012/04/08/scaling-node-js-to-100k-concurrent-connections/\n- http://blog.caustik.com/2012/04/11/escape-the-1-4gb-v8-heap-limit-in-node-js/\n.  Agreed on all (yes, it does work in the shebang).\nWe kind of opened the pandora box here anyhow, by fiddling with the way node handles memory...\nRight now, I can't think of anything better... Though, I'm worried about the perf impact of explicit stop-the-world gc calls - how long will these calls take under prod stress? (and why on earth do we need to do that in the first place?)\nI'll continue digging further into node http library.\n. About nodejs iddle garbage collection (that got removed in 0.11...): https://github.com/joyent/node/commit/d607d85\nAbout nodejs gc generally: http://www.tuicool.com/articles/zUnMby - mind the advice:\n--expose-gc\nThis option will allow you to call the gc() function within your javascript and trigger a\nmanual garbage collection. This is an expensive garbage collection and will stop your world. \nUse with caution, or better yet, don\u2019t use.\nUpdate: I'm not sayinggc() is not the way to go - just that we need to tread carefully.\n. Anything is better than what we had :-)\nI'm a bit concerned with people stating that gc can take up to 3 to 4 seconds though - either way, we will tweak that up eventually, so, let's go with that!\n. With heartbleed's sh storm, I won't be able to check before late tomorrow, so, don't wait for me if you need it in very quickly.\n. Should we close this?\nTrue, we haven't been deep enough (yet) into node's lair to understand why it's not freeing immediately, but at least the symptoms are fixed thanks to @samalba 's patches.\n. Hi @ruudud \nThanks a lot for this!\nWe will definitely look into this - but that will have to wait a bit.\nAs we will soon release a new version with some quite big changes, we need to focus on bugs redux for now.\nWill get back to this soon, hopefully.\nThanks again.\n. @samalba would you look into that one? If it's ok for you, I would rather have it merged before I refactor worker as it will be harder later on.\n. @ruudud Let's get this in then ;)\n. Apart from my nitpicking, this looks good.\nAdmittedly, we lack documentation on the tooling I commited, but you should:\n- run gulp hint once in a while to see if the linter is happy\n- or alternatively run gulp hack:hipache in a console alongside your editor to have it run live while your work (will run both the linter and unit tests)\nand I should really move to fix the build on travis now :/\n. @samalba I tested against my own hipache droplet - this does prevent the worker from getting killed, but obviously doesn't fix the leak (memory consumption is still skyrocketing).\nAgreed for the rest: lets merge, issue 0.2.5, and gut worker.js for 0.3.\n. @samalba I finally managed to have a test for that in the test suite (PR ASAP): PUTting 100MB from /dev/urandom which does (fortunately) show that your workaround works ok.\nThat will make it way easier to hunt down the leaks.\nBtw: any idea why the memory consumption is logarithmic?\n. Short answer is \"they don't\" - but I'm reluctant to go into possibly breaking changes with websockets without any ws tests. Depending on the outcome of our memory hogging problem, I may push this to 0.4 instead.\n. Note to self: current version 1.1.1 brings nothing over what we use (1.0.2) \n. Unfortunately the bug is in node core, and not in http-proxy - their \"patch\" brushes the underlying node problem under the rug, but breaks subtly in a number of ways (that they don't test).\nI'll look into this and see if I can find something satisfactory.\n. @cywjackson we can discuss this at #141 - thanks a lot for your report\n. The travis build failing is expected (now) - as soon as we merge with the new tolerant memorymonitor, the failing test should pass. \n. \\o/ travis is green at last!!! \\o/\n. I porked it, it seems... :/\nProper PR in #112\n. Sounds good to me! Let's rock it! :+1: \n. http://www.joyent.com/blog/walmart-node-js-memory-leak\nhttps://github.com/davepacheco/node-stackvis\nhttp://stackoverflow.com/questions/19595184/nodejs-application-with-memory-leak-where-is-it\n. And the failing test on travis is resolved by downgrading http-proxy to 1.0.2 (see PR #103)\n. Thanks a lot.\nWill look into this ASAP.\n. Indeed #53 broke it (should have used git+https I think, which is now committed on the branch).\n@samalba can we release 0.2.7 ASAP? Also, maybe we should rename that branch to 0.2.\n. @samalba it seems you overwrote my commit while renaming the branch: https://github.com/dotcloud/hipache/commits/0.2\nFixing it.\n. Fixed.\nInstall tested npm install git://github.com/dotcloud/hipache.git#0.2.\nGood to go ;)\n. npm install hipache now WFM\n. It is env that sucks and doesn't split the argument (in some setups - it's working ok on OSX) .\nMaybe we can fallback on sh and exec (if feasible)?\n. There:\n```\n!/bin/sh\n':' //; exec node --expose-gc --always-compact -- \"$0\" ${1+\"$@\"}\n```\nThe tricky part is to get the javascript runtime and sh to parse that the way we want.\nThe question remains, though: do we care about using sh vs. env?\n. @shin- are you talking about the warning you list below? (node) warning: possible EventEmitter memory leak detected. 11 listeners added. Use emitter.setMaxListeners() to increase limit.\nIf so, it's an indication that we might be leaking (many listeners getting stacked on an EventEmitter), not that node has a problem.\n. > @dmp42 I don't think there is a need for a \"$0\" ${1+\"$@\"}\nAgreed.\n. Let's get this in :+1: \n. I'm doing a couple of tests and then we can npm publish - I'll ping you in a minute.\n. @samalba meanwhile, can you backport the changes to master? \n. I wasn't clear enough either - should have gist-ed in the first place!\nI'll get better at that ;)\n. Looks ok for npm publish!\n. \\o/\n. Hi @stefanfoulis \nHipache marks a backend as dead when it returns an error code (>= 500/501) save the case it returns a 503 along with a retry-after header (which would indicate maintenance).\nhttps://github.com/dotcloud/hipache/blob/master/lib/worker.js#L294\nWhat happens when a backend is marked as dead is, if passiveCheck is enabled, it adds en entry into redis indicating that this specific backend is dead:\nhttps://github.com/dotcloud/hipache/blob/master/lib/drivers/redis.js#L120\nThat redis entry will expire after some time (the deadBackendTTL config setting).\nTo answer your question specifically: under no circumstance a dead:<domain-name> entry can be created by Hipache. It is always per-backend.\nSo:\n- are you using hipache-hchecker alongside hipache?\n- can you copy your current hipache version and config file?\n- can you dump your redis config for that domain?\n- are you positive the entry was exactly dead:<name> without anything else?\nThanks!\n. > Why is there a global dead: entry for the whole domain? Wouldn't it make more sense to have dead: entries for individual backends?\nThat was a performance choice to reduce the number of round-trips to redis.\nWhat we have:\n- lookup backends list and dead backends list for the domain in one trip\n- pick a valid backend\nThat's 1 redis trip per http request.\nWhat we would have with your suggestion:\n- lookup backend list\n- pick a backend\n- check if the backend is alive\n- rinse and repeat steps 2 and 3 until we have a valid backend\nThat's (2 + K / N) redis trips per http request, where K is the number of dead backends and N the total number of backends for that domain.\nWether or not this is significant performance-wise is up for debate - but that was the main argument at the time.\nAs for your problem, unfortunately, I'll need more than that to diagnose it - specifically, the content of \"dead:www.example-domain.com\".\nIf this is not a production system, may I ask you that you add an extra (bogus) backend to your domain, and do a dozen of queries on it, so that the bogus backend gets hit and create an entry in redis that you will be able to list?\nAs for entirely disabling hipache handling of dead backends, I don't think there is a simple way to do that right now (with the Redis driver at least) - I'll consider it for future releases though.\nAlthough, you can achieve it by:\n- writing a very simple script that writes a \"hchecker_ping\" entry in redis every 20 seconds (containing the timestamp) - that will actually prevent Hipache from populating dead entries\n- maybe use hipache-checker in dry-run mode (not sure about that - @samalba will have to confirm)\n. About deadBackendon500 it will actually prevent backends returning 500 to be considered as dead - but that's just for 500, not 50x...\n. With your actual config (deadBackendTTL: 0.5) the \"dead\" backend entries will stay just for half a second - this might be too short a time window for you to get one.\nEither way, there are tests on master now for failing backends (https://github.com/dotcloud/hipache/blob/master/test/functional/http.js#L287) - though they are a bit messy right now.\nI'm confident that hipache on master is working as it should - now, you are certainly running it from npm, so if you would like to investigate a bit more on this before we close this ticket, just to be sure it's working ok, that would be cool.\nbtw, I just found out an alternative hchecker implementation written for node (here: https://github.com/runnable/hipcheck)\n. Hi @tjmehta \nThanks for this!\nHappy to see interest in Hipache, and an alternative health checker - I will definitely take a closer look at it later next week!\nAbout this PR:\n- @samalba 's Hipache Health Checker is not written in python, but in go - can you change that in your PR?\n- nevermind the failing travis build for now - not your fault\n. @tjmehta you guys at runnable are using Hipache?\n. Nice! :-)\n. I know you from Musicbrainz, don't I? (my nick is dmppanda there)\nI'll take a look at this tomorrow - thanks for the report!\n. @warpr what version of hipache are you using? stock from npm?\n. @warpr one thing is sure - \"address\": [\"0.0.0.0\", \"::1\"] is the syntax for master (upcoming 0.3) - and that changed. The syntax for your (npm) version, is visible on the 0.2 branch: https://github.com/dotcloud/hipache/tree/0.2\nHaving said that, have a look here: https://github.com/dotcloud/hipache/pull/100\n... and you might try master indeed :+1: \nTell me if that helps.\nBest.\n. @warpr let me know if anything new here - I let you close this if master is working for you then. \n. LGTM - feels good to see travis happy again as well! \n. @samalba ok, let me know then if you want more things to get into the release, and/or when you want to release it ;)\n. Ok! package.json reads 0.3.0 already - I think all is left to do is git tag & npm publish :-) (I don't think I have the rights to do it? sharing rights on npm used to be a total pita - not sure it got any better...)\n. It's live.\n. @thaJeztah +1\n. The linked PR does some validation, and paves the way for more.\n. @willdurand agreed in the grand scheme of things :)\nNow, there apparently exists one already here: https://github.com/tsuru/tsuru-deb/tree/master/node-hipache-deb\nand since people are using it and are having issues linked to it (#16) I would love to connect with @morpheu (or someone else maintaining this package)\n. Mmmm... it still failed these past days. Let's keep it open for now.\n. http://googleonlinesecurity.blogspot.fr/2013/11/a-roster-of-tls-cipher-suites-weaknesses.html\nhttp://www.bolet.org/TestSSLServer/\nhttps://www.ssllabs.com/\n. https://github.com/joyent/node/issues/2727\n. \\o/\n. Tests fixed.\nMisses documentation work, but I need #124 to get in first.\n. The legacy class is here to support the previous syntax transparently (map to the new format magically and output a warning).\n. Nice :-)\nI'd like to wait for 0.3 to be released (hint, hint :-)) before merging it anyhow\n. #143 has the merged / cherry picked version of this. \n. @niclashoyer Thanks for the feedback.\nMy main question is roughly: is the use case a setup with a massive number of certificates that gets added / removed often (just like vhost / backends), or is it more of a limited number (dozen) certificates that could be loaded / handled on start.\nFirst scenario means we really need asynchronous SNI - second scenario we can live with synchronous SNI (as upstream).\nI can even implement both - I would like to get a feeling about which scenario is likely / not likely.\nAbout upstreaming my changes, that wouldn't make much sense, because node 0.11 (and upcoming 0.12) completely rewrote http(s) (which was admittedly crappy) and does indeed support asynch SNI - and I doubt they would accept such a (complex / API breaking) change on the current stable / legacy (0.10) branch.\n. Well, the problem is node 0.11 is not exactly reliable (it does crash a lot on our current test suite).\nLet me sleep over this a few days to come up with a nice solution.\nThanks a lot for the feedback!\n. I'll go with a two tier system, where activating dynamic SNI loads https2 while the basic (on boot) support only loads vanilla https.\nThanks for your feedback guys.\n. Hi @Siedrix \nLet me get back to you about this later this week - definitely good to see interest!\n. @hpg4815 I'm not sure what you ask for, but what you describe really looks to me like SNI (the capacity to handle multiple SSL vhosts with different certificates on the same ip) - and yes, that ticket is about adding support for that :-)\nCurrently Hipache doesn't have said capacity (this branch is about adding that feature though).\nPlease note that no other node project has this capacity either (which is why this is a complicated subject), for the reason nodejs doesn't allow asynchronous SNI certificate retrieval (making the certificate looking blocking, which is unacceptable).\nAbout SSO:\n- there is no such feature in Hipache - if authentication there is, it's the responsibility of the backends to implement it (using SSO or otherwise)\n- if you would want hipache to perform some kind of authentication for all backends, then it's a matter of adding a middleware - requires hacking code, but probably not that complicated\n- nodejs doesn't have per-se a notion of SSO (and after-all SSO is just a generic term, not a specific technology, right?)\nAbout SiteMinder / Ping Federate I don't know what they are or what they do :-)\nHope that helps!\n. > This would also allow me to add cert and key files for each hostname/ip binding but would just come with the requirement of restarting Hipache afterwards?\nYes.\n\nwhen the term vhost is used in regards to redis, does this imply one would be able to add all configurations in Redis similar to that of the Apache NameBasedVirtualHost?\n\nYes and no. There is very little you can configure for a specific \"vhost\" with Hipache. Pretty much, all you do is point an \"entry\" domain name to a set of backends, and you don't have more control than that.\nThat being said, hostname are preserved.\nHope that helps!\nBest\n. @stefanfoulis lack of time and motivation, sorry about that... \nI'll look into this again early January as time permits.\n. @stefanfoulis thanks for reaching out and for your proposal. Since I'm a Docker employee, I'll decline your offer - though I would happily review a PR fully implementing this, or finish my own PR if time/motivation permits.\n. @fanatic : great!!! Love it!\nLet me have a look at it and get back to you soon (we need to get 0.3 released first, but this will definitely make it for 0.4).\nAbout skipping the tests (on environments where we don't have / want a zookeeper service), have a look at how it's done for etcd:\n- https://github.com/dotcloud/hipache/blob/master/.travis.yml#L6\n- https://github.com/dotcloud/hipache/blob/master/test/unit/driver-etcd.js#L7\nThanks!\n. Also: can you run gulp hint (or jshint) and make jshint happy on your files?\n. @fanatic I guess zookeeper can be launched (by our fixtures script) specifying command line arguments and/or a custom config file. That config file would then point the zookeeper dataDir to a specific folder (say: /tmp/hipache-test) that we may be able to rm once the tests are finished - at least it wouldn't mess-up with other things on that zookeeper.\nWhat do you think?\n. I have the hope that zkServer is just a (very? simple?) wrapper that ultimately does something in the line of: java org.apache.zookeeper pathtoconfig\nGiven this is for the purpose of running tests available to developers only, I don't quite care if our launcher is not \"very\" portable.\nSo, if you can find a hack, or even commit a copy of zkServer.sh in our fixtures, I'd be ok with that.\nAnd yes, commit the tests' config file as well.\n. Ok, I only have some minor nitpicks on this (see inline), mainly:\n- would love to have a functioning fixture script to start / stop zookeepers servers instances on demand with custom config\n- would love to have travis install zookeeper and run the tests\nApart from that, I'm a bit worried about performance right now - domains.with.lots.of.subdomains will generate lots of queries to ZK. Any idea how fast it is at that?\n. Hi @fanatic \nNo problem for the delay - I was and am busy as well.\nI like what you did quite a lot.\nCan you rebase it on top of master?\n. LGTM\n. Would be happy to merge this once rebased / reviewed again (cc @willdurand)\n. Just adding that we forgot to tag 0.2.9 though we released it on npm :(\nWe really need a safer process.\n. @samalba dmp (https://www.npmjs.org/~dmp)\n. Interesting.\nDo I read you well? This is node http request module which does the _read, not node-http-proxy, right? (the http-proxy module really does little, and I'm thinking about dropping it entirely anyhow)\nAbout how to do deal with this... really need to start with a mocha test that reproduces the scenario.\nThen either dive into node http request or try with an intermediate stream (pipe the request into it and see if we can \"keep\" a copy of the first buffer until we are sure it's not failing).\nNow, this raises another question: what if a working backend fails in the middle of a post? (thinking about the registry, and very huge PUT)\nRight now, I guess we are retrying that as well, and that will fail for the same reason - and there, it's not reasonable to keep the whole payload, is it?\nPut otherwise, I'm tempted to say that we simply shouldn't retry a request that has a payload, as a rule of thumb. Then, we might try to make an exception for the ECONNREFUSED case.\nWhat do you think?\nI'm scheduling this for 0.4.\n. It might very well be http server as well that is responsible for this.\nWhen I see this: https://github.com/joyent/node/blob/v0.10.26/lib/http.js#L127 .........\nI wouldn't blame the streams model (and buffering) too early...\n. Scratch that: only the tests are doing multiple arguments rpush. They need to be fixed though.\n. Hi there - it should be fixed. I published an update to npm.\nThis is unrelated to the use of Ubuntu 14:04, although it's (also) telling you that for 0.3 we changed the syntax for redis driver - you might want to review the new doc and update your config.\nTell me if that helps.\nThanks for catching this!\n. @cywjackson\nFor some reason I can't reproduce the leakage here.\nDo you have a simple test case that we could possibly add to mocha?\nThanks! \n. Thanks a lot for the detailed feedback! It does help.\nhttp-proxy 1.0.3 \"fix\" is not acceptable - as it also kills client-side keep-alive (and websockets).\nI need to do a few tests, and hopefully we can come up with a fix.\n. And I'll be overworked in upcoming weeks, so, don't expect a lightning fast fix :-)\n. Thanks!\n. Thanks\n. Thanks!\n. Thanks!\n. Agreed. Anyone willing to PR it?\n. Happy you figured it out ;)\nClosing this, then.\n. Thanks!\n. Do you mean, is Hipache able to route imap or pop protocols? If so, no, it's an HTTP load-balancer and routing layer.\n. There are no such plans ATM - but throwing in ideas can't hurt.\n. I like the eson approach to allow overriding any config settings via the env.\nDoes go well with docker run way IMO.\n. @qbraksa @alibby @raelmax if you still see this problem, please say so and I will reopen.\nThanks!\n. Thanks!\n. Thanks. Merged.\n. X-FORWARDED-FOR is informative only. Also, given a chain of proxies, there is no way to validate the ip end-to-end.\nWhat problem are you trying to solve?\n. Ok. I guess it boils down to: how do we differentiate between legit proxies setting the X-FORWARDED-FOR header from rogue clients spoofing it?\nEither way, if you want to do something, that should be here: https://github.com/hipache/hipache/blob/master/lib/worker.js#L151\n. Would you be interesting in submitting a patch for this to fly?\n. You should rebase on master - there is no plan to drop websockets (http-proxy broke it though, among other issues with it, hence the desire to rewrite it) - but then, little dev happen here these days for lack of time.\nAs for developing on it, you do need to understand (a bit) the nodejs environment - but then I wouldn't expect you would need more than understanding what basic npm command do.\n. Running\n. Probably needs a rebase.\n. Looks good! Thanks a lot!\n. I have mixed feelings...\nBut then if people see this as a valid use-case, ok. I'll review.\n. I would say, go for it - I'll merge a solid/slick version of this :).\n. @willdurand keep up the good work.\n. Minor nitpick - like it otherwise :)\n. Sorry for the delay @willdurand and thanks for pushing this.\nAre the tests still failing with redis? (eg: is this a memcache only problem)\nThanks again!\n. Ok... Let's merge this then, at least until I have time enough to drill to the reason and re-enable the tests.\nThanks again!\n. I think the build is ok. A couple of notes inline.\n. I haven't read your blog post - but node 0.10 indeed was terrible as far as TLS was concerned...\n. I like it - as long as node 0.11 is sufficiently tested, I would be happy to merged this in place.\n. Mmmm... this is my hand writing :-) -> https://github.com/hipache/hipache/blob/c32e3d2138590cbdb70a67e696c5f8252f7143dc/.travis.yml#L3\nNow, it was quite some time ago, so it's worth trying again.\nLet's start with that: is it stable against the test suite?\n. Yep\n. LGTM - thanks!\n. Thanks a lot!\nLet travis build and merged this.\n. Merged :)\n. That's probably worth bringing up to upstream.\n. I can't make promises... my schedule is unfortunately very busy these days :(\n... but I will try!\n. @thaJeztah @willdurand a good first step would be a simplified way (extracted from the tests) to trigger the segfault.\n. @willdurand did you try IO yet?\n. Yeah.\n. @willdurand keep up the great work.\nI'll catch-up reviewing the PRs.\n. Agreed.\n. I guess we can still name it 0.4 - I'm going to move remaining issues to the next release though.\n. IIRC starting with 0.3 we did follow semver, and used PATCH for what it is.\nSo, I'd be happy releasing 0.4 soon with the bunch of new features that are in (then 0.5, or maybe indeed 1.0 if we have enough meat).\n. Agreed again :).\n. I know :) (and bonne ann\u00e9e \u00e0 toi aussi!)\nI'll need a couple more days though, sorry about that.\n. LGTM\n. You are now overriding the travis install step - which does npm install IIRC.\n. Actually, I'm seriously considering moving to circleci.\n. @willdurand told you I was growing useless here :-) - am too slow and too old for this!\n. If it was failing properly when there is a linting error, yes.\n. Actually, it should... https://github.com/hipache/hipache/blame/master/gulpfile.js#L26\n. Well, not passing style checks should break the build IMHO. Maybe I'm a bit fascist on this but well :)\n. You would bypass these special cases like that: https://github.com/hipache/hipache/blob/a3ab4a8f149c7a0b418c1dabf64149632454eba2/test/functional/http.js#L252\nFor the camelcase one, /*jshint camelcase:false*/ would do (valid for a block) - so, yeah, let's restore failing the build on style issues, and fix these issues :)\n. ... and our style is defined :-)\nhttps://github.com/hipache/hipache/blob/master/.jshintrc\n. It's in!\n. \u2764\n. Rad!\n. Thanks @fanatic for all the good work in writing this - and thanks @willdurand for the final push.\n. Etc tests never ran on travis. Should figure out a way to install and configure etcd.\nAbout memcache, I would like to investigate but we can live with them not being on for now.\n. \\o/\n. One minor nitpick about secureOptions, otherwise LGTM!\n. LGTM\n. Ok.\n. Ok.\n. Mmm, I see the discussion over there.\nI'm reluctant to that kind of thing in the core - would love to see rather a clean middleware extension mechanism...\n. Disable them all if you don't have any installed.\neg:  NO_MEMCACHED=true NO_ETCD=true NO_ZOOKEEPER=true npm test\n. etcd and memcached should be fixed by the PR I submitted.\nI'm looking into zk now.\n. Merged, thanks!\n. Restarted. You should be able to trigger a rebuild by just git push -f it.\n. LGTM otherwise! Nice to see interest in these.\n. What about a CONTRIBUTING.md document where these (and other dev infos) would be put, instead of into the general README doc?\n. +1 on @willdurand suggestion\n. Merged then :)\n. See my comment over there.\nThis points to a bug in either the redis driver or the test itself - either way that should be fixed, not removed.\nBottom-line: this should fail connecting to redis and be caught silently, like it does for the other tests.\n(and, ah, mocks are evil :))\n. Ok, let's get to the bottom of this - since I can't reproduce.\nCan you please copy the output of these:\nnode --version\nredis-server --version\ncd your_hipache_checkout; npm list | grep redis\nps aux | grep redis\n. Bottom-line: they don't rely on a running redis server. They should run without (and they do for me).\nI'll try on a random ubuntu box from scratch and see what goes.\n. Fixed\n. Given this: https://github.com/hipache/hipache/blob/master/test/unit/factory.js#L31\nit was meant so that errors are caught.\nThe way the test is written is possibly faulty (maybe the object has been collected when the driver emits?) - but it should be fixed, not removed.\n. Instead, can you try to insert here:\nhttps://github.com/hipache/hipache/blob/master/lib/drivers/redis.js#L26\nthe error handler declaration from below?\neg, move this below at line 26:\nclient.on('error', function (err) {\n            // Re-emit unspecified error as is\n            this.emit(this.ERROR, new DriverError(DriverError.UNSPECIFIED, err));\n        }.bind(this));\n. Fixed\n. Fixed bogus memcached path handling as well.\n. @willdurand PTAL\nSpecifically https://github.com/hipache/hipache/commit/7d3e0044d06edbe9adf6937bebeeff38d8e4fecd\n. @willdurand @msabramo additionally should finally fix #198 as well.\nBottom-line: you were right - redis is erroring out twice, in the case where:\n- redis server is not running\n- an explicit database is specified (first erroring on createClient then on select, which are done sequentially)\n. It's merged.\nZookeeper still needs to be tamed.\n. Love it! (just one comment about the zookeeper modification).\n. @willdurand @msabramo care to review #203? :)\n. Ok. A better way would be to let functional tests be run against whatever backend specified, but let's start with that.\n. Duplicates part of #203:\nhttps://github.com/hipache/hipache/pull/203/files#diff-a8b480116e174a3be08a289d738e6ba7R37\n. Fixed by #203\n. Yeah, java stuff... timeouts are not a proper solution - need to figure out of to proper quit this.\n. - it seems random - sometimes the java process is gone, sometimes it's still there.\n. IIRC it did break the tests - but maybe it's worth another try.\n. @msabramo @willdurand https://github.com/hipache/hipache/issues/107\nWould love to see something like this happen. People (including you :-)) want extensibility / additional features - and I want the \"core\" to stay simple.\n. Being compatible with the ecosystem is a plus - but not at the cost of either performance or complexity.\nMaybe it's possible to implement some very lightweight mechanism of our own?\nEither way, looking into what connect does is definitely a good idea.\n. @msabramo sorry for the delay - can this wait a bit more? I'll get back to Hipache as soon as possible (including a long due release to make @willdurand happy :-))\n. Thanks!\n. Thanks!\n. I've been holding of the release for too long, agreed.\nYou should be pretty happy with the master currently indeed.\n. @Starefossen I'm not sure I understand what you mean by \"the test suite require the dbs to live on the host\".\nAlso, you can selectively disable any backend you don't want to run tests against using env vars (eg: NO_ETCD etc)\n. Then the db servers can / should live in the container, not in the host.\nYou should be able to write a simple Dockerfile with all the required servers bundled. For example, here is what we do for Travis: https://github.com/hipache/hipache/blob/master/.travis.yml\nAlternatively, indeed you should disable most alternative db tests (save redis probably).\n. 0.12 is untested so far.\nI would recommend you stick with 0.10 until 0.12 received proper testing.\nThanks for this, though, it will help!\n. Yes, this is quite likely a nodejs behavior.\nNot sure what can be done here (beside rewriting part of node core http code...).\n. Ok, a couple of extra points here:\n- last I checked node-0.12 was segfaulting on some of our tests - unfortunately, the upgrade is not going to be painless\n- getting rid of http-proxy is something I'd love to do - but I really lack time to work on this\n- I understand it's painful for you - though, if your c++ client is expecting CamelCase headers, maybe you can hack hipache/http-proxy to re-normalize the headers that way? otherwise, yeah, ditch that c++ client and rewrite it.......\n. How was this installed?\n. Thanks for the work on this guys.\nSorry for I sucked so much at releasing lately.\nWill take a look ASAP.\n. > Wait a second has this become deprecated because the maintainers cant be bothered\nLack of time is the main issue. Interests shifting, as well.\n\nor is there a technical reason for it? The project has 16 pull requests and over 2000 stars why just give up on the project I don't understand.\n\nThe reason above ^ - in that context, not acknowledging that, while there are great alternatives, would not be very responsible.. ",
    "mattparlane": "It seems to have added both commits here sorry, I guess I branched after the first commit rather than before. I can redo if that's a problem for you.\n. On Wed, Sep 26, 2012 at 9:32 AM, Samuel Alba notifications@github.com wrote:\n\nthis changeset looks good, let me do some tests first since it adds\nanother lookup for each request.\n\nI did wonder about that, but I figured that if that cache was being\nused (I am using it) then that would mitigate the extra workload.\nMatt\n. Hi again. Sorry to hassle, any chance of pulling this? I've been using it in production for several months and it's going well. No measurable impact on performance either.\n. It seems to have added both commits here sorry, I guess I branched after the first commit rather than before. I can redo if that's a problem for you.\n. On Wed, Sep 26, 2012 at 9:32 AM, Samuel Alba notifications@github.com wrote:\n\nthis changeset looks good, let me do some tests first since it adds\nanother lookup for each request.\n\nI did wonder about that, but I figured that if that cache was being\nused (I am using it) then that would mitigate the extra workload.\nMatt\n. Hi again. Sorry to hassle, any chance of pulling this? I've been using it in production for several months and it's going well. No measurable impact on performance either.\n. ",
    "kencochrane": "That is pretty cool. Do you have any screenshots you can share?\n. Thanks\n. That is pretty cool. Do you have any screenshots you can share?\n. Thanks\n. ",
    "emblicaorg": "Mainly everything is happening in one view where you can directly edit or create routes (new settings for hipache)\nAll changes are updated to redis and so hipache can use those.\nThere will be lot of work to do if you wan't to use that one in real production use, but it's yet a working prototype.\n\nYou should try it, installation should be very easy.\n. Few bugs fixed and now it should work without tweaking any code (except settings ofcourse :smiley: )\nIt would be glad to hear feedback about this.\n. Yes correct\n. Hi!\nI think that could ruin the main idea of using hipache as proxy :D\nRedis is very powerful and doesn't have much of overhead except space in disk.\nHipache is a fork from project https://github.com/nodejitsu/node-http-proxy if you want to have same kind of features but more lightweight that can be answer for your question.\nIf you mean that you don't want to restart server every time you make new configuration, hipache doesn't provide yet support for that without redis because redis is the core component of hipache. You can make same kind of features with Nodejitsus node-http-proxy with just little hacking.\nI'm not developing hipache myself but I've been familiar with it because I'm working with it. Main developers should correct me if I'm wrong.\n. I meant that hipache is proxy for really fast changing, dynamic environments and thats why there is redis as one of the components.\nYou didn't specify what are the features that you need for. If you need something like apache then I recommend nginx. If you need something that can be used in very dynamic environment then I recommend hipache, you lose that advantage of dynamic configuring if you use just plain-text configuration files.\n. Mainly everything is happening in one view where you can directly edit or create routes (new settings for hipache)\nAll changes are updated to redis and so hipache can use those.\nThere will be lot of work to do if you wan't to use that one in real production use, but it's yet a working prototype.\n\nYou should try it, installation should be very easy.\n. Few bugs fixed and now it should work without tweaking any code (except settings ofcourse :smiley: )\nIt would be glad to hear feedback about this.\n. Yes correct\n. Hi!\nI think that could ruin the main idea of using hipache as proxy :D\nRedis is very powerful and doesn't have much of overhead except space in disk.\nHipache is a fork from project https://github.com/nodejitsu/node-http-proxy if you want to have same kind of features but more lightweight that can be answer for your question.\nIf you mean that you don't want to restart server every time you make new configuration, hipache doesn't provide yet support for that without redis because redis is the core component of hipache. You can make same kind of features with Nodejitsus node-http-proxy with just little hacking.\nI'm not developing hipache myself but I've been familiar with it because I'm working with it. Main developers should correct me if I'm wrong.\n. I meant that hipache is proxy for really fast changing, dynamic environments and thats why there is redis as one of the components.\nYou didn't specify what are the features that you need for. If you need something like apache then I recommend nginx. If you need something that can be used in very dynamic environment then I recommend hipache, you lose that advantage of dynamic configuring if you use just plain-text configuration files.\n. ",
    "kaka3052": "Thank you!  I have tried  hipache -c config_test.json, but show the same error. Do you kown what is the port that worker will take?\n. Thank you!  I have tried  hipache -c config_test.json, but show the same error. Do you kown what is the port that worker will take?\n. ",
    "aledbf": "Check if IPV6 is enabled.\n. Check if IPV6 is enabled.\n. ",
    "mikhail": "I had the same issue and my problem was resolved by uninstalling node-hipache via apt-get, and installing it via npm.\n. @dmp42 yes.\n. I had the same issue and my problem was resolved by uninstalling node-hipache via apt-get, and installing it via npm.\n. @dmp42 yes.\n. ",
    "dcarley": "We also got this error when using the 0.2.5 package from Tsuru on a Ubuntu 14.04 instance in Google Compute Engine. It was caused by IPv6 being disabled by default and the machine having no link-local v6 addresses. The solution was to change the configuration to not attempt to bind v6 with: \"address6\": []\n. We also got this error when using the 0.2.5 package from Tsuru on a Ubuntu 14.04 instance in Google Compute Engine. It was caused by IPv6 being disabled by default and the machine having no link-local v6 addresses. The solution was to change the configuration to not attempt to bind v6 with: \"address6\": []\n. ",
    "aionescu": "Any idea about this?\nThank you very much.\n. @jpetazzo thank you for the explanation.\n. > If we want to route using URIs, things get more complicated: what do we want to lookup?\nHow does Apache HTTPD do it? \n( There must be some easy trick, otherwise they wouldn't have implemented :). )\n. > I think that could ruin the main idea of using hipache as proxy \nSorry but that makes no sense to me. Node apps need a proxy, one that can compete with the stuff it's trying to replace. The node-http-proxy doesn't seem to be usable out of the box to me, at least not a real alternative to a simple, lightweight and production reliable Apache HTTPD configured as proxy with vhosting (something that every admin can quickly maintain).\n\nRedis is very powerful and doesn't have much of overhead except space in disk.\n\nI don't want to argue about the merits of Redis, but as an extra \"component\" in the system,\nit has an installation overhead (e.g. production environments where there's no available build - e.g. SPARC Solaris)\nIt will have upgrade and and maintenance overhead, as well as config and security overhead - things the admins will have to master in addition to the other stuff.\nI simply can't replace Apache HTTPD sitting in front of some apps with something else, if that replacement is a much higher overhead for the rest of the team(except me) that will have to use and maintain it.\n\nYou can make same kind of features with Nodejitsus node-http-proxy with just little hacking.\n\nSorry but I can't afford to \"hack\" production systems, especially not infrastructure :).\nInstead of upgrading existing running production web applications, I would like to rewrite some of them from scratch but based on Node (as a first step). For this, Node needs to play nice with the rest of the existing systems. As such, one of the problems so far seems to be the Apache HTTPD proxy, and so far comparing only the features, only Hipache seems to come close.\n. @jpetazzo  and @emblica \nThank you all for your responses!\nI'm looking to use WebSockets in one webapp that should replace an existing Java webapp in an existing infrastructure: Apache HTTPD as proxy in front of some Tomcat instances and some other legacy webapps.\nIf it turns out to work reliably than further webapps might be migrated to Node.\nFor some unknown reason, the ApacheHTTPD authors seem refuse to include any WebSockets extension. Extensions existing in the wild don't really seem to work correctly so far :(.\nNginx was evaluated, but refused by the admins because of the very bad performance of the Tomcat instances give with it :(.\nTo me Hipache seem like the only alternative because:\n- for Node, it seem the only solution that could change config while running (Apache HTTPD can do this too with several modes without a Redis like solution)\n- it wasn't an \"extra component\" but just a \"node app\" so much easier to get it in the \"landscape\" :) (well except the Redis part :) ).\nAlso hopefully Hipache works without problems with Tomcat, but I don't know that yet, hence my other question https://github.com/dotcloud/hipache/issues/19\nSad fact: Node webapps look nicer, easier and faster to develop, but the deployment in existing infrastructure seems like a PITA so far for me :(.\nThank you very much.\n. Any idea about this?\nThank you very much.\n. @jpetazzo thank you for the explanation.\n. > If we want to route using URIs, things get more complicated: what do we want to lookup?\nHow does Apache HTTPD do it? \n( There must be some easy trick, otherwise they wouldn't have implemented :). )\n. > I think that could ruin the main idea of using hipache as proxy \nSorry but that makes no sense to me. Node apps need a proxy, one that can compete with the stuff it's trying to replace. The node-http-proxy doesn't seem to be usable out of the box to me, at least not a real alternative to a simple, lightweight and production reliable Apache HTTPD configured as proxy with vhosting (something that every admin can quickly maintain).\n\nRedis is very powerful and doesn't have much of overhead except space in disk.\n\nI don't want to argue about the merits of Redis, but as an extra \"component\" in the system,\nit has an installation overhead (e.g. production environments where there's no available build - e.g. SPARC Solaris)\nIt will have upgrade and and maintenance overhead, as well as config and security overhead - things the admins will have to master in addition to the other stuff.\nI simply can't replace Apache HTTPD sitting in front of some apps with something else, if that replacement is a much higher overhead for the rest of the team(except me) that will have to use and maintain it.\n\nYou can make same kind of features with Nodejitsus node-http-proxy with just little hacking.\n\nSorry but I can't afford to \"hack\" production systems, especially not infrastructure :).\nInstead of upgrading existing running production web applications, I would like to rewrite some of them from scratch but based on Node (as a first step). For this, Node needs to play nice with the rest of the existing systems. As such, one of the problems so far seems to be the Apache HTTPD proxy, and so far comparing only the features, only Hipache seems to come close.\n. @jpetazzo  and @emblica \nThank you all for your responses!\nI'm looking to use WebSockets in one webapp that should replace an existing Java webapp in an existing infrastructure: Apache HTTPD as proxy in front of some Tomcat instances and some other legacy webapps.\nIf it turns out to work reliably than further webapps might be migrated to Node.\nFor some unknown reason, the ApacheHTTPD authors seem refuse to include any WebSockets extension. Extensions existing in the wild don't really seem to work correctly so far :(.\nNginx was evaluated, but refused by the admins because of the very bad performance of the Tomcat instances give with it :(.\nTo me Hipache seem like the only alternative because:\n- for Node, it seem the only solution that could change config while running (Apache HTTPD can do this too with several modes without a Redis like solution)\n- it wasn't an \"extra component\" but just a \"node app\" so much easier to get it in the \"landscape\" :) (well except the Redis part :) ).\nAlso hopefully Hipache works without problems with Tomcat, but I don't know that yet, hence my other question https://github.com/dotcloud/hipache/issues/19\nSad fact: Node webapps look nicer, easier and faster to develop, but the deployment in existing infrastructure seems like a PITA so far for me :(.\nThank you very much.\n. ",
    "hamiltont": "What about only supporting a simple set? I'd guessing that most users would be pretty happy with just this: \nSupported\nwww.example.com/server1\n  -> Routes: 10.0.0.1:4000, 10.0.0.2:4000 (round robin)\n  www.example.com\n  -> Routes: 10.0.0.2:3000\n  www.example.com/site3\n  -> Routes: 10.0.0.3:2000\nNot supported\nwww.example.com/site3/*/s2\nwww.example.com/site3/whataboutthis\nwww.example.com/foo/bar/b[0-9]\nPerhaps there is a way to rewrite URLs too, if you only support top-level paths. It also seems that this could go into the configuration file (enable/disable url-based routing), so that parties not using it don't have to take any performance hit due to the extra redis queries. \nAnother thought is that users that are not satisfied with the above are (IMO) likely to be power users that would know other mechanisms to achieve this anyway\nThe main potential downfall I see is users expecting \"url-rewriting\" or \"url-based routing\" to be fully supportive of wildcards\n. What about only supporting a simple set? I'd guessing that most users would be pretty happy with just this: \nSupported\nwww.example.com/server1\n  -> Routes: 10.0.0.1:4000, 10.0.0.2:4000 (round robin)\n  www.example.com\n  -> Routes: 10.0.0.2:3000\n  www.example.com/site3\n  -> Routes: 10.0.0.3:2000\nNot supported\nwww.example.com/site3/*/s2\nwww.example.com/site3/whataboutthis\nwww.example.com/foo/bar/b[0-9]\nPerhaps there is a way to rewrite URLs too, if you only support top-level paths. It also seems that this could go into the configuration file (enable/disable url-based routing), so that parties not using it don't have to take any performance hit due to the extra redis queries. \nAnother thought is that users that are not satisfied with the above are (IMO) likely to be power users that would know other mechanisms to achieve this anyway\nThe main potential downfall I see is users expecting \"url-rewriting\" or \"url-based routing\" to be fully supportive of wildcards\n. ",
    "nathankleyn": "I thought of a much simpler way of doing this, and would be happy to knock together a prototype at some stage.\nRather than storing the paths as different keys, you could keep the key as the hostname and add the path onto the end of the array members, ie.:\n\nwww.example.com -> [10.0.0.1:3000, 10.0.0.1:3001/foo, 10.0.0.1:3002/bar]\n\nWe could start by only supporting prefix matching (ie. /foo will match /foo and /foo/bar), and finding the right node will be easy enough; you can strip the path suffix off the end, put them into a Trie (could be cached) and do a longest prefix search (O(m) worst case). At a later stage we could add wildcarding support (Tries support these types of wildcard lookups efficiently).\nThe nice thing about this is that it's backwards compatible with any previous entries in Redis.\n. I thought of a much simpler way of doing this, and would be happy to knock together a prototype at some stage.\nRather than storing the paths as different keys, you could keep the key as the hostname and add the path onto the end of the array members, ie.:\n\nwww.example.com -> [10.0.0.1:3000, 10.0.0.1:3001/foo, 10.0.0.1:3002/bar]\n\nWe could start by only supporting prefix matching (ie. /foo will match /foo and /foo/bar), and finding the right node will be easy enough; you can strip the path suffix off the end, put them into a Trie (could be cached) and do a longest prefix search (O(m) worst case). At a later stage we could add wildcarding support (Tries support these types of wildcard lookups efficiently).\nThe nice thing about this is that it's backwards compatible with any previous entries in Redis.\n. ",
    "langri-sha": "Great design @nathankleyn! \n+1 for this ticket. If the router could be extended and provided independently so everyone has an opportunity to optimize based on their own needs, this would really be a godsend.\n. Great design @nathankleyn! \n+1 for this ticket. If the router could be extended and provided independently so everyone has an opportunity to optimize based on their own needs, this would really be a godsend.\n. ",
    "blackrosezy": "+1 This feature is really come in handy for multiple docker containers\n. :thumbsup:\n. Here is another Hipache Cli written in Go. https://github.com/blackrosezy/hic\n. +1 This feature is really come in handy for multiple docker containers\n. :thumbsup:\n. Here is another Hipache Cli written in Go. https://github.com/blackrosezy/hic\n. ",
    "bobblanchett": "Apache httpd mod_jk currently allows routing to tomcat workers usually on URI resource paths.  This would be a great feature as currently apache has to be reloaded to modify the configuration \n. Apache httpd mod_jk currently allows routing to tomcat workers usually on URI resource paths.  This would be a great feature as currently apache has to be reloaded to modify the configuration \n. ",
    "sstelfox": "It seems this was manually merged by hand by @alfss rather than accepting the pull request. I don't believe it has made it into an updated version of the project. Relevant commit: 749964de9b3e6b5ac211eb98a28ed332e2204ea2.\n. This was found to ultimately be caused by being on a heavily restricted network which does not allow outbound access of unprivileged ports (> 1024). The git protocols use tcp/9418. I still feel like this minor change is valuable.\n. It seems this was manually merged by hand by @alfss rather than accepting the pull request. I don't believe it has made it into an updated version of the project. Relevant commit: 749964de9b3e6b5ac211eb98a28ed332e2204ea2.\n. This was found to ultimately be caused by being on a heavily restricted network which does not allow outbound access of unprivileged ports (> 1024). The git protocols use tcp/9418. I still feel like this minor change is valuable.\n. ",
    "bdon": "+1. Neither 0.10 nor 0.9.12 worked for me, going all the way back to 0.8.25 did though.\n. +1. Neither 0.10 nor 0.9.12 worked for me, going all the way back to 0.8.25 did though.\n. ",
    "vlebedev": "+1\n. +1\n. ",
    "andybarlow": "+1\n. +1\n. ",
    "sebas5384": "Having the same problem here :(\n+1\n. Having the same problem here :(\n+1\n. ",
    "jolos": "+1, using 0.8.26 does the trick.\n. +1, using 0.8.26 does the trick.\n. ",
    "vieux": "Actually, this error is related to an issue in docker build.\nBUT the current Dockerfile is incompatible with docker 0.4.0 because it uses COPY (now add)\n. Hi,\nbecause npm install hipache -g uses git\n. docker from ppa should be at least 0.4.0, you have 0.3.x ?\nOn Wed, Jun 19, 2013 at 5:38 PM, Thomas Hansen notifications@github.comwrote:\n\nrunning docker build . in the repository root produces the following\noutput:\nhttps://gist.github.com/hansent/5815177\nPretty sure its a docker bug, but thought i'd post here in case anyone\nruns into it. I tried changing the ADD to INSERT and giving it the\nraw.github url for the supervisord config, but that got me:\nError build: INSERT has been deprecated. Please use ADD instead\nso yeah...wget it is for now:\nDockerfile with workaround is here:\nhttps://gist.github.com/hansent/5815302\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/30\n.\n\n\nVictor VIEUX\nhttp://vvieux.com\n. Ok, there is no INSERT in the Dockerfile in master: https://github.com/dotcloud/hipache/blob/master/Dockerfile\nI updated it 6 days ago.\nYou are probably using an old version.\nFYI there is an issue about ADD in docker, it'll fixed soon\n. Hi @jplock \nIt's a docker issue, I'll fix this very soon. \nThis is caused by the verbose output, in the meantime, you can use docker build . -q=true\nCheers.\n. Actually, this error is related to an issue in docker build.\nBUT the current Dockerfile is incompatible with docker 0.4.0 because it uses COPY (now add)\n. Hi,\nbecause npm install hipache -g uses git\n. docker from ppa should be at least 0.4.0, you have 0.3.x ?\nOn Wed, Jun 19, 2013 at 5:38 PM, Thomas Hansen notifications@github.comwrote:\n\nrunning docker build . in the repository root produces the following\noutput:\nhttps://gist.github.com/hansent/5815177\nPretty sure its a docker bug, but thought i'd post here in case anyone\nruns into it. I tried changing the ADD to INSERT and giving it the\nraw.github url for the supervisord config, but that got me:\nError build: INSERT has been deprecated. Please use ADD instead\nso yeah...wget it is for now:\nDockerfile with workaround is here:\nhttps://gist.github.com/hansent/5815302\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/30\n.\n\n\nVictor VIEUX\nhttp://vvieux.com\n. Ok, there is no INSERT in the Dockerfile in master: https://github.com/dotcloud/hipache/blob/master/Dockerfile\nI updated it 6 days ago.\nYou are probably using an old version.\nFYI there is an issue about ADD in docker, it'll fixed soon\n. Hi @jplock \nIt's a docker issue, I'll fix this very soon. \nThis is caused by the verbose output, in the meantime, you can use docker build . -q=true\nCheers.\n. ",
    "hansent": "docker version\nClient version: 0.4.2\nServer version: 0.4.2\nGo version: go1.1\n. # docker version\nClient version: 0.4.2\nServer version: 0.4.2\nGo version: go1.1\n. ",
    "alfssobsd": "[+] redis select database, enable auth\nAllows use multiple databases redis for one instance.\nAllows you to use authentication.\n. [+] redis select database, enable auth\nAllows use multiple databases redis for one instance.\nAllows you to use authentication.\n. ",
    "marcusramberg": "seems to be related to old node version, worked fine on latest.\n. seems to be related to old node version, worked fine on latest.\n. ",
    "jplock": "That worked, thanks\n. That worked, thanks\n. ",
    "mohangk": "It works if I use Webrick. I have the same issue with Unicorn.\n. It works if I use Webrick. I have the same issue with Unicorn.\n. ",
    "yuvipanda": "bump anyone?\n. bump anyone?\n. ",
    "crosbymichael": "@keeb Wouldn't the best practice for CMD be cmd [\"supervisor\", \"-n\"] ?\n. @keeb Wouldn't the best practice for CMD becmd [\"supervisor\", \"-n\"] ?\n. ",
    "keeb": "Done! Thanks for the feedback!\n. Done! Thanks for the feedback!\n. ",
    "andrevdm": "Option 1 sounds the simplest and is probably what my change should have done in the first place. \nI like option 3 though, I'll need to take a look at how to implement that. (My original idea was to specify a custom function in config.json but unfortunately that does not seem to be supported by the parser)\nI'll try spend some time on this tomorrow.\nThanks for the feedback\n.  @samalba & @jpetazzo how opposed would you be to the following\n   1) in \\bin\\hipache use eval rather than JSON.parse\n   2) in config.json the user can then specify a function that would\n      2.a) return the host key\n      2.b) return additional keys e.g. the wild card keys\n   3) in cache.js if the function exists use it rather than the default host key & wildcard matching\nI think this give the most flexibility and apart from eval it seems pretty clean, i.e. I can use host keys the way I want and you get custom wildcard behavior etc as suggested by @jpetazzo \nI've got this working locally, I want to test tomorrow and I'll commit & update the pull request if you are happy to look at it.\nThanks\n. Hi Jerome,\nI've pushed my change so that you can see what I am proposing.\nQuick summary:\n 1) bin\\hipache changed to eval the config.json file so that the custom frontendLookup function can be specified\n 2) cache.js changed to support frontendLookup \n   2.a) Override the lookup keys if frontendLookup is specified\n   2.b) Change the backend selection method to allow for a custom number of frontend lookups\nThanks for the assistance. I'll patiently wait for @samalba now :)\n. Ok I promised to be patient... but I though I'd check now :)\n@samalba / @jpetazzo: Are you happy with this change or does it affect to much of your core code? I'll be happy to maintain my own branch but obviously it would be great it this was merged.\nAnyway thanks again for such a fantastic product\nAndre\n. Option 1 sounds the simplest and is probably what my change should have done in the first place. \nI like option 3 though, I'll need to take a look at how to implement that. (My original idea was to specify a custom function in config.json but unfortunately that does not seem to be supported by the parser)\nI'll try spend some time on this tomorrow.\nThanks for the feedback\n.  @samalba & @jpetazzo how opposed would you be to the following\n   1) in \\bin\\hipache use eval rather than JSON.parse\n   2) in config.json the user can then specify a function that would\n      2.a) return the host key\n      2.b) return additional keys e.g. the wild card keys\n   3) in cache.js if the function exists use it rather than the default host key & wildcard matching\nI think this give the most flexibility and apart from eval it seems pretty clean, i.e. I can use host keys the way I want and you get custom wildcard behavior etc as suggested by @jpetazzo \nI've got this working locally, I want to test tomorrow and I'll commit & update the pull request if you are happy to look at it.\nThanks\n. Hi Jerome,\nI've pushed my change so that you can see what I am proposing.\nQuick summary:\n 1) bin\\hipache changed to eval the config.json file so that the custom frontendLookup function can be specified\n 2) cache.js changed to support frontendLookup \n   2.a) Override the lookup keys if frontendLookup is specified\n   2.b) Change the backend selection method to allow for a custom number of frontend lookups\nThanks for the assistance. I'll patiently wait for @samalba now :)\n. Ok I promised to be patient... but I though I'd check now :)\n@samalba / @jpetazzo: Are you happy with this change or does it affect to much of your core code? I'll be happy to maintain my own branch but obviously it would be great it this was merged.\nAnyway thanks again for such a fantastic product\nAndre\n. ",
    "sosedoff": "Well, reading documentation in detail solved the the problem. \nIssue was with first 2 steps where you need to create frontend identifier first. \n. @samalba Thanks!\n. @jpetazzo makes sense, i've been doing the same by having an additional backend to listen for redirect requests.\n. Well, reading documentation in detail solved the the problem. \nIssue was with first 2 steps where you need to create frontend identifier first. \n. @samalba Thanks!\n. @jpetazzo makes sense, i've been doing the same by having an additional backend to listen for redirect requests.\n. ",
    "hanwoody": "I met this problem too, hipache return 502 application not responding.\nbut I am sure that I can curl backend.\nand sure configure are all ok.\n. I met this problem too, hipache return 502 application not responding.\nbut I am sure that I can curl backend.\nand sure configure are all ok.\n. ",
    "drefined": "+1\n. Thanks for the quick response @samalba. I have also tried the retryOnError setting to see if that would proxy the request again, but to no avail. Here are the logs:\n6 Mar 17:20:53 - (worker #48) Cache: Proxying: dev.app.drefined.com -> 10.0.2.15:49155\n6 Mar 17:20:53 - (worker #48) dev.app.drefined.com: backend #1 is dead ({\"code\":\"ECONNREFUSED\",\"errno\":\"ECONNREFUSED\",\"syscall\":\"connect\"}) while handling request for /\n6 Mar 17:20:53 - (worker #48) Retrying on dev.app.drefined.com\n6 Mar 17:20:53 - (worker #48) Cache: Proxying: dev.app.drefined.com -> 10.0.2.15:49154\n6 Mar 17:21:53 - (worker #48) dev.app.drefined.com: backend #0 reported an error ({\"code\":\"ECONNRESET\"}) while handling request for /\n6 Mar 17:21:53 - (worker #48) dev.app.drefined.com: Retry limit reached (true), aborting.\nI know backend 0 is perfectly fine so I'm not sure why it is getting the connection reset. Anyway, I'll checkout the hipache-hchecker.\nThanks again.\n. @dmp42 From the logs it looks like retryOnError is working, but it was never able to proxy to backend 0 when in fact it was available. I opted to use hipache-hchecker instead.\n. +1\n. Thanks for the quick response @samalba. I have also tried the retryOnError setting to see if that would proxy the request again, but to no avail. Here are the logs:\n6 Mar 17:20:53 - (worker #48) Cache: Proxying: dev.app.drefined.com -> 10.0.2.15:49155\n6 Mar 17:20:53 - (worker #48) dev.app.drefined.com: backend #1 is dead ({\"code\":\"ECONNREFUSED\",\"errno\":\"ECONNREFUSED\",\"syscall\":\"connect\"}) while handling request for /\n6 Mar 17:20:53 - (worker #48) Retrying on dev.app.drefined.com\n6 Mar 17:20:53 - (worker #48) Cache: Proxying: dev.app.drefined.com -> 10.0.2.15:49154\n6 Mar 17:21:53 - (worker #48) dev.app.drefined.com: backend #0 reported an error ({\"code\":\"ECONNRESET\"}) while handling request for /\n6 Mar 17:21:53 - (worker #48) dev.app.drefined.com: Retry limit reached (true), aborting.\nI know backend 0 is perfectly fine so I'm not sure why it is getting the connection reset. Anyway, I'll checkout the hipache-hchecker.\nThanks again.\n. @dmp42 From the logs it looks like retryOnError is working, but it was never able to proxy to backend 0 when in fact it was available. I opted to use hipache-hchecker instead.\n. ",
    "jamescarr": "Also sorry for the many commits... a bit overcaffeinated this morning. More than happy to squash if too much. :)\n. Also sorry for the many commits... a bit overcaffeinated this morning. More than happy to squash if too much. :)\n. ",
    "willdurand": "You could also configure such a redirection at the DNS level :-)\n. :+1:\n. Nope, because:\n1. Ruby is installed on most OS by default.\n2. Chef/Puppet are the way to provision servers and install apps/tools/libs\n3. Docker\n:smile:\n. Seems pretty stable now..\n. Done.\n. see: #186 \n1. test suite is all green\n2. looks like a good adaptation of existing drivers, so I think it is ok...\n. This issue should be updated I guess.\n. Redis 2.4 is quite old now so this should not be a problem anymore.\n. For the website you can generate gh-pages from the README file :stuck_out_tongue: \n. I can take care of this\n. Like most Open Source projects, dropping a line to a maintainer or opening an issue on GitHub are your best bet. What would you expect?\n. What if this url cannot be reached? Endless loop. It is both much simpler and safer to use static pages, and now those pages are easily configurable so you can customize them to fit your app layout, and even tell your users to move to another url.\n. It seems out of Hipache's scope to me..\n. To me, the right approach would be to override the CMD directive by creating your own, extending the official image.\nAn improvement to this image might be to add Hipache's binary ENTRYPOINT though.\n. This should be fairly easy to add, but first, Hipache needs a better config validation (#122) layer.\n. @ryandub would it be possible to redirect HTTP to HTTPS with Hipache + Nginx backend, without your PR?\n. @ryandub yes, thanks for the reply. However, I slightly adapted your work: #172.\n. Do you run hipache using root/sudo? Because, binding port 80 is restricted on most platforms.\n. @qbraksa @alibby would @raelmax's comments fix this issue?\n. @dmp42 could you please trigger travis-ci again for this PR?\n. Failed.\nPing @slawo-ch \n. @dmp42 any thoughts on this feature?\n. @dmp42 I can work on this feature if it can land into Hipache at some point, but you decide :-)\n. I will try to come up with something better at some point. Current status: fighting against this damn test suite...\n. @msabramo you should not use this PR. it works for my very own needs.\n. This one has been rebased too.\n. ping @dmp42 :confused: \n. No problem.\nUnfortunately yes, tests fail with Redis for some obscure reasons. I can't really understand why so if you can look at it, it would be nice :-)\n. This is not perfect, but at least... most of the tests are run now.\n. see: #172 \n. @dmp42 on my fork, the build passes: https://travis-ci.org/willdurand/docker-hipache/builds/44520179. Here it does not but I think it is just unfortunate...\n1) Logger #legit logging empty data:\n     Uncaught AssertionError: expected undefined to deeply equal '::ffff:undefined - - [NaN/undefined/NaN:NaN:NaN:NaN +0000] \"undefined undefined HTTP/undefined\" undefined 0 \"\" \"\" \"undefined\" NaN NaN\\n'\n      at /home/travis/build/hipache/hipache/test/unit/logger.js:86:37\n      at fs.js:208:20\n      at OpenReq.Req.done (/home/travis/build/hipache/hipache/node_modules/mocha/node_modules/glob/node_modules/graceful-fs/graceful-fs.js:143:5)\n      at OpenReq.done (/home/travis/build/hipache/hipache/node_modules/mocha/node_modules/glob/node_modules/graceful-fs/graceful-fs.js:63:22)\n      at Object.oncomplete (fs.js:108:15)\n  2) Logger #legit logging legit data:\n     Uncaught AssertionError: expected undefined to deeply equal '::1 - - [16/Feb/2014:00:43:10 +0000] \"GET / HTTP/1.1\" 200 3236 \"\" \"curl/7.30.0\" \"mywebsite\" 0.011 0.009\\n'\n      at /home/travis/build/hipache/hipache/test/unit/logger.js:114:37\n      at fs.js:208:20\n      at OpenReq.Req.done (/home/travis/build/hipache/hipache/node_modules/mocha/node_modules/glob/node_modules/graceful-fs/graceful-fs.js:143:5)\n      at OpenReq.done (/home/travis/build/hipache/hipache/node_modules/mocha/node_modules/glob/node_modules/graceful-fs/graceful-fs.js:63:22)\n      at Object.oncomplete (fs.js:108:15)\n. We could probably use an official node base image actually...\n. By the way, don't know if you read my blog post but using node v0.11.14 would be better for SSL/TLS... Personally, I ended up using node:0.11.14 as base image.\n. Here is my Dockerfile, much simpler:\n```\nFROM node:0.11.14\nMAINTAINER William Durand william.durand1@gmail.com\nENV DEBIAN_FRONTEND noninteractive\nADD . /srv/hipache\nRUN npm install -g /srv/hipache --production\nENV NODE_ENV production\nRUN mkdir -p /var/log/hipache\nEXPOSE 80\nEXPOSE 443\nCMD [ \"/usr/local/bin/hipache\", \"-c\", \"/etc/hipache/config.json\" ]\n```\n. Don't really know what \"unstable\" means for Node people, but if it is like Debian, I should be stable enough :p \nSo, what's the plan? Making changes on this PR, keeping the current base image? Or moving to bleeding edge version?\n. Ha ha ha. I don't know, we can try I'd say.\n. @dmp42 updated, should be better now.\n. Good to go or?\n. More explanation here: http://williamdurand.fr/2014/12/23/configuring-ssl-tls-with-hipache-and-nodejs/.\n. More explanation here: http://williamdurand.fr/2014/12/23/configuring-ssl-tls-with-hipache-and-nodejs/.\n. @dmp42 fixed the patch to use constants, and rebased.\n. LGTM from Travis POV :p\n. thanks!\n. :disappointed: \n. @dmp42 don't get the aim of this 500Mb test, so if you can report this issue to Node people, it would be awesome :-)\n. No, but it would be interesting to see if:\n1. IO supports ECDH ciphers\n2. the 500MB test does not fail\n. Well, it looks like there are a lot of things planned for this milestone, that's why I thought about 1.0, which would stand for \"real stable version\". You/we could keep adding new features either part of this milestone or new ones (like the ones I proposed), and you could bump the minor number time to time if new features are available. Incrementing the patch number when new features are merged is quite bad IMO.\nHowever, I don't know if you want to follow semver..\n. Yep, I'm fine with this plan. Just saying that the 0.4 milestone is way to big for a 0.5.\n. Time to release 0.4 maybe?\n. ping @dmp42 (bonne ann\u00e9e en passant ;-))\n. thanks!\n. > You are now overriding the travis install step - which does npm install IIRC.\n@dmp42 I fixed this issue already :)\n. Aaaaand, it works \\o/\n. hrhrhr \n. Is gulp hint really useful BTW?\n. Yes. I guess it does, but I added a ||\u00a0true (a while ago) because a linter should not determine the status of a build..\n. How would you deal with false-positives? For instance: https://travis-ci.org/hipache/hipache#L494.\n. If you want to configure check style rules, feel free though ;-)\n. Fair enough. I still have one last upcoming PR for travis-ci. I'll look at the check style issues right after it.\n. From the .travis.yml:\n```\nHinting - XXX should fail!\n\nnpm run-script hint\n```\n\nNow I remember why I added || :p \n. Merci!\n. - Memcached tests still don't work because of an issue with #prefix stuff. Don't know what it is to be honest.\n- etcd tests don't run at all, getting the following error:\n```\n\nhipache@0.4.0 test /home/travis/build/willdurand/docker-hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec\nERR! Test No memcached server on this machine! No tests, then.\n  1) \"before all\" hook\n  0 passing (2s)\n  1 failing\n  1)  \"before all\" hook:\n     Uncaught Error: spawn ENOENT\n      at errnoException (child_process.js:1011:11)\n      at Process.ChildProcess._handle.onexit (child_process.js:802:34)\nnpm ERR! Test failed.  See above for more details.\nnpm ERR! not ok code 0\n```\n. > Etc tests never ran on travis. Should figure out a way to install and configure etcd.\n\nDone in #188, last night was a bit too late, I forgot about adding etcd to the PATH...\n. Yay! :star2: \n. @dmp42 done\n. I added a third commit to fix the doc for the Python test suite.\n. It's described in the config section but being explicit here never hurts, so :+1:\n. :-1: static files are fast and safe, adding templaring vars would not fit these two advantages.. \n. While I may get your point, I dont see any benefit since the vhost name is equal to the host users want to browse. So getting a \"application not responding\" error page  seems already pretty clear.\n. It does. However it still seems to be a very specific use case, and I'd rather go for a node-http-proxy middleware for that (if static pages are handled by this proxy in the code, don't have the code in mind).\n. It is documented but definitely not easy to use.\nSuch an error might mean that you don't have redis-server available on your system.\n. See:\n- https://github.com/hipache/hipache/blob/master/test/README.md\n- https://github.com/hipache/hipache/blob/master/test/functional/README.md\n. Then try to disable them with NO_ETCD and NO_ZOOKEEPER env vars (see travis-ci file)\n. look at the test/fixtures/services/zookeeper file then \n. and disabling zookeeper does not work? \n. LGTM\n. Weird that the test suite fails for such a change..\n:+1: otherwise\n. Agreed on a CONTRIBUTING file. Any template that you would like to use? I use this in most of my projects, and it should be easy to adapt it for this project.\n. To me, it is really important to add:\n\nHere are a few rules to follow in order to ease code reviews, and discussions before maintainers accept and merge your work.\nYou MUST run the test suite.\nYou MUST write (or update) unit tests.\nYou SHOULD write documentation.\nPlease, write commit messages that make sense, and rebase your branch before submitting your Pull Request.\nOne may ask you to squash your commits too. This is used to \"clean\" your Pull Request before merging it (we don't want commits such as \"fix tests\", \"fix 2\", \"fix 3\", etc.).\nAlso, while creating your Pull Request on GitHub, you MUST write a description which gives the context and/or explains why you are creating it.\nThank you!\n. youpi!\n. This test does not fail on my side, so I think it is not worth removing it.\n. Unit tests don't rely on a running redis server on my side as well :/\n. :+1:\n\nLGTM\n. I'd say a matter of environment...\n. :+1: for more readability\n. This is a really good idea! However, this should be optional, and disabled by default IMO\n. I read a bit RFC about this header, and there are warnings against that. Not everything should be exposed.\n. @msabramo good job. What you should also is:\n1. add the new config param to the default config (see defaults.js)\n2. update the doc\n. @dmp42 yes, I am aware of this issue. What about using node-http-proxy middlewares?\n. You're probably right: https://github.com/nodejitsu/node-http-proxy/tree/caronte/examples/middleware. It relies on \"connect\".\n. I like the overall idea!\n. \n. This project is still maintained. It works pretty well, and does exactly what it says it does, nothing much. Most of the issues are \"would be nice to have\", but technically, it does the job right.\nMy 2 cents\n. Create a PR on master. A new version should be released at some point.\n. sure. I guess the best plan would be to release a 0.3.2 with your pinning PR (and maybe one or two \"bug\" fixes), and to release 0.4 right after. (poke @dmp42)\n. Traditionally, there is a proxy that does SSL offloading in architectures with multiple backends. That's why Hipache talks to backends using http only. Another reason is handling certificates for all backends.\n. \ud83d\udc4d \n. You could also configure such a redirection at the DNS level :-)\n. :+1:\n. Nope, because:\n1. Ruby is installed on most OS by default.\n2. Chef/Puppet are the way to provision servers and install apps/tools/libs\n3. Docker\n:smile:\n. Seems pretty stable now..\n. Done.\n. see: #186 \n1. test suite is all green\n2. looks like a good adaptation of existing drivers, so I think it is ok...\n. This issue should be updated I guess.\n. Redis 2.4 is quite old now so this should not be a problem anymore.\n. For the website you can generate gh-pages from the README file :stuck_out_tongue: \n. I can take care of this\n. Like most Open Source projects, dropping a line to a maintainer or opening an issue on GitHub are your best bet. What would you expect?\n. What if this url cannot be reached? Endless loop. It is both much simpler and safer to use static pages, and now those pages are easily configurable so you can customize them to fit your app layout, and even tell your users to move to another url.\n. It seems out of Hipache's scope to me..\n. To me, the right approach would be to override the CMD directive by creating your own, extending the official image.\nAn improvement to this image might be to add Hipache's binary ENTRYPOINT though.\n. This should be fairly easy to add, but first, Hipache needs a better config validation (#122) layer.\n. @ryandub would it be possible to redirect HTTP to HTTPS with Hipache + Nginx backend, without your PR?\n. @ryandub yes, thanks for the reply. However, I slightly adapted your work: #172.\n. Do you run hipache using root/sudo? Because, binding port 80 is restricted on most platforms.\n. @qbraksa @alibby would @raelmax's comments fix this issue?\n. @dmp42 could you please trigger travis-ci again for this PR?\n. Failed.\nPing @slawo-ch \n. @dmp42 any thoughts on this feature?\n. @dmp42 I can work on this feature if it can land into Hipache at some point, but you decide :-)\n. I will try to come up with something better at some point. Current status: fighting against this damn test suite...\n. @msabramo you should not use this PR. it works for my very own needs.\n. This one has been rebased too.\n. ping @dmp42 :confused: \n. No problem.\nUnfortunately yes, tests fail with Redis for some obscure reasons. I can't really understand why so if you can look at it, it would be nice :-)\n. This is not perfect, but at least... most of the tests are run now.\n. see: #172 \n. @dmp42 on my fork, the build passes: https://travis-ci.org/willdurand/docker-hipache/builds/44520179. Here it does not but I think it is just unfortunate...\n1) Logger #legit logging empty data:\n     Uncaught AssertionError: expected undefined to deeply equal '::ffff:undefined - - [NaN/undefined/NaN:NaN:NaN:NaN +0000] \"undefined undefined HTTP/undefined\" undefined 0 \"\" \"\" \"undefined\" NaN NaN\\n'\n      at /home/travis/build/hipache/hipache/test/unit/logger.js:86:37\n      at fs.js:208:20\n      at OpenReq.Req.done (/home/travis/build/hipache/hipache/node_modules/mocha/node_modules/glob/node_modules/graceful-fs/graceful-fs.js:143:5)\n      at OpenReq.done (/home/travis/build/hipache/hipache/node_modules/mocha/node_modules/glob/node_modules/graceful-fs/graceful-fs.js:63:22)\n      at Object.oncomplete (fs.js:108:15)\n  2) Logger #legit logging legit data:\n     Uncaught AssertionError: expected undefined to deeply equal '::1 - - [16/Feb/2014:00:43:10 +0000] \"GET / HTTP/1.1\" 200 3236 \"\" \"curl/7.30.0\" \"mywebsite\" 0.011 0.009\\n'\n      at /home/travis/build/hipache/hipache/test/unit/logger.js:114:37\n      at fs.js:208:20\n      at OpenReq.Req.done (/home/travis/build/hipache/hipache/node_modules/mocha/node_modules/glob/node_modules/graceful-fs/graceful-fs.js:143:5)\n      at OpenReq.done (/home/travis/build/hipache/hipache/node_modules/mocha/node_modules/glob/node_modules/graceful-fs/graceful-fs.js:63:22)\n      at Object.oncomplete (fs.js:108:15)\n. We could probably use an official node base image actually...\n. By the way, don't know if you read my blog post but using node v0.11.14 would be better for SSL/TLS... Personally, I ended up using node:0.11.14 as base image.\n. Here is my Dockerfile, much simpler:\n```\nFROM node:0.11.14\nMAINTAINER William Durand william.durand1@gmail.com\nENV DEBIAN_FRONTEND noninteractive\nADD . /srv/hipache\nRUN npm install -g /srv/hipache --production\nENV NODE_ENV production\nRUN mkdir -p /var/log/hipache\nEXPOSE 80\nEXPOSE 443\nCMD [ \"/usr/local/bin/hipache\", \"-c\", \"/etc/hipache/config.json\" ]\n```\n. Don't really know what \"unstable\" means for Node people, but if it is like Debian, I should be stable enough :p \nSo, what's the plan? Making changes on this PR, keeping the current base image? Or moving to bleeding edge version?\n. Ha ha ha. I don't know, we can try I'd say.\n. @dmp42 updated, should be better now.\n. Good to go or?\n. More explanation here: http://williamdurand.fr/2014/12/23/configuring-ssl-tls-with-hipache-and-nodejs/.\n. More explanation here: http://williamdurand.fr/2014/12/23/configuring-ssl-tls-with-hipache-and-nodejs/.\n. @dmp42 fixed the patch to use constants, and rebased.\n. LGTM from Travis POV :p\n. thanks!\n. :disappointed: \n. @dmp42 don't get the aim of this 500Mb test, so if you can report this issue to Node people, it would be awesome :-)\n. No, but it would be interesting to see if:\n1. IO supports ECDH ciphers\n2. the 500MB test does not fail\n. Well, it looks like there are a lot of things planned for this milestone, that's why I thought about 1.0, which would stand for \"real stable version\". You/we could keep adding new features either part of this milestone or new ones (like the ones I proposed), and you could bump the minor number time to time if new features are available. Incrementing the patch number when new features are merged is quite bad IMO.\nHowever, I don't know if you want to follow semver..\n. Yep, I'm fine with this plan. Just saying that the 0.4 milestone is way to big for a 0.5.\n. Time to release 0.4 maybe?\n. ping @dmp42 (bonne ann\u00e9e en passant ;-))\n. thanks!\n. > You are now overriding the travis install step - which does npm install IIRC.\n@dmp42 I fixed this issue already :)\n. Aaaaand, it works \\o/\n. hrhrhr \n. Is gulp hint really useful BTW?\n. Yes. I guess it does, but I added a ||\u00a0true (a while ago) because a linter should not determine the status of a build..\n. How would you deal with false-positives? For instance: https://travis-ci.org/hipache/hipache#L494.\n. If you want to configure check style rules, feel free though ;-)\n. Fair enough. I still have one last upcoming PR for travis-ci. I'll look at the check style issues right after it.\n. From the .travis.yml:\n```\nHinting - XXX should fail!\n\nnpm run-script hint\n```\n\nNow I remember why I added || :p \n. Merci!\n. - Memcached tests still don't work because of an issue with #prefix stuff. Don't know what it is to be honest.\n- etcd tests don't run at all, getting the following error:\n```\n\nhipache@0.4.0 test /home/travis/build/willdurand/docker-hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec\nERR! Test No memcached server on this machine! No tests, then.\n  1) \"before all\" hook\n  0 passing (2s)\n  1 failing\n  1)  \"before all\" hook:\n     Uncaught Error: spawn ENOENT\n      at errnoException (child_process.js:1011:11)\n      at Process.ChildProcess._handle.onexit (child_process.js:802:34)\nnpm ERR! Test failed.  See above for more details.\nnpm ERR! not ok code 0\n```\n. > Etc tests never ran on travis. Should figure out a way to install and configure etcd.\n\nDone in #188, last night was a bit too late, I forgot about adding etcd to the PATH...\n. Yay! :star2: \n. @dmp42 done\n. I added a third commit to fix the doc for the Python test suite.\n. It's described in the config section but being explicit here never hurts, so :+1:\n. :-1: static files are fast and safe, adding templaring vars would not fit these two advantages.. \n. While I may get your point, I dont see any benefit since the vhost name is equal to the host users want to browse. So getting a \"application not responding\" error page  seems already pretty clear.\n. It does. However it still seems to be a very specific use case, and I'd rather go for a node-http-proxy middleware for that (if static pages are handled by this proxy in the code, don't have the code in mind).\n. It is documented but definitely not easy to use.\nSuch an error might mean that you don't have redis-server available on your system.\n. See:\n- https://github.com/hipache/hipache/blob/master/test/README.md\n- https://github.com/hipache/hipache/blob/master/test/functional/README.md\n. Then try to disable them with NO_ETCD and NO_ZOOKEEPER env vars (see travis-ci file)\n. look at the test/fixtures/services/zookeeper file then \n. and disabling zookeeper does not work? \n. LGTM\n. Weird that the test suite fails for such a change..\n:+1: otherwise\n. Agreed on a CONTRIBUTING file. Any template that you would like to use? I use this in most of my projects, and it should be easy to adapt it for this project.\n. To me, it is really important to add:\n\nHere are a few rules to follow in order to ease code reviews, and discussions before maintainers accept and merge your work.\nYou MUST run the test suite.\nYou MUST write (or update) unit tests.\nYou SHOULD write documentation.\nPlease, write commit messages that make sense, and rebase your branch before submitting your Pull Request.\nOne may ask you to squash your commits too. This is used to \"clean\" your Pull Request before merging it (we don't want commits such as \"fix tests\", \"fix 2\", \"fix 3\", etc.).\nAlso, while creating your Pull Request on GitHub, you MUST write a description which gives the context and/or explains why you are creating it.\nThank you!\n. youpi!\n. This test does not fail on my side, so I think it is not worth removing it.\n. Unit tests don't rely on a running redis server on my side as well :/\n. :+1:\n\nLGTM\n. I'd say a matter of environment...\n. :+1: for more readability\n. This is a really good idea! However, this should be optional, and disabled by default IMO\n. I read a bit RFC about this header, and there are warnings against that. Not everything should be exposed.\n. @msabramo good job. What you should also is:\n1. add the new config param to the default config (see defaults.js)\n2. update the doc\n. @dmp42 yes, I am aware of this issue. What about using node-http-proxy middlewares?\n. You're probably right: https://github.com/nodejitsu/node-http-proxy/tree/caronte/examples/middleware. It relies on \"connect\".\n. I like the overall idea!\n. \n. This project is still maintained. It works pretty well, and does exactly what it says it does, nothing much. Most of the issues are \"would be nice to have\", but technically, it does the job right.\nMy 2 cents\n. Create a PR on master. A new version should be released at some point.\n. sure. I guess the best plan would be to release a 0.3.2 with your pinning PR (and maybe one or two \"bug\" fixes), and to release 0.4 right after. (poke @dmp42)\n. Traditionally, there is a proxy that does SSL offloading in architectures with multiple backends. That's why Hipache talks to backends using http only. Another reason is handling certificates for all backends.\n. \ud83d\udc4d \n. ",
    "darron": "Yeah - I'm assuming that's why I'm having problems installing:\nhttps://gist.github.com/darron/7205306\n. Yeah - I'm assuming that's why I'm having problems installing:\nhttps://gist.github.com/darron/7205306\n. ",
    "aterreno": "@samalba , that commit is unrelated to this pull request, sorry about that, just ignore.\n. @samalba , that commit is unrelated to this pull request, sorry about that, just ignore.\n. ",
    "zbyte64": "+1 for Redis authentication\n. +1 for Redis authentication\n. ",
    "stefanfoulis": "sorry for the noise, this pull request was meant to go to our own repo.\n. Thanks for the quick reply.\nWhy is there a global dead: entry for the whole domain? Wouldn't it make more sense to have dead: entries for individual backends?\nI'm not using hipache-hchecker.\nhipache.conf:\n{\n    \"server\": {\n    \"debug\": true,\n        \"accessLog\": \"/var/log/hipache_access.log\",\n        \"port\": 80,\n        \"workers\": 10,\n        \"maxSockets\": 200,\n        \"deadBackendTTL\": 0.5,\n        \"deadBackendOn500\": false\n    },\n    \"redisHost\": \"127.0.0.1\",\n    \"redisPort\": 6379,\n    \"redisPassword\": \"my-password\"\n}\nWith deadBackendTTL and deadBackendOn500 I am trying to disable the functionality of hipache to not call backends that returned a 500 before. I always want hipache to contact the backends, no matter what they return.\nThe redis entries for that domain:\nredis> lrange frontend:www.example-domain.com 0 -1\n1) \"example-domain\"\n2) \"http://192.168.128.16:50443/\"\n3) \"http://192.168.128.17:50444/\"\n4) \"http://192.168.128.18:50445/\"\n5) \"http://192.168.128.19:50446/\"\nredis> keys dead*\n1) \"dead:www.example-domain.com\"\nI'm afraid I already deleted the dead: entry, without checking its contents.\n. Thanks for the detailed response. \nI tried to get a dead: entry by hammering a frontend with a non-existent backend. But no dead entry was added.\nI'll look into maybe setting up hchecker for real, or look into a fake hchecker script.\n. We at aldryn.com are also very interested in this branch. For us the same applies as @Siedrix mentioned for tutum:\n- certs in redis\n- not so aggressive caching, as we'd like to be able to update the certs without restarting hipache\nWhat is currently blocking further development? Can we help in some way?\n. @dmp42 We could possibly sponsor development of this feature. @dmp42 interested?\n. sorry for the noise, this pull request was meant to go to our own repo.\n. Thanks for the quick reply.\nWhy is there a global dead: entry for the whole domain? Wouldn't it make more sense to have dead: entries for individual backends?\nI'm not using hipache-hchecker.\nhipache.conf:\n{\n    \"server\": {\n    \"debug\": true,\n        \"accessLog\": \"/var/log/hipache_access.log\",\n        \"port\": 80,\n        \"workers\": 10,\n        \"maxSockets\": 200,\n        \"deadBackendTTL\": 0.5,\n        \"deadBackendOn500\": false\n    },\n    \"redisHost\": \"127.0.0.1\",\n    \"redisPort\": 6379,\n    \"redisPassword\": \"my-password\"\n}\nWith deadBackendTTL and deadBackendOn500 I am trying to disable the functionality of hipache to not call backends that returned a 500 before. I always want hipache to contact the backends, no matter what they return.\nThe redis entries for that domain:\nredis> lrange frontend:www.example-domain.com 0 -1\n1) \"example-domain\"\n2) \"http://192.168.128.16:50443/\"\n3) \"http://192.168.128.17:50444/\"\n4) \"http://192.168.128.18:50445/\"\n5) \"http://192.168.128.19:50446/\"\nredis> keys dead*\n1) \"dead:www.example-domain.com\"\nI'm afraid I already deleted the dead: entry, without checking its contents.\n. Thanks for the detailed response. \nI tried to get a dead: entry by hammering a frontend with a non-existent backend. But no dead entry was added.\nI'll look into maybe setting up hchecker for real, or look into a fake hchecker script.\n. We at aldryn.com are also very interested in this branch. For us the same applies as @Siedrix mentioned for tutum:\n- certs in redis\n- not so aggressive caching, as we'd like to be able to update the certs without restarting hipache\nWhat is currently blocking further development? Can we help in some way?\n. @dmp42 We could possibly sponsor development of this feature. @dmp42 interested?\n. ",
    "adrianbro": "Thanks Sam\nOn 22 November 2013 09:19, Sam Alba notifications@github.com wrote:\n\nYes\nWe have a production setup with several instances of Hipache. The only\ndifference is that each hipache node runs a separate Redis. A shared Redis\nshould work just fine.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/59#issuecomment-29030340\n.\n\n\nRegards\nAdrian Brown\n0477173894\n. Thanks Jerome, you make a good point\nOn 22 November 2013 09:37, J\u00e9r\u00f4me Petazzoni notifications@github.comwrote:\n\nWarning: if you use a non-local Redis, you will have some extra latency.\nLocal Redis has almost no latency (it's not measurable compared to the\ntotal request time), but using a remote Redis will add a couple of\nroundtrips.\nIf you are on a fast 10G LAN, the latency should be less than 1ms so you\nshould be fine.\nIf you are on some public cloud on different AZ, you could see more than\n10ms penalty at each request, tough.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/59#issuecomment-29031773\n.\n\n\nRegards\nAdrian Brown\n0477173894\n. Thanks Sam\nOn 22 November 2013 09:19, Sam Alba notifications@github.com wrote:\n\nYes\nWe have a production setup with several instances of Hipache. The only\ndifference is that each hipache node runs a separate Redis. A shared Redis\nshould work just fine.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/59#issuecomment-29030340\n.\n\n\nRegards\nAdrian Brown\n0477173894\n. Thanks Jerome, you make a good point\nOn 22 November 2013 09:37, J\u00e9r\u00f4me Petazzoni notifications@github.comwrote:\n\nWarning: if you use a non-local Redis, you will have some extra latency.\nLocal Redis has almost no latency (it's not measurable compared to the\ntotal request time), but using a remote Redis will add a couple of\nroundtrips.\nIf you are on a fast 10G LAN, the latency should be less than 1ms so you\nshould be fine.\nIf you are on some public cloud on different AZ, you could see more than\n10ms penalty at each request, tough.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/59#issuecomment-29031773\n.\n\n\nRegards\nAdrian Brown\n0477173894\n. ",
    "tabolario": "+1\n. +1\n. ",
    "sahilsk": "One suggestion: Could it be possible to edit headers before forwarding request to backends? I've seen apache doing it. See if you guys can add it.\n. Since hipache uses  http-proxy , It does support this feature.\nhttps://github.com/nodejitsu/node-http-proxy/issues/508\n. One suggestion: Could it be possible to edit headers before forwarding request to backends? I've seen apache doing it. See if you guys can add it.\n. Since hipache uses  http-proxy , It does support this feature.\nhttps://github.com/nodejitsu/node-http-proxy/issues/508\n. ",
    "tuxmonteiro": "\n0.2.5 implemented\n. > 0.2.5 implemented\n. \n",
    "milesrichardson": "Hi guys (hi Jerome!)\nThanks for the comments. I'm still a bit confused though.\nThe readme says hipache supports https. Does this mean that https is only supported by the frontend? Shouldn't an https request to the frontend be forwarded to the backend as https as well? Regardless, I see it is pretty trivial to implement.\nI am still looking for an answer to this question:\n- Can hipache use set of IPs squid is listening to as backends? (not vhost) Example:\n$ redis-cli rpush frontend:* http://127.0.0.1:3128\n(integer) 2\n$ redis-cli rpush frontend:* http://127.0.0.2:3128\n(integer) 3\nThe key difference being that these are squid IP addresses, not vhosts.\nWould I want to configure squid like this?\nhttp://wiki.squid-cache.org/ConfigExamples/Reverse/VirtualHosting\n. Hi guys (hi Jerome!)\nThanks for the comments. I'm still a bit confused though.\nThe readme says hipache supports https. Does this mean that https is only supported by the frontend? Shouldn't an https request to the frontend be forwarded to the backend as https as well? Regardless, I see it is pretty trivial to implement.\nI am still looking for an answer to this question:\n- Can hipache use set of IPs squid is listening to as backends? (not vhost) Example:\n$ redis-cli rpush frontend:* http://127.0.0.1:3128\n(integer) 2\n$ redis-cli rpush frontend:* http://127.0.0.2:3128\n(integer) 3\nThe key difference being that these are squid IP addresses, not vhosts.\nWould I want to configure squid like this?\nhttp://wiki.squid-cache.org/ConfigExamples/Reverse/VirtualHosting\n. ",
    "asbjornenge": "Excellent! :+1:\n. @fmd I actually ended up using dns (https://github.com/crosbymichael/skydock) for service discovery, and then had no use for configuring hipache via etcd.\nSo to answer you question; no. At least not by me.\n. Excellent! :+1:\n. @fmd I actually ended up using dns (https://github.com/crosbymichael/skydock) for service discovery, and then had no use for configuring hipache via etcd.\nSo to answer you question; no. At least not by me.\n. ",
    "fmd": "Is there any movement on this? An etcd backend would be really relevant on something like CoreOS. I'd be writing an etcd backend for Hipache myself, but if someone else is already looking at it I'd help their project.\n. Is there any movement on this? An etcd backend would be really relevant on something like CoreOS. I'd be writing an etcd backend for Hipache myself, but if someone else is already looking at it I'd help their project.\n. ",
    "fastner": "No, it is not possible to load certs on request as sniCallback have to be synchronous. This is a limitation of NodeJS's https module itself.\n. No, it is not possible to load certs on request as sniCallback have to be synchronous. This is a limitation of NodeJS's https module itself.\n. ",
    "nonsense": "This must be a shipyard issue, because installing hipache with redis on the host system, worked out of the box.\n. This must be a shipyard issue, because installing hipache with redis on the host system, worked out of the box.\n. ",
    "aymericbeaumet": "Yes it could if I do not have any other choice, but I'd prefer to be able to change the prefix :)\n. It is totally a personal preference. And as you said, I could just choose a different database.\n. Yes it could if I do not have any other choice, but I'd prefer to be able to change the prefix :)\n. It is totally a personal preference. And as you said, I could just choose a different database.\n. ",
    "ksikka": "This came at the perfect time. Awesome! I'll test it out right now.\n. The http-proxy module sets up a pipe between the incoming request and the outgoing proxy request:\nhttps://github.com/nodejitsu/node-http-proxy/blob/v1.0.1/lib/http-proxy/passes/web-incoming.js#L117\nI'm not entirely sure how pipes work but my guess is that when the incoming request becomes readable, the pipe starts buffering the data until the sooner of the following happens:\n1. the buffer becomes full\n2. the error event is emitted\nSource: http://nodejs.org/api/stream.html#stream_buffering\nThis is just my guess, and you're right, the first step is to reproduce this with a mocha test\n. oh god that is a mess\n. This came at the perfect time. Awesome! I'll test it out right now.\n. The http-proxy module sets up a pipe between the incoming request and the outgoing proxy request:\nhttps://github.com/nodejitsu/node-http-proxy/blob/v1.0.1/lib/http-proxy/passes/web-incoming.js#L117\nI'm not entirely sure how pipes work but my guess is that when the incoming request becomes readable, the pipe starts buffering the data until the sooner of the following happens:\n1. the buffer becomes full\n2. the error event is emitted\nSource: http://nodejs.org/api/stream.html#stream_buffering\nThis is just my guess, and you're right, the first step is to reproduce this with a mocha test\n. oh god that is a mess\n. ",
    "bjyoungblood": "@dmp42 I initially looked at doing a proper lookup, but I was concerned about potential performance penalties. I was also concerned that it could introduce a vulnerability, allowing a potential attacker to overload the server by flooding it with requests for d.o.m.a.i.n.s.l.i.k.e.t.h.i.s. I think setting some sane limits (with a config override maybe?) could mitigate that risk.\n. I've updated the PR to do true wildcard matching, up to 5 subdomains deep (though I think there is an argument for reducing it). My particular use-case only really requires foo.bar.domain.com to work, and I can't really think of a real life use-case for more than that.\nI don't think this will have any major performance impact other than increased I/O to Redis. I think for nearly all common configurations, the difference will be negligible.\n. @dmp42 I initially looked at doing a proper lookup, but I was concerned about potential performance penalties. I was also concerned that it could introduce a vulnerability, allowing a potential attacker to overload the server by flooding it with requests for d.o.m.a.i.n.s.l.i.k.e.t.h.i.s. I think setting some sane limits (with a config override maybe?) could mitigate that risk.\n. I've updated the PR to do true wildcard matching, up to 5 subdomains deep (though I think there is an argument for reducing it). My particular use-case only really requires foo.bar.domain.com to work, and I can't really think of a real life use-case for more than that.\nI don't think this will have any major performance impact other than increased I/O to Redis. I think for nearly all common configurations, the difference will be negligible.\n. ",
    "jkingyens": "Do you have any more detail on this? After 1 day, hipache is crashing for me with a flood of these messages:\n12 Apr 21:37:56 - (worker #18) TCP error from {\"remoteAddress\":\"169.254.169.254\",\"remotePort\":53288,\"bytesWritten\":918,\"bytesRead\":41,\"elapsed\":0.01}; Error: {\"code\":\"ECONNRESET\",\"errno\":\"ECONNRESET\",\"syscall\":\"read\"}\n. @dmp42 I traced the problem back to my own setup. The ECONNRESET's were not the cause. You are also right about this address being used by the platform to distribute metadata. I am using Google Compute Engine but it turns out the address is the same as EC2. I think these are from the health checker as my instances are behind a load balancer.\n. @dmp42 It was actually CoreOS/systemd configuration. My instances were rebooting to self update, but I hard-coded hipache docker container names (-n hipache-1) in my docker run commands. When the machine rebooted it failed to start hipache since the old container still existed.\n. Just to provide alternative feedback, I would benefit from the limited form of SNI support in this pull request. For example, I don't want to purchase an expensive wildcard SLL certificate, but I have more than 1 subdomain that require trusted SSL connections. It would be sufficient for me to just use JSON configuration to load multiple, cheaper,  SSL certificates. Perhaps merging a PR similar to this would be a good starting point. Full-blown, dynamic configuration of SSL certificates could come at a later time?\n. +1, I've been on this SNI branch for a long time. I've been using it and it works fine for my use case of having a handful of TLD certificates. It would be nice to support everything through dynamic configuration  as well though.\n. +1 because hipache now works with other backends such as etcd where there is no redis depedenec at all.\n. I ran into a similar issue when running hipache with docker. Here is the patch if you want to use that for either your own use or as a pull request here:\nhttps://github.com/SensePlatform/hipache/commit/afe7e768ab0b4aa6c6a810342da9f3176b446f46\nThis patch actually uses https://github.com/visionmedia/eson so that any of the fields in the config.json file can be overwritten with HIAPCHE_. This way you have a default config.json and can override the appropriate fields where necessary ie) HIPACHE_DRIVER=etcd://127.0.0.1:4001  would override the \"driver\" field in config.json\n. Do you have any more detail on this? After 1 day, hipache is crashing for me with a flood of these messages:\n12 Apr 21:37:56 - (worker #18) TCP error from {\"remoteAddress\":\"169.254.169.254\",\"remotePort\":53288,\"bytesWritten\":918,\"bytesRead\":41,\"elapsed\":0.01}; Error: {\"code\":\"ECONNRESET\",\"errno\":\"ECONNRESET\",\"syscall\":\"read\"}\n. @dmp42 I traced the problem back to my own setup. The ECONNRESET's were not the cause. You are also right about this address being used by the platform to distribute metadata. I am using Google Compute Engine but it turns out the address is the same as EC2. I think these are from the health checker as my instances are behind a load balancer.\n. @dmp42 It was actually CoreOS/systemd configuration. My instances were rebooting to self update, but I hard-coded hipache docker container names (-n hipache-1) in my docker run commands. When the machine rebooted it failed to start hipache since the old container still existed.\n. Just to provide alternative feedback, I would benefit from the limited form of SNI support in this pull request. For example, I don't want to purchase an expensive wildcard SLL certificate, but I have more than 1 subdomain that require trusted SSL connections. It would be sufficient for me to just use JSON configuration to load multiple, cheaper,  SSL certificates. Perhaps merging a PR similar to this would be a good starting point. Full-blown, dynamic configuration of SSL certificates could come at a later time?\n. +1, I've been on this SNI branch for a long time. I've been using it and it works fine for my use case of having a handful of TLD certificates. It would be nice to support everything through dynamic configuration  as well though.\n. +1 because hipache now works with other backends such as etcd where there is no redis depedenec at all.\n. I ran into a similar issue when running hipache with docker. Here is the patch if you want to use that for either your own use or as a pull request here:\nhttps://github.com/SensePlatform/hipache/commit/afe7e768ab0b4aa6c6a810342da9f3176b446f46\nThis patch actually uses https://github.com/visionmedia/eson so that any of the fields in the config.json file can be overwritten with HIAPCHE_. This way you have a default config.json and can override the appropriate fields where necessary ie) HIPACHE_DRIVER=etcd://127.0.0.1:4001  would override the \"driver\" field in config.json\n. ",
    "prologic": "WHen doing a Google Search for \"hipache CLI tools\" I came across this issue.\nThis is what I currently use (locally):\nhipache-add:\n``` #!bash\n!/bin/bash\nID=${1}\nDOMAIN=\"vz1.bne.shortcircuit.net.au\"\nHOST=\"${ID}.${DOMAIN}\"\nIP=$(resolveip -s ${ID})\nPORT=8000\nredis-cli -h hipache <<EOF\ndel frontend:${HOST} ${ID}\nrpush frontend:${HOST} ${ID}\nrpush frontend:${HOST} http://${IP}:${PORT}\nlrange frontend:${HOST} 0 -1\nEOF\n```\nhipache-del:\n``` #!bash\n!/bin/bash\nID=${1}\nDOMAIN=\"vz1.bne.shortcircuit.net.au\"\nHOST=\"${ID}.${DOMAIN}\"\nredis-cli -h hipache <<EOF\ndel frontend:${HOST} ${ID}\nlrange frontend:${HOST} 0 -1\nEOF\n```\nhipache-list:\n``` #!bash\n!/bin/bash\nDOMAIN=\"vz1.bne.shortcircuit.net.au\"\nredis-cli -h hipache <<EOF\nkeys *\nEOF\n```\nThese tools assume:\n- Hiepache is running as a container on the same host as the tools.\n- skydns/skydock is also configured so \"hipache\" resolves to the hipache internal container IP Address.\nThese tools could be adapted further I'm sure...\n. You're most welcome! I will try to improve upon this at some point!\ncheers\nJames\nJames Mills / prologic\nE: prologic@shortcircuit.net.au\nW: prologic.shortcircuit.net.au\nOn Wed, Apr 30, 2014 at 5:20 PM, Mangled Deutz notifications@github.comwrote:\n\nNice!\nThanks a lot for these!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/95#issuecomment-41767505\n.\n. I will try :) I have enough motificiaotn at least to try :)\n\ncheers\nJames\nJames Mills / prologic\nE: prologic@shortcircuit.net.au\nW: prologic.shortcircuit.net.au\nOn Thu, May 1, 2014 at 6:21 PM, Mangled Deutz notifications@github.comwrote:\n\nI will try to improve upon this at some point!\nI'm not sure yet what form these CLI will look like, and I won't work on\nthem immediately, so please do!\nand definitely add any idea / wish / remark in here, they are welcome.\nBest.\n- Olivier\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/95#issuecomment-41890180\n.\n. Just thought I'd quickly update this issue with a quck update on my side... I've since written a very quick tool (in Python) here: https://bitbucket.org/prologic/hipachectl which uses the Python redis library to talk to and control any hipache instnace.\n\nI'm also planning to Dockerize this and expand on this tool.\nFeedback welcome.\n. I have a few ideas to improve upon this tool:\n- Auto Detect the hipache instance in some way\n- Determine IP Address(es) of containers by CID and/or Name\ncheers\nJames\nJames Mills / prologic\nE: prologic@shortcircuit.net.au\nW: prologic.shortcircuit.net.au\nOn Fri, Aug 1, 2014 at 4:06 PM, Mohd Rozi notifications@github.com wrote:\n\n[image: :thumbsup:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/hipache/hipache/issues/95#issuecomment-50851979.\n. So this is weird; I'm getting this exact issue trying to run:\n\n#!bash\ndocker run -i -t --rm -p 80:80 hipache:0.2.8\nThe reason I'm using the 0.2.8 tag is the latest tag (0.3.1) is broken such that the node server constantly died. The same image above works perfectly fine on 4 other hosts except one that I'm having trouble(s) with. Any ideas?\n. +1 confirmed here too\n. WHen doing a Google Search for \"hipache CLI tools\" I came across this issue.\nThis is what I currently use (locally):\nhipache-add:\n``` #!bash\n!/bin/bash\nID=${1}\nDOMAIN=\"vz1.bne.shortcircuit.net.au\"\nHOST=\"${ID}.${DOMAIN}\"\nIP=$(resolveip -s ${ID})\nPORT=8000\nredis-cli -h hipache <<EOF\ndel frontend:${HOST} ${ID}\nrpush frontend:${HOST} ${ID}\nrpush frontend:${HOST} http://${IP}:${PORT}\nlrange frontend:${HOST} 0 -1\nEOF\n```\nhipache-del:\n``` #!bash\n!/bin/bash\nID=${1}\nDOMAIN=\"vz1.bne.shortcircuit.net.au\"\nHOST=\"${ID}.${DOMAIN}\"\nredis-cli -h hipache <<EOF\ndel frontend:${HOST} ${ID}\nlrange frontend:${HOST} 0 -1\nEOF\n```\nhipache-list:\n``` #!bash\n!/bin/bash\nDOMAIN=\"vz1.bne.shortcircuit.net.au\"\nredis-cli -h hipache <<EOF\nkeys *\nEOF\n```\nThese tools assume:\n- Hiepache is running as a container on the same host as the tools.\n- skydns/skydock is also configured so \"hipache\" resolves to the hipache internal container IP Address.\nThese tools could be adapted further I'm sure...\n. You're most welcome! I will try to improve upon this at some point!\ncheers\nJames\nJames Mills / prologic\nE: prologic@shortcircuit.net.au\nW: prologic.shortcircuit.net.au\nOn Wed, Apr 30, 2014 at 5:20 PM, Mangled Deutz notifications@github.comwrote:\n\nNice!\nThanks a lot for these!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/95#issuecomment-41767505\n.\n. I will try :) I have enough motificiaotn at least to try :)\n\ncheers\nJames\nJames Mills / prologic\nE: prologic@shortcircuit.net.au\nW: prologic.shortcircuit.net.au\nOn Thu, May 1, 2014 at 6:21 PM, Mangled Deutz notifications@github.comwrote:\n\nI will try to improve upon this at some point!\nI'm not sure yet what form these CLI will look like, and I won't work on\nthem immediately, so please do!\nand definitely add any idea / wish / remark in here, they are welcome.\nBest.\n- Olivier\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/hipache/issues/95#issuecomment-41890180\n.\n. Just thought I'd quickly update this issue with a quck update on my side... I've since written a very quick tool (in Python) here: https://bitbucket.org/prologic/hipachectl which uses the Python redis library to talk to and control any hipache instnace.\n\nI'm also planning to Dockerize this and expand on this tool.\nFeedback welcome.\n. I have a few ideas to improve upon this tool:\n- Auto Detect the hipache instance in some way\n- Determine IP Address(es) of containers by CID and/or Name\ncheers\nJames\nJames Mills / prologic\nE: prologic@shortcircuit.net.au\nW: prologic.shortcircuit.net.au\nOn Fri, Aug 1, 2014 at 4:06 PM, Mohd Rozi notifications@github.com wrote:\n\n[image: :thumbsup:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/hipache/hipache/issues/95#issuecomment-50851979.\n. So this is weird; I'm getting this exact issue trying to run:\n\n#!bash\ndocker run -i -t --rm -p 80:80 hipache:0.2.8\nThe reason I'm using the 0.2.8 tag is the latest tag (0.3.1) is broken such that the node server constantly died. The same image above works perfectly fine on 4 other hosts except one that I'm having trouble(s) with. Any ideas?\n. +1 confirmed here too\n. ",
    "thiago": "Another Library and CLI to control Hipache writen in NodeJS: https://github.com/trsouz/node-hipachectl\n. I'm having this problem using the image of Docker. I tested upgrading http-proxy to the stable version 1.11.1 and apparently solved the problem.\n. Yes @dmitryck, http-proxy is a hipache dependence. See https://github.com/hipache/hipache/blob/master/package.json#L25\nThis update worked for me. See my fork: https://github.com/trsouz/hipache/commit/f221bca2d947b1604af1657164788ab575aa9f0a\nAnd I think that is affecting all versions of hipache. See the travis with current commit: https://travis-ci.org/trsouz/hipache/builds/61718250\nAnd travis with my update: https://travis-ci.org/trsouz/hipache/builds/61717531 \n. Another Library and CLI to control Hipache writen in NodeJS: https://github.com/trsouz/node-hipachectl\n. I'm having this problem using the image of Docker. I tested upgrading http-proxy to the stable version 1.11.1 and apparently solved the problem.\n. Yes @dmitryck, http-proxy is a hipache dependence. See https://github.com/hipache/hipache/blob/master/package.json#L25\nThis update worked for me. See my fork: https://github.com/trsouz/hipache/commit/f221bca2d947b1604af1657164788ab575aa9f0a\nAnd I think that is affecting all versions of hipache. See the travis with current commit: https://travis-ci.org/trsouz/hipache/builds/61718250\nAnd travis with my update: https://travis-ci.org/trsouz/hipache/builds/61717531 \n. ",
    "coderofsalvation": "hi, I wrapped the shellsnippets above into one portable bash-script called hipcli.\nGrab it here:\nhttps://gist.github.com/coderofsalvation/e0a516c5dfe11cd1c64d\n. hi, I wrapped the shellsnippets above into one portable bash-script called hipcli.\nGrab it here:\nhttps://gist.github.com/coderofsalvation/e0a516c5dfe11cd1c64d\n. ",
    "cpuguy83": "It's Rails that is bolting on a Sintra webapp to a route.  There is no shared sessions since the Sinatra app doesn't handle any of that.  It's just Rails authorizing access to the route.  To Hipache it should look like one big app.\nOnly the session id is stored in a cookie, everything else is server-side.\nHipache config.json\njson\n{\n    \"server\": {\n        \"accessLog\": \"/var/log/hipache.log\",\n        \"port\": 80,\n        \"workers\": 3,\n        \"maxSockets\": 100,\n        \"deadBackendTTL\": 1,\n        \"tcpTimeout\": 30,\n        \"retryOnError\": 3,\n        \"deadBackendOn500\": false,\n        \"httpKeepAlive\": false,\n        \"https\":\n            {\n              \"port\": 443,\n              \"key\": \"/etc/ssl/ssl.key\",\n              \"cert\": \"/etc/ssl/ssl.crt\"\n            }\n    },\n    \"redisHost\": \"hipache-redis.redis.prod.envu.io\",\n    \"redisPort\": 6379,\n    \"redisDatabase\": 0\n}\n. Correct, only one backend:\nfrontend:emp.envuplay.com emp-front.emp.prod.envu.io\n. 1) \"emp\"\n2) \"http://emp-front.emp.prod.envu.io:8080\"\n. https://www.dropbox.com/s/q0lafbs50739xlu/emp.envuplay.com.har\n. So that is Hipache in front.\nWhat is happening is this:\n1st request, it loads the dashboard\n2nd request session data is gone(??) and as such you end up with a 404.\n. https://www.dropbox.com/s/fz3svcgcraqe5ae/dump.txt\n. Yeah, I tried making a simple failing app and couldn't.  Perhaps something else weird is happening.\n. That may have been an issue with my nginx config manually setting scheme to https instead of $scheme.  Fixed that but still having the same issues.\n. Also, I have tested with both http and https, same results.\nWas really hoping it was that dumb nginx config\n. And actually, it does look like it fixed it.  I'm closing this.\n. It's Rails that is bolting on a Sintra webapp to a route.  There is no shared sessions since the Sinatra app doesn't handle any of that.  It's just Rails authorizing access to the route.  To Hipache it should look like one big app.\nOnly the session id is stored in a cookie, everything else is server-side.\nHipache config.json\njson\n{\n    \"server\": {\n        \"accessLog\": \"/var/log/hipache.log\",\n        \"port\": 80,\n        \"workers\": 3,\n        \"maxSockets\": 100,\n        \"deadBackendTTL\": 1,\n        \"tcpTimeout\": 30,\n        \"retryOnError\": 3,\n        \"deadBackendOn500\": false,\n        \"httpKeepAlive\": false,\n        \"https\":\n            {\n              \"port\": 443,\n              \"key\": \"/etc/ssl/ssl.key\",\n              \"cert\": \"/etc/ssl/ssl.crt\"\n            }\n    },\n    \"redisHost\": \"hipache-redis.redis.prod.envu.io\",\n    \"redisPort\": 6379,\n    \"redisDatabase\": 0\n}\n. Correct, only one backend:\nfrontend:emp.envuplay.com emp-front.emp.prod.envu.io\n. 1) \"emp\"\n2) \"http://emp-front.emp.prod.envu.io:8080\"\n. https://www.dropbox.com/s/q0lafbs50739xlu/emp.envuplay.com.har\n. So that is Hipache in front.\nWhat is happening is this:\n1st request, it loads the dashboard\n2nd request session data is gone(??) and as such you end up with a 404.\n. https://www.dropbox.com/s/fz3svcgcraqe5ae/dump.txt\n. Yeah, I tried making a simple failing app and couldn't.  Perhaps something else weird is happening.\n. That may have been an issue with my nginx config manually setting scheme to https instead of $scheme.  Fixed that but still having the same issues.\n. Also, I have tested with both http and https, same results.\nWas really hoping it was that dumb nginx config\n. And actually, it does look like it fixed it.  I'm closing this.\n. ",
    "juanibiapina": "We have the exact same issue.\n. We have the exact same issue.\n. ",
    "shin-": "The output I provided is from master.\ntcpdump\n. @dmp42 result of tcpdump.\n. This worked for me locally.\n. Looks fine, not sure if we should investigate/report the warning to nodejistsu? Also scripts.start needs to be updated in package.json since bin/hipache is no longer a node script.\n```\nnpm start\n\nhipache@0.2.9 start /home/shin/work/hipache\nsh bin/hipache\n\n11 Apr 16:47:50 - Loading config from /home/shin/work/hipache/config/config.json\n11 Apr 16:47:50 - Spawning worker #0\n11 Apr 16:47:50 - Spawning worker #1\n11 Apr 16:47:50 - Server is running. {\"debug\":true,\"accessLog\":\"/tmp/proxy2_access.log\",\"port\":2800,\"workers\":2,\"maxSockets\":100,\"deadBackendTTL\":10,\"tcpTimeout\":3600,\"retryOnError\":3,\"deadBackendOn500\":true,\"httpKeepAlive\":false,\"lruCache\":{\"size\":5,\"ttl\":5}}\n11 Apr 16:47:51 - Loading config from /home/shin/work/hipache/config/config.json\n11 Apr 16:47:51 - Loading config from /home/shin/work/hipache/config/config.json\n11 Apr 16:47:51 - (worker #15777) Cache: LRU cache is enabled\n11 Apr 16:47:51 - (worker #15778) Cache: LRU cache is enabled\n11 Apr 16:48:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:48:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:49:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:49:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:50:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:50:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:51:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:51:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:52:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:52:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:53:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:53:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:54:28 - (worker #15778) TCP error from {\"remoteAddress\":\"127.0.0.1\",\"remotePort\":41924,\"bytesWritten\":0,\"bytesRead\":140,\"elapsed\":0.007}; Error: {\"bytesParsed\":0,\"code\":\"HPE_INVALID_METHOD\"}\n11 Apr 16:54:28 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:28 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:28 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:28 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:29 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:29 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:29 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:29 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:30 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:43 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:51 - (worker #15778) MemoryMonitor: Memory usage is OK (34MB)\n11 Apr 16:54:51 - (worker #15777) MemoryMonitor: Memory usage is OK (17MB)\n11 Apr 16:55:18 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:18 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:19 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:19 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n(node) warning: possible EventEmitter memory leak detected. 11 listeners added. Use emitter.setMaxListeners() to increase limit.\nTrace\n    at ClientRequest.EventEmitter.addListener (events.js:160:15)\n    at ClientRequest.EventEmitter.once (events.js:185:8)\n    at IncomingMessage. (/home/shin/work/hipache/node_modules/http-proxy/lib/node-http-proxy/http-proxy.js:393:22)\n    at IncomingMessage.EventEmitter.emit (events.js:117:20)\n    at Object.resume (/home/shin/work/hipache/node_modules/http-proxy/lib/node-http-proxy.js:253:18)\n    at HttpProxy.proxyRequest (/home/shin/work/hipache/node_modules/http-proxy/lib/node-http-proxy/http-proxy.js:431:16)\n    at /home/shin/work/hipache/lib/worker.js:372:23\n    at Cache. (/home/shin/work/hipache/lib/cache.js:203:9)\n    at Cache. (/home/shin/work/hipache/lib/cache.js:156:13)\n    at /home/shin/work/hipache/node_modules/redis/index.js:1131:13\n11 Apr 16:55:21 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:21 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:21 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:51 - (worker #15778) MemoryMonitor: Memory usage is OK (47MB)\n11 Apr 16:55:51 - (worker #15777) MemoryMonitor: Memory usage is OK (18MB)\n11 Apr 16:55:52 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:56:51 - (worker #15778) MemoryMonitor: Memory usage is OK (47MB)\n11 Apr 16:56:51 - (worker #15777) MemoryMonitor: Memory usage is OK (102MB)\n11 Apr 16:57:51 - (worker #15777) MemoryMonitor: Memory usage is OK (92MB)\n11 Apr 16:57:51 - (worker #15778) MemoryMonitor: Memory usage is OK (47MB)\n11 Apr 16:58:09 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:58:09 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:58:09 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:58:51 - (worker #15778) MemoryMonitor: Memory usage is OK (47MB)\n11 Apr 16:58:51 - (worker #15777) MemoryMonitor: Memory usage is OK (102MB)\n```\n```\ndocker push my.registry.io:2800/big\nThe push refers to a repository [my.registry.io:2800/big] (len: 1)\nSending image list\nPushing repository my.registry.io:2800/big (1 tags)\nImage 511136ea3c5a already pushed, skipping\n1c7f181e78b9: Image successfully pushed \nd0732e6ce563: Image successfully pushed \n25593492b938: Image successfully pushed \nbd33a67c4fd5: Image successfully pushed \nPushing tag for rev [bd33a67c4fd5] on {http://my.registry.io:2800/v1/repositories/big/tags/latest}\n``\n. The output I provided is from master.\n[tcpdump](https://gist.github.com/anonymous/9933561)\n. @dmp42 [result of tcpdump](https://www.dropbox.com/s/7c9y8nfcljppo7p/tcpdump2.log).\n. This worked for me locally.\n. Looks fine, not sure if we should investigate/report the warning to nodejistsu? Alsoscripts.startneeds to be updated inpackage.jsonsincebin/hipache` is no longer a node script.\n```\nnpm start\n\nhipache@0.2.9 start /home/shin/work/hipache\nsh bin/hipache\n\n11 Apr 16:47:50 - Loading config from /home/shin/work/hipache/config/config.json\n11 Apr 16:47:50 - Spawning worker #0\n11 Apr 16:47:50 - Spawning worker #1\n11 Apr 16:47:50 - Server is running. {\"debug\":true,\"accessLog\":\"/tmp/proxy2_access.log\",\"port\":2800,\"workers\":2,\"maxSockets\":100,\"deadBackendTTL\":10,\"tcpTimeout\":3600,\"retryOnError\":3,\"deadBackendOn500\":true,\"httpKeepAlive\":false,\"lruCache\":{\"size\":5,\"ttl\":5}}\n11 Apr 16:47:51 - Loading config from /home/shin/work/hipache/config/config.json\n11 Apr 16:47:51 - Loading config from /home/shin/work/hipache/config/config.json\n11 Apr 16:47:51 - (worker #15777) Cache: LRU cache is enabled\n11 Apr 16:47:51 - (worker #15778) Cache: LRU cache is enabled\n11 Apr 16:48:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:48:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:49:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:49:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:50:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:50:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:51:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:51:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:52:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:52:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:53:51 - (worker #15778) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:53:51 - (worker #15777) MemoryMonitor: Memory usage is OK (16MB)\n11 Apr 16:54:28 - (worker #15778) TCP error from {\"remoteAddress\":\"127.0.0.1\",\"remotePort\":41924,\"bytesWritten\":0,\"bytesRead\":140,\"elapsed\":0.007}; Error: {\"bytesParsed\":0,\"code\":\"HPE_INVALID_METHOD\"}\n11 Apr 16:54:28 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:28 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:28 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:28 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:29 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:29 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:29 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:29 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:30 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:43 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:54:51 - (worker #15778) MemoryMonitor: Memory usage is OK (34MB)\n11 Apr 16:54:51 - (worker #15777) MemoryMonitor: Memory usage is OK (17MB)\n11 Apr 16:55:18 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:18 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:19 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:19 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n(node) warning: possible EventEmitter memory leak detected. 11 listeners added. Use emitter.setMaxListeners() to increase limit.\nTrace\n    at ClientRequest.EventEmitter.addListener (events.js:160:15)\n    at ClientRequest.EventEmitter.once (events.js:185:8)\n    at IncomingMessage. (/home/shin/work/hipache/node_modules/http-proxy/lib/node-http-proxy/http-proxy.js:393:22)\n    at IncomingMessage.EventEmitter.emit (events.js:117:20)\n    at Object.resume (/home/shin/work/hipache/node_modules/http-proxy/lib/node-http-proxy.js:253:18)\n    at HttpProxy.proxyRequest (/home/shin/work/hipache/node_modules/http-proxy/lib/node-http-proxy/http-proxy.js:431:16)\n    at /home/shin/work/hipache/lib/worker.js:372:23\n    at Cache. (/home/shin/work/hipache/lib/cache.js:203:9)\n    at Cache. (/home/shin/work/hipache/lib/cache.js:156:13)\n    at /home/shin/work/hipache/node_modules/redis/index.js:1131:13\n11 Apr 16:55:21 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:21 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:21 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:55:51 - (worker #15778) MemoryMonitor: Memory usage is OK (47MB)\n11 Apr 16:55:51 - (worker #15777) MemoryMonitor: Memory usage is OK (18MB)\n11 Apr 16:55:52 - (worker #15777) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:56:51 - (worker #15778) MemoryMonitor: Memory usage is OK (47MB)\n11 Apr 16:56:51 - (worker #15777) MemoryMonitor: Memory usage is OK (102MB)\n11 Apr 16:57:51 - (worker #15777) MemoryMonitor: Memory usage is OK (92MB)\n11 Apr 16:57:51 - (worker #15778) MemoryMonitor: Memory usage is OK (47MB)\n11 Apr 16:58:09 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:58:09 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:58:09 - (worker #15778) Cache: Proxying: my.registry.io -> 127.0.0.1:5000\n11 Apr 16:58:51 - (worker #15778) MemoryMonitor: Memory usage is OK (47MB)\n11 Apr 16:58:51 - (worker #15777) MemoryMonitor: Memory usage is OK (102MB)\n```\n```\ndocker push my.registry.io:2800/big\nThe push refers to a repository [my.registry.io:2800/big] (len: 1)\nSending image list\nPushing repository my.registry.io:2800/big (1 tags)\nImage 511136ea3c5a already pushed, skipping\n1c7f181e78b9: Image successfully pushed \nd0732e6ce563: Image successfully pushed \n25593492b938: Image successfully pushed \nbd33a67c4fd5: Image successfully pushed \nPushing tag for rev [bd33a67c4fd5] on {http://my.registry.io:2800/v1/repositories/big/tags/latest}\n```\n. ",
    "ruudud": "Sure thing. \nIt should be a rather safe change though, removing complexity and making stuff simpler.\n. Sure thing. \nIt should be a rather safe change though, removing complexity and making stuff simpler.\n. ",
    "cywjackson": "we are running into issue that'd appear has been addressed by 1.0.3: https://github.com/nodejitsu/node-http-proxy/issues/570\nPreviously I have:\nnpm list http-proxy -g\n/usr/local/lib\n\u2514\u2500\u252c hipache@0.3.1\n  \u2514\u2500\u2500 http-proxy@1.0.2\nwhen installing hipache via \"# npm install hipache -g\"\nBut we run into the FD leak issue. This is confirmed by taking out hipache in between and connect to the backend webserver directly.\nAfter finding out https://github.com/nodejitsu/node-http-proxy/issues/570 , I reinstall hipache with modifying the package.json for http-proxy to 1.0.3. I reinstall it via: \n\"#npm install /mypath/to/hipache -g\"\nnpm list http-proxy -g\n/usr/local/lib\n\u251c\u2500\u252c hipache@0.3.0\n\u2502 \u2514\u2500\u2500 http-proxy@1.0.3 \n\u2514\u2500\u2500 http-proxy@1.0.3\nAnd this confirms the FD leak is resolved. \nI understand and agree the reasoning of not putting 1.0.3 (since it breaks websockets). But how do you plan to address the FD leak issue? Thx!\n. Here is a simple test case. All running on local machine.\nnpm list http-proxy -g\n/usr/local/lib\n\u251c\u2500\u252c hipache@0.3.1\n\u2502 \u2514\u2500\u2500 http-proxy@1.0.2 \n\u2514\u2500\u2500 http-proxy@1.0.3 \nhipache config as:\njson\n{\n    \"server\": {\n        \"accessLog\": \"/logs/hipache_access.log\",\n        \"address\": [\"<non-localhost interface>\"],\n        \"port\": 80,\n        \"workers\": 10,\n        \"maxSockets\": 100,\n        \"deadBackendTTL\": 5,\n        \"tcpTimeout\": 60,\n        \"retryOnError\": 3,\n        \"deadBackendOn500\": false,\n        \"httpKeepAlive\": false,\n        \"lruCache\": {\n            \"size\": 5,\n            \"ttl\": 5\n        }\n    },\n    \"driver\": \"redis://127.0.0.1:6379\"\n}\nredis config as:\nredis 127.0.0.1:6379> lrange frontend:* 0 -1\n1) \"blah\"\n2) \"http://127.0.0.1:8888\"\nstart a twisted python webserver on 127.0.0.1:8888 with the following:\n``` python\n! /usr/bin/env python\nfrom twisted.web import server, resource\nfrom twisted.internet import reactor\nclass Simple(resource.Resource):\n    isLeaf = True\n    def render_GET(self, request):\n        return \"Hello, world!\"\nsite = server.Site(Simple())\nreactor.listenTCP(8888, site,interface=\"127.0.0.1\")\nreactor.run()\n```\nI use netstat to monitor the existing connections, but you could use others (lsof, etc). eg:\nwatch -n 1 \"sudo netstat -antp | grep :8888\"\n1. Open a browser (curl won't work) and hit the hipache address (http://<non-localhost interface>)\n2. Observe the netstat/connection lists\n3. Close the browser\n4. Observe the netstat/connection lists again.\n5. Repeast step 2-4 a few time to observe the increases of the established connections\n   my sample:\ntcp        0      0 127.0.0.1:8888          0.0.0.0:*               LISTEN      10548/python\n    tcp        0      0 127.0.0.1:60725         127.0.0.1:8888          ESTABLISHED 11097/nodejs\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60725         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:60673         127.0.0.1:8888          ESTABLISHED 11093/nodejs\n    tcp        0      0 127.0.0.1:60700         127.0.0.1:8888          ESTABLISHED 11108/nodejs\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60724         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:60724         127.0.0.1:8888          ESTABLISHED 11097/nodejs\n    tcp        0      0 127.0.0.1:60672         127.0.0.1:8888          ESTABLISHED 11093/nodejs\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60700         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:60699         127.0.0.1:8888          ESTABLISHED 11108/nodejs\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60699         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60673         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60672         ESTABLISHED 10548/python\n1. (Optional) Repeat the same test with http-proxy 1.0.3. Those connections will be gone.\n. cool! glad to hear you could reproduce it, and thx for looking into it! \n(ps: I will be out with no internet access in upcoming weeks)\n. we are running into issue that'd appear has been addressed by 1.0.3: https://github.com/nodejitsu/node-http-proxy/issues/570\nPreviously I have:\nnpm list http-proxy -g\n/usr/local/lib\n\u2514\u2500\u252c hipache@0.3.1\n  \u2514\u2500\u2500 http-proxy@1.0.2\nwhen installing hipache via \"# npm install hipache -g\"\nBut we run into the FD leak issue. This is confirmed by taking out hipache in between and connect to the backend webserver directly.\nAfter finding out https://github.com/nodejitsu/node-http-proxy/issues/570 , I reinstall hipache with modifying the package.json for http-proxy to 1.0.3. I reinstall it via: \n\"#npm install /mypath/to/hipache -g\"\nnpm list http-proxy -g\n/usr/local/lib\n\u251c\u2500\u252c hipache@0.3.0\n\u2502 \u2514\u2500\u2500 http-proxy@1.0.3 \n\u2514\u2500\u2500 http-proxy@1.0.3\nAnd this confirms the FD leak is resolved. \nI understand and agree the reasoning of not putting 1.0.3 (since it breaks websockets). But how do you plan to address the FD leak issue? Thx!\n. Here is a simple test case. All running on local machine.\nnpm list http-proxy -g\n/usr/local/lib\n\u251c\u2500\u252c hipache@0.3.1\n\u2502 \u2514\u2500\u2500 http-proxy@1.0.2 \n\u2514\u2500\u2500 http-proxy@1.0.3 \nhipache config as:\njson\n{\n    \"server\": {\n        \"accessLog\": \"/logs/hipache_access.log\",\n        \"address\": [\"<non-localhost interface>\"],\n        \"port\": 80,\n        \"workers\": 10,\n        \"maxSockets\": 100,\n        \"deadBackendTTL\": 5,\n        \"tcpTimeout\": 60,\n        \"retryOnError\": 3,\n        \"deadBackendOn500\": false,\n        \"httpKeepAlive\": false,\n        \"lruCache\": {\n            \"size\": 5,\n            \"ttl\": 5\n        }\n    },\n    \"driver\": \"redis://127.0.0.1:6379\"\n}\nredis config as:\nredis 127.0.0.1:6379> lrange frontend:* 0 -1\n1) \"blah\"\n2) \"http://127.0.0.1:8888\"\nstart a twisted python webserver on 127.0.0.1:8888 with the following:\n``` python\n! /usr/bin/env python\nfrom twisted.web import server, resource\nfrom twisted.internet import reactor\nclass Simple(resource.Resource):\n    isLeaf = True\n    def render_GET(self, request):\n        return \"Hello, world!\"\nsite = server.Site(Simple())\nreactor.listenTCP(8888, site,interface=\"127.0.0.1\")\nreactor.run()\n```\nI use netstat to monitor the existing connections, but you could use others (lsof, etc). eg:\nwatch -n 1 \"sudo netstat -antp | grep :8888\"\n1. Open a browser (curl won't work) and hit the hipache address (http://<non-localhost interface>)\n2. Observe the netstat/connection lists\n3. Close the browser\n4. Observe the netstat/connection lists again.\n5. Repeast step 2-4 a few time to observe the increases of the established connections\n   my sample:\ntcp        0      0 127.0.0.1:8888          0.0.0.0:*               LISTEN      10548/python\n    tcp        0      0 127.0.0.1:60725         127.0.0.1:8888          ESTABLISHED 11097/nodejs\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60725         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:60673         127.0.0.1:8888          ESTABLISHED 11093/nodejs\n    tcp        0      0 127.0.0.1:60700         127.0.0.1:8888          ESTABLISHED 11108/nodejs\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60724         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:60724         127.0.0.1:8888          ESTABLISHED 11097/nodejs\n    tcp        0      0 127.0.0.1:60672         127.0.0.1:8888          ESTABLISHED 11093/nodejs\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60700         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:60699         127.0.0.1:8888          ESTABLISHED 11108/nodejs\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60699         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60673         ESTABLISHED 10548/python\n    tcp        0      0 127.0.0.1:8888          127.0.0.1:60672         ESTABLISHED 10548/python\n1. (Optional) Repeat the same test with http-proxy 1.0.3. Those connections will be gone.\n. cool! glad to hear you could reproduce it, and thx for looking into it! \n(ps: I will be out with no internet access in upcoming weeks)\n. ",
    "notnmeyer": "Thanks guys!\n. Looks good to me. Thanks!\n. Thanks guys!\n. Looks good to me. Thanks!\n. ",
    "tjmehta": "Thanks, corrected hchecker to golang (whoops!). Definitely do checkout hipcheck it provides some intelligent features like ignoring manually removed hosts and resuming heartbeats after restarting the process. Also, it is in the docker registry so installation is a cinch :)\n. Yes we are, and using hipcheck in production\n. sorry wrong repo!\n. Thanks, corrected hchecker to golang (whoops!). Definitely do checkout hipcheck it provides some intelligent features like ignoring manually removed hosts and resuming heartbeats after restarting the process. Also, it is in the docker registry so installation is a cinch :)\n. Yes we are, and using hipcheck in production\n. sorry wrong repo!\n. ",
    "warpr": "Hey dmp, yes you know me from MusicBrainz :)\nI installed from npm, yes.  I probably should have tried master before opening an issue.\n. Sure. I haven't had a chance to try it yet, but I can re-open if it isn't working for me on master.\n. Hey dmp, yes you know me from MusicBrainz :)\nI installed from npm, yes.  I probably should have tried master before opening an issue.\n. Sure. I haven't had a chance to try it yet, but I can re-open if it isn't working for me on master.\n. ",
    "thaJeztah": "Or, option 3; leave The documentation for master as-is, but put a clear warning on top:\nnote: this is the documentation for the development version / unreleased version\nLooking for the documentation for the current release? Click here (link to current release)\n. Looks like the 500Mb test still fails on node 0.11 :(\n. Or, option 3; leave The documentation for master as-is, but put a clear warning on top:\nnote: this is the documentation for the development version / unreleased version\nLooking for the documentation for the current release? Click here (link to current release)\n. Looks like the 500Mb test still fails on node 0.11 :(\n. ",
    "actionjack": "Bounty opened on this at bounty source feel free to add to it (or get it implemented): \nhttps://www.bountysource.com/issues/6406959-sni-passthrough\n. Bounty opened on this at bounty source feel free to add to it (or get it implemented): \nhttps://www.bountysource.com/issues/6406959-sni-passthrough\n. +1\n. Bounty opened on this at bounty source feel free to add to it (or get it implemented): \nhttps://www.bountysource.com/issues/6406959-sni-passthrough\n. Bounty opened on this at bounty source feel free to add to it (or get it implemented): \nhttps://www.bountysource.com/issues/6406959-sni-passthrough\n. Bounty opened on this at bounty source feel free to add to it (or get it implemented): \nhttps://www.bountysource.com/issues/6406959-sni-passthrough\n. +1\n. Bounty opened on this at bounty source feel free to add to it (or get it implemented): \nhttps://www.bountysource.com/issues/6406959-sni-passthrough\n. ",
    "Siedrix": "Hi, I work at tutum.co and we use hipache so we can push the domains changes to redis and have them reflected on our loadbalancer, we would like to do the same for certificates.\nI have been reviewing the code for this branch and it seems that certificates are cached after they are read and they can't be expired without restarting hipache and that we need to pass the certificates to the filesystem on the hipache server so they can be read on the SNICallback on the worker.\nI have time to add support for \"Redis storage for certificates\". I would like to get your feedback on what are you planning to do with the SNI support. \nIf you are interested on this feature, I have the following questions:\n- How much of this should be done on worker level and how much should it be done in a driver level?\n- What kind of cache would you like to have for certificates? or reading redis every request would be ok?\n- Would you like a flag to overwrite the current implementation of SNI and use one provided on the driver?\n- What kind of test do you require for this feature?\nThanks for your time.\n. Hi, I work at tutum.co and we use hipache so we can push the domains changes to redis and have them reflected on our loadbalancer, we would like to do the same for certificates.\nI have been reviewing the code for this branch and it seems that certificates are cached after they are read and they can't be expired without restarting hipache and that we need to pass the certificates to the filesystem on the hipache server so they can be read on the SNICallback on the worker.\nI have time to add support for \"Redis storage for certificates\". I would like to get your feedback on what are you planning to do with the SNI support. \nIf you are interested on this feature, I have the following questions:\n- How much of this should be done on worker level and how much should it be done in a driver level?\n- What kind of cache would you like to have for certificates? or reading redis every request would be ok?\n- Would you like a flag to overwrite the current implementation of SNI and use one provided on the driver?\n- What kind of test do you require for this feature?\nThanks for your time.\n. ",
    "hpg4815": "Sorry, i need people to spell it out for me... Can Hipache accept traffic over SSL with multiple entry points (e.g. GSLBs, LLBs). Each entry point binding to their respective Key & Cert contents/file? All from redis, and not requiring me to restart hipache from adding a new ip or port  binding in the hipache config? Lastly, does hipache or NodeJS have a module/plugin that would incorporate SSO? Like CA's SiteMinder or Ping Federate's agent less approach.  Warm Regards,\n. @dmp42 Thanks for that. Yeah, SNI is exactly what I'm looking for in Hipache. Since it is not currently available, I also wanted to confirm that I could still set up Hipache to except requests to individual hostnames by modeling after the config.jason example provided in #128 ? This would also allow me to add cert and key files for each hostname/ip binding but would just come with the requirement of restarting Hipache afterwards? I like Hipache, and I'm just trying to ensure it is a good fit in our company. We are looking for a Reverse Proxy Solution to provide as a PaaS, but the challenge is the company wide Single Sign On (SSO) we use currently is provided by CA's SiteMinder, and it calls for a agent install on Apache (no agent install for the app servers such as WebLogic, JBoss,Tomcat). Requiring us to front all application servers with Apaches. We are in the process of moving to Ping Federate which has a agent-less setup that will now allow the application to leverage SSO with a snip of code. One final question... when the term vhost is used in regards to redis, does this imply one would be able to add all configurations in Redis similar to that of the Apache NameBasedVirtualHost? Things such as ProxyPreserveHost, Timout, lbreqests, etc. etc. Or am I perceiving Redis's vhost term incorrectly?\nFTR: I realize this type of development is not easy, and I condone all the contributors for their hard work. I wish I possessed the same skills set so I could also contribute. Thanks. \n. Sorry, i need people to spell it out for me... Can Hipache accept traffic over SSL with multiple entry points (e.g. GSLBs, LLBs). Each entry point binding to their respective Key & Cert contents/file? All from redis, and not requiring me to restart hipache from adding a new ip or port  binding in the hipache config? Lastly, does hipache or NodeJS have a module/plugin that would incorporate SSO? Like CA's SiteMinder or Ping Federate's agent less approach.  Warm Regards,\n. @dmp42 Thanks for that. Yeah, SNI is exactly what I'm looking for in Hipache. Since it is not currently available, I also wanted to confirm that I could still set up Hipache to except requests to individual hostnames by modeling after the config.jason example provided in #128 ? This would also allow me to add cert and key files for each hostname/ip binding but would just come with the requirement of restarting Hipache afterwards? I like Hipache, and I'm just trying to ensure it is a good fit in our company. We are looking for a Reverse Proxy Solution to provide as a PaaS, but the challenge is the company wide Single Sign On (SSO) we use currently is provided by CA's SiteMinder, and it calls for a agent install on Apache (no agent install for the app servers such as WebLogic, JBoss,Tomcat). Requiring us to front all application servers with Apaches. We are in the process of moving to Ping Federate which has a agent-less setup that will now allow the application to leverage SSO with a snip of code. One final question... when the term vhost is used in regards to redis, does this imply one would be able to add all configurations in Redis similar to that of the Apache NameBasedVirtualHost? Things such as ProxyPreserveHost, Timout, lbreqests, etc. etc. Or am I perceiving Redis's vhost term incorrectly?\nFTR: I realize this type of development is not easy, and I condone all the contributors for their hard work. I wish I possessed the same skills set so I could also contribute. Thanks. \n. ",
    "stvnwrgs": "Are there any new news for that? We also want to use hipache for our loadbalancers. But we need SNI because of different tlds using the same backend application.\n. Are there any new news for that? We also want to use hipache for our loadbalancers. But we need SNI because of different tlds using the same backend application.\n. ",
    "fanatic": "Thanks for the encouragement (this is my first pull request)!  I'll clean it up -- thanks for your feedback.\n. I'd still like to refactor some of the get-then-set methods to respect versioning for atomic changes.\n@dmp42 do you have any suggestions for cleaning up the entries in zookeeper that the unit tests add?  I've been cleaning them up by hand after each run. \nZookeeper\n    zookeeper://:2182\n      \u2713 Domain with no match, no fallback\n      \u2713 Single domain with a backend\n      \u2713 Single domain with multiple backends\n      \u2713 Single domain with multiple backends and fallback\n      \u2713 Single domain with multiple backends and fallback plus dead\n      \u2713 Single domain with multiple backends and fallback plus a second dead\n      \u2713 ... let it expire (1505ms)\n    zookeeper://:2182/#someprefix\n      \u2713 Domain with no match, no fallback\n      \u2713 Single domain with a backend\n      \u2713 Single domain with multiple backends\n      \u2713 Single domain with multiple backends and fallback\n      \u2713 Single domain with multiple backends and fallback plus dead\n      \u2713 Single domain with multiple backends and fallback plus a second dead\n      \u2713 ... let it expire (1504ms)\n14 passing (3s)\n. Sounds like a plan.  I was starting the server like this:\n/usr/lib/zookeeper/bin/zkServer.sh start ./zoo-dev.cfg\nAnd using this config:\n```\nzoo-dev.cfg\nThe number of milliseconds of each tick\ntickTime=2000\nThe number of ticks that the initial\nsynchronization phase can take\ninitLimit=10\nThe number of ticks that can pass between\nsending a request and getting an acknowledgement\nsyncLimit=5\nmaxClientCnxns=1000\nthe directory where the snapshot is stored.\ndataDir=/tmp/hipache-test/zoo-data\nthe port at which the clients will connect\nclientPort=2182\n```\n. Apologies for the delay.  I believe I've addressed your concerns to the best of my abilities.  I'm forced to drop the configuration file as ZK expects it to exist on disk, and I clean up the data directory during 'stop'.\nZK performance has always been respectable in my experience.  On our 5 node cluster, we regularly reach 500k op/s with various applications with latency under 1ms, but I'm happy to make improvements if you have some suggestions.\n. Thanks for the encouragement (this is my first pull request)!  I'll clean it up -- thanks for your feedback.\n. I'd still like to refactor some of the get-then-set methods to respect versioning for atomic changes.\n@dmp42 do you have any suggestions for cleaning up the entries in zookeeper that the unit tests add?  I've been cleaning them up by hand after each run. \nZookeeper\n    zookeeper://:2182\n      \u2713 Domain with no match, no fallback\n      \u2713 Single domain with a backend\n      \u2713 Single domain with multiple backends\n      \u2713 Single domain with multiple backends and fallback\n      \u2713 Single domain with multiple backends and fallback plus dead\n      \u2713 Single domain with multiple backends and fallback plus a second dead\n      \u2713 ... let it expire (1505ms)\n    zookeeper://:2182/#someprefix\n      \u2713 Domain with no match, no fallback\n      \u2713 Single domain with a backend\n      \u2713 Single domain with multiple backends\n      \u2713 Single domain with multiple backends and fallback\n      \u2713 Single domain with multiple backends and fallback plus dead\n      \u2713 Single domain with multiple backends and fallback plus a second dead\n      \u2713 ... let it expire (1504ms)\n14 passing (3s)\n. Sounds like a plan.  I was starting the server like this:\n/usr/lib/zookeeper/bin/zkServer.sh start ./zoo-dev.cfg\nAnd using this config:\n```\nzoo-dev.cfg\nThe number of milliseconds of each tick\ntickTime=2000\nThe number of ticks that the initial\nsynchronization phase can take\ninitLimit=10\nThe number of ticks that can pass between\nsending a request and getting an acknowledgement\nsyncLimit=5\nmaxClientCnxns=1000\nthe directory where the snapshot is stored.\ndataDir=/tmp/hipache-test/zoo-data\nthe port at which the clients will connect\nclientPort=2182\n```\n. Apologies for the delay.  I believe I've addressed your concerns to the best of my abilities.  I'm forced to drop the configuration file as ZK expects it to exist on disk, and I clean up the data directory during 'stop'.\nZK performance has always been respectable in my experience.  On our 5 node cluster, we regularly reach 500k op/s with various applications with latency under 1ms, but I'm happy to make improvements if you have some suggestions.\n. ",
    "zeisss": "+1\n. +1\n. ",
    "phlegx": "I solved it with using a unified certificate as it is described here: https://www.startssl.com/?app=42\n. I solved it with using a unified certificate as it is described here: https://www.startssl.com/?app=42\n. ",
    "bobeagan": "dupe of #144 \n. dupe of #144 \n. ",
    "EslamElHusseiny": "sorry, there was a typo in config file path :)\n. sorry, there was a typo in config file path :)\n. ",
    "d7p": "Yes. That is what i was asking thanks for the quick reply. Are there plans to add support for IMAP/POP3 any other protocols than HTTP.\n. Ok. maybe adding support through modules like nginx does (http://wiki.nginx.org/Modules) so you can include them if need.\n. Yes. That is what i was asking thanks for the quick reply. Are there plans to add support for IMAP/POP3 any other protocols than HTTP.\n. Ok. maybe adding support through modules like nginx does (http://wiki.nginx.org/Modules) so you can include them if need.\n. ",
    "jgallen23": "@jkingyens that looks awesome!  Would be great to get that feature in the main hipache repo\n. @jkingyens that looks awesome!  Would be great to get that feature in the main hipache repo\n. ",
    "denderello": "My bad. Based the PR against the wrong project. Please ignore it. :)\n. My bad. Based the PR against the wrong project. Please ignore it. :)\n. ",
    "ryandub": "@willdurand Yes, you could run Nginx behind Hipache and have it hand out the redirects without this PR. However, I know that some people probably don't need/want multiple proxies and may want to just have everything handled by Hipache.\n. @willdurand Yes, you could run Nginx behind Hipache and have it hand out the redirects without this PR. However, I know that some people probably don't need/want multiple proxies and may want to just have everything handled by Hipache.\n. ",
    "alibby": "Bump.\n. I do indeed. \nRunning as root on an ubuntu 14.04 docker image.\nMy config:\n{\n    \"server\": {\n        \"accessLog\": \"/var/log/hipache_access.log\",\n        \"workers\": 2,\n        \"maxSockets\": 100,\n        \"deadBackendTTL\": 30\n    },\n    \"http\": {\n        \"port\": 80,\n        \"bind\": [\"0.0.0.0\"]\n    },\n    \"driver\": \"redis://redis:6379/0\"\n}\n. Looks fixed to me, thanks!\n. Bump.\n. I do indeed. \nRunning as root on an ubuntu 14.04 docker image.\nMy config:\n{\n    \"server\": {\n        \"accessLog\": \"/var/log/hipache_access.log\",\n        \"workers\": 2,\n        \"maxSockets\": 100,\n        \"deadBackendTTL\": 30\n    },\n    \"http\": {\n        \"port\": 80,\n        \"bind\": [\"0.0.0.0\"]\n    },\n    \"driver\": \"redis://redis:6379/0\"\n}\n. Looks fixed to me, thanks!\n. ",
    "raelmax": "The same problem here. :(\n. I install hipache from npm and i'm using a config.json file from readme in the master branch instead using from 0.3 tag as recommended. Using a config.json example from 0.3.1 tag works fine.\nhttps://github.com/hipache/hipache/tree/0.3.1\nSee the warning:\nhttps://github.com/hipache/hipache/blob/master/README.md#warning\n. The same problem here. :(\n. I install hipache from npm and i'm using a config.json file from readme in the master branch instead using from 0.3 tag as recommended. Using a config.json example from 0.3.1 tag works fine.\nhttps://github.com/hipache/hipache/tree/0.3.1\nSee the warning:\nhttps://github.com/hipache/hipache/blob/master/README.md#warning\n. ",
    "ghost": "If you are on a recent (>=  2.6.26) Kernel, try Capabilities:\nbash\nsetcap 'cap_net_bind_service=+ep' /usr/local/bin/hipache\n. If you are on a recent (>=  2.6.26) Kernel, try Capabilities:\nbash\nsetcap 'cap_net_bind_service=+ep' /usr/local/bin/hipache\n. ",
    "slawo-ch": "Hey thanks for commenting,\nyes, I understand the chain cannot be validated. I do care only about the last IP talking to the proxy, which is something hipache should know about with certainty. I'm also fine with any other header that would contain the IP hipache talks to. \nReasons are manifold: rate limiting performance sensitive or security sensitive end points based on originating IP, logging malicious requests, tracing attackers, etc.\n. Thanks,\nwell, the quasi-standard way seems to be to append to the header. The header is supposed to contain a comma separated value for a chain of proxies. Nginx does just that as well.\nhttp://en.wikipedia.org/wiki/X-Forwarded-For\n\nThe general format of the field is:\nX-Forwarded-For: client, proxy1, proxy2\nwhere the value is a comma+space separated list of IP addresses, the left-most being the original client, and each successive proxy that passed the request adding the IP address where it received the request from. In this example, the request passed through proxy1, proxy2, and then proxy3 (not shown in the header). proxy3 appears as remote address of the request.\n. I can try and do that, but I'd need directions about the branches to touch.\n\nI understand master is somehow off to new pastures experimenting with re-inventing http-proxy, potentially dropping support for websockets, depend on different versions of node itself, etc. \nSince I'm not that much at home with nodejs ecosystem, I don't just know what to do and what/how to test.\nIf you'd enlighten me there, I'll give it a shot.\n. OK, I'll see what I can do.\n. Hey thanks for commenting,\nyes, I understand the chain cannot be validated. I do care only about the last IP talking to the proxy, which is something hipache should know about with certainty. I'm also fine with any other header that would contain the IP hipache talks to. \nReasons are manifold: rate limiting performance sensitive or security sensitive end points based on originating IP, logging malicious requests, tracing attackers, etc.\n. Thanks,\nwell, the quasi-standard way seems to be to append to the header. The header is supposed to contain a comma separated value for a chain of proxies. Nginx does just that as well.\nhttp://en.wikipedia.org/wiki/X-Forwarded-For\n\nThe general format of the field is:\nX-Forwarded-For: client, proxy1, proxy2\nwhere the value is a comma+space separated list of IP addresses, the left-most being the original client, and each successive proxy that passed the request adding the IP address where it received the request from. In this example, the request passed through proxy1, proxy2, and then proxy3 (not shown in the header). proxy3 appears as remote address of the request.\n. I can try and do that, but I'd need directions about the branches to touch.\n\nI understand master is somehow off to new pastures experimenting with re-inventing http-proxy, potentially dropping support for websockets, depend on different versions of node itself, etc. \nSince I'm not that much at home with nodejs ecosystem, I don't just know what to do and what/how to test.\nIf you'd enlighten me there, I'll give it a shot.\n. OK, I'll see what I can do.\n. ",
    "teemow": "Sry this was meant to be in our fork :cake: \n. Sry this was meant to be in our fork :cake: \n. ",
    "ipanousis": "+1 waiting for this\n. thanks, in this case I will tolerate the nginx workaround but am very much looking forward to seeing hipache taking care of this\n. +1 waiting for this\n. thanks, in this case I will tolerate the nginx workaround but am very much looking forward to seeing hipache taking care of this\n. ",
    "msabramo": "has merge conflicts now\n. Cool!\nCc: @fsouza, @andrewsmedina\n. Well yeah static files are fastest and safest. \nHowever the case I'm interested in here is error pages - namely:\n1. Error page when client hits a vhost that hipache doesn't yet know about. \n2. Error page when client hits a vhost that has no available backends. \nThis is in the context of a PaaS (tsuru) when a developer registers an application and has not deployed it to any nodes for example. They are not going to be hitting this page a lot. But when they do, it would be nice if the error page could tell them what vhost it was that hipache was dealing with. It may not be obvious, because the request could be going through another router or load balancer that maps requests to a vhost that the user never sees from the outside. \nSo this case could benefit from having the ability to display a small amount of dynamic info and it wouldn't be particularly concerned with squeezing out every last drop of performance. \nWhat I had in mind is very minimal. Not a full-blown templating language like Jinja2. Something logic-less and fast like Mustache or perhaps even as simple as just doing string.replace on a few predefined variables. Should be very fast and secure. \n. See https://github.com/hipache/hipache/issues/193 to see how minimal I mean. Rough cut since I did it on an iPhone. :smile: \n. I'd be surprised if doing string.replace a handful of times takes time comparable to doing a query to Redis. \n. See https://github.com/tsuru/tsuru/issues/970 for details about my motivation for this. \n. Thanks for your comments!\nI agree that what you said would be true if accessing hipache directly. \nBut in my case, there is a layer of indirection. The user goes to a URL like http://testenv/ or http://testenv/getstarted, which is an nginx server that proxies various paths to different vhosts - e.g.:\n/ => http://anonweb.paas.dev/\n/getstarted => http://profileweb.paas.dev/getstarted\n/upgrade => http://billweb.paas.dev/upgrade\nThe *.paas.dev vhosts are hipache. \nSo with this setup, if a user goes to http://testenv/getstarted and gets a hipache error, they don't know that it's because of a problem with the \"profileweb\" vhost. \nThis is why I want to add the vhost to the error page. \nI hope that makes sense?\n. Yeah, agreed that this is a very specific use case. And I'm starting to think maybe doing string substitution is a bit too much to solve this small problem, unless there were other uses for the string substitution.\nI'm now thinking about just adding a small bit of code to add the vhost to the output. That would be simpler and doesn't require inventing a substitution syntax or modifying the templates.\n. Thanks! I didn't see those files. I guess I expected them to be in the README.md in the root directory. Not a bad idea to have them in a special place, though I think I might add a link to the main README.md to make them more discoverable. See https://github.com/hipache/hipache/pull/197\n. I don't think it's redis-server as that is getting launched (and also not cleaned up; which I should add to #195).\n```\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n22379 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 4000\n\n1) \"before all\" hook\n0 passing (4s)\n  1 failing\n1)  \"before all\" hook:\n     Uncaught Error: spawn ENOENT\n      at errnoException (child_process.js:1011:11)\n      at Process.ChildProcess._handle.onexit (child_process.js:802:34)\nnpm ERR! Test failed.  See above for more details.\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n22379 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n65544 redis-server :7777\n65552 redis-server :7001\n65554 redis-server :7002\n65555 redis-server :7003\n```\nIt must be some other process it's trying to launch, but unfortunately the error message is not terribly helpful.\nI think it's not:\n- redis-server\n- memcached\n- hipache\nMaybe zookeeper or etcd as IIRC I saw some references to those.\n. It appears to be zookeeper that's causing the problem for me on OS X. When I set NO_ZOOKEEPER=1, then the spawn error goes away (though I'm now getting  a bunch of failures in memcached tests that all say Error: timeout of 2000ms exceeded).\nThis is on OS X 10.9.5 and I've installed zookeeper with homebrew.\n. I have all of those installed actually. But for some reason I get problems with some of them. I don't know why yet.\n. Looks probably related that Homebrew renames zkServer to zkServer.sh?\n[marca@marca-mac2 hipache]$ which zkServer.sh\nzkServer.sh not found\n[marca@marca-mac2 hipache]$ which zkServer\n/usr/local/bin/zkServer\n[marca@marca-mac2 hipache]$ zkServer\nJMX enabled by default\nUsing config: /usr/local/etc/zookeeper/zoo.cfg\nUsage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}\nMade a symlink and then the spawn error goes away and I get a timeout error:\n```\n[marca@marca-mac2 hipache]$ sudo ln -s zkServer /usr/local/bin/zkServer.sh\n[marca@marca-mac2 hipache]$ zkServer.sh\nJMX enabled by default\nUsing config: /usr/local/etc/zookeeper/zoo.cfg\nUsage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 4000\n\nERR! Server#zkServer.sh  JMX enabled by default\nERR! Server#zkServer.sh\nERR! Server#zkServer.sh  Using config: /Users/marca/zookeeper/zoo.cfg\nERR! Server#zkServer.sh\n  1) \"before all\" hook\n  2) \"after all\" hook\n0 passing (14s)\n  2 failing\n1)  \"before all\" hook:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\n2)  \"after all\" hook:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\nnpm ERR! Test failed.  See above for more details.\n```\n. Well, it does get rid of the zookeeper failures, but then I get etcd and memcached failures and not because I don't have them installed.\ne.g.:\n```\n...\n  1) Etcd etcd://:8001 Domain with no match, no fallback:\n  Uncaught AssertionError: expected [ Array(3) ] to deeply equal [ [], [], [] ]\n  + expected - actual\n\n   [\n     []\n  -  [\n  -    \"supervhost\"\n  -    \"backend:910\"\n  -  ]\n     []\n  +  []\n   ]\n\n  at /Users/marca/dev/git-repos/hipache/test/unit/driver-test-reading.js:20:33\n  at lookup (/Users/marca/dev/git-repos/hipache/lib/drivers/etcd.js:108:21)\n  at /Users/marca/dev/git-repos/hipache/lib/drivers/etcd.js:118:21\n  at Client._handleResponse (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/lib/client.js:78:14)\n  at Request._callback (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/lib/client.js:32:22)\n  at Request.self.callback (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/node_modules/request/request.js:122:22)\n  at Request.emit (events.js:98:17)\n  at Request.<anonymous> (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/node_modules/request/request.js:888:14)\n  at Request.emit (events.js:117:20)\n  at IncomingMessage.<anonymous> (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/node_modules/request/request.js:839:12)\n  at IncomingMessage.emit (events.js:117:20)\n  at _stream_readable.js:944:16\n  at process._tickCallback (node.js:442:13)\n\n...\n  19) Memcached memcached://:9001/#prefix,memcached://:9002/#prefix Single domain with multiple backends and fallback plus a second dead:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\n```\n. OK, well this works:\n$ NO_ETCD=true NO_MEMCACHED=true NO_ZOOKEEPER=true npm test\nbut it's curious that I have to disable 3 things to get the tests to pass. Thought it may very well be a configuration problem on my system.\n. Interesting that when I run without NO_ZOOKEEPER=true, it fails but it is definitely able to talk to Zookeeper, because it leaves it running after the test errors out (https://github.com/hipache/hipache/issues/195):\n```\n[marca@marca-mac2 hipache]$ pgrep -fl 'memcache'; pgrep -fl 'redis-server'; pgrep -fl 'hipache'; pgrep -fl 'zookeeper'; pgrep -fl 'etcd'\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 4000\n\nERR! Server#zkServer.sh  JMX enabled by default\nERR! Server#zkServer.sh\nERR! Server#zkServer.sh  Using config: /Users/marca/zookeeper/zoo.cfg\nERR! Server#zkServer.sh\n  1) \"before all\" hook\n  2) \"after all\" hook\n0 passing (14s)\n  2 failing\n1)  \"before all\" hook:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\n2)  \"after all\" hook:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\nnpm ERR! Test failed.  See above for more details.\n[marca@marca-mac2 hipache]$ pgrep -fl 'memcache'; pgrep -fl 'redis-server'; pgrep -fl 'hipache'; pgrep -fl 'zookeeper'; pgrep -fl 'etcd'\n28552 /usr/bin/java -Dzookeeper.log.dir=/Users/marca/zookeeper/ -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../build/classes:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../build/lib/.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/slf4j-log4j12-1.6.1.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/slf4j-api-1.6.1.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/netty-3.7.0.Final.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/log4j-1.2.16.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/jline-0.9.94.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../zookeeper-3.4.6.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../src/java/lib/.jar:/usr/local/etc/zookeeper: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /Users/marca/zookeeper/zoo.cfg\n```\n. I also tried raising the timeout from 4000ms to 16000ms and that didn't help.\n```\n[marca@marca-mac2 hipache]$ git diff\ndiff --git a/package.json b/package.json\nindex c05b7dd..7bdadfc 100644\n--- a/package.json\n+++ b/package.json\n@@ -48,7 +48,7 @@\n   },\n   \"scripts\": {\n     \"start\": \"./bin/hipache\",\n-    \"test\": \"istanbul test _mocha --report html -- test//*.js --reporter spec --timeout 4000\",\n+    \"test\": \"istanbul test _mocha --report html -- test//.js --reporter spec --timeout 16000\",\n     \"coveralls\": \"istanbul cover _mocha --report lcovonly -- test//.js -R spec && cat ./coverage/lcov.info | coveralls && rm -rf ./coverage\",\n     \"hint\": \"gulp hint\"\n   },\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 16000\n\nERR! Server#zkServer.sh  JMX enabled by default\nERR! Server#zkServer.sh\nERR! Server#zkServer.sh  Using config: /Users/marca/zookeeper/zoo.cfg\nERR! Server#zkServer.sh\n  1) \"before all\" hook\n  2) \"after all\" hook\n0 passing (37s)\n  2 failing\n1)  \"before all\" hook:\n     Error: timeout of 16000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\n2)  \"after all\" hook:\n     Error: timeout of 16000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\nnpm ERR! Test failed.  See above for more details.\n``\n. Working on adding aCONTRIBUTING.mdfile in https://github.com/hipache/hipache/pull/197\n. I think I see the problem - comma instead of space in thezkServer.shcommand. I'll send a PR later.\n. comma/space thing was a red herring.\n. Just updated #197 \n. Ditto forredis-server`:\n```\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n22379 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 4000\n\n1) \"before all\" hook\n0 passing (4s)\n  1 failing\n1)  \"before all\" hook:\n     Uncaught Error: spawn ENOENT\n      at errnoException (child_process.js:1011:11)\n      at Process.ChildProcess._handle.onexit (child_process.js:802:34)\nnpm ERR! Test failed.  See above for more details.\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n22379 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n65544 redis-server :7777\n65552 redis-server :7001\n65554 redis-server :7002\n65555 redis-server :7003\n``\n. Ditto foretcd`:\n\u276f pgrep -fl 'memcache'; pgrep -fl 'redis-server'; pgrep -fl 'hipache'; pgrep -fl 'zookeeper'; pgrep -fl 'etcd'\n44293 memcached -p 9001\n44294 memcached -p 9002\n44287 redis-server *:7777\n44296 redis-server *:7001\n44297 redis-server *:7002\n44298 redis-server *:7003\n44288 /usr/local/bin/node --expose-gc --always-compact -- ./bin/hipache -c test/fixtures/configs/hipache-config.json\n44290 /usr/local/Cellar/node/0.10.35_1/bin/node --expose-gc --always-compact -- /Users/marca/dev/git-repos/hipache/bin/hipache -c test/fixtures/configs/hipache-config.json\n44291 /usr/local/Cellar/node/0.10.35_1/bin/node --expose-gc --always-compact -- /Users/marca/dev/git-repos/hipache/bin/hipache -c test/fixtures/configs/hipache-config.json\n34906 vim test/unit/driver-zookeeper.js +9\n44292 etcd -bind-addr=127.0.0.1:8001 -addr=127.0.0.1:8001 -peer-bind-addr=127.0.0.1:8011 -peer-addr=127.0.0.1:8011\n. Is this related to the fact that the process seems to exit right away when it hits the first error? I.e.: I don't see all the failures at once only the first one. Is there a way to get it to keep going on an error so that it can report all the test results and then clean up?\n. Yeah, test failure looks spurious and unrelated to my change. \nCan an admin please tell Travis to retest?\n. OK, I moved the testing info from README.md to CONTRIBUTING.md (and put a link in README.md to CONTRIBUTING.md.\nI'll leave it to someone more experienced to:\n- Document the canonical way to run the tests - I'm getting a lot of failures, so I'm not sure I'm doing stuff right.\n- Document what the standards are for coding, commit messages, etc.\n. Implemented @willdurand's suggestion.\nSee: https://github.com/msabramo/hipache/blob/patch-4/CONTRIBUTING.md\n. See https://github.com/hipache/hipache/pull/199 for a possible solution...\n. [marca@marca-mac2 hipache]$ node --version\nv0.10.35\n[marca@marca-mac2 hipache]$ redis-server --version\nRedis server v=2.8.17 sha=00000000:0 malloc=libc bits=64 build=32eb139b4f2b63\n[marca@marca-mac2 hipache]$ npm list | grep redis\n\u251c\u2500\u2500 redis@0.10.3\nnpm ERR! extraneous: npm@1.4.10 /Users/marca/dev/git-repos/hipache/node_modules/npm\nnpm ERR! extraneous: inherits@2.0.1 /Users/marca/dev/git-repos/hipache/node_modules/npm/node_modules/inherits\nnpm ERR! extraneous: normalize-package-data@1.0.3 /Users/marca/dev/git-repos/hipache/node_modules/npm/node_modules/normalize-package-data\nnpm ERR! not ok code 0\n[marca@marca-mac2 hipache]$ ps aux | grep redis\nmarca           98256   0.0  0.1  2506948  14564 s002  T     4:21PM   0:06.19 vim lib/drivers/redis.js +38\nmarca           24616   0.0  0.0  2440976    528 s002  R+   11:08PM   0:00.00 grep redis\n. Hmmm, those npm ERR! look concerning. I'm going to try blowing away node_modules and doing npm install again.\n. 1 test failed.\n[marca@marca-mac2 hipache]$ npm list | grep redis\n\u251c\u2500\u2500 redis@0.10.3\nAlso upgraded redis-server.\n[marca@marca-mac2 hipache]$ redis-server --version\nRedis server v=2.8.19 sha=00000000:0 malloc=libc bits=64 build=f9a9a1252a078d97\nTests still failing:\n```\n[marca@marca-mac2 hipache]$ gulp test:unit\n[23:11:47] Using gulpfile ~/dev/git-repos/hipache/gulpfile.js\n[23:11:47] Starting 'test:unit'...\nConfig\n    Configuration loading\n      \u2713 From string\n      \u2713 From buffer\n      \u2713 From object\n    Broken...\n      \u2713 ... broken json\n      \u2713 ... https missing key file\n      \u2713 ... https missing cert file\n      \u2713 ... https missing ca file\n      \u2713 ... https missing key\n      \u2713 ... https missing cert\n    Running as root\n      \u2713 Not running root test as we are not root...\nDriver factory\n    #existing-drivers\n      \u2713 redis:\n      \u2713 memcached://\n      \u2713 etcd: (97ms)\n      \u2713 etcds://\n      \u2713 redis:///\u00df\u221e\n      1) redis:///\u00df\u221e\n      \u2713 redis://,redis://\n    #erroring-on-bogus-urls\n      \u2713 \"nopassaran://\" (unregistered) should emit error\n      \u2713 \"bogus\" (no scheme) should emit error\n      \u2713 drivertpl:// (which is broken) should emit error\nLogger\n    #creation\n      \u2713 err with undefined path\n      \u2713 err trying to open a folder\n      \u2713 err trying to open file in nested non existent directory\n      \u2713 err trying to open file with no permissions\n    #trying to log in the wrong state\n      \u2713 stream has been closed\n    #legit logging\n      \u2713 empty data\n      \u2713 legit data\nLru\n    #dumb lru\n      \u2713 Get, set, del\n    #real lru\n      \u2713 Simple get, set, del\n    #real lru limits\n      \u2713 Reaching the number of objects limits\n      \u2713 Reaching the ttl (52ms)\n30 passing (228ms)\n  1 failing\n1) Driver factory #existing-drivers redis:///\u00df\u221e:\n     Uncaught\n  DriverError: Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED\n      at null. (/Users/marca/dev/git-repos/hipache/lib/drivers/redis.js:38:35)\n      at RedisClient.emit (events.js:95:17)\n      at RedisClient.on_error (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:185:10)\n      at Socket. (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:95:14)\n      at Socket.emit (events.js:95:17)\n      at net.js:441:14\n      at process._tickCallback (node.js:442:13)\n[23:11:47] 'test:unit' errored after 291 ms\n[23:11:47] Error in plugin 'gulp-mocha'\n1 test failed.\n```\n. On my Ubuntu 14.04 system where all the tests pass, I have:\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ node --version\nv0.10.26\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ redis-server --version\nRedis server v=2.8.4 sha=00000000:0 malloc=jemalloc-3.4.1 bits=64 build=a44a05d76f06a5d9\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ npm list | grep redis\n\u251c\u2500\u2500 redis@0.10.3\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ ps aux | grep redis\nredis     1176  0.1  0.7  36996  7296 ?        Ssl  06:20   0:03 /usr/bin/redis-server 127.0.0.1:6379\nvagrant   7567  0.0  0.0  10468   916 pts/1    S+   07:14   0:00 grep --color=auto redis\n. Ah ha. I noticed that my Ubuntu 14.04 had a redis-server running and my OS X did not. I did:\n[marca@marca-mac2 hipache]$ launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.redis.plist\n[marca@marca-mac2 hipache]$ launchctl load ~/Library/LaunchAgents/homebrew.mxcl.redis.plist\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n25424 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n98256 vim lib/drivers/redis.js +38\nand now the tests pass.\nThat seems wrong though that the tests would require a redis server to already be running...? They should be starting their own redis server, no?\n. Just to add extra proof, I stopped redis-server on the Ubuntu box and then I get the same failure there:\n```\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ sudo /etc/init.d/redis-server stop\nStopping redis-server: redis-server.\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ gulp test:unit\n[07:17:36] Using gulpfile ~/hipache/gulpfile.js\n[07:17:36] Starting 'test:unit'...\nConfig\n    Configuration loading\n      \u2713 From string\n      \u2713 From buffer\n      \u2713 From object\n    Broken...\n      \u2713 ... broken json\n      \u2713 ... https missing key file\n      \u2713 ... https missing cert file\n      \u2713 ... https missing ca file\n      \u2713 ... https missing key\n      \u2713 ... https missing cert\n    Running as root\n      \u2713 Not running root test as we are not root...\nDriver factory\n    #existing-drivers\n      \u2713 redis:\n      \u2713 memcached://\n      \u2713 etcd: (95ms)\n      \u2713 etcds://\n      \u2713 redis:///\u00df\u221e\n      1) redis:///\u00df\u221e\n      \u2713 redis://,redis://\n    #erroring-on-bogus-urls\n      \u2713 \"nopassaran://\" (unregistered) should emit error\n      \u2713 \"bogus\" (no scheme) should emit error\n      \u2713 drivertpl:// (which is broken) should emit error\nLogger\n    #creation\n      \u2713 err with undefined path\n      \u2713 err trying to open a folder\n      \u2713 err trying to open file in nested non existent directory\n      \u2713 err trying to open file with no permissions\n    #trying to log in the wrong state\n      \u2713 stream has been closed\n    #legit logging\n      \u2713 empty data\n      \u2713 legit data\nLru\n    #dumb lru\n      \u2713 Get, set, del\n    #real lru\n      \u2713 Simple get, set, del\n    #real lru limits\n      \u2713 Reaching the number of objects limits\n      \u2713 Reaching the ttl (53ms)\n30 passing (258ms)\n  1 failing\n1) Driver factory #existing-drivers redis:///\u00df\u221e:\n     Uncaught\n  DriverError: Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED\n      at null. (/home/vagrant/hipache/lib/drivers/redis.js:38:35)\n      at RedisClient.EventEmitter.emit (events.js:95:17)\n      at RedisClient.on_error (/home/vagrant/hipache/node_modules/redis/index.js:185:10)\n      at Socket. (/home/vagrant/hipache/node_modules/redis/index.js:95:14)\n      at Socket.EventEmitter.emit (events.js:95:17)\n      at net.js:440:14\n      at process._tickCallback (node.js:415:13)\n[07:17:37] 'test:unit' errored after 323 ms\n[07:17:37] Error in plugin 'gulp-mocha'\n1 test failed.\n```\n. So now the question is why do the tests rely on having a redis server running and how can that be fixed?\n. Alright, I'll close this. Hopefully someone else can figure out why that test is failing.\n. Tried this:\n``` diff\n\u276f git diff\ndiff --git a/lib/drivers/redis.js b/lib/drivers/redis.js\nindex fe8d155..8565f76 100644\n--- a/lib/drivers/redis.js\n+++ b/lib/drivers/redis.js\n@@ -24,6 +24,11 @@\n             prefix = slave.hash.substr(1);\n         }\n\nclient.on('error', function (err) {\n// Re-emit unspecified error as is\nthis.emit(this.ERROR, new DriverError(DriverError.UNSPECIFIED, err));\n\n}.bind(this));\n+\n         var db;\n         if (slave.path && (db = slave.path.substr(1))) {\n             client.select(db);\n@@ -33,11 +38,6 @@\n             client.auth(password);\n         }\n\n\nclient.on('error', function (err) {\n\n// Re-emit unspecified error as is\nthis.emit(this.ERROR, new DriverError(DriverError.UNSPECIFIED, err));\n\n}.bind(this));\n client.on('ready', function (err) {\n     clientReady = true;\n     if (!clientWrite || clientWriteReady) {\n\n```\n\n\nStill fails:\n```\n[marca@marca-mac2 hipache]$ gulp test:unit\n[22:14:33] Using gulpfile ~/dev/git-repos/hipache/gulpfile.js\n[22:14:33] Starting 'test:unit'...\nConfig\n    Configuration loading\n      \u2713 From string\n      \u2713 From buffer\n      \u2713 From object\n    Broken...\n      \u2713 ... broken json\n      \u2713 ... https missing key file\n      \u2713 ... https missing cert file\n      \u2713 ... https missing ca file\n      \u2713 ... https missing key\n      \u2713 ... https missing cert\n    Running as root\n      \u2713 Not running root test as we are not root...\nDriver factory\n    #existing-drivers\n      \u2713 redis:\n      \u2713 memcached://\n      \u2713 etcd: (101ms)\n      \u2713 etcds://\n      \u2713 redis:///\u00df\u221e\n      1) redis:///\u00df\u221e\n      \u2713 redis://,redis://\n    #erroring-on-bogus-urls\n      \u2713 \"nopassaran://\" (unregistered) should emit error\n      \u2713 \"bogus\" (no scheme) should emit error\n      \u2713 drivertpl:// (which is broken) should emit error\nLogger\n    #creation\n      \u2713 err with undefined path\n      \u2713 err trying to open a folder\n      \u2713 err trying to open file in nested non existent directory\n      \u2713 err trying to open file with no permissions\n    #trying to log in the wrong state\n      \u2713 stream has been closed\n    #legit logging\n      \u2713 empty data\n      \u2713 legit data\nLru\n    #dumb lru\n      \u2713 Get, set, del\n    #real lru\n      \u2713 Simple get, set, del\n    #real lru limits\n      \u2713 Reaching the number of objects limits\n      \u2713 Reaching the ttl (53ms)\n30 passing (232ms)\n  1 failing\n1) Driver factory #existing-drivers redis:///\u00df\u221e:\n     Uncaught\n  DriverError: Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED\n      at null. (/Users/marca/dev/git-repos/hipache/lib/drivers/redis.js:29:35)\n      at RedisClient.emit (events.js:95:17)\n      at RedisClient.on_error (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:185:10)\n      at Socket. (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:95:14)\n      at Socket.emit (events.js:95:17)\n      at net.js:441:14\n      at process._tickCallback (node.js:442:13)\n[22:14:33] 'test:unit' errored after 296 ms\n[22:14:33] Error in plugin 'gulp-mocha'\n1 test failed.\n```\nLet me know if there's anything else you want me to try.\n. By the way, here are my locale settings:\n[marca@marca-mac2 hipache]$ locale\nLANG=\"en_US.UTF-8\"\nLC_COLLATE=\"en_US.UTF-8\"\nLC_CTYPE=\"en_US.UTF-8\"\nLC_MESSAGES=\"en_US.UTF-8\"\nLC_MONETARY=\"en_US.UTF-8\"\nLC_NUMERIC=\"en_US.UTF-8\"\nLC_TIME=\"en_US.UTF-8\"\nLC_ALL=\"en_US.UTF-8\"\n. Here's something kind of odd to me. The tests pass fine in an Ubuntu 14.04 VM and it says there are 30 passing tests.\n```\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ gulp test:unit\n...\n  30 passing (255ms)\n[06:28:31] Finished 'test:unit' after 318 ms\n```\nOn OS X, it says that one test fails but it also says that 30 tests are passing. How can it have the same number of passing tests + 1 that fails?\n```\n[marca@marca-mac2 hipache]$ gulp test:unit\n[22:31:06] Using gulpfile ~/dev/git-repos/hipache/gulpfile.js\n[22:31:06] Starting 'test:unit'...\n...\n  30 passing (228ms)\n  1 failing\n1) Driver factory #existing-drivers redis:///\u00df\u221e:\n     Uncaught\n  DriverError: Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED\n      at null. (/Users/marca/dev/git-repos/hipache/lib/drivers/redis.js:38:35)\n      at RedisClient.emit (events.js:95:17)\n      at RedisClient.on_error (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:185:10)\n      at Socket. (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:95:14)\n      at Socket.emit (events.js:95:17)\n      at net.js:441:14\n      at process._tickCallback (node.js:442:13)\n[22:31:06] 'test:unit' errored after 291 ms\n[22:31:06] Error in plugin 'gulp-mocha'\n1 test failed.\n```\n. LGTM\n. @dmp42: How come I hit this redis erroring twice even though you and @willdurand did not?\n. Yeah, this can probably be closed after #203 is merged.\n. I still have problem that the tests pass but process doesn't exit. Probably waiting for a zookeeper process to terminate because one is left running and when I kill it, test process exits. \nYou guys see this?\n. The code calls kill but I wonder if it's killing the shell script zkServer.sh but leaving the Java process running?\n. For me, the tests were always failing to exit and every time that I looked there was a Java process still running.\nI found a workaround on my system (OS X 10.9.4 with Zookeeper installed from Homebrew). I edited /usr/local/bin/zkServer and added an exec to the last line so it looks like this:\n```\n!/usr/bin/env bash\n. \"/usr/local/etc/zookeeper/defaults\"\ncd \"/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin\"\nexec ./zkServer.sh \"$@\"\n```\nThat seemed to do the trick for me.\n. Also your change in https://github.com/hipache/hipache/pull/210 seems to solve it so probably this can be closed.\n. Tests are working much better on my Mac now. Thanks!\n```\n$ gulp test:drivers\n[21:45:07] Using gulpfile ~/dev/git-repos/hipache/gulpfile.js\n[21:45:07] Starting 'test:drivers'...\n...\n  91 passing (22s)\n[21:45:29] Finished 'test:drivers' after 23 s\n```\n. Agreed on optional. \nAlso think it needs some work so that it maintains a chain of proxies. E.g.:\nUser agent => nginx => hipache => origin server\n. Yeah, what did you have in mind specifically for changing it?\nSurely hipache should preserve data from existing Via headers? I would think it's up to each server to decide how much it's comfortable sharing and here we might want to dial it back. For example, one could make an argument that I should take out the (hipache/0.4.0) part because that might aid someone with bad intentions. OTOH, maybe it doesn't matter so much if the entire feature is optional and turned off by default, like you suggested (and documented the ramifications). I'd probably only use this for dev and testing; not prod. Heck in our prod, there's a layer of F5 load balancers in front of the nginx and our Ops people might've set them up to strip out sensitive headers like these.\nAnyway, definitely going to make it optional like you said. And I'd like to make it preserve other Via headers unless you think that's problematic (another configuration option perhaps?).\nThanks to you and @dmp42 for all your help with hipache -- I'm not experienced with NodeJS (Python is my preferred language), so it's nice to have some folks looking over my shoulder and giving some bits of advice.\n. I found a nicer way to do this using middleware. I plan to update it shortly.\n. OK, just updated this. Now it:\n1. Uses connect middleware, which I think is much cleaner.\n2. Has a configuration setting and it's off by default:\njavascript\n    if (config.server.setViaResponseHeader) {\n        app.use(setViaResponseHeader);\n    }\nI add:\n\"setViaResponseHeader\": true,\nto my config/config.json and run bin/hipache and hit it:\n$ http HEAD http://my-cool-app.127.0.0.1.xip.io:8080/apps\nHTTP/1.1 400 Bad Request\nConnection: keep-alive\nDate: Thu, 08 Jan 2015 00:32:01 GMT\nVia: 1.1 my-cool-app.127.0.0.1.xip.io:8080 (hipache/0.4.0)\ncache-control: no-cache\ncontent-type: text/html\nexpires: -1\npragma: no-cache\n. Thanks, will do.\nOther idea I'm toying with is generalizing the middleware and configuration. There are lots of interesting middleware for logging, authentication, error reporting, compression, etc. (see https://github.com/senchalabs/connect#middleware for a bunch of existing middleware). It might be interesting for folks to use them all with hipache but it will be tedious to add support (including a configuration param) for each one. So I was wondering about using something like meddleware that lets you plug middleware in automatically using a config file. Not sure if meddleware can work without express, but I'm pretty sure that the basic concept could be made to work with just connect alone.\nThoughts?\n. Ah so when you mention node-http-proxy middleware I went googling for it. I found one article about it but it seemed to be outdated as it was talking about APIs that are different now. Not sure where or how but I got the impression from googling that the node-http-proxy middleware was removed as they instead got behind \"connect\" which is a general middleware that works with any kind of http server. So I used that.\nIf you think there's something better and you can send me a link to the docs on it, I'll try to use it. \n. See https://github.com/hipache/hipache/pull/213\n. This will need documentation of course but wanted to see first if the idea was sound and desirable.\n. Ping\n. @dmp42: What do you think?\n. @dmp42: Yeah, no rush on my end. I just like to remind sometimes on PRs so that they don't get forgotten about and then merge conflicts happen and what not. If you're busy, it can wait.\n. has merge conflicts now\n. Cool!\nCc: @fsouza, @andrewsmedina\n. Well yeah static files are fastest and safest. \nHowever the case I'm interested in here is error pages - namely:\n1. Error page when client hits a vhost that hipache doesn't yet know about. \n2. Error page when client hits a vhost that has no available backends. \nThis is in the context of a PaaS (tsuru) when a developer registers an application and has not deployed it to any nodes for example. They are not going to be hitting this page a lot. But when they do, it would be nice if the error page could tell them what vhost it was that hipache was dealing with. It may not be obvious, because the request could be going through another router or load balancer that maps requests to a vhost that the user never sees from the outside. \nSo this case could benefit from having the ability to display a small amount of dynamic info and it wouldn't be particularly concerned with squeezing out every last drop of performance. \nWhat I had in mind is very minimal. Not a full-blown templating language like Jinja2. Something logic-less and fast like Mustache or perhaps even as simple as just doing string.replace on a few predefined variables. Should be very fast and secure. \n. See https://github.com/hipache/hipache/issues/193 to see how minimal I mean. Rough cut since I did it on an iPhone. :smile: \n. I'd be surprised if doing string.replace a handful of times takes time comparable to doing a query to Redis. \n. See https://github.com/tsuru/tsuru/issues/970 for details about my motivation for this. \n. Thanks for your comments!\nI agree that what you said would be true if accessing hipache directly. \nBut in my case, there is a layer of indirection. The user goes to a URL like http://testenv/ or http://testenv/getstarted, which is an nginx server that proxies various paths to different vhosts - e.g.:\n/ => http://anonweb.paas.dev/\n/getstarted => http://profileweb.paas.dev/getstarted\n/upgrade => http://billweb.paas.dev/upgrade\nThe *.paas.dev vhosts are hipache. \nSo with this setup, if a user goes to http://testenv/getstarted and gets a hipache error, they don't know that it's because of a problem with the \"profileweb\" vhost. \nThis is why I want to add the vhost to the error page. \nI hope that makes sense?\n. Yeah, agreed that this is a very specific use case. And I'm starting to think maybe doing string substitution is a bit too much to solve this small problem, unless there were other uses for the string substitution.\nI'm now thinking about just adding a small bit of code to add the vhost to the output. That would be simpler and doesn't require inventing a substitution syntax or modifying the templates.\n. Thanks! I didn't see those files. I guess I expected them to be in the README.md in the root directory. Not a bad idea to have them in a special place, though I think I might add a link to the main README.md to make them more discoverable. See https://github.com/hipache/hipache/pull/197\n. I don't think it's redis-server as that is getting launched (and also not cleaned up; which I should add to #195).\n```\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n22379 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 4000\n\n1) \"before all\" hook\n0 passing (4s)\n  1 failing\n1)  \"before all\" hook:\n     Uncaught Error: spawn ENOENT\n      at errnoException (child_process.js:1011:11)\n      at Process.ChildProcess._handle.onexit (child_process.js:802:34)\nnpm ERR! Test failed.  See above for more details.\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n22379 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n65544 redis-server :7777\n65552 redis-server :7001\n65554 redis-server :7002\n65555 redis-server :7003\n```\nIt must be some other process it's trying to launch, but unfortunately the error message is not terribly helpful.\nI think it's not:\n- redis-server\n- memcached\n- hipache\nMaybe zookeeper or etcd as IIRC I saw some references to those.\n. It appears to be zookeeper that's causing the problem for me on OS X. When I set NO_ZOOKEEPER=1, then the spawn error goes away (though I'm now getting  a bunch of failures in memcached tests that all say Error: timeout of 2000ms exceeded).\nThis is on OS X 10.9.5 and I've installed zookeeper with homebrew.\n. I have all of those installed actually. But for some reason I get problems with some of them. I don't know why yet.\n. Looks probably related that Homebrew renames zkServer to zkServer.sh?\n[marca@marca-mac2 hipache]$ which zkServer.sh\nzkServer.sh not found\n[marca@marca-mac2 hipache]$ which zkServer\n/usr/local/bin/zkServer\n[marca@marca-mac2 hipache]$ zkServer\nJMX enabled by default\nUsing config: /usr/local/etc/zookeeper/zoo.cfg\nUsage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}\nMade a symlink and then the spawn error goes away and I get a timeout error:\n```\n[marca@marca-mac2 hipache]$ sudo ln -s zkServer /usr/local/bin/zkServer.sh\n[marca@marca-mac2 hipache]$ zkServer.sh\nJMX enabled by default\nUsing config: /usr/local/etc/zookeeper/zoo.cfg\nUsage: ./zkServer.sh {start|start-foreground|stop|restart|status|upgrade|print-cmd}\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 4000\n\nERR! Server#zkServer.sh  JMX enabled by default\nERR! Server#zkServer.sh\nERR! Server#zkServer.sh  Using config: /Users/marca/zookeeper/zoo.cfg\nERR! Server#zkServer.sh\n  1) \"before all\" hook\n  2) \"after all\" hook\n0 passing (14s)\n  2 failing\n1)  \"before all\" hook:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\n2)  \"after all\" hook:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\nnpm ERR! Test failed.  See above for more details.\n```\n. Well, it does get rid of the zookeeper failures, but then I get etcd and memcached failures and not because I don't have them installed.\ne.g.:\n```\n...\n  1) Etcd etcd://:8001 Domain with no match, no fallback:\n  Uncaught AssertionError: expected [ Array(3) ] to deeply equal [ [], [], [] ]\n  + expected - actual\n\n   [\n     []\n  -  [\n  -    \"supervhost\"\n  -    \"backend:910\"\n  -  ]\n     []\n  +  []\n   ]\n\n  at /Users/marca/dev/git-repos/hipache/test/unit/driver-test-reading.js:20:33\n  at lookup (/Users/marca/dev/git-repos/hipache/lib/drivers/etcd.js:108:21)\n  at /Users/marca/dev/git-repos/hipache/lib/drivers/etcd.js:118:21\n  at Client._handleResponse (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/lib/client.js:78:14)\n  at Request._callback (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/lib/client.js:32:22)\n  at Request.self.callback (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/node_modules/request/request.js:122:22)\n  at Request.emit (events.js:98:17)\n  at Request.<anonymous> (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/node_modules/request/request.js:888:14)\n  at Request.emit (events.js:117:20)\n  at IncomingMessage.<anonymous> (/Users/marca/dev/git-repos/hipache/node_modules/node-etcd/node_modules/request/request.js:839:12)\n  at IncomingMessage.emit (events.js:117:20)\n  at _stream_readable.js:944:16\n  at process._tickCallback (node.js:442:13)\n\n...\n  19) Memcached memcached://:9001/#prefix,memcached://:9002/#prefix Single domain with multiple backends and fallback plus a second dead:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\n```\n. OK, well this works:\n$ NO_ETCD=true NO_MEMCACHED=true NO_ZOOKEEPER=true npm test\nbut it's curious that I have to disable 3 things to get the tests to pass. Thought it may very well be a configuration problem on my system.\n. Interesting that when I run without NO_ZOOKEEPER=true, it fails but it is definitely able to talk to Zookeeper, because it leaves it running after the test errors out (https://github.com/hipache/hipache/issues/195):\n```\n[marca@marca-mac2 hipache]$ pgrep -fl 'memcache'; pgrep -fl 'redis-server'; pgrep -fl 'hipache'; pgrep -fl 'zookeeper'; pgrep -fl 'etcd'\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 4000\n\nERR! Server#zkServer.sh  JMX enabled by default\nERR! Server#zkServer.sh\nERR! Server#zkServer.sh  Using config: /Users/marca/zookeeper/zoo.cfg\nERR! Server#zkServer.sh\n  1) \"before all\" hook\n  2) \"after all\" hook\n0 passing (14s)\n  2 failing\n1)  \"before all\" hook:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\n2)  \"after all\" hook:\n     Error: timeout of 4000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\nnpm ERR! Test failed.  See above for more details.\n[marca@marca-mac2 hipache]$ pgrep -fl 'memcache'; pgrep -fl 'redis-server'; pgrep -fl 'hipache'; pgrep -fl 'zookeeper'; pgrep -fl 'etcd'\n28552 /usr/bin/java -Dzookeeper.log.dir=/Users/marca/zookeeper/ -Dzookeeper.root.logger=INFO,CONSOLE -cp /usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../build/classes:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../build/lib/.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/slf4j-log4j12-1.6.1.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/slf4j-api-1.6.1.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/netty-3.7.0.Final.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/log4j-1.2.16.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../lib/jline-0.9.94.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../zookeeper-3.4.6.jar:/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin/../src/java/lib/.jar:/usr/local/etc/zookeeper: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false org.apache.zookeeper.server.quorum.QuorumPeerMain /Users/marca/zookeeper/zoo.cfg\n```\n. I also tried raising the timeout from 4000ms to 16000ms and that didn't help.\n```\n[marca@marca-mac2 hipache]$ git diff\ndiff --git a/package.json b/package.json\nindex c05b7dd..7bdadfc 100644\n--- a/package.json\n+++ b/package.json\n@@ -48,7 +48,7 @@\n   },\n   \"scripts\": {\n     \"start\": \"./bin/hipache\",\n-    \"test\": \"istanbul test _mocha --report html -- test//*.js --reporter spec --timeout 4000\",\n+    \"test\": \"istanbul test _mocha --report html -- test//.js --reporter spec --timeout 16000\",\n     \"coveralls\": \"istanbul cover _mocha --report lcovonly -- test//.js -R spec && cat ./coverage/lcov.info | coveralls && rm -rf ./coverage\",\n     \"hint\": \"gulp hint\"\n   },\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 16000\n\nERR! Server#zkServer.sh  JMX enabled by default\nERR! Server#zkServer.sh\nERR! Server#zkServer.sh  Using config: /Users/marca/zookeeper/zoo.cfg\nERR! Server#zkServer.sh\n  1) \"before all\" hook\n  2) \"after all\" hook\n0 passing (37s)\n  2 failing\n1)  \"before all\" hook:\n     Error: timeout of 16000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\n2)  \"after all\" hook:\n     Error: timeout of 16000ms exceeded\n      at null. (/Users/marca/dev/git-repos/hipache/node_modules/mocha/lib/runnable.js:158:19)\n      at Timer.listOnTimeout [as ontimeout] (timers.js:112:15)\nnpm ERR! Test failed.  See above for more details.\n``\n. Working on adding aCONTRIBUTING.mdfile in https://github.com/hipache/hipache/pull/197\n. I think I see the problem - comma instead of space in thezkServer.shcommand. I'll send a PR later.\n. comma/space thing was a red herring.\n. Just updated #197 \n. Ditto forredis-server`:\n```\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n22379 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n[marca@marca-mac2 hipache]$ npm test\n\nhipache@0.4.0 test /Users/marca/dev/git-repos/hipache\nistanbul test _mocha --report html -- test/*/.js --reporter spec --timeout 4000\n\n1) \"before all\" hook\n0 passing (4s)\n  1 failing\n1)  \"before all\" hook:\n     Uncaught Error: spawn ENOENT\n      at errnoException (child_process.js:1011:11)\n      at Process.ChildProcess._handle.onexit (child_process.js:802:34)\nnpm ERR! Test failed.  See above for more details.\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n22379 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n65544 redis-server :7777\n65552 redis-server :7001\n65554 redis-server :7002\n65555 redis-server :7003\n``\n. Ditto foretcd`:\n\u276f pgrep -fl 'memcache'; pgrep -fl 'redis-server'; pgrep -fl 'hipache'; pgrep -fl 'zookeeper'; pgrep -fl 'etcd'\n44293 memcached -p 9001\n44294 memcached -p 9002\n44287 redis-server *:7777\n44296 redis-server *:7001\n44297 redis-server *:7002\n44298 redis-server *:7003\n44288 /usr/local/bin/node --expose-gc --always-compact -- ./bin/hipache -c test/fixtures/configs/hipache-config.json\n44290 /usr/local/Cellar/node/0.10.35_1/bin/node --expose-gc --always-compact -- /Users/marca/dev/git-repos/hipache/bin/hipache -c test/fixtures/configs/hipache-config.json\n44291 /usr/local/Cellar/node/0.10.35_1/bin/node --expose-gc --always-compact -- /Users/marca/dev/git-repos/hipache/bin/hipache -c test/fixtures/configs/hipache-config.json\n34906 vim test/unit/driver-zookeeper.js +9\n44292 etcd -bind-addr=127.0.0.1:8001 -addr=127.0.0.1:8001 -peer-bind-addr=127.0.0.1:8011 -peer-addr=127.0.0.1:8011\n. Is this related to the fact that the process seems to exit right away when it hits the first error? I.e.: I don't see all the failures at once only the first one. Is there a way to get it to keep going on an error so that it can report all the test results and then clean up?\n. Yeah, test failure looks spurious and unrelated to my change. \nCan an admin please tell Travis to retest?\n. OK, I moved the testing info from README.md to CONTRIBUTING.md (and put a link in README.md to CONTRIBUTING.md.\nI'll leave it to someone more experienced to:\n- Document the canonical way to run the tests - I'm getting a lot of failures, so I'm not sure I'm doing stuff right.\n- Document what the standards are for coding, commit messages, etc.\n. Implemented @willdurand's suggestion.\nSee: https://github.com/msabramo/hipache/blob/patch-4/CONTRIBUTING.md\n. See https://github.com/hipache/hipache/pull/199 for a possible solution...\n. [marca@marca-mac2 hipache]$ node --version\nv0.10.35\n[marca@marca-mac2 hipache]$ redis-server --version\nRedis server v=2.8.17 sha=00000000:0 malloc=libc bits=64 build=32eb139b4f2b63\n[marca@marca-mac2 hipache]$ npm list | grep redis\n\u251c\u2500\u2500 redis@0.10.3\nnpm ERR! extraneous: npm@1.4.10 /Users/marca/dev/git-repos/hipache/node_modules/npm\nnpm ERR! extraneous: inherits@2.0.1 /Users/marca/dev/git-repos/hipache/node_modules/npm/node_modules/inherits\nnpm ERR! extraneous: normalize-package-data@1.0.3 /Users/marca/dev/git-repos/hipache/node_modules/npm/node_modules/normalize-package-data\nnpm ERR! not ok code 0\n[marca@marca-mac2 hipache]$ ps aux | grep redis\nmarca           98256   0.0  0.1  2506948  14564 s002  T     4:21PM   0:06.19 vim lib/drivers/redis.js +38\nmarca           24616   0.0  0.0  2440976    528 s002  R+   11:08PM   0:00.00 grep redis\n. Hmmm, those npm ERR! look concerning. I'm going to try blowing away node_modules and doing npm install again.\n. 1 test failed.\n[marca@marca-mac2 hipache]$ npm list | grep redis\n\u251c\u2500\u2500 redis@0.10.3\nAlso upgraded redis-server.\n[marca@marca-mac2 hipache]$ redis-server --version\nRedis server v=2.8.19 sha=00000000:0 malloc=libc bits=64 build=f9a9a1252a078d97\nTests still failing:\n```\n[marca@marca-mac2 hipache]$ gulp test:unit\n[23:11:47] Using gulpfile ~/dev/git-repos/hipache/gulpfile.js\n[23:11:47] Starting 'test:unit'...\nConfig\n    Configuration loading\n      \u2713 From string\n      \u2713 From buffer\n      \u2713 From object\n    Broken...\n      \u2713 ... broken json\n      \u2713 ... https missing key file\n      \u2713 ... https missing cert file\n      \u2713 ... https missing ca file\n      \u2713 ... https missing key\n      \u2713 ... https missing cert\n    Running as root\n      \u2713 Not running root test as we are not root...\nDriver factory\n    #existing-drivers\n      \u2713 redis:\n      \u2713 memcached://\n      \u2713 etcd: (97ms)\n      \u2713 etcds://\n      \u2713 redis:///\u00df\u221e\n      1) redis:///\u00df\u221e\n      \u2713 redis://,redis://\n    #erroring-on-bogus-urls\n      \u2713 \"nopassaran://\" (unregistered) should emit error\n      \u2713 \"bogus\" (no scheme) should emit error\n      \u2713 drivertpl:// (which is broken) should emit error\nLogger\n    #creation\n      \u2713 err with undefined path\n      \u2713 err trying to open a folder\n      \u2713 err trying to open file in nested non existent directory\n      \u2713 err trying to open file with no permissions\n    #trying to log in the wrong state\n      \u2713 stream has been closed\n    #legit logging\n      \u2713 empty data\n      \u2713 legit data\nLru\n    #dumb lru\n      \u2713 Get, set, del\n    #real lru\n      \u2713 Simple get, set, del\n    #real lru limits\n      \u2713 Reaching the number of objects limits\n      \u2713 Reaching the ttl (52ms)\n30 passing (228ms)\n  1 failing\n1) Driver factory #existing-drivers redis:///\u00df\u221e:\n     Uncaught\n  DriverError: Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED\n      at null. (/Users/marca/dev/git-repos/hipache/lib/drivers/redis.js:38:35)\n      at RedisClient.emit (events.js:95:17)\n      at RedisClient.on_error (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:185:10)\n      at Socket. (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:95:14)\n      at Socket.emit (events.js:95:17)\n      at net.js:441:14\n      at process._tickCallback (node.js:442:13)\n[23:11:47] 'test:unit' errored after 291 ms\n[23:11:47] Error in plugin 'gulp-mocha'\n1 test failed.\n```\n. On my Ubuntu 14.04 system where all the tests pass, I have:\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ node --version\nv0.10.26\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ redis-server --version\nRedis server v=2.8.4 sha=00000000:0 malloc=jemalloc-3.4.1 bits=64 build=a44a05d76f06a5d9\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ npm list | grep redis\n\u251c\u2500\u2500 redis@0.10.3\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ ps aux | grep redis\nredis     1176  0.1  0.7  36996  7296 ?        Ssl  06:20   0:03 /usr/bin/redis-server 127.0.0.1:6379\nvagrant   7567  0.0  0.0  10468   916 pts/1    S+   07:14   0:00 grep --color=auto redis\n. Ah ha. I noticed that my Ubuntu 14.04 had a redis-server running and my OS X did not. I did:\n[marca@marca-mac2 hipache]$ launchctl unload ~/Library/LaunchAgents/homebrew.mxcl.redis.plist\n[marca@marca-mac2 hipache]$ launchctl load ~/Library/LaunchAgents/homebrew.mxcl.redis.plist\n[marca@marca-mac2 hipache]$ pgrep -fl redis\n25424 /usr/local/opt/redis/bin/redis-server 127.0.0.1:6379\n98256 vim lib/drivers/redis.js +38\nand now the tests pass.\nThat seems wrong though that the tests would require a redis server to already be running...? They should be starting their own redis server, no?\n. Just to add extra proof, I stopped redis-server on the Ubuntu box and then I get the same failure there:\n```\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ sudo /etc/init.d/redis-server stop\nStopping redis-server: redis-server.\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ gulp test:unit\n[07:17:36] Using gulpfile ~/hipache/gulpfile.js\n[07:17:36] Starting 'test:unit'...\nConfig\n    Configuration loading\n      \u2713 From string\n      \u2713 From buffer\n      \u2713 From object\n    Broken...\n      \u2713 ... broken json\n      \u2713 ... https missing key file\n      \u2713 ... https missing cert file\n      \u2713 ... https missing ca file\n      \u2713 ... https missing key\n      \u2713 ... https missing cert\n    Running as root\n      \u2713 Not running root test as we are not root...\nDriver factory\n    #existing-drivers\n      \u2713 redis:\n      \u2713 memcached://\n      \u2713 etcd: (95ms)\n      \u2713 etcds://\n      \u2713 redis:///\u00df\u221e\n      1) redis:///\u00df\u221e\n      \u2713 redis://,redis://\n    #erroring-on-bogus-urls\n      \u2713 \"nopassaran://\" (unregistered) should emit error\n      \u2713 \"bogus\" (no scheme) should emit error\n      \u2713 drivertpl:// (which is broken) should emit error\nLogger\n    #creation\n      \u2713 err with undefined path\n      \u2713 err trying to open a folder\n      \u2713 err trying to open file in nested non existent directory\n      \u2713 err trying to open file with no permissions\n    #trying to log in the wrong state\n      \u2713 stream has been closed\n    #legit logging\n      \u2713 empty data\n      \u2713 legit data\nLru\n    #dumb lru\n      \u2713 Get, set, del\n    #real lru\n      \u2713 Simple get, set, del\n    #real lru limits\n      \u2713 Reaching the number of objects limits\n      \u2713 Reaching the ttl (53ms)\n30 passing (258ms)\n  1 failing\n1) Driver factory #existing-drivers redis:///\u00df\u221e:\n     Uncaught\n  DriverError: Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED\n      at null. (/home/vagrant/hipache/lib/drivers/redis.js:38:35)\n      at RedisClient.EventEmitter.emit (events.js:95:17)\n      at RedisClient.on_error (/home/vagrant/hipache/node_modules/redis/index.js:185:10)\n      at Socket. (/home/vagrant/hipache/node_modules/redis/index.js:95:14)\n      at Socket.EventEmitter.emit (events.js:95:17)\n      at net.js:440:14\n      at process._tickCallback (node.js:415:13)\n[07:17:37] 'test:unit' errored after 323 ms\n[07:17:37] Error in plugin 'gulp-mocha'\n1 test failed.\n```\n. So now the question is why do the tests rely on having a redis server running and how can that be fixed?\n. Alright, I'll close this. Hopefully someone else can figure out why that test is failing.\n. Tried this:\n``` diff\n\u276f git diff\ndiff --git a/lib/drivers/redis.js b/lib/drivers/redis.js\nindex fe8d155..8565f76 100644\n--- a/lib/drivers/redis.js\n+++ b/lib/drivers/redis.js\n@@ -24,6 +24,11 @@\n             prefix = slave.hash.substr(1);\n         }\n\nclient.on('error', function (err) {\n// Re-emit unspecified error as is\nthis.emit(this.ERROR, new DriverError(DriverError.UNSPECIFIED, err));\n\n}.bind(this));\n+\n         var db;\n         if (slave.path && (db = slave.path.substr(1))) {\n             client.select(db);\n@@ -33,11 +38,6 @@\n             client.auth(password);\n         }\n\n\nclient.on('error', function (err) {\n\n// Re-emit unspecified error as is\nthis.emit(this.ERROR, new DriverError(DriverError.UNSPECIFIED, err));\n\n}.bind(this));\n client.on('ready', function (err) {\n     clientReady = true;\n     if (!clientWrite || clientWriteReady) {\n\n```\n\n\nStill fails:\n```\n[marca@marca-mac2 hipache]$ gulp test:unit\n[22:14:33] Using gulpfile ~/dev/git-repos/hipache/gulpfile.js\n[22:14:33] Starting 'test:unit'...\nConfig\n    Configuration loading\n      \u2713 From string\n      \u2713 From buffer\n      \u2713 From object\n    Broken...\n      \u2713 ... broken json\n      \u2713 ... https missing key file\n      \u2713 ... https missing cert file\n      \u2713 ... https missing ca file\n      \u2713 ... https missing key\n      \u2713 ... https missing cert\n    Running as root\n      \u2713 Not running root test as we are not root...\nDriver factory\n    #existing-drivers\n      \u2713 redis:\n      \u2713 memcached://\n      \u2713 etcd: (101ms)\n      \u2713 etcds://\n      \u2713 redis:///\u00df\u221e\n      1) redis:///\u00df\u221e\n      \u2713 redis://,redis://\n    #erroring-on-bogus-urls\n      \u2713 \"nopassaran://\" (unregistered) should emit error\n      \u2713 \"bogus\" (no scheme) should emit error\n      \u2713 drivertpl:// (which is broken) should emit error\nLogger\n    #creation\n      \u2713 err with undefined path\n      \u2713 err trying to open a folder\n      \u2713 err trying to open file in nested non existent directory\n      \u2713 err trying to open file with no permissions\n    #trying to log in the wrong state\n      \u2713 stream has been closed\n    #legit logging\n      \u2713 empty data\n      \u2713 legit data\nLru\n    #dumb lru\n      \u2713 Get, set, del\n    #real lru\n      \u2713 Simple get, set, del\n    #real lru limits\n      \u2713 Reaching the number of objects limits\n      \u2713 Reaching the ttl (53ms)\n30 passing (232ms)\n  1 failing\n1) Driver factory #existing-drivers redis:///\u00df\u221e:\n     Uncaught\n  DriverError: Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED\n      at null. (/Users/marca/dev/git-repos/hipache/lib/drivers/redis.js:29:35)\n      at RedisClient.emit (events.js:95:17)\n      at RedisClient.on_error (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:185:10)\n      at Socket. (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:95:14)\n      at Socket.emit (events.js:95:17)\n      at net.js:441:14\n      at process._tickCallback (node.js:442:13)\n[22:14:33] 'test:unit' errored after 296 ms\n[22:14:33] Error in plugin 'gulp-mocha'\n1 test failed.\n```\nLet me know if there's anything else you want me to try.\n. By the way, here are my locale settings:\n[marca@marca-mac2 hipache]$ locale\nLANG=\"en_US.UTF-8\"\nLC_COLLATE=\"en_US.UTF-8\"\nLC_CTYPE=\"en_US.UTF-8\"\nLC_MESSAGES=\"en_US.UTF-8\"\nLC_MONETARY=\"en_US.UTF-8\"\nLC_NUMERIC=\"en_US.UTF-8\"\nLC_TIME=\"en_US.UTF-8\"\nLC_ALL=\"en_US.UTF-8\"\n. Here's something kind of odd to me. The tests pass fine in an Ubuntu 14.04 VM and it says there are 30 passing tests.\n```\nvagrant@vagrant-ubuntu-trusty-64:~/hipache$ gulp test:unit\n...\n  30 passing (255ms)\n[06:28:31] Finished 'test:unit' after 318 ms\n```\nOn OS X, it says that one test fails but it also says that 30 tests are passing. How can it have the same number of passing tests + 1 that fails?\n```\n[marca@marca-mac2 hipache]$ gulp test:unit\n[22:31:06] Using gulpfile ~/dev/git-repos/hipache/gulpfile.js\n[22:31:06] Starting 'test:unit'...\n...\n  30 passing (228ms)\n  1 failing\n1) Driver factory #existing-drivers redis:///\u00df\u221e:\n     Uncaught\n  DriverError: Error: Redis connection to 127.0.0.1:6379 failed - connect ECONNREFUSED\n      at null. (/Users/marca/dev/git-repos/hipache/lib/drivers/redis.js:38:35)\n      at RedisClient.emit (events.js:95:17)\n      at RedisClient.on_error (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:185:10)\n      at Socket. (/Users/marca/dev/git-repos/hipache/node_modules/redis/index.js:95:14)\n      at Socket.emit (events.js:95:17)\n      at net.js:441:14\n      at process._tickCallback (node.js:442:13)\n[22:31:06] 'test:unit' errored after 291 ms\n[22:31:06] Error in plugin 'gulp-mocha'\n1 test failed.\n```\n. LGTM\n. @dmp42: How come I hit this redis erroring twice even though you and @willdurand did not?\n. Yeah, this can probably be closed after #203 is merged.\n. I still have problem that the tests pass but process doesn't exit. Probably waiting for a zookeeper process to terminate because one is left running and when I kill it, test process exits. \nYou guys see this?\n. The code calls kill but I wonder if it's killing the shell script zkServer.sh but leaving the Java process running?\n. For me, the tests were always failing to exit and every time that I looked there was a Java process still running.\nI found a workaround on my system (OS X 10.9.4 with Zookeeper installed from Homebrew). I edited /usr/local/bin/zkServer and added an exec to the last line so it looks like this:\n```\n!/usr/bin/env bash\n. \"/usr/local/etc/zookeeper/defaults\"\ncd \"/usr/local/Cellar/zookeeper/3.4.6_1/libexec/bin\"\nexec ./zkServer.sh \"$@\"\n```\nThat seemed to do the trick for me.\n. Also your change in https://github.com/hipache/hipache/pull/210 seems to solve it so probably this can be closed.\n. Tests are working much better on my Mac now. Thanks!\n```\n$ gulp test:drivers\n[21:45:07] Using gulpfile ~/dev/git-repos/hipache/gulpfile.js\n[21:45:07] Starting 'test:drivers'...\n...\n  91 passing (22s)\n[21:45:29] Finished 'test:drivers' after 23 s\n```\n. Agreed on optional. \nAlso think it needs some work so that it maintains a chain of proxies. E.g.:\nUser agent => nginx => hipache => origin server\n. Yeah, what did you have in mind specifically for changing it?\nSurely hipache should preserve data from existing Via headers? I would think it's up to each server to decide how much it's comfortable sharing and here we might want to dial it back. For example, one could make an argument that I should take out the (hipache/0.4.0) part because that might aid someone with bad intentions. OTOH, maybe it doesn't matter so much if the entire feature is optional and turned off by default, like you suggested (and documented the ramifications). I'd probably only use this for dev and testing; not prod. Heck in our prod, there's a layer of F5 load balancers in front of the nginx and our Ops people might've set them up to strip out sensitive headers like these.\nAnyway, definitely going to make it optional like you said. And I'd like to make it preserve other Via headers unless you think that's problematic (another configuration option perhaps?).\nThanks to you and @dmp42 for all your help with hipache -- I'm not experienced with NodeJS (Python is my preferred language), so it's nice to have some folks looking over my shoulder and giving some bits of advice.\n. I found a nicer way to do this using middleware. I plan to update it shortly.\n. OK, just updated this. Now it:\n1. Uses connect middleware, which I think is much cleaner.\n2. Has a configuration setting and it's off by default:\njavascript\n    if (config.server.setViaResponseHeader) {\n        app.use(setViaResponseHeader);\n    }\nI add:\n\"setViaResponseHeader\": true,\nto my config/config.json and run bin/hipache and hit it:\n$ http HEAD http://my-cool-app.127.0.0.1.xip.io:8080/apps\nHTTP/1.1 400 Bad Request\nConnection: keep-alive\nDate: Thu, 08 Jan 2015 00:32:01 GMT\nVia: 1.1 my-cool-app.127.0.0.1.xip.io:8080 (hipache/0.4.0)\ncache-control: no-cache\ncontent-type: text/html\nexpires: -1\npragma: no-cache\n. Thanks, will do.\nOther idea I'm toying with is generalizing the middleware and configuration. There are lots of interesting middleware for logging, authentication, error reporting, compression, etc. (see https://github.com/senchalabs/connect#middleware for a bunch of existing middleware). It might be interesting for folks to use them all with hipache but it will be tedious to add support (including a configuration param) for each one. So I was wondering about using something like meddleware that lets you plug middleware in automatically using a config file. Not sure if meddleware can work without express, but I'm pretty sure that the basic concept could be made to work with just connect alone.\nThoughts?\n. Ah so when you mention node-http-proxy middleware I went googling for it. I found one article about it but it seemed to be outdated as it was talking about APIs that are different now. Not sure where or how but I got the impression from googling that the node-http-proxy middleware was removed as they instead got behind \"connect\" which is a general middleware that works with any kind of http server. So I used that.\nIf you think there's something better and you can send me a link to the docs on it, I'll try to use it. \n. See https://github.com/hipache/hipache/pull/213\n. This will need documentation of course but wanted to see first if the idea was sound and desirable.\n. Ping\n. @dmp42: What do you think?\n. @dmp42: Yeah, no rush on my end. I just like to remind sometimes on PRs so that they don't get forgotten about and then merge conflicts happen and what not. If you're busy, it can wait.\n. ",
    "Starefossen": "LGTM\n. I'm looking over the node-redis client changelog, can't see many breaking changes. Did you hit any errors while upgrading or is this still on the todo list?\n. I have started to look into the code of Hipache to see if I could merge in some of the pull requests to this repo. I have set up a Docker Compose development environment for Hipache development, but I very quickly ran into major problems with the test suite which requires the different databases to be installed directly on the host to be configurable through the test suite.\n. I am referring to the way the Hipache test suite sets up it's database dependencies using the database fixtures: https://github.com/hipache/hipache/blob/master/test/fixtures/servers/generic.js#L31\n. LGTM\n. Looks like the port (80 and/or 443) is already in use\n. LGTM\n. I'm looking over the node-redis client changelog, can't see many breaking changes. Did you hit any errors while upgrading or is this still on the todo list?\n. I have started to look into the code of Hipache to see if I could merge in some of the pull requests to this repo. I have set up a Docker Compose development environment for Hipache development, but I very quickly ran into major problems with the test suite which requires the different databases to be installed directly on the host to be configurable through the test suite.\n. I am referring to the way the Hipache test suite sets up it's database dependencies using the database fixtures: https://github.com/hipache/hipache/blob/master/test/fixtures/servers/generic.js#L31\n. LGTM\n. Looks like the port (80 and/or 443) is already in use\n. ",
    "mlehner616": "Just a quick ping on this. Any particular reason there hasn't been a \"release\" in almost a year (~100 commits)? I'm interested in trying it out but it seems like the project is getting a lot of contributions but they don't seem to be getting merged into a release. I'm wondering if it's really a good idea for me to be working off of the master branch.\n. Just a quick ping on this. Any particular reason there hasn't been a \"release\" in almost a year (~100 commits)? I'm interested in trying it out but it seems like the project is getting a lot of contributions but they don't seem to be getting merged into a release. I'm wondering if it's really a good idea for me to be working off of the master branch.\n. ",
    "gillesdemey": "I have the same \"Issue\". I'm currently thinking of using hipache with etcd, but was wondering why it hasn't seen a release in quite a while.\nOther alternatives like vulcand don't support websocket proxying and using nginx or haproxy with confd seems like too much of a hassle. \nNot to mention that our entire stack runs on Node, so being able to understand the code and debug potential issues is a huge plus.\n. I have the same \"Issue\". I'm currently thinking of using hipache with etcd, but was wondering why it hasn't seen a release in quite a while.\nOther alternatives like vulcand don't support websocket proxying and using nginx or haproxy with confd seems like too much of a hassle. \nNot to mention that our entire stack runs on Node, so being able to understand the code and debug potential issues is a huge plus.\n. ",
    "FLYBYME": "The last publish to npm was over 2 years ago. I will consider this project died..\n. Wait a second has this become deprecated because the maintainers cant be bothered or is there a technical reason for it? The project has 16 pull requests and over 2000 stars why just give up on the project I don't understand.\n@dmp42\n@kencochrane. The last publish to npm was over 2 years ago. I will consider this project died..\n. Wait a second has this become deprecated because the maintainers cant be bothered or is there a technical reason for it? The project has 16 pull requests and over 2000 stars why just give up on the project I don't understand.\n@dmp42\n@kencochrane. ",
    "aduermael": "Apparently with Node 0.12.0 we can access raw header. \nSo I guess node-http-proxy could be patched with that.\nBut you're right, it seems that no magic can be done on Hipache side.\n. Ok, thanks for the advices. \nI think we'll hack hipache/http-proxy temporarily. \nWe'll remove it as soon as the client can be updated.\n. Apparently with Node 0.12.0 we can access raw header. \nSo I guess node-http-proxy could be patched with that.\nBut you're right, it seems that no magic can be done on Hipache side.\n. Ok, thanks for the advices. \nI think we'll hack hipache/http-proxy temporarily. \nWe'll remove it as soon as the client can be updated.\n. ",
    "DimShadoWWW": "I cloned the github repository, edited the config file and created the docker image from the included Dockerfile\n. I cloned the github repository, edited the config file and created the docker image from the included Dockerfile\n. ",
    "kgorskowski": "+1 here - any suggestions on how to fix this? am I missing something? \nupdate - fixed it myself. had to add \nrun ln -s /usr/bin/nodejs /usr/bin/node\nrun npm install -g node-etcd\nto dockerfile, otherwise it wouldn't build correct unless it didn't throw an error.\n. +1 here - any suggestions on how to fix this? am I missing something? \nupdate - fixed it myself. had to add \nrun ln -s /usr/bin/nodejs /usr/bin/node\nrun npm install -g node-etcd\nto dockerfile, otherwise it wouldn't build correct unless it didn't throw an error.\n. ",
    "llonchj": "hipache@0.3.1 and node-etcd@3.0.2 works for me\n. hipache@0.3.1 and node-etcd@3.0.2 works for me\n. ",
    "nathwill": "i'm seeing this issue as well, not just when testing, but just a basic installation blows up with these errors as well.\n. so, apparently if you update the dep for http-proxy to lock eventemitter3 to 0.1.6, it works... \n. aha: https://github.com/primus/eventemitter3/issues/29\n. @arnaudsj  sure thing!\ni'm just an operator trying to get hipache working in one of our systems, so i'm not sure about the right way to fix it in a more permanent fashion, but as it stands, both HEAD and the 0.3.x branch are completely broken on installation without manual intervention afterwards :cry: \nit'd be great if we could get an 0.3.2 release that somehow locks down the http-proxy deps as well. anybody know if that's possible?\n. ok, looks like \"shrinkwrap\" is a thing (cool!), and it seems less intrusive than the work that would be needed to update to the latest http-proxy that actually has some kind of pinning, so i set up a branch with the intention of submitting a PR for the stable release (0.3.1 -> 0.3.2), but there's no branch on this repo for the stable release, so nothing to PR against.... /cc @dmp42 how would you like to handle this?\n. @willdurand okey doke. fwiw, a new 0.3.x release'd be nice as well, since a lot has changed on master since 0.3.1 that requires a bit more involved effort and testing for us before we're willing or able to drop it into production.\n. @dmp42 any update on this? we're working on determining if this project is still maintained; from what i can gather it sounds like dotcloud may have switched to an nginx-lua-based implementation internally? is there still interest in maintaining hipache, or should we start looking to switch to another solution?\nwhatever the situation is is fine, we're just trying to determine our best option going forward.\n. it's due to eventemitter3, see https://github.com/hipache/hipache/issues/223\n. i'm seeing this issue as well, not just when testing, but just a basic installation blows up with these errors as well.\n. so, apparently if you update the dep for http-proxy to lock eventemitter3 to 0.1.6, it works... \n. aha: https://github.com/primus/eventemitter3/issues/29\n. @arnaudsj  sure thing!\ni'm just an operator trying to get hipache working in one of our systems, so i'm not sure about the right way to fix it in a more permanent fashion, but as it stands, both HEAD and the 0.3.x branch are completely broken on installation without manual intervention afterwards :cry: \nit'd be great if we could get an 0.3.2 release that somehow locks down the http-proxy deps as well. anybody know if that's possible?\n. ok, looks like \"shrinkwrap\" is a thing (cool!), and it seems less intrusive than the work that would be needed to update to the latest http-proxy that actually has some kind of pinning, so i set up a branch with the intention of submitting a PR for the stable release (0.3.1 -> 0.3.2), but there's no branch on this repo for the stable release, so nothing to PR against.... /cc @dmp42 how would you like to handle this?\n. @willdurand okey doke. fwiw, a new 0.3.x release'd be nice as well, since a lot has changed on master since 0.3.1 that requires a bit more involved effort and testing for us before we're willing or able to drop it into production.\n. @dmp42 any update on this? we're working on determining if this project is still maintained; from what i can gather it sounds like dotcloud may have switched to an nginx-lua-based implementation internally? is there still interest in maintaining hipache, or should we start looking to switch to another solution?\nwhatever the situation is is fine, we're just trying to determine our best option going forward.\n. it's due to eventemitter3, see https://github.com/hipache/hipache/issues/223\n. ",
    "arnaudsj": "@nathwill thank you. I had pinpointed it to eventemmitter, but did not go any further. Great catch! Thank you for your help!\n. @nathwill thank you. I had pinpointed it to eventemmitter, but did not go any further. Great catch! Thank you for your help!\n. ",
    "sdwr98": "@nathwill @willdurand Is there any way to fix an NPM install right now-ish?  I've got a prod system that was being rebuilt and is now broken because of this.\n. to answer my own question, after an NPM install of hipache, I went into /usr/lib/node_modules/hipache/node_modules/http-proxy/package.json, fixed the version of eventemitter3 to 0.1.6, and ran npm install in that directory.  That's probably not the node-ish way of doing it, but it worked. \n. See this issue: https://github.com/hipache/hipache/issues/223.\n. @nathwill @willdurand Is there any way to fix an NPM install right now-ish?  I've got a prod system that was being rebuilt and is now broken because of this.\n. to answer my own question, after an NPM install of hipache, I went into /usr/lib/node_modules/hipache/node_modules/http-proxy/package.json, fixed the version of eventemitter3 to 0.1.6, and ran npm install in that directory.  That's probably not the node-ish way of doing it, but it worked. \n. See this issue: https://github.com/hipache/hipache/issues/223.\n. ",
    "fpereiro": "For anyone looking for a workaround to this issue if they are running Hipache within a Docker container, the following Dockerfile command worked for me:\nRUN npm install eventemitter3@0.1.6 hipache@0.3.1 -g --production\n. For anyone looking for a workaround to this issue if they are running Hipache within a Docker container, the following Dockerfile command worked for me:\nRUN npm install eventemitter3@0.1.6 hipache@0.3.1 -g --production\n. ",
    "yoanisgil": "Seem issue here. Made it work with hack from @fpereiro however I have the same concern as @nathwill. Is the project actively supported?\n. Seem issue here. Made it work with hack from @fpereiro however I have the same concern as @nathwill. Is the project actively supported?\n. ",
    "varunpalekar": "I too not able to run Hipache from it's docker image hipache:0.3.1(latest). I get the same error TypeError: Cannot read property 'prototype' of undefined.\nPlease have a look at docker image  hipache:latest. \n. I too not able to run Hipache from it's docker image hipache:0.3.1(latest). I get the same error TypeError: Cannot read property 'prototype' of undefined.\nPlease have a look at docker image  hipache:latest. \n. ",
    "pirog": "Can confirm this and that downgrading http-proxy resolves. Also https://github.com/hipache/hipache/pull/224 seems like a good idea.\n. Can confirm this and that downgrading http-proxy resolves. Also https://github.com/hipache/hipache/pull/224 seems like a good idea.\n. ",
    "robsonpeixoto": "I'm having the same problem. How can I solve it?\n. I'm having the same problem. How can I solve it?\n. ",
    "dmitryck": "@trsouz what did you mean \"upgrading http-proxy to the stable version 1.11.1\"\nhttp-proxy is the a some package?\n. @trsouz what did you mean \"upgrading http-proxy to the stable version 1.11.1\"\nhttp-proxy is the a some package?\n. ",
    "mamciek": "after updating http-proxy i have problem with headers x-forwarded-proto. My Nginx that request is forwarded to doesn't see them somehow\n. It only happens with nodejs 0.12. It might be related: https://github.com/nodejitsu/node-http-proxy/issues/772\n. after updating http-proxy i have problem with headers x-forwarded-proto. My Nginx that request is forwarded to doesn't see them somehow\n. It only happens with nodejs 0.12. It might be related: https://github.com/nodejitsu/node-http-proxy/issues/772\n. ",
    "koliyo": "+1\n. +1\n. ",
    "kgrvamsi": "@sdwr98  this error happens even if i run the official docker image from the docker hub.Can we have a fixed version of the Docker image in registry?\n. @sdwr98  this error happens even if i run the official docker image from the docker hub.Can we have a fixed version of the Docker image in registry?\n. ",
    "siccovansas": "I'm using Hipache's VHOST to link to several backend webservers which also host different domains. Am I correct in that it is currently impossible to configure SSL certificates for each of these different domains using Hipache? If not, how would I do this?\n. I'm using Hipache's VHOST to link to several backend webservers which also host different domains. Am I correct in that it is currently impossible to configure SSL certificates for each of these different domains using Hipache? If not, how would I do this?\n. ",
    "SantoDE": "I'm basically asking this myself :D Does anyone have a solution? I could only think of having a wildcard certificate loaded into hipache will will take care of everything\n. I'm basically asking this myself :D Does anyone have a solution? I could only think of having a wildcard certificate loaded into hipache will will take care of everything\n. ",
    "shanmugakarna": "Me too facing the same issue.. Any solutions yet\n. Me too facing the same issue.. Any solutions yet\n. ",
    "travisby": "(user, not maintainer) - I've seen this with different versions of node-http-proxy, while running on node 0.10 or 0.12.  We upgraded to:\n-    \"http-proxy\": \"^1.11.0\",\nand it alleviated our problems.\n(Note, we did this about a year ago, so not sure if the landscape is still the same.  Good luck!)\n. (user, not maintainer) - I've seen this with different versions of node-http-proxy, while running on node 0.10 or 0.12.  We upgraded to:\n-    \"http-proxy\": \"^1.11.0\",\nand it alleviated our problems.\n(Note, we did this about a year ago, so not sure if the landscape is still the same.  Good luck!)\n. ",
    "skarungan": "The issue gets solved by using 1.11.1 version of http-proxy as @travisby has said.\n. The issue gets solved by using 1.11.1 version of http-proxy as @travisby has said.\n. ",
    "relvacode": "Facing same issue as well with npm installed hipache on Fedora 23. Manually installing the latest http-proxy (1.12.0) resolved this issue.\nnpm: 3.3.12\nnode: v5.3.0\nsystem: 4.2.6-301.fc23.x86_64\nCould the package.json please get updated to fix this?\n. Facing same issue as well with npm installed hipache on Fedora 23. Manually installing the latest http-proxy (1.12.0) resolved this issue.\nnpm: 3.3.12\nnode: v5.3.0\nsystem: 4.2.6-301.fc23.x86_64\nCould the package.json please get updated to fix this?\n. ",
    "tcyrus": "Are there any issues with this?. Are there any issues with this?. "
}
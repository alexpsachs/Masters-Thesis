{
    "karpathy": "Hi Jacob, \nIt's a little tricky right now but I hope to bridge this gap.\nYou'd have to use Caffe to extract the top-layer representation from a CNN that looks at the image. If you have Matlab that would be ideal because I provide some skeleton code for it inside matlab_reference folder.\nIf all you have is Python then right now you'd kind of have to be an expert on CNNs and Caffe. You'd have to use the Python Caffe wrapper to extract CNN features using the VGG 16-layer model and then use these as input to this code. I'll have some code up soon. Sorry about that.\n. Hi Evan, \nthe VGG model is significantly better, not just in classification but also in the features it produces (which directly support that classification). The features are probably similar near the first few layers but the final features before the classifier (which are used here) are much stronger with VGG.\n. Hi thank you for reporting this. The exp should be guarded, you're right, but despite the warning I don't believe this should be too big of an issue. I will fix it soon, I'm overwhelmed with other work right now. Thank you!\n. Hi, yes you're missing a requirement of \"modern numpy version\". Upgrade your numpy to more recent version and it will work fine.\nThanks for bringing it up, I should edit the Readme file to mention this.\n. With default parameters? I Thoguht I tuned them so that this doesn't happen, sorry about that. As the message suggests, lowering the learning rate does it. Set learning_rate to be about half or fifth of what it is now, until it doesn't explode :)\n. @StevenLOL  Nice! Looking at the Model Zoo, \nhttp://cs.stanford.edu/people/karpathy/neuraltalk/\nmy LSTM model achieves perplexity of about 15.7 (which is slightly better). I ran it for longer and cross-validated it on our cluster, though.\n. Hey Eliott, \nThe script, as currently presented, is exactly what's being used by Vinyals et al. and Kiros et al., with no penalty. In practice, I'm told this barely changes the results because most of the generated sentences are not too short, so in practice the computed BP ~= 1.\nI'll keep it fixed for consistency. COCO is setting up evaluation server soon, so all of this will  be properly standardized.\n. Hi Subhasis, thanks for the pull! Before we merge, could we please double check that the Matlab and Python features are identical, after image resizing? I.e. Take the Matlab image (after resizing), and run exactly that volume through ConvNet on both sides. \nThe problem could be that the channels / xy flips etc are somehow not correctly configured, and could be giving wrong results at the end.\nEDIT: we want to do after resizing because somehow the Matlab/Python 'BICUBIC' doesn't give identical results. It's possible this is influencing final features.\nEDIT2: Saw your Piazza post, which sounds like exactly what I suggested here. Thanks!\n. Wait, should this also contain the def file? It's alluded to in readme but doesn't appear in the commit\n. I was alluding to this line:\n\"Note that I provide my _features deploy network def as well, which is exactly what you see on that page but I chopped off the softmax to get the 4096-D codes below.\"\nin your readme. It makes it sound like you are attaching the deploy net to the commit?\n. RE: what's inside matlab directory you have to be careful! Caffe has changed many proto fields, it might give errors if you try to use this. Or it gets automatically converted? I haven't tried any of this yet.\n. @npow the deploy prototxt that's used during feature extraction has the top layer (probability layer) cut off. Maybe in more recent caffe versions there is no need to use separate deploy files and you can specify the layer directly. I haven't been keeping up with this too closely.\n. thanks!\n. Hi thanks a lot for writing this up. I came to the same conclusion when I tried to move the code from Matlab to Python and never resolved it :(\nAs for the pull request I'd be eager to merge, but could we please make your version an independent script without changing predict on images file? In other words, there could be a predict_on_images_caffe.py, or something like that. \nI'm worried that the Caffe dependency is a little too heavy. Or maybe there's some other more elegant way to isolate the Caffe part to prevent too much code duplication? E.g. this could be just a file that compartmentalizes all the Caffe functionality, and the predict on images script could have an option to use Caffe instead of .mat files? \nHmm\n. Yep, that sounds good. However, when using Python features the results will be slightly different because of the image resizing, correct?\nAlso, caffe should not be a dependency of NeuralTalk, so it would be best if the import python_caffe_helper was wrapped inside the if statement, so the import only happens on demand if the .mat file doesn't exist.\n. If that worked that would be a super-valuable contribution not just to this but also Caffe. Thanks!\n. Yep! I took them out and didn't see a huge decrease in performance, so I decided to get rid of it for NeuralTalk for simplicity.\n. This is awesome, thanks a lot for putting this together! You may want to see if the people Caffe might be interested in this snippet as well.\nI didn't look in depth, but is it non-trivial to use 4 decimal place precision to get the python and matlab to match exactly?\n. Hi, there are two models in my paper: an alignment/retrieval model and a generation model. I released the generation model. The retrieval model does ranking (image -> sentence or vice versa), and includes computing the alignments, etc. This part was not released.\n. Hi, there isn't any explicit code for this. Look at the json's manually and inspect the DataProvider class. I think it's relatively simple so I didn't add any explicit utilities.\n. Gah, yeah you're right they moved the phase from caffe global into Net class. In your case since you already added the phase as 3rd arg in constructor of Net (the new way), should you just delete that line that's causing trouble? You've already set the test phase with your fix.\n. Yeah I didn't fold this code into the code release that is NeuralTalk. I only took a small chunk of my paper, the one that predicts sequence of words for a block of pixels (whole image here).\n. doh, sorry about this and thank you for issuing the fix.\n. fixed, thanks!\n. Not with this code, it isn't\n. it doesn't use gpu at all\n. thanks!\n. kk,thanks!\n. it's init_from, not resume_from. it loads in the weights but it doesnt actually resume the optimization exactly at the epoch it did before. instead it starts the counter at scratch. i decided that it would be slightly tricky and didn't have time to do it.\n. Hey Sam, I think I use rand, which should be correct. rand give you values between [0,1]. Using randn would be a serious bug. Which lines are you referring to?\n. phew, that could have a bad and hard to notice bug! thanks!\n. Thanks, looks good!\n. Hi, I will be discontinuing NeuralTalk in a few weeks because it is slow and I think just confuses people at this point. I will be releasing a significantly faster version soon, as a complete rewrite. Also I'll stop accepting PRs here I think, and I think travis would be overkill for a \"dead\" codebase. Thanks! Andrej\n. no, moving to Torch.\n. thank you for the PR!\n. I trust you've tested it and that it works :) thank you for the PR!\n. I did not release the ranking part, only the generation part. The NIPS code gives the gist though.\n. oops, i think you're probably right. If you try to meddle with the sizes it would error.\n. ",
    "YafahEdelman": "Thanks, I look foward to seeing the code!\n. ",
    "EvanWeiner": "Hi Andrej,\nSorry to make a forked question here - but why did you choose to use the VGG16 model for feature extraction rather than the BLVC_reference_model/AlexNet model which is shipped in Caffe core? I know the VGG16 has higher accuracy in classification, but for features / neural codes, aren't they similar at earlier layers? \n\nOn Dec 3, 2014, at 2:02 PM, Andrej notifications@github.com wrote:\nHi Jacob,\nIt's a little tricky right now but I hope to bridge this gap.\nYou'd have to use Caffe to extract the top-layer representation from a CNN that looks at the image. If you have Matlab that would be ideal because I provide some skeleton code for it inside matlab_reference folder.\nIf all you have is Python then right now you'd kind of have to be an expert on CNNs and Caffe. You'd have to use the Python Caffe wrapper to extract CNN features using the VGG 16-layer model and then use these as input to this code. I'll have some code up soon. Sorry about that.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "HanuManuOm": "@vimalthilak , Thank you sir. It really removed the warning. But, as @karpathy sir said, warning does't make any trouble. Anyways, good job sir.\n. ",
    "monajalal": "@karpathy I changed my sigmoid to expit from scipy.special to overcome the overflow but now I am getting overflow somewhere else.\n/Users/mona/machine_learning/pset4/mlp.py:304: RuntimeWarning: overflow encountered in exp\n  return expit(-x)\nWhat should I use?\nWould it make sense to replace sigmoid with tanh?\ndef logistic_function(x):\n    return .5 * (1 + np.tanh(.5 * x)). ",
    "webeng": "I had the same issue and I thought I was using the most recent version of numpy (1.9.1) that I installed via pip.\nHowever, the system was using an old version (1.6.1) not installed via pip. I found the version that the script was using by printing \"print numpy.version in the script.\nI used 'python -c 'import os,numpy;print(numpy.file)' to find the version that the system was using. I deleted that version and now the system uses the version that I installed via pip.\nI found the solution here: http://stackoverflow.com/questions/16391335/using-old-version-of-numpy-even-with-a-newer-version-installed\nI hope it helps somebody else.\n. ",
    "StevenLOL": "Here is my result on the default setting:\npython driver.py \nparsed parameters:\n{\n  \"grad_clip\": 5, \n  \"rnn_relu_encoders\": 0, \n  \"dataset\": \"flickr8k\", \n  \"image_encoding_size\": 256, \n  \"eval_max_images\": -1, \n  \"drop_prob_decoder\": 0.5, \n  \"word_encoding_size\": 256, \n  \"max_epochs\": 50, \n  \"eval_batch_size\": 100, \n  \"fappend\": \"baseline\", \n  \"generator\": \"lstm\", \n  \"write_checkpoint_ppl_threshold\": -1, \n  \"decay_rate\": 0.999, \n  \"tanhC_version\": 0, \n  \"hidden_size\": 256, \n  \"momentum\": 0.0, \n  \"worker_status_output_directory\": \"status/\", \n  \"learning_rate\": 0.001, \n  \"checkpoint_output_directory\": \"cv/\", \n  \"do_grad_check\": 0, \n  \"word_count_threshold\": 5, \n  \"batch_size\": 100, \n  \"regc\": 1e-08, \n  \"smooth_eps\": 1e-08, \n  \"solver\": \"rmsprop\", \n  \"eval_period\": 1.0, \n  \"drop_prob_encoder\": 0.5\n}\nInitializing data provider for dataset flickr8k...\nBasicDataProvider: reading data/flickr8k/dataset.json\nBasicDataProvider: reading data/flickr8k/vgg_feats.mat\npreprocessing word counts and creating vocab based on word count threshold 5\n253/15000 batch done in 3.242s. at epoch 0.84. loss cost = 39.264201, reg cost = 0.000001, ppl2 = 29.60 (smooth 47.89)\n254/15000 batch done in 3.133s. at epoch 0.85. loss cost = 39.633654, reg cost = 0.000001, ppl2 = 33.57 (smooth 47.74)\n255/15000 batch done in 3.169s. at epoch 0.85. loss cost = 38.571550, reg cost = 0.000001, ppl2 = 29.56 (smooth 47.56)\n.\n.\n.\n.\n.\none day later...\n.\n14999/15000 batch done in 3.492s. at epoch 50.00. loss cost = 28.621228, reg cost = 0.000004, ppl2 = 11.19 (smooth 10.80)\nevaluating val performance in batches of 100\nevaluated 5000 sentences and got perplexity = 17.785250\nvalidation perplexity = 17.785250\n. ",
    "pannous": "Thanks I will try again with reduced learning rate\n\nOn Jan 10, 2015, at 10:59 AM, Steven notifications@github.com wrote:\nHere is my result on the default setting:\npython driver.py \nparsed parameters:\n{\n\"grad_clip\": 5, \n\"rnn_relu_encoders\": 0, \n\"dataset\": \"flickr8k\", \n\"image_encoding_size\": 256, \n\"eval_max_images\": -1, \n\"drop_prob_decoder\": 0.5, \n\"word_encoding_size\": 256, \n\"max_epochs\": 50, \n\"eval_batch_size\": 100, \n\"fappend\": \"baseline\", \n\"generator\": \"lstm\", \n\"write_checkpoint_ppl_threshold\": -1, \n\"decay_rate\": 0.999, \n\"tanhC_version\": 0, \n\"hidden_size\": 256, \n\"momentum\": 0.0, \n\"worker_status_output_directory\": \"status/\", \n\"learning_rate\": 0.001, \n\"checkpoint_output_directory\": \"cv/\", \n\"do_grad_check\": 0, \n\"word_count_threshold\": 5, \n\"batch_size\": 100, \n\"regc\": 1e-08, \n\"smooth_eps\": 1e-08, \n\"solver\": \"rmsprop\", \n\"eval_period\": 1.0, \n\"drop_prob_encoder\": 0.5\n}\nInitializing data provider for dataset flickr8k...\nBasicDataProvider: reading data/flickr8k/dataset.json\nBasicDataProvider: reading data/flickr8k/vgg_feats.mat\npreprocessing word counts and creating vocab based on word count threshold 5\n253/15000 batch done in 3.242s. at epoch 0.84. loss cost = 39.264201, reg cost = 0.000001, ppl2 = 29.60 (smooth 47.89)\n254/15000 batch done in 3.133s. at epoch 0.85. loss cost = 39.633654, reg cost = 0.000001, ppl2 = 33.57 (smooth 47.74)\n255/15000 batch done in 3.169s. at epoch 0.85. loss cost = 38.571550, reg cost = 0.000001, ppl2 = 29.56 (smooth 47.56)\n.\n.\n.\n.\n.\none day later...\n.\n14999/15000 batch done in 3.492s. at epoch 50.00. loss cost = 28.621228, reg cost = 0.000004, ppl2 = 11.19 (smooth 10.80)\nevaluating val performance in batches of 100\nevaluated 5000 sentences and got perplexity = 17.785250\nvalidation perplexity = 17.785250\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "subhasis256": "The def file should be the prototxt file that you download from the VGG model website or some other network. Maybe I should call it the caffe prototxt file?\nAhh sorry... now I see that you already have the prototxt files in the matlab directory. I will add them.\n. Got it. I actually used this version of the file myself with caffe which\nwas cloned on Jan 17 and it works.\nOn Tue, Feb 24, 2015 at 5:07 PM, Andrej notifications@github.com wrote:\n\nRE: what's inside matlab directory you have to be careful! Caffe has\nchanged many proto fields, it might give errors if you try to use this. Or\nit gets automatically converted? I haven't tried any of this yet.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/karpathy/neuraltalk/pull/7#issuecomment-75885814.\n\n\nSubhasis\n. Added new pull request with the prototxt file.\nOn Tue, Feb 24, 2015 at 5:09 PM, Subhasis Das subhasis256@gmail.com wrote:\n\nGot it. I actually used this version of the file myself with caffe which\nwas cloned on Jan 17 and it works.\nOn Tue, Feb 24, 2015 at 5:07 PM, Andrej notifications@github.com wrote:\n\nRE: what's inside matlab directory you have to be careful! Caffe has\nchanged many proto fields, it might give errors if you try to use this. Or\nit gets automatically converted? I haven't tried any of this yet.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/karpathy/neuraltalk/pull/7#issuecomment-75885814.\n\n\nSubhasis\n\n\nSubhasis\n. thank you! :)\n. ",
    "npow": "Hi, I'm wondering why the features are extracted from net.outputs[0], which is the prob layer? Shouldn't the features be from the fc7 layer?\n. ",
    "ahmedosman": "Sure, I'll create a new script that could be independantly ran to generate image features and dump a .mat file on disk,  and the  predict on images script could optionally call the caffe method if the .mat file is not on disk. Sounds Good? \nthx,\nAhmed\n. I'll write our own image resizing in python similar to what matlab resize do, so results should be identical in the end and submit a new pull request that reflects the changes above. \n. Turns out Python Numpy has a np.round function for rounding to the nearest nth decimal place. I'll do minor tweeks in the code to match matlab precision, that will bring down the residual error to 0. I'll do that and send another pull request. \n. ",
    "sxjzwq": "Me too. It seems that there is no code for that part.\n@karpathy , do you have any plan to publish the code of that part (text segment alignment using MRF).\nThanks.\nQi\n. ",
    "Beanocean": "Thank you very much @karpathy . I have fixed it as you advised.\n. > Hi I'm a total newbie in Caffe in CNNs. I have the same problem when I wanted to run the code for visualization. I didn't get the solution from your response. Could you please explain it a little more?\n\nI have n't done what Beanocean did first I mean:\n\"So I change the code in extract_features.py#101 as: net = caffe.Net(args.model_def, args.model, caffe.TEST). It worked, but a new problem came out:\"\n\nI think we have different problem though you got the same error. my problem is about feature extraction, but yours is about visualization.\nHere you can see the different version of python_features/extract_features.py. My problem was caused by the update of CAFFE's API, and it has been fixed up in new version.\n. > Hi , I want to check the trained model on my own set of images, and while running extract_features.m \n\nand after downloading caffe , \"caffe('set_device', 1); \" is not running.\nI couldn't find caffe.m\n\nI think you need to compile matcaffe firstly(execute make matcaffe in your caffe's directory), and add caffe path to the system path of your MATLAB by using addpath(\"Your caffe path\")\n. ",
    "ghost": "Hi I'm a total newbie in Caffe in CNNs. I have the same problem when I wanted to run the code for visualization. I didn't get the solution from your response. Could you please explain it a little more?\nI have n't done what Beanocean did first I mean:\n\"So I change the code in extract_features.py#101 as: net = caffe.Net(args.model_def, args.model, caffe.TEST). It worked, but a new problem came out:\"\n. ",
    "ShraddhaBhattad": "Hi , I want to check the trained model on my own set of images, and while running extract_features.m \nand after downloading caffe , \"caffe('set_device', 1); \" is not running.\nI couldn't find caffe.m\n. ",
    "Guyunee": "I also met this problems:\nmy code follows here:\nimport numpy as np\nimport matplotlib.pyplot as plt\nMake sure that caffe is on the python path:\ncaffe_root = '/home/guyunee/Desktop/caffe-master/'  # this file is expected to be in {caffe_root}/examples\uff0c\u5efa\u8bae\u4f7f\u7528\u7edd\u5bf9\u8def\u5f84\nimport sys\nsys.path.insert(0, caffe_root + 'python')\nimport caffe\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\ncaffe.set_phase_test()\ncaffe.set_mode_cpu()\nnet = caffe.Classifier(caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt',\n                       caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel')\ninput preprocessing: 'data' is the name of the input blob == net.inputs[0]\nnet.set_mean('data', np.load(caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy'))  # ImageNet mean\nnet.set_raw_scale('data', 255) \nnet.set_channel_swap('data', (2,1,0)) \nit says:\nAttributeError                            Traceback (most recent call last)\n in ()\n----> 1 caffe.Net.set_phase_test()\nAttributeError: type object 'Net' has no attribute 'set_phase_test'\nAttributeError: 'Classifier' object has no attribute 'set_mean'\nhow to solve the problems? because it is too old? @karpathy \n. ",
    "mlguy": "@karpathy Hi Andrej, Thank you very much for sharing the code. Do you plan to share the code for the object detection part sometime in the future?\nThanks\n. ",
    "linhanxiao": "@jazzsaxmafia i have  same question with you .Have you known how to detect object in images in order to generate a set of h-dimentional representation for every image. @mlguy  i have same question with you .Have you known how to detect object in images in order to generate a set of h-dimentional representation for every image. @karpathy Hi Andrej, Thank you very much for sharing the code. Can you  share the code for the object detection so that I can generate a set of h-dimentional representation for every image\nThanks. ",
    "phiresky": "as far as I can see this typo appears far more often\n. Yf\nAm 28.10.2015 15:11 schrieb \"RaniemAR\" notifications@github.com:\n\nWhen I am calling the code I am getting this error:\nOutputs will be scaled by mean and standard deviation specified in NC\nfile.\nComputing outputs for data fraction 1...FAILED: CUBLAS matrix\nmultiplication failed\nit happens after creating the neural network is done and this error\nmessage shows after :\nCreating the neural network... done.\nLayers:\n(0) input size: 4096 lstm size: 512, bias: 1.0, weights: 9440768 softmax\nsize: 8148, bias: 1.0, weights: 4179924 multiclass_classification [size:\n8148]\nTotal weights: 13620692\nAny help would be appreciated ..\nThank you very much..\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "EricZeiberg": "I'm not sure about the first question, but caffe itself is GPU supported with the CUDA Nvidia libraries. Go here http://caffe.berkeleyvision.org/installation.html for more info\n. You can use GPU for caffe predictions, as long as you have the CUDA software installed. \n. haha I was re reading my changes and found another typo. Fixed that one aswell\n. Oh alright\n. ",
    "andyyuan78": "that's bad news!\n. ",
    "liuchang8am": "Maybe should check \"-m\" parameter in predict_on_images.py.\n. The vocabulary size is different for the two datasets, Flickr30k's vocab_size is larger than Flickr8k's, given the same word_threshold. And the vocabulary size also contributes to the total number of the parameters.\n. Of course the speed of the CPU core is also a key feature. And this code does not support GPU. Please check neuraltalk2 instead, which is a re-implementation of the code based on Torch, with GPU support. https://github.com/karpathy/neuraltalk2\n. ",
    "pecorarista": "Thank you @liuchang8am.\nBut I don't understand what you are saying.\nI don't think predict_on_images.py provides \"-m\" option.\n. I understand why I got such a strange result.\nI didn't know how vgg_feats.mat was used and\nthat filenames in tasks.txt should have been aligned according to the order of features in that file.\nAfter I wrote filenames in tasks.txt according to the order of files in dataset.json,\nwhich I got from here,\nneuraltalk generates reasonable sentences for each picture of MS COCO.\nThank you.\n. ",
    "lebronze": "Well, i had the same problem. You can make the para 'logppl' and 'logppln' to a very little value, such as 0.0001 in the line 72,73 driver.py.\nThat works for me.\n. Dear @goelankur7 , did you finally train your multimodal RNN model successfully? And could you test you own images on your multimodal RNN model?\n. Sorry @Utsavz ,the author of this code said he only provided the generating stage in the paper , but didn't provide the alignment stage. \nMaybe you can send an  email to him for some help.\n. ",
    "infinity0a": "hey @lebronze, I had this problem as well, and I use the \"0.0001\". It still doesn't work. Do you have any other fixes?\n. ",
    "sballas8": "Ahh, that explains my confusion. The portion I was looking at was lines 48-59 in imagernn/rnn_generator.py and you do indeed use rand (not randn). I will make sure to read more carefully in the future.\nBest,\nSam\n. ",
    "alyxb": "+1 I had to modify extract_features.py with these changes to get it to run on latest caffe\n. ",
    "linkerlin": "Expecting NEW version.\nWould you use Cython for speed up?\n. ",
    "healthyjk": "I think you can download the code of his paper in nips 2014 from here  http://cs.stanford.edu/people/karpathy/. I think it's a basic prototype of Visual-Semantic Alignment. \n. ",
    "qmiwang": "@healthyjk Thank you !\n. ",
    "liu09114": "I know why it doesn't raise an error. The default parameter of the image encoding size (also the input size) id equal to the hidden size.\nBut the mistake should be correct.\n. ",
    "Utsavz": "@lebronze Did you find out the solution method to the problem? If yes, please answer \n. Thank you @lebronze for the help. \n. ",
    "apanimesh061": "Hi,\nI have a GTX860M with CUDA 7.5 installed on my Windows system. \nBut when I use flickr8 dataset, it takes ~5 seconds for a batch. I installed caffe with cuDNN v4 support. Did you do anything else while building Caffe?\n. ",
    "evercherish": "I didn't do anything else...\nThe only difference is probably my Ubuntu14.04 system...\n\n\u53d1\u4ef6\u4eba: Animesh Pandey notifications@github.com\n\u53d1\u9001\u65f6\u95f4: 2016\u5e743\u67087\u65e5 3:52\n\u6536\u4ef6\u4eba: karpathy/neuraltalk\n\u6284\u9001: evercherish\n\u4e3b\u9898: Re: [neuraltalk] when i train a batch(batchsize 100) of flickr8k, it takes around 0.8 seconds, (#43)\nHi,\nI have a GTX860M with CUDA 7.5 installed on my Windows system.\nBut when I use flickr8 dataset, it takes ~5 seconds for a batch. I installed caffe with cuDNN v4 support. Did you do anything else while building Caffe?\n\u2015\nReply to this email directly or view it on GitHubhttps://github.com/karpathy/neuraltalk/issues/43#issuecomment-193081246.\n. "
}
{
    "syedhali": "You can obtain the FFT using the Accelerate framework. Using the audio data (buffer[0] = left channel) and the buffersize you should be able to calculate it. \nI know that's a kind of cruddy answer, but I've thought about adding an EZFFT class to allow generic fft operations (get frequency of input) on any input (float* or AudioBufferList). After I see how the community uses EZAudio or various applications I'll provide a second release with these kind of extended classes to simplify this process.\nI'll provide maybe a development branch soon with these additional classes before I push the changes live. Stay tuned!\nCheers,\nHaris\n. Alrightly, I've updated the examples to include an EZAudioFFTExample that shows how to use the Accelerate framework to calculate the FFT of the EZMicrophone. The top plot is the time domain and the bottom plot is the frequency domain output via the FFT. Enjoy!\niOS:\n\nOSX:\n\n. Thank you :)\nAre you recording the two sounds to the same file path using the same instance of the EZRecorder?\nIf you're using the same instance of the EZRecorder for both inputs then it will just continue to add the audio data from each to the tail of the existing file, which seems to be the issue you're experiencing. \nIf you're trying to isolate each recording I'd create a different EZRecorder instance for each file path you want to create. Also, each time you create an EZRecorder instance for a file path it will erase the file if it already exists at that path (this is an issue I'm trying to figure out - if the path exists just open it for editing). \nHope that helps!\n. Hello! I really appreciate your enthusiasm and definitely encourage pull requests as you move through the source to help make this thing as awesome as possible :)\nI just looked into this issue and realized I accidentally hard coded the sample rate for the client format on the audio file to be 44.1 kHz. I just pushed a fix to the master branch that pulls the sample rate from the file's format in the EZAudioFile.m: \nobjectivec\n  // Set the client format on the stream\n  _clientFormat.mBitsPerChannel   = 8 * sizeof(AudioUnitSampleType);\n  _clientFormat.mBytesPerFrame    = sizeof(AudioUnitSampleType);\n  _clientFormat.mBytesPerPacket   = sizeof(AudioUnitSampleType);\n  _clientFormat.mChannelsPerFrame = 1;\n  _clientFormat.mFormatFlags      = kAudioFormatFlagsCanonical | kAudioFormatFlagIsNonInterleaved;\n  _clientFormat.mFormatID         = kAudioFormatLinearPCM;\n  _clientFormat.mFramesPerPacket  = 1;\n  _clientFormat.mSampleRate       = _fileFormat.mSampleRate; // Pulls from file format instead of hardcoded 44100\nI tested it with a 48 kHZ, AIFF file before the change and experienced the same problem and tested it with this modification and it's working fine. The change is live now on the master branch if you would like to give it a test. If it all checks out I'll update the Cocoapod.\n. Ah okay, I'll close this issue then :) \n. Hmm I think this has to do with how the EZOutput is trying to release the AudioBufferList from the datasource method: \nobjectivec\n    if( bufferList ){\n      dispatch_async(dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_DEFAULT,0ul),^{\n        [EZAudio freeBufferList:bufferList];\n      });\n    }\nI think a better approach would be to make the EZOutputDataSource responsible for freeing the AudioBufferList from memory, similar to how the EZAudioPassThroughExample works with the TPCircularBuffer datasource method.\n. Probably also the root cause of the Hash table corrupted issue.\n. Hey, just an update on this - I'm rewriting the EZOutput class so it's much less hacky than it was when this issue was originally filed. The fix will be pushed sometime this week in an upcoming release\n. This shouldn't be an issue anymore (the EZAudioFile doesn't load all the audio data into memory anymore so there's never a time where a huge allocation happens)\n. Hey @wdwen, \nI checked out the issue and there's no need to add the extra global for _bgcolor since: \nobjectivec\n-(void)_refreshWithBackgroundColor:(UIColor*)backgroundColor {\n  // Extract colors\n  CGFloat red; CGFloat green; CGFloat blue; CGFloat alpha;\n  [backgroundColor getRed:&red\n                    green:&green\n                     blue:&blue\n                    alpha:&alpha];\n  // Set them on the context\n  glClearColor((GLclampf)red,(GLclampf)green,(GLclampf)blue,(GLclampf)alpha);\n}\nsets it on the OpenGLContext for that view. You were correct in noticing the draw function doesn't draw the background when it doesn't have buffer data so the only change that needs to occur is changing the draw function like so:\n``` objectivec\n-(void)glkView:(GLKView *)view drawInRect:(CGRect)rect {\n// Clear the context\n  glClear(GL_COLOR_BUFFER_BIT);\nif( _hasBufferPlotData || _hasRollingPlotData ){\n    // Prepare the effect for drawing\n    [self.baseEffect prepareToDraw];\n    // Plot either a buffer plot or a rolling plot\n    switch(_plotType) {\n      case EZPlotTypeBuffer:\n        [self _drawBufferPlotWithView:view\n                           drawInRect:rect];\n        break;\n      case EZPlotTypeRolling:\n        [self _drawRollingPlotWithView:view\n                            drawInRect:rect];\n        break;\n      default:\n        break;\n    }\n  }\n}\n```\nIf you want to make that change instead and send another pull request I'd be happy to merge it and give you credit :)\n. You're calling glClear(GL_COLOR_BUFFER_BIT) twice in the glkView:drawInRect: method, try removing the second call now that you've added the top one.\n. Merged, thank you!\n. Which app crashes? One of the EZAudio examples?\n. Which example? Can you provide the console out from the crash? I need some context. \n. @MattFoley  Thank you! This gave me enough information to figure out where the problem was occurring. The CrashIfClientProvidedBogusAudioBufferList occurs when an AudioBufferList that has either been misformatted or previously freed is provided into the ExtAudioFileRead function. Long story short, I needed to make a tweak in the EZAudioFile and EZOutput to fix the issue above (freeing before done using) as well as the EZAudioPlayFromFileExample. I'm testing out the fix now and will try to get it into the master branch and cocoapod for later this afternoon.\n. @MattFoley For sure, I'd love to check it out! Maybe call it EZAudioAdvancedPlayAndRecord?\nThe only thing I ask for license stuff is add the MIT license block to the top of each file in the example project (your AppDelegate, ViewController, and whatever else src you got).\nSomething like:\n//\n//  ViewController.m\n//  EZAudioAdvancedPlayAndRecord\n//\n//  Created by [YOUR NAME] on [TODAY'S DATE FORMATTED MM/DD/YYYY].\n//  Copyright (c) 2014 [YOUR NAME]. All rights reserved.\n//\n//  Permission is hereby granted, free of charge, to any person obtaining a copy\n//  of this software and associated documentation files (the \"Software\"), to deal\n//  in the Software without restriction, including without limitation the rights\n//  to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n//  copies of the Software, and to permit persons to whom the Software is\n//  furnished to do so, subject to the following conditions:\n//\n//  The above copyright notice and this permission notice shall be included in\n//  all copies or substantial portions of the Software.\n//\n//  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n//  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n//  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n//  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n//  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n//  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n//  THE SOFTWARE.\nThanks!\n. Sweet, looking forward to it :)\n. Alright I tweaked the master branch with the changes to the EZOutput and the EZAudioPlayFileExample projects for iOS and OSX to prevent the crash due to the CrashIfClientProvidedBogusAudioBufferList err. I tested it on my iPad, iPhone, and Mac and verified it's working (wasn't working before). Let me know if the EZAudioPlayFileExample projects on the master branch are working for you and I'll update the Cocoapod. \n. This crash sounds related to #15. I'm going to close this ticket and continue working on #15 so go there for updates!\n. Hello! \nWell the goal for the EZMicrophone was to have it so you could start getting microphone data in one line of code without needing to know exactly what's happening under the hood.\nHowever, I can see how setting the AudioStreamBasicDescription yourself would be useful for plenty of use cases. I'm cleaning up the EZMicrophone so I'll add this in the next Cocoapod release (0.0.3).\nThe API for the EZMicrophone will contain one more additional setter (setAudioStreamBasicDescription:) that can be set after you initialize the EZMicrophone instance.\n. Tweaked it, feel free to submit pull requests in the future so you can get credit for your contributions to EZAudio!\n. Modified EZAudioFile to pull number of channels from file format AudioStreamBasicDescription:\nobjectivec\n  // Pull number of channels of frames from file format\n  UInt32 mChannelsPerFrame        = _fileFormat.mChannelsPerFrame;\n  _clientFormat.mBitsPerChannel   = 8 * sizeof(AudioUnitSampleType);\n  _clientFormat.mBytesPerFrame    = mChannelsPerFrame * sizeof(AudioUnitSampleType);\n  _clientFormat.mBytesPerPacket   = mChannelsPerFrame * sizeof(AudioUnitSampleType);\n  _clientFormat.mChannelsPerFrame = mChannelsPerFrame;\n  _clientFormat.mFormatFlags      = kAudioFormatFlagIsPacked|kAudioFormatFlagIsFloat;\n  _clientFormat.mFormatID         = kAudioFormatLinearPCM;\n  _clientFormat.mFramesPerPacket  = 1;\n  _clientFormat.mSampleRate       = _fileFormat.mSampleRate;\nModified EZOutput to properly iterate through interleaved audio data:\nobjectivec\n    // Interleaved\n    if( !(ioData->mNumberBuffers == 1) ){\n      AudioUnitSampleType *left        = (AudioUnitSampleType*)ioData->mBuffers[0].mData;\n      AudioUnitSampleType *right       = (AudioUnitSampleType*)ioData->mBuffers[1].mData;\n      AudioUnitSampleType *interleaved = (AudioUnitSampleType*)bufferList->mBuffers[0].mData;\n      for(int i = 0; i < inNumberFrames; i++ ){\n        if( bufferList ){\n          // pull left and right data from interleaved array\n          *left++  = *interleaved++;\n          *right++ = *interleaved++;\n        }\n        else {\n          left[  i ] = 0.0f;\n          right[ i ] = 0.0f;\n        }\n      }\n    }\nModified output AudioStreamBasicDescription for iOS:\nobjectivec\n  // Setup an ASBD\n  AudioStreamBasicDescription asbd;\n  UInt32 mChannelsPerFrame = 2;\n  asbd.mBitsPerChannel   = 8 * sizeof(AudioUnitSampleType);\n  asbd.mBytesPerFrame    = mChannelsPerFrame * sizeof(AudioUnitSampleType);\n  asbd.mBytesPerPacket   = mChannelsPerFrame * sizeof(AudioUnitSampleType);\n  asbd.mChannelsPerFrame = mChannelsPerFrame;\n  asbd.mFormatFlags      = kAudioFormatFlagIsPacked|kAudioFormatFlagIsFloat;\n  asbd.mFormatID         = kAudioFormatLinearPCM;\n  asbd.mFramesPerPacket  = 1;\n  asbd.mSampleRate       = hardwareSampleRate;\n. Finished adding stereo support to EZAudioFile and added custom AudioStreamBasicDescription setters for EZOutput to allow flexible playback for any EZAudioFile client format. Also updated the EZAudioPlayFile examples for iOS and OSX to demonstrate setting custom AudioStreamBasicDescriptions.\n. Hey thanks for the pull request, I just pushed a tweak a little earlier tonight to do:\n_glViewController.view.frame = self.bounds;\nwhich is the same as \n_glViewController.view.frame = CGRectMake(0, 0, self.frame.size.width, self.frame.size.height);\nso I'm going to close this request\n. Helloo, glad you're digging EZAudio! I think a scrolling waveform would be an awesome addition. Perhaps adding an additional EZPlotType like:\nobjectivec\n/**\n The types of plots that can be displayed in the view using the data.\n */\ntypedef NS_ENUM(NSInteger,EZPlotType){\n  /**\n   *  Plot that displays only the samples of the current buffer\n   */\n  EZPlotTypeBuffer,\n  /**\n   *  Plot that displays a rolling history of values using the RMS calculated for each incoming buffer\n   */\n  EZPlotTypeRolling,\n  /**\n   *  Plot that displays a scrolling, rolling history of values using the RMS calculated for each incoming buffer\n   */\n  EZPlotTypeScrolling,\n};\nand extending the plots to accompany the custom history buffer handling to support the scrolling (once full, add N data to front and remove N data from beginning of display buffer).\nI'm currently busy working on an EZAudio version 0.0.3 with a couple bug fixes, updates, and extensions for the EZMicrophone, EZAudioFile, and EZOutput based on the emails I've been getting in the last few weeks. I'll put this on the radar for the next version 0.0.4 as an extension for the EZAudioPlot and EZAudioPlotGL. \nContributing\nIf anyone is interested in becoming an EZAudio contributor and would like to take a crack at implementing this scrolling waveform for either the EZAudioPlot or the EZAudioPlotGL I'd happily accept a pull request!\n. I hacked on this a little last night and got the scrolling waveform with an adjustable history buffer. This is more of the behavior I had in mind when I was first designing the EZPlotTypeRolling so I'm going to refactor the EZAudioPlot and EZAudioPlotGL now so you can expect it in the update for 0.0.3.\n. Alright, I've added scrolling behavior to the EZAudioPlot and EZAudioPlotGL for iOS and OSX. All rolling plots included in the examples will now scroll instead of wipe the whole screen. You can test the master branch if you'd like - here's a gif of it in action (check the examples to see the actual speed):\n\n. Interesting, I think URLs referenced by the MPMediaItem can only be opened using the AVFoundation. I'll take a look and see if I can find a way around. \n. So any file coming from the iPod library as an MPMediaItem will have a path prefix, ipod://, which doesn't agree with the EZAudioFile. Here's an approach to create an EZAudioFile using the AVAssetExportSession to export the audio data from the MPMediaItem to the app's bundle.\n``` objectivec\n//\n// Creates an EZAudioFile from an MPMediaItem representing a song coming from\n// an iOS device's iPod library. Since an MPMediaItem's URL is always prefixed\n// with an ipod:// path we must use the AVAssetExportSession to first export\n// the song to a file path that the EZAudioFile can actually find in the app's bundle.\n//\n- (void)openMediaItem:(MPMediaItem )item\n           completion:(void(^)(EZAudioFile audioFile, NSError error))completion\n{\n    NSURL url = [item valueForProperty:MPMediaItemPropertyAssetURL];\n    NSString title = [item valueForProperty:MPMediaItemPropertyTitle];\n    if (url)\n    {\n        //\n        // Create an AVAudioExportSession to export the MPMediaItem to a non-iPod\n        // file path url we can actually use for an EZAudioFile\n        //\n        AVURLAsset asset = [AVURLAsset assetWithURL:url];\n        AVAssetExportSession exporter = [AVAssetExportSession exportSessionWithAsset:asset\n                                                                           presetName:AVAssetExportPresetAppleM4A];\n        exporter.outputFileType = @\"com.apple.m4a-audio\";\n        NSString exportURLPath = [[self applicationDocumentsDirectory] stringByAppendingFormat:@\"/%@.m4a\", title];\n        NSURL *exportURL = [NSURL fileURLWithPath:exportURLPath];\n        exporter.outputURL = exportURL;\n    //\n    // Delete any existing path in the bundle if one already exists\n    //\n    NSFileManager *fileManager = [NSFileManager defaultManager];\n    if ([fileManager fileExistsAtPath:exportURLPath])\n    {\n        NSError *error;\n        [fileManager removeItemAtPath:exportURLPath error:&error];\n        if (error)\n        {\n            NSLog(@\"error deleting file: %@\", error.localizedDescription);\n        }\n    }\n\n    //\n    // Export the audio data using the AVAudioExportSession to the\n    // exportURL in the application bundle\n    //\n    [exporter exportAsynchronouslyWithCompletionHandler:^{\n        AVAssetExportSessionStatus status = [exporter status];\n        switch (status)\n        {\n            case AVAssetExportSessionStatusCompleted:\n            {\n                EZAudioFile *file = [EZAudioFile audioFileWithURL:exportURL];\n                completion(file ,nil);\n                break;\n            }\n            case AVAssetExportSessionStatusFailed:\n            {\n                completion(nil, exporter.error);\n                break;\n            }\n            default: \n            {\n                NSLog(@\"Exporter status not fialed or complete: %ld\", status);\n                break;\n            }\n        }\n    }];\n}\nelse\n{\n    NSError *error = [NSError errorWithDomain:@\"com.myapp.sha\"\n                                         code:404\n                                     userInfo:@{ NSLocalizedDescriptionKey : @\"Media item's URL not found\" }];\n    completion(nil, error);\n}\n\n}\n```\nUsage:\nobjectivec\n//\n// Get an EZAudioFile from the first valid MPMediaItem (meaning it has a URL)\n// from the user's iPod library\n//\nMPMediaQuery *everything = [[MPMediaQuery alloc] init];\nMPMediaItem *song = [[everything items] firstObject];\n[self openMediaItem:song\n         completion:^(EZAudioFile *audioFile,\n                      NSError *error)\n{\n     NSLog(@\"audio file: %@, error: %@\", audioFile, error);\n}];\nI could build this into the EZAudioFile, but it would require including the MediaPlayer API as a dependency for EZAudio on iOS devices. Anyone feel strongly that this should be integrated directly into EZAudio?\n. Going to close this ticket for now\n. Very interesting, I had an idea a while ago to write a streamer that could also provide audio data to generate full waveforms so people could write soundcloud style client apps without having to render the audio data ahead of time in the backend. \nI think there is definitely an http streamer in the near future for EZAudio, but it may be a few minor versions away and I'd like to get a few perspectives on this so I can get a better idea of what people want/need.\n. I think it would be interesting to integrate EZAudio with the DOUAudioStreamer. That's a really well written audio streamer framework that's also iOS and Mac. \n. Hello! \nThat doesn't sound good - I'll take a look at it and try to get it in before the upcoming 0.0.3 release.\n. Hey, \nJust an update - I've been doing quite a bit of work on the EZOutput class and it's getting to a much better state. This issue should be resolved sometime this week with a new update.\n. This should be fixed. The problem seemed to be with the AudioStreamBasicDescription set on the EZOutput. \n. Hello! Thanks for sending over the issue, I took a look at it and found the issue in the EZOutput (has to do with how I'm freeing those AudioBufferLists when the EZOutput is done with it to prevent memory leaks). I'm testing out the tweaks tonight so the changes should be on the master branch sometime tomorrow.\n. Alright I tweaked the master branch with the changes to the EZOutput and the EZAudioPlayFileExample projects for iOS and OSX to prevent the crash due to the CrashIfClientProvidedBogusAudioBufferList err. I tested it on my iPad, iPhone, and Mac and verified it's working (wasn't working before). Let me know if the EZAudioPlayFileExample projects on the master branch are working for you and I'll update the Cocoapod. \n. Hey guys, sorry for the delayed response. \nI've figured out what the (bigger) issue was and I'm refactoring the EZOutput and EZAudioFile to better solve the underlying issue under the hood. I'll be pushing the fix live tomorrow night when I've done a little testing. Thanks for bearing with me!\n. Hey @prisonerjohn and @littlebearuk, I updated the EZAudio source last night so this issue should be resolved. Please let me know when you get a chance to try out the EZAudioPlayFileExample!\n. This should be working after the last refactor - please let me know if it's still an issue in a new ticket\n. Hey @bssayeda ,\nGet idea! I'll add this to the EZOutput so an EZOutputDelegate can listen for the rendered audio data like with the EZMicrophoneDelegate method: \nobjective-c\n-(void)    microphone:(EZMicrophone*)microphone\n     hasAudioReceived:(float**)buffer\n       withBufferSize:(UInt32)bufferSize\n withNumberOfChannels:(UInt32)numberOfChannels;\nFor EZOutput it will contain an EZOutputDelegate be:\nobjective-c\n-(void)    output:(EZOutput*)output\n     playedAudio:(float**)buffer\n       withBufferSize:(UInt32)bufferSize\n withNumberOfChannels:(UInt32)numberOfChannels;\n. Hey @swtlovewtt , \nSorry for the late reply, but this is actually the same issue as #15. I'm going to close this one as a duplicate and continue updating #15 as it gets completed.\n. Hey @jorge-ramos, \nThanks for the heads up! Fortunately this isn't breaking anything (yet?), but I'll update the upcoming 1.0 version of EZAudio to use the shared AVAudioSession singleton like apple recommends.\n. The AudioSession API was removed in 0.0.6 and has since been replaced with the AVAudioSession API.\n. Hey @felart ,\nThank you! Unfortunately I haven't been able to figure out how to embed multiple EZAudioPlotGL plots per visible screen. I'm by no means an OpenGL expert, but I anticipate a better way to handle this would be to have a subclass of UIView that provides a table or grid of EZAudioPlotGL within on glContext. Ideas and pull requests are welcome :)\n. Just an update - \n@andykorth has made this possible for iOS by setting the shared context. I'm looking to add this tweak for Mac soon.\n. Hey @augmentedworks,\nSorry for the delay. So the EZAudioPlotGL (unfortunately) does currently require you to stop updating the plot when the app moves into the background (screen is not in the foreground). I would happily encourage a pull request if you would like to add a fix to the EZAudioPlotGL classes to register with the NSNotificationCenter and listen for that particular event on iOS (just needs to pause the drawing). Either way, I'll anticipate a fix for this in the upcoming 1.0 release.\n. This has been fixed as of the 0.7.1 release\n. Currently on the master branch I added a closeAudioFile function to the EZRecorder. I'm finishing up adding a clear function to the EZAudioPlot that may be helpful.\n. @Wahoozaboo are you using the same instance of the EZRecorder? Technically, if you close the audio file and create a new instance for your second recording you should be fine. \n. Which component are you using? The EZRecorder to record to a file or the EZAudioFile to read from a file?\n. There's now a M4A recording format for the EZRecorder that significantly lowers the recorded file's size. Check out the EZRecorderFileTypeM4A value. The iOS and Mac EZAudioRecordExample examples in the 0.3.0 release use this format if you'd like an example.\n``` objectivec\n/**\n To ensure valid recording formats are used when recording to a file the EZRecorderFileType describes the most common file types that a file can be encoded in. Each of these types can be used to output recordings as such:\nEZRecorderFileTypeAIFF - .aif, .aiff, .aifc, .aac\n EZRecorderFileTypeM4A  - .m4a, .mp4\n EZRecorderFileTypeWAV  - .wav\n/\ntypedef NS_ENUM(NSInteger, EZRecorderFileType)\n{\n    /\n     Recording format that describes AIFF file types. These are uncompressed, LPCM files that are completely lossless, but are large in file size.\n     */\n    EZRecorderFileTypeAIFF,\n    /\n     Recording format that describes M4A file types. These are compressed, but yield great results especially when file size is an issue.\n     /\n    EZRecorderFileTypeM4A,\n    /*\n     Recording format that describes WAV file types. These are uncompressed, LPCM files that are completely lossless, but are large in file size.\n     /\n    EZRecorderFileTypeWAV\n};\n```\n. Hey @jwmao,\n1) Stereo buffers are typically identical. The input sensors are so close to each other you're not going to see a big difference between the two channels. \n2) The EZMicrophone callback you specified will only return buffer[1] if it is a stereo signal. Similarly, the numberOfChannels argument gives you the number of channels set on the stream description. \n. Hi, not sure if this is still an issue, but check out the EZAudioCoreGraphicsWaveformExample to see how to incorporate the plot into your code. Specifically your viewDidLoad should look something like:\n``` objectivec\n- (void)viewDidLoad\n{\n    [super viewDidLoad];\n//\n// Setup the AVAudioSession. EZMicrophone will not work properly on iOS\n// if you don't do this!\n//\nAVAudioSession *session = [AVAudioSession sharedInstance];\nNSError *error;\n[session setCategory:AVAudioSessionCategoryPlayAndRecord error:&error];\nif (error)\n{\n    NSLog(@\"Error setting up audio session category: %@\", error.localizedDescription);\n}\n[session setActive:YES error:&error];\nif (error)\n{\n    NSLog(@\"Error setting up audio session active: %@\", error.localizedDescription);\n}\n\n// Plot type\nself.audioPlot.plotType = EZPlotTypeBuffer;\n\n//\n// Create the microphone\n//\nself.microphone = [EZMicrophone microphoneWithDelegate:self];\n\n}\n```\nFeel free to reopen this ticket if it is still an issue\n. Nice :)\n. @chrisballinger,\nThanks for the heads up, were you able to patch it up? If so, I'd gladly accept a pull request, and, if not, I'll add it to my todos :) \n. As of 0.1.0 release the EZMicrophone was rewritten to better support custom AudioStreamBasicDescription so you should just be able to use the initializer:\nobjectivec\n/**\n Creates an instance of the EZMicrophone with a delegate to respond to the audioReceived callback. This will not start fetching the audio until startFetchingAudio has been called. Use microphoneWithDelegate:startsImmediately: to instantiate this class and immediately start fetching audio data.\n @param     delegate    A EZMicrophoneDelegate delegate that will receive the audioReceived callback.\n @param     audioStreamBasicDescription A custom AudioStreamBasicFormat for the microphone input.\n @return    An instance of the EZMicrophone class. This should be declared as a strong property!\n */\n+ (EZMicrophone *)microphoneWithDelegate:(id<EZMicrophoneDelegate>)delegate\n         withAudioStreamBasicDescription:(AudioStreamBasicDescription)audioStreamBasicDescription;\n. Hey @foozmeat, \nI thought about this a little while ago, but I think it's out of the scope for the moment. The idea of EZAudio was to simplify the processing of actually getting and playing the audio data - what you do with the data (visualizing, filtering, fingerprinting) is completely up to you for your specific use case.\nI use musicdsp all the time for filters and effects. I encourage you to check it out and experiment using the EZMicrophone to get the samples in and perform your DSP, EZAudioPlotGL to visualize the samples coming out of the DSP, and EZOutput to playback the processed samples.\nHope that helps!\n. As of 0.7.1 you can provide an interleaved format as the inputFormat to the EZOutput during initialization like so:\nobjectivec\nAudioStreamBasicDescription inputFormat = [EZAudioUtilities stereoFloatInterleavedFormatWithSampleRate:44100.0f];\nself.output = [EZOutput outputWithDataSource:self inputFormat:inputFormat];\n. The EZRecorder and EZMicrophone must have the same format, see the \nEZAudioRecordingExample. The format you are setting on the EZRecorder is the client format, not the file format.\nThink of the client format as the common language that allows the component to talk to other components in the context of an application, independent of what the actual file format is on disk. The client format must be linear PCM to work properly (the mic is always linear PCM). \nMeanwhile, the file format is determined when you initialize the EZRecorder using one of the destination file types enumerated below:\n``` objectivec\n/**\n To ensure valid recording formats are used when recording to a file the EZRecorderFileType describes the most common file types that a file can be encoded in. Each of these types can be used to output recordings as such:\nEZRecorderFileTypeAIFF - .aif, .aiff, .aifc, .aac\n EZRecorderFileTypeM4A  - .m4a, .mp4\n EZRecorderFileTypeWAV  - .wav\n/\ntypedef NS_ENUM(NSInteger, EZRecorderFileType)\n{\n    /\n     Recording format that describes AIFF file types. These are uncompressed, LPCM files that are completely lossless, but are large in file size.\n     */\n    EZRecorderFileTypeAIFF,\n    /\n     Recording format that describes M4A file types. These are compressed, but yield great results especially when file size is an issue.\n     /\n    EZRecorderFileTypeM4A,\n    /*\n     Recording format that describes WAV file types. These are uncompressed, LPCM files that are completely lossless, but are large in file size.\n     /\n    EZRecorderFileTypeWAV\n};\n```\nUsage:\nobjectivec\nself.recorder = [EZRecorder recorderWithDestinationURL:[self testFilePathURL]\n                                          sourceFormat:self.microphone.audioStreamBasicDescription\n                                   destinationFileType:EZRecorderFileTypeM4A];\n. As of version 0.3.0 there's been a few changes that will make this much easier. \n1.) The EZMicrophone supports direct out so you can hear the microphone input by using the setOutput: method.\n2.) You can process the microphone's incoming audio via the EZMicrophoneDelegate method, microphone:hasBufferList:withBufferSize:withNumberOfChannels:\nFor example, \n``` objectivec\n- (void)viewDidLoad\n{\n    // \n    // Create an EZMicrophone instance\n    //\n    self.microphone = [EZMicrophone microphoneWithDelegate:self];\n//\n// Set the EZOutput singleton as the EZMicrophone's output\n//\n[self.microphone setOutput:[EZOutput sharedOutput]];\n\n// \n// Start the microphone\n//\n[self.microphone startFetchingAudio];\n\n//\n// Start the output\n//\n[[EZOutput sharedOutput] startPlayback];\n\n}\n\n(void)   microphone:(EZMicrophone )microphone\n        hasBufferList:(AudioBufferList )bufferList\n       withBufferSize:(UInt32)bufferSize\n withNumberOfChannels:(UInt32)numberOfChannels\n{\n    //\n    // Process audio (here we're just silencing it)\n    //\n    for (int i = 0; i < bufferList->mNumberBuffers; i++)\n    {\n        memset(bufferList->mBuffers[i].mData,\n               0,\n               bufferList->mBuffers[i].mDataByteSize);\n    }\n}\n```\n. Nice catch, thanks for the contribution!\n. Thanks!\n. This should be fixed as of the 0.7.1 release. \n. The EZMicrophone now properly handles interruptions on iOS:\n\nobjectivec\n- (void)microphoneWasInterrupted:(NSNotification *)notification\n{\n    AVAudioSessionInterruptionType type = [notification.userInfo[AVAudioSessionInterruptionTypeKey] unsignedIntegerValue];\n    switch (type)\n    {\n        case AVAudioSessionInterruptionTypeBegan:\n        {\n            [self stopFetchingAudio];\n            break;\n        }\n        case AVAudioSessionInterruptionTypeEnded:\n        {\n            AVAudioSessionInterruptionOptions option = [notification.userInfo[AVAudioSessionInterruptionOptionKey] unsignedIntegerValue];\n            if (option == AVAudioSessionInterruptionOptionShouldResume)\n            {\n                [self startFetchingAudio];\n            }\n            break;\n        }\n        default:\n        {\n            break;\n        }\n    }\n}\n. Awesome, thanks @andykorth!\n. Hey @andykorth, thanks for the pull request! \nJust want to give an update- the tweaks look pretty straightforward. I'll take a closer look tonight and hopefully get them merged or get back to you by tomorrow.\n. I ended up rewriting the EZAudioPlotGL in the 0.6.0 release with these fixes so I'm going to close this one for now.\n. Hey @scriptedSheep, thank you! Check out #45 for a clear background.\n\"Setting the opaque value to NO fixes this.\"\n. Actually EZAudio's EZAudioFile already supports reading all the following file formats:\nobjectivec\n+ (NSArray *)supportedAudioFileTypes\n{\n    return @\n    [\n        @\"aac\",\n        @\"caf\",\n        @\"aif\",\n        @\"aiff\",\n        @\"aifc\",\n        @\"mp3\",\n        @\"mp4\",\n        @\"m4a\",\n        @\"snd\",\n        @\"au\",\n        @\"sd2\",\n        @\"wav\"\n    ];\n}\nwhile the EZRecorder supports writing to m4a, aiff, and wav via the destination format value:\n``` objectivec\n/**\n To ensure valid recording formats are used when recording to a file the EZRecorderFileType describes the most common file types that a file can be encoded in. Each of these types can be used to output recordings as such:\nEZRecorderFileTypeAIFF - .aif, .aiff, .aifc, .aac\n EZRecorderFileTypeM4A  - .m4a, .mp4\n EZRecorderFileTypeWAV  - .wav\n/\ntypedef NS_ENUM(NSInteger, EZRecorderFileType)\n{\n    /\n     Recording format that describes AIFF file types. These are uncompressed, LPCM files that are completely lossless, but are large in file size.\n     */\n    EZRecorderFileTypeAIFF,\n    /\n     Recording format that describes M4A file types. These are compressed, but yield great results especially when file size is an issue.\n     /\n    EZRecorderFileTypeM4A,\n    /*\n     Recording format that describes WAV file types. These are uncompressed, LPCM files that are completely lossless, but are large in file size.\n     /\n    EZRecorderFileTypeWAV\n};\n```\n. Please see my response to issue #12. You can still access iPod songs as EZAudioFile, but you first have to use the AVAssetExportSession to move the audio to a valid url that EZAudioFile can use.\n``` objectivec\n//\n// Creates an EZAudioFile from an MPMediaItem representing a song coming from\n// an iOS device's iPod library. Since an MPMediaItem's URL is always prefixed\n// with an ipod:// path we must use the AVAssetExportSession to first export\n// the song to a file path that the EZAudioFile can actually find in the app's bundle.\n//\n- (void)openMediaItem:(MPMediaItem )item\n           completion:(void(^)(EZAudioFile audioFile, NSError error))completion\n{\n    NSURL url = [item valueForProperty:MPMediaItemPropertyAssetURL];\n    NSString title = [item valueForProperty:MPMediaItemPropertyTitle];\n    if (url)\n    {\n        //\n        // Create an AVAudioExportSession to export the MPMediaItem to a non-iPod\n        // file path url we can actually use for an EZAudioFile\n        //\n        AVURLAsset asset = [AVURLAsset assetWithURL:url];\n        AVAssetExportSession exporter = [AVAssetExportSession exportSessionWithAsset:asset\n                                                                           presetName:AVAssetExportPresetAppleM4A];\n        exporter.outputFileType = @\"com.apple.m4a-audio\";\n        NSString exportURLPath = [[self applicationDocumentsDirectory] stringByAppendingFormat:@\"/%@.m4a\", title];\n        NSURL *exportURL = [NSURL fileURLWithPath:exportURLPath];\n        exporter.outputURL = exportURL;\n    //\n    // Delete any existing path in the bundle if one already exists\n    //\n    NSFileManager *fileManager = [NSFileManager defaultManager];\n    if ([fileManager fileExistsAtPath:exportURLPath])\n    {\n        NSError *error;\n        [fileManager removeItemAtPath:exportURLPath error:&error];\n        if (error)\n        {\n            NSLog(@\"error deleting file: %@\", error.localizedDescription);\n        }\n    }\n\n    //\n    // Export the audio data using the AVAudioExportSession to the\n    // exportURL in the application bundle\n    //\n    [exporter exportAsynchronouslyWithCompletionHandler:^{\n        AVAssetExportSessionStatus status = [exporter status];\n        switch (status)\n        {\n            case AVAssetExportSessionStatusCompleted:\n            {\n                EZAudioFile *file = [EZAudioFile audioFileWithURL:exportURL];\n                completion(file ,nil);\n                break;\n            }\n            case AVAssetExportSessionStatusFailed:\n            {\n                completion(nil, exporter.error);\n                break;\n            }\n            default: \n            {\n                NSLog(@\"Exporter status not fialed or complete: %ld\", status);\n                break;\n            }\n        }\n    }];\n}\nelse\n{\n    NSError *error = [NSError errorWithDomain:@\"com.myapp.sha\"\n                                         code:404\n                                     userInfo:@{ NSLocalizedDescriptionKey : @\"Media item's URL not found\" }];\n    completion(nil, error);\n}\n\n}\n``\n. @JeanRintoul sounds like you resolved this, I'm going to close this for now!\n. This should be fixed as of the 0.7.1 release!\n. Yup, seems we should definitely remove the exit(1) when a build is being built for release and raise an exception instead of exiting for debug.\n. As of the 0.1.0 release can just globally setsetShouldExitOnCheckResultFail:` to NO on the EZAudioUtilities class to prevent EZAudio from exiting the application.\nobjectivec\n[EZAudioUtilities setShouldExitOnCheckResultFail:NO];\n. Hm, I'm not actually able to reproduce. Is this in Xcode 5 or Xcode 6? iOS or Mac?\n. Updated!\n. Hm, it's working for me. \n-> EZAudio (0.0.5)\n   A simple, intuitive audio framework for iOS and OSX useful for anyone doing audio processing and/or audio-based visualizations.\n   pod 'EZAudio', '~> 0.0.5'\n   - Homepage: http://syedharisali.com/projects/EZAudio/getting-started\n   - Source:   https://github.com/syedhali/EZAudio.git\n   - Versions: 0.0.5, 0.0.4, 0.0.3, 0.0.2, 0.0.1 [master repo]\nMaybe try updating your cocoapods?\nsudo gem install cocoapods\n. Sorry for the late reply, have you checked out the EZAudioPlayer? \n. This should be fixed in the 0.3.0 release. Read/seek operations are wrapped in a mutex lock to make sure things are thread-safe.\n. Hm, when the EZOutput is created it should be using the default output device:\nobjectivec\n  AudioComponentDescription outputcd;\n  outputcd.componentType         = kAudioUnitType_Output;\n  outputcd.componentSubType      = kAudioUnitSubType_DefaultOutput;\n  outputcd.componentManufacturer = kAudioUnitManufacturer_Apple;\nI might have messed something up here, so you may want to play with the code here:\n``` objectivec\n  //\n  AudioComponentDescription outputcd;\n  outputcd.componentType         = kAudioUnitType_Output;\n  outputcd.componentSubType      = kAudioUnitSubType_DefaultOutput;\n  outputcd.componentManufacturer = kAudioUnitManufacturer_Apple;\n//\n  AudioComponent comp = AudioComponentFindNext(NULL,&outputcd);\n  if( comp == NULL ){\n    NSLog(@\"Failed to get output unit\");\n    exit(-1);\n  }\n  [EZAudio checkResult:AudioComponentInstanceNew(comp,&_outputUnit)\n             operation:\"Failed to open component for output unit\"];\n```\n. I'm not sure about the current version of EZAudio, but for the new version of EZAudio I'm going to keep this in mind as a feature on the default audio plots. New release will probably be sometime in October.\n. Hey guys, sorry I've been a little MIA. I've been holding off on compiling EZAudio for Xcode 6 until it's out of beta, however, the reason it's not playing nicely (from what I can see) is that they deprecated the glPush and glPop methods in OpenGL ES 2.0. \nThere is a pull request #44 that addresses these issues, but I had other issues testing it (I think the waveform wasn't drawing in the simulator?). I'll look into it sometime this weekend and try to get an update to the repo.\n. hey all, just a quick update. I'm working on this now (just switched over my workflow to using xcode 6) so there should be a new pod release relatively soon\n. I just updated EZAudio to version 0.0.6, which [should] resolve the Xcode 6 and iOS 8 issues.\npod 'EZAudio', '~> 0.0.6'\n. @castles  Are you using the EZAudioFile to extract the audio data and plotting it with the EZAudioPlot?\n. hey @castles, the EZAudioFile currently extracts out the audio into a interleaved format, which means the AudioBufferList doesn't contain 2 separate buffers for left and right channels, but one buffer with left and right channels one after the other:\nnon-interleaved:\nLLLLLLLLLLLL\nRRRRRRRRRR\ninterleaved:\nLRLRLRLRLR\nThe EZAudioFile takes the RMS of total buffer with L and R channels being averaged together, meaning:\nRMS = (L^2 + R^2 + L^2 + R^2 + L^2 + R^2 + ...) / bufferSize\n==\nRMS = (audio[0]^2 + audio[1]^2 + audio[2]^2 + audio[3]^2 + ...) / bufferSize\nTo break these into stereo channels (which makes total sense and is something I have planned for the next release, however that may not be for a little while since there's a bigger rewrite in the works), you'd want to have two separate arrays to hold the RMS values and calculate them like:\nbuffer left:\nRMS = (L^2 + L^2 + L^2 + L^2 + ...) / ( bufferSize / 2 )\n== \nRMS = (audio[0]^2 + audio[2]^2 + audio[4]^2 + audio[6]^2 + ...) / ( bufferSize / 2 )\nbuffer right:\nRMS = (R^2 + R^2 + R^2 + R^2 + ...) / ( bufferSize / 2 )\n== \nRMS = (audio[1]^2 + audio[3]^2 + audio[5]^2 + audio[7]^2 + ...) / ( bufferSize / 2 )\n. awesome!\n. have you tried \nview.opaque = NO;\n. Try setting the glkviewcontroller.view.opaque = NO\n. awesome!\n. Interesting, can you print out the RMS of the audio buffer coming in? Is the audio plot blank or is it a flat line on the x-axis?\n. As of 0.0.6 EZAudio doesn't use AudioUnitSampleType values anymore, just floats. Judging by the response on @davidwadge's gist there might've been some Swift bridging weirdness that caused issues, but other people were able to make it work. Going to close this for now.\n. Know I'm a little late here, but this seems to be an issue with the OpenGL context. I'll look into this a bit, but TBH I'm in the process of writing a new version of EZAudio with a completely new EZAudioPlotGL that's much easier to customize and allows multiple GL plots at once. This probably won't get released until sometime in October, but I figured I'll let you guys know!\n. You want to adjust the rolling history length. When you update the audio plot it takes the current buffer of audio data and calculates the RMS of that buffer to append one point to the audio plot. So the bigger the rolling history length the more points you're storing so the more buffers it will take to fully fill up the plot. \nFor instance, for a sample rate = 44100 the microphone callback will trigger ~86 times a second (44100 frames per sec / 512 frames per callback ). So a rolling history length of 1024 will correspond to displaying about 11.9 seconds of audio. Hence, you can calculate the required rolling history length using the formula:\nrollingHistoryLength = secondsOfAudio * (sampleRate / framesPerCallback)\nframesPerCallback you can get from the first time the audio callback executes (microphone:receivedAudio:...etc)\nobjectivec\n[self.audioPlot setRollingHistoryLength:rollingHistoryLength]\n. I'd recommend taking a look at the getWaveformDataWithNumberOfPoints: method on the EZAudioFile to get a better idea of how you can could create a waveform for the segment of data you're interested in.\n``` objectivec\n- (EZAudioFloatData )getWaveformDataWithNumberOfPoints:(UInt32)numberOfPoints\n{\n    EZAudioFloatData waveformData;\n    if (pthread_mutex_trylock(&_lock) == 0)\n    {\n        // store current frame\n        SInt64 currentFrame = self.frameIndex;\n        BOOL interleaved = [EZAudioUtilities isInterleaved:self.clientFormat];\n        UInt32 channels = self.clientFormat.mChannelsPerFrame;\n        float data = (float )malloc(sizeof(float ) * channels);\n        for (int i = 0; i < channels; i++)\n        {\n            data[i] = (float )malloc(sizeof(float) * numberOfPoints);\n        }\n    // seek to 0\n    [EZAudioUtilities checkResult:ExtAudioFileSeek(self.info->extAudioFileRef,\n                                                   0)\n                        operation:\"Failed to seek frame position within audio file\"];\n\n    // calculate the required number of frames per buffer\n    SInt64 framesPerBuffer = ((SInt64) self.totalClientFrames / numberOfPoints);\n    SInt64 framesPerChannel = framesPerBuffer / channels;\n\n    // allocate an audio buffer list\n    AudioBufferList *audioBufferList = [EZAudioUtilities audioBufferListWithNumberOfFrames:(UInt32)framesPerBuffer\n                                                                          numberOfChannels:self.info->clientFormat.mChannelsPerFrame\n                                                                               interleaved:interleaved];\n\n    // read through file and calculate rms at each point\n    for (SInt64 i = 0; i < numberOfPoints; i++)\n    {\n        UInt32 bufferSize = (UInt32) framesPerBuffer;\n        [EZAudioUtilities checkResult:ExtAudioFileRead(self.info->extAudioFileRef,\n                                                       &bufferSize,\n                                                       audioBufferList)\n                            operation:\"Failed to read audio data from file waveform\"];\n        if (interleaved)\n        {\n            float *buffer = (float *)audioBufferList->mBuffers[0].mData;\n            for (int channel = 0; channel < channels; channel++)\n            {\n                float channelData[framesPerChannel];\n                for (int frame = 0; frame < framesPerChannel; frame++)\n                {\n                    channelData[frame] = buffer[frame * channels + channel];\n                }\n                float rms = [EZAudioUtilities RMS:channelData length:(UInt32)framesPerChannel];\n                data[channel][i] = rms;\n            }\n        }\n        else\n        {\n            for (int channel = 0; channel < channels; channel++)\n            {\n                float *channelData = audioBufferList->mBuffers[channel].mData;\n                float rms = [EZAudioUtilities RMS:channelData length:bufferSize];\n                data[channel][i] = rms;\n            }\n        }\n    }\n\n    // clean up\n    [EZAudioUtilities freeBufferList:audioBufferList];\n\n    // seek back to previous position\n    [EZAudioUtilities checkResult:ExtAudioFileSeek(self.info->extAudioFileRef,\n                                                   currentFrame)\n                        operation:\"Failed to seek frame position within audio file\"];\n\n    pthread_mutex_unlock(&_lock);\n\n    waveformData = [EZAudioFloatData dataWithNumberOfChannels:channels\n                                                      buffers:(float **)data\n                                                   bufferSize:numberOfPoints];\n\n    // cleanup\n    for (int i = 0; i < channels; i++)\n    {\n        free(data[i]);\n    }\n    free(data);\n}\nreturn waveformData;\n\n}\n```\n. Unforunately there's no way to fetch streaming live audio data in EZAudio yet (it's upcoming, but I haven't had time to integrate with the DOUAudioStreamer yet). The only thing I can think of now would be fetching the audio as AudioBufferLists and feed those to the EZOutput.\n. Could you post the crash here (i just need to see the crash log to determine what might be happening). What audio plot are you using, the EZAudioPlotGL probably?\n. Ah that project is using the EZAudioFile to wrap the audio files and the EZOutput for playback using the AudioBufferList provided from the EZAudioFile instance. The EZAudioFile's supported formats are only those natively supported by Core Audio, specifically:\nobjectivec\n+(NSArray *)supportedAudioFileTypes {\n  return @[ @\"aac\",\n            @\"caf\",\n            @\"aif\",\n            @\"aiff\",\n            @\"aifc\",\n            @\"mp3\",\n            @\"mp4\",\n            @\"m4a\",\n            @\"snd\",\n            @\"au\",\n            @\"sd2\",\n            @\"wav\" ];\n}\nThis array of file names are used in that example to prevent file formats not natively supported by Core Audio to be loaded in the EZAudioFile here:\nobjectivec\n/**\n Here's an example how to filter the open panel to only show the supported file types by the EZAudioFile (which are just the audio file types supported by Core Audio).\n */\n-(BOOL)panel:(id)sender shouldShowFilename:(NSString *)filename {\n  NSString* ext = [filename pathExtension];\n  if ([ext isEqualToString:@\"\"] || [ext isEqualToString:@\"/\"] || ext == nil || ext == NULL || [ext length] < 1) {\n    return YES;\n  }\n  NSArray *fileTypes = [EZAudioFile supportedAudioFileTypes];\n  NSEnumerator* tagEnumerator = [fileTypes objectEnumerator];\n  NSString* allowedExt;\n  while ((allowedExt = [tagEnumerator nextObject]))\n  {\n    if ([ext caseInsensitiveCompare:allowedExt] == NSOrderedSame)\n    {\n      return YES;\n    }\n  }\n  return NO;\n}\n. This seems to be a duplicate of (#80)\n. Looks like this is coming from the getWaveformData function is getting triggered at the same time as when the EZAudioFile is being used for playback. The getWaveformData function internally reads and seeks through the audio file to generate the waveform data (at least the first time, then it caches the data). If you are trying to perform playback while this is happening it will probably crash because the audio file is in the middle of a seek operation. I'd either wait until the getWaveformData function has completed, or create two instances of the EZAudioFile for that file and use one for generating the waveform and another for playback.\n. This has since been fixed in the 0.3.0 release where read/seek operations are wrapped in a mutex lock. The EZAudioPlayFileExample includes a continuous slider for the play position that now doesn't crash when reading/seeking quickly on different threads.\n. Interesting, I wonder if there is anything in unity that may be trying to do its own audio processing that's blocking the audio initialization. What EZAudio components are you using?\n. That's a good question, when I was originally writing the components I wanted to make sure the program would completely exit if I messed something up instead of gracefully fail. This should probably only be present for debug builds and stripped for releases.\n. See #56. \nIn short - as of the 0.1.0 release can just globally set setShouldExitOnCheckResultFail: to NO on the EZAudioUtilities class to prevent EZAudio from exiting the application.\nobjectivec\n[EZAudioUtilities setShouldExitOnCheckResultFail:NO];\n. Unfortunately, not yet. Because the framework works on a pull model that is not exactly node based (like how the Amazing Audio Engine has an Audio Unit Processing Graph) you'd have to perform your processing using circular buffers. Something like:\nInput:\nEZMicrophone --> writes to circular buffer -->\nProcessing\nreads from circular buffer <-- audio unit --> performs processing on samples and writes to processed circular buffer -->\nOutput\nreads from processed circular buffer --> EZOutput --> performs playback\n. @clementprem you'd add a render notification callback to the AUGraph via the AUGraphAddRenderNotify(...) function. Your callback implementation would look something like:\nobjectivec\nstatic OSStatus GraphRenderCallback(void*                       inRefCon,\n                                                AudioUnitRenderActionFlags* ioActionFlags,\n                                                const AudioTimeStamp*       inTimeStamp,\n                                                UInt32                      inBusNumber,\n                                                UInt32                      inNumberFrames,\n                                                AudioBufferList*            ioData)\n{\n    ViewController* controller = (__bridge ViewController*)inRefCon;\n    if (*ioActionFlags == kAudioUnitRenderAction_PostRender) {\n            dispatch_async(dispatch_get_main_queue(), ^{\n                float *buffer = (float*)ioData->mBuffers[0].mData; // assuming you're using a float AudioStreamBasicDescription\n                [controller.plot updateBuffer:buffer withBufferSize:inNumberFrames];\n        });\n    }\n    return noErr;\n}\n. For only the microphone component? If it's purely for a graphical display then you can adjust the rollingHistoryLength on the audio plot.\nobjectivec\n[self.audioPlot setRollingHistoryLength:128]\nFor the microphone it will default to the buffer size specified by the AVAudioSession, so before instantiating the microphone you could try adjusting the preferred IO buffer duration like:\nobjectivec\nNSError* err;\n[[AVAudioSession sharedInstance] setPreferredIOBufferDuration:(DURATION IN SECONDS) error:&err];\nReference:\nhttps://developer.apple.com/library/ios/documentation/avfoundation/reference/AVAudioSession_ClassReference/Reference/Reference.html#//apple_ref/occ/instm/AVAudioSession/setPreferredIOBufferDuration:error:\n. As of 1.1.0 you can just use the EZAudioFFTRolling to get an FFT with a larger window. Please check out the answer for #185 for a code example of how to store more than 512 audio samples using an EZPlotHistoryInfo structure. \nAlso, the EZAudioFFTExample has been updated to be a high resolution pitch detector using a 4096 length FFT.\n\n. I'm going to assume this has been resolved thanks to @JeanRintoul's solution\n. As of 0.1.0 the EZMicrophone now supports setting different EZAudioDevice instances that wrap the AVAudioSession API for iOS and the AudioObject API for OSX.\nSpecifically, you can first get an array of devices from the EZAudioDevice class via:\n``` objectivec\n//------------------------------------------------------------------------------\n// @name Getting The Devices\n//------------------------------------------------------------------------------\n/\n Enumerates all the available input devices and returns the result in an NSArray of EZAudioDevice instances.\n @return An NSArray containing EZAudioDevice instances, one for each available input device.\n /\n+ (NSArray )inputDevices;\n```\nand then just set one of them on the EZMicrophone instance using the following method:\nobjectivec\n/**\n Sets the EZAudioDevice being used to pull the microphone data.\n - On iOS this can be any of the available microphones on the iPhone/iPad devices (usually there are 3). Defaults to the first microphone found (bottom mic)\n - On OSX this can be any of the plugged in devices that Core Audio can detect (see kAudioUnitSubType_HALOutput for more information)\n System Preferences -> Sound for the available inputs)\n @param device An EZAudioDevice instance that should be used to fetch the microphone data.\n */\n- (void)setDevice:(EZAudioDevice *)device;\nCheck out the new EZAudioCoreGraphicsWaveformExample project where you can now select from any of the different microphones.\n\n. Don't know if this is still an issue with the new EZMicrophone 0.1.0 release. I'm going to close this ticket because I'm not able to reproduce.\n. The EZAudioFile in the 0.3.0 release should provide you with what you need. Specifically you can read in the audio samples using the readFrames:audioBufferList:bufferSize:eof: method. The getWaveformDataWithCompletionBlock has also been fixed (it had a bug before that would cause it to crash if called too many times).\n. As of 0.3.0 the AEFloatConverter has been removed\n. I'm going to close this guy because the EZAudioPlayer doesn't set the AVAudioSession anymore as of the 0.5.0 release. \n. Added as of 0.1.0\n. What's the use case of this? Typically you would perform gain adjustment on a secondary component. For instance EZOutput and EZAudioPlayer have setVolume: properties while the EZAudioPlot and EZAudioPlotGL have setGain: properties. \n. Xcode 6 and iOS 8 compile issues were fixed in the recent 0.0.6 cocoapod release. You can use that version or clone the repo.\n. Same as #102. It's up to you to setup the AVAudioSession as of the 0.5.0 release\n. Should be fixed as of 0.3.0\n. This should be fixed as of 0.3.0 (the AEFloatConverter has been removed)\n. Are you updating the EZAudioPlot on the main thread? Please update to 0.4.0 and let me know if this is still an issue (the EZAudioPlot is now thread-safe).\n. Going to close this for now, please re-open if you are still seeing the crashes \n. As of 0.7.1 the EZAudioPlayer supports setting the volume (and pan!) by setting the volume property like so:\nobjectivec\nfloat volume =  // a value between 0 and 1\n[self.player setVolume:volume];\n. There's no plan to do this just yet for the EZAudioPlot, but as of the 0.2.0 release it is much easier to subclass the EZAudioPlot to get a custom path. Try subclassing the EZAudioPlot and overriding the following method to provide a discrete CGPathRef that the waveform CALayer will use for its path property.\nobjectivec\n/**\n Main method that handles converting the points created from the `updatedBuffer:withBufferSize:` method into a CGPathRef to store in the `waveformLayer`. In this method you can create any path you'd like using the point array (for instance, maybe mapping the points to a circle instead of the standard 2D plane).\n @param points     An array of CGPoint structures, with the x values ranging from 0 - (pointCount - 1) and y values containing the last audio data's buffer.\n @param pointCount A UInt32 of the length of the point array.\n @param rect       An EZRect (CGRect on iOS or NSRect on OSX) that the path should be created relative to.\n @return A CGPathRef that is the path you'd like to store on the `waveformLayer` to visualize the audio data.\n */\n- (CGPathRef)createPathWithPoints:(CGPoint *)points\n                       pointCount:(UInt32)pointCount\n                           inRect:(EZRect)rect;\n. @lususvir, I'm [finally] working on a new version of EZAudioFile that provides support read/write support so the EZRecorder will be going away. I recommend following the audio_file_refactor branch!\n. The AEFloatConverter has been removed as of 0.3.0\n. Hmm for a stereo, float-based, interleaved stream format you'd want something like this:\nobjectivec\n    AudioStreamBasicDescription asbd;\n    UInt32 floatByteSize   = sizeof(float);\n    asbd.mChannelsPerFrame = 2;\n    asbd.mBitsPerChannel   = 8 * floatByteSize;\n    asbd.mBytesPerFrame    = asbd.mChannelsPerFrame * floatByteSize;\n    asbd.mFramesPerPacket  = 1;\n    asbd.mBytesPerPacket   = asbd.mFramesPerPacket * asbd.mBytesPerFrame;\n    asbd.mFormatFlags      = kAudioFormatFlagIsFloat;\n    asbd.mFormatID         = kAudioFormatLinearPCM;\n    asbd.mSampleRate       = 44100.0;\n    asbd.mReserved         = 0;\n    return asbd;\n. The EZOutput has been rewritten in the 0.4.0 release. Please try again and let me know if this is still an issue...going to close this for now.\n. The weird split line thing has been fixed as of the 0.2.0 release. \nFor the stretched out thing - the getWaveformData methods on the EZAudioFile provide a fixed amount of audio data for a plot. \nThis works like so:\n- A series of windows of samples are calculated, each the length of the total frames of the song divided by the number of points to calculate\n- The root mean squared average of each window is used as the point value.\nHence, the shorter the song the more it will look stretched out. If you need a sample accurate waveform that stretches check out FDWaveformView\n. This should be resolved in the 0.3.0 release\n. This is currently being worked on in the play_file_cleanup branch with the new EZAudioDevice for OSX and likely the AVAudioSession's input devices for iOS. It will look something like this: https://github.com/syedhali/EZAudio/blob/play_file_cleanup/EZAudio/EZMicrophone.m#L262 \n. You should be able to toggle whatever input device you need using the EZAudioDevice addition to the EZMicrophone\n. Hey @AlexEdunov these changes look great, but could remove the .gitignore file so the changes are only in the EZAudio.m file?\n. Thanks!\n. For the EZMicrophone you can access the binary data via the EZMicrophoneDelegate method microphone:hasBufferList:withBufferSize:withNumberOfChannels:.\nTo get the binary data from the AudioBufferList you'd just do something like:\n``` objectivec\n- (void)   microphone:(EZMicrophone )microphone\n        hasBufferList:(AudioBufferList )bufferList\n       withBufferSize:(UInt32)bufferSize\n withNumberOfChannels:(UInt32)numberOfChannels\n{\n    //\n    // Binary data, if interleaved then bufferList->mNumberBuffers = 1 and\n    // this is all the data\n    //\n    void *buffer = bufferList->mBuffers[0].mData;\n//\n// Binary data, if non-interleaved then each channel of data can be\n// found\n//\nfor (int i = 0; i < bufferList->mNumberBuffers; i++)\n{\n    void *channelBuffer = bufferList->mBuffers[i].mData;\n}\n\n}\n``\n. As of 0.1.0 you can just setsetShouldExitOnCheckResultFail:on the EZAudioUtilities class to prevent thecheckResult:operation:` method from quitting the application. \nobjectivec\n[EZAudioUtilities setShouldExistOnCheckResultFail:NO];\n. EZAudio doesn't support streaming from a remote URL. See #139 \n. This should be fixed as of the 0.7.1 release. Just tested it now, please let me know if you're still having problems after updating.\n. Look into the setRollingHistoryLength: function on the EZAudioPlot and EZAudioPlotGL. Each time updateBuffer:withBufferSize: is called it appends one sample to the history buffer so this just depends on your sample rate. \nAssuming you're using the EZMicrophone's microphone:hasAudioReceived:withBufferSize:withNumberOfChannels:\n44.1 kHz = 512 samples per callback = 86 times a second\nSo for 15 seconds of audio you need a rollingHistoryLength of 86 * 15 = 1290\n. EZAudio currently doesn't support streaming from a remote URL. I'd recommend the DOUAudioStreamer instead.\n. Is this iOS or Mac?\n. You must do initWithFrame:. Here's a code sample:\n``` objectivec\n// Declare in interface\n@property (strong) EZAudioPlotGL *plot;\n// Creating GL plot in code (assuming this is in a UIViewController)\nself.microphone = [EZMicrophone microphoneWithDelegate:self];\nself.plot = [[EZAudioPlotGL alloc] initWithFrame:self.view.bounds];\nself.plot.autoresizingMask = UIViewAutoresizingFlexibleHeight|UIViewAutoresizingFlexibleWidth;\n[self.view addSubview:self.plot];\n```\n. Fixed as of 0.3.0\n. See my answer to #148 \n. Please check out the answer for #185 for a code example of how to store more than 512 audio samples using an EZPlotHistoryInfo structure. \nAlso, the EZAudioFFTExample has been updated to be a high resolution pitch detector using a 4096 length FFT.\n.\n. Think you want to just adjust the gain property (try 3 or 4)\n. Hey guys, doing this kind of processing is out of the scope of EZAudio's core classes for now. In a future release I will be releasing a version of EZAudio that includes a graph-based way to chain audio nodes together that will make it much easier to just subclass a node to do this kind of analysis on whatever audio you have in your chain, but currently you'd have to do this kind of processing from the EZMicrophone delegate. \n. great work! @JamieCruwys \n. Thanks @JamieCruwys, I'm going to merge this into the current master branch. I'm actually in the process of refactoring the EZAudioFile and EZMicrophone on the play_file_cleanup branch (https://github.com/syedhali/EZAudio/tree/play_file_cleanup) so these memory fixes will be in the newer versions of these classes as well. \nCheers!\n. Hey guys, sorry I've been away. I'm slowly refactoring EZAudio on the play_file_cleanup branch, but I don't think any of that involves a fix for this. Though, it sounds like a mismatch between sample rates, can you verify the AudioStreamBasicDescription on the EZOutput and EZMicrophone is the same? Also, is this iOS or Mac?\n. Ok, is it happening on the actual device or simulator or both kinda equally?\n. I'm wondering if this was happening because the previous version of the EZAudioPlot before the 0.2.0 was pretty CPU intensive. Could you update your EZAudio version to 0.2.0 or later and try to reproduce this issue?\n. I can't seem to reproduce so I'm going to close this for now. Feel free to reopen if you update EZAudio to at least 0.7.1 and still experience the same issues.\n. Check out the play_file_cleanup branch's EZAudioWaveformFromFileExample project. You can take a snapshot of an EZAudioPlot like so:\nobjectivec\n// kSnapshotFileDefault is a NSString to where you want to write the file\n- (void)snapshot:(id)sender\n{\n    NSBitmapImageRep* imageRep = [self.audioPlot bitmapImageRepForCachingDisplayInRect:self.audioPlot.bounds];\n    [self.audioPlot cacheDisplayInRect:self.audioPlot.bounds toBitmapImageRep:imageRep];\n    NSData* data = [imageRep representationUsingType:NSPNGFileType properties:nil];\n    [data writeToFile:kSnapshotFileDefault atomically:NO];\n}\n. Over the last week the new_examples branch has been broken up into smaller PRs and since has become deprecated. Please use the 0.4.0 release or current master branch. \n. As of 0.3.0 AudioUnitSampleType and AudioSampleType have just been replaced with floats and the AudioSession API has been replaced by using the AVAudioSession API instead so there aren't any of these warnings anymore!\n. As of 0.1.0 this was cleaned up\n. Swift support will begin after I publish the play_file_cleanup branch. That branch has updates from the last few months of work that I've been trying to merge into master but haven't gotten a chance to until the last few weeks (no freelance, yay!). \n. Hey everyone, I created a separate repo (EZAudio-Swift) to demonstrate how to use EZAudio in a swift project using a bridging header. It shows how to use an EZMicrophone + EZAudioPlotGL to get something similar to the EZAudioOpenGLWaveformExample:\n\nAnyone feel strongly that there should be at least one Swift example included in the official EZAudio repo?\n. Not too sure about this one, is the motion acceleration using the microphones for anything?\n. Are you drawing multiple plots?\n. Going to assume you're drawing multiple plots. If you still are having this issue use the EZAudioPlot instead. Since the 0.2.0 release the EZAudioPlot has had a dramatic increase in performance.\n. Quickly looking at the error (-66563) looks like a kExtAudioFileError_NonPCMClientFormat. You can only set a linear PCM format as the EZAudioFile's client format. I think you may be trying to set the file format instead, however, the only supported file formats are m4a, wav, and aiff. \nSee https://github.com/syedhali/EZAudio/blob/master/EZAudio/EZRecorder.h#L37-L51\n``` objectivec\n/**\n To ensure valid recording formats are used when recording to a file the EZRecorderFileType describes the most common file types that a file can be encoded in. Each of these types can be used to output recordings as such:\nEZRecorderFileTypeAIFF - .aif, .aiff, .aifc, .aac\n EZRecorderFileTypeM4A  - .m4a, .mp4\n EZRecorderFileTypeWAV  - .wav\n/\ntypedef NS_ENUM(NSInteger, EZRecorderFileType)\n{\n    /\n     Recording format that describes AIFF file types. These are uncompressed, LPCM files that are completely lossless, but are large in file size.\n     */\n    EZRecorderFileTypeAIFF,\n    /\n     Recording format that describes M4A file types. These are compressed, but yield great results especially when file size is an issue.\n     /\n    EZRecorderFileTypeM4A,\n    /*\n     Recording format that describes WAV file types. These are uncompressed, LPCM files that are completely lossless, but are large in file size.\n     /\n    EZRecorderFileTypeWAV\n};\n``\n. @dddx80 Like @gotuckgo said, make sure you've first created your EZMicrophone instance or elseself.microphone.audioStreamBasicDescriptionwill be invalid. Please update to EZAudio 0.7.2 and let me know if this is still an issue (going to close this for now)\n. Fixed as of 0.7.1!\n. This is a duplicate on #38. EZAudio doesn't provide any mechanism to actually perform BPM or Pitch detection, but you could consider one of the following libraries:\n1.) [aubio](http://aubio.org/) - probably one of the best, free for non-commercial\n2.) [SoundTouch](http://www.surina.net/soundtouch/) - free for non-commercial\n3.) [BeatDetektor](https://github.com/cjcliffe/beatdetektor) - haven't used, but is MIT so free for commercial + non-commercial use\n. So I just released 0.7.1 that allows EZAudio and the Amazing Audio Engine to work together by providing a subspec for EZAudio. Just change your Podfile to use theEZAudio/Core` subspec like so:\n```\nplatform :ios, '8.0'\ntarget 'EZAudioAmazingAudio' do\npod 'EZAudio/Core', '0.7.1'\npod 'TheAmazingAudioEngine', '~> 1.4'\nend\n```\n. What iOS device are you running it on?\n. Ahhhh I see, the reason it is crashing seems to be because sometimes it is unable to capture the audio coming in the microphone. Specifically, I forgot to set the audio session on that project...heh.\nTry adding this to the top of the iOS FFT example in viewDidLoad before creating the EZMicrophone:\nobjectivec\n//\n// Setup the AVAudioSession. EZMicrophone will not work properly on iOS\n// if you don't do this!\n//\nAVAudioSession *session = [AVAudioSession sharedInstance];\nNSError *error;\n[session setCategory:AVAudioSessionCategoryPlayAndRecord error:&error];\nif (error)\n{\n    NSLog(@\"Error setting up audio session category: %@\", error.localizedDescription);\n}\n[session setActive:YES error:&error];\nif (error)\n{\n    NSLog(@\"Error setting up audio session active: %@\", error.localizedDescription);\n}\n. Hey @bennyguitar, please try the 0.4.0 release. EZAudio has been under active development over the last week and the EZOutput was specifically rewritten in 0.4.0 to fix a lot of these kind of issues.\n. Are you using the EZAudioPlayFileExample?  I just realized I removed the override speaker setting on the AVAudioSession...the viewDidLoad method should actually be:\n``` objectivec\n- (void)viewDidLoad\n{\n    [super viewDidLoad];\n//\n// Setup the AVAudioSession. EZMicrophone will not work properly on iOS\n// if you don't do this!\n//\nAVAudioSession *session = [AVAudioSession sharedInstance];\nNSError *error;\n[session setCategory:AVAudioSessionCategoryPlayback error:&error];\nif (error)\n{\n    NSLog(@\"Error setting up audio session category: %@\", error.localizedDescription);\n}\n[session setActive:YES error:&error];\nif (error)\n{\n    NSLog(@\"Error setting up audio session active: %@\", error.localizedDescription);\n}\n\n//\n// Customize the plot's look\n//\n// Background color\nself.audioPlot.backgroundColor = [UIColor colorWithRed: 0.816 green: 0.349 blue: 0.255 alpha: 1];\n// Waveform color\nself.audioPlot.color           = [UIColor colorWithRed:1.0 green:1.0 blue:1.0 alpha:1.0];\n// Plot type\nself.audioPlot.plotType        = EZPlotTypeBuffer;\n// Fill\nself.audioPlot.shouldFill      = YES;\n// Mirror\nself.audioPlot.shouldMirror    = YES;\n\n//\n// Create an EZOutput instance\n//\nself.output = [EZOutput outputWithDataSource:self];\nself.output.delegate = self;\n\n//\n// This will tell the AVAudioSession to output to the speakers\n// \n[audioSession overrideOutputAudioPort:AVAudioSessionPortOverrideSpeaker error:&error];\nif (error)\n{\n    NSLog(@\"There was an error sending the audio to the speakers\");\n}\n\n//\n// Customize UI controls\n//\nself.volumeSlider.value = [self.output volume];\nself.rollingHistorySlider.value = [self.audioPlot rollingHistoryLength];\n\n//\n// Try opening the sample file\n//\n[self openFileWithFilePathURL:[NSURL fileURLWithPath:kAudioFileDefault]];\n\n}\n```\n. Specifically, make sure you override the speaker setting after the EZOutput is created:\n``` objectivec\n    //\n    // Create an EZOutput instance\n    //\n    self.output = [EZOutput outputWithDataSource:self];\n    self.output.delegate = self;\n//\n// This will tell the AVAudioSession to output to the speakers\n// \n[audioSession overrideOutputAudioPort:AVAudioSessionPortOverrideSpeaker error:&error];\nif (error)\n{\n    NSLog(@\"There was an error sending the audio to the speakers\");\n}\n\n``\n. Yay, glad it worked for you!\n. Ah that makes sense. Could you submit a PR? If not, just let me know and I'll make the tweak in the next release.\n. This has been resolved as of the 0.7.2 release. Needed to callDisposeAUGraph`\n. Hey @hakanw, looks good! If you make those small tweaks above I'll merge this into master :)\n. Nice work!\n. Will get back to you on this\n. Hey @antonm76 if you make the tweak above I'll merge into master for the next release.\n. Hey @jamrader, just added the EZAudioFFTRolling in the 1.1.0 release. In the implementation you'll see how to use a EZPlotHistoryInfo structure to collect a larger number of samples to use for getting a much higher resolution FFT. \nSpecifically,\n``` objectivec\n// Use the EZPlotHistoryInfo structure to hold the audio history data while also having access to a current window of audio data\n@property (assign, nonatomic) EZPlotHistoryInfo *historyInfo;\n// Allocate an appropriately sized history buffer in bytes\nself.historyInfo = [EZAudioUtilities historyInfoWithDefaultLength:(UInt32)windowSize\n                                                    maximumLength:(UInt32)historyBufferSize];\n// Append buffer to history window\n[EZAudioUtilities appendBuffer:buffer\n                withBufferSize:bufferSize\n                 toHistoryInfo:self.historyInfo];\n```\nSpecifically, for what you're trying to do check out the new EZAudioFFTExample, which is now just a pitch detector that uses the EZAudioFFTRolling to get the frequency with the highest energy and converts it to a musical note:\n\n. Whoops, yeah you need the AVAudioSession stuff. Just updated 1.1.2 to have that fix.\n. There's a PR for this #180 . Going to merge it in now and bump the version to 1.0.1\n. Fixed now, check out the 1.0.1 release\n. Hi @scamhaji, the EZOutput has two stream formats: the inputFormat for the audio coming into the component and the clientFormat for the audio leaving the component (which is the mixer's format). When the EZMicrophone sets the EZOutput on itself (code snippet here) it sets the inputFormat of the EZOutput, but not the clientFormat, which defaults to the EZOutput's stereo, float format.\nSo you should just set the clientFormat for the EZOutput to the EZMicrophone's stream format to get what you want:\nobjectivec\nself.output.clientFormat = self.microphone.audioStreamBasicDescription;\n. Hm interesting, looking into this now. I probably shouldn't be setting the input scope of the mixer audio unit\n. Add Accelerate to the list of dependencies! (P.s. you know you're going crazy when you start leaving code reviews for yourself)\n. Update FFT examples to use EZAudioFFTRolling\n. @dedjw Thanks! I literally just ripped my headphones off from horrible distortion caused by my incorrect buffer size calculation and malloc'ing instead of calloc'ing.\n. Sweet, really appreciate this @megastep. I'll take a quick look and get these changes in later today!\n. Hm, are you sure you're using EZAudio 1.0.0 or greater? 66570 corresponds to an async write error (kExtAudioFileError_AsyncWriteBufferOverflow), which can't really happen because the write has been moved to be synchronous since 1.0.0.\n. Well, try it :) If you're still having this issue just let me know.\n. What is the error? Also, make sure you close the audio file before reading it using the closeAudioFile method\n. Nice!\n. Hey guys, to be honest I'm not too hip on Carthage at the moment. I saw #211, but doesn't it also need a target for the OSX framework as well? \n. Looks good! Just have one comment above\n. Thank you @AndrewSB @blender! \n. Hi @picciano, are you setting the AVAudioSession? \n. Have you set the AVAudioSession?\n. Hmm, I'll look into this. I'm not sure how the iPod library urls were working at all in the previous version, but I'll see if the old code is still working and make some tweaks.\n. Whoops, sorry about that - just pushed 1.1.2 to the pods repo. Should be there in ~15 minutes\n. Done!\n. Hm interesting, sorry to hear it's crashing. I just updated all my devices to iOS 9 today so I'll test out the examples tonight and see if I can reproduce. \n. Hey guys seems like the EZAudioFloatConverter is bugging out for buffer sizes that aren't 512 or 1024. I'll need to look into that component and see what I'm doing wrong :/\n. Hey everyone, I was able to reproduce this issue by setting the preferred sample rate to 48000 (default on iPhone 6s + iPhone 6s+) and found out the issue was that the scratch AudioBufferList structures' mDataByteSize property used in the EZMicrophone and the EZAudioFloatConverter was not being set property.\nThis issue is fixed as of the 1.1.4 release. I'm closing this for now, but let me know if this is still an issue after updating.\n. This should be fixed\n. @nicole1314 when the buffer is 0, is it NULL or does the buffer actually exist in memory? Seems like the buffer might be getting deallocated before entering this function. If you're using one of the EZAudio delegate functions and using anything besides the audioReceived float buffers provided then there's a chance those Core Audio provided buffers are being deallocated before the RMS calculation is performed. The EZMicrophone copies the audio data from the Audio Unit callback into the float buffers, which are not freed until the whole EZMicrophone instance is deallocated.\n. Hi @Boggartfly, good catch. I just submitted a PR to the TPCircularBuffer repo with a fix, but for now I've just released a new version of EZAudio (1.1.4) that addresses this issue by using TPCircularBuffer 1.1\n. What version of Xcode and cocoapods are you running?\n. Can you just include it like:\n```\nimport \n```\n. Hey everyone, please try again with 1.1.5\npod 'EZAudio', '1.1.5'\n. good catch!\n. For the EZRecorder make sure you call the close function because trying to read it back: https://github.com/syedhali/EZAudio/blob/master/EZAudio/EZRecorder.h#L362\n. Thanks @JanX2! \n. Could you document this like the other delegate methods?\n. Move brace to a new line\n. You can just do !buffers to check for a NULL value\n. Just one tweak, could you set the iOS deployment target to 6.0? Is there anything that would require the minimum to be 8.0?\n. Ah that's right!\n. ",
    "toytonics": "Thanks for your response.\nI found some code using the vDSP API (Accelerate/Accelerate.h) by Apple and wrapped it inside a method.\n``` objc\n-(void)DetectFrequencyWithBuffer:(float *)buffer\n             withBufferSize:(UInt32)bufferSize{\nfloat *samples = buffer; // This is filled with samples, loaded from a file\nint numSamples = bufferSize;  // The number of samples\n\n// Setup the length\nvDSP_Length log2n = log2f(numSamples);\n\n// Calculate the weights array. This is a one-off operation.\nFFTSetup fftSetup = vDSP_create_fftsetup(log2n, FFT_RADIX2);\n\n// For an FFT, numSamples must be a power of 2, i.e. is always even\nint nOver2 = numSamples/2;\n\n// Populate *window with the values for a hamming window function\nfloat *window = (float *)malloc(sizeof(float) * numSamples);\nvDSP_hamm_window(window, numSamples, 0);\n// Window the samples\nvDSP_vmul(samples, 1, window, 1, samples, 1, numSamples);\n\n// Define complex buffer\nCOMPLEX_SPLIT A;\nA.realp = (float *) malloc(nOver2*sizeof(float));\nA.imagp = (float *) malloc(nOver2*sizeof(float));\n\n// Pack samples:\n// C(re) -> A[n], C(im) -> A[n+1]\nvDSP_ctoz((COMPLEX*)samples, 2, &A, 1, numSamples/2);\n\n//Perform a forward FFT using fftSetup and A\n//Results are returned in A\nvDSP_fft_zrip(fftSetup, &A, 1, log2n, FFT_FORWARD);\n\n//Convert COMPLEX_SPLIT A result to magnitudes\nfloat amp[numSamples];\namp[0] = A.realp[0]/(numSamples*2);\nfor(int i=1; i<numSamples; i++) {\n    amp[i]=A.realp[i]*A.realp[i]+A.imagp[i]*A.imagp[i];\n    printf(\"%f \",amp[i]);\n}\n\n}\n```\nBut I'ts not working properly yet.\nMaybe you could reuse some of this code- I hope.\n. buffer[0] and buffer[1] are both arrays representing the left and right channel data.\nConsequently both arrays are holding a set of values you can work with.\nSo try to find the highest value within buffer[0].\n. ",
    "paulfavier": "Thx for the fast reply :)\n. ",
    "johnnyclem": "Nice. I found the same and was about to send you a pull request, but you beat me to it!\nThanks\n-John\u00a0\n\u2014\nSent from Mailbox for iPhone\nOn Tue, Dec 31, 2013 at 11:36 AM, Syed Haris Ali notifications@github.com\nwrote:\n\nHello! I really appreciate your enthusiasm and definitely encourage pull requests as you move through the source to help make this thing as awesome as possible :)\nI just looked into this issue and realized I accidentally hard coded the sample rate for the client format on the audio file to be 44.1 kHz. I just pushed a fix to the master branch that pulls the sample rate from the file's format in the EZAudioFile.m: \nobjectivec\n  // Set the client format on the stream\n  _clientFormat.mBitsPerChannel   = 8 * sizeof(AudioUnitSampleType);\n  _clientFormat.mBytesPerFrame    = sizeof(AudioUnitSampleType);\n  _clientFormat.mBytesPerPacket   = sizeof(AudioUnitSampleType);\n  _clientFormat.mChannelsPerFrame = 1;\n  _clientFormat.mFormatFlags      = kAudioFormatFlagsCanonical | kAudioFormatFlagIsNonInterleaved;\n  _clientFormat.mFormatID         = kAudioFormatLinearPCM;\n  _clientFormat.mFramesPerPacket  = 1;\n  _clientFormat.mSampleRate       = _fileFormat.mSampleRate; // Pulls from file format instead of hardcoded 44100\nI tested it with a 48 kHZ, AIFF file before the change and experienced the same problem and tested it with this modification and it's working fine. The change is live now on the master branch if you would like to give it a test. If it all checks out I'll update the Cocoapod.\nReply to this email directly or view it on GitHub:\nhttps://github.com/syedhali/EZAudio/issues/3#issuecomment-31408171\n. I have to tread lightly around the C++ bits, but I'll take a crack at it\u2014\nSent from Mailbox for iPhone\n\nOn Tue, Dec 31, 2013 at 11:57 AM, Syed Haris Ali notifications@github.com\nwrote:\n\nProbably also the root cause of the Hash table corrupted issue.\nReply to this email directly or view it on GitHub:\nhttps://github.com/syedhali/EZAudio/issues/4#issuecomment-31408949\n. \n",
    "wdwen": "had changed and pulled\n. sorry i careless,ths\n. ",
    "AndrexOfficial": "Yes, one of the EZAudio examples\n. ",
    "MattFoley": "Hey, sounds like I came across the exact same bug today.\nI'm on an iPhone 5 with 7.0.4. It crashes into assembler code with the tag \"CrashIfClientProvidedBogusAudioBufferList\", somewhere inside AudioToolbox with an EXC_BAD_ACCESS. This is on the stock \"EZAudioPlayFileExample\", no changes. Since it's a bad access, there's no console output to give you, I will give you as much information as I have though, because I'd really like to use your library.\nIt seems to be a different amount of time into playing the track, but usually within 2-4 seconds it crashes. I thought maybe the audio file itself was corrupted, but swapping in my own MP3 file has the same result. I tried this on both the \"Buffer\" and \"Rolling\" plot types from the switch in the sample app. \nThis crash doesn't happen in the Simulator either as far as I can tell.\nHeading on the assembler file that the crash is in:\nAudioToolbox`CrashIfClientProvidedBogusAudioBufferList:\n. This might even give you a better clue. I've been working for the past little while to combine the above mentioned project with your recording demo, in order to create a Hold to Record, and then Playback with Waveform sort of demo control. \nDoing it in that way, recording with EZAudioMicrophone/EZAudioRecorder and then playing back with EZOutput/EZAudioFile, the crash doesn't seem to be happening. A lot of this sound manipulation stuff is new to me, but I wish I could be more help!\nTj-\n. Awesome! \nIf you'd like, I could add this Hold to Record and Playback project that I've built as a sort of advanced use example of your library in a pull request. Just let me know what you'd like the project to be named if you'd like me to do that. :)\n. Sure thing. At my day job right now, but I will get something to you tonight hopefully! Thanks for all the hard work on the library!\n. I'm also seeing the audio pops. They seem to be happening during the looping phase.\n. ",
    "icanswiftabit": "Thanks for response :+1: indeed that will be helpful. So I'm looking for 0.0.3. \n. ",
    "vfxdrummer": "Hey Syed -\nFirst of all, amazing framework! This is a bit of a generic comment, but it seems that whenever I try to override the ASBD on the microphone I get errors like :\nError: Failed to fill complex buffer in float converter ('insz')\nHere are example settings :\n  audioStreamBasicDescription.mFormatID          = kAudioFormatLinearPCM;\n  audioStreamBasicDescription.mSampleRate        = 16000;\n  audioStreamBasicDescription.mFramesPerPacket   = 1;\n  audioStreamBasicDescription.mBytesPerPacket    = 2; //16 bits * 1 channel\n  audioStreamBasicDescription.mBytesPerFrame     = 2;\n  audioStreamBasicDescription.mChannelsPerFrame  = 1; //1 channel\n  audioStreamBasicDescription.mBitsPerChannel    = 16;\n  audioStreamBasicDescription.mFormatFlags       = kAudioFormatFlagIsSignedInteger |\n  kAudioFormatFlagIsBigEndian | kAudioFormatFlagIsPacked;\n  audioStreamBasicDescription.mReserved          = 0;\nBasically, I am trying to make the stream 16 bits / 16 kHz so that I can upload it to  Nuance's speech to text dictation.\nAny thoughts on this? Thanks again!\n. I also tried creating my ASBD using \n audioStreamBasicDescription = [EZAudioUtilities monoFloatFormatWithSampleRate:16000];\nBut, that produces errors, as well.\nI am only using EZMicrophone. I need 16 bit / 16 kHz pcm audio data in order to POST it to the Nuance speech-To-Text dictation API. We were saving a wav file and uploading it which works, but has a lag.\nShould I just convert the 32-bit / 44.1kHz pcm data that I get from the AudoBufferList in the microphone delegate? Any thoughts or pointers?\nThanks again for this great framework!\n-Tim\n. Hey - \nQuick Update after scouring the interwebs. I got it to work. This was the missing piece :\naudioStreamBasicDescription.mFormatFlags = kAudioFormatFlagIsSignedInteger | kAudioFormatFlagsNativeEndian | kAudioFormatFlagIsPacked;\nAfter looking at this code snippet from 'The Amazing Audio Engine' :\n//  The Amazing Audio Engine\n//  .mFormatID = kAudioFormatLinearPCM,\n//  .mFormatFlags = (sampleType == AEAudioStreamBasicDescriptionSampleTypeFloat32\n//                   ? kAudioFormatFlagIsFloat : kAudioFormatFlagIsSignedInteger | kAudioFormatFlagsNativeEndian)\n//  | kAudioFormatFlagIsPacked\n//  | (interleaved ? 0 : kAudioFormatFlagIsNonInterleaved),\n//  .mChannelsPerFrame  = numberOfChannels,\n//  .mBytesPerPacket    = sampleSize * (interleaved ? numberOfChannels : 1),\n//  .mFramesPerPacket   = 1,\n//  .mBytesPerFrame     = sampleSize * (interleaved ? numberOfChannels : 1),\n//  .mBitsPerChannel    = 8 * sampleSize,\n//  .mSampleRate        = sampleRate,\nI realized that 16 bit data is SInt16 and needs these format flags :\naudioStreamBasicDescription.mFormatFlags = kAudioFormatFlagIsSignedInteger | kAudioFormatFlagsNativeEndian | kAudioFormatFlagIsPacked;\nAnd, I retrieve it from the buffer like so :\n  AudioBuffer audioBuffer = bufferList->mBuffers[0];\nSInt16 frame = (SInt16)audioBuffer.mData;\n[self.audioData appendBytes:frame length:audioBuffer.mDataByteSize];\nWorks for me - thanks again for the great framework!\n. ",
    "jbouaziz": "I'll look into that, thanks!\n. Are you planning on integrating it in EZAudio 1.0?\n. I need that same functionality, I'd be interested in knowing how to do that too. Did you find a safe way to reset EZRecorder yet? I also need to reset EZAudioPlot.\n. +1\n. ",
    "mayank4ios": "I couldn't find the scrolling behavior in EZAudio.. Can you please help me?\n. Ok... Thanks.. :)\n. Thanks syedhali.. I was not closing audio file. Now its working fine..!!!\nThanks....!!!!!!\n. ",
    "mnearents": "It doesn't scroll, it just removes points from the beginning and adds them to the end so it appears to be scrolling while recordin . But when you're done recording you can't scroll back through it. \n. Andy, that functionality would be useful for my project, has that code been pushed yet?\n. Cool, thanks for the info. Just curious, have you solved any other issues\nlike the ability to stop recording then start again and append the audio to\nthe current file rather than overwriting, or showing current time/total\ntime when recording?\nThanks,\nMatt\nOn Dec 5, 2014 9:28 AM, \"Andy Korth\" notifications@github.com wrote:\n\n@lususvir https://github.com/lususvir My code has diverged quite a bit\nfrom the current EZAudio code, and it might be too big of a structural\nchange to the project to do things the way I do them. I've also added\nscrolling and zooming within a waveform, etc. I also dropped Mac support...\nso that's not too good. There are some oddities regarding differences\nbetween views on iOS and Mac, so that simplified things for me a bit.\nStrictly speaking, I think the changes should all be possible for both\nplatforms, but I don't have them. I would suggest starting from this pull\nrequest:\n44 https://github.com/syedhali/EZAudio/pull/44\n(The removal of push/pop became a requirement in the current version of\nxcode, which is probably why the commit can't be auto-merged anymore, but\nthe other issues that were fixed weren't addressed yet, I think)\nThe changes I made in order to be able to draw more than one plot (one gl\ncontext per plot) were very straightforward:\nCall: [EAGLContext setCurrentContext: self.context];\nas the first line in: -(void)glkView:(GLKView )view\ndrawInRect:(CGRect)rect\nand as the first line in: -(void)updateBuffer:(float )buffer\nwithBufferSize:(UInt32)bufferSize {\nFor performance reasons, I had to make some redrawing changes: Change\n@interface https://github.com/interface EZAudioPlotGLKViewController :\nGLKViewController\nto\n@interface https://github.com/interface EZAudioPlotGLKViewController :\nUIViewController\nAdd to EZAudioPlotGLKViewController.m\n-(void)loadView\n{\n    self.view = [[GLKView alloc] init];\n}\nThen be sure to set [self.view setNeedsDisplay]; when gl drawing occurs\nin that class. And set the view's delegate to the view controller when you\nsetup the view's context.\nIf you want to use a shared context for all your different EZAudio plots,\nuse something like this in viewDidLoad in EZAudioPlotGLKViewController.\n-(void)viewDidLoad {\n  [super viewDidLoad];\nstatic EAGLContext* sharedContext = nil;\nif(sharedContext == nil){\n    sharedContext = [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];\n}\n// Setup the context\nself.context =  sharedContext;// [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];\n    //[EAGLContext setCurrentContext: self.context];\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/syedhali/EZAudio/issues/20#issuecomment-65824144.\n. Does this branch allow scrolling of the waveform? How is the scrolling done, can you swipe it with your finger? Does the audioplot grow to allow scrolling, or are you just able to move the waveform within the audio plot? What change made this possible and how are you dealing with scrolling in your ViewController?\n. +1, would be awesome\n. I was able to get this by subclassing the EZRecorder and making the ivars declared in EZRecorder.m public instead of private.\n\nSo my custom recorder no longer shows this in the .m file:\nobjective-c\n@interface EZRecorder (){\n    ExtAudioFileRef             _destinationFile;\n    AudioFileTypeID             _destinationFileTypeID;\n    CFURLRef                    _destinationFileURL;\n    AudioStreamBasicDescription _destinationFormat;\n    AudioStreamBasicDescription _sourceFormat;\n}\nIn my .h file I did this:\nobjective-c\n@interface AwesomeRecorder : NSObject\n@property (nonatomic) ExtAudioFileRef             destinationFile;\n@property (nonatomic) AudioFileTypeID             destinationFileTypeID;\n@property (nonatomic) CFURLRef                    destinationFileURL;\n@property (nonatomic) AudioStreamBasicDescription destinationFormat;\n@property (nonatomic) AudioStreamBasicDescription sourceFormat;\nThen in your microphone callback, you can use the sourceFormat to the the sampleRate. Once you have the sampleRate, it's just number of frames / sampleRate\n``` objective-c\n-(void)microphone:(EZMicrophone )microphone\n hasAudioReceived:(float *)buffer\n   withBufferSize:(UInt32)bufferSize\nwithNumberOfChannels:(UInt32)numberOfChannels {\ndispatch_async(dispatch_get_main_queue(),^{\n\n    [[self getActiveAudioPlot] updateBuffer:buffer[0] withBufferSize:bufferSize];\n\n\n    self.numFrames = self.numFrames + 1;\n    if(self.numFrames >= self.totalFrames) {\n        self.totalFrames = self.numFrames;\n    }\n\n//this is where you can calculate the current recording time:\n        self.currentTime = (self.numFrames / (float)self.recorder.sourceFormat.mSampleRate) * 1000;\n    self.totalTime = (self.totalFrames / (float)self.recorder.sourceFormat.mSampleRate) * 1000;\n\n//convert the time to 00:00 format to display it in a UILabel\n       self.timerSeekerPositionLabel.text = [self convertTime:self.currentTime];\n       self.timerTotalAudioLengthLabel.text = [self convertTime:self.totalTime];\n});\n\n}\n```\n. Castles, would you mind sharing how you did it? How is the performance? When you say \"it took a while to piece together\" do you mean performance-wise it is slow? Or do you mean it took you a long time to figure out how to do it?\n. Did you ever make any progress on this? I was also trying to emulate the Voice Memos functionality.\n. I tried adding length to the audio plot while recording, that doesn't work. The plot will grow but the waveform doesn't adjust, it just gets distorted.\nI also tried putting the plot in a scrollview. I tried making the plot really long, and then revealing more of it over time as I record. The scrolling works, but for some reason the waveform stops showing up when you make the plot too wide in IB. \nWould be cool to calculate how much audio data fits on the screen at once, then when looping through the audio data, specify the start and end index based on the scroll position. Then you don't have to loop through the entire buffer, which causes performance issues.\nWould be even cooler if I could specify the number of seconds for a screen-width of waveform data. So if I wanted one screen width to represent 5 seconds I could set that. Then as I record, it would use the start/end index to only render what is visible on the screen.\n. I am able to generate a zoomed-in waveform with horizontal scrolling by creating a buffer array that only contains the audio represented on screen. I created a gist. The code is messy, I'm hoping to improve this eventually. If any of you find ways to improve this, please share:\nhttps://gist.github.com/lususvir/42dc604c4bc870ec00cceb9e21f4ca56. This is a core graphics waveform generator that I was able to get working with EZAudio's recorder. I ended up not using it, but here it is if it helps you:\nhttps://gist.github.com/emajcher/a2c7a696ae15ae326d50\n. I would like to be able to do this as well, Like the Voice Memo iOS app, which lets you append audio at any point in the audio file.\n. I found a workaround for this: set a flag that tells you whether it is your first or second time recording. If first time, just do what you've already done. If second, initialize the ezrecorder with a new temporary file (\"testfile2.m4a\"). Then once the user pauses recording, append the two files together and delete the temporary file. Export the appended audio to \"testfile.m4a\" to overwrite it. Here's the code I'm using to append (if anyone finds a better way, please let me know):\n```\n- (void)appendAudioFiles\n{\nNSURL originalURL = [self originalFilePathURL];\nNSURL newURL = [self appendFilePathURL];\n// Create a new audio track we can append to\nAVMutableComposition composition = [AVMutableComposition composition];\nAVMutableCompositionTrack appendedAudioTrack =[composition addMutableTrackWithMediaType:AVMediaTypeAudio preferredTrackID:kCMPersistentTrackID_Invalid];\nAVURLAsset originalAsset = [[AVURLAsset alloc] initWithURL:originalURL options:nil];\nAVURLAsset newAsset = [[AVURLAsset alloc] initWithURL:newURL options:nil];\nNSError* error = nil;\n// Grab the first audio track and insert it into our appendedAudioTrack\n//    AVAssetTrack *originalTrack = [[originalAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];\nCMTimeRange timeRange1 = CMTimeRangeMake(kCMTimeZero, originalAsset.duration);\n[appendedAudioTrack insertTimeRange:timeRange1\n                            ofTrack:[[originalAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0]\n                             atTime:kCMTimeZero\n                              error:&error];\nif (error)\n{\n    // do something\n    return;\n}\n// Grab the second audio track and insert it at the end of the first one\n//    AVAssetTrack *newTrack = [[newAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0];\nCMTimeRange timeRange = CMTimeRangeMake(kCMTimeZero, newAsset.duration);\n[appendedAudioTrack insertTimeRange:timeRange\n                            ofTrack:[[newAsset tracksWithMediaType:AVMediaTypeAudio] objectAtIndex:0]\n                             atTime:originalAsset.duration\n                              error:&error];\nif (error)\n{\n    // do something\n    return;\n}\n// Create a new audio file using the appendedAudioTrack\nAVAssetExportSession* exportSession = [AVAssetExportSession\n                                       exportSessionWithAsset:composition\n                                       presetName:AVAssetExportPresetAppleM4A];\nif (!exportSession)\n{\n    // do something\n    return;\n}\n[self deleteOriginalFile]; //a function I wrote to delete the original\n[self deleteTempFile]; // a function I wrote to delete the temp\nexportSession.outputURL = [self originalFilePathURL]; //recreate the original as the new appended file\nexportSession.outputFileType = AVFileTypeAppleM4A;\n[exportSession exportAsynchronouslyWithCompletionHandler:^{\n// exported successfully?\nswitch (exportSession.status)\n{\n    case AVAssetExportSessionStatusFailed:\n        NSLog(@\"Export failed\");\n        break;\n    case AVAssetExportSessionStatusCompleted:\n        NSLog(@\"Export completed\");\n        break;\n    case AVAssetExportSessionStatusWaiting:\n        break;\n    default:\n        break;\n}\n\n}];\n}\n```\n. Although I just read this in one of the closed issues https://github.com/syedhali/EZAudio/issues/2\n\"If you're using the same instance of the EZRecorder for both inputs then it will just continue to add the audio data from each to the tail of the existing file...\"\nTo me this is saying that it shouldn't be overwriting, it should append to the tail. Was this functionality changed? I noticed in the EZRecorder file there is a flag called: kAudioFileFlags_EraseFile in the EZRecorder init function. Is this causing the recorder to erase the file and overwrite rather than append to the tail? Or am I initializing the recorder incorrectly in my app (initializing with testfile.m4a each time the record button is pressed)?\n. I've noticed that the appending process takes a long time. If you were making a recording app, your users would notice that it takes your app 5 to 10 seconds to append two files together, whereas Voice Memos does it instantaneously. \nHas anyone figured out how to simply record to the same file multiple times and just append audio at the end?\n. Finally found a better way to do this. You should initialize your EZRecorder in viewDidLoad: when you press your record/pause toggle, it should really only set your flag isRecording to YES or NO. Then just surround everything in your EZMicrophone delegate with an if statement to see if you are recording:\nI downloaded the EZRecorder example project and changed the following:\n```\n-(void)toggleMicrophone:(id)sender {\n   //I erased everything in here, I control the microphone with my record/pause button.\n}\n// In this method I never close the audio file, I only change the isRecording flag from YES to NO.\n// I also start fetching audio the first time record is pressed, and never stop until the user exits the recorder.\n-(void)toggleRecording:(id)sender {\nself.playingTextField.text = @\"Not Playing\";\nif( self.audioPlayer )\n{\n    if( self.audioPlayer.playing )\n    {\n        [self.audioPlayer stop];\n    }\n    self.audioPlayer = nil;\n}\nself.playButton.hidden = NO;\nif( [sender isOn] )\n{\n//start the recorder\n    self.isRecording = YES;\n    [self.microphone startFetchingAudio];\n}\nelse\n{\n\n//don't stop the recorder\n        self.isRecording = NO;\n}\nself.isRecording = (BOOL)[sender isOn];\nself.recordingTextField.text = self.isRecording ? @\"Recording\" : @\"Not Recording\";\n\n}\n```\nHere is where you pause/unpause the actual recording and waveform drawing:\n```\n-(void)microphone:(EZMicrophone )microphone\n hasAudioReceived:(float *)buffer\n withBufferSize:(UInt32)bufferSize\nwithNumberOfChannels:(UInt32)numberOfChannels {\ndispatch_async(dispatch_get_main_queue(),^{\n//add this flag to pause/unpause\nif(self.isRecording) {\n      [self.audioPlot updateBuffer:buffer[0] withBufferSize:bufferSize];\n  }\n});\n}\n-(void)microphone:(EZMicrophone )microphone\nhasBufferList:(AudioBufferList )bufferList\nwithBufferSize:(UInt32)bufferSize\nwithNumberOfChannels:(UInt32)numberOfChannels {\n//I believe this flag was already there\nif( self.isRecording ){\n  [self.recorder appendDataFromBufferList:bufferList\n                         withBufferSize:bufferSize];\n  }\n}\n```\nYou can adapt this to your own UI. I was able to record, pause, record, pause, record, and pause, and all three recordings were appended together instantaneously without loading or delay. It was as good as the stock Voice Memos app. Maybe this was obvious to some, but no one answered back on here so I just now figured it out.\nI'm now working on a way to append somewhere other than the end of the audio file. So if I record and pause, I should be able to seek backward and record from within the middle of the file.\n. I will have to check again, but I think I was able to play the audio file using AVAudioPlayer. I would think it wouldn't be possible, but I definitely played the file back to make sure it was appending and it played. So let me double check what I was doing.\n. Yeah, you're right. I could record multiple times and play it, but the audio player closes the file, so if you try to record after playing it, nothing happens.\n. Awesome, just curious, what's your estimated delivery date? Thanks.\nOn Wed, Dec 10, 2014 at 2:06 PM, Syed Haris Ali notifications@github.com\nwrote:\n\n@lususvir https://github.com/lususvir, I'm [finally] working on a new\nversion of EZAudioFile that provides support read/write support so the\nEZRecorder will be going away. I recommend following the\naudio_file_refactor\nhttps://github.com/syedhali/EZAudio/tree/audio_file_refactor branch!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/syedhali/EZAudio/issues/120#issuecomment-66533128.\n. What happened to the audio_file_refactor branch?\n. I've been asking around to see if this is possible, this is what I've found:\n\nI was told you can use AudioFileWritePackets to specify a starting point when writing to an audio file. See here: http://stackoverflow.com/a/30196698/1391672\nIf anyone knows how to do this with EZAudio, whether it be by subclassing the EZRecorder, or whatever, please let me know. This is one of my last major roadblocks.\n. No. In my app I have been using 2 separate files, one permanent and one temporary. After each time the user records, the temporary file is appended to the original at a specified position. The problem with this is that with more than 10 seconds of audio, the append takes time, and I don't think doing it in a background thread will help because the file isn't ready for playback or recording until that export happens. \n. Sure, all the code I used can be found here in Issue #118 \nThird comment down.\nOn Thu, Jul 9, 2015 at 4:20 PM, alexgizh notifications@github.com wrote:\n\n@lususvir https://github.com/lususvir would you be kind enough to share\nthe piece of code used for appending?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/syedhali/EZAudio/issues/120#issuecomment-120172700.\n. See #120 \n. @rbarbish solution worked for me. I have an iPad mini running iOS 9.2, never had an issue. I had an iPhone 4s running iOS 7.X, never had an issue. Upgraded my iPhone 4s to iOS 9.2, started having the issue every time I started recording.\n. \n",
    "raghunathkavuri": "Hi SyedhAli,\n   Great work on EZAudio and thanks. \n   I'm using EZAudio for applying custom audio equalization in our video player project.\n   I was able to open the MPMediaItem url and play it in the iOS 7.0+ version of iPad but not on the versions < iOS 7.0.\nAlso, frequent change to seek position (scrubbing the slider) crashes the application with EXC_BAD_ACCESS.\n. ",
    "rmutter": "@raghunathkavuri how did you get this working? Any advice would be appreciated!\n@syedhali was a solution to this ever figured out? When creating a new EZAudioFile with the MPMediaItem's assetURL, I'm getting a crash here:\n[EZAudio checkResult:ExtAudioFileSetProperty(_audioFile,\n                                               kExtAudioFileProperty_ClientDataFormat,\n                                               sizeof (AudioStreamBasicDescription),\n                                               &_clientFormat)\n             operation:\"Couldn't set client data format on input ext file\"];\n. ",
    "nitingohel": "Having the same issue get EXC_BAD_ACCESS at \n[EZAudio checkResult:ExtAudioFileSetProperty(_audioFile,\n                                               kExtAudioFileProperty_ClientDataFormat,\n                                               sizeof (AudioStreamBasicDescription),\n                                               &_clientFormat)\n             operation:\"Couldn't set client data format on input ext file\"];\nWith AssetURL\n. ",
    "tspent": "Guys did you found any solution?\n. ",
    "solomon23": "ipod-library urls were working in 0.0.6 but I just upgraded to 1.1.1 and it stopped working.  I am now required to do the above ?  What I'm seeing is the EZAudioFile dealloc as soon as i create it and crash on dealloc with: \"Error: Failed to dispose of ext audio file (-50)\"\n. Yea:\nhttps://github.com/syedhali/EZAudio/compare/master...solomon23:remove-file-check\nOn Wed, Dec 9, 2015 at 6:13 PM, Uzoma Orji notifications@github.com wrote:\n\n@solomon23 https://github.com/solomon23 Were you able to find a\nsolution to this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/syedhali/EZAudio/issues/12#issuecomment-163464236.\n. @Kowalik how was this solved ? I'm running into the same issue\n. Thanks - I created a fork so my project could move forward : https://github.com/solomon23/EZAudio/tree/remove-file-check\n\nNot sure if there are other repercussions to that.  But it fixed my use case of loading a music track from the library and making a plot out of it.\n. ",
    "superuzoma": "@solomon23 Were you able to find a solution to this?\n. ",
    "vinhnguyendinh": "2016-07-14 16:21:55.911 AppEditMusic[2922:914540] outputs: (\n    \" { port: , data source: (null) }\"\n)\n2016-07-14 16:21:55.950 AppEditMusic[2922:914540] Error overriding output to the speaker: The operation couldn\u2019t be completed. (OSStatus error -50.)\n2016-07-14 16:22:36.548 AppEditMusic[2922:914540] * Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '* -[NSURL initFileURLWithPath:]: nil string parameter'\n. Thank you very much ! :)\n2016-07-14 16:43 GMT+07:00 saitjr notifications@github.com:\n\nYou have to copy item from iPod library to sandbox.\nCheckout: https://github.com/jasonsaw/TSLibraryImport\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/syedhali/EZAudio/issues/12#issuecomment-232618015,\nor mute the thread\nhttps://github.com/notifications/unsubscribe/ANJRvGRLVbsBWjhS6TAfb6L8EoE8E2Xiks5qVgTdgaJpZM4BZHDL\n.\n. \n",
    "saitjr": "You have to copy item from iPod library to sandbox.\nCheckout: https://github.com/jasonsaw/TSLibraryImport\n. But the sound comes from microphone, not the speaker. Did you solve it?\n. That's work for me, thank you very much. And #303 is work for me too.\n. \ud83d\udc4c\n. Have you ever take a look at #303, It works for me.\n. ",
    "schardt": "Syed, I'm just starting with EZAudioPlayer.  Is it still the case that one must copy an iPod Library song into the Documents folder before playing it with this class?  If so, how are you and other users handling the case where two songs have the same title?  (I suppose one could use the songs' IDs but wondering if you have something better.)\nHow long does it take, in your experience, for an iPhone 5s to copy a 5min song?  Is it as quick as a raw file duplication?\nA couple of people in this thread have noted that EZAudio DID play iPod URLs in an early version.  Did you ever check out whether this is true and perhaps how you can get this functionality back?\nAlso, someone noted that playing iPod URLs works in iOS 7+ but not in 6.  It seems that can't be true, but just wondering.\nBasically, I'm trying to avoid copying files like this.  It's a fuss to either keep track of which files have been copied and/or when to delete them.\nThanks so much for making this library!  I've been using EZMicrophone for years now in an LED display client app called LED Lab.  I'm looking forward to using more of the library soon!\n. Thanks dxclancy!  Yes, MPMediaItem's URLs are useable by EZAudioFile, once you take out the \"fileExistsAtPath\" call.  I ended up tweaking a LOT of code in EZAudio.  There were a number of loose ends and things that imagine would sometimes crash.  (eg:  Changing the length of buffers in an AudioBufferList.)  Things are working REALLY well in LED Lab now!\n. ",
    "dxclancy": "No, it is not necessary to copy the files, you should be able to play them directly, but you need to make a small few changes to some of the error handling in the library.  There is an issue in the library when validating the url.  You should be able to search other issues for the things that need to be changed.\nI believe this is the issue in EZAudioFile:\n```\n    // dxc add:\n    [EZAudioUtilities checkResult:ExtAudioFileOpenURL(url, &self.info->extAudioFileRef)\n                        operation:\"Failed to create ExtAudioFileRef\"];\n// dxc: removed\n//NSURL *fileURL = (__bridge NSURL *)(url);\n//BOOL fileExists = [[NSFileManager defaultManager] fileExistsAtPath:fileURL.path];\n//\n////\n//// Create an ExtAudioFileRef for the file handle\n////\n//if (fileExists)\n//{\n//    [EZAudioUtilities checkResult:ExtAudioFileOpenURL(url, &self.info->extAudioFileRef)\n//                        operation:\"Failed to create ExtAudioFileRef\"];\n//}\n//else\n//{\n//    return NO;\n//}\n\n```\n. Did you try the commit listed above by thuydx55?\n. Hi, I know very little about audio, or this library.  \nHere is how I am achieving it.  I don't know if it is dumb or if there is an easier way, etc...\nWhat I do is i use EZAudioFFT.  You can take a look at the sample app that is provided.  Essentially, you feed the delegate data from the player or microphone to the FFT.  From the FFT delegate you can get \nfloat maxFrequencyMagnitude = [fft maxFrequencyMagnitude];\nThis may be enough to achieve what you want.  Again, it may be overkill or the wrong approach, but it is feasible.\n. Hi, I also had this problem, and I believe I've found the solution for this. (Note, for me, it worked on the \"lock screen\" when it was visible, but no audio when the phone was locked and screen was dark)\nThere is an Apple Tech Note that says you must set kAudioUnitProperty_MaximumFramesPerSlice to 4096, which EZOutput does do like so:\n[EZAudioUtilities checkResult:AudioUnitSetProperty(self.info->mixerNodeInfo.audioUnit,\n                                                       kAudioUnitProperty_MaximumFramesPerSlice,\n                                                       kAudioUnitScope_Global,\n                                                       0,\n                                                       &maximumFramesPerSlice,\n                                                       sizeof(maximumFramesPerSlice))\n                        operation:\"Failed to set maximum frames per slice on mixer node\"];\nHowever, the tech note says it must be set \"for each Audio Unit being used\".\nSo, I added the same setting for the converterNode and outputNode, and audio started working on the lock screen:   \n```\n [EZAudioUtilities checkResult:AudioUnitSetProperty(self.info->converterNodeInfo.audioUnit,\n                                                       kAudioUnitProperty_MaximumFramesPerSlice,\n                                                       kAudioUnitScope_Global,\n                                                       0,\n                                                       &maximumFramesPerSlice,\n                                                       sizeof(maximumFramesPerSlice))\n                        operation:\"Failed to set maximum frames per slice on mixer node\"];\n    [EZAudioUtilities checkResult:AudioUnitSetProperty(self.info->outputNodeInfo.audioUnit,\n                                                       kAudioUnitProperty_MaximumFramesPerSlice,\n                                                       kAudioUnitScope_Global,\n                                                       0,\n                                                       &maximumFramesPerSlice,\n                                                       sizeof(maximumFramesPerSlice))\n                        operation:\"Failed to set maximum frames per slice on mixer node\"];\n```\n. I see this as well.\n. Placing a breakpoint in malloc_error_debug hasn't helped me at all, so I turned on Address Sanitizer.  (edit schemes).\nThis causes an immediate break in\n- (void)convertDataFromAudioBufferList:(AudioBufferList *)audioBufferList ...\nat ..\n[EZAudioUtilities checkResult:AudioConverterFillComplexBuffer(self.info->converterRef,\nThe issue is that for some reason when the lock screen is activated, the amount of frames goes from 1024 to 4096.  And this triggers this offending line:\naudioBufferList->mBuffers[i].mDataByteSize = frames * self.info->inputFormat.mBytesPerFrame;\nEssentially, the bufferSize that was allocated was 4096, and the size is set to 16384 even though it is STILL  only a buffer of 4096.  This causes an overwrite of the buffer in AudioConverterFillComplexBuffer.\nI'm not knowledgable enough to know if this is a bug in EZAudio, or a result of incorrect configuration of the properties that lead to the buffer initialization.  It definitely seems EZAudio is wrong to blindly change the buffer size despite what it's ACTUAL capacity size it is.\n. I'm not sure what the proper fix for this is.  You can work around it by multiplying 4 to the calculated buffer size in \nEZAudioUtilities.m : audioBufferListWithNumberOfFrames\naudioBufferList->mBuffers[i].mData = calloc(bufferSize * 4, 1);\nAlso see #303 which is unrelated to this, but related to lock screen.\n. Please do not close this. My workaround is not a fix but an awful hack. We need to understand if this is a bug in EZAudio or a problem in setup of the properties that lead to buffer allocation, and this needs to be fixed. \nPlease reopen. \n. My mistake, I assumed they were the same, that EZAudioPlot was backed by EZAudioPlotGL, and didn't bother to try the other, just tried duplicating the outlet/view from the example app.  thanks.\n. Every time you call readFrames, it will read a certain number of frames and call through to the delegate method with the data for those frames.\nOne approach you can take is to loop and read until eof while you process the data in the delegate.\n. I always find it instructive to look at the code when I have questions like this.\nYou can see that readFrames calls ExtAudioFileRead, which is documented to be synchronous.\nAfter that, you can see that the AudioBufferList is filled, and you could, upon the completion of readFrames, access this buffer list.  However, readFrames additionally calls convertDataFromAudioBufferList toFloatBuffers. These float buffers are what you receive in the delegate callback, and is what the rest of the EZAudio library uses for graphing/fft calculation etc...  It looks to me that the delegate will get called even before the completion of read frames, so that when you get the eof result, the AudioBufferList is full and your delegate has already been called with float buffers.\nIf you are using the data directly from the AudioBufferList, you needn't even use the delegate.  However, I think that part of the point of EZAudio is the conversion it provides.\nThat's my interpretation anyway.  I'm not one of the collaborators :)\nI'm not very experienced with audio data, so it isn't clear to me what format either the AudioBuffers or the Float buffers really represent.  For my own uses, I use EZAudio to process audio files and determine frequency and intensity using various EZAudio calls, so the data is \"opaque\" to me.\n. ",
    "JungHsu": "@dxclancy thankyou , use your code, has solved my problem!. ",
    "neophit": "DOUAudioStream looks interesting, but I can't find much documentation for it. Does DOUAudioStream support sending audio over a network or only receiving?\n. ",
    "NemoAir": "But DOUAudioStream seems had not be maintenanced over a year\uff0cit be a long time that douban.fm app to update new version.\n. ",
    "ghost": "HI,\nThank you so much for the swift fix. However now, I am getting this error when running the app on a device:\nError: Failed to read audio data from audio file (-50)\nI have tried various formats, and all result in this error.\nKind Regards\nJosh\n. Has anybody got like some pointers as to where to start with this same topic? I mean creating a custom asbd\n. Thanks for you reeply. \nI have tried but no luck. Here is my initializeView method:\n-(void)initializeView {\n    // Initialize the subview controller\n    _glViewController = [[EZAudioPlotGLKViewController alloc] init];\n    _glViewController.view.frame = self.bounds;\n    [self insertSubview:self.glViewController.view atIndex:0];\n    // Set the default properties\n    self.gain           = 1.0;\n    self.plotType       = EZPlotTypeBuffer;\n    self.opaque     = NO;\n    self.backgroundColor= [UIColor clerColor];\n    self.color          = [UIColor colorWithRed:0.481 green:0.548 blue:0.637 alpha:1];\n    self.shouldFill     = NO;\n    self.shouldMirror   = NO;\n}\nI still get a black background.\nCan glClearColor(0, 0, 0, 0) do anything ?\nGreat library btw. Thanks for sharing.\nPascal\n. It worked. Great! Thanks.\n. Rahav, I think it would be excellent to add this. Additive Synthesis to add sine waves of different phases and levels to produce tri's, saw and squares; the latter two having the best harmonics; then have 2 so you can master/slave them; produce richer waveforms and filter out using low-hi or band/notch filters.\n. ",
    "prisonerjohn": "Hey,\nI'm getting the same error in the EZAudioPlayFileExample, but I'm on iOS 6. I'm just trying with the example WAV file. Any updates on this?\nThanks!\n. Hey Haris,\nSorry I haven't had a chance to test yet. I'll let you know when I do!\n\nElie\nOn Thu, Mar 6, 2014 at 12:28 AM, Syed Haris Ali notifications@github.comwrote:\n\nClosed #15 https://github.com/syedhali/EZAudio/issues/15.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/syedhali/EZAudio/issues/15\n.\n. \n",
    "tspecht": "I still experience the EXC_BAD_ACCESS with CrashIfClientProvidedBogusAudioBufferList. Any updates or hints on how to resolve this?\n. Great, that fixes it! Strange why the GitHub search didn't bring that issue up... Sorry for that!\n. ",
    "jorgej-ramos": "Thanks @syedhali!\nEl 10/02/2014, a las 08:09, Syed Haris Ali notifications@github.com escribi\u00f3:\n\nHey @jorge-ramos,\nThanks for the heads up! Fortunately this isn't breaking anything (yet?), but I'll update the upcoming 1.0 version of EZAudio to use the shared AVAudioSession singleton like apple recommends.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "warpling": "Hi @syedhali. Do you have an estimate when 1.0 will be ready? Thanks!\n. Awesome!\n. It looks like master is still 0.0.5? \n. +1\n. ",
    "andykorth": "Oh yes, I actually made a lot of changes (that I don't think I have commit yet) to draw multiple plots.\nI moved away from GLKViewController, and just created GLKViews instead. This allows me to only update the plot when it's dirty, which saves hugely on the CPU time. That's largely automatically handled thanks to UIView's setNeedsDisplay\nI also have some code where all the GLKViews share the same context. Since you can't rely on the context destruction doing your cleanup, you need to clean up after your own gl state by calling glDeleteBuffers. However, creation of contexts seems to take quite a while (like 100 ms), so if you have 10 displays on your screen, it can really add up. In my case, I also create and delete them frequently, so that can get rough.\nI fixed a bug where different EZAudioPlotGLs couldn't have different glClearColors. \nI also made some changes for my specific purpose- adding zooming and scrolling, changing the default viewport, adding outlines and radiused edges, etc. These are very use-specific and aren't really useful for other people... so my code currently needs some cleanup before I push that stuff.\nOn Apr 8, 2014, at 4:07 PM, Syed Haris Ali notifications@github.com wrote:\n\nJust an update -\n@andykorth has made this possible for iOS by setting the shared context. I'm looking to add this tweak for Mac soon.\n\u2014\nReply to this email directly or view it on GitHub.\n. @lususvir My code has diverged quite a bit from the current EZAudio code, and it might be too big of a structural change to the project to do things the way I do them. I've also added scrolling and zooming within a waveform, etc. I also dropped Mac support... so that's not too good. There are some oddities regarding differences between views on iOS and Mac, so that simplified things for me a bit. Strictly speaking, I think the changes should all be possible for both platforms, but I don't have them. I would suggest starting from this pull request:\nhttps://github.com/syedhali/EZAudio/pull/44\n(The removal of push/pop became a requirement in the current version of xcode, which is probably why the commit can't be auto-merged anymore, but the other issues that were fixed weren't addressed yet, I think)\n\nThe changes I made in order to be able to draw more than one plot (one gl context per plot) were very straightforward: \nCall: [EAGLContext setCurrentContext: self.context];\nas the first line in: -(void)glkView:(GLKView *)view drawInRect:(CGRect)rect\nand as the first line in: -(void)updateBuffer:(float *)buffer\n     withBufferSize:(UInt32)bufferSize {\nFor performance reasons, I had to make some redrawing changes: Change\n    @interface EZAudioPlotGLKViewController : GLKViewController\nto\n    @interface EZAudioPlotGLKViewController : UIViewController\nAdd to EZAudioPlotGLKViewController.m\n-(void)loadView\n{\n    self.view = [[GLKView alloc] init];\n}\nThen be sure to set [self.view setNeedsDisplay]; when gl drawing occurs in that class. And set the view's delegate to the view controller when you setup the view's context. \nIf you want to use a shared context for all your different EZAudio plots, use something like this in viewDidLoad in EZAudioPlotGLKViewController.\n```\n-(void)viewDidLoad {\n  [super viewDidLoad];\nstatic EAGLContext* sharedContext = nil;\nif(sharedContext == nil){\n    sharedContext = [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];\n}\n\n// Setup the context\nself.context =  sharedContext;// [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];\n    //[EAGLContext setCurrentContext: self.context];\n```\n. Nah, I do recording in my app, but I don't draw the waveform til it's done. So no idea what issues might be associated with that. Good luck!\n. Nice fix- I also fixed this issue in my code (although I don't have a pull request, since my changes have diverged quite a bit). \n. ",
    "davidevincenzi": "I discovered that by disabling the microphone when getting the UIApplicationWillResignActiveNotification the problem disappears.\nI don't know if this is the correct solution, anyway.\n. ",
    "nmcc24": "It may not be the most perfect solution but does the job in this situation: keeps the buffer updated but does not draw while the app is in background.\nIn EZAudioPlotGLKViewController's glkView:drawInRect: insert this before the existing code:\n//EZAudioPlotGL is currently crashing while drawing in background. Thus, we only allow the plot to be drawn if the app is in foreground\n    if ([UIApplication sharedApplication].applicationState != UIApplicationStateActive) {\n        return;\n    }\n. Some fix on the AudioPlotGL plot would be great, otherwise i'll adopt AudioPlot\n. ",
    "Wahoozaboo": "I am using the closeAudioFile function with EZRecorder and the clear function with the EZAudioPlot. I am still having crashes when trying to begin recording a second file. Has anyone else solved this? I get the error EXC_BAD_ACCESS associated with the function AudioRingBuffer::GetTimeBounds. I have tried lots of debugging strategies, including NSZombies, trying to catch some over-released object. No dice. Admittedly, I am not an extremely experienced iOS programmer. Any suggestions are welcome.\n. @syedhali Thanks for responding. I am not not using the same instance. But I was monkeying around with little changes all morning, and switched the order of the methods within the stopRecording call. I think the problem was that I was calling [self.microphone stopFetchingAudio] after [self.recorder closeAudioFile]. Now that I stop fetching audio from the microphone BEFORE closing the audio file, it isn't crashing. I'm going to do some more rigorous testing to make sure that's really all there is to it... Feeling optimistic!\n. @rosiematt Yes, it works for me. Are you having a similar issue? Now my struggle is getting audio monitoring working (I want to be able to play and record, simultaneously. Making a smart phone stethoscope - I want the user to be able to listen while recording a heart sound). \n. ",
    "rosiematt": "@Wahoozaboo was this the case? Did [self.microphone stopFetchingAudio]  before you close the file [self.recorder closeAudioFile] resolve this issue?\n. ",
    "vinbhai4u": "I am still facing this issue, Any updates?\n. @ForbeZ I am also facing the same issue, were you able to find a solution?\n. I got it fixed by changing \n#import <EZAudioiOS/EZAudio.h>\nto\n#import \"EZAudio.h\"\nin \nEZAudioiOS.h and EZAudioOSX.h\neven after updating to 1.1.5\n. ",
    "dhosny": "I am still facing this issue, Any updates?. ",
    "RabbitMC": "I use EZRecorder for recording and I use AVAudioPlayer to play the .caf file.\n. *up\n. ",
    "magin": "@syedhali I have the same problem. Recording audio with EZRecorder. It seems to records speech in a non-compressed caf. 10 seconds = 5 MB.\n. ",
    "chrisballinger": "Looks like #19 is the same issue\n. The patch will require a bit of refactoring so that the init methods don't\nduplicate code and that the majority of the init is done in the one with\nthe most parameters, if that makes sense\nOn Thursday, February 20, 2014, Syed Haris Ali notifications@github.com\nwrote:\n\n@chrisballinger https://github.com/chrisballinger,\nThanks for the heads up, were you able to patch it up? If so, I'd gladly\naccept a pull request, and, if not, I'll add it to my todos :)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/syedhali/EZAudio/issues/27#issuecomment-35702241\n.\n. \n",
    "tylerb": "I am experiencing this same issue.\n. I am currently working on this. The refactor is complete, but the system just fails now. I've changed the code so it sets the hardware to the given sample rate, then sets up all the streams using that same sample rate. When the callback is called and AudioUnitRender runs, it returns -10863 (kAudioUnitErr_CannotDoInCurrentContext ) as an error. I read that this could be caused by a mismatch between the hardware sample rate and stream sample rate, but I think I've set them up appropriately:\nobjective-c\nif (_customASBD){\n    Float64 sampleRate = streamFormat.mSampleRate;\n    UInt32 sampleRateSize = sizeof(sampleRate);\n    // Set the sample rate of the input device\n    [EZAudio checkResult:AudioUnitSetProperty(microphoneInput,\n                                              kAudioUnitProperty_SampleRate,\n                                              kAudioUnitScope_Input,\n                                              kEZAudioMicrophoneOutputBus,\n                                              &sampleRate,\n                                              sampleRateSize)\n               operation:\"Could not set microphone's sample rate bus 0\"];\n    [EZAudio checkResult:AudioUnitSetProperty(microphoneInput,\n                                              kAudioUnitProperty_SampleRate,\n                                              kAudioUnitScope_Output,\n                                              kEZAudioMicrophoneInputBus,\n                                              &sampleRate,\n                                              sampleRateSize)\n               operation:\"Could not set microphone's sample rate bus 1\"];\n  }\nAnyone have an idea on what could be happening? When I use the Audio MIDI Setup program, it shows that the sample rate is 44100. So perhaps it isn't actually changing the sample rate of the hardware?\nAdditionally, changing the sample rate to 48000 in Audio MIDI Setup does not resolve the issue.\n. Reading further, it seems the sample rate conversion must be done with a separate Audio Unit - an AudioConverter. Is this correct? I'm afraid I don't know too much about Core Audio. \n. It seems attempting to shoehorn the conversion and whatnot into this framework is a bad idea. I'm going with a custom solution.\n. Looks like this is a duplicate of #27. Sorry about that. \n. ",
    "orkenstein": "Hi everybody! \nSo any solution is available now? I need custom sample rate. Cause EZAudioPlot freezes the whole app when using EZMicrophone.\n. @syedhali, I'm not sure why, but I'm getting incorrect frequency for generated tone. This is true for your  EZAudioFFTExample project. For example, you generate sound at 440hz, but it shows 441.43.\nValues are incorrect all the way.\nThe other problem is that fft.maxFrequencyMagnitude changes quite randomly. I generate constant sound, but it changes all the time. \nAny thoughts?\n. @jrader22, thanks for your reply!\nNot sure, what \"bin\" is. Could you please explain.\nAfter all, how to increase accuracy?\n. ",
    "Ronileco": "I believe I have the same issue.\nI can't set the mic sample rate, it overrides my configuration no matter how I try it.\nShould I just move to the core audio session? or is there something I'm missing.\nThanks.\n. ",
    "ogoldfinger": "The problem is the everything gets initialized BEFORE the custom absd is applied. What worked for me is just reinitializing. You can refactor this a bit more but what worked for me is calling _createInputUnit again after my absd is set:\n-(EZMicrophone *)initWithMicrophoneDelegate:(id<EZMicrophoneDelegate>)microphoneDelegate\n        withAudioStreamBasicDescription:(AudioStreamBasicDescription)audioStreamBasicDescription {\n  self = [self initWithMicrophoneDelegate:microphoneDelegate];\n  if(self){\n    _customASBD  = YES;\n    streamFormat = audioStreamBasicDescription;\n    [self _createInputUnit]; // ADD THIS LINE TO RE-INITIALIZE\n  }\n  return self;\n}\n. +1 for this. I had the same problem so I tried using just audioplot (not GL) and it worked fine except then I ran into this problem: https://github.com/syedhali/EZAudio/issues/55\nBut uncommenting that line as suggested in that thread worked for me as well so now I have it working in a modal (albeit not with openGL)\n. I'm seeing this too when trying to plot a file from ipod-library. I even tried copying the file into NSDocumentDirectory but it doesn't seem to help. \nIf it helps this looks like it's the same error (although caused by different problem):\nhttp://stackoverflow.com/questions/8990770/failed-to-make-complete-framebuffer-object-8cd6-ios-programmatically-created-o\n. +1 for this merge if it works. I'm continually seeing this crash occurring in my app using EZRecorder. I stumbled on the same SO post as well but I'm also not familiar with the framework at this level. \n. I'm seeing the same thing! Although to me it's more like 1/100. I think the issue is memory related but I can't reliably reproduce to know for sure.\n. This is iOS for me. I'm playing it back with an AVAudioPlayer though. This is how I'm setting up the recorder with mic. Anything else you'd want to see that could help?\niOS\nself.ezrecorder = [EZRecorder recorderWithDestinationURL:recordingPath\n                                               sourceFormat:self.ezmicrophone.audioStreamBasicDescription\n                                        destinationFileType:EZRecorderFileTypeM4A];\n. Definitely devices and I've been able to repro in sim as well. \n. One thing worth noting is that I'm playing an AVAudioPlayer in the background the same time as I'm recording. Think that has anything to do with it?\n. If it gives you an idea, it feels to me like this issue only appears if my CPU is working hard. I'm using EZAudioPlot to show microphone activity and I think every time I've seen this issue the UI seems to be choking a bit. \n. One thing I just tried which looks like it may have fixed this issue is delaying the initialization of the microphone to viewDidAppear from viewDidLoad. Swaroop -- may be worth you trying the same and see if that worked for you.\n- (void) viewDidAppear:(BOOL)animated {\n  [super viewDidAppear:animated];\nif(!self._microphone) {\n      self._microphone = [EZMicrophone microphoneWithDelegate:self];\n  }\n  }\n. ",
    "chenjishi": "@ogoldfinger your method worked for me also!\n. ",
    "amritsahay": "Hey    \nI have managed to  done this.  Thanks  anyways.\n. ",
    "eftalersoy": "Seconded for this issue.\nI want to record a low qualiity sound with mp4 format. (I need to send it to my server)\nI tried many things but I cannot init recorder with custom AudioStreamBasicDescription.\nBut there is a simple trick. I don't use recorder of the EZAudio. I initialize my own AVAudioRecorder. While the EZAudioPlotGL drawing the record my own AVAudioRecorder saving the file.\nIt is not very beautiful but it works :)\n. ",
    "KevinGong2013": "If i want take a .wav file, just change EZRecorderFileTypeM4A to EZRecorderFileTypeWAV ?\nActually i did it then app crashed.\nerror message : Error: Failed to create audio file ('fmt?')\nif i set destinationFileType to EZRecorderFileTypeM4A it's works fine.\n. Thanks. I've used another plugin. Next time I'll follow this. :)\n. ",
    "MichaelIT": "I got same problem as same as @KevinGong2013 .Did you finger it out?\n. All right, I found some way to fixed that problem according to http://stackoverflow.com/questions/13171563/ios-extaudiofilecreatewithurl-getting-error-creating-file-fmt\nI change file in EZAudio/Core/EZRecorder.m line 263\naudioFileTypeID = kAudioFileWAVEType;\nto\naudioFileTypeID = kAudioFileCAFType;\nit work!But I think the formate maybe change to caf from wav.\n@KevinGong2013 \n. ",
    "SeoBright": "Hello. I can not record audio as WAV with EZRecorder.\nI tried as following.\nself.audioRecorder = [EZRecorder recorderWithURL:[self testFilePathURL]\n                                            clientFormat:self.microphone.audioStreamBasicDescription\n                                                fileType:EZRecorderFileTypeWAV];\nHope to reply soon. Thanks for your time.\n. ",
    "DaesikJang": "Hello. I got the same problem and solved by changing the formatForFileType for EZRecorderFileTypeWAV format as follows.(in EZRecorder.m file)\n- (AudioStreamBasicDescription)formatForFileType:(EZRecorderFileType)fileType\n                              withSourceFormat:(AudioStreamBasicDescription)sourceFormat\n  {\n  AudioStreamBasicDescription asbd;\n  switch (fileType)\n  {\n      case EZRecorderFileTypeWAV:\n          asbd =  [EZAudioUtilities monoFloatFormatWithSampleRate:sourceFormat.mSampleRate];\n          //asbd = [EZAudioUtilities stereoFloatInterleavedFormatWithSampleRate:sourceFormat.mSampleRate];\n. ",
    "lei-zxx": "YES\uff0cI Agreed jang\n. ",
    "minakolta": "Did you manage to stream a url ?\n. ",
    "LanceFu": "I noticed the same thing. I am using EZAudioPlotGL. On simulator, waveform are drawn as expected. However, I couldn't see anything when running on the device itself. After trying a few things, I found out that if gain property is big enough, the waveform will be visible. It means waveform does get drawn, but they are just too tiny for us to see. It might be a problem in frame size or camera is too far away? \n. ",
    "garylevans": "I'm having the same issue, did you ever found out why? The AudioPlot works fine in sim, but not on my device.\n. ",
    "loudmouth": "I am also having this same problem. I had assumed that I had improperly set the audio session, but after making sure the session category is set to AVAudioSessionCategoryPlayAndRecord and overriding the audio port, still getting nothing on the device despite the fact that it is working in the simulator. I'm using EZAudioPlot rather than the openGL class. Changing the gain did not solve the problem for me.\nEDIT: I actually got this working. In debugging, I noticed that my EZMicrophone delegate functions were not being called, leading me to believe my delegate was somehow not set properly. My workaround was to reinit my mic by pointing it to nil, then calling the following initializer a second time. Apologies for not mentioning I was using the microphone before everybody.\nself.microphone = [EZMicrophone microphoneWithDelegate:self];\n. ",
    "gomezo76": "Hi,\ndid you succeed in detecting BPM or tempo of a song ? \nI would like to have those kind of util method in my app by listening to the microphone but i'm a beginner with EZAudio :-)\nBest regards,\nJulien\n. ",
    "andresteves": "Hi @ilkinn ,\nDid you manage to detect BPM or tempo?\nCheers,\nAndre\n. Careful if you include the example code in your own code and don't call the init for the controller you are showing. \n. Done it. Will submit soon.\n. Hi @amitmalhotra \nI end up using Superpowered framework and use the data from superpowered to feed the audioPlot.\nCheers\n. ",
    "mmarkov-appolica": "+1\n. I'm interested in this feature too\n. I understand you, but have another question. Is there a way to set start and end positions(like time) of the   file and generate waveform just for it?\n. ",
    "LouisBorlee": "+1\n. ",
    "codyrotwein": "I'm also interested in this feature.\n. ",
    "bchessin": "I am not sure why this hasn't been merged yet but the glPush and glPop issue forced me to use this branch in order to use this library at all. Please merge!\n. ",
    "elprl": "Setting the opaque value to NO fixes this. \n. ",
    "Kolineal": "Hi\nI faced this issue, trying to set background color to transparent \nI've tried using Opaque property in both IB and from code but background remains black\nIt seems that PlotView is actually has transparent background, but the GLKView has a black background no matter what I do. It  ignores the alpha of the color\n. ",
    "justin999": "Hi @syedhali \nIn order to get clear background, I tried \"opaque value to NO\" and modified EZAudioPlotGLKViewController.m so that background color become clear color. However, I was not able to get clear background of EZAudioPlotGL. \nAny suggestion will be helpful. thanks,\n. Hi @syedhali \nIn order to get clear background, I tried \"opaque value to NO\" and modified EZAudioPlotGLKViewController.m so that background color become clear color. However, I was not able to get clear background of EZAudioPlotGL. \nAny suggestion will be helpful. thanks,\n. @syedhali \nI finally solved my problem. thanks anyway\n. @joeldrotleff \nDid the solution really work for you? I can't make my EZAudioPlotGL transparent even if I set opaque of subviews YES (or No). Please give some advise. \n. @joeldrotleff \nI finally solved my problem. thanks anyway\n. ",
    "swaroopbutala": "For some reason I was not able to get clear background just by setting background color and opaque on audioPlot. I also did set the backgroundColor to clearColor and opaque to false on all of its subviews. Else I was seeing black background. E.g code:\nfor subview in self.audioPlot.subviews {\n        var subview1 = subview as UIView\n        subview1.opaque = false\n        subview1.backgroundColor = UIColor.clearColor()\n    }\n. Thanks @lususvir! I was actually doing similar appending but was doing it using aiff format. m4a is much better with space! :) \n. Thanks @syedhali \nSame for me too. I am working on iOS, I am using swift though. For me it happens only on phone, never seen happen on simulator. \nI am not really using EZOutput anywhere though. I followed https://github.com/syedhali/EZAudio/tree/master/EZAudioExamples/iOS/EZAudioRecordExample \nHere is how I create my recorder. \nself.recorder = EZRecorder(destinationURL: recordingFileUrl, sourceFormat: self.microphone.audioStreamBasicDescription(), destinationFileType: EZRecorderFileType.M4A) \n. I am using EZAudioPlot too.\nOn Tuesday, May 5, 2015, ogoldfinger notifications@github.com wrote:\n\nIf it gives you an idea, it feels to me like this issue only appears if my\nCPU is working hard. I'm using EZAudioPlot to show microphone activity and\nI think every time I've seen this issue the UI seems to be choking a bit.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/syedhali/EZAudio/issues/152#issuecomment-99180796.\n\n\nSwaroop\n. ",
    "JeanRintoul": "You can calculate the frequency binsize with: float binspacing = (float)(44100)/(float)nOver2; Then, in the FFT example, as it loops to calculate amplitude you can also gather  frequencies[i] = i*binspacing; I do not know about the way to display using the OpenGL library they use here(but that would also be nice), but this is the other axis information you need to plot it(i.e. coreplot has some easy plotting implementations and that has worked for me before for plotting detailed labelled axes).\n. if i try hardcoding the buffersize, then the graphs don't update at the expected interval. Something isn't quite right.\n. Now I can run it with any window size to get better resolution. \n-(void)microphone:(EZMicrophone *)microphone\nhasAudioReceived:(float **)buffer\nwithBufferSize:(UInt32)bufferSize\nwithNumberOfChannels:(UInt32)numberOfChannels {\n```\n// Getting audio data as an array of float buffer arrays. What does that mean? Because the audio is coming in as a stereo signal the data is split into a left and right channel. So buffer[0] corresponds to the float data for the left channel while buffer[1] corresponds to the float data for the right channel.\n// See the Thread Safety warning above, but in a nutshell these callbacks happen on a separate audio thread. We wrap any UI updating in a GCD block on the main thread to avoid blocking that audio flow.\ndispatch_async(dispatch_get_main_queue(),^{\n// All the audio plot needs is the buffer data (float*) and the size. Internally the audio plot will handle all the drawing related code, history management, and freeing its own resources. Hence, one badass line of code gets you a pretty plot :)\n\n\n\n[self.audioPlot updateBuffer:buffer[0] withBufferSize:bufferSize];\n\n\n\n// Decibel Calculation.\n\nfloat one       = 1.0;\n\nfloat meanVal   = 0.0;\n\nfloat tiny      = 0.1;\n\nvDSP_vsq(buffer[0], 1, buffer[0], 1, bufferSize);\n\nvDSP_meanv(buffer[0], 1, &meanVal, bufferSize);\n\nvDSP_vdbcon(&meanVal, 1, &one, &meanVal, 1, 1, 0);\n\n// Exponential moving average to dB level to only get continous sounds.\n\nfloat currentdb = 1.0 - (fabs(meanVal)/100);\n\nif (lastdbValue == INFINITY || lastdbValue == -INFINITY || isnan(lastdbValue)) {\n\n    lastdbValue = 0.0;\n\n}\n\ndbValue =   ((1.0 - tiny)*lastdbValue) + tiny*currentdb;\n\nlastdbValue = dbValue;\n\n// NSLog(@\"dbval:  %f\",dbValue);\n\n//\n\n// Setup the FFT if it's not already setup\n\nint samplestoCopy = fmin(bufferSize, FFTLEN - _fftBufIndex);\n\nfor ( size_t i = 0; i < samplestoCopy; i++ ) {\n\n    _fftBuf[_fftBufIndex+i] = buffer[0][i];\n\n}\n\n_fftBufIndex        += samplestoCopy;\n\n_samplesRemaining    -= samplestoCopy;\nif (_fftBufIndex == FFTLEN) {\n\n    if( !_isFFTSetup ){\n\n        [self createFFTWithBufferSize:FFTLEN withAudioData:_fftBuf];\n\n        _isFFTSetup = YES;\n\n    }\n\n    [self updateFFTWithBufferSize:FFTLEN withAudioData:_fftBuf];\n\n    _fftBufIndex        = 0.0f;\n\n    _samplesRemaining   = FFTLEN;\n\n}\n\n});\n```\n}\n. Now I can run it with any window size to get better resolution. \n-(void)microphone:(EZMicrophone *)microphone\nhasAudioReceived:(float **)buffer\nwithBufferSize:(UInt32)bufferSize\nwithNumberOfChannels:(UInt32)numberOfChannels {\n```\n// Getting audio data as an array of float buffer arrays. What does that mean? Because the audio is coming in as a stereo signal the data is split into a left and right channel. So buffer[0] corresponds to the float data for the left channel while buffer[1] corresponds to the float data for the right channel.\n// See the Thread Safety warning above, but in a nutshell these callbacks happen on a separate audio thread. We wrap any UI updating in a GCD block on the main thread to avoid blocking that audio flow.\ndispatch_async(dispatch_get_main_queue(),^{\n// All the audio plot needs is the buffer data (float*) and the size. Internally the audio plot will handle all the drawing related code, history management, and freeing its own resources. Hence, one badass line of code gets you a pretty plot :)\n\n\n\n[self.audioPlot updateBuffer:buffer[0] withBufferSize:bufferSize];\n\n\n\n// Decibel Calculation.\n\nfloat one       = 1.0;\n\nfloat meanVal   = 0.0;\n\nfloat tiny      = 0.1;\n\nvDSP_vsq(buffer[0], 1, buffer[0], 1, bufferSize);\n\nvDSP_meanv(buffer[0], 1, &meanVal, bufferSize);\n\nvDSP_vdbcon(&meanVal, 1, &one, &meanVal, 1, 1, 0);\n\n// Exponential moving average to dB level to only get continous sounds.\n\nfloat currentdb = 1.0 - (fabs(meanVal)/100);\n\nif (lastdbValue == INFINITY || lastdbValue == -INFINITY || isnan(lastdbValue)) {\n\n    lastdbValue = 0.0;\n\n}\n\ndbValue =   ((1.0 - tiny)*lastdbValue) + tiny*currentdb;\n\nlastdbValue = dbValue;\n\n// NSLog(@\"dbval:  %f\",dbValue);\n\n//\n\n// Setup the FFT if it's not already setup\n\nint samplestoCopy = fmin(bufferSize, FFTLEN - _fftBufIndex);\n\nfor ( size_t i = 0; i < samplestoCopy; i++ ) {\n\n    _fftBuf[_fftBufIndex+i] = buffer[0][i];\n\n}\n\n_fftBufIndex        += samplestoCopy;\n\n_samplesRemaining    -= samplestoCopy;\nif (_fftBufIndex == FFTLEN) {\n\n    if( !_isFFTSetup ){\n\n        [self createFFTWithBufferSize:FFTLEN withAudioData:_fftBuf];\n\n        _isFFTSetup = YES;\n\n    }\n\n    [self updateFFTWithBufferSize:FFTLEN withAudioData:_fftBuf];\n\n    _fftBufIndex        = 0.0f;\n\n    _samplesRemaining   = FFTLEN;\n\n}\n\n});\n```\n}\n. const UInt32 FFTLEN = 4410;\nint _fftBufIndex = 0; \nYou may want to change FFTLEN to whatever you want, based on what resolution you want... This gave 5Hz bins.\n.   The following accelerate code converts the buffer to dB. Your meanval is then the dB value. \nfloat meanVal   = 0.0;\nvDSP_vsq(buffer[0], 1, buffer[0], 1, bufferSize);\nvDSP_meanv(buffer[0], 1, &meanVal, bufferSize);\nvDSP_vdbcon(&meanVal, 1, &one, &meanVal, 1, 1, 0);\n. MPVolumeView is external to the library, but it is one way. \n. ",
    "syrakozz": "+1\n. this is an important merge +1\n. +1\n. +2\n. +1\n. +1  it will be great if it can merged with this project\n. +1\n\u2014\nSent from Mailbox\nOn Wed, Feb 4, 2015 at 4:48 PM, embassem notifications@github.com wrote:\n\nWould be Great if it support RTMP streaming Audio\nReply to this email directly or view it on GitHub:\nhttps://github.com/syedhali/EZAudio/issues/128\n. +1\n. \n",
    "gopdahl": "Where and how do you initialize the _fftBuf, _fftBufIndex, and _samplesRemaining? \nIf I do it in the microphoneDelegate they get initialized every time it's called and the next 512 samples from the buffer never get appended to the _fftBuf.\n. ",
    "rahav": "I would like to add to this question: Is there a way to let the audio keep going (playing) while the output being forced to mute? The idea is that if my the phone switches to another task the output will be muted, but keep rolling. I think it would be very cool\n. I ended up writing a new EZPureWave component. Its a rough version just for testing at this time, but it is a solution if you do not want to save everything into an audio file and then play it. If there will be an interest I will clean it up and post it.\n. Thanks Syed, I did just clone the repo and I still get the three warnings. Any idea what could have went wrong?\n. I'm not sure because I did not investigate this part of the lib, but there is a variable called gain in EZAudioPlot. gain is used to scale each of the points in the audio buffer it draws, so I presume its a value in the range (0, 1] and your highest point would be gain * highestValueInYourBuffer. \n. ",
    "zammitjames": "Did you find a solution for this ?\n. ",
    "cdstamper": "Any solution for this? I'm would like to adjust volume for output as well...\n. ",
    "hakanw": "+1 on this\n. Maybe this is fixed with my pull request https://github.com/syedhali/EZAudio/pull/102\n. See also this other related pull request: https://github.com/syedhali/EZAudio/pull/108\n. Oops. A lot of other changes were accidently added to this. I'll re-open a new PR.\n. A problem with this is that once a file is done playing, the default output keeps pushing null data (silence) to it. It doesn't stop playback. And so this fix will keep reporting eof multiple times until you stop/pause the player. Probably the player should stop playback in this method, if loop is false.\n. I've got a new PR coming up for this\n. There are multiple pull requests with code that fixes this bug. See them for how to fix it (and let's hope they will be merged to master soon).\n. See issue #61 \n. There's a pull request for it here https://github.com/syedhali/EZAudio/pull/104 but unfortunately the owner of this project hasn't merged it yet (like many other bug fixes available)\n. +1\nIt's impossible right now to have both EZAudio and AmazingAudio in the same projekt due to duplicate symbol errors from the linker.\n. Done!\n. ",
    "jordanful": "Same question here. Doesn't seem to be possible though \u2014\u00a0I'm having to save the file locally first.\n. ",
    "nicka": "Would be really amazing! Doing the same thing - First saving locally.\n. ",
    "kwade101": "In the EZAudioWaveformFromFileExample project, I swapped out the AudioPlot UIView with an AudioPlotGL.  No other changes. With AudioPlotGL, there is no increasing gap on the centerline of the waveform.  So, the issue seems isolated to the Core Graphics-driven AudioPlot. I took a look but I don't see anything obvious there.  Any thoughts?\n. The issue appears to be that -- in the latest version 0.0.5 -- this line is commented out:\n        plotData[plotLength-1] = CGPointMake(plotLength-1,0.0f);\nIf I restore that line, the waveform looks normal again without the gap along the centerline.\nIs there a reason why this was commented out in the  0.0.5 refactoring of EZAudioPlot.m?\n. ",
    "sabymike": "I would also like this type of behavior. Crashing the application is rarely a desired behavior. Raise an exception and let me handle the error.\n. @shannonchou Not the best solution but I have the same problem and ended up using this. It gets the job done.\nhttps://github.com/williamFalcon/SwiftTryCatch\n. +1\n. ",
    "castillejoale": "I get the same, were you able to figure it out?\n. ",
    "JJSaccolo": "I think that you should enable the exception breakpoint in order to reproduce it.\n. ",
    "andypotato": "I can reproduce this issue with Xcode 6 developing for IOS.\nEnable the \"All exceptions\" break point and run the EZAudioRecordExample. The application will crash during EZRecorder::_initializeRecorder at the following method:\n[EZAudio checkResult:ExtAudioFileSetProperty(_destinationFile,\n                                             kExtAudioFileProperty_ClientDataFormat,\n                                             sizeof(_sourceFormat),\n                                             &_sourceFormat)\nThe crash is a result from _destinationFile being created, but giving a \"BAD_ACCESS\" exception when trying to access it.\nAny ideas what might be the cause?\n. ",
    "Steveybrown": "Getting the same issue on 0.0.6. Did you ever find a solution / reason for this ? \n. having the same problem but it seems to be an edge case. Using microphone and recorder. \n. ",
    "rednebmas": "I am getting this issue as well when playing MP3 files from the EZAudioPlayFileExample.\n. After a lot of research it seems that it is supposed to be throwing and exception. More information can be found on this stackoverflow post:\nhttp://stackoverflow.com/questions/9683547/avaudioplayer-throws-breakpoint-in-debug-mode\n. ",
    "acerbetti": "Thanks. I appreciated the quick push\n. ",
    "dabing1022": "I used \"pod search EZAudio\" in my console,  but the version is still 0.0.4? Am I missing something?\n\n. ",
    "cutmaster": "Answering to myself and to people who are looking for a solution :\nTo do so, you must :\n1) not use the dispatch_once token\n2) create a pointer to an EZOutput YOU handle by allocating it (instead of using sharedOutput)\n\nif (audioOutput != nil) {\n        if ([audioOutput isPlaying]) [audioOutput stopPlayback];\n        [audioOutput setOutputDataSource:nil];\n        audioOutput = nil;\n    }\n    audioOutput = [[EZOutput alloc] init];\n    [audioOutput setOutputDataSource:self];\n\n. hi syedhali,\nThis component is in fact derivated from your EZAudioPlayer. It uses the same first methods and adds some other to be able to have start zone, loop zone, end zone, and playlist inside the audiofile.\nBut actually if you try to do the same with EZAudioPlayer you get the same behaviors.\n. hi Syedhali,\nI'll try it and let you know.\nBy the same way, can you please tell me if :\n1) you plan to fix the bugs especially the one that randomly crashes the application\n2) you plan to create a component working the same as EZAudioPlayer but without storing the whole file in memory (usefull to play long play files as real music titles)\nThanks a lot.\n. Duplicate of #60 too.\nWe're all hoping you'll be able to fix it asap ;)\n. Hi SyedHali,\nAs this is a recurring bug, when do you plan to post a new fixed version ?\nThanks\n. You need to use FFT's on input buffers to be able to get amplitude in a frequency range ;-)\n. ",
    "castles": "Hi, mmarkov. I managed to get it to work.. I ended up splitting the audio file into frames and creating  images from each segment. It took a while to piece together and I don't think its pretty but it works.\n. Hi lususvir, It took a while for me to work it out. Performance wise its ok, It takes about 1 minute for 3 minutes of audio. This is fine as I'm caching the generated images and so its only happens once per song. I'll see if I can dig up the code.\n. Hi Joni-Aaltonen. I don't really have a working example but this might help: http://audiokit.io/playgrounds/Analysis/Tracking%20Frequency%20of%20Audio%20File/\nhave you seen this?. I was having the same issue.. this has fixed those compile errors.. but now when I add the EZAudio folder to a brand new project I get the following error: \n...testcompile/EZAudio/EZPlot.h:34:38: Function definition is not allowed here\nI can compile with the samples.. but can't work out what the difference is between the samples and a new project. Is it a compile setting?\nEDIT: I've worked it out.. I needed to add the A Prefix Header to Apple LLVM 6.0 - Language\n. Hi Vortec4800, I've been compiling successfully for iOS8 with the commit changes and the prefix comment above.\n. I'm using EZAudio to generate a waveform and saving it as png files. So I'm assuming that would be EZOutput?\n. eugenehp - I don't think OutputRenderCallback is being called.\nWhat I'm doing is pretty ugly.. I'm basically stepping through the audio file and creating 1024x200 pixel snapshots of the waveform... I added a method to set the total frames to 300000 for each snapshot.\n```\n    audioPlot = [[EZAudioPlot alloc] initWithFrame:CGRectMake(0, 0, 1024, 200)]; \n    audioPlot.backgroundColor = [UIColor colorWithRed: 0 green: 0 blue: 0 alpha: 1];\n    audioPlot.color= [UIColor colorWithRed:1.0 green:1.0 blue:1.0 alpha:1.0];\n    audioPlot.plotType= EZPlotTypeBuffer;\n    audioPlot.shouldFill= YES;\n    audioPlot.shouldMirror= YES;\naudioFile          = [EZAudioFile audioFileWithURL:[[NSURL alloc] initWithString:exportPath]];\n\n[audioFile seekToFrame:i];\n[audioFile setWaveformResolution:800];\n[audioFile setTotalFrames:300000];\n\n[audioFile getWaveformDataWithCompletionBlock:^(float *waveformData, UInt32 length) {\n        [audioPlot updateBuffer:waveformData withBufferSize:length];\n        //save image as png.. then run the whole lot again...\n}];\n\n```\nsyedhali - yes, thats what I'm using.\nIf there is a better way to do this please let me know!\n. Thanks, I'm starting to understand what is happening, however if I set interleaved to NO the ExtAudioFileRead fails. Any chance you can explain in a bit more detail what I need to do? \n. Sure, here is a sample project I've been playing with.. (its basically the EZAudio Waveform from File Example)\nhttps://www.dropbox.com/s/rsp48e68avi6nae/EZAudio%20Split%20Left%20Right.zip\nAlso, I've been using the Xcode 6 Beta if that makes a difference.\n. Thanks, that would be great\n. Hi eugenehp. Did you manage to have a look at this?\n. Is anyone else able to help out with this?\n. Thanks syedhall!!!\nI've managed to get it working based on your suggestions.. I create a new RMS method:\n```\n+(NSArray )RMSLR:(float )buffer\n     length:(int)bufferSize {\n    float leftsum = 0.0;\n    float rightsum = 0.0;\n    for(int i = 0; i < bufferSize; i++) {\n        if(fmod(i, 2) == 0) {\n            leftsum += buffer[i] * buffer[i];\n        } else {\n            rightsum += buffer[i] * buffer[i];\n        }\n    }\n    return [NSArray arrayWithObjects:[NSNumber numberWithFloat:sqrtf( leftsum / bufferSize )],[NSNumber numberWithFloat:sqrtf( rightsum / bufferSize )], nil];\n}\n```\nand modified EZAudioFile.m so that waveformDataCompletionBlock returned a left and right _waveformData\n. Hi Saiday. I never got this to work and ended up building my own generator based on this blog post: http://www.davidstarke.com/2015/04/waveforms.html\n. ",
    "Joni-Aaltonen": "@castles were you ever able to find the code, I'd be interested too.\n@syedhali Was this ever added to EZAudio as a feature?. Hi @castles - thanks for the example, however already seen that. AudioKit's plotter doesn't seem to do quite what I'm after.. ",
    "eugenehp": "Have you added frameworks?\ns.ios.frameworks = 'AudioToolbox','AVFoundation','GLKit'\ns.osx.frameworks = 'AudioToolbox','AudioUnit','CoreAudio','QuartzCore','OpenGL','GLKit'\nalso some Xcode 6 related changes are here:\nhttps://github.com/eugenehp/EZAudio/commit/848b9909523b222f6501522bd0db2c65c153c23d\n. What about changes in the EZAudio/EZAudioPlotGL.h?\n```\n@@ -23,11 +23,16 @@\n  //  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n  //  THE SOFTWARE.\n+#import \n  #import \"TargetConditionals.h\"\n  #import \"EZPlot.h\"\n#if TARGET_OS_IPHONE\n  #import \n +#import \n +#import \n +#import \n +#import \n  @class EZAudioPlotGLKViewController;\n  #elif TARGET_OS_MAC\n  #import \n```\n. Marc,\nCheckout my commit, you will see the required changes in that header file.\nRegards,\nEugene\nOn Thursday, June 26, 2014, Marc notifications@github.com wrote:\n\nI was having the same issue.. this has fixed those compile errors.. but\nnow when I add the EZAudio folder to a brand new project I get the\nfollowing error:\n...testcompile/EZAudio/EZPlot.h:34:38: Function definition is not allowed\nhere\nI can compile with the samples.. but can't work out what the difference is\nbetween the samples and a new project. Is it a compile setting?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/syedhali/EZAudio/issues/63#issuecomment-47230912.\n. Marc, do you need it for the EZMicrophone or for the EZOutput?\n. Take a look here https://gist.github.com/eugenehp/b586c9e884bf4bcd1662, you might get some ideas from it.\n. you will need to do similar changes in the EZOutput, on line 41\n\nstatic OSStatus OutputRenderCallback(void                        *inRefCon,\n                                     AudioUnitRenderActionFlags  *ioActionFlags,\n                                     const AudioTimeStamp        *inTimeStamp,\n                                     UInt32                      inBusNumber,\n                                     UInt32                      inNumberFrames,\n                                     AudioBufferList             *ioData){\n. Okay, in this case, frames are being interleaved starting from here https://github.com/syedhali/EZAudio/blob/master/EZAudio/EZAudioFile.m#L228\nso you can limit it to one channel, or parse the buffers to get only certain channel.\n. Marc, can you make some Xcode project and upload it somewhere, it will help me to debug it from there.\n. Thanks Marc. Let me take a look at it this weekend, and I'll write back to you about the results I get.\n. Great idea. But the implementation based on top of the AudioSessions won't work here in its best way.\n. ",
    "pkasson": "Yes, just confirmed.  This is being built for Xcode 6 and iOS 8.\n. Yep, that file is different.  For the example projects, a few didn't have that change, but others did.  \nThanks - that fixed it !\n. EStock - did you figure this out?  I am trying to do the same, or at least the ability to change the volume / show the volume.  Thanks.\n. ",
    "Vortec4800": "Any updates on this? I'd like to start working on modifying my app to work with some new iOS 8 features and I want to make sure this works properly.\n. Thanks for the info. Everything is working properly with these changes?\nWould be nice if this stuff was integrated and released so I can update the Pod and have it compile, but this will work in the meantime.\n. I'm running into this too and it's a major roadblock. Happening on iOS 9 iPad Air 2. Haven't been able to track down the cause yet.\n. I was able to fix my problem by releasing (setting to nil in my case) my recorder instance immediately after closing the file. It seems like something was trying to calculate the current recorder time after the file closed which caused the crash. Maybe that would help you?\n. More or less, I was going through and updating all the Pods in my project and EZAudio was one of them. So far I haven't found any incompatibility issues, but I really only use the recorder and plot view so there may be issues elsewhere that I haven't seen.\n. I just started running into this exact issue. Works in debug mode, crashes from jetsam in release mode.\nWere you ever able to track down the root cause on this?\n. ",
    "eneko": "Any new pod release expected soon with the changes in the includes in EZAudio/EZAudioPlotGL.h? 0.0.5 seems to be the latest and does not have those headers, so it does not compile on Xcode 6.\n. ",
    "leegoodrich": "+1 for a new pod release. Just discovered EZAudio when looking for ways to create waveforms. While the results look impressive in XCode 5, I'm targeting iOS 8.0 and now it appears I have to maintain my own fork to get this working.\n. ",
    "bkmu": "Adding my voice here as a +1 -- I don't see a PR open for the Xcode 6 changes, perhaps @eugenehp can open a PR with the changes identified so far to get the ball rolling for @syedhall's review now that we've gotten to GM.\n. Excellent news -- Thanks for posting an update!\n. ",
    "rbarbish": "Please merge the new header file into master!\n. @cutmaster or @rahav do you have any ideas?\n. Thanks @Vortec4800 , I'll try that and post the results. @syedhali are you in the process of looking into EZAudio's iOS9 compatibility? I haven't seen a release in a while and I'd just like to know that support is still being generated for this framework on the latest iOS versions and devices.\nThanks - Ross\n. @syedhali  - Any ideas?\n. Solution - @trkfabi 's code snippet led to my solution of this crash, which occurred on launch / microphone instantiation. And only starting occurring upon upgrading to iOS 9.2 for the majority of the users of my app.\nI changed the microphone instantiation code from:\nself.microphone = [EZMicrophone microphoneWithDelegate:self];\nto\nAudioStreamBasicDescription inputFormat = [EZAudioUtilities monoFloatFormatWithSampleRate:44100.0f];\nself.microphone = [EZMicrophone microphoneWithDelegate:self withAudioStreamBasicDescription: inputFormat];\nTagging people so that this can hopefully get fixed and merged as soon as possible... @syedhali @davidwadge @patriksharma @Danbana @snyuryev @koooootake @dsmelov @garsdle @hyd00 \n. ",
    "sanderman123": "The Master is indeed still 0.0.5, could you update this to 0.0.6? Thanks!\nEdit:\nDecided to try how far I could get using CocoaPods and after some fiddling around I got it to work :)\nThis is a nice tutorial: https://www.youtube.com/watch?v=9_FbAlq2g9o\nNote that at the moment you should have the following as first line in your pod file: \nsource 'https://github.com/CocoaPods/Specs.git'\nUsing CocoaPods I managed to get EZAudio version 0.0.6\nThis solved the errors for me, I now only have a \"deprication\" warning:\n\"kAudioFormatFlagsCanonical\" is depricated: The concept of canonical formats is depricated \n. Actually I also still have troubles properly importing EZAudio.\nAfter installing EZAudio with CocoaPods, I found out that for some weird reason I couldn't access the library from any other file in my project except for AppDelegate. So for instance in the ViewController I couldn't access EZAudio, but in AppDelegate I could.\nIn the end, because I had so much troubles setting it up I decided to just edit one of the example projects instead for now... I wonder if you're having similar issues? (Btw, just so you know I only used EZAudio for OSX applications until now)\n. Probably not the best way, but it works. Look for this code snippet in EZOutput:\n```\n// Get the available bytes in the circular buffer\nint32_t availableBytes;\nfloat *buffer = TPCircularBufferTail(circularBuffer,&availableBytes);\n// Ideally we'd have all the bytes to be copied, but compare it against the available bytes (get min)\nint32_t amount = MIN(bytesToCopy,availableBytes);\nmemcpy( left,  buffer, amount );\nmemcpy( right, buffer, amount );\n```\nImport the Accelerate framework into your project and into EZOutput. Then  change the code snippet into:\n```\n// Get the available bytes in the circular buffer\nint32_t availableBytes;\nfloat *buffer = TPCircularBufferTail(circularBuffer,&availableBytes);\n//Added customisation for volume control:\n  float volume = 0.5f;\n  vDSP_vsmul(buffer, 1, &volume, buffer, 1, inNumberFrames);\n// Ideally we'd have all the bytes to be copied, but compare it against the available bytes (get min)\nint32_t amount = MIN(bytesToCopy,availableBytes);\nmemcpy( left,  buffer, amount );\nmemcpy( right, buffer, amount );\n```\nI guess you can make the volume variable in an instance variable for EZOutput and create your own get and set methods, so you can access the volume from your own app.\nHope it helps!\n. ",
    "grimabe": "Hi, I downloaded the 0.06 version and the error is still there.\nCould someone explain me which prefix header I need to add in the building settings ?\n. ",
    "demiantres": "To fix the problem add a new file, lets say \"MyApp-Prefix.pch\" with the following content:\n```\n//\n//  Prefix header\n//\n//  The contents of this file are implicitly included at the beginning of every source file.\n//\nimport \nifndef __IPHONE_5_0\nwarning \"This project uses features only available in iOS SDK 5.0 and later.\"\nendif\nifdef OBJC\n#import \n  #import \nendif\n```\nThen go to the Build Settings in xCode and set:\nPrecompile Prefix Header: YES\nPrefix Header: \"PATH TO THE FILE, e.g. MyApp-Prefix.pch\".\n. ",
    "audiocommander": "I'm not using GL plot, so I can't say anything about this (yet), but there's a comment about why AudioUnitSampleType is deprecated in  CoreAudioTypes.h: \nThese types are deprecated. Code performing signal processing should use concrete types\n(e.g. float, Float32, SInt16, SInt32). Format-agnostic code, instead of relying on the sizes\nof these types, should calculate the size of a sample from an AudioStreamBasicDescription's\nmBytesPerChannel, mChannelsPerFrame, and (mFlags & kLinearPCMFormatFlagIsNonInterleaved).\nFor interleaved formats, the size of a sample is mBytesPerFrame / mChannelsPerFrame.\nFor non-interleaved formats, it is simply mBytesPerFrame.\n. ",
    "davidwadge": "I'm having the same issue as @avelasquezn - no audio is plotted and the line is flat with a constant bufferSize of 1024. I can see that there's nothing coming in via the mic which the Swift gist below will demonstrate.\nAny ideas what's causing this? I have tried playing around with the AudioSession but no such luck.\nHere's a gist illustrating the problem.\n. Also having the same issue on 6S and the solution above doesn't work.\n. Has anyone had any luck with this? I'm having the same problem since I upgraded to the latest version.\nI've discovered that it only happens when the EZRecorder fileType is set to EZRecorderFileTypeWAV.\n. I can confirm that this affects the EZAudio Examples when changing the fileType to EZRecorderFileTypeWAV (Xcode 7 / Swift 2.0 / iOS9).\n. Also having the same issue on 6S.\n. ",
    "dyelax": "@syedhali: Update: I'm now pretty sure it really is an issue inside the framework, as I tweaked the included \"EZAudioRecordExample\" project so the recording view controller is presented modally and the same error happens.\n. @emajcher Thanks! That would be really helpful. You can email me at matthew_cooper@brown.edu. (Or just post it here if that's possible to help others with the problem until it's resolved.)\n. Thanks a ton guys. I'll try out those two options. Glad I'm not the only one with this issue though -- hopefully it'll get patched up for the openGL plot.\n. ",
    "emajcher": "@dyelax Unfortunately I ran into this also ... after spending a night trying to fix it (with little Open GL knowledge) I gave up and wrote my own waveform display with Core Graphics that sort of mimics the audio display of the voice memo app, but instead of a contiguous graph, it just draws straight vertical lines (which was in the original design anyway).  I would be happy to share this with you, it was only about 70 lines of code and might be a good starting point for you to write something custom if you need to launch in a modal ... I suspect instead of vertical lines you could modify it to use a connected path, discarding old data points. \n. AHH!!  I ran into that issue also (non open GL audio plot) which is why I ended up writing my own waveform display, I hadn't seen the problem/solution #55  which would have saved me some time.  Dyelax, you might want to try that option first (use AudioPlot vs. AudioPlotGL) which will use core graphics and adding the fix in #55.  My implementation will require you to do some additional work to get a connected graph like audio plot, but I will still send it as a simple reference.\n. ",
    "don-inkscreen": "Having this issue as well; have switched to EZAudioPlot in the meantime.\n. Had to use lluisgerard's solution for 1.1.5. Cocoapods refused to install 1.1.2 for me.\n. Had this issue as well and fixed using npalamar's solution. Not a great workaround for multi-developer projects though.\n. ",
    "TareqElMasri": "Guys, does anyone made this happen? I need this. ",
    "mital87": "Hi Syedhali,\nThanks for your reply. I am using your EZAudioRecordin's code in my application. When my app going to back ground mode then Bad access are generated. I am also check your example EZAudioRecording Code in Device and goto background, same issue are generated. I am also attach here my device log. Please solve my issue as soon as possible so i have completed my application which are in hold due to this issue. Do needful. Waiting for your reply soon.\nThanking You. \n\n. ",
    "skg54": "I was able to get solid functionality zooming/scrolling a EZAudioPlot:\nSubclass a UIScrollView and add the audio plot as a subview.  I had no issues with making the plot really long.  All plot and scrollview logic should be contained in the subclass.  \nThis UIScrollView subclass' layoutSubviews method is important to keep the audio plot properly centered on zoom and scroll events.  \nIf you desire only horizontal scroll set the contentSize's height to 0.\nself.contentSize = CGSizeMake(SCREEN_WIDTH*20, 0);\n`///////////////////////////////////////////////////////////////////////////////////////////////////\n- (void)layoutSubviews\n{\n  [super layoutSubviews];\n// center the zoom view as it becomes smaller than the size of the screen\n  CGSize boundsSize = self.bounds.size;\n  CGRect frameToCenter = self.audioPlotStatic.frame;\n// center horizontally\n  if (frameToCenter.size.width < boundsSize.width)\n    frameToCenter.origin.x = (boundsSize.width - frameToCenter.size.width) / 2;\n  else\n    frameToCenter.origin.x = 0;\n// center vertically\n  if (frameToCenter.size.height < boundsSize.height)\n    frameToCenter.origin.y = (boundsSize.height - frameToCenter.size.height) / 2;\n  else\n    frameToCenter.origin.y = 0;\nself.audioPlotStatic.frame = frameToCenter;\n}`\n. Not sure if this is the best solution but I captured 2 images of the EZAudioPlot \nhttps://stackoverflow.com/questions/3539717/getting-a-screenshot-of-a-uiscrollview-including-offscreen-parts/3539944#3539944\nAnd animated a UIBezierPath mask on the top one to make it disappear in a left to right manner\nhttps://stackoverflow.com/questions/24644362/animated-image-wipe-for-waveform-color-change\n. ",
    "whydna": "@skg54 - how big of a waveform did you generate?\nI tried making a waveform of a 5 minute sound, at a resolution of about 50px per second, resulted in a view that is over 10,000px wide.\nThis completely kills the performance of the scroll view.\nI'm thinking the plot needs to be broken up into many smaller NSViews so that the scroll view can optimize and cache properly. \nAny thoughts?. @lususvir Thanks for the code sample.\nI believe you can use NSScrollView (and it's built-in NSClipView) to handle much of the scrolling, caching, and event logic. \nIt will trigger drawRect: with the appropriate rect that needs to be drawn (i.e only what the user is seeing/about to see). You can use this to calculate what range of the buffer that needs to be rendered.\nMy issue is with EZAudioPlot does not seem to be respecting the rect, and instead redrawing the entire waveform. Anyone have any insight?. ",
    "sodastsai": "Merged to next pull request\n. ",
    "thedofl": "I am trying similar one made with Unity. Got code from FFT sample and got error with below line.\n-(Float32)_configureDeviceBufferDurationWithDefault:(float)defaultBufferDuration {\n  Float32 bufferDuration = defaultBufferDuration; // Type 1/43 by default\n#if TARGET_OS_IPHONE\n  // Use approximations for simulator and pull from real device if connected\n    #if !(TARGET_IPHONE_SIMULATOR)\n        NSError *err;\n        [[AVAudioSession sharedInstance] setPreferredIOBufferDuration:bufferDuration error:&err];\n        if (err) {\n            NSLog(@\"Error setting preferredIOBufferDuration for audio session: %@\", err.localizedDescription); <-- Error occur\n        }\n```\n    // Buffer Size\n    bufferDuration = [[AVAudioSession sharedInstance] IOBufferDuration];\nendif\n```\n#elif TARGET_OS_MAC\n#endif\n  return bufferDuration;\n}\n. ",
    "AriX": "Would be great if you'd make this change - exiting the program is helpful for neither the user nor debugging!\n. @shannonchou, feel free to use my branch here: https://github.com/DeskConnect/EZAudio/commits/master\n. Thanks for adding this. I would strongly suggest that NO be the default here. Exiting on error is completely unreasonable behavior for any production software, and most users of EZAudio probably won't be aware of this category.\n. ",
    "shannonchou": "I'm waiting for this change.\n. Thanks @AriX ! I merged you commit on my fork. Now the main problem is that, I'm using swift in my project. Swift can not catch the exception throwed by Objective-C. I suppose changing the source code is the only way.\n. @sabymike Cool\uff01It's useful. I'd love to try it later.\n. I really need this. I merged it in my fork.\n. ",
    "clementprem": "Thanks for the reply. \nIf I want to use EZAudio's audio waveform plot classes in my custom audio player, which I created using AUGraph with audioFilePlayerUnit, how can I fill  waveformData?\n[self.audioPlot updateBuffer:waveformData withBufferSize:length];  \nIs it possible?\n. ",
    "LukasJoswiak": "Yes, this is for the microphone. When the microphone is on, in the microphone:hasAudioReceived:withBufferSize:withNumberOfChannels: method, I am calling createFFTwithBufferSize:WithAudioData: and updateFFTWithBufferSize:withAudioData:, taken from the FFT example project. In updateFFTWithBufferSize:withAudioData:, I am storing the max frequency each loop in an array, which eventually gets turned into a graph.\nSetting the preferred IO buffer duration does not change bufferSize in updateFFTWithBufferSize:withAudioData: and does not effect the number of data points being received. What I want to do is increase the number of data points by calling updateFFTWithBufferSize:withAudioData: more often, which I believe can be achieved by decreasing the buffer size.\n. Jean, would you mind posting the Xcode project, so I can get a feel for how it works? If not, what are FFTLEN and fftBufIndex initialized as?\n. ",
    "jamrader": "JeanRintoul or LikasJoswiak, did either of you manage to get a FFTViewController that could measure the Hz of a sound through the microphone with greater accuracy than that which is provided for with a buffer size of 512? I'm trying to manage increasing its accuracy (most likely by increasing buffer size) but am coming up short.\n. Yes, I'm looking for the highest. So you mean this:\n```\n for(int i=0; i<nOver2; i++) {\n// Calculate the magnitude\nfloat mag = _A.realp[i]_A.realp[i]+_A.imagp[i]_A.imagp[i];\nif(mag > 0) {\n      _i_max = i;\n  }\n// Bind the value to be less than 1.0 to fit in the graph\namp[i] = [EZAudioUtilities MAP:mag leftMin:0.0 leftMax:maxMag rightMin:0.0 rightMax:1.0];\n  }\n```\nAnd thank you so much.\n. iPhone 6\n. It seems the error is completely random. Either it works and will continue to work (for a few runs), then the errors comes and every time I run it, it comes. I'm not changing anything in the project.\n. It is seriously the weirdest error. What is setting off the error is this:\n1) I ran the FFT audio project and got the error Failed to fill complex buffer in float converter ('insz').\n2) I ran the EZAudioRecordings project: no error.\n3) Run the FFT audio project: no error.\nThen I answered I phone call. After hanging up, I ran the FFT audio project again and got the error. I repeated steps 2 and 3: no error. \n. Great. It seems to be working. Thank you for your awesome code.\n. The edits look great. Quick question, are you no longer required to put the code\nAVAudioSession *session = [AVAudioSession sharedInstance];\nNSError *error;\n[session setCategory:AVAudioSessionCategoryPlayAndRecord error:&error];\nif (error)\n{\n    NSLog(@\"Error setting up audio session category: %@\", error.localizedDescription);\n}\n[session setActive:YES error:&error];\nif (error)\n{\n    NSLog(@\"Error setting up audio session active: %@\", error.localizedDescription);\n}\nin     viewDidLoad before the microphone is created?\n. ",
    "johndpope": "great thx for your help JeanRintoul. From my results with http://theamazingaudioengine.com/ - it ticks a few more boxes so I'm going to stick with that.\n. ",
    "kosso": "@codegastudio Hi. I am trying to do the same thing: Get the waveform data, then use this to generate a video (maybe using AVAssetWriter, AVMutableComposition and CALayers?) - using the recorded audio as the audio track (and 'timer').\nI know this is an old thread, but did you ever find come up with a way to do it? (ie: is it possible?)\nMany thanks!\n. Did you figure out how to get the PassThrough demo to reach both left and right earphones? . If you don't perform closeAudioFile, then you should be able to set an isRecording boolean flag to decide if to append data or not, while the mic is fetching. \neg:\n- (void)   microphone:(EZMicrophone *)microphone\n        hasBufferList:(AudioBufferList *)bufferList\n       withBufferSize:(UInt32)bufferSize\n withNumberOfChannels:(UInt32)numberOfChannels\n{\n    if (self.isRecording)\n    {\n        [self.recorder appendDataFromBufferList:bufferList\n                                 withBufferSize:bufferSize];\n    }\n}. ",
    "vdeltoral": "Hey, I just discovered EZAudio yesterday and I'm having a lot of fun with it. I don't think syedhali built functionality in, but you can easily do this using UIGraphics:\n```\nAppDelegate *myAppDelegate = [[UIApplication sharedApplication] delegate];\n//Grab an image of the entire screen\nUIGraphicsBeginImageContext(self.view.frame.size);\n[myAppDelegate.window.layer renderInContext:UIGraphicsGetCurrentContext()];\nUIImage *wholeScreenshot = UIGraphicsGetImageFromCurrentImageContext();\nUIGraphicsEndImageContext();\nCGRect croppedRect = myAudioPlot.frame;//The frame in the wholeScreenshot you want to save\nCGImageRef imageRef = CGImageCreateWithImageInRect([wholeScreenshot CGImage], croppedRect);\nUIImage *croppedScreenshot = [UIImage imageWithCGImage:imageRef];\nCGImageRelease(imageRef);\n[myImageView setImage:croppedScreenshot];\n```\n. ",
    "dkavraal": "Have you found a way for this?\n. ",
    "edwardtoday": "I have an error with the same error code -66628.\nIn an app that has both microphone input and audio playback views in 2 tabs. No matter what view I enter first, it works all right. Things are still working when I switch to the second tab.\nWhen I switch back to the first tab, the error occurs.\nERROR:     [0x1995d4310] 1230: AUIOClient_StartIO failed (-66628)\nError: Microphone failed to start fetching audio (-66628)\nNo exception threw. App crashes...\n. Here is my diff to make things work.\n-  [[AVAudioSession sharedInstance] setCategory:AVAudioSessionCategoryPlayback\n-                                         error:&error];\n+  [[AVAudioSession sharedInstance]\n+      setCategory:AVAudioSessionCategoryPlayAndRecord\n+            error:&error];\n. ",
    "JamieCruwys": "The memory leak(s) have been fixed in the following merged pull requests: https://github.com/syedhali/EZAudio/pull/149 and https://github.com/syedhali/EZAudio/pull/150.\n. ",
    "joeldrotleff": "I'm seeing this issue as well. Were you able to find a fix?\n. sanderman123's solution didn't work for me, but this did: http://stackoverflow.com/a/11577849/737387\nAdded a volume property to EZAudioFile, then added this code to the readFrames:audioBufferList:bufferSize:eof method:\nfloat desiredGain = self.volume;\n    for(UInt32 bufferIndex = 0; bufferIndex < audioBufferList->mNumberBuffers; ++bufferIndex) {\n        float *rawBuffer = (float *)audioBufferList->mBuffers[bufferIndex].mData;\n        vDSP_Length frameCount = audioBufferList->mBuffers[bufferIndex].mDataByteSize / sizeof(float);\n        vDSP_vsmul(rawBuffer, 1, &desiredGain, rawBuffer, 1, frameCount);\n    }\nMake sure to include the Accelerate framework\n. Problem solved, just had to set view.opaque = YES for the subviews of EZAudioPlotGL.\n. ",
    "goham": "I have the same problem and how to fix it?\n. ",
    "bishalg": "im getting the same issue, in my case i'm using the project as Framework, and EZAudio is part of the framework.. In my case I had not give frame to EZAudioPlotGL, giving the frame in init solve the issue for me.. ",
    "maheshverma": "@zjszyms, If you found any solution, please update. \n. @sanderman123 Thank you, It works :)\n. ",
    "zjszyms": "@sanderman123 \u8c22\u8c22\uff01\n. ",
    "KevinJacob": "@lususvir with your method it is no longer possible to play your recording till you close the file correct? If so do you know of anyway to play the recording between appends?\n. Ya I don't think it would. Since by not closing the recording you have no file to read in. I tired to use data buffers and load the NSData into AVAudioPlayer but that didn't work.\n. ",
    "subinkk08": "i need to merge two files . i converted to swift code . 2nd audio is  not merging and exportSession.status is failed\ncan any one knows please help me \n```\nfunc combineTwoFile(session:Int) { \nif session > 0 { \nlet composition: AVMutableComposition = AVMutableComposition()\nvar appendedAudioTrack: AVMutableCompositionTrack =    composition.addMutableTrackWithMediaType(AVMediaTypeAudio, preferredTrackID: kCMPersistentTrackID_Invalid)\nlet path1 = NSSearchPathForDirectoriesInDomains(\n    .DocumentDirectory, .UserDomainMask, true\n    ).first\nlet orginalFile = NSURL.fileURLWithPath(path1! + \"/EZAudioTest_0.m4a\")\n let currentFile = NSURL.fileURLWithPath(path1! + \"/EZAudioTest_\"+session.description+\".m4a\")\n let originalAsset: AVURLAsset = AVURLAsset(URL: orginalFile, options: nil)\n let newAsset: AVURLAsset = AVURLAsset(URL: currentFile, options: nil)\nvar tracks1 =  originalAsset.tracksWithMediaType(AVMediaTypeAudio)\nvar tracks2 =  newAsset.tracksWithMediaType(AVMediaTypeAudio)\nvar assetTrack1:AVAssetTrack = tracks1[0] as! AVAssetTrack\n var assetTrack2:AVAssetTrack = tracks2[0] as! AVAssetTrack\nvar duration1: CMTime = assetTrack1.timeRange.duration\n var duration2: CMTime = assetTrack2.timeRange.duration\nvar timeRange1 = CMTimeRangeMake(kCMTimeZero, duration1)\n var timeRange2 = CMTimeRangeMake(duration1, duration2)\ntry? appendedAudioTrack.insertTimeRange(timeRange1, ofTrack: assetTrack1, atTime: kCMTimeZero)\ntry? appendedAudioTrack.insertTimeRange(timeRange2, ofTrack: assetTrack2, atTime: duration1)\n let exportSession: AVAssetExportSession = AVAssetExportSession(asset: composition, presetName: AVAssetExportPresetAppleM4A)!\n\nif exportSession == false {\n    // do something\n    return\n}\n// make sure to fill this value in\nexportSession.outputURL = NSURL.fileURLWithPath(path1! + \"/EZAudioTest_0.m4a\")\nexportSession.outputFileType = AVFileTypeAppleM4A;\nexportSession.exportAsynchronouslyWithCompletionHandler({() -> Void in\n    // exported successfully?\n    switch exportSession.status {\n    case .Failed: break\ncase .Completed: break\n    // you should now have the appended audio file\n\ncase .Waiting: break\n\ndefault: break\n}\n\n//  var error: NSErrorPointer? = nil\n})\n }\n }\n```\n. Failed to write audio data to recorded audio file ('insz') i got this error iphone 5  iOS 9.2 .\n. ",
    "KelvinJin": "@Newsid The frequency of each \"amp\" is calculated separately. They depend on the \"SampleRate\" and the \"bufferSize\" which is the input size of the FFT function. Say if you have 44100Hz sample rate, and bufferSize is 1024, then the corresponding frequency will be 0Hz, 44100/1024=44Hz, 44*2=88Hz, etc...You would have 512 frequency values (since you only have 512 amp values). \n0: 0.00001\n44Hz: 0.00000\n88Hz: 0.00000\n132Hz: 0.00000\n...\n...\nCheck this for more details.\n. ",
    "Newsid": "@KelvinJin Thank you very much! Your explanation is very usefull for me!\nedit: \nI need a restricted range of frequencies from base band to \u22481khz but with hight precision\nSo I thought, if I set sample rate to \u22482000Hz I will have: 0Hz, 2000/1024\u22481.95Hz, 1.95*2\u22483.9Hz etc..\nI tried to change this number (everywhere i found 44100 I replaced with 2000) but nothing changed. I played the same note to the phone's mic and I had the same plot (index 35 of the amp array set to 1)\nSo.. do you know how can modify this parameters?\nThank you!\nRe edit:\ni think i figure it out\nthe method of EZMicrophone\n-(Float64)_configureDeviceSampleRateWithDefault:(float)defaultSampleRate\ndid not set any sample rate, so i modified in this way:\n-(Float64)_configureDeviceSampleRateWithDefault:(float)defaultSampleRate {\n  Float64 hardwareSampleRate = defaultSampleRate;\n  #if TARGET_OS_IPHONE\n    // Use approximations for simulator and pull from real device if connected\n    #if !(TARGET_IPHONE_SIMULATOR)\n    // Sample Rate\n    NSError *error;\n    [[AVAudioSession sharedInstance] setPreferredSampleRate:defaultSampleRate error:&error];\n    if (error) {\n        hardwareSampleRate = [[AVAudioSession sharedInstance] sampleRate];\n    }\n    #endif\n  #elif TARGET_OS_MAC\n    hardwareSampleRate = inputScopeSampleRate;\n  #endif\n  return hardwareSampleRate;\n}\nI did not reached the exact result i'm looking for, but it is a little step :)\n. ",
    "alexgizh": "@lususvir Did you find a solution to this problem? If so, I'd be really grateful if you shared it here.\n. @lususvir would you be kind enough to share the piece of code used for appending?\n. Thanks a lot! Having no other options for such a small amount of time, this will have to do! :)\n. ",
    "ZaidPathan": "@justin999 ,How did you find the solution, would you please explain here?\n. ",
    "erikcore": "Hmm, I'm having a similar issue. Care to elaborate on how you resolved this?\n. Experiencing the same issue. Would love some insight. \n. ",
    "dddx80": "I'm also having the same issue. I moved parts of the code from sample project to my own project and every time I press record, program crashes with this error: Error: Failed to set client format on recorded audio file (-66563)\n@andresteves and @erikcore  (how) did you solve this? Thanks\n. thanks for your response syedhali. This is the line of code where I initialize the recorder:\nself.recorder = [EZRecorder recorderWithDestinationURL:[NSURL fileURLWithPath:kAudioFilePath] sourceFormat:self.microphone.audioStreamBasicDescription destinationFileType:EZRecorderFileTypeM4A];\nThis is the line giving me the error but when i comment this line out, i don't get that error anymore. However, the recorder does not record if i don't initialize it. \nI used your example files from EZAudioRecordExample project. I made sure everything from RecordViewController.m and RecordViewController.h was properly moved. I also added the proper switches, buttons ad labels. But still same error. \nIs there any other way to initialize the recorder?\n. ",
    "griches": "If you have copied and pasted make sure you are initialised the microphone with: EZAudioPlotGLself.microphone = [EZMicrophone microphoneWithDelegate:self];\nThat's what fixed the crash for me.\n. I have this issue sporadically. Cannot find the cause annoyingly and happens rarely enough that I can't debug it.\n. This fixed it for me: https://github.com/syedhali/EZAudio/issues/173\n. ",
    "jorman86": "same problem here, if someone has any clue about this... it will be appreciated\n. ",
    "AlexEdunov": "Yes, of course. I'm sorry, next time I'll use gitflow, instead committing to master.\n. ",
    "orenk86": "update:\nI have played a bit more with the audio format params, and have gotten a new error. Here's my new description:\nAudioStreamBasicDescription audioFormat;\n    audioFormat.mSampleRate = 8000;\n    audioFormat.mChannelsPerFrame = 1;\n    audioFormat.mBitsPerChannel = 16;\n```\naudioFormat.mBytesPerPacket = 2;\naudioFormat.mBytesPerFrame = 2;\naudioFormat.mFormatID = kAudioFormatLinearPCM;\naudioFormat.mFramesPerPacket = 1;\naudioFormat.mFormatFlags = kAudioFormatFlagsNativeFloatPacked;\n```\nThe app still crashes and gives this message:\nError: Failed to set client format on recorded audio file ('fmt?')\n. tried that as well, did't work. I ended up using this library only for the UI and implementing the audio recording and processing separately with a AVAudioRecorder...\n. ",
    "jonasman": "looks like self.microphone = [EZMicrophone microphoneWithDelegate:self withAudioStreamBasicDescription:audioStreamBasicDescription startsImmediately:NO]; does nothing.\nyou need to do self.microphone.audioStreamBasicDescription =  audioStreamBasicDescription;\n. ",
    "GordonHuangYong": "@syedhali \n    for (int i = 0; i < bufferList->mNumberBuffers; i++)\n    {\n        void *channelBuffer = bufferList->mBuffers[i].mData;\n       //save the pcm file. but playing noise.\n        [self apppendAdpcmData:[NSData dataWithBytes:channelBuffer length:bufferList->mBuffers[i].mDataByteSize] toFile:@\"luck.pcm\"];\n    }. ",
    "moduscreate": "Never mind. Found the problem, didn't include the pch :)\n. ",
    "jcavar": "Yeah, I understand. I will close this issue then. Thanks.\n. ",
    "quinnthomson": "sorry, GH shenanigans...\n. ",
    "hadwanihardik": "Resolved issue. resolution is in same link\n. ",
    "amitmalhotra": "@andresteves how did you calibrate the playback rate speed? \n. ",
    "ChiragCreative1": "@andresteves can you share some code, i dont know how to use superpowered with audioPlot.\ni want to play back speed like -0.5, 0.5, 1.0 and 1.5. ",
    "abeldomingues": "I tried taking a whack at this a few months back.  If you set up a bridging header for importing EZAudio.h, it's simply (or at least, should be) of matter of translating your EZ calls to Swift syntax.  In my case I hit a wall when it came to calling the delegate methods, specifically: how to convert something like \"hasAudioReceived:(float**)buffer\" to \"buffer:UnsafeMutablePointer\". If I made the call like that (i.e. leaving out the hasAudioReceived external parameter name) the app would build and run, but the the delegate call would silently fail (no sound); whereas if I changed it to \"hasAudioReceived buffer:UnsafeMutablePointer\" (including the external parameter name) the compiler would choke because the method signatures were now in conflict. I never got as far as addressing the fact that the original parameter is actually a pointer to a pointer, and perhaps that's why the first version fails silently. In any case, I moved on to other things. But: Swift's handling of pointers has shifted around quite a bit since last June, and things may have changed with Swift 1.2... it would be interesting to hear if anyone else has had better luck.\n. Assuming you mean after recording has stopped and you have a saved - and closed - file containing that recording's data, you would (I believe) simply create an EZFile object to open the file for reading; retrieve its _totalFrames property, and calculate your duration from that...\nThat's sort of overkill if all you want to do is get the file's duration, though, so you might instead consider using Apple's Extended Audio Services API directly: ExtAudioFileOpenURL to open the file for reading, then ExtAudioFileGetProperty(kExtAudioFileProperty_FileLengthFrames) to get its duration in sample frames, then ExtAudioFileDispose to close the file... \nAnd sure, you can call these Objective-C (EZAudio) and C (Extended Audio File Services) APIs from your own Swift code, as long as (a) you're conversant with translating ObjC calls into Swift; (b) you've included a bridging header (in the case of EZAudio or other ObjC frameworks) so your Swift classes can access the necessary ObjC classes; and (c) you're not doing any raw C stuff that Swift hasn't quite figured out how to handle yet (like passing pointer addresses around to delegate methods, per my original comment.) Your go-to reference for all of this is going to be Apple's 'Using Swift with Cocoa and Objective-C'.\nFundamentally, of course, Audio Programming means C/C++, not Swift/ObjC: EZAudio - and Novocaine and The Amazing Audio Engine, etc (not to mention Apple's own higher level audio frameworks like AVAudioEngine) - is essentially a well-thought-out wrapper around lower level C routines. When it comes to pushing buffers of data around at audio speeds, object calls, method table lookups and so on are simply too time consuming. So there's a reason why Core Audio, OpenGL ES and other highly performant frameworks are C APIs, not Obj-C/Swift (which in turn is why we need toolkits like EZAudio!)\n. cool, I think that\u2019s gonna help a lot of people\u2026 +1 for adding some examples to the official repo\u2026\n-a\n\nOn Jul 13, 2015, at 7:23 PM, Syed Haris Ali notifications@github.com wrote:\nHey everyone, I created a separate repo (EZAudio-Swift https://github.com/syedhali/EZAudio-Swift) to demonstrate how to use EZAudio in a swift project using a bridging header. It shows how to use an EZMicrophone + EZAudioPlotGL to get something similar to the EZAudioOpenGLWaveformExample https://github.com/syedhali/EZAudio/tree/master/EZAudioExamples/iOS/EZAudioOpenGLWaveformExample:\n https://cloud.githubusercontent.com/assets/1275640/8663059/4094f3ec-297b-11e5-94d9-602d62018eee.gif\nAnyone feel strongly that there should be at least one Swift example included in the official EZAudio repo?\n\u2014\nReply to this email directly or view it on GitHub https://github.com/syedhali/EZAudio/issues/159#issuecomment-121088967.\n. \n",
    "scheung38": "Any idea how to capture the duration of recording, from start to finishing time? Ideally in Swift.\n. Because I started looking into AudioKit since they have swift support.... cool if Swift is supported because hard enough to learn swift as it is. But I think they mentioned they also use EZAudio?\n1. I just want to Start Record, Stop Record. Then after stopped, I could get the average dB audio level and duration of recording.\n2. Want something like POVoiceHUD has that Start, and Stop when after say 10 secs elapsed and no audio.\n. Also any idea why XCode suddenly now broken for me:\n\n. Yes please show thanks\n. No, just record audio using EZAudio with mic and at the same time record motion using Accelerometer and then get snapshot of dB and motion data once recording done\n. ",
    "wlaurance": "Awesome I appreciate the updates @syedhali \n. ",
    "vivekCZ": "I am drwaing multiple plot in once screen. I have same issue. When I popviewcontroller and again goes to same. It is not drawing any plot.But when I start recording its shows waves. I am getting GL ERROR: 0x0502. I have tryed using EZAudioPlot but it is not giving me same output as EZAudioPlotGL\n. ",
    "yuedong56": "i have the same error. ",
    "gotuckgo": "I had the same weirdness and error.  For me it came down to declaring microphone a strong instead of a weak reference.  When I looked at microphone in the debugger after it was instantiated it looked fine, but microphone was getting freed up by the OS at some point before the recorder init call and providing bad values (all zeros.)\n. ",
    "hsin919": "Hi I found another good framework for bit detection named superpowered sdk. Here is the sample code for SuperpoweredOfflineAnalyzer FYI\n. ",
    "louis49": "Thanks.\nI will add a bug on microphone tomorrow ;-)\n. ",
    "bennyguitar": "Here's what I do, with the FFT example:\n``` objc\n// For an FFT, numSamples must be a power of 2, i.e. is always even\n    int nOver2 = bufferSize/2;\n// Pack samples:\n// C(re) -> A[n], C(im) -> A[n+1]\nvDSP_ctoz((COMPLEX*)data, 2, &_A, 1, nOver2);\n\n// Perform a forward FFT using fftSetup and A\n// Results are returned in A\nvDSP_fft_zrip(_FFTSetup, &_A, 1, _log2n, FFT_FORWARD);\n\n// Convert COMPLEX_SPLIT A result to magnitudes\nfloat *amp = calloc(nOver2, sizeof(float));\nfloat maxMag = 0;\nint maxIdx = 0;\n\nfor(int i=0; i<nOver2; i++) {\n    // Calculate the magnitude\n    float mag = _A.realp[i]*_A.realp[i]+_A.imagp[i]*_A.imagp[i];\n    float m = maxMag;\n    maxMag = mag > maxMag ? mag : maxMag;\n    if (m != maxMag) {\n        maxIdx = i;\n    }\n}\nfor(int i=0; i<nOver2; i++) {\n    // Calculate the magnitude\n    float mag = _A.realp[i]*_A.realp[i]+_A.imagp[i]*_A.imagp[i];\n    // Bind the value to be less than 1.0 to fit in the graph\n    amp[i] = [EZAudioUtilities MAP:mag leftMin:0.0 leftMax:maxMag rightMin:0.0 rightMax:1.0];\n    if (i == frames) {\n        break;\n    }\n}\n\nCGFloat freq = (44100.0f/bufferSize)*maxIdx;\n\n```\n44100 is the sample rate, and depending on your source of audio you can retrieve that dynamically. Here's how to get it from an EZAudioFile:\nobjc\nCGFloat rate = self.audioFile.clientFormat.mSampleRate;\nEDIT: The above gets you the largest amplitude, aka the most prevalent frequency in that audio buffer. If you're looking for highest one, then instead you'll set maxIdx to the one when mag > 0 in the last for loop.\n. Just did, and still nothing from my speaker. Let me post some code using 0.4.0 and see if this helps:\nStarting the output:\nobjc\n[[EZOutput sharedOutput] setClientFormat:self.audioFile.clientFormat];\n[[EZOutput sharedOutput] setDataSource:self];\n[[EZOutput sharedOutput] startPlayback];\nI've also done it with this:\nobjc\n[[EZOutput sharedOutput] setDevice:[EZAudioDevice outputDevices][0]];\nIt won't play through the speaker currently. Should I be using an instance of EZOutput instead of relying on the sharedOutput? self.audioFile is not nil, and actually contains audio. I can verify this by plugging in headphones and listening just fine to the output.\nEDIT this is the NSLog of all outputDevices when I use my iPhone:\n\"<EZAudioDevice: 0x174032860> { port: <AVAudioSessionPortDescription: 0x1740060e0, type = Speaker; name = Speaker; UID = Speaker; selectedDataSource = (null)>, data source: (null) }\"\nNot sure if selectedDataSource and data source are supposed to be null here.\n. Wow, I'm an idiot - thanks for your help. Turns out I wasn't setting the AVAudioSession stuff at all.\n. ",
    "pjebs": "AVAudioRecorder only captures the mic audio and not the speakers.\n. Any news on this issue?\n. ",
    "jrader22": "You're accuracy is dependent on your sample rate (say 44100) and your number of bins (say 512). That means each bin is about 86 Hz (44100 / 512), so you're accuracy would only be exactly that, 86 Hz. Having enough accuracy to only be off by 1.4 Hz is pretty accurate actually. \n. Ahh. I'm an idiot. maxMag should be a float. I made it an int.\n. ",
    "scamhaji": "Thanks for your answer. I've added this line in PassThroughViewController.m (around line 52) :\n[EZOutput sharedOutput].clientFormat = [EZMicrophone sharedMicrophone].audioStreamBasicDescription;\nUnfortunately, this leads to an error : \nError: Failed to set input client format on mixer audio unit (-10868)\n. The soundflower virtual card is open source and can be found here:\nhttps://rogueamoeba.com/freebies/soundflower\n. Hi, did you have a chance to look into this issue? \n. ",
    "JanX2": "@scamhaji EZAudio has been deprecated. See the current README.\n. Is there a reason, why this hasn\u2019t been merged? LGTM. :)\n. That is sad news indeed. But better to deprecate, then to have people hope for a maintained EZAudio. :)\nThank you for sharing your audio code! It is greatly appreciated.\n. ",
    "dedjw": "Hey Haris, hope you are well. We're running with this change in out local version. Thought you might want it.\n. cool! Nice timing then.\nOn Mon, Jul 13, 2015 at 5:07 PM, Syed Haris Ali notifications@github.com\nwrote:\n\nMerged #190 https://github.com/syedhali/EZAudio/pull/190.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/syedhali/EZAudio/pull/190#event-354905902.\n. \n",
    "megastep": "I just found a bug that surfaced in AudioKit and included a fix for it, by the way. :)\n. Well it's been a while so I hope this is still relevant!\n. ",
    "robotsu": "@syedhali  Thank you for your very quick reply. Sorry I am using 0.5, so if I use 1.0.0+, my problem will not happen right?\nThanks.\n. Sorry I am using 0.0.5, shows in VERSION file.\n. ",
    "andre991": "Have any prediction to be resolved?\n. ",
    "dannyighsu": "The previously mentioned Obj-C wrapper actually does work - I wasn't using it correctly when I had posted the issue. So that's a temporary fix. Another option is to avoid using this by calling EZAudioFile's .getWaveformData manually and then calling whatever you need after that (i.e. audioPlot.updateBuffer(...))\n. ",
    "knaoe": "This swift code works for me.\nswift\nself.audioFile = EZAudioFile(URL: fileUrl)\nlet waveformData = self.audioFile!.getWaveformData()\nself.audioPlot.updateBuffer(waveformData.buffers[0], withBufferSize: waveformData.bufferSize)\n. I faced this issue when app have multi instances of EZMicrophone(). (iOS9.2 and iPhone5S, 6S)\nI solved this issue by using EZMicrophone.sharedMicrophone() instead of EZMicrophone().\n. ",
    "fform": "The comment from @kenzukan works for me. I was trying to follow the EZAudioWaveformFromFileExample project and it has you use the block callback with the buffer. For whatever reason, that always ends up with an empty buffer and a zero length in Swift.\nOften I've seen buffers come in as reference pointers but that doesn't seem to work in the EZAudioWaveformDataCompletionBlock! which gets written as\n(UnsafeMutablePointer<UnsafeMutablePointer<Float>>, Int32) -> Void\n. Can confirm this with 1.1.5 and 1.1.2 using XCode 7.2. Changing the path fixes it.\nFWIW, I was using 1.1.2 before and it was working with XCode 7.2 but I had to clean out my Pod folder on a branch change. When I reinstalled 1.1.2 fresh, it no longer would compile.\n. ",
    "varex33": "Hi guys,\nThanks for these comments, they helped me get unstuck with swift. I agree, the @kenzukan  solution is the only one that worked for me however when run my app the waveform doesn't animate, the audio file plays but the wave is just static image. Any suggestion on that?\n. @martatenes Thanks so much for replying, I haven't been able to achieve it yet so will give a try to this approach.\n. ",
    "martatenes": "Hi guys, I am also stucked with swift in this point. I need to show the progress of the waveform meanwhile the audio is playing :\\\nAny new solution with this?\n. @varex33  I ended up with a solution. If you want animate the waveform you can implement the EzAudioPlayerDelegate and then update the audio plot buffer inside this function:\n```\n func audioPlayer(audioPlayer: EZAudioPlayer!, playedAudio buffer: UnsafeMutablePointer>, withBufferSize bufferSize: UInt32, withNumberOfChannels numberOfChannels: UInt32, inAudioFile audioFile: EZAudioFile!) {\n    dispatch_async(dispatch_get_main_queue(), { () -> Void in\n\n        self.audioPlot.updateBuffer(buffer[0], withBufferSize: bufferSize)\n    })\n}\n\n```\n. ",
    "hyd00": "Please someone check this issue https://github.com/syedhali/EZAudio/issues/246\nsame problem occurring now in iPhone 6s and 6S Plus\n. ^Exactly! The examples provided work perfect on ALL the devices (including 6S and 6S Plus)\n. ",
    "cmmartin": "Make sure you set the category and activate your AVAudioSession instance before you instantiate the EZMicrophone or any other EZAudio specific code. Even if they are in the same method. I ran into this issue on the 6s and was able to resolve it that way\n. Same issue here. I just reverted to 1.1.2 for now...\npod 'EZAudio', '1.1.2'\n. ",
    "livedeveloper": "@eikebartels  do u know this way?\n. hi, @ysfox  do u know this way?. ",
    "imownbey": "Would love this\n. I am not totally sure that what you merged in actually works. There are still no header files I think there are a couple steps left to do\n. I think this might be related to this bug: https://github.com/syedhali/EZAudio/issues/217\n. Yes\n. Although it seems to happen regardless of if I do.\n. I have found the same as well (swift 2.0 and Xcode 7 GM).\nThe error happens when calling stopFetchingAudio() before a segue. This causes the transition to behave weirdly (it essentially never completes and appears like the new controller isn't put fully on the stack)\nUsing the delegate callbacks in the following order:\n1. Stop mic\n2. On microphone(microphone: EZMicrophone!, changedPlayingState isPlaying: Bool) close file\n3. on recorderDidClose(recorder: EZRecorder!) do segue\ndoesn't fix the issue either weirdly. But delaying the segue a couple seconds WILL fix the issue. \nThis is also using the sharedMic\n. ",
    "AndrewSB": "@imownbey figured this out #211, @syedhali would love to have it merged!\n. @imownbey saw your fork and pulled those changes in to the PR https://github.com/AndrewSB/EZAudio/commit/5f283af9d322cf2f12a833f20582911745117ae8. \nThank you for pointing that out :smiley: \n. I think I have it working - just tried building with this library and it seemed to work \n. You are right @syedhali, it would need a target for OS X, I have not implemented it because I've never targeted OS X before.\nI could try it, but I think someone with more experience on integrating Carthage into OS X projects should take a stab at it.\n. @blender your changes look good, merged :smile: \n. hey @syedhali!\nlove the work you've done, any chance you've had a chance to look at these changes yet?\nThanks,\nAndrew\n. \ud83d\ude00\ud83d\ude00\ud83d\ude00\n. ",
    "danishin": "+1\n. ",
    "blender": "Awesome! :+1: \n. I have added a OSX target and made a pull request on @AndrewSB 's fork\n. :tada: :tada: :tada: :tada: :tada: \n. Hi, this happens for me as well.\n. Frameworks are only supported from iOS 8 on.\n. ",
    "picciano": "Yes, I do this as the app launches.\n```\n    //configure audio session\n    NSError *setCategoryError = nil;\n    BOOL setCategorySuccess = [[AVAudioSession sharedInstance]\n                               setCategory:AVAudioSessionCategoryPlayAndRecord\n                               withOptions:AVAudioSessionCategoryOptionDuckOthers|AVAudioSessionCategoryOptionDefaultToSpeaker|AVAudioSessionCategoryOptionAllowBluetooth\n                               error:&setCategoryError];\nif (setCategorySuccess) {\n    NSLog(@\"Audio Session options set.\");\n} else {\n    NSLog(@\"WARNING: Could not set audio session options.\");\n}\n\n```\n. ",
    "saiday": "Me too.\nBut, how do you ensure that malloc: *** error for object 0x793e3a04: incorrect checksum for freed object - object was probably modified after being freed. caused by numberOfFramesToRead increasing?\n. ",
    "dcunited001": "Well I didn't change my implementation, but the audio is playing now.  Not sure what changed, but issue fixed.\n. ",
    "ronjumola": "My bad I thought I was downloading an MP3 file but it's actually an HTML file. Closing now\n. ",
    "ashishkumargit": "Hi,\nI also need this. Can any one suggest me how to impliment EZAudio in xamarin.ios?\n. ",
    "Harigharan": "Anybody tried to bind this iOS library to Xamarin?\n. ",
    "casamia918": "I tried to bind this library but end in fail.\n(This is not the \"How to bind guide\". You should study binding objective-c library by reading Xamarin official guide. \nhttps://developer.xamarin.com/guides/cross-platform/macios/binding/objective-c-libraries/\nI wrote this to share my library binding experience and I assumed that you've already know about binding process)\nTo be fortunately, EZAudio already shipped it's precompiled framework file. So, what I have to do is just create APIDefinition.cs code using sharpie.\nAt first, when I tried to run \"sharpie bind\" command using header files as source, like this\n$ sharpie bind -output sharpie -namespace EZAudioBinding -sdk iphoneos10.0 ./EZAudio/*.h\nthe result is fail. The reported error messages are related with __tg_sqrt, __tg_fmax blabla.\n```\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS10.0.sdk/usr/include/simd/common.h:1381:92: error: call to\n      'tg_fmin' is ambiguous\nstatic double __SIMD_ATTRIBUTES vector_reduce_min(vector_double3 __x) { return __tg_fmin(__tg_fmin(__x.x, __x.z), __x.y); }\n/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/SDKs/iPhoneOS10.0.sdk/usr/include/simd/common.h:1419:81: error: call to\n      'tg_fmax' is ambiguous\nstatic float __SIMD_ATTRIBUTES vector_reduce_max(vector_float2  __x) { return __tg_fmax(__x.x, __x.y); }\n```\nAfter googling, I found that these errors are related with clang compiler.\nhttps://bugzilla.xamarin.com/show_bug.cgi?id=34594\nSo, I attached the compiler options at the end of sharpie bind command\n```\n$ sharpie bind -output sharpie -namespace EZAudioBinding -sdk iphoneos10.0 ./EZAudio/*.h  \\\n\n -c \\\n-DINFINITY=1e500 \\\n-D__SIMD_MATH_HEADER__ \\\n-D__SIMD_LOGIC_HEADER__ \\\n-D__SIMD_GEOMETRY_HEADER__ \\\n-D__SIMD_BOOLEAN_TYPE__=_Bool\n\n```\n\nAfter that, the __tg_ related errors are gone. But still 1 error occurs. (The bind result shows lots of warning messages but they can be ignored, so I just proceeded.)\n```\nIn file included from /var/folders/lf/4frwgk3n4pb72b8z54ty0r740000gn/T/tmp7b81c06b.h:8:\n[PathToLibrary]/EZAudio/EZAudioOSX.h:26:9: fatal error: 'EZAudioOSX/EZAudio.h' file not found\nimport \n```\nAgain, I searched the google and found that these are framework related problem.\nhttps://forums.xamarin.com/discussion/40207/objective-sharpie-dosent-provide-bindings-for-framework-file\nSo, I modified to this command and the sharpie bind is end with success.\n$ sharpie bind -output sharpie -namespace EZAudioBinding -sdk iphoneos10.0 -f [FrameworkBuildResultFolderPath]/EZAudioiOS.framework -fx-umbrella EZAudioiOS.h \\\n-c \\     \n-DINFINITY=1e500  \\\n-D__SIMD_MATH_HEADER__   \\\n-D__SIMD_LOGIC_HEADER__  \\ \n-D__SIMD_GEOMETRY_HEADER__  \\   \n-D__SIMD_BOOLEAN_TYPE__=_Bool\n(You can get the  [FrameworkBuildResultFolderPath] by opening the EZAudio.xcodeproj, right click on the Products/EZAudioiOS.framework and click the 'Show in finder' menu item)\nAfter that, I can get APIDefiinition.cs and StructsAndEnums.cs files. Of course, I copy this codes to my Xamarin's .iOS binding project and proceed to manual verifying job.\nAt the first build, I got 50~60 errors. I fix these errors one by one. \n- Every C functions are private codes, so I commented out. (I do not need the P/Invoke call at this API level). \n- Lot's of method parameters are pointer. But as C# reference-type are call by references, we need to remove pointer syntax (*). In some cases, the type used as pointers are implemented  corresponding Xamarin.iOS c# type. So you have to find the corresponding type by reading xamarin.ios Doc. Some pointers are needed \"ref\" keyword explicitly.\n- The iOS Accelerate framework are not fully supported by xamarin.iOS. In our case, EZAudioFFT class is using type defined in DSP library of Accelerate framework (FFTSetup, COMPLEX_SPLIT) . In my case, I don't need FFT so I comment out the entire EZAudioFFT class. Pass.\nThis is the my binding project source code, after the above jobs.\nhttps://github.com/casamia918/EZAudioBinding\nAfter that, I got the last error message.\nError BI1017: btouch: Do not know how to make a signature for System.Single** in method 'FloatBuffersWithNumberOfFrames' (BI1017) (EZAudioBinding)\nThe error message are related with float* type. Many EZaudio methods are using float* to contain audio buffers. This is essential part but after read this below thread, I realized that I can't fix this problem.\nhttps://forums.xamarin.com/discussion/48065/btouch-error-when-binding-unsafe-code\nI'm really unhappy to get this result. Hope my exprience would be useful to someone.\n. ",
    "aronsuarez": "Do you have a solution for the problem?\n. ",
    "player500": "Found the answer: have to replace microphone.output = speaker\nwith speaker.dataSource = microphone\n. ",
    "wstcyx": "how to fix it ? i don't understand\n. Did you solve it ?\n. help~\n. found answer //    [[EZMicrophone sharedMicrophone] setOutput:[EZOutput sharedOutput]];\n[EZOutput sharedOutput].dataSource = [EZMicrophone sharedMicrophone];\n. ",
    "Charlisim": "I had the same problem because I was stop fetching audio and stop the recording before the recorder had updated last current time, so I set a flag, recordingShouldStopRecording \nand in recorderUpdatedCurrentTime check and rally stopFeching audio and recording \n```\n  func recorderUpdatedCurrentTime(recorder:EZRecorder){\n    NSOperationQueue.mainQueue().addOperationWithBlock { () -> Void in\n    self.recordButton.progressStatus = CGFloat(recorder.duration / 30.0)\n    if self.shouldStopRecording{\n        self.microphone.stopFetchingAudio()\n\n        self.recorder.closeAudioFile()\n        self.shouldStopRecording = false\n        self.recordButton.progressStatus = 1.0\n\n    }\n\n}\n\n}\n``\n. Where are you stopping fetching audio and close the audio file? Since I moved the code that stops audio toEZMicrophoneDelegatethere are no crashesfunc microphone(microphone: EZMicrophone!, hasBufferList bufferList: UnsafeMutablePointer, withBufferSize bufferSize: UInt32, withNumberOfChannels numberOfChannels: UInt32)`\nAfter appending data from buffer to the recorder I stop the recording and close the file.\nAlso you need to know that there is a method(not well documented) to choose if there is an error in the recording the app should crash or not.\nEZAudioUtilities.setShouldExitOnCheckResultFail(false), by default is set to true\n. ",
    "amit-shelgaonkar": "having the same problem on ios 8 also with version 1.1.5 \n. ",
    "Mindbowser": "Any update here? We are also waiting to get this fixed asap, as it is affecting lot of users.\n. @syedhali We are also having similar requirement, let us know if you have better way of handling it, quick help is appreciated.\n. ",
    "imphila": "i meet the same problem.You can try this in the function recorderUpdatedCurrentTime(recorder: EZRecorder!)\nfunc recorderUpdatedCurrentTime(recorder: EZRecorder!)\n    {\n        var formarredCurrentTime:String?\n        formarredCurrentTime = recorder.formattedCurrentTime\n        dispatch_async(dispatch_get_main_queue())\n        {\n            self.timeLabel.text = formarredCurrentTime\n}\n}\ni  don't meet the problem anymore but i don't know why it can solve the problem. I think maybe it is associated with thread.\nif anynoe knows please tell me.\n. ",
    "benlachman": "Interestingly while we've now had 2K+ crashes with this issue, it doesn't seem to be easily reproducible, nor are users very aware that it's crashing. This makes me think it it must be happening on app switch (foreground->background).\n. ",
    "Weezlo": "@vfxdrummer Thanks for your update Tim ! you saved me from some really bad headaches ! (i had quite a lot aleady)\nI was casting mData to Floats instead of Signed Ints, that was my mistake...\nBy the way, i'm also sending audio data to Nuance Speech Recognition API :)\nI'm wondering, are you streaming audio speech to a remote server and doing real time transcription with it ? For the moment I send the whole Wav file once recorder but i was also thinking about streaming data. What API are you using? SpeechMagic SDK?\nThank you again.\nAnis\n. ",
    "d-jiang": "same\n. ",
    "vineethvijayan": "Same here too\n. Any update on above? Its showing issue in iOS 9.2, with multiple devices\n. Any update on this issue? Same issue in 9.2.1 too \n. ",
    "DavidDeBels": "Having the same issue\n. ",
    "WFurthado": "You have to delete the previously recorded audio before recording a new one.\n. ",
    "Pierre-Loup": "Hi,\nI have the same problem.\nI use Xcode 7.1\nOn the simulator or the device the app crashes when I try to record a wav file.\nJust try changing the file format from EZRecorderFileTypeM4A to EZRecorderFileTypeWAV in the EZAudioRecordExample project.\n. ",
    "LiweiDong": "same problem\n. ",
    "nick-thompson": "Just ran into this as well. Any updates?\n. I see, that makes much more sense. Thank you!\n. One more question @dxclancy: within the loop issuing calls to readFrames, can I assume that once eof is reached that the AudioBufferList has been fully populated? Or is the actual read operation happening off on another thread or something? That is, must I wait for the final delegate call before I can assume the AudioBufferList has been fully populated?\n. Thank you @dxclancy! That was super helpful. I got everything working now; I'll go ahead and close this issue.\n. ",
    "hansonboy": "same problem\n. find the solution to the problem at the site: http://stackoverflow.com/questions/27791605/ezaudio-framework-error-couldnt-initialize-output-unit-fmt.\ncase EZRecorderFileTypeWAV:          \n  asbd = [EZAudioUtilities stereoFloatInterleavedFormatWithSampleRate:sourceFormat.mSampleRate];\nupdate the absd as the site. \n. ",
    "blixt": "We are also getting this issue in our app when trying to play a specific MP3 file (attached below). The app just crashes and there seems to be no way to detect that it will happen. Before playing this file the app is barely using any memory and it doesn't crash for other, much larger, files.\n84b5ba842668ea22a90058a1ec90f4b5b2eeabc98f60d5c6a289bd8e33f08df0.mp3.zip\n. ",
    "vitonzhangtt": "I have the similar problem \"Application 'UIKitApplication:com.evolve.myApp[0xf61]' was killed by jetsam.\". ",
    "Danbana": "Hooboy... also seeing this crash. No bueno!  It does not occur everytime either :/\nThe exception is in: EZRecorder line 448\n- (void)setClientFormat:(AudioStreamBasicDescription)clientFormat\n  {\n  [EZAudioUtilities checkResult:ExtAudioFileSetProperty(self.info->extAudioFileRef,\n                                                        kExtAudioFileProperty_ClientDataFormat,\n                                                        sizeof(clientFormat),\n                                                        &clientFormat)\n                      operation:\"Failed to set client format on recorded audio file\"];\n  self.info->clientFormat = clientFormat;\n  }\nWhen setting the client data format\n. ",
    "smspence": "Any updates on this issue?\nI am now receiving this crash and error on an iPhone 6 since updating to iOS 9.2:\nError: Failed to fill complex buffer in float converter ('insz')\nIt worked fine before on iOS 8.4 through 9.1\n. ",
    "snyuryev": "9.2 - same for me (iPhone 6)\n. @rbarbish Thank you very much! Works for me! \n(iPhone 6 - iOS 9.2 and 9.1)\n. ",
    "koooootake": "9.2 - same for me (iPhone 5S)\n. ",
    "dsmelov": "The source of this problem is in EZAudioFloatConverterCallback (EZAudioFloatConverter.m). This function should update ioData->mBuffers[i].mDataByteSize. For 16bit mono audio stream it will look like \nioData->mBuffers[0].mDataByteSize = *ioNumberDataPackets * sizeof(short);\n. ",
    "garsdle": "What is really strange is that the FFT example seems unaffected by this bug?\n. ",
    "BrandonZacharie": "I haven't tested the examples. I simply rebuilt my own project. The problem seems to have gone away on real devices but I still get crashes in the simulator. \u00af\\_(\u30c4)_/\u00af\n. ",
    "patriksharma": "any update on this ? \nI'm facing crash in Iphone 6s and IOS 9.2 \n. ",
    "trkfabi": "I don't know if it is related to what you guys are discussing, but I was getting this same error line when I tried to change the sample rate from 44100 to 8000 (for a guitar tuner app).\nWhat I did was basically use this line of code to initialize the mic instance with a different sample rate:\nAudioStreamBasicDescription inputFormat = [EZAudioUtilities monoFloatFormatWithSampleRate:SAMPLE_RATE];\n    self.microphone = [EZMicrophone microphoneWithDelegate:self withAudioStreamBasicDescription: inputFormat];\nif I used double const SAMPLE_RATE = 8000.0f; I'd get the error but it works with double const SAMPLE_RATE = 11025.0f; \n11025 is pretty enough for what I want so...\n. I'm using NVDSP library to perform filtering, but I'm not sure if I'm doing this right. Any comment is appreciated.\nThis is EZAudioFFT.m. I have included \n```\nimport \"NVDSP.h\"\nimport \"NVLowpassFilter.h\"\n```\n```\n- (float )computeFFTWithBuffer:(float )buffer withBufferSize:(UInt32)bufferSize\n{\n    if (buffer == NULL)\n    {\n        return NULL;\n    }\n//NVLowpassFilter *LPF = [[NVLowpassFilter alloc] initWithSamplingRate:self.sampleRate];\n//LPF.cornerFrequency = 400.0f;\n//LPF.Q = 0.8f;\n[self.LPF filterData:buffer numFrames:bufferSize numChannels:1];\n\n\n//\n// Calculate real + imaginary components and normalize\n//\nvDSP_Length log2n = log2f(bufferSize);\nlong nOver2 = bufferSize / 2;\nfloat mFFTNormFactor = 10.0 / (2 * bufferSize);\n\n// fabi\n//float *window = (float *)malloc(sizeof(float) * nOver2);\n//vDSP_hann_window(window, nOver2, 0); // Window the samples\n/////////\n\nvDSP_ctoz((COMPLEX*)buffer, 2, &(self.info->complexA), 1, nOver2);\nvDSP_fft_zrip(self.info->fftSetup, &(self.info->complexA), 1, log2n, FFT_FORWARD);\n\nvDSP_vsmul(self.info->complexA.realp, 1, &mFFTNormFactor, self.info->complexA.realp, 1, nOver2);\nvDSP_vsmul(self.info->complexA.imagp, 1, &mFFTNormFactor, self.info->complexA.imagp, 1, nOver2);\n\n// fabi\n//vDSP_vmul(self.info->complexA.realp, 1, window, 1, self.info->complexA.realp, 1, nOver2);\n//vDSP_vmul(self.info->complexA.imagp, 1, window, 1, self.info->complexA.imagp, 1, nOver2);\n/////////\n\nvDSP_zvmags(&(self.info->complexA), 1, self.info->outFFTData, 1, nOver2);\nvDSP_fft_zrip(self.info->fftSetup, &(self.info->complexA), 1, log2n, FFT_INVERSE);\nvDSP_ztoc(&(self.info->complexA), 1, (COMPLEX *) self.info->inversedFFTData , 2, nOver2);\nself.info->outFFTDataLength = nOver2;\n\n\n//\n// Calculate max freq\n//\nif (self.sampleRate > 0.0f)\n{\n    vDSP_maxvi(self.info->outFFTData, 1, &self.info->maxFrequencyMangitude, &self.info->maxFrequencyIndex, nOver2);\n    self.info->maxFrequency = [self frequencyAtIndex:self.info->maxFrequencyIndex];\n}\n\n//\n// Notify delegate\n//\nif ([self.delegate respondsToSelector:@selector(fft:updatedWithFFTData:bufferSize:)])\n{\n    [self.delegate fft:self\n    updatedWithFFTData:self.info->outFFTData\n            bufferSize:nOver2];\n}\n\n//\n// Return the FFT\n//\nreturn self.info->outFFTData;\n\n}\n```\nI read that filtering should be done in time domain before doing the fft, so I think this is the right place\n. ",
    "Xinaction": "@rbarbish @kenzukan  I'm no luck with your code. like @syedhali  said, buffer sizes that aren't 512 or 1024, in my device 6S(9.2) buffer size 940...crash..\n. It's so odd that demo without any error and buffer size is 1024 with iPhone6S 9.2\n. @snyuryev do you try iPhone6S iOS9.2?, all my devices are ok except 6S(iOS9.2)\n. @syedhali  I found the cause of the problem. The internal speaker on the iPhone 6S models only support a sample rate of 48kHz while previous iPhone models supported a collection of sample rates.\n1024 * (44100/48000) = 940.8, Micphone buffer size in EZAudio is 940 or 941 that crash.\nI cann't understand why the demo didn't crash. I had fix my code with 48k.\n. ",
    "cameronmccord2": "@syedhali solution fixed our problem. On the iOS 9.2 with iPhone 6s Plus we were having the \"Error: Failed to fill complex buffer in float converter ('insz')\" error. We fixed it by specifying the microphone sample rate at 48000. But, doing that then broke it for our iOS 9.2 iPhone 5s and caused the same error. We fixed it by adding this piece of code:\nswift\nif UIDevice.currentDevice().modelName.containsString(\"iPhone 6s\") {\n    let inputFormat = EZAudioUtilities.monoFloatFormatWithSampleRate(48000.0)\n    microphone = EZMicrophone(microphoneDelegate: self, withAudioStreamBasicDescription: inputFormat)\n}else{\n    microphone = EZMicrophone(microphoneDelegate: self)\n}\nCaveat: This may break when the iPhone 7 comes out because of how we are detecting the iPhone version.\n. ",
    "kajensen": "I can reproduce this on 6s when fetching audio from an EZMicrophone and transitioning the output from iphone speaker to apple tv airplay. Any thoughts on what might be the reason?\n. ",
    "nilshott": "+1\n. ",
    "keithics": "+1\n. ",
    "zbencz3": "I get this crash once in a while. Any idea why? If more details are needed will attach the setup, etc.\n. any ideas why? I have this issue keep popping back.\n. No. I can't reproduce it myself, but the end users experience it sometimes.\n. and what triggers it? easy to reproduce it for you?\n. this is how it looks in my case. So deallocated I presume.\nCrashed: com.apple.main-thread\nEXC_BAD_ACCESS KERN_INVALID_ADDRESS 0xe000000012d963e6\n. @nicole1314 do you use tag 1.1.2 ?\n. I am trying to see if that's the latest code usable..\n\n. does the new update help you? 1.1.4\n. haven't had a crash for a while, but I removed ezaudio from part of the functionality until it will be figured out or I can do something about it.\n. ",
    "DenisLaboureyras": "Any progress ? I'm interested in this feature as well :)\n. ",
    "ciso": "For me it worked...\nself.audioPlot.backgroundColor = UIColor.clearColor()\n        self.audioPlot.color = UIColor.fromRGB(0xfafafa)\n. ",
    "akac": "I've seen this happen as well randomly. \n. Using EZAudio with Swift has been pretty simple for me.\nplayer.audioFile.getWaveformDataWithNumberOfPoints(1024, completion: { [unowned self] (buffer: UnsafeMutablePointer<UnsafeMutablePointer<Float>>, bufferSize: Int32) in\n            self.audioPlot?.updateBuffer(buffer[0], withBufferSize: UInt32(bufferSize))\n        })\n. ",
    "nicole1314": "We also experiencing the same issue as #251 .\n. @zbencz3 I have no idea. Do you find anything might be helpful?\n. @zbencz3 I tried to debug. When it happens, the buffer is 0 and the buffersize is 1024.\n. @zbencz3 One out of 10 times I will get the crash.\n. @syedhali Buffer is a pointer so the buffer is not in the memory. We are using hasAudioReceived for the plot and hasBufferList for the recording.\npublic func microphone(microphone: EZMicrophone, hasAudioReceived buffer: UnsafeMutablePointer>, withBufferSize bufferSize: UInt32, withNumberOfChannels numberOfChannels: UInt32)\npublic func microphone(microphone: EZMicrophone, hasBufferList bufferList: UnsafeMutablePointer, withBufferSize bufferSize: UInt32, withNumberOfChannels numberOfChannels: UInt32)\n. @zbencz3 What do you mean by tag 1.1.2?\n. @zbencz3 I changed the tag from 1.1.2 to 1.1.4. I still have RMS crash. How about you?\n. @lluisgerard \nI tried 1.1.5 and it still crashes at line sum += buffer[i] * buffer[i];\nCan you please show me how to prevent this?\n- (float)RMS:(float *)buffer   length:(int)bufferSize\n  {\n  float sum = 0.0;\n  for(int i = 0; i < bufferSize; i++)\n      sum += buffer[i] * buffer[i];\n  return sqrtf( sum / bufferSize);\n  }\n. ",
    "lluisgerard": "The only workaround I found to fix this in my code is to check for the microphone delegate before sending the buffer around. Tested with 1.1.5\nThat's because on my EZRecorderDelegate -> recorderDidClose I have:\nfunc recorderDidClose(recorder: EZRecorder!) {\n        self.microphone?.delegate = nil\n    }\nAnd in my EZMicrophoneDelegate I just check for that delegate before sending the buffer:\nfunc microphone(microphone: EZMicrophone!, hasAudioReceived buffer: UnsafeMutablePointer<UnsafeMutablePointer<Float>>, withBufferSize bufferSize: UInt32, withNumberOfChannels numberOfChannels: UInt32) {\n        mainqueue { [weak self] in\n            guard let _ = self?.microphone?.delegate else {\n                print(\"You shall not pass!\")\n                return\n            }\n            // Your code here to update the wave, etc...\n        }\n    }\n- OT: the mainqueue func is just a regular dispatch_async(dispatch_get_main_queue(), closure) wrapper\n. Same problem here with 1.1.5, had to manually comment EZAudioOSX.h and EZAudioiOS.h imports to compile.\n. ",
    "magnus80a": "I get this too when app is returned into foreground from background. I didn't see this issue and created my own #314 . Maybe it should be closed, but I have a screenshot there that might be helpful for others.\n. Here are some methods to obtain the frequency\nhttps://ccrma.stanford.edu/~pdelac/154/m154paper.htm\nThe closest to calculating frequency I've found in EZAudio (1.1.5) is to use the FFT delegation and find the maximum bin in the resulting buffer and correlate it to frequency. There's a sample project for that, it might be enough for your tuner app.\n. I'm running EZAudio 1.1.5\n. I think this can be resolved by retaining the data before passing it on to the audio plot. Like this:\nlet samples: [Float] = self.convertToSamples(buffer, withBufferSize: bufferSize)\nself.audioPlot?.updateBuffer(&samples, withBufferSize: UInt32(samples.count))\nThis seems to solve the problem for me. Or at least make it a lot less probable to crash.\n. No, you can in my experience only have one and I believe it's even mentioned in the README.md\nUse EZAudioPlot instead if you need more than one.\n. ",
    "thuydx55": "@syedhali could you please take a look and merge this pull request? Thanks\n. ",
    "Boggartfly": "Yep, no problem man!\n. ",
    "steve21124": "@ trkfabi do you figure it out?\n. ",
    "hailucmg": "I have the same issue. To fix this issue, I add the following code before playing and it works normally. I don't know the reason, but hope it help.\nlet session: AVAudioSession = AVAudioSession.sharedInstance()\ndo {\n    try session.setCategory(AVAudioSessionCategoryPlayback)\n    try session.setActive(true)\n    try session.overrideOutputAudioPort(AVAudioSessionPortOverride.Speaker)\n} catch {\n}\n. Please take a look at issue #259 \n. ",
    "shuyangsun": "Thank you. I haven't tried it but I assume this code would fix the output volume issue, do you know if this would also fix input volume issue?\n. ",
    "rickyjwhong": "Same here, I set the volume to 1 but still get really low volume sounds. Any idea?\n. ",
    "tezcatlipoca": "Any luck figuring this out? I'm having the same problem with MOTU Ultralite via Firewire. In my case my application  just crashes on startup with that message, but runs fine when it's unplugged. Doesn't seem to matter if the MOTU is selected as a source or not, only if it's present on the bus. \n. I get this crash with any  >2 channel external sound device (I've tried two MOTUs and a different brand). I think it is a bug in how the channels are iterated, but I haven't had a chance to investigate further (I ended up reworking the soundboard so I could make it work with the built-in stereo jack :P ) \nIt doesn't matter if you're trying to use it, only if it's plugged in. \n. Just to say thank you - I finally got to test today and patch works great with my MOTU as well. Cheers.\n. ",
    "thermogl": "Not exactly the same error, but I'm getting a crash when trying to use a FireFace 802 over Firewire. No crash when it's not plugged in.\n. Yeah I can confirm the > 2 channels - I just created an aggregate device (built-in output and cinema display) with 4 channels and that crashes.\n. In + channelCountForScope:forDeviceID:\nReplace:\nUInt32 propSize = sizeof(streamConfiguration);\nWith:\nUInt32 propSize;\nAudioObjectGetPropertyDataSize(deviceID, &address, 0, NULL, &propSize);\n. with the same error?\n. I just had to make some further changes to stop it crashing with high channel counts, but optimisation doesn't make a difference over here.\nhttps://github.com/thermogl/EZAudio/commit/aed381d2085e285be8019dedf58ef36fa412a23f\n. Closing because this project appears dead.. ",
    "sarrass": "still no luck with\nNSArray *inputDevices = [EZAudioDevice inputDevices];\nafter changing +channelCountForScope:forDeviceID as you suggested...\n. When I turn off all compiler optimizations, using the -O0 flag, it works as it should, displaying all (external+internal) devices connected and working properly, sorry for the confusing message.\nI am further investigating whether it crashes somewhere in the strlen method when using any other optimization flag... maybe someone has an idea?\n. updated all files, everything works, all optimizations work :-)\nthanks alot!\n. ",
    "estebansolis": "Same \n. ",
    "gate8team": "+1, version 1.1.5, xCode 8.1. ",
    "andreaslindahl": "Cocoapods 0.39.0 and XCode 7.2\n. The error doesn't appear in any of my files, but in the EZAudioiOS.h file in the Cocoapods project.\n. EZAudioiOS.h isn't included in the Pod if I use EZAudio version 1.1.2\n. Seems like this file was changed for 1.1.3: https://github.com/syedhali/EZAudio/commit/470d3cdd8b84db9b25b7a8a3e87fd804034a8fbe.\nBut 1.1.3 doesn't seem to be available via Cocoapods, meaning I have used 1.1.2 up until now, so this is the first time I'm seeing this (old?) bug.\nMight be related to the two targets in your project for iOS and OS X. Don't know how pods work exactly, but I'm guessing targets are missing from the Cocoapods files...\n. I changed it to make it compile, as well. But I would rather not edit the files in my Cocoapods project...\n. Nope, still doesn't work. It looks like you're trying to remove these files in the podspec file, but that doesn't seem to work...\n. ",
    "fcjxxl": "I met the same problem, and I change EZAudioiOS.h and EZAudioOSX.h to #import \"EZAudio.h\", then it build succeeded\n. still\n/Pods/EZAudio/EZAudio/EZAudioiOS.h:26:9: 'EZAudioiOS/EZAudio.h' file not found\n. ",
    "brnunes": "Same problem on 1.1.4, had to change #import <EZAudioiOS/EZAudio.h> to #import \"EZAudio.h\" in EZAudioiOS.h and EZAudioOSX.h\n. ",
    "codezero-jp": "I use pod/diffs to change \n#import <EZAudioiOS/EZAudio.h>\nto \n#import \"EZAudio.h\"\n in EZAudioiOS.h and EZAudioOSX.h\n. ",
    "davidchappelle": "Seems like a super trivial thing to patch? can't we just get a working version that has the path fix that everyone is using above?\n. ",
    "abdultcb": "I got it fixed by changing\nimport \nto\nimport \"EZAudio.h\"\nin\nEZAudioiOS.h and EZAudioOSX.h\neven after updating to 1.1.5\n. ",
    "bideal": "@syedhali , thanks for your EZAudio project which saves me a lot of work, but please help solve this problem asap.\nThe error is obvious. the two files \"Pods/EZAudio/EZAudioiOS.h\" & \"Pods/EZAudio/EZAudioOSX.h\" that Cocoapods tries to build into the \"Products/EZAudio.framework\" import header files in this framework per se. \nSolution: please either remove these two header files or change the header reference in \"*.h\" format.\nThanks.\n. ",
    "shreyasthiagaraj": "I'm getting this same error:\n\"Error: Failed to create ExtAudioFileRef ('dta?')\"\nI see from the device logs that the application was killed by jetsam.\nHere is an audio file which reliably reproduces the crash:\n84b5ba842668ea22a90058a1ec90f4b5b2eeabc98f60d5c6a289bd8e33f08df0.mp3.zip\nAny ideas why?\n. @TianGongZhongYu is yours a Swift project? Here's a post that may be relevant to this issue:\nFrom an Apple engineer:\n\"This is a bug in the Swift SDK. Swift thinks ExtAudioFileRef is a CoreFoundation-style object that can retained and released, but that is not true. Please file a bug report so we can fix that.\nI can't think of any workaround that will convince Swift not to try to retain and release your ExtAudioFileRef objects.\"\nhttp://www.openradar.me/17211521\n. ",
    "bennettl": "This issue was brought up 5 months ago and is effecting Swift projects including mine. I was able to play the audio file right after i record it, but not after\n. THANK YOU @syedhali \n. ",
    "lastcommit": "If you look at the dta? code with this lookup site:\nhttps://www.osstatus.com/search/results?platform=all&framework=all&search=dta%3F\nYou can see that your audio file is corrupt. I ran into this when I tried to read a .wav file that was encoded with the mp3 codec.\nDigging further, I found this block in the EZAudioUtilities.m file:\n+ (void)checkResult:(OSStatus)result operation:(const char *)operation\n{\n    if (result == noErr) return;\n    char errorString[20];\n    // see if it appears to be a 4-char-code\n    *(UInt32 *)(errorString + 1) = CFSwapInt32HostToBig(result);\n    if (isprint(errorString[1]) && isprint(errorString[2]) && isprint(errorString[3]) && isprint(errorString[4]))\n    {\n        errorString[0] = errorString[5] = '\\'';\n        errorString[6] = '\\0';\n    } else\n        // no, format it as an integer\n        sprintf(errorString, \"%d\", (int)result);\n    fprintf(stderr, \"Error: %s (%s)\\n\", operation, errorString);\n    if (__shouldExitOnCheckResultFail)\n    {\n        exit(-1);\n    }\n}\nAnd this warning in EZAudioUtilities.h:\n```\n///-----------------------------------------------------------\n/// @name Debugging EZAudio\n///-----------------------------------------------------------\n/*\n Globally sets whether or not the program should exit if a checkResult:operation: operation fails. Currently the behavior on EZAudio is to quit if a checkResult:operation: fails, but this is not desirable in any production environment. Internally there are a lot of checkResult:operation: operations used on all the core classes. This should only ever be set to NO in production environments since a checkResult:operation: failing means something breaking has likely happened.\n @param shouldExitOnCheckResultFail A BOOL indicating whether or not the running program should exist due to a checkResult:operation: fail.\n /\n+ (void)setShouldExitOnCheckResultFail:(BOOL)shouldExitOnCheckResultFail;\n//------------------------------------------------------------------------------\n```\nSo it looks like EZAudio is designed to terminate the app if the audio check fails at any step of the process. You should change the shouldExitOnCheckResultFail param to \"NO\". Since the framework is no longer being developed it'll be up to you to write code that will handle a failed check.\n. ",
    "ewanmellor": "Duplicate of #272.\n. @jasper-chan Did you ever fix this?  I have the same backtrace right now.  Thanks.\n. I decided to switch to EZAudioPlot rather than EZAudioPlotGL, so I'm just ducking the problem.  Thanks for the help nonetheless, @jasper-chan.\n. ",
    "ryanholden8": "I also have this same question :) @syedhali \nThanks for the hard work and great library!\n. ",
    "szarif": "looking for help on this too!\n. ",
    "963431755": "me too!\n. ",
    "Nikilicious09": "The problem can be because of the device (whether it support lpcm or not) try changing the method to - (void)setClientFormat:(AudioStreamBasicDescription)clientFormat\n{\n    if(![FGAudioUtilities checkResult:ExtAudioFileSetProperty(self.info->extAudioFileRef,\n                                                              kExtAudioFileProperty_ClientDataFormat,\n                                                              sizeof(clientFormat),\n                                                              &clientFormat)\n                            operation:\"Failed to set client format on recorded audio file\"])\n    {\n        clientFormat.mFormatID = kAudioFileM4AType;\n    }\n    self.info->clientFormat = clientFormat;\n}\n. ",
    "dennislysenko": "One possible workaround would be to quickly copy the iTunes library URL to a file that you know EZAudioFile can work with. Here is a gist that provides a class with a single method to do just that:\nhttps://gist.github.com/abeldomingues/fe8fa797fd55603f2f4a\nI guess it would be preferable to be able to stream these MPMediaItems directly, but this should be a good enough workaround for now if it works. Also, change the audio format on line 50 to something more familiar (MP3/M4A) if it has trouble reading the PCM file, though I can't foresee that being a big issue seeing as PCM is supposedly pretty great for reading audio data.\n. ",
    "rikola": "Thanks. This looks like it will do the trick.\nI'm still not sure why the ipod URL prefix breaks EZAudioFile though. Strange stuff.\n. ",
    "bintu1234": "@rikola  I am also facing same issue in objective c, How you resolve thjis issue Please help me. ",
    "vickydhas": "Hey friends, Anybody got hold of this issue. Still get this error\nError: Failed to dispose of ext audio file (-50) error, and suspect this happens if the playing file is recorded with very small interval of 3-4 sec.. No response closing the issue with no resolution found.. ",
    "shoheiyokoyama": "I am also having this same problem. EZAudioPlotGL are drawn as expected, Only simulator.\nbelow EZMicrophone Delegate Method doesn't woek. However, func microphone(microphone: EZMicrophone!, changedPlayingState isPlaying: Bool) is work.\nswift\nfunc microphone(microphone: EZMicrophone!, hasAudioReceived buffer: UnsafeMutablePointer<UnsafeMutablePointer<Float>>, withBufferSize bufferSize: UInt32, withNumberOfChannels numberOfChannels: UInt32) \nfunc microphone(microphone: EZMicrophone!, hasBufferList bufferList: UnsafeMutablePointer<AudioBufferList>, withBufferSize bufferSize: UInt32, withNumberOfChannels numberOfChannels: UInt32)\nXcode: Version 7.3\nSwift 2.2\nEZAudio 1.1.5\niOS : iPhone6s(OS 9.2.1)\n. I've discovered that it only happens when the AVAudioSession category is set to AVAudioSessionCategoryPlayback.\nPlease AVAudioSession category set to AVAudioSessionCategoryRecord. EZAudioPlotGL are drawn as expected.\n. ",
    "karuppub": "Hi @shoheiyokoyama  thanks for your reply, will try and check\n. ",
    "parsley75": "Seconded! Not that I don't appreciate the effort already put in, but the learning curve for both is steep\n. @freak4pc i still think swift documentation would be good, but i will take your advice also. thanks.\n. thanks @freak4pc , i really could come round and hug you. \ni'd solved many of the delegate problems and that was the final piece of the puzzle.\ni suppose its the trouble with learning ios apps and swift and audio at the same time. thanks again \n. ",
    "freak4pc": "I don't see why a special documentation would be needed? Objective-C and Swift are pretty much interchangeable minus the bridging header. A good starting point would be the EZAudio-Swift project. \n. If there's a notion for doing this I'd love to help. Wouldn't mind migrating the README.md file to Swift and have a link connecting the two. \n@syedhali Would you accept a PR for this? \n. It should just be \n``` swift\nvar player: EZAudioFile!\nself.player = EZAudioPlayer(delegate: self)\n```\n. Sure thing. Good luck! \n. ",
    "EK230948230598": "It would really help beginners who are trying to use this library. I consider myself somewhere between the beginner and intermediate levels of Swift-based iOS development, and while I am now able to understand a good portion of Objective-C code pretty well, I still don't understand everything, and truthfully, I'd rather focus on Swift than spend time learning Objective-C at the moment. So having a Swift documentation would be very nice to really help us Swift-only developers take advantage of the library.\n. ",
    "ceeyang": "@jidanyu   Hi, I met the seam problem,  can you told me how did you solve it ? thanks.. ",
    "jidanyu": "@ceeyang \u8fd9\u4e2a\u95ee\u9898\u65f6\u95f4\u6709\u70b9\u957f\u4e86\uff0c\u6211\u4e0d\u8bb0\u5f97\u4e86\u3002. ",
    "Dino4674": "I am stupid stupid stupid. After 4 hours of trying I opened this issue and then I realized I did not set self.isRecording in my app. Thats what you get with c/p coding :) Closing this\n. ",
    "npalamar": "I use static frameworks too and have the same issue. Is the fix above is a correct way to resolve this kind of problem?\n. ",
    "osrl": "I don't know if this is the correct way either, but it works fine. It would be good if someone explained this.\n. ",
    "danfsd": "Happened to me too. I'm using Xcode 7.3 and CocoaPods 0.38.2.\nChanging the import directive to #import \"EZAudio.h\" fixed the issue for the moment.\nAny updates on the solving of this issue?\n. ",
    "spotby": "I am running into this too. \nYour suggestion fixed by bad access error. thanks!\n. ",
    "ryanfrancesconi": "Thank you - this has fixed my error as well.\n. ",
    "dsrees": "I'm having the same issue. I am unable to update to anything newer than 1.1.2 while platform :ios = \"7.0\". Bumping to :ios = \"8.0\" resolved the issue but my deployment target needs to stay at 7.0\n. ",
    "gevariya-ajit": "Having same problem, @norbertmocsnik do you have any solution?. ",
    "norbertmocsnik": "@gevariya-ajit Adding the if above in EZAudioFile.m seems to solve it technically although I'm not sure about the implications. It depends on how you use the data afterwards because now there's less data than expected. It works for my use case for now.. ",
    "ohld": "Any update on this? . ",
    "phohale": "The \"noise\" is the result of the physics of the situation.\nThe microphone records the environment at time t. \nAt time t+1, it plays the sound recorded at time t back through the passthrough out of the device speaker. Also at time t+1 the microphone records the environment which includes this passthrough sound. \nThis problem amplifies as time keeps passing, a very quiet noise will eventually get louder and louder as it compounds.\nThe solution to this problem is well-researched and called \"noise canceling\": it involves removing any output noise from the microphone's input, but due to the distortion of the environment between the speaker and the mic it isn't perfect.\n. ",
    "longpham2310": "Hi @SamerPTUK. I have the same issue. Have you got any solutions yet? Thanks!\n. ",
    "mydy1987": "I solve it with following codes.\nAVAudioSession *session = [AVAudioSession sharedInstance];\n[session setActive:YES error:nil];\n[session setCategory:AVAudioSessionCategoryPlayAndRecord error:nil];\n. ",
    "pratik1311": "Hi @etayluz \nI am getting same issue. \nCan you please help me how you solve your issue?\nThanks,\nPratik. ",
    "matanvr": "This fix is causing crashes in my app, any idea what the actual fix is?\n. ",
    "Tanisha27": "i want to decrease the frames from 60 fps to around 30-40 fps on x-axis\nalso can i display the time line??\n. ",
    "dantarakan": "Just a quick update:\nUpon inspecting waveformData, which is the Array extracted from the file, I noticed that I'm getting 56 zeros in the beginning of the array. So perhaps the problem lies within EZAudioFile.\nAs a temporary (dirty) workaround, I stripped zeros from the beginning:\nswift\n// Get data from file\nlet waveformData = audioFile!.getWaveformData()\n// Get buffer size\nlet bufferSize = waveformData.bufferSize\n// Get the pointer\nlet dataPointer = waveformData.buffers[0]\n// Initialise output array\nvar dataArray = [Float]()\n// Append all non-zero elements\nfor i in 0..<Int(bufferSize) {\n    if dataPointer[i] != Float(0.0) {\n        dataArray.append(dataPointer[i])\n    }\n}\n// Update audio plot\nself.audioPlot.updateBuffer(&dataArray, withBufferSize: UInt32(dataArray.count))\nI understand that this is not the correct way to do it, but it works for my purposes.\n. ",
    "jasper-chan": "@ewanmellor Yes I did, but I haven't kept up with updates from EZAudio so I wouldn't be able to give a pull request.\nThere was essentially a problem with the EAGLContext switching. So in EZAudioPlotGL.m instead of just saying\nif (!self.context)\n{\nself.context = [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];\n}\nWe changed it to:\nif (![EAGLContext currentContext])\n    {\n        self.context = [[EAGLContext alloc] initWithAPI:kEAGLRenderingAPIOpenGLES2];\n    }\n    else\n    {\n        self.context = [EAGLContext currentContext];\n    }\n. ",
    "titman": "Nice work! . ",
    "fortunar": "Running into the same issue. Any solutions?. ",
    "glm4": "For me, the problem was causing by appending buffer to the recorder after the closeFile().\nCheck out the example, they use an isRecording var to control this. Hope it helps. For me, the problem was causing by appending buffer to the recorder after the closeFile().\nCheck out the example, they use an isRecording var to control this. Hope it helps. ",
    "ysfox": "Can i do it with EZAudio??help . ",
    "LSDdev": "Lookin into circular plotting also, did you ever find any examples on circular drawing with EZAudio?\n. ",
    "andorfermichael": "I am interested in those plots too.. ",
    "wongshouxin": "hi,brother, do you resolve this problem, i want to expand buffer when i using delegate method for EZMicrophone, do you have any suggestion?. ",
    "JayakarVangeti": "@wongshouxin I guess we cant control it. Its predefined.. ",
    "calioptrix": "I've been comparing my project to the EZAudio-Swift project. On that project, when clicking on the build scheme (upper left corner where the name of the project is), they show the project as well as each of the three different pods. My project doesn't have a choice for the pods on the build menu. Is this my issue? How do I resolve this?. I \"solved\" the problem by downloading the sample project \nhttps://github.com/syedhali/EZAudio-Swift\nand importing all of my code, storyboard, and core data files into that.. ",
    "sriranjanr": "Were you able to get the apps compile and running? I was able to compile but I get osatomic is deprecated warnings and then the app crashes. Could someone help me.\n\n\n. I solved this run greyed out project by mentioning the output as executable. But now the application crashes. I am running the app on iphone6 using xcode 8.3. I tried audiokit but swift programming is not as obvious as objective c. I like the code style of ezaudio and ezaudio we can mention different mic inputs. I just want to get the ezaudio apps running on my phone.please help me.I am new to iphone app development. I am getting the following warnings \n\" OsAtomicAdd32Barrier is deprecated \". Is this the reason why the passthrough app is crashing?. \n\n. ",
    "tolik85": "Did you solve your problem?. ",
    "nguyentrongbang": "Have the same question. ",
    "sunil28011": "@Simeryn  have you got the solution for same. ",
    "czjeep": "@Simeryn I get the same problem, have you got the solution?. active audio session before could solve. i don't have the experience about ipod music library, su ni ma se.. ",
    "Simeryn": "No, @sunil28011  @czjeep  I didn't.. ",
    "MrLoh": "@audiokit16 you can close your own issues, you know. since it is deprecated and replaced by AudioKit you better ask there. . Well obviously they ate not interested in building android or react native. They are iOS developers. Dude that\u2019s not how open source works. You can run around spamming issues with requests that other people write code for you. Issues are for actual problems with this library, not requests to people to build code for you. \nHow about you figure out how to write a React native plugin yourself and just do it, it\u2019s a ton of work and nothing about it is straight forward in this case as it is very data heavy. \nAlso this is deprecated so probably no one will do any work on it again. . ",
    "jamesharvey2": "Hello, \nI already did .. and they said they're not interested in Android. and will not support framework in the near future.\nIt would be still great if you guys can make react native version. \nhttps://github.com/AudioKit/AudioKit/issues/1202. React Native ships with only bare minimum audio plugins... . Unfortunately, I'm not able to program something like AudioKit from scratch. I wish I could... but I'm only good enough to use what's already been programmed... . ",
    "WangMing1998": "Why does not support the ipod path play?If play ipod music library files need to convert the path and time consuming.Users quickly click cut song will appear a lot of problems.. "
}